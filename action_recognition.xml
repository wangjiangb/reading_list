<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 14 Jul 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EA-VTR: Event-Aware Video-Text Retrieval</title><link>http://arxiv.org/abs/2407.07478v1</link><description>Understanding the content of events occurring in the video and their inherenttemporal logic is crucial for video-text retrieval. However, web-crawledpre-training datasets often lack sufficient event information, and the widelyadopted video-level cross-modal contrastive learning also struggles to capturedetailed and complex video-text event alignment. To address these challenges,we make improvements from both data and model perspectives. In terms ofpre-training data, we focus on supplementing the missing specific event contentand event temporal transitions with the proposed event augmentation strategies.Based on the event-augmented data, we construct a novel Event-Aware Video-TextRetrieval model, ie, EA-VTR, which achieves powerful video-text retrievalability through superior video event awareness. EA-VTR can efficiently encodeframe-level and video-level visual representations simultaneously, enablingdetailed event content and complex event temporal cross-modal alignment,ultimately enhancing the comprehensive understanding of video events. Ourmethod not only significantly outperforms existing approaches on multipledatasets for Text-to-Video Retrieval and Video Action Recognition tasks, butalso demonstrates superior event content perceive ability on Multi-eventVideo-Text Retrieval and Video Moment Retrieval tasks, as well as outstandingevent temporal logic understanding ability on Test of Time task.</description><author>Zongyang Ma, Ziqi Zhang, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Yingmin Luo, Xu Li, Xiaojuan Qi, Ying Shan, Weiming Hu</author><pubDate>Wed, 10 Jul 2024 09:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07478v1</guid></item><item><title>Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space</title><link>http://arxiv.org/abs/2407.08572v1</link><description>Skeletal motion plays a pivotal role in human activity recognition (HAR).Recently, attack methods have been proposed to identify the universalvulnerability of skeleton-based HAR(S-HAR). However, the research ofadversarial transferability on S-HAR is largely missing. More importantly,existing attacks all struggle in transfer across unknown S-HAR models. Weobserved that the key reason is that the loss landscape of the actionrecognizers is rugged and sharp. Given the established correlation in priorstudies~\cite{qin2022boosting,wu2020towards} between loss landscape andadversarial transferability, we assume and empirically validate that smoothingthe loss landscape could potentially improve adversarial transferability onS-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy,which can effectively explore the model posterior space for a collection ofsurrogates without the need for re-training. Furthermore, to craft adversarialexamples along the motion manifold, we incorporate the attack gradient withinformation of the motion dynamics in a Bayesian manner. Evaluated on benchmarkdatasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach ashigh as 35.9\% and 45.5\% respectively. In comparison, current state-of-the-artskeletal attacks achieve only 3.6\% and 9.8\%. The high adversarialtransferability remains consistent across various surrogate, victim, and evendefense models. Through a comprehensive analysis of the results, we provideinsights on what surrogates are more likely to exhibit transferability, to shedlight on future research.</description><author>Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Xun Yang, Meng Wang, He Wang</author><pubDate>Thu, 11 Jul 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08572v1</guid></item></channel></rss>