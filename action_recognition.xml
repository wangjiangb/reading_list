<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 21 Jul 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v1</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 29 May 2023 09:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v1</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Improving Zero-Shot Action Recognition using Human Instruction with Text Description</title><link>http://arxiv.org/abs/2301.08874v2</link><description>Zero-shot action recognition, which recognizes actions in videos withouthaving received any training examples, is gaining wide attention considering itcan save labor costs and training time. Nevertheless, the performance ofzero-shot learning is still unsatisfactory, which limits its practicalapplication. To solve this problem, this study proposes a framework to improvezero-shot action recognition using human instructions with text descriptions.The proposed framework manually describes video contents, which incurs somelabor costs; in many situations, the labor costs are worth it. We manuallyannotate text features for each action, which can be a word, phrase, orsentence. Then by computing the matching degrees between the video and all textfeatures, we can predict the class of the video. Furthermore, the proposedmodel can also be combined with other models to improve its accuracy. Inaddition, our model can be continuously optimized to improve the accuracy byrepeating human instructions. The results with UCF101 and HMDB51 showed thatour model achieved the best accuracy and improved the accuracies of othermodels.</description><author>Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 12 Jun 2023 09:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08874v2</guid></item><item><title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2212.04761v2</link><description>Skeleton-based action recognition has attracted considerable attention due toits compact representation of the human body's skeletal sructure. Many recentmethods have achieved remarkable performance using graph convolutional networks(GCNs) and convolutional neural networks (CNNs), which extract spatial andtemporal features, respectively. Although spatial and temporal dependencies inthe human skeleton have been explored separately, spatio-temporal dependency israrely considered. In this paper, we propose the Spatio-Temporal Curve Network(STC-Net) to effectively leverage the spatio-temporal dependency of the humanskeleton. Our proposed network consists of two novel elements: 1) TheSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for GraphConvolution (DK-GC). The STC module dynamically adjusts the receptive field byidentifying meaningful node connections between every adjacent frame andgenerating spatio-temporal curves based on the identified node connections,providing an adaptive spatio-temporal coverage. In addition, we propose DK-GCto consider long-range dependencies, which results in a large receptive fieldwithout any additional parameters by applying an extended kernel to the givenadjacency matrices of the graph. Our STC-Net combines these two modules andachieves state-of-the-art performance on four skeleton-based action recognitionbenchmarks.</description><author>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 03:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04761v2</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>Graph Contrastive Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2301.10900v2</link><description>In the field of skeleton-based action recognition, current top-performinggraph convolutional networks (GCNs) exploit intra-sequence context to constructadaptive graphs for feature aggregation. However, we argue that such context isstill \textit{local} since the rich cross-sequence relations have not beenexplicitly investigated. In this paper, we propose a graph contrastive learningframework for skeleton-based action recognition (\textit{SkeletonGCL}) toexplore the \textit{global} context across all sequences. In specific,SkeletonGCL associates graph learning across sequences by enforcing graphs tobe class-discriminative, \emph{i.e.,} intra-class compact and inter-classdispersed, which improves the GCN capacity to distinguish various actionpatterns. Besides, two memory banks are designed to enrich cross-sequencecontext from two complementary levels, \emph{i.e.,} instance and semanticlevels, enabling graph contrastive learning in multiple context scales.Consequently, SkeletonGCL establishes a new training paradigm, and it can beseamlessly incorporated into current GCNs. Without loss of generality, wecombine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), andachieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. Thesource code will be available at\url{https://github.com/OliverHxh/SkeletonGCL}.</description><author>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</author><pubDate>Sat, 10 Jun 2023 11:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10900v2</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</title><link>http://arxiv.org/abs/2307.09238v1</link><description>As collaborative robots (cobots) continue to gain popularity in industrialmanufacturing, effective human-robot collaboration becomes crucial. Cobotsshould be able to recognize human actions to assist with assembly tasks and actautonomously. To achieve this, skeleton-based approaches are often used due totheir ability to generalize across various people and environments. Althoughbody skeleton approaches are widely used for action recognition, they may notbe accurate enough for assembly actions where the worker's fingers and handsplay a significant role. To address this limitation, we propose a method inwhich less detailed body skeletons are combined with highly detailed handskeletons. We investigate CNNs and transformers, the latter of which areparticularly adept at extracting and combining important information from bothskeleton types using attention. This paper demonstrates the effectiveness ofour proposed approach in enhancing action recognition in assembly scenarios.</description><author>Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Tue, 18 Jul 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09238v1</guid></item><item><title>How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</title><link>http://arxiv.org/abs/2306.05844v1</link><description>As the use of collaborative robots (cobots) in industrial manufacturingcontinues to grow, human action recognition for effective human-robotcollaboration becomes increasingly important. This ability is crucial forcobots to act autonomously and assist in assembly tasks. Recently,skeleton-based approaches are often used as they tend to generalize better todifferent people and environments. However, when processing skeletons alone,information about the objects a human interacts with is lost. Therefore, wepresent a novel approach of integrating object information into skeleton-basedaction recognition. We enhance two state-of-the-art methods by treating objectcenters as further skeleton joints. Our experiments on the assembly datasetIKEA ASM show that our approach improves the performance of thesestate-of-the-art methods to a large extent when combining skeleton joints withobjects predicted by a state-of-the-art instance segmentation model. Ourresearch sheds light on the benefits of combining skeleton joints with objectinformation for human action recognition in assembly tasks. We analyze theeffect of the object detector on the combination for action classification anddiscuss the important factors that must be taken into account.</description><author>Dustin Aganian, Mona Köhler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Fri, 09 Jun 2023 13:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05844v1</guid></item><item><title>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</title><link>http://arxiv.org/abs/2306.11046v1</link><description>Existing skeleton-based action recognition methods typically follow acentralized learning paradigm, which can pose privacy concerns when exposinghuman-related videos. Federated Learning (FL) has attracted much attention dueto its outstanding advantages in privacy-preserving. However, directly applyingFL approaches to skeleton videos suffers from unstable training. In this paper,we investigate and discover that the heterogeneous human topology graphstructure is the crucial factor hindering training stability. To address thislimitation, we pioneer a novel Federated Skeleton-based Action Recognition(FSAR) paradigm, which enables the construction of a globally generalized modelwithout accessing local sensitive data. Specifically, we introduce an AdaptiveTopology Structure (ATS), separating generalization and personalization bylearning a domain-invariant topology shared across clients and adomain-specific topology decoupled from global model aggregation.Furthermore,we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancybetween clients and server caused by distinct updating patterns throughaligning shallow block-wise motion features. Extensive experiments on multipledatasets demonstrate that FSAR outperforms state-of-the-art FL-based methodswhile inherently protecting privacy.</description><author>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si</author><pubDate>Mon, 19 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11046v1</guid></item><item><title>SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network</title><link>http://arxiv.org/abs/2306.17574v1</link><description>Recent advancements in technology have expanded the possibilities of humanaction recognition by leveraging 3D data, which offers a richer representationof actions through the inclusion of depth information, enabling more accurateanalysis of spatial and temporal characteristics. However, 3D human actionrecognition is a challenging task due to the irregularity and Disarrangement ofthe data points in action sequences. In this context, we present our novelmodel for human action recognition from fixed topology mesh sequences based onSpiral Auto-encoder and Transformer Network, namely SpATr. The proposed methodfirst disentangles space and time in the mesh sequences. Then, an auto-encoderis utilized to extract spatial geometrical features, and tiny transformer isused to capture the temporal evolution of the sequence. Previous methods eitheruse 2D depth images, sample skeletons points or they require a huge amount ofmemory leading to the ability to process short sequences only. In this work, weshow competitive recognition rate and high memory efficiency by building ourauto-encoder based on spiral convolutions, which are light weight convolutiondirectly applied to mesh data with fixed topologies, and by modeling temporalevolution using a attention, that can handle large sequences. The proposedmethod is evaluated on on two 3D human action datasets: MoVi and BMLrub fromthe Archive of Motion Capture As Surface Shapes (AMASS). The results analysisshows the effectiveness of our method in 3D human action recognition whilemaintaining high memory efficiency. The code will soon be made publiclyavailable.</description><author>Hamza Bouzid, Lahoucine Ballihi</author><pubDate>Fri, 30 Jun 2023 12:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17574v1</guid></item><item><title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</title><link>http://arxiv.org/abs/2211.13466v3</link><description>Contrastive learning has been proven beneficial for self-supervisedskeleton-based action recognition. Most contrastive learning methods utilizecarefully designed augmentations to generate different movement patterns ofskeletons for the same semantics. However, it is still a pending issue to applystrong augmentations, which distort the images/skeletons' structures and causesemantic loss, due to their resulting unstable training. In this paper, weinvestigate the potential of adopting strong augmentations and propose ageneral hierarchical consistent contrastive learning framework (HiCLR) forskeleton-based action recognition. Specifically, we first design a gradualgrowing augmentation policy to generate multiple ordered positive pairs, whichguide to achieve the consistency of the learned representation from differentviews. Then, an asymmetric loss is proposed to enforce the hierarchicalconsistency via a directional clustering operation in the feature space,pulling the representations from strongly augmented views closer to those fromweakly augmented views for better generalizability. Meanwhile, we propose andevaluate three kinds of strong augmentations for 3D skeletons to demonstratethe effectiveness of our method. Extensive experiments show that HiCLRoutperforms the state-of-the-art methods notably on three large-scale datasets,i.e., NTU60, NTU120, and PKUMMD.</description><author>Jiahang Zhang, Lilang Lin, Jiaying Liu</author><pubDate>Mon, 10 Jul 2023 11:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13466v3</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition</title><link>http://arxiv.org/abs/2305.18479v1</link><description>3D Convolutional Neural Networks are gaining increasing attention fromresearchers and practitioners and have found applications in many domains, suchas surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval. However, their widespread adoption is hindered by their highcomputational and memory requirements, especially when resource-constrainedsystems are targeted. This paper addresses the problem of mapping X3D, astate-of-the-art model in Human Action Recognition that achieves accuracy of95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflowgenerates an optimised stream-based hardware system, taking into account theavailable resources and off-chip memory characteristics of the FPGA device. Thegenerated designs push further the current performance-accuracy pareto front,and enable for the first time the targeting of such complex model architecturesfor the Human Action Recognition task.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18479v1</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v1</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Wed, 14 Jun 2023 20:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v1</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Action Recognition with Multi-stream Motion Modeling and Mutual Information Maximization</title><link>http://arxiv.org/abs/2306.07576v1</link><description>Action recognition has long been a fundamental and intriguing problem inartificial intelligence. The task is challenging due to the high dimensionalitynature of an action, as well as the subtle motion details to be considered.Current state-of-the-art approaches typically learn from articulated motionsequences in the straightforward 3D Euclidean space. However, the vanillaEuclidean space is not efficient for modeling important motion characteristicssuch as the joint-wise angular acceleration, which reveals the driving forcebehind the motion. Moreover, current methods typically attend to each channelequally and lack theoretical constrains on extracting task-relevant featuresfrom the input. In this paper, we seek to tackle these challenges from three aspects: (1) Wepropose to incorporate an acceleration representation, explicitly modeling thehigher-order variations in motion. (2) We introduce a novel Stream-GCN networkequipped with multi-stream components and channel attention, where differentrepresentations (i.e., streams) supplement each other towards a more preciseaction recognition while attention capitalizes on those important channels. (3)We explore feature-level supervision for maximizing the extraction oftask-relevant information and formulate this into a mutual information loss.Empirically, our approach sets the new state-of-the-art performance on threebenchmark datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our code isanonymously released at https://github.com/ActionR-Group/Stream-GCN, hoping toinspire the community.</description><author>Yuheng Yang, Haipeng Chen, Zhenguang Liu, Yingda Lyu, Beibei Zhang, Shuang Wu, Zhibo Wang, Kui Ren</author><pubDate>Tue, 13 Jun 2023 07:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07576v1</guid></item><item><title>FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes</title><link>http://arxiv.org/abs/2306.10858v1</link><description>A typical task in the field of video understanding is hand actionrecognition, which has a wide range of applications. Existing works eithermainly focus on full-body actions, or the defined action categories arerelatively coarse-grained. In this paper, we propose FHA-Kitchens, a noveldataset of fine-grained hand actions in kitchen scenes. In particular, we focuson human hand interaction regions and perform deep excavation to further refinehand action information and interaction regions. Our FHA-Kitchens datasetconsists of 2,377 video clips and 30,047 images collected from 8 differenttypes of dishes, and all hand interaction regions in each image are labeledwith high-quality fine-grained action classes and bounding boxes. We representthe action information in each hand interaction region as a triplet, resultingin a total of 878 action triplets. Based on the constructed dataset, webenchmark representative action recognition and detection models on thefollowing three tracks: (1) supervised learning for hand interaction region andobject detection, (2) supervised learning for fine-grained hand actionrecognition, and (3) intra- and inter-class domain generalization for handinteraction region detection. The experimental results offer compellingempirical evidence that highlights the challenges inherent in fine-grained handaction recognition, while also shedding light on potential avenues for futureresearch, particularly in relation to pre-training strategy, model design, anddomain generalization. The dataset will be released athttps://github.com/tingZ123/FHA-Kitchens.</description><author>Ting Zhe, Yongqian Li, Jing Zhang, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao</author><pubDate>Mon, 19 Jun 2023 12:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10858v1</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v1</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Si-Fan Zhang, Wen-Yue Chen, Ning Zhou, Hao Liu, Gui-Hong Lao</author><pubDate>Thu, 06 Jul 2023 03:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v1</guid></item><item><title>A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023</title><link>http://arxiv.org/abs/2307.06569v1</link><description>In this technical report, we present our findings from a study conducted onthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for ActionRecognition. Our research focuses on the innovative application of adifferentiable logic loss in the training to leverage the co-occurrencerelations between verb and noun, as well as the pre-trained Large LanguageModels (LLMs) to generate the logic rules for the adaptation to unseen actionlabels. Specifically, the model's predictions are treated as the truthassignment of a co-occurrence logic formula to compute the logic loss, whichmeasures the consistency between the predictions and the logic constraints. Byusing the verb-noun co-occurrence matrix generated from the dataset, we observea moderate improvement in model performance compared to our baseline framework.To further enhance the model's adaptability to novel action labels, weexperiment with rules generated using GPT-3.5, which leads to a slight decreasein performance. These findings shed light on the potential and challenges ofincorporating differentiable logic and LLMs for knowledge extraction inunsupervised domain adaptation for action recognition. Our final submission(entitled `NS-LLM') achieved the first place in terms of top-1 actionrecognition accuracy.</description><author>Yi Cheng, Ziwei Xu, Fen Fang, Dongyun Lin, Hehe Fan, Yongkang Wong, Ying Sun, Mohan Kankanhalli</author><pubDate>Thu, 13 Jul 2023 06:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06569v1</guid></item><item><title>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2208.10741v3</link><description>Graph convolutional networks (GCNs) are the most commonly used methods forskeleton-based action recognition and have achieved remarkable performance.Generating adjacency matrices with semantically meaningful edges isparticularly important for this task, but extracting such edges is challengingproblem. To solve this, we propose a hierarchically decomposed graphconvolutional network (HD-GCN) architecture with a novel hierarchicallydecomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes everyjoint node into several sets to extract major structurally adjacent and distantedges, and uses them to construct an HD-Graph containing those edges in thesame semantic spaces of a human skeleton. In addition, we introduce anattention-guided hierarchy aggregation (A-HA) module to highlight the dominanthierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-wayensemble method, which uses only joint and bone stream without any motionstream. The proposed model is evaluated and achieves state-of-the-artperformance on four large, popular datasets. Finally, we demonstrate theeffectiveness of our model with various comparative experiments.</description><author>Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 10:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10741v3</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2307.07791v1</link><description>Considering the instance-level discriminative ability, contrastive learningmethods, including MoCo and SimCLR, have been adapted from the original imagerepresentation learning task to solve the self-supervised skeleton-based actionrecognition task. These methods usually use multiple data streams (i.e., joint,motion, and bone) for ensemble learning, meanwhile, how to construct adiscriminative feature space within a single stream and effectively aggregatethe information from multiple streams remains an open problem. To this end, wefirst apply a new contrastive learning method called BYOL to learn fromskeleton data and formulate SkeletonBYOL as a simple yet effective baseline forself-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, wefurther present a joint Adversarial and Collaborative Learning (ACL) framework,which combines Cross-Model Adversarial Learning (CMAL) and Cross-StreamCollaborative Learning (CSCL). Specifically, CMAL learns single-streamrepresentation by cross-model adversarial loss to obtain more discriminativefeatures. To aggregate and interact with multi-stream information, CSCL isdesigned by generating similarity pseudo label of ensemble learning assupervision and guiding feature generation for individual streams. Exhaustiveexperiments on three datasets verify the complementary properties between CMALand CSCL and also verify that our method can perform favorably againststate-of-the-art methods using various evaluation protocols. Our code andmodels are publicly available at \url{https://github.com/Levigty/ACL}.</description><author>Tianyu Guo, Mengyuan Liu, Hong Liu, Wenhao Li, Jingwen Guo, Tao Wang, Yidi Li</author><pubDate>Sat, 15 Jul 2023 13:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07791v1</guid></item><item><title>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2307.01985v1</link><description>In the research field of few-shot learning, the main difference betweenimage-based and video-based is the additional temporal dimension for videos. Inrecent years, many approaches for few-shot action recognition have followed themetric-based methods, especially, since some works use the Transformer to getthe cross-attention feature of the videos or the enhanced prototype, and theresults are competitive. However, they do not mine enough information from theTransformer because they only focus on the feature of a single level. In ourpaper, we have addressed this problem. We propose an end-to-end method named"Task-Specific Alignment and Multiple Level Transformer Network (TSA-MLT)". Inour model, the Multiple Level Transformer focuses on the multiple-level featureof the support video and query video. Especially before Multiple LevelTransformer, we use task-specific TSA to filter unimportant or misleadingframes as a pre-processing. Furthermore, we adopt a fusion loss using two kindsof distance, the first is L2 sequence distance, which focuses on temporal orderalignment. The second one is Optimal transport distance, which focuses onmeasuring the gap between the appearance and semantics of the videos. Using asimple fusion network, we fuse the two distances element-wise, then use thecross-entropy loss as our fusion loss. Extensive experiments show our methodachieves state-of-the-art results on the HMDB51 and UCF101 datasets and acompetitive result on the benchmark of Kinetics and something-2-something V2datasets. Our code will be available at the URL:https://github.com/cofly2014/tsa-mlt.git</description><author>Fei Guo, Li Zhu, YiWang Wang</author><pubDate>Wed, 05 Jul 2023 03:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01985v1</guid></item><item><title>Human Action Recognition in Still Images Using ConViT</title><link>http://arxiv.org/abs/2307.08994v1</link><description>Understanding the relationship between different parts of the image plays acrucial role in many visual recognition tasks. Despite the fact thatConvolutional Neural Networks (CNNs) have demonstrated impressive results indetecting single objects, they lack the capability to extract the relationshipbetween various regions of an image, which is a crucial factor in human actionrecognition. To address this problem, this paper proposes a new module thatfunctions like a convolutional layer using Vision Transformer (ViT). Theproposed action recognition model comprises two components: the first part is adeep convolutional network that extracts high-level spatial features from theimage, and the second component of the model utilizes a Vision Transformer thatextracts the relationship between various regions of the image using thefeature map generated by the CNN output. The proposed model has been evaluatedon the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%mAP and 91.5% mAP results, respectively, which are promising compared to otherstate-of-the-art methods.</description><author>Seyed Rohollah Hosseyni, Hasan Taheri, Sanaz Seyedin, Ali Ahmad Rahmani</author><pubDate>Tue, 18 Jul 2023 07:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08994v1</guid></item><item><title>Optimizing ViViT Training: Time and Memory Reduction for Action Recognition</title><link>http://arxiv.org/abs/2306.04822v1</link><description>In this paper, we address the challenges posed by the substantial trainingtime and memory consumption associated with video transformers, focusing on theViViT (Video Vision Transformer) model, in particular the Factorised Encoderversion, as our baseline for action recognition tasks. The factorised encodervariant follows the late-fusion approach that is adopted by many state of theart approaches. Despite standing out for its favorable speed/accuracy tradeoffsamong the different variants of ViViT, its considerable training time andmemory requirements still pose a significant barrier to entry. Our method isdesigned to lower this barrier and is based on the idea of freezing the spatialtransformer during training. This leads to a low accuracy model if naivelydone. But we show that by (1) appropriately initializing the temporaltransformer (a module responsible for processing temporal information) (2)introducing a compact adapter model connecting frozen spatial representations((a module that selectively focuses on regions of the input image) to thetemporal transformer, we can enjoy the benefits of freezing the spatialtransformer without sacrificing accuracy. Through extensive experimentationover 6 benchmarks, we demonstrate that our proposed training strategysignificantly reduces training costs (by $\sim 50\%$) and memory consumptionwhile maintaining or slightly improving performance by up to 1.79\% compared tothe baseline model. Our approach additionally unlocks the capability to utilizelarger image transformer models as our spatial transformer and access moreframes with the same memory consumption.</description><author>Shreyank N Gowda, Anurag Arnab, Jonathan Huang</author><pubDate>Thu, 08 Jun 2023 00:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04822v1</guid></item><item><title>Multimodal Distillation for Egocentric Action Recognition</title><link>http://arxiv.org/abs/2307.07483v2</link><description>The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views. We release our code athttps://github.com/gorjanradevski/multimodal-distillation.</description><author>Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</author><pubDate>Tue, 18 Jul 2023 10:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07483v2</guid></item><item><title>Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</title><link>http://arxiv.org/abs/2306.01075v1</link><description>Accurate understanding and prediction of human behaviors are criticalprerequisites for autonomous vehicles, especially in highly dynamic andinteractive scenarios such as intersections in dense urban areas. In this work,we aim at identifying crossing pedestrians and predicting their futuretrajectories. To achieve these goals, we not only need the context informationof road geometry and other traffic participants but also need fine-grainedinformation of the human pose, motion and activity, which can be inferred fromhuman keypoints. In this paper, we propose a novel multi-task learningframework for pedestrian crossing action recognition and trajectory prediction,which utilizes 3D human keypoints extracted from raw sensor data to capturerich information on human pose and activity. Moreover, we propose to apply twoauxiliary tasks and contrastive learning to enable auxiliary supervisions toimprove the learned keypoints representation, which further enhances theperformance of major tasks. We validate our approach on a large-scale in-housedataset, as well as a public benchmark dataset, and show that our approachachieves state-of-the-art performance on a wide range of evaluation metrics.The effectiveness of each model component is validated in a detailed ablationstudy.</description><author>Jiachen Li, Xinwei Shi, Feiyu Chen, Jonathan Stroud, Zhishuai Zhang, Tian Lan, Junhua Mao, Jeonhyung Kang, Khaled S. Refaat, Weilong Yang, Eugene Ie, Congcong Li</author><pubDate>Thu, 01 Jun 2023 19:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01075v1</guid></item><item><title>High-Performance Inference Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2305.18710v1</link><description>Recently, significant achievements have been made in skeleton-based humanaction recognition with the emergence of graph convolutional networks (GCNs).However, the state-of-the-art (SOTA) models used for this task focus onconstructing more complex higher-order connections between joint nodes todescribe skeleton information, which leads to complex inference processes andhigh computational costs, resulting in reduced model's practicality. To addressthe slow inference speed caused by overly complex model structures, weintroduce re-parameterization and over-parameterization techniques to GCNs, andpropose two novel high-performance inference graph convolutional networks,namely HPI-GCN-RP and HPI-GCN-OP. HPI-GCN-RP uses re-parameterization techniqueto GCNs to achieve a higher inference speed with competitive model performance.HPI-GCN-OP further utilizes over-parameterization technique to bringsignificant performance improvement with inference speed slightly decreased.Experimental results on the two skeleton-based action recognition datasetsdemonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves anaccuracy of 93% on the cross-subject split of the NTU-RGB+D 60 dataset, and90.1% on the cross-subject benchmark of the NTU-RGB+D 120 dataset and is 4.5times faster than HD-GCN at the same accuracy.</description><author>Ziao Li, Junyi Wang, Guhong Nie</author><pubDate>Tue, 30 May 2023 04:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18710v1</guid></item><item><title>High-order Tensor Pooling with Attention for Action Recognition</title><link>http://arxiv.org/abs/2110.05216v2</link><description>We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.</description><author>Piotr Koniusz, Lei Wang, Ke Sun</author><pubDate>Thu, 20 Jul 2023 15:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05216v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v1</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Wed, 24 May 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v1</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v2</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Mon, 19 Jun 2023 09:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v2</guid></item><item><title>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</title><link>http://arxiv.org/abs/2307.07469v1</link><description>Recognizing interactive action plays an important role in human-robotinteraction and collaboration. Previous methods use late fusion andco-attention mechanism to capture interactive relations, which have limitedlearning capability or inefficiency to adapt to more interacting entities. Withassumption that priors of each entity are already known, they also lackevaluations on a more general setting addressing the diversity of subjects. Toaddress these problems, we propose an Interactive Spatiotemporal TokenAttention Network (ISTA-Net), which simultaneously model spatial, temporal, andinteractive relations. Specifically, our network contains a tokenizer topartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way torepresent motions of multiple diverse entities. By extending the entitydimension, ISTs provide better interactive representations. To jointly learnalong three dimensions in ISTs, multi-head self-attention blocks integratedwith 3D convolutions are designed to capture inter-token correlations. Whenmodeling correlations, a strict entity ordering is usually irrelevant forrecognizing interactive actions. To this end, Entity Rearrangement is proposedto eliminate the orderliness in ISTs for interchangeable entities. Extensiveexperiments on four datasets verify the effectiveness of ISTA-Net byoutperforming state-of-the-art methods. Our code is publicly available athttps://github.com/Necolizer/ISTA-Net</description><author>Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu</author><pubDate>Fri, 14 Jul 2023 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07469v1</guid></item><item><title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</title><link>http://arxiv.org/abs/2307.07768v1</link><description>Classifying player actions from soccer videos is a challenging problem, whichhas become increasingly important in sports analytics over the years. Moststate-of-the-art methods employ highly complex offline networks, which makes itdifficult to deploy such models in resource constrained scenarios. Here, inthis paper we propose a novel end-to-end knowledge distillation based transferlearning network pre-trained on the Kinetics400 dataset and then performextensive analysis on the learned framework by introducing a unique lossparameterization. We also introduce a new dataset named SoccerDB1 containing448 videos and consisting of 4 diverse classes each of players playing soccer.Furthermore, we introduce an unique loss parameter that help us linearly weighthe extent to which the predictions of each network are utilized. Finally, wealso perform a thorough performance study using various changedhyperparameters. We also benchmark the first classification results on the newSoccerDB1 dataset obtaining 67.20% validation accuracy. Apart fromoutperforming prior arts significantly, our model also generalizes to newdatasets easily. The dataset has been made publicly available at:https://bit.ly/soccerdb1</description><author>Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</author><pubDate>Sat, 15 Jul 2023 11:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07768v1</guid></item><item><title>Multi-Dimensional Refinement Graph Convolutional Network with Robust Decouple Loss for Fine-Grained Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2306.15321v1</link><description>Graph convolutional networks have been widely used in skeleton-based actionrecognition. However, existing approaches are limited in fine-grained actionrecognition due to the similarity of inter-class data. Moreover, the noisy datafrom pose extraction increases the challenge of fine-grained recognition. Inthis work, we propose a flexible attention block called Channel-VariableSpatial-Temporal Attention (CVSTA) to enhance the discriminative power ofspatial-temporal joints and obtain a more compact intra-class featuredistribution. Based on CVSTA, we construct a Multi-Dimensional Refinement GraphConvolutional Network (MDR-GCN), which can improve the discrimination amongchannel-, joint- and frame-level features for fine-grained actions.Furthermore, we propose a Robust Decouple Loss (RDL), which significantlyboosts the effect of the CVSTA and reduces the impact of noise. The proposedmethod combining MDR-GCN with RDL outperforms the known state-of-the-artskeleton-based approaches on fine-grained datasets, FineGym99 and FSD-10, andalso on the coarse dataset NTU-RGB+D X-view version.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Jin-Rong Zhang, Kai-Yuan Liu, Si-Fan Zhang, Fei-Long Wang, Gao Huang</author><pubDate>Tue, 27 Jun 2023 10:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15321v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v1</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 13 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v1</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>How can objects help action recognition?</title><link>http://arxiv.org/abs/2306.11726v1</link><description>Current state-of-the-art video models process a video clip as a long sequenceof spatio-temporal tokens. However, they do not explicitly model objects, theirinteractions across the video, and instead process all the tokens in the video.In this paper, we investigate how we can use knowledge of objects to designbetter video models, namely to process fewer tokens and to improve recognitionaccuracy. This is in contrast to prior works which either drop tokens at thecost of accuracy, or increase accuracy whilst also increasing the computationrequired. First, we propose an object-guided token sampling strategy thatenables us to retain a small fraction of the input tokens with minimal impacton accuracy. And second, we propose an object-aware attention module thatenriches our feature representation with object information and improvesoverall accuracy. Our resulting framework achieves better performance whenusing fewer tokens than strong baselines. In particular, we match our baselinewith 30%, 40%, and 60% of the input tokens on SomethingElse,Something-something v2, and Epic-Kitchens, respectively. When we use our modelto process the same number of tokens as our baseline, we improve by 0.6 to 4.2points on these datasets.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11726v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v2</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Sun, 16 Jul 2023 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v2</guid></item><item><title>Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective</title><link>http://arxiv.org/abs/2208.07365v2</link><description>Unsupervised video domain adaptation is a practical yet challenging task. Inthis work, for the first time, we tackle it from a disentanglement view. Ourkey idea is to handle the spatial and temporal domain divergence separatelythrough disentanglement. Specifically, we consider the generation ofcross-domain videos from two sets of latent factors, one encoding the staticinformation and another encoding the dynamic information. A Transfer SequentialVAE (TranSVAE) framework is then developed to model such generation. To betterserve for adaptation, we propose several objectives to constrain the latentfactors. With these constraints, the spatial divergence can be readily removedby disentangling the static domain-specific information out, and the temporaldivergence is further reduced from both frame- and video-levels throughadversarial learning. Extensive experiments on the UCF-HMDB, Jester, andEpic-Kitchens datasets verify the effectiveness and superiority of TranSVAEcompared with several state-of-the-art methods. The code with reproducibleresults is publicly accessible.</description><author>Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin</author><pubDate>Fri, 09 Jun 2023 16:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07365v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Humans in 4D: Reconstructing and Tracking Humans with Transformers</title><link>http://arxiv.org/abs/2305.20091v2</link><description>We present an approach to reconstruct humans and track them over time. At thecore of our approach, we propose a fully "transformerized" version of a networkfor human mesh recovery. This network, HMR 2.0, advances the state of the artand shows the capability to analyze unusual poses that have in the past beendifficult to reconstruct from single images. To analyze video, we use 3Dreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.This enables us to deal with multiple people and maintain identities throughocclusion events. Our complete approach, 4DHumans, achieves state-of-the-artresults for tracking people from monocular video. Furthermore, we demonstratethe effectiveness of HMR 2.0 on the downstream task of action recognition,achieving significant improvements over previous pose-based action recognitionapproaches. Our code and models are available on the project website:https://shubham-goel.github.io/4dhumans/.</description><author>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Thu, 29 Jun 2023 06:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20091v2</guid></item><item><title>Volterra Neural Networks (VNNs)</title><link>http://arxiv.org/abs/1910.09616v5</link><description>The importance of inference in Machine Learning (ML) has led to an explosivenumber of different proposals in ML, and particularly in Deep Learning. In anattempt to reduce the complexity of Convolutional Neural Networks, we propose aVolterra filter-inspired Network architecture. This architecture introducescontrolled non-linearities in the form of interactions between the delayedinput samples of data. We propose a cascaded implementation of VolterraFiltering so as to significantly reduce the number of parameters required tocarry out the same classification task as that of a conventional NeuralNetwork. We demonstrate an efficient parallel implementation of this VolterraNeural Network (VNN), along with its remarkable performance while retaining arelatively simpler and potentially more tractable structure. Furthermore, weshow a rather sophisticated adaptation of this network to nonlinearly fuse theRGB (spatial) information and the Optical Flow (temporal) information of avideo sequence for action recognition. The proposed approach is evaluated onUCF-101 and HMDB-51 datasets for action recognition, and is shown to outperformstate of the art CNN approaches.</description><author>Siddharth Roheda, Hamid Krim</author><pubDate>Thu, 15 Jun 2023 17:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.09616v5</guid></item><item><title>Is end-to-end learning enough for fitness activity recognition?</title><link>http://arxiv.org/abs/2305.08191v1</link><description>End-to-end learning has taken hold of many computer vision tasks, inparticular, related to still images, with task-specific optimization yieldingvery strong performance. Nevertheless, human-centric action recognition isstill largely dominated by hand-crafted pipelines, and only individualcomponents are replaced by neural networks that typically operate on individualframes. As a testbed to study the relevance of such pipelines, we present a newfully annotated video dataset of fitness activities. Any recognitioncapabilities in this domain are almost exclusively a function of human posesand their temporal dynamics, so pose-based solutions should perform well. Weshow that, with this labelled data, end-to-end learning on raw pixels cancompete with state-of-the-art action recognition pipelines based on poseestimation. We also show that end-to-end learning can support temporallyfine-grained tasks such as real-time repetition counting.</description><author>Antoine Mercier, Guillaume Berger, Sunny Panchal, Florian Letsch, Cornelius Boehm, Nahua Kang, Ingo Bax, Roland Memisevic</author><pubDate>Sun, 14 May 2023 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08191v1</guid></item><item><title>EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video</title><link>http://arxiv.org/abs/2307.05784v1</link><description>In egocentric action recognition a single population model is typicallytrained and subsequently embodied on a head-mounted device, such as anaugmented reality headset. While this model remains static for new users andenvironments, we introduce an adaptive paradigm of two phases, where afterpretraining a population model, the model adapts on-device and online to theuser's experience. This setting is highly challenging due to the change frompopulation to user domain and the distribution shifts in the user's datastream. Coping with the latter in-stream distribution shifts is the focus ofcontinual learning, where progress has been rooted in controlled benchmarks butchallenges faced in real-world applications often remain unaddressed. Weintroduce EgoAdapt, a benchmark for real-world egocentric action recognitionthat facilitates our two-phased adaptive paradigm, and real-world challengesnaturally occur in the egocentric video streams from Ego4d, such as long-tailedaction distributions and large-scale classification over 2740 actions. Weintroduce an evaluation framework that directly exploits the user's data streamwith new metrics to measure the adaptation gain over the population model,online generalization, and hindsight performance. In contrast to single-streamevaluation in existing works, our framework proposes a meta-evaluation thataggregates the results from 50 independent user streams. We provide anextensive empirical study for finetuning and experience replay.</description><author>Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway</author><pubDate>Tue, 11 Jul 2023 21:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05784v1</guid></item><item><title>HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding</title><link>http://arxiv.org/abs/2307.05721v1</link><description>Understanding comprehensive assembly knowledge from videos is critical forfuturistic ultra-intelligent industry. To enable technological breakthrough, wepresent HA-ViD - the first human assembly video dataset that featuresrepresentative industrial assembly scenarios, natural procedural knowledgeacquisition process, and consistent human-robot shared annotations.Specifically, HA-ViD captures diverse collaboration patterns of real-worldassembly, natural human behaviors and learning progression during assembly, andgranulate action annotations to subject, action verb, manipulated object,target object, and tool. We provide 3222 multi-view, multi-modality videos(each video contains one assembly task), 1.5M frames, 96K temporal labels and2M spatial labels. We benchmark four foundational video understanding tasks:action recognition, action segmentation, object detection and multi-objecttracking. Importantly, we analyze their performance for comprehending knowledgein assembly progress, process efficiency, task collaboration, skill parametersand human intention. Details of HA-ViD is available at:https://iai-hrc.github.io/ha-vid.</description><author>Hao Zheng, Regina Lee, Yuqian Lu</author><pubDate>Sun, 09 Jul 2023 09:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05721v1</guid></item><item><title>Bullying10K: A Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition</title><link>http://arxiv.org/abs/2306.11546v1</link><description>The prevalence of violence in daily life poses significant threats toindividuals' physical and mental well-being. Using surveillance cameras inpublic spaces has proven effective in proactively deterring and preventing suchincidents. However, concerns regarding privacy invasion have emerged due totheir widespread deployment. To address the problem, we leverage Dynamic VisionSensors (DVS) cameras to detect violent incidents and preserve privacy since itcaptures pixel brightness variations instead of static imagery. We introducethe Bullying10K dataset, encompassing various actions, complex movements, andocclusions from real-life scenarios. It provides three benchmarks forevaluating different tasks: action recognition, temporal action localization,and pose estimation. With 10,000 event segments, totaling 12 billion events and255 GB of data, Bullying10K contributes significantly by balancing violencedetection and personal privacy persevering. And it also poses a challenge tothe neuromorphic dataset. It will serve as a valuable resource for training anddeveloping privacy-protecting video systems. The Bullying10K opens newpossibilities for innovative approaches in these domains.</description><author>Yiting Dong, Yang Li, Dongcheng Zhao, Guobin Shen, Yi Zeng</author><pubDate>Tue, 20 Jun 2023 14:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11546v1</guid></item><item><title>GoferBot: A Visual Guided Human-Robot Collaborative Assembly System</title><link>http://arxiv.org/abs/2304.08840v2</link><description>The current transformation towards smart manufacturing has led to a growingdemand for human-robot collaboration (HRC) in the manufacturing process.Perceiving and understanding the human co-worker's behaviour introduceschallenges for collaborative robots to efficiently and effectively performtasks in unstructured and dynamic environments. Integrating recent data-drivenmachine vision capabilities into HRC systems is a logical next step inaddressing these challenges. However, in these cases, off-the-shelf componentsstruggle due to generalisation limitations. Real-world evaluation is requiredin order to fully appreciate the maturity and robustness of these approaches.Furthermore, understanding the pure-vision aspects is a crucial first stepbefore combining multiple modalities in order to understand the limitations. Inthis paper, we propose GoferBot, a novel vision-based semantic HRC system for areal-world assembly task. It is composed of a visual servoing module thatreaches and grasps assembly parts in an unstructured multi-instance and dynamicenvironment, an action recognition module that performs human action predictionfor implicit communication, and a visual handover module that uses theperceptual understanding of human behaviour to produce an intuitive andefficient collaborative assembly experience. GoferBot is a novel assemblysystem that seamlessly integrates all sub-modules by utilising implicitsemantic information purely from visual perception.</description><author>Zheyu Zhuang, Yizhak Ben-Shabat, Jiahao Zhang, Stephen Gould, Robert Mahony</author><pubDate>Wed, 17 May 2023 08:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08840v2</guid></item><item><title>Theater Aid System for the Visually Impaired Through Transfer Learning of Spatio-Temporal Graph Convolution Networks</title><link>http://arxiv.org/abs/2306.16357v1</link><description>The aim of this research is to recognize human actions performed on stage toaid visually impaired and blind individuals. To achieve this, we have created atheatre human action recognition system that uses skeleton data captured bydepth image as input. We collected new samples of human actions in a theatreenvironment, and then tested the transfer learning technique with threepre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based humanaction recognition: the spatio-temporal graph convolution network, thetwo-stream adaptive graph convolution network, and the multi-scale disentangledunified graph convolution network. We selected the NTU-RGBD human actionbenchmark as the source domain and used our collected dataset as the targetdomain. We analyzed the transferability of the pre-trained models and proposedtwo configurations to apply and adapt the transfer learning technique to thediversity between the source and target domains. The use of transfer learninghelped to improve the performance of the human action system within the contextof theatre. The results indicate that Spatio-Temporal Graph ConvolutionNetworks is positively transferred, and there was an improvement in performancecompared to the baseline without transfer learning.</description><author>Leyla Benhamida, Slimane Larabi</author><pubDate>Wed, 28 Jun 2023 17:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16357v1</guid></item><item><title>Learning Higher-order Object Interactions for Keypoint-based Video Understanding</title><link>http://arxiv.org/abs/2305.09539v1</link><description>Action recognition is an important problem that requires identifying actionsin video by learning complex interactions across scene actors and objects.However, modern deep-learning based networks often require significantcomputation, and may capture scene context using various modalities thatfurther increases compute costs. Efficient methods such as those used for AR/VRoften only use human-keypoint information but suffer from a loss of scenecontext that hurts accuracy. In this paper, we describe an action-localizationmethod, KeyNet, that uses only the keypoint data for tracking and actionrecognition. Specifically, KeyNet introduces the use of object based keypointinformation to capture context in the scene. Our method illustrates how tobuild a structured intermediate representation that allows modelinghigher-order interactions in the scene from object and human keypoints withoutusing any RGB information. We find that KeyNet is able to track and classifyhuman actions at just 5 FPS. More importantly, we demonstrate that objectkeypoints can be modeled to recover any loss in context from using keypointinformation over AVA action and Kinetics datasets.</description><author>Yi Huang, Asim Kadav, Farley Lai, Deep Patel, Hans Peter Graf</author><pubDate>Tue, 16 May 2023 16:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09539v1</guid></item><item><title>HomE: Homography-Equivariant Video Representation Learning</title><link>http://arxiv.org/abs/2306.01623v1</link><description>Recent advances in self-supervised representation learning have enabled moreefficient and robust model performance without relying on extensive labeleddata. However, most works are still focused on images, with few working onvideos and even fewer on multi-view videos, where more powerful inductivebiases can be leveraged for self-supervision. In this work, we propose a novelmethod for representation learning of multi-view videos, where we explicitlymodel the representation space to maintain Homography Equivariance (HomE). Ourmethod learns an implicit mapping between different views, culminating in arepresentation space that maintains the homography relationship betweenneighboring views. We evaluate our HomE representation via action recognitionand pedestrian intent prediction as downstream tasks. On action classification,our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better thanmost state-of-the-art self-supervised learning methods. Similarly, on the STIPdataset, we outperform the state-of-the-art by 6% for pedestrian intentprediction one second into the future while also obtaining an accuracy of 91.2%for pedestrian action (cross vs. not-cross) classification. Code is availableat https://github.com/anirudhs123/HomE.</description><author>Anirudh Sriram, Adrien Gaidon, Jiajun Wu, Juan Carlos Niebles, Li Fei-Fei, Ehsan Adeli</author><pubDate>Fri, 02 Jun 2023 16:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01623v1</guid></item><item><title>CMD: Self-supervised 3D Action Representation Learning with Cross-modal Mutual Distillation</title><link>http://arxiv.org/abs/2208.12448v3</link><description>In 3D action recognition, there exists rich complementary information betweenskeleton modalities. Nevertheless, how to model and utilize this informationremains a challenging problem for self-supervised 3D action representationlearning. In this work, we formulate the cross-modal interaction as abidirectional knowledge distillation problem. Different from classicdistillation solutions that transfer the knowledge of a fixed and pre-trainedteacher to the student, in this work, the knowledge is continuously updated andbidirectionally distilled between modalities. To this end, we propose a newCross-modal Mutual Distillation (CMD) framework with the following designs. Onthe one hand, the neighboring similarity distribution is introduced to modelthe knowledge learned in each modality, where the relational information isnaturally suitable for the contrastive frameworks. On the other hand,asymmetrical configurations are used for teacher and student to stabilize thedistillation process and to transfer high-confidence information betweenmodalities. By derivation, we find that the cross-modal positive mining inprevious works can be regarded as a degenerated version of our CMD. We performextensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets.Our approach outperforms existing self-supervised methods and sets a series ofnew records. The code is available at: https://github.com/maoyunyao/CMD</description><author>Yunyao Mao, Wengang Zhou, Zhenbo Lu, Jiajun Deng, Houqiang Li</author><pubDate>Thu, 25 May 2023 15:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12448v3</guid></item><item><title>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events</title><link>http://arxiv.org/abs/2005.04490v6</link><description>Along with the development of modern smart cities, human-centric videoanalysis has been encountering the challenge of analyzing diverse and complexevents in real scenes. A complex event relates to dense crowds, anomalousindividuals, or collective behaviors. However, limited by the scale andcoverage of existing video datasets, few human analysis approaches havereported their performances on such complex events. To this end, we present anew large-scale dataset with comprehensive annotations, named Human-in-Eventsor HiEve (Human-centric video analysis in complex Events), for theunderstanding of human motions, poses, and actions in a variety of realisticevents, especially in crowd &amp; complex events. It contains a record number ofposes (&gt;1M), the largest number of action instances (&gt;56k) under complexevents, as well as one of the largest numbers of trajectories lasting forlonger time (with an average trajectory length of &gt;480 frames). Based on itsdiverse annotation, we present two simple baselines for action recognition andpose estimation, respectively. They leverage cross-label information duringtraining to enhance the feature learning in corresponding visual tasks.Experiments show that they could boost the performance of existing actionrecognition and pose estimation pipelines. More importantly, they prove thewidely ranged annotations in HiEve can improve various video tasks.Furthermore, we conduct extensive experiments to benchmark recent videoanalysis approaches together with our baseline methods, demonstrating HiEve isa challenging dataset for human-centric video analysis. We expect that thedataset will advance the development of cutting-edge techniques inhuman-centric analysis and the understanding of complex events. The dataset isavailable at http://humaninevents.org</description><author>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe</author><pubDate>Thu, 13 Jul 2023 14:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.04490v6</guid></item><item><title>Physical Adversarial Attacks for Surveillance: A Survey</title><link>http://arxiv.org/abs/2305.01074v1</link><description>Modern automated surveillance techniques are heavily reliant on deep learningmethods. Despite the superior performance, these learning systems areinherently vulnerable to adversarial attacks - maliciously crafted inputs thatare designed to mislead, or trick, models into making incorrect predictions. Anadversary can physically change their appearance by wearing adversarialt-shirts, glasses, or hats or by specific behavior, to potentially avoidvarious forms of detection, tracking and recognition of surveillance systems;and obtain unauthorized access to secure properties and assets. This poses asevere threat to the security and safety of modern surveillance systems. Thispaper reviews recent attempts and findings in learning and designing physicaladversarial attacks for surveillance applications. In particular, we propose aframework to analyze physical adversarial attacks and provide a comprehensivesurvey of physical adversarial attacks on four key surveillance tasks:detection, identification, tracking, and action recognition under thisframework. Furthermore, we review and analyze strategies to defend against thephysical adversarial attacks and the methods for evaluating the strengths ofthe defense. The insights in this paper present an important step in buildingresilience within surveillance systems to physical adversarial attacks.</description><author>Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan</author><pubDate>Mon, 01 May 2023 21:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01074v1</guid></item><item><title>VideoGLUE: Video General Understanding Evaluation of Foundation Models</title><link>http://arxiv.org/abs/2307.03166v1</link><description>We evaluate existing foundation models video understanding capabilities usinga carefully designed experiment protocol consisting of three hallmark tasks(action recognition, temporal localization, and spatiotemporal localization),eight datasets well received by the community, and four adaptation methodstailoring a foundation model (FM) for a downstream task. Moreover, we propose ascalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency whenadapting to general video understanding tasks. Our main findings are asfollows. First, task-specialized models significantly outperform the six FMsstudied in this work, in sharp contrast to what FMs have achieved in naturallanguage and image understanding. Second,video-native FMs, whose pretrainingdata contains the video modality, are generally better than image-native FMs inclassifying motion-rich videos, localizing actions in time, and understanding avideo of more than one action. Third, the video-native FMs can perform well onvideo tasks under light adaptations to downstream tasks(e.g., freezing the FMbackbones), while image-native FMs win in full end-to-end finetuning. The firsttwo observations reveal the need and tremendous opportunities to conductresearch on video-focused FMs, and the last confirms that both tasks andadaptation methods matter when it comes to the evaluation of FMs.</description><author>Liangzhe Yuan, Nitesh Bharadwaj Gundavarapu, Long Zhao, Hao Zhou, Yin Cui, Lu Jiang, Xuan Yang, Menglin Jia, Tobias Weyand, Luke Friedman, Mikhail Sirotenko, Huisheng Wang, Florian Schroff, Hartwig Adam, Ming-Hsuan Yang, Ting Liu, Boqing Gong</author><pubDate>Thu, 06 Jul 2023 18:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03166v1</guid></item><item><title>Dynamic Perceiver for Efficient Visual Recognition</title><link>http://arxiv.org/abs/2306.11248v1</link><description>Early exiting has become a promising approach to improving the inferenceefficiency of deep networks. By structuring models with multiple classifiers(exits), predictions for ``easy'' samples can be generated at earlier exits,negating the need for executing deeper layers. Current multi-exit networkstypically implement linear classifiers at intermediate layers, compellinglow-level features to encapsulate high-level semantics. This sub-optimal designinvariably undermines the performance of later exits. In this paper, we proposeDynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedureand the early classification task with a novel dual-branch architecture. Afeature branch serves to extract image features, while a classification branchprocesses a latent code assigned for classification tasks. Bi-directionalcross-attention layers are established to progressively fuse the information ofboth branches. Early exits are placed exclusively within the classificationbranch, thus eliminating the need for linear separability in low-levelfeatures. Dyn-Perceiver constitutes a versatile and adaptable framework thatcan be built upon various architectures. Experiments on image classification,action recognition, and object detection demonstrate that our methodsignificantly improves the inference efficiency of different backbones,outperforming numerous competitive approaches across a broad range ofcomputational budgets. Evaluation on both CPU and GPU platforms substantiatethe superior practical efficiency of Dyn-Perceiver. Code is available athttps://www.github.com/LeapLabTHU/Dynamic_Perceiver.</description><author>Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, Gao Huang</author><pubDate>Tue, 20 Jun 2023 04:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11248v1</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v1</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Fri, 26 May 2023 01:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v1</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v2</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Mon, 03 Jul 2023 08:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v2</guid></item><item><title>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</title><link>http://arxiv.org/abs/2307.08476v1</link><description>Skeleton sequence representation learning has shown great advantages foraction recognition due to its promising ability to model human joints andtopology. However, the current methods usually require sufficient labeled datafor training computationally expensive models, which is labor-intensive andtime-consuming. Moreover, these methods ignore how to utilize the fine-graineddependencies among different skeleton joints to pre-train an efficient skeletonsequence learning model that can generalize well across different datasets. Inthis paper, we propose an efficient skeleton sequence learning framework, namedSkeleton Sequence Learning (SSL). To comprehensively capture the human pose andobtain discriminative skeleton sequence representation, we build an asymmetricgraph-based encoder-decoder pre-training architecture named SkeletonMAE, whichembeds skeleton joint sequence into Graph Convolutional Network (GCN) andreconstructs the masked skeleton joints and edges based on the prior humantopology knowledge. Then, the pre-trained SkeletonMAE encoder is integratedwith the Spatial-Temporal Representation Learning (STRL) module to build theSSL framework. Extensive experimental results show that our SSL generalizeswell across different datasets and outperforms the state-of-the-artself-supervised skeleton-based action recognition methods on FineGym, Diving48,NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance tosome fully supervised methods. The code is avaliable athttps://github.com/HongYan1123/SkeletonMAE.</description><author>Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin</author><pubDate>Mon, 17 Jul 2023 14:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08476v1</guid></item><item><title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation</title><link>http://arxiv.org/abs/2307.08779v1</link><description>Low-light conditions not only hamper human visual experience but also degradethe model's performance on downstream vision tasks. While existing works makeremarkable progress on day-night domain adaptation, they rely heavily on domainknowledge derived from the task-specific nighttime dataset. This paperchallenges a more complicated scenario with border applicability, i.e.,zero-shot day-night domain adaptation, which eliminates reliance on anynighttime data. Unlike prior zero-shot adaptation approaches emphasizing eitherimage-level translation or model-level adaptation, we propose a similaritymin-max paradigm that considers them under a unified framework. On the imagelevel, we darken images towards minimum feature similarity to enlarge thedomain gap. Then on the model level, we maximize the feature similarity betweenthe darkened images and their normal-light counterparts for better modeladaptation. To the best of our knowledge, this work represents the pioneeringeffort in jointly optimizing both aspects, resulting in a significantimprovement of model generalizability. Extensive experiments demonstrate ourmethod's effectiveness and broad applicability on various nighttime visiontasks, including classification, semantic segmentation, visual placerecognition, and video action recognition. Code and pre-trained models areavailable at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.</description><author>Rundong Luo, Wenjing Wang, Wenhan Yang, Jiaying Liu</author><pubDate>Mon, 17 Jul 2023 19:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08779v1</guid></item><item><title>fpgaHART: A toolflow for throughput-oriented acceleration of 3D CNNs for HAR onto FPGAs</title><link>http://arxiv.org/abs/2305.19896v1</link><description>Surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval are just few of the many applications in which 3D ConvolutionalNeural Networks are exploited. However, their extensive use is restricted bytheir high computational and memory requirements, especially when integratedinto systems with limited resources. This study proposes a toolflow thatoptimises the mapping of 3D CNN models for Human Action Recognition onto FPGAdevices, taking into account FPGA resources and off-chip memorycharacteristics. The proposed system employs Synchronous Dataflow (SDF) graphsto model the designs and introduces transformations to expand and explore thedesign space, resulting in high-throughput designs. A variety of 3D CNN modelswere evaluated using the proposed toolflow on multiple FPGA devices,demonstrating its potential to deliver competitive performance compared toearlier hand-tuned and model-specific designs.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Wed, 31 May 2023 15:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19896v1</guid></item><item><title>HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices</title><link>http://arxiv.org/abs/2303.17218v6</link><description>For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networkshave proven to be highly effective, achieving state-of-the-art results. Thisstudy introduces a novel streaming architecture based toolflow for mapping suchmodels onto FPGAs considering the model's inherent characteristics and thefeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a3D CNN in ONNX format and a description of the FPGA characteristics, generatinga design that minimizes the latency of the computation. The toolflow iscomprised of a number of parts, including i) a 3D CNN parser, ii) a performanceand resource model, iii) a scheduling algorithm for executing 3D models on thegenerated hardware, iv) a resource-aware optimization engine tailored for 3Dmodels, v) an automated mapping to synthesizable code for FPGAs. The ability ofthe toolflow to support a broad range of models and devices is shown through anumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, thetoolflow has produced high-performing results for 3D CNN models that have notbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systemsin this space. Overall, HARFLOW3D has demonstrated its ability to delivercompetitive latency compared to a range of state-of-the-art hand-tunedapproaches being able to achieve up to 5$\times$ better performance compared tosome of the existing works.</description><author>Petros Toupas, Alexander Montgomerie-Corcoran, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17218v6</guid></item><item><title>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</title><link>http://arxiv.org/abs/2307.10922v1</link><description>Recent contrastive language image pre-training has led to learning highlytransferable and robust image representations. However, adapting these modelsto video domains with minimal supervision remains an open problem. We explore asimple step in that direction, using language tied self-supervised learning toadapt an image CLIP model to the video domain. A backbone modified for temporalmodeling is trained under self-distillation settings with train objectivesoperating in an action concept space. Feature vectors of various actionconcepts extracted from a language encoder using relevant textual promptsconstruct this space. We introduce two train objectives, concept distillationand concept alignment, that retain generality of original representations whileenforcing relations between actions and their attributes. Our approach improveszero-shot and linear probing performance on three action recognitionbenchmarks.</description><author>Kanchana Ranasinghe, Michael Ryoo</author><pubDate>Thu, 20 Jul 2023 15:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10922v1</guid></item><item><title>Deep recurrent spiking neural networks capture both static and dynamic representations of the visual cortex under movie stimuli</title><link>http://arxiv.org/abs/2306.01354v1</link><description>In the real world, visual stimuli received by the biological visual systemare predominantly dynamic rather than static. A better understanding of how thevisual cortex represents movie stimuli could provide deeper insight into theinformation processing mechanisms of the visual system. Although some progresshas been made in modeling neural responses to natural movies with deep neuralnetworks, the visual representations of static and dynamic information undersuch time-series visual stimuli remain to be further explored. In this work,considering abundant recurrent connections in the mouse visual system, wedesign a recurrent module based on the hierarchy of the mouse cortex and add itinto Deep Spiking Neural Networks, which have been demonstrated to be a morecompelling computational model for the visual cortex. Using Time-SeriesRepresentational Similarity Analysis, we measure the representationalsimilarity between networks and mouse cortical regions under natural moviestimuli. Subsequently, we conduct a comparison of the representationalsimilarity across recurrent/feedforward networks and image/video trainingtasks. Trained on the video action recognition task, recurrent SNN achieves thehighest representational similarity and significantly outperforms feedforwardSNN trained on the same task by 15% and the recurrent SNN trained on the imageclassification task by 8%. We investigate how static and dynamicrepresentations of SNNs influence the similarity, as a way to explain theimportance of these two forms of representations in biological neural coding.Taken together, our work is the first to apply deep recurrent SNNs to model themouse visual cortex under movie stimuli and we establish that these networksare competent to capture both static and dynamic representations and makecontributions to understanding the movie information processing mechanisms ofthe visual cortex.</description><author>Liwei Huang, ZhengYu Ma, Huihui Zhou, Yonghong Tian</author><pubDate>Fri, 02 Jun 2023 09:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01354v1</guid></item><item><title>Miniaturized Graph Convolutional Networks with Topologically Consistent Pruning</title><link>http://arxiv.org/abs/2306.17590v1</link><description>Magnitude pruning is one of the mainstream methods in lightweightarchitecture design whose goal is to extract subnetworks with the largestweight connections. This method is known to be successful, but under very highpruning regimes, it suffers from topological inconsistency which renders theextracted subnetworks disconnected, and this hinders their generalizationability. In this paper, we devise a novel magnitude pruning method that allowsextracting subnetworks while guarantying their topological consistency. Thelatter ensures that only accessible and co-accessible -- impactful --connections are kept in the resulting lightweight networks. Our solution isbased on a novel reparametrization and two supervisory bi-directional networkswhich implement accessibility/co-accessibility and guarantee that onlyconnected subnetworks will be selected during training. This solution allowsenhancing generalization significantly, under very high pruning regimes, ascorroborated through extensive experiments, involving graph convolutionalnetworks, on the challenging task of skeleton-based action recognition.</description><author>Hichem Sahbi</author><pubDate>Fri, 30 Jun 2023 13:09:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17590v1</guid></item><item><title>Make A Long Image Short: Adaptive Token Length for Vision Transformers</title><link>http://arxiv.org/abs/2307.02092v1</link><description>The vision transformer is a model that breaks down each image into a sequenceof tokens with a fixed length and processes them similarly to words in naturallanguage processing. Although increasing the number of tokens typically resultsin better performance, it also leads to a considerable increase incomputational cost. Motivated by the saying "A picture is worth a thousandwords," we propose an innovative approach to accelerate the ViT model byshortening long images. Specifically, we introduce a method for adaptivelyassigning token length for each image at test time to accelerate inferencespeed. First, we train a Resizable-ViT (ReViT) model capable of processinginput with diverse token lengths. Next, we extract token-length labels fromReViT that indicate the minimum number of tokens required to achieve accuratepredictions. We then use these labels to train a lightweight Token-LengthAssigner (TLA) that allocates the optimal token length for each image duringinference. The TLA enables ReViT to process images with the minimum sufficientnumber of tokens, reducing token numbers in the ViT model and improvinginference speed. Our approach is general and compatible with modern visiontransformer architectures, significantly reducing computational costs. Weverified the effectiveness of our methods on multiple representative ViT modelson image classification and action recognition.</description><author>Qiqi Zhou, Yichen Zhu</author><pubDate>Wed, 05 Jul 2023 09:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02092v1</guid></item><item><title>Contrastive Predictive Autoencoders for Dynamic Point Cloud Self-Supervised Learning</title><link>http://arxiv.org/abs/2305.12959v1</link><description>We present a new self-supervised paradigm on point cloud sequenceunderstanding. Inspired by the discriminative and generative self-supervisedmethods, we design two tasks, namely point cloud sequence based ContrastivePrediction and Reconstruction (CPR), to collaboratively learn morecomprehensive spatiotemporal representations. Specifically, dense point cloudsegments are first input into an encoder to extract embeddings. All but thelast ones are then aggregated by a context-aware autoregressor to makepredictions for the last target segment. Towards the goal of modelingmulti-granularity structures, local and global contrastive learning areperformed between predictions and targets. To further improve thegeneralization of representations, the predictions are also utilized toreconstruct raw point cloud sequences by a decoder, where point cloudcolorization is employed to discriminate against different frames. By combiningclassic contrast and reconstruction paradigms, it makes the learnedrepresentations with both global discrimination and local perception. Weconduct experiments on four point cloud sequence benchmarks, and report theresults on action recognition and gesture recognition under multipleexperimental settings. The performances are comparable with supervised methodsand show powerful transferability.</description><author>Xiaoxiao Sheng, Zhiqiang Shen, Gang Xiao</author><pubDate>Mon, 22 May 2023 13:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12959v1</guid></item><item><title>Full-Body Articulated Human-Object Interaction</title><link>http://arxiv.org/abs/2212.10621v2</link><description>Fine-grained capturing of 3D HOI boosts human activity understanding andfacilitates downstream visual tasks, including action recognition, holisticscene reconstruction, and human motion synthesis. Despite its significance,existing works mostly assume that humans interact with rigid objects using onlya few body parts, limiting their scope. In this paper, we address thechallenging problem of f-AHOI, wherein the whole human bodies interact witharticulated objects, whose parts are connected by movable joints. We presentCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hoursof versatile interactions between 46 participants and 81 articulated and rigidsittable objects. CHAIRS provides 3D meshes of both humans and articulatedobjects during the entire interactive process, as well as realistic andphysically plausible full-body interactions. We show the value of CHAIRS withobject pose estimation. By learning the geometrical relationships in HOI, wedevise the very first model that leverage human pose estimation to tackle theestimation of articulated object poses and shapes during whole-bodyinteractions. Given an image and an estimated human pose, our model firstreconstructs the pose and shape of the object, then optimizes thereconstruction according to a learned interaction prior. Under both evaluationsettings (e.g., with or without the knowledge of objects'geometries/structures), our model significantly outperforms baselines. We hopeCHAIRS will promote the community towards finer-grained interactionunderstanding. We will make the data/code publicly available.</description><author>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang</author><pubDate>Tue, 16 May 2023 20:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10621v2</guid></item><item><title>The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose</title><link>http://arxiv.org/abs/2007.00394v2</link><description>The availability of a large labeled dataset is a key requirement for applyingdeep learning methods to solve various computer vision tasks. In the context ofunderstanding human activities, existing public datasets, while large in size,are often limited to a single RGB camera and provide only per-frame or per-clipaction annotations. To enable richer analysis and understanding of humanactivities, we introduce IKEA ASM -- a three million frame, multi-view,furniture assembly video dataset that includes depth, atomic actions, objectsegmentation, and human pose. Additionally, we benchmark prominent methods forvideo action recognition, object segmentation and human pose estimation taskson this challenging dataset. The dataset enables the development of holisticmethods, which integrate multi-modal and multi-view data to better perform onthese tasks.</description><author>Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould</author><pubDate>Wed, 17 May 2023 08:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.00394v2</guid></item><item><title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation</title><link>http://arxiv.org/abs/2307.08779v2</link><description>Low-light conditions not only hamper human visual experience but also degradethe model's performance on downstream vision tasks. While existing works makeremarkable progress on day-night domain adaptation, they rely heavily on domainknowledge derived from the task-specific nighttime dataset. This paperchallenges a more complicated scenario with border applicability, i.e.,zero-shot day-night domain adaptation, which eliminates reliance on anynighttime data. Unlike prior zero-shot adaptation approaches emphasizing eitherimage-level translation or model-level adaptation, we propose a similaritymin-max paradigm that considers them under a unified framework. On the imagelevel, we darken images towards minimum feature similarity to enlarge thedomain gap. Then on the model level, we maximize the feature similarity betweenthe darkened images and their normal-light counterparts for better modeladaptation. To the best of our knowledge, this work represents the pioneeringeffort in jointly optimizing both aspects, resulting in a significantimprovement of model generalizability. Extensive experiments demonstrate ourmethod's effectiveness and broad applicability on various nighttime visiontasks, including classification, semantic segmentation, visual placerecognition, and video action recognition. Code and pre-trained models areavailable at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.</description><author>Rundong Luo, Wenjing Wang, Wenhan Yang, Jiaying Liu</author><pubDate>Wed, 19 Jul 2023 06:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08779v2</guid></item><item><title>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</title><link>http://arxiv.org/abs/2304.07775v2</link><description>Cross-modal distillation has been widely used to transfer knowledge acrossdifferent modalities, enriching the representation of the target unimodal one.Recent studies highly relate the temporal synchronization between vision andsound to the semantic consistency for cross-modal distillation. However, suchsemantic consistency from the synchronization is hard to guarantee inunconstrained videos, due to the irrelevant modality noise and differentiatedsemantic correlation. To this end, we first propose a \textit{Modality NoiseFilter} (MNF) module to erase the irrelevant noise in teacher modality withcross-modal context. After this purification, we then design a\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distilluseful knowledge for target modality, by referring to the differentiatedsample-wise semantic correlation in a contrastive fashion. Extensiveexperiments show that our method could bring a performance boost compared withother distillation methods in both visual action recognition and videoretrieval task. We also extend to the audio tagging task to prove thegeneralization of our method. The source code is available at\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.</description><author>Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu</author><pubDate>Thu, 27 Apr 2023 05:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07775v2</guid></item><item><title>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</title><link>http://arxiv.org/abs/2307.09936v1</link><description>This paper focuses on motion prediction for point cloud sequences in thechallenging case of deformable 3D objects, such as human body motion. First, weinvestigate the challenges caused by deformable shapes and complex motionspresent in this type of representation, with the ultimate goal of understandingthe technical limitations of state-of-the-art models. From this understanding,we propose an improved architecture for point cloud prediction of deformable 3Dobjects. Specifically, to handle deformable shapes, we propose a graph-basedapproach that learns and exploits the spatial structure of point clouds toextract more representative features. Then we propose a module able to combinethe learned features in an adaptative manner according to the point cloudmovements. The proposed adaptative module controls the composition of local andglobal motions for each point, enabling the network to model complex motions indeformable 3D objects more effectively. We tested the proposed method on thefollowing datasets: MNIST moving digits, the Mixamo human bodies motions, JPEGand CWIPC-SXR real-world dynamic bodies. Simulation results demonstrate thatour method outperforms the current baseline methods given its improved abilityto model complex movements as well as preserve point cloud shape. Furthermore,we demonstrate the generalizability of the proposed framework for dynamicfeature learning, by testing the framework for action recognition on theMSRAction3D dataset and achieving results on-par with state-of-the-art methods</description><author>Pedro Gomes, Silvia Rossi, Laura Toni</author><pubDate>Wed, 19 Jul 2023 13:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09936v1</guid></item><item><title>Lightweight Delivery Detection on Doorbell Cameras</title><link>http://arxiv.org/abs/2305.07812v1</link><description>Despite recent advances in video-based action recognition and robustspatio-temporal modeling, most of the proposed approaches rely on the abundanceof computational resources to afford running huge and computation-intensiveconvolutional or transformer-based neural networks to obtain satisfactoryresults. This limits the deployment of such models on edge devices with limitedpower and computing resources. In this work we investigate an important smarthome application, video based delivery detection, and present a simple andlightweight pipeline for this task that can run on resource-constraineddoorbell cameras. Our proposed pipeline relies on motion cues to generate a setof coarse activity proposals followed by their classification with amobile-friendly 3DCNN network. For training we design a novel semi-supervisedattention module that helps the network to learn robust spatio-temporalfeatures and adopt an evidence-based optimization objective that allows forquantifying the uncertainty of predictions made by the network. Experimentalresults on our curated delivery dataset shows the significant effectiveness ofour pipeline compared to alternatives and highlights the benefits of ourtraining phase novelties to achieve free and considerable inference-timeperformance gains.</description><author>Pirazh Khorramshahi, Zhe Wu, Tianchen Wang, Luke Deluccia, Hongcheng Wang</author><pubDate>Sat, 13 May 2023 02:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07812v1</guid></item><item><title>Learning Video-Conditioned Policies for Unseen Manipulation Tasks</title><link>http://arxiv.org/abs/2305.06289v1</link><description>The ability to specify robot commands by a non-expert user is critical forbuilding generalist agents capable of solving a large variety of tasks. Oneconvenient way to specify the intended robot goal is by a video of a persondemonstrating the target task. While prior work typically aims to imitate humandemonstrations performed in robot environments, here we focus on a morerealistic and challenging setup with demonstrations recorded in natural anddiverse human environments. We propose Video-conditioned Policy learning (ViP),a data-driven approach that maps human demonstrations of previously unseentasks to robot manipulation skills. To this end, we learn our policy togenerate appropriate actions given current scene observations and a video ofthe target task. To encourage generalization to new tasks, we avoid particulartasks during training and learn our policy from unlabelled robot trajectoriesand corresponding robot videos. Both robot and human videos in our frameworkare represented by video embeddings pre-trained for human action recognition.At test time we first translate human videos to robot videos in the commonvideo embedding space, and then use resulting embeddings to condition ourpolicies. Notably, our approach enables robot control by human demonstrationsin a zero-shot manner, i.e., without using robot trajectories paired with humaninstructions during training. We validate our approach on a set of challengingmulti-task robot manipulation environments and outperform state of the art. Ourmethod also demonstrates excellent performance in a new challenging zero-shotsetup where no paired data is used during training.</description><author>Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev</author><pubDate>Wed, 10 May 2023 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06289v1</guid></item><item><title>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection</title><link>http://arxiv.org/abs/2307.09465v1</link><description>Given that approximately half of science, technology, engineering, andmathematics (STEM) undergraduate students in U.S. colleges and universitiesleave by the end of the first year [15], it is crucial to improve the qualityof classroom environments. This study focuses on monitoring students' emotionsin the classroom as an indicator of their engagement and proposes an approachto address this issue. The impact of different facial parts on the performanceof an emotional recognition model is evaluated through experimentation. To testthe proposed model under partial occlusion, an artificially occluded dataset isintroduced. The novelty of this work lies in the proposal of an occlusion-awarearchitecture for facial action units (AUs) extraction, which employs attentionmechanism and adaptive feature learning. The AUs can be used later to classifyfacial expressions in classroom settings. This research paper's findings provide valuable insights into handlingocclusion in analyzing facial images for emotional engagement analysis. Theproposed experiments demonstrate the significance of considering occlusion andenhancing the reliability of facial analysis models in classroom environments.These findings can also be extended to other settings where occlusions areprevalent.</description><author>Shrouk Wally, Ahmed Elsayed, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 18:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09465v1</guid></item><item><title>Meerkat Behaviour Recognition Dataset</title><link>http://arxiv.org/abs/2306.11326v1</link><description>Recording animal behaviour is an important step in evaluating the well-beingof animals and further understanding the natural world. Current methods fordocumenting animal behaviour within a zoo setting, such as scan sampling,require excessive human effort, are unfit for around-the-clock monitoring, andmay produce human-biased results. Several animal datasets already exist thatfocus predominantly on wildlife interactions, with some extending to action orbehaviour recognition. However, there is limited data in a zoo setting or datafocusing on the group behaviours of social animals. We introduce a largemeerkat (Suricata Suricatta) behaviour recognition video dataset with diverseannotated behaviours, including group social interactions, tracking ofindividuals within the camera view, skewed class distribution, and varyingillumination conditions. This dataset includes videos from two positions withinthe meerkat enclosure at the Wellington Zoo (Wellington, New Zealand), with848,400 annotated frames across 20 videos and 15 unannotated videos.</description><author>Mitchell Rogers, Gaël Gendron, David Arturo Soriano Valdez, Mihailo Azhar, Yang Chen, Shahrokh Heidari, Caleb Perelini, Padriac O'Leary, Kobe Knowles, Izak Tait, Simon Eyre, Michael Witbrock, Patrice Delmas</author><pubDate>Tue, 20 Jun 2023 07:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11326v1</guid></item><item><title>Rendezvous in Time: An Attention-based Temporal Fusion approach for Surgical Triplet Recognition</title><link>http://arxiv.org/abs/2211.16963v2</link><description>One of the recent advances in surgical AI is the recognition of surgicalactivities as triplets of (instrument, verb, target). Albeit providing detailedinformation for computer-assisted intervention, current triplet recognitionapproaches rely only on single frame features. Exploiting the temporal cuesfrom earlier frames would improve the recognition of surgical action tripletsfrom videos. In this paper, we propose Rendezvous in Time (RiT) - a deeplearning model that extends the state-of-the-art model, Rendezvous, withtemporal modeling. Focusing more on the verbs, our RiT explores theconnectedness of current and past frames to learn temporal attention-basedfeatures for enhanced triplet recognition. We validate our proposal on thechallenging surgical triplet dataset, CholecT45, demonstrating an improvedrecognition of the verb and triplet along with other interactions involving theverb such as (instrument, verb). Qualitative results show that the RiT producessmoother predictions for most triplet instances than the state-of-the-arts. Wepresent a novel attention-based approach that leverages the temporal fusion ofvideo frames to model the evolution of surgical actions and exploit theirbenefits for surgical triplet recognition.</description><author>Saurav Sharma, Chinedu Innocent Nwoye, Didier Mutter, Nicolas Padoy</author><pubDate>Fri, 16 Jun 2023 10:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16963v2</guid></item><item><title>Egocentric Audio-Visual Noise Suppression</title><link>http://arxiv.org/abs/2211.03643v2</link><description>This paper studies audio-visual noise suppression for egocentric videos --where the speaker is not captured in the video. Instead, potential noisesources are visible on screen with the camera emulating the off-screenspeaker's view of the outside world. This setting is different from prior workin audio-visual speech enhancement that relies on lip and facial visuals. Inthis paper, we first demonstrate that egocentric visual information is helpfulfor noise suppression. We compare object recognition and actionclassification-based visual feature extractors and investigate methods to alignaudio and visual representations. Then, we examine different fusion strategiesfor the aligned features, and locations within the noise suppression model toincorporate visual information. Experiments demonstrate that visual featuresare most helpful when used to generate additive correction masks. Finally, inorder to ensure that the visual features are discriminative with respect todifferent noise types, we introduce a multi-task learning framework thatjointly optimizes audio-visual noise suppression and video-based acoustic eventdetection. This proposed multi-task framework outperforms the audio-onlybaseline on all metrics, including a 0.16 PESQ improvement. Extensive ablationsreveal the improved performance of the proposed model with multiple activedistractors, overall noise types, and across different SNRs.</description><author>Roshan Sharma, Weipeng He, Ju Lin, Egor Lakomkin, Yang Liu, Kaustubh Kalgaonkar</author><pubDate>Wed, 03 May 2023 03:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03643v2</guid></item></channel></rss>