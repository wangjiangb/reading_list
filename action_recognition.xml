<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 14 Feb 2024 06:00:19 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RALACs: Action Recognition in Autonomous Vehicles using Interaction Encoding and Optical Flow</title><link>http://arxiv.org/abs/2209.14408v3</link><description>When applied to autonomous vehicle (AV) settings, action recognition canenhance an environment model's situational awareness. This is especiallyprevalent in scenarios where traditional geometric descriptions and heuristicsin AVs are insufficient. However, action recognition has traditionally beenstudied for humans, and its limited adaptability to noisy, un-clipped,un-pampered, raw RGB data has limited its application in other fields. To pushfor the advancement and adoption of action recognition into AVs, this workproposes a novel two-stage action recognition system, termed RALACs. RALACsformulates the problem of action recognition for road scenes, and bridges thegap between it and the established field of human action recognition. This workshows how attention layers can be useful for encoding the relations acrossagents, and stresses how such a scheme can be class-agnostic. Furthermore, toaddress the dynamic nature of agents on the road, RALACs constructs a novelapproach to adapting Region of Interest (ROI) Alignment to agent tracks fordownstream action classification. Finally, our scheme also considers theproblem of active agent detection, and utilizes a novel application of fusingoptical flow maps to discern relevant agents in a road scene. We show that ourproposed scheme can outperform the baseline on the ICCV2021 Road Challengedataset and by deploying it on a real vehicle platform, we provide preliminaryinsight to the usefulness of action recognition in decision making.</description><author>Eddy Zhou, Alex Zhuang, Alikasim Budhwani, Owen Leather, Rowan Dempster, Quanquan Li, Mohammad Al-Sharman, Derek Rayside, William Melek</author><pubDate>Sun, 14 Jan 2024 16:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14408v3</guid></item><item><title>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2310.10547v1</link><description>Skeleton-based action recognition has made significant advancements recently,with models like InfoGCN showcasing remarkable accuracy. However, these modelsexhibit a key limitation: they necessitate complete action observation prior toclassification, which constrains their applicability in real-time situationssuch as surveillance and robotic systems. To overcome this barrier, weintroduce InfoGCN++, an innovative extension of InfoGCN, explicitly developedfor online skeleton-based action recognition. InfoGCN++ augments the abilitiesof the original InfoGCN model by allowing real-time categorization of actiontypes, independent of the observation sequence's length. It transcendsconventional approaches by learning from current and anticipated futuremovements, thereby creating a more thorough representation of the entiresequence. Our approach to prediction is managed as an extrapolation issue,grounded on observed actions. To enable this, InfoGCN++ incorporates NeuralOrdinary Differential Equations, a concept that lets it effectively model thecontinuous evolution of hidden states. Following rigorous evaluations on threeskeleton-based action recognition benchmarks, InfoGCN++ demonstratesexceptional performance in online action recognition. It consistently equals orexceeds existing techniques, highlighting its significant potential to reshapethe landscape of real-time action recognition applications. Consequently, thiswork represents a major leap forward from InfoGCN, pushing the limits of what'spossible in online, skeleton-based action recognition. The code for InfoGCN++is publicly available at https://github.com/stnoah1/infogcn2 for furtherexploration and validation.</description><author>Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani</author><pubDate>Mon, 16 Oct 2023 17:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10547v1</guid></item><item><title>Action Recognition in Video Recordings from Gynecologic Laparoscopy</title><link>http://arxiv.org/abs/2311.18666v1</link><description>Action recognition is a prerequisite for many applications in laparoscopicvideo analysis including but not limited to surgical training, operation roomplanning, follow-up surgery preparation, post-operative surgical assessment,and surgical outcome estimation. However, automatic action recognition inlaparoscopic surgeries involves numerous challenges such as (I) cross-actionand intra-action duration variation, (II) relevant content distortion due tosmoke, blood accumulation, fast camera motions, organ movements, objectocclusion, and (III) surgical scene variations due to different illuminationsand viewpoints. Besides, action annotations in laparoscopy surgeries arelimited and expensive due to requiring expert knowledge. In this study, wedesign and evaluate a CNN-RNN architecture as well as a customizedtraining-inference framework to deal with the mentioned challenges inlaparoscopic surgery action recognition. Using stacked recurrent layers, ourproposed network takes advantage of inter-frame dependencies to negate thenegative effect of content distortion and variation in action recognition.Furthermore, our proposed frame sampling strategy effectively manages theduration variations in surgical actions to enable action recognition with hightemporal resolution. Our extensive experiments confirm the superiority of ourproposed method in action recognition compared to static CNNs.</description><author>Sahar Nasirihaghighi, Negin Ghamsarian, Daniela Stefanics, Klaus Schoeffmann, Heinrich Husslein</author><pubDate>Thu, 30 Nov 2023 16:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18666v1</guid></item><item><title>Balanced Representation Learning for Long-tailed Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.14024v1</link><description>Skeleton-based action recognition has recently made significant progress.However, data imbalance is still a great challenge in real-world scenarios. Theperformance of current action recognition algorithms declines sharply whentraining data suffers from heavy class imbalance. The imbalanced data actuallydegrades the representations learned by these methods and becomes thebottleneck for action recognition. How to learn unbiased representations fromimbalanced action data is the key to long-tailed action recognition. In thispaper, we propose a novel balanced representation learning method to addressthe long-tailed problem in action recognition. Firstly, a spatial-temporalaction exploration strategy is presented to expand the sample spaceeffectively, generating more valuable samples in a rebalanced manner. Secondly,we design a detached action-aware learning schedule to further mitigate thebias in the representation space. The schedule detaches the representationlearning of tail classes from training and proposes an action-aware loss toimpose more effective constraints. Additionally, a skip-modal representation isproposed to provide complementary structural information. The proposed methodis validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA,and Kinetics. It not only achieves consistently large improvement compared tothe state-of-the-art (SOTA) methods, but also demonstrates a superiorgeneralization capacity through extensive experiments. Our code is available athttps://github.com/firework8/BRL.</description><author>Hongda Liu, Yunlong Wang, Min Ren, Junxing Hu, Zhengquan Luo, Guangqi Hou, Zhenan Sun</author><pubDate>Sun, 27 Aug 2023 08:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14024v1</guid></item><item><title>SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge</title><link>http://arxiv.org/abs/2401.00496v1</link><description>Surgical tool segmentation and action recognition are fundamental buildingblocks in many computer-assisted intervention applications, ranging fromsurgical skills assessment to decision support systems. Nowadays,learning-based action recognition and segmentation approaches outperformclassical methods, relying, however, on large, annotated datasets. Furthermore,action recognition and tool segmentation algorithms are often trained and makepredictions in isolation from each other, without exploiting potentialcross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, werelease the first multimodal, publicly available, in-vivo, dataset for surgicalaction recognition and semantic instrumentation segmentation, containing 50suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). Theaim of the challenge is twofold. First, to enable researchers to leverage thescale of the provided dataset and develop robust and highly accuratesingle-task action recognition and tool segmentation approaches in the surgicaldomain. Second, to further explore the potential of multitask-based learningapproaches and determine their comparative advantage against their single-taskcounterparts. A total of 12 teams participated in the challenge, contributing 7action recognition methods, 9 instrument segmentation techniques, and 4multitask approaches that integrated both action recognition and instrumentsegmentation.</description><author>Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam, Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai Wang, Yang Liu, Maxence Boels, Jiayu Huo, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin, Mengya Xu, An Wang, Yanan Wu, Long Bai, Hongliang Ren, Atsushi Yamada, Yuriko Harai, Yuto Ishikawa, Kazuyuki Hayashi, Jente Simoens, Pieter DeBacker, Francesco Cisternino, Gabriele Furnari, Alex Mottrie, Federica Ferraguti, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Soohee Kim, Seung Hyun Lee, Kyu Eun Lee, Hyoun-Joong Kong, Kui Fu, Chao Li, Shan An, Stefanie Krell, Sebastian Bodenstedt, Nicolas Ayobi, Alejandra Perez, Santiago Rodriguez, Juanita Puentes, Pablo Arbelaez, Omid Mohareri, Danail Stoyanov</author><pubDate>Sun, 31 Dec 2023 13:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00496v1</guid></item><item><title>SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge</title><link>http://arxiv.org/abs/2401.00496v2</link><description>Surgical tool segmentation and action recognition are fundamental buildingblocks in many computer-assisted intervention applications, ranging fromsurgical skills assessment to decision support systems. Nowadays,learning-based action recognition and segmentation approaches outperformclassical methods, relying, however, on large, annotated datasets. Furthermore,action recognition and tool segmentation algorithms are often trained and makepredictions in isolation from each other, without exploiting potentialcross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, werelease the first multimodal, publicly available, in-vivo, dataset for surgicalaction recognition and semantic instrumentation segmentation, containing 50suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). Theaim of the challenge is twofold. First, to enable researchers to leverage thescale of the provided dataset and develop robust and highly accuratesingle-task action recognition and tool segmentation approaches in the surgicaldomain. Second, to further explore the potential of multitask-based learningapproaches and determine their comparative advantage against their single-taskcounterparts. A total of 12 teams participated in the challenge, contributing 7action recognition methods, 9 instrument segmentation techniques, and 4multitask approaches that integrated both action recognition and instrumentsegmentation. The complete SAR-RARP50 dataset is available at:https://rdr.ucl.ac.uk/projects/SARRARP50_Segmentation_of_surgical_instrumentation_and_Action_Recognition_on_Robot-Assisted_Radical_Prostatectomy_Challenge/191091</description><author>Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam, Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai Wang, Yang Liu, Maxence Boels, Jiayu Huo, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin, Mengya Xu, An Wang, Yanan Wu, Long Bai, Hongliang Ren, Atsushi Yamada, Yuriko Harai, Yuto Ishikawa, Kazuyuki Hayashi, Jente Simoens, Pieter DeBacker, Francesco Cisternino, Gabriele Furnari, Alex Mottrie, Federica Ferraguti, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Soohee Kim, Seung Hyun Lee, Kyu Eun Lee, Hyoun-Joong Kong, Kui Fu, Chao Li, Shan An, Stefanie Krell, Sebastian Bodenstedt, Nicolas Ayobi, Alejandra Perez, Santiago Rodriguez, Juanita Puentes, Pablo Arbelaez, Omid Mohareri, Danail Stoyanov</author><pubDate>Tue, 23 Jan 2024 23:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00496v2</guid></item><item><title>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition</title><link>http://arxiv.org/abs/2402.03241v1</link><description>In this paper, we introduce FROSTER, an effective framework foropen-vocabulary action recognition. The CLIP model has achieved remarkablesuccess in a range of image-based tasks, benefiting from its stronggeneralization capability stemming from pretaining on massive image-text pairs.However, applying CLIP directly to the open-vocabulary action recognition taskis challenging due to the absence of temporal information in CLIP'spretraining. Further, fine-tuning CLIP on action recognition datasets may leadto overfitting and hinder its generalizability, resulting in unsatisfactoryresults when dealing with unseen actions. To address these issues, FROSTER employs a residual feature distillationapproach to ensure that CLIP retains its generalization capability whileeffectively adapting to the action recognition task. Specifically, the residualfeature distillation treats the frozen CLIP model as a teacher to maintain thegeneralizability exhibited by the original CLIP and supervises the featurelearning for the extraction of video-specific features to bridge the gapbetween images and videos. Meanwhile, it uses a residual sub-network forfeature distillation to reach a balance between the two distinct objectives oflearning generalizable and video-specific features. We extensively evaluate FROSTER on open-vocabulary action recognitionbenchmarks under both base-to-novel and cross-dataset settings. FROSTERconsistently achieves state-of-the-art performance on all datasets across theboard. Project page: https://visual-ai.github.io/froster.</description><author>Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han</author><pubDate>Mon, 05 Feb 2024 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03241v1</guid></item><item><title>Explore Human Parsing Modality for Action Recognition</title><link>http://arxiv.org/abs/2401.02138v1</link><description>Multimodal-based action recognition methods have achieved high success usingpose and RGB modality. However, skeletons sequences lack appearance depictionand RGB images suffer irrelevant noise due to modality limitations. To addressthis, we introduce human parsing feature map as a novel modality, since it canselectively retain effective semantic features of the body parts, whilefiltering out most irrelevant noise. We propose a new dual-branch frameworkcalled Ensemble Human Parsing and Pose Network (EPP-Net), which is the first toleverage both skeletons and human parsing modalities for action recognition.The first human pose branch feeds robust skeletons in graph convolutionalnetwork to model pose features, while the second human parsing branch alsoleverages depictive parsing feature maps to model parsing festures viaconvolutional backbones. The two high-level features will be effectivelycombined through a late fusion strategy for better action recognition.Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistentlyverify the effectiveness of our proposed EPP-Net, which outperforms theexisting action recognition methods. Our code is available at:https://github.com/liujf69/EPP-Net-Action.</description><author>Jinfu Liu, Runwei Ding, Yuhang Wen, Nan Dai, Fanyang Meng, Shen Zhao, Mengyuan Liu</author><pubDate>Thu, 04 Jan 2024 08:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02138v1</guid></item><item><title>Semantic-aware Video Representation for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2311.06218v1</link><description>Recent work on action recognition leverages 3D features and textualinformation to achieve state-of-the-art performance. However, most of thecurrent few-shot action recognition methods still rely on 2D frame-levelrepresentations, often require additional components to model temporalrelations, and employ complex distance functions to achieve accurate alignmentof these representations. In addition, existing methods struggle to effectivelyintegrate textual semantics, some resorting to concatenation or addition oftextual and visual features, and some using text merely as an additionalsupervision without truly achieving feature fusion and information transferfrom different modalities. In this work, we propose a simple yet effectiveSemantic-Aware Few-Shot Action Recognition (SAFSAR) model to address theseissues. We show that directly leveraging a 3D feature extractor combined withan effective feature-fusion scheme, and a simple cosine similarity forclassification can yield better performance without the need of extracomponents for temporal modeling or complex distance functions. We introduce aninnovative scheme to encode the textual semantics into the video representationwhich adaptively fuses features from text and video, and encourages the visualencoder to extract more semantically consistent features. In this scheme,SAFSAR achieves alignment and fusion in a compact way. Experiments on fivechallenging few-shot action recognition benchmarks under various settingsdemonstrate that the proposed SAFSAR model significantly improves thestate-of-the-art performance.</description><author>Yutao Tang, Benjamin Bejar, Rene Vidal</author><pubDate>Fri, 10 Nov 2023 18:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06218v1</guid></item><item><title>Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks</title><link>http://arxiv.org/abs/2112.07921v2</link><description>Recently, video-based action recognition methods using convolutional neuralnetworks (CNNs) achieve remarkable recognition performance. However, there isstill lack of understanding about the generalization mechanism of actionrecognition models. In this paper, we suggest that action recognition modelsrely on the motion information less than expected, and thus they are robust torandomization of frame orders. Furthermore, we find that motion monotonicityremaining after randomization also contributes to such robustness. Based onthis observation, we develop a novel defense method using temporal shuffling ofinput videos against adversarial attacks for action recognition models. Anotherobservation enabling our defense method is that adversarial perturbations onvideos are sensitive to temporal destruction. To the best of our knowledge,this is the first attempt to design a defense method without additionaltraining for 3D CNN-based video action recognition models.</description><author>Jaehui Hwang, Huan Zhang, Jun-Ho Choi, Cho-Jui Hsieh, Jong-Seok Lee</author><pubDate>Thu, 07 Dec 2023 09:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.07921v2</guid></item><item><title>Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art</title><link>http://arxiv.org/abs/2311.12300v1</link><description>Automated human action recognition, a burgeoning field within computervision, boasts diverse applications spanning surveillance, security,human-computer interaction, tele-health, and sports analysis. Precise actionrecognition in infants serves a multitude of pivotal purposes, encompassingsafety monitoring, developmental milestone tracking, early intervention fordevelopmental delays, fostering parent-infant bonds, advancing computer-aideddiagnostics, and contributing to the scientific comprehension of childdevelopment. This paper delves into the intricacies of infant actionrecognition, a domain that has remained relatively uncharted despite theaccomplishments in adult action recognition. In this study, we introduce agroundbreaking dataset called ``InfActPrimitive'', encompassing fivesignificant infant milestone action categories, and we incorporate specializedpreprocessing for infant data. We conducted an extensive comparative analysisemploying cutting-edge skeleton-based action recognition models using thisdataset. Our findings reveal that, although the PoseC3D model achieves thehighest accuracy at approximately 71%, the remaining models struggle toaccurately capture the dynamics of infant actions. This highlights asubstantial knowledge gap between infant and adult action recognition domainsand the urgent need for data-efficient pipeline models.</description><author>Elaheh Hatamimajoumerd, Pooria Daneshvar Kakhaki, Xiaofei Huang, Lingfei Luan, Somaieh Amraee, Sarah Ostadabbas</author><pubDate>Tue, 21 Nov 2023 02:36:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12300v1</guid></item><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v3</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 Jan 2024 05:32:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v3</guid></item><item><title>Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition</title><link>http://arxiv.org/abs/2308.10557v1</link><description>Hand action recognition is essential. Communication, human-robotinteractions, and gesture control are dependent on it. Skeleton-based actionrecognition traditionally includes hands, which belong to the classes whichremain challenging to correctly recognize to date. We propose a methodspecifically designed for hand action recognition which uses relative angularembeddings and local Spherical Harmonics to create novel hand representations.The use of Spherical Harmonics creates rotation-invariant representations whichmake hand action recognition even more robust against inter-subject differencesand viewpoint changes. We conduct extensive experiments on the hand joints inthe First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand PoseAnnotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit ofusing Local Spherical Harmonics Representations. Our code is available athttps://github.com/KathPra/LSHR_LSHT.</description><author>Katharina Prasse, Steffen Jung, Yuxuan Zhou, Margret Keuper</author><pubDate>Mon, 21 Aug 2023 09:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10557v1</guid></item><item><title>Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition</title><link>http://arxiv.org/abs/2308.10557v2</link><description>Hand action recognition is essential. Communication, human-robotinteractions, and gesture control are dependent on it. Skeleton-based actionrecognition traditionally includes hands, which belong to the classes whichremain challenging to correctly recognize to date. We propose a methodspecifically designed for hand action recognition which uses relative angularembeddings and local Spherical Harmonics to create novel hand representations.The use of Spherical Harmonics creates rotation-invariant representations whichmake hand action recognition even more robust against inter-subject differencesand viewpoint changes. We conduct extensive experiments on the hand joints inthe First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand PoseAnnotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit ofusing Local Spherical Harmonics Representations. Our code is available athttps://github.com/KathPra/LSHR_LSHT.</description><author>Katharina Prasse, Steffen Jung, Yuxuan Zhou, Margret Keuper</author><pubDate>Tue, 14 Nov 2023 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10557v2</guid></item><item><title>Analysis and Evaluation of Kinect-based Action Recognition Algorithms</title><link>http://arxiv.org/abs/2112.08626v2</link><description>Human action recognition still exists many challenging problems such asdifferent viewpoints, occlusion, lighting conditions, human body size and thespeed of action execution, although it has been widely used in different areas.To tackle these challenges, the Kinect depth sensor has been developed torecord real time depth sequences, which are insensitive to the color of humanclothes and illumination conditions. Many methods on recognizing human actionhave been reported in the literature such as HON4D, HOPC, RBD and HDG, whichuse the 4D surface normals, pointclouds, skeleton-based model and depthgradients respectively to capture discriminative information from depth videosor skeleton data. In this research project, the performance of fouraforementioned algorithms will be analyzed and evaluated using five benchmarkdatasets, which cover challenging issues such as noise, change of viewpoints,background clutters and occlusions. We also implemented and improved the HDGalgorithm, and applied it in cross-view action recognition using the UWA3DMultiview Activity dataset. Moreover, we used different combinations ofindividual feature vectors in HDG for performance evaluation. The experimentalresults show that our improvement of HDG outperforms other threestate-of-the-art algorithms for cross-view action recognition.</description><author>Lei Wang</author><pubDate>Wed, 27 Sep 2023 16:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.08626v2</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v2</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Tue, 06 Feb 2024 12:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v2</guid></item><item><title>hear-your-action: human action recognition by ultrasound active sensing</title><link>http://arxiv.org/abs/2309.08087v1</link><description>Action recognition is a key technology for many industrial applications.Methods using visual information such as images are very popular. However,privacy issues prevent widespread usage due to the inclusion of privateinformation, such as visible faces and scene backgrounds, which are notnecessary for recognizing user action. In this paper, we propose aprivacy-preserving action recognition by ultrasound active sensing. As actionrecognition from ultrasound active sensing in a non-invasive manner is not wellinvestigated, we create a new dataset for action recognition and conduct acomparison of features for classification. We calculated feature values byfocusing on the temporal variation of the amplitude of ultrasound reflectedwaves and performed classification using a support vector machine and VGG foreight fundamental action classes. We confirmed that our method achieved anaccuracy of 97.9% when trained and evaluated on the same person and in the sameenvironment. Additionally, our method achieved an accuracy of 89.5% even whentrained and evaluated on different people. We also report the analyses ofaccuracies in various conditions and limitations.</description><author>Risako Tanigawa, Yasunori Ishii</author><pubDate>Fri, 15 Sep 2023 02:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08087v1</guid></item><item><title>Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition</title><link>http://arxiv.org/abs/2401.14034v1</link><description>Unsupervised skeleton based action recognition has achieved remarkableprogress recently. Existing unsupervised learning methods suffer from severeoverfitting problem, and thus small networks are used, significantly reducingthe representation capability. To address this problem, the overfittingmechanism behind the unsupervised learning for skeleton based actionrecognition is first investigated. It is observed that the skeleton is alreadya relatively high-level and low-dimension feature, but not in the same manifoldas the features for action recognition. Simply applying the existingunsupervised learning method may tend to produce features that discriminate thedifferent samples instead of action classes, resulting in the overfittingproblem. To solve this problem, this paper presents an Unsupervisedspatial-temporal Feature Enrichment and Fidelity Preservation framework(U-FEFP) to generate rich distributed features that contain all the informationof the skeleton sequence. A spatial-temporal feature transformation subnetworkis developed using spatial-temporal graph convolutional network and graphconvolutional gate recurrent unit network as the basic feature extractionnetwork. The unsupervised Bootstrap Your Own Latent based learning is used togenerate rich distributed features and the unsupervised pretext task basedlearning is used to preserve the information of the skeleton sequence. The twounsupervised learning ways are collaborated as U-FEFP to produce robust anddiscriminative representations. Experimental results on three widely usedbenchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstratethat the proposed U-FEFP achieves the best performance compared with thestate-of-the-art unsupervised learning methods. t-SNE illustrations furthervalidate that U-FEFP can learn more discriminative features for unsupervisedskeleton based action recognition.</description><author>Chuankun Li, Shuai Li, Yanbo Gao, Ping Chen, Jian Li, Wanqing Li</author><pubDate>Thu, 25 Jan 2024 09:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14034v1</guid></item><item><title>DVANet: Disentangling View and Action Features for Multi-View Action Recognition</title><link>http://arxiv.org/abs/2312.05719v1</link><description>In this work, we present a novel approach to multi-view action recognitionwhere we guide learned action representations to be separated fromview-relevant information in a video. When trying to classify action instancescaptured from multiple viewpoints, there is a higher degree of difficulty dueto the difference in background, occlusion, and visibility of the capturedaction from different camera angles. To tackle the various problems introducedin multi-view action recognition, we propose a novel configuration of learnabletransformer decoder queries, in conjunction with two supervised contrastivelosses, to enforce the learning of action features that are robust to shifts inviewpoints. Our disentangled feature learning occurs in two stages: thetransformer decoder uses separate queries to separately learn action and viewinformation, which are then further disentangled using our two contrastivelosses. We show that our model and method of training significantly outperformsall other uni-modal models on four multi-view action recognition datasets: NTURGB+D, NTU RGB+D 120, PKU-MMD, and N-UCLA. Compared to previous RGB works, wesee maximal improvements of 1.5\%, 4.8\%, 2.2\%, and 4.8\% on each dataset,respectively.</description><author>Nyle Siddiqui, Praveen Tirupattur, Mubarak Shah</author><pubDate>Sun, 10 Dec 2023 01:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05719v1</guid></item><item><title>Multi-modal Instance Refinement for Cross-domain Action Recognition</title><link>http://arxiv.org/abs/2311.14281v1</link><description>Unsupervised cross-domain action recognition aims at adapting the modeltrained on an existing labeled source domain to a new unlabeled target domain.Most existing methods solve the task by directly aligning the featuredistributions of source and target domains. However, this would cause negativetransfer during domain adaptation due to some negative training samples in bothdomains. In the source domain, some training samples are of low-relevance totarget domain due to the difference in viewpoints, action styles, etc. In thetarget domain, there are some ambiguous training samples that can be easilyclassified as another type of action under the case of source domain. Theproblem of negative transfer has been explored in cross-domain objectdetection, while it remains under-explored in cross-domain action recognition.Therefore, we propose a Multi-modal Instance Refinement (MMIR) method toalleviate the negative transfer based on reinforcement learning. Specifically,a reinforcement learning agent is trained in both domains for every modality torefine the training data by selecting out negative samples from each domain.Our method finally outperforms several other state-of-the-art baselines incross-domain action recognition on the benchmark EPIC-Kitchens dataset, whichdemonstrates the advantage of MMIR in reducing negative transfer.</description><author>Yuan Qing, Naixing Wu, Shaohua Wan, Lixin Duan</author><pubDate>Fri, 24 Nov 2023 05:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14281v1</guid></item><item><title>Egocentric RGB+Depth Action Recognition in Industry-Like Settings</title><link>http://arxiv.org/abs/2309.13962v1</link><description>Action recognition from an egocentric viewpoint is a crucial perception taskin robotics and enables a wide range of human-robot interactions. While mostcomputer vision approaches prioritize the RGB camera, the Depth modality -which can further amplify the subtleties of actions from an egocentricperspective - remains underexplored. Our work focuses on recognizing actionsfrom egocentric RGB and Depth modalities in an industry-like environment. Tostudy this problem, we consider the recent MECCANO dataset, which provides awide range of assembling actions. Our framework is based on the 3D Video SWINTransformer to encode both RGB and Depth modalities effectively. To address theinherent skewness in real-world multimodal action occurrences, we propose atraining strategy using an exponentially decaying variant of the focal lossmodulating factor. Additionally, to leverage the information in both RGB andDepth modalities, we opt for late fusion to combine the predictions from eachmodality. We thoroughly evaluate our method on the action recognition task ofthe MECCANO dataset, and it significantly outperforms the prior work. Notably,our method also secured first place at the multimodal action recognitionchallenge at ICIAP 2023.</description><author>Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah</author><pubDate>Mon, 25 Sep 2023 09:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13962v1</guid></item><item><title>Cross-Modal Learning with 3D Deformable Attention for Action Recognition</title><link>http://arxiv.org/abs/2212.05638v3</link><description>An important challenge in vision-based action recognition is the embedding ofspatiotemporal features with two or more heterogeneous modalities into a singlefeature. In this study, we propose a new 3D deformable transformer for actionrecognition with adaptive spatiotemporal receptive fields and a cross-modallearning scheme. The 3D deformable transformer consists of three attentionmodules: 3D deformability, local joint stride, and temporal stride attention.The two cross-modal tokens are input into the 3D deformable attention module tocreate a cross-attention token with a reflected spatiotemporal correlation.Local joint stride attention is applied to spatially combine attention and posetokens. Temporal stride attention temporally reduces the number of input tokensin the attention module and supports temporal expression learning without thesimultaneous use of all tokens. The deformable transformer iterates L-times andcombines the last cross-modal token for classification. The proposed 3Ddeformable transformer was tested on the NTU60, NTU120, FineGYM, and PennActiondatasets, and showed results better than or similar to pre-trainedstate-of-the-art methods even without a pre-training process. In addition, byvisualizing important joints and correlations during action recognition throughspatial joint and temporal stride attention, the possibility of achieving anexplainable potential for action recognition is presented.</description><author>Sangwon Kim, Dasom Ahn, Byoung Chul Ko</author><pubDate>Thu, 17 Aug 2023 08:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05638v3</guid></item><item><title>Flow Dynamics Correction for Action Recognition</title><link>http://arxiv.org/abs/2310.10059v1</link><description>Various research studies indicate that action recognition performance highlydepends on the types of motions being extracted and how accurate the humanactions are represented. In this paper, we investigate different optical flow,and features extracted from these optical flow that capturing both short-termand long-term motion dynamics. We perform power normalization on the magnitudecomponent of optical flow for flow dynamics correction to boost subtle ordampen sudden motions. We show that existing action recognition models whichrely on optical flow are able to get performance boosted with our correctedoptical flow. To further improve performance, we integrate our corrected flowdynamics into popular models through a simple hallucination step by selectingonly the best performing optical flow features, and we show that by'translating' the CNN feature maps into these optical flow features withdifferent scales of motions leads to the new state-of-the-art performance onseveral benchmarks including HMDB-51, YUP++, fine-grained action recognition onMPII Cooking Activities, and large-scale Charades.</description><author>Lei Wang, Piotr Koniusz</author><pubDate>Mon, 16 Oct 2023 05:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10059v1</guid></item><item><title>Action Recognition Utilizing YGAR Dataset</title><link>http://arxiv.org/abs/2310.00831v1</link><description>The scarcity of high quality actions video data is a bottleneck in theresearch and application of action recognition. Although significant effort hasbeen made in this area, there still exist gaps in the range of available datatypes a more flexible and comprehensive data set could help bridge. In thispaper, we present a new 3D actions data simulation engine and generate 3 setsof sample data to demonstrate its current functionalities. With the new datageneration process, we demonstrate its applications to image classifications,action recognitions and potential to evolve into a system that would allow theexploration of much more complex action recognition tasks. In order to show offthese capabilities, we also train and test a list of commonly used models forimage recognition to demonstrate the potential applications and capabilities ofthe data sets and their generation process.</description><author>Shuo Wang, Amiya Ranjan, Lawrence Jiang</author><pubDate>Mon, 02 Oct 2023 01:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00831v1</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>M$^3$Net: Multi-view Encoding, Matching, and Fusion for Few-shot Fine-grained Action Recognition</title><link>http://arxiv.org/abs/2308.03063v1</link><description>Due to the scarcity of manually annotated data required for fine-grainedvideo understanding, few-shot fine-grained (FS-FG) action recognition hasgained significant attention, with the aim of classifying novel fine-grainedaction categories with only a few labeled instances. Despite the progress madein FS coarse-grained action recognition, current approaches encounter twochallenges when dealing with the fine-grained action categories: the inabilityto capture subtle action details and the insufficiency of learning from limiteddata that exhibit high intra-class variance and inter-class similarity. Toaddress these limitations, we propose M$^3$Net, a matching-based framework forFS-FG action recognition, which incorporates \textit{multi-view encoding},\textit{multi-view matching}, and \textit{multi-view fusion} to facilitateembedding encoding, similarity matching, and decision making across multipleviewpoints. \textit{Multi-view encoding} captures rich contextual details fromthe intra-frame, intra-video, and intra-episode perspectives, generatingcustomized higher-order embeddings for fine-grained data. \textit{Multi-viewmatching} integrates various matching functions enabling flexible relationmodeling within limited samples to handle multi-scale spatio-temporalvariations by leveraging the instance-specific, category-specific, andtask-specific perspectives. \textit{Multi-view fusion} consists ofmatching-predictions fusion and matching-losses fusion over the above views,where the former promotes mutual complementarity and the latter enhancesembedding generalizability by employing multi-task collaborative learning.Explainable visualizations and experimental results on three challengingbenchmarks demonstrate the superiority of M$^3$Net in capturing fine-grainedaction details and achieving state-of-the-art performance for FS-FG actionrecognition.</description><author>Hao Tang, Jun Liu, Shuanglin Yan, Rui Yan, Zechao Li, Jinhui Tang</author><pubDate>Sun, 06 Aug 2023 10:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03063v1</guid></item><item><title>On the Importance of Spatial Relations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2308.07119v1</link><description>Deep learning has achieved great success in video recognition, yet stillstruggles to recognize novel actions when faced with only a few examples. Totackle this challenge, few-shot action recognition methods have been proposedto transfer knowledge from a source dataset to a novel target dataset with onlyone or a few labeled videos. However, existing methods mainly focus on modelingthe temporal relations between the query and support videos while ignoring thespatial relations. In this paper, we find that the spatial misalignment betweenobjects also occurs in videos, notably more common than the temporalinconsistency. We are thus motivated to investigate the importance of spatialrelations and propose a more accurate few-shot action recognition method thatleverages both spatial and temporal information. Particularly, a novel SpatialAlignment Cross Transformer (SA-CT) which learns to re-adjust the spatialrelations and incorporates the temporal information is contributed. Experimentsreveal that, even without using any temporal information, the performance ofSA-CT is comparable to temporal based methods on 3/4 benchmarks. To furtherincorporate the temporal information, we propose a simple yet effectiveTemporal Mixer module. The Temporal Mixer enhances the video representation andimproves the performance of the full SA-CT model, achieving very competitiveresults. In this work, we also exploit large-scale pretrained models forfew-shot action recognition, providing useful insights for this researchdirection.</description><author>Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 14 Aug 2023 13:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07119v1</guid></item><item><title>Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition</title><link>http://arxiv.org/abs/2311.01283v1</link><description>This paper presents a study on improving human action recognition through theutilization of knowledge distillation, and the combination of CNN and ViTmodels. The research aims to enhance the performance and efficiency of smallerstudent models by transferring knowledge from larger teacher models. Theproposed method employs a Transformer vision network as the student model,while a convolutional network serves as the teacher model. The teacher modelextracts local image features, whereas the student model focuses on globalfeatures using an attention mechanism. The Vision Transformer (ViT)architecture is introduced as a robust framework for capturing globaldependencies in images. Additionally, advanced variants of ViT, namely PVT,Convit, MVIT, Swin Transformer, and Twins, are discussed, highlighting theircontributions to computer vision tasks. The ConvNeXt model is introduced as ateacher model, known for its efficiency and effectiveness in computer vision.The paper presents performance results for human action recognition on theStanford 40 dataset, comparing the accuracy and mAP of student models trainedwith and without knowledge distillation. The findings illustrate that thesuggested approach significantly improves the accuracy and mAP when compared totraining networks under regular settings. These findings emphasize thepotential of combining local and global features in action recognition tasks.</description><author>Hamid Ahmadabadi, Omid Nejati Manzari, Ahmad Ayatollahi</author><pubDate>Thu, 02 Nov 2023 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01283v1</guid></item><item><title>Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey</title><link>http://arxiv.org/abs/2310.13039v1</link><description>Human pose analysis has garnered significant attention within both theresearch community and practical applications, owing to its expanding array ofuses, including gaming, video surveillance, sports performance analysis, andhuman-computer interactions, among others. The advent of deep learning hassignificantly improved the accuracy of pose capture, making pose-basedapplications increasingly practical. This paper presents a comprehensive surveyof pose-based applications utilizing deep learning, encompassing poseestimation, pose tracking, and action recognition.Pose estimation involves thedetermination of human joint positions from images or image sequences. Posetracking is an emerging research direction aimed at generating consistent humanpose trajectories over time. Action recognition, on the other hand, targets theidentification of action types using pose estimation or tracking data. Thesethree tasks are intricately interconnected, with the latter often reliant onthe former. In this survey, we comprehensively review related works, spanningfrom single-person pose estimation to multi-person pose estimation, from 2Dpose estimation to 3D pose estimation, from single image to video, from miningtemporal context gradually to pose tracking, and lastly from tracking topose-based action recognition. As a survey centered on the application of deeplearning to pose analysis, we explicitly discuss both the strengths andlimitations of existing techniques. Notably, we emphasize methodologies forintegrating these three tasks into a unified framework within video sequences.Additionally, we explore the challenges involved and outline potentialdirections for future research.</description><author>Lijuan Zhou, Xiang Meng, Zhihuan Liu, Mengqi Wu, Zhimin Gao, Pichao Wang</author><pubDate>Thu, 19 Oct 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13039v1</guid></item><item><title>Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data</title><link>http://arxiv.org/abs/2309.15635v1</link><description>In this work, we propose a position and orientation-aware one-shot learningframework for medical action recognition from signal data. The proposedframework comprises two stages and each stage includes signal-level imagegeneration (SIG), cross-attention (CsA), dynamic time warping (DTW) modules andthe information fusion between the proposed privacy-preserved position andorientation features. The proposed SIG method aims to transform the rawskeleton data into privacy-preserved features for training. The CsA module isdeveloped to guide the network in reducing medical action recognition bias andmore focusing on important human body parts for each specific action, aimed ataddressing similar medical action related issues. Moreover, the DTW module isemployed to minimize temporal mismatching between instances and further improvemodel performance. Furthermore, the proposed privacy-preservedorientation-level features are utilized to assist the position-level featuresin both of the two stages for enhancing medical action recognition performance.Extensive experimental results on the widely-used and well-known NTU RGB+D 60,NTU RGB+D 120, and PKU-MMD datasets all demonstrate the effectiveness of theproposed method, which outperforms the other state-of-the-art methods withgeneral dataset partitioning by 2.7%, 6.2% and 4.1%, respectively.</description><author>Leiyu Xie, Yuxing Yang, Zeyu Fu, Syed Mohsen Naqvi</author><pubDate>Wed, 27 Sep 2023 14:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15635v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v1</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 29 May 2023 09:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v2</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Sat, 30 Dec 2023 12:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v2</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Learning Human Action Recognition Representations Without Real Humans</title><link>http://arxiv.org/abs/2311.06231v1</link><description>Pre-training on massive video datasets has become essential to achieve highaction recognition performance on smaller downstream datasets. However, mostlarge-scale video datasets contain images of people and hence are accompaniedwith issues related to privacy, ethics, and data protection, often preventingthem from being publicly shared for reproducible research. Existing work hasattempted to alleviate these problems by blurring faces, downsampling videos,or training on synthetic data. On the other hand, analysis on thetransferability of privacy-preserving pre-trained models to downstream taskshas been limited. In this work, we study this problem by first asking thequestion: can we pre-train models for human action recognition with data thatdoes not include real humans? To this end, we present, for the first time, abenchmark that leverages real-world videos with humans removed and syntheticdata containing virtual humans to pre-train a model. We then evaluate thetransferability of the representation learned on this data to a diverse set ofdownstream action recognition benchmarks. Furthermore, we propose a novelpre-training strategy, called Privacy-Preserving MAE-Align, to effectivelycombine synthetic data and human-removed real data. Our approach outperformsprevious baselines by up to 5% and closes the performance gap between human andno-human action recognition representations on downstream tasks, for bothlinear probing and fine-tuning. Our benchmark, code, and models are availableat https://github.com/howardzh01/PPMA .</description><author>Howard Zhong, Samarth Mishra, Donghyun Kim, SouYoung Jin, Rameswar Panda, Hilde Kuehne, Leonid Karlinsky, Venkatesh Saligrama, Aude Oliva, Rogerio Feris</author><pubDate>Fri, 10 Nov 2023 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06231v1</guid></item><item><title>Early Action Recognition with Action Prototypes</title><link>http://arxiv.org/abs/2312.06598v1</link><description>Early action recognition is an important and challenging problem that enablesthe recognition of an action from a partially observed video stream where theactivity is potentially unfinished or even not started. In this work, wepropose a novel model that learns a prototypical representation of the fullaction for each class and uses it to regularize the architecture and the visualrepresentations of the partial observations. Our model is very simple in designand also efficient. We decompose the video into short clips, where a visualencoder extracts features from each clip independently. Later, a decoderaggregates together in an online fashion features from all the clips for thefinal class prediction. During training, for each partial observation, themodel is jointly trained to both predict the label as well as the actionprototypical representation which acts as a regularizer. We evaluate our methodon multiple challenging real-world datasets and outperform the currentstate-of-the-art by a significant margin. For example, on early recognitionobserving only the first 10% of each video, our method improves the SOTA by+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 onSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used eithermulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we alsopresent exhaustive ablation studies to motivate the design choices we made, aswell as gather insights regarding what our model is learning semantically.</description><author>Guglielmo Camporese, Alessandro Bergamo, Xunyu Lin, Joseph Tighe, Davide Modolo</author><pubDate>Mon, 11 Dec 2023 18:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06598v1</guid></item><item><title>Improving Zero-Shot Action Recognition using Human Instruction with Text Description</title><link>http://arxiv.org/abs/2301.08874v2</link><description>Zero-shot action recognition, which recognizes actions in videos withouthaving received any training examples, is gaining wide attention considering itcan save labor costs and training time. Nevertheless, the performance ofzero-shot learning is still unsatisfactory, which limits its practicalapplication. To solve this problem, this study proposes a framework to improvezero-shot action recognition using human instructions with text descriptions.The proposed framework manually describes video contents, which incurs somelabor costs; in many situations, the labor costs are worth it. We manuallyannotate text features for each action, which can be a word, phrase, orsentence. Then by computing the matching degrees between the video and all textfeatures, we can predict the class of the video. Furthermore, the proposedmodel can also be combined with other models to improve its accuracy. Inaddition, our model can be continuously optimized to improve the accuracy byrepeating human instructions. The results with UCF101 and HMDB51 showed thatour model achieved the best accuracy and improved the accuracies of othermodels.</description><author>Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 12 Jun 2023 09:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08874v2</guid></item><item><title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2212.04761v2</link><description>Skeleton-based action recognition has attracted considerable attention due toits compact representation of the human body's skeletal sructure. Many recentmethods have achieved remarkable performance using graph convolutional networks(GCNs) and convolutional neural networks (CNNs), which extract spatial andtemporal features, respectively. Although spatial and temporal dependencies inthe human skeleton have been explored separately, spatio-temporal dependency israrely considered. In this paper, we propose the Spatio-Temporal Curve Network(STC-Net) to effectively leverage the spatio-temporal dependency of the humanskeleton. Our proposed network consists of two novel elements: 1) TheSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for GraphConvolution (DK-GC). The STC module dynamically adjusts the receptive field byidentifying meaningful node connections between every adjacent frame andgenerating spatio-temporal curves based on the identified node connections,providing an adaptive spatio-temporal coverage. In addition, we propose DK-GCto consider long-range dependencies, which results in a large receptive fieldwithout any additional parameters by applying an extended kernel to the givenadjacency matrices of the graph. Our STC-Net combines these two modules andachieves state-of-the-art performance on four skeleton-based action recognitionbenchmarks.</description><author>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 03:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04761v2</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>GTAutoAct: An Automatic Datasets Generation Framework Based on Game Engine Redevelopment for Action Recognition</title><link>http://arxiv.org/abs/2401.13414v1</link><description>Current datasets for action recognition tasks face limitations stemming fromtraditional collection and generation methods, including the constrained rangeof action classes, absence of multi-viewpoint recordings, limited diversity,poor video quality, and labor-intensive manually collection. To address thesechallenges, we introduce GTAutoAct, a innovative dataset generation frameworkleveraging game engine technology to facilitate advancements in actionrecognition. GTAutoAct excels in automatically creating large-scale,well-annotated datasets with extensive action classes and superior videoquality. Our framework's distinctive contributions encompass: (1) itinnovatively transforms readily available coordinate-based 3D human motion intorotation-orientated representation with enhanced suitability in multipleviewpoints; (2) it employs dynamic segmentation and interpolation of rotationsequences to create smooth and realistic animations of action; (3) it offersextensively customizable animation scenes; (4) it implements an autonomousvideo capture and processing pipeline, featuring a randomly navigating camera,with auto-trimming and labeling functionalities. Experimental resultsunderscore the framework's robustness and highlights its potential tosignificantly improve action recognition model training.</description><author>Xingyu Song, Zhan Li, Shi Chen, Kazuyuki Demachi</author><pubDate>Wed, 24 Jan 2024 12:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13414v1</guid></item><item><title>SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</title><link>http://arxiv.org/abs/2309.11445v1</link><description>We present SkeleTR, a new framework for skeleton-based action recognition. Incontrast to prior work, which focuses mainly on controlled environments, wetarget more general scenarios that typically involve a variable number ofpeople and various forms of interaction between people. SkeleTR works with atwo-stage paradigm. It first models the intra-person skeleton dynamics for eachskeleton sequence with graph convolutions, and then uses stacked Transformerencoders to capture person interactions that are important for actionrecognition in general scenarios. To mitigate the negative impact of inaccurateskeleton associations, SkeleTR takes relative short skeleton sequences as inputand increases the number of sequences. As a unified solution, SkeleTR can bedirectly applied to multiple skeleton-based action tasks, including video-levelaction classification, instance-level action detection, and group-levelactivity recognition. It also enables transfer learning and joint trainingacross different action tasks and datasets, which result in performanceimprovement. When evaluated on various skeleton-based action recognitionbenchmarks, SkeleTR achieves the state-of-the-art performance.</description><author>Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo</author><pubDate>Wed, 20 Sep 2023 17:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11445v1</guid></item><item><title>Two-stream joint matching method based on contrastive learning for few-shot action recognition</title><link>http://arxiv.org/abs/2401.04150v1</link><description>Although few-shot action recognition based on metric learning paradigm hasachieved significant success, it fails to address the following issues: (1)inadequate action relation modeling and underutilization of multi-modalinformation; (2) challenges in handling video matching problems with differentlengths and speeds, and video matching problems with misalignment of videosub-actions. To address these issues, we propose a Two-Stream Joint Matchingmethod based on contrastive learning (TSJM), which consists of two modules:Multi-modal Contrastive Learning Module (MCL) and Joint Matching Module (JMM).The objective of the MCL is to extensively investigate the inter-modal mutualinformation relationships, thereby thoroughly extracting modal information toenhance the modeling of action relationships. The JMM aims to simultaneouslyaddress the aforementioned video matching problems. The effectiveness of theproposed method is evaluated on two widely used few shot action recognitiondatasets, namely, SSv2 and Kinetics. Comprehensive ablation experiments arealso conducted to substantiate the efficacy of our proposed approach.</description><author>Long Deng, Ziqiang Li, Bingxin Zhou, Zhongming Chen, Ao Li, Yongxin Ge</author><pubDate>Mon, 08 Jan 2024 13:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04150v1</guid></item><item><title>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</title><link>http://arxiv.org/abs/2308.03950v1</link><description>Zero-shot skeleton-based action recognition aims to recognize actions ofunseen categories after training on data of seen categories. The key is tobuild the connection between visual and semantic space from seen to unseenclasses. Previous studies have primarily focused on encoding sequences into asingular feature vector, with subsequent mapping the features to an identicalanchor point within the embedded space. Their performance is hindered by 1) theignorance of the global visual/semantic distribution alignment, which resultsin a limitation to capture the true interdependence between the two spaces. 2)the negligence of temporal information since the frame-wise features with richaction clues are directly pooled into a single feature vector. We propose a newzero-shot skeleton-based action recognition method via mutual information (MI)estimation and maximization. Specifically, 1) we maximize the MI between visualand semantic space for distribution alignment; 2) we leverage the temporalinformation for estimating the MI by encouraging MI to increase as more framesare observed. Extensive experiments on three large-scale skeleton actiondatasets confirm the effectiveness of our method. Code:https://github.com/YujieOuO/SMIE.</description><author>Yujie Zhou, Wenwen Qiang, Anyi Rao, Ning Lin, Bing Su, Jiaqi Wang</author><pubDate>Tue, 08 Aug 2023 00:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03950v1</guid></item><item><title>Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2302.13434v2</link><description>Recently, skeleton-based human action has become a hot research topic becausethe compact representation of human skeletons brings new blood to this researchdomain. As a result, researchers began to notice the importance of using RGB orother sensors to analyze human action by extracting skeleton information.Leveraging the rapid development of deep learning (DL), a significant number ofskeleton-based human action approaches have been presented with fine-designedDL structures recently. However, a well-trained DL model always demandshigh-quality and sufficient data, which is hard to obtain without costing highexpenses and human labor. In this paper, we introduce a novel data augmentationmethod for skeleton-based action recognition tasks, which can effectivelygenerate high-quality and diverse sequential actions. In order to obtainnatural and realistic action sequences, we propose denoising diffusionprobabilistic models (DDPMs) that can generate a series of synthetic actionsequences, and their generation process is precisely guided by aspatial-temporal transformer (ST-Trans). Experimental results show that ourmethod outperforms the state-of-the-art (SOTA) motion generation approaches ondifferent naturality and diversity metrics. It proves that its high-qualitysynthetic data can also be effectively deployed to existing action recognitionmodels with significant performance improvement.</description><author>Yifan Jiang, Han Chen, Hanseok Ko</author><pubDate>Tue, 25 Jul 2023 03:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13434v2</guid></item><item><title>Active Generation Network of Human Skeleton for Action Recognition</title><link>http://arxiv.org/abs/2401.17086v1</link><description>Data generation is a data augmentation technique for enhancing thegeneralization ability for skeleton-based human action recognition. Mostexisting data generation methods face challenges to ensure the temporalconsistency of the dynamic information for action. In addition, the datagenerated by these methods lack diversity when only a few training samples areavailable. To solve those problems, We propose a novel active generativenetwork (AGN), which can adaptively learn various action categories by motionstyle transfer to generate new actions when the data for a particular action isonly a single sample or few samples. The AGN consists of an action generationnetwork and an uncertainty metric network. The former, with ST-GCN as theBackbone, can implicitly learn the morphological features of the target actionwhile preserving the category features of the source action. The latter guidesgenerating actions. Specifically, an action recognition model generatesprediction vectors for each action, which is then scored using an uncertaintymetric. Finally, UMN provides the uncertainty sampling basis for the generatedactions.</description><author>Long Liu, Xin Wang, Fangming Li, Jiayu Chen</author><pubDate>Tue, 30 Jan 2024 15:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17086v1</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v3</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code is made available athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Wed, 10 Jan 2024 12:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v3</guid></item><item><title>Collaboratively Self-supervised Video Representation Learning for Action Recognition</title><link>http://arxiv.org/abs/2401.07584v1</link><description>Considering the close connection between action recognition and human poseestimation, we design a Collaboratively Self-supervised Video Representation(CSVR) learning framework specific to action recognition by jointly consideringgenerative pose prediction and discriminative context matching as pretexttasks. Specifically, our CSVR consists of three branches: a generative poseprediction branch, a discriminative context matching branch, and a videogenerating branch. Among them, the first one encodes dynamic motion feature byutilizing Conditional-GAN to predict the human poses of future frames, and thesecond branch extracts static context features by pulling the representationsof clips and compressed key frames from the same video together while pushingapart the pairs from different videos. The third branch is designed to recoverthe current video frames and predict the future ones, for the purpose ofcollaboratively improving dynamic motion features and static context features.Extensive experiments demonstrate that our method achieves state-of-the-artperformance on the UCF101 and HMDB51 datasets.</description><author>Jie Zhang, Zhifan Wan, Lanqing Hu, Stephen Lin, Shuzhe Wu, Shiguang Shan</author><pubDate>Mon, 15 Jan 2024 10:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07584v1</guid></item><item><title>Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments</title><link>http://arxiv.org/abs/2309.12029v1</link><description>To integrate action recognition methods into autonomous robotic systems, itis crucial to consider adverse situations involving target occlusions. Such ascenario, despite its practical relevance, is rarely addressed in existingself-supervised skeleton-based action recognition methods. To empower robotswith the capacity to address occlusion, we propose a simple and effectivemethod. We first pre-train using occluded skeleton sequences, then use k-meansclustering (KMeans) on sequence embeddings to group semantically similarsamples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeletondata based on the closest sample neighbors. Imputing incomplete skeletonsequences to create relatively complete sequences as input provides significantbenefits to existing skeleton-based self-supervised models. Meanwhile, buildingon the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introducean Occluded Partial Spatio-Temporal Learning (OPSTL) framework. Thisenhancement utilizes Adaptive Spatial Masking (ASM) for better use ofhigh-quality, intact skeletons. The effectiveness of our imputation methods isverified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D120. The source code will be made publicly available athttps://github.com/cyfml/OPSTL.</description><author>Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</author><pubDate>Thu, 21 Sep 2023 13:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12029v1</guid></item><item><title>Graph Contrastive Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2301.10900v2</link><description>In the field of skeleton-based action recognition, current top-performinggraph convolutional networks (GCNs) exploit intra-sequence context to constructadaptive graphs for feature aggregation. However, we argue that such context isstill \textit{local} since the rich cross-sequence relations have not beenexplicitly investigated. In this paper, we propose a graph contrastive learningframework for skeleton-based action recognition (\textit{SkeletonGCL}) toexplore the \textit{global} context across all sequences. In specific,SkeletonGCL associates graph learning across sequences by enforcing graphs tobe class-discriminative, \emph{i.e.,} intra-class compact and inter-classdispersed, which improves the GCN capacity to distinguish various actionpatterns. Besides, two memory banks are designed to enrich cross-sequencecontext from two complementary levels, \emph{i.e.,} instance and semanticlevels, enabling graph contrastive learning in multiple context scales.Consequently, SkeletonGCL establishes a new training paradigm, and it can beseamlessly incorporated into current GCNs. Without loss of generality, wecombine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), andachieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. Thesource code will be available at\url{https://github.com/OliverHxh/SkeletonGCL}.</description><author>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</author><pubDate>Sat, 10 Jun 2023 11:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10900v2</guid></item><item><title>Video BagNet: short temporal receptive fields increase robustness in long-term action recognition</title><link>http://arxiv.org/abs/2308.11249v1</link><description>Previous work on long-term video action recognition relies on deep3D-convolutional models that have a large temporal receptive field (RF). Weargue that these models are not always the best choice for temporal modeling invideos. A large temporal receptive field allows the model to encode the exactsub-action order of a video, which causes a performance decrease when testingvideos have a different sub-action order. In this work, we investigate whetherwe can improve the model robustness to the sub-action order by shrinking thetemporal receptive field of action recognition models. For this, we designVideo BagNet, a variant of the 3D ResNet-50 model with the temporal receptivefield size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet onsynthetic and real-world video datasets and experimentally compare models withvarying temporal receptive fields. We find that short receptive fields arerobust to sub-action order changes, while larger temporal receptive fields aresensitive to the sub-action order.</description><author>Ombretta Strafforello, Xin Liu, Klamer Schutte, Jan van Gemert</author><pubDate>Tue, 22 Aug 2023 08:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11249v1</guid></item><item><title>From Detection to Action Recognition: An Edge-Based Pipeline for Robot Human Perception</title><link>http://arxiv.org/abs/2312.03477v1</link><description>Mobile service robots are proving to be increasingly effective in a range ofapplications, such as healthcare, monitoring Activities of Daily Living (ADL),and facilitating Ambient Assisted Living (AAL). These robots heavily rely onHuman Action Recognition (HAR) to interpret human actions and intentions.However, for HAR to function effectively on service robots, it requires priorknowledge of human presence (human detection) and identification of individualsto monitor (human tracking). In this work, we propose an end-to-end pipelinethat encompasses the entire process, starting from human detection andtracking, leading to action recognition. The pipeline is designed to operate innear real-time while ensuring all stages of processing are performed on theedge, reducing the need for centralised computation. To identify the mostsuitable models for our mobile robot, we conducted a series of experimentscomparing state-of-the-art solutions based on both their detection performanceand efficiency. To evaluate the effectiveness of our proposed pipeline, weproposed a dataset comprising daily household activities. By presenting ourfindings and analysing the results, we demonstrate the efficacy of our approachin enabling mobile robots to understand and respond to human behaviour inreal-world scenarios relying mainly on the data from their RGB cameras.</description><author>Petros Toupas, Georgios Tsamis, Dimitrios Giakoumis, Konstantinos Votis, Dimitrios Tzovaras</author><pubDate>Wed, 06 Dec 2023 13:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03477v1</guid></item><item><title>IndGIC: Supervised Action Recognition under Low Illumination</title><link>http://arxiv.org/abs/2308.15345v1</link><description>Technologies of human action recognition in the dark are gaining more andmore attention as huge demand in surveillance, motion control andhuman-computer interaction. However, because of limitation in image enhancementmethod and low-lighting video datasets, e.g. labeling cost, existing methodsmeet some problems. Some video-based approached are effect and efficient inspecific datasets but cannot generalize to most cases while others methodsusing multiple sensors rely heavily to prior knowledge to deal with noisynature from video stream. In this paper, we proposes action recognition methodusing deep multi-input network. Furthermore, we proposed a Independent GammaIntensity Corretion (Ind-GIC) to enhance poor-illumination video, generatingone gamma for one frame to increase enhancement performance. To prove ourmethod is effective, there is some evaluation and comparison between our methodand existing methods. Experimental results show that our model achieves highaccuracy in on ARID dataset.</description><author>Jingbo Zeng</author><pubDate>Tue, 29 Aug 2023 15:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15345v1</guid></item><item><title>Robustness Evaluation of Machine Learning Models for Robot Arm Action Recognition in Noisy Environments</title><link>http://arxiv.org/abs/2401.09606v1</link><description>In the realm of robot action recognition, identifying distinct but spatiallyproximate arm movements using vision systems in noisy environments poses asignificant challenge. This paper studies robot arm action recognition in noisyenvironments using machine learning techniques. Specifically, a vision systemis used to track the robot's movements followed by a deep learning model toextract the arm's key points. Through a comparative analysis of machinelearning methods, the effectiveness and robustness of this model are assessedin noisy environments. A case study was conducted using the Tic-Tac-Toe game ina 3-by-3 grid environment, where the focus is to accurately identify theactions of the arms in selecting specific locations within this constrainedenvironment. Experimental results show that our approach can achieve precisekey point detection and action classification despite the addition of noise anduncertainties to the dataset.</description><author>Elaheh Motamedi, Kian Behzad, Rojin Zandi, Hojjat Salehinejad, Milad Siami</author><pubDate>Wed, 17 Jan 2024 21:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09606v1</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</title><link>http://arxiv.org/abs/2307.09238v1</link><description>As collaborative robots (cobots) continue to gain popularity in industrialmanufacturing, effective human-robot collaboration becomes crucial. Cobotsshould be able to recognize human actions to assist with assembly tasks and actautonomously. To achieve this, skeleton-based approaches are often used due totheir ability to generalize across various people and environments. Althoughbody skeleton approaches are widely used for action recognition, they may notbe accurate enough for assembly actions where the worker's fingers and handsplay a significant role. To address this limitation, we propose a method inwhich less detailed body skeletons are combined with highly detailed handskeletons. We investigate CNNs and transformers, the latter of which areparticularly adept at extracting and combining important information from bothskeleton types using attention. This paper demonstrates the effectiveness ofour proposed approach in enhancing action recognition in assembly scenarios.</description><author>Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Tue, 18 Jul 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09238v1</guid></item><item><title>How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</title><link>http://arxiv.org/abs/2306.05844v1</link><description>As the use of collaborative robots (cobots) in industrial manufacturingcontinues to grow, human action recognition for effective human-robotcollaboration becomes increasingly important. This ability is crucial forcobots to act autonomously and assist in assembly tasks. Recently,skeleton-based approaches are often used as they tend to generalize better todifferent people and environments. However, when processing skeletons alone,information about the objects a human interacts with is lost. Therefore, wepresent a novel approach of integrating object information into skeleton-basedaction recognition. We enhance two state-of-the-art methods by treating objectcenters as further skeleton joints. Our experiments on the assembly datasetIKEA ASM show that our approach improves the performance of thesestate-of-the-art methods to a large extent when combining skeleton joints withobjects predicted by a state-of-the-art instance segmentation model. Ourresearch sheds light on the benefits of combining skeleton joints with objectinformation for human action recognition in assembly tasks. We analyze theeffect of the object detector on the combination for action classification anddiscuss the important factors that must be taken into account.</description><author>Dustin Aganian, Mona Köhler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Fri, 09 Jun 2023 13:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05844v1</guid></item><item><title>Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring</title><link>http://arxiv.org/abs/2401.14838v1</link><description>Driver Action Recognition (DAR) is crucial in vehicle cabin monitoringsystems. In real-world applications, it is common for vehicle cabins to beequipped with cameras featuring different modalities. However, multi-modalityfusion strategies for the DAR task within car cabins have rarely been studied.In this paper, we propose a novel yet efficient multi-modality driver actionrecognition method based on dual feature shift, named DFS. DFS first integratescomplementary features across modalities by performing modality featureinteraction. Meanwhile, DFS achieves the neighbour feature propagation withinsingle modalities, by feature shifting among temporal frames. To learn commonpatterns and improve model efficiency, DFS shares feature extracting stagesamong multiple modalities. Extensive experiments have been carried out toverify the effectiveness of the proposed DFS model on the Drive\&amp;Act dataset.The results demonstrate that DFS achieves good performance and improves theefficiency of multi-modality driver action recognition.</description><author>Dan Lin, Philip Hann Yung Lee, Yiming Li, Ruoyu Wang, Kim-Hui Yap, Bingbing Li, You Shing Ngim</author><pubDate>Fri, 26 Jan 2024 13:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14838v1</guid></item><item><title>SOAR: Scene-debiasing Open-set Action Recognition</title><link>http://arxiv.org/abs/2309.01265v1</link><description>Deep learning models have a risk of utilizing spurious clues to makepredictions, such as recognizing actions based on the background scene. Thisissue can severely degrade the open-set action recognition performance when thetesting samples have different scene distributions from the training samples.To mitigate this problem, we propose a novel method, called Scene-debiasingOpen-set Action Recognition (SOAR), which features an adversarial scenereconstruction module and an adaptive adversarial scene classification module.The former prevents the decoder from reconstructing the video background givenvideo features, and thus helps reduce the background information in featurelearning. The latter aims to confuse scene type classification given videofeatures, with a specific emphasis on the action foreground, and helps to learnscene-invariant information. In addition, we design an experiment to quantifythe scene bias. The results indicate that the current open-set actionrecognizers are biased toward the scene, and our proposed SOAR method bettermitigates such bias. Furthermore, our extensive experiments demonstrate thatour method outperforms state-of-the-art methods, and the ablation studiesconfirm the effectiveness of our proposed modules.</description><author>Yuanhao Zhai, Ziyi Liu, Zhenyu Wu, Yi Wu, Chunluan Zhou, David Doermann, Junsong Yuan, Gang Hua</author><pubDate>Sun, 03 Sep 2023 21:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01265v1</guid></item><item><title>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</title><link>http://arxiv.org/abs/2306.11046v1</link><description>Existing skeleton-based action recognition methods typically follow acentralized learning paradigm, which can pose privacy concerns when exposinghuman-related videos. Federated Learning (FL) has attracted much attention dueto its outstanding advantages in privacy-preserving. However, directly applyingFL approaches to skeleton videos suffers from unstable training. In this paper,we investigate and discover that the heterogeneous human topology graphstructure is the crucial factor hindering training stability. To address thislimitation, we pioneer a novel Federated Skeleton-based Action Recognition(FSAR) paradigm, which enables the construction of a globally generalized modelwithout accessing local sensitive data. Specifically, we introduce an AdaptiveTopology Structure (ATS), separating generalization and personalization bylearning a domain-invariant topology shared across clients and adomain-specific topology decoupled from global model aggregation.Furthermore,we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancybetween clients and server caused by distinct updating patterns throughaligning shallow block-wise motion features. Extensive experiments on multipledatasets demonstrate that FSAR outperforms state-of-the-art FL-based methodswhile inherently protecting privacy.</description><author>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si</author><pubDate>Mon, 19 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11046v1</guid></item><item><title>SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2309.05834v1</link><description>Contrastive learning has achieved great success in skeleton-based actionrecognition. However, most existing approaches encode the skeleton sequences asentangled spatiotemporal representations and confine the contrasts to the samelevel of representation. Instead, this paper introduces a novel contrastivelearning framework, namely Spatiotemporal Clues Disentanglement Network(SCD-Net). Specifically, we integrate the decoupling module with a featureextractor to derive explicit clues from spatial and temporal domainsrespectively. As for the training of SCD-Net, with a constructed global anchor,we encourage the interaction between the anchor and extracted clues. Further,we propose a new masking strategy with structural constraints to strengthen thecontextual associations, leveraging the latest development from masked imagemodelling into the proposed SCD-Net. We conduct extensive evaluations on theNTU-RGB+D (60&amp;120) and PKU-MMD (I&amp;II) datasets, covering various downstreamtasks such as action recognition, action retrieval, transfer learning, andsemi-supervised learning. The experimental results demonstrate theeffectiveness of our method, which outperforms the existing state-of-the-art(SOTA) approaches significantly.</description><author>Cong Wu, Xiao-Jun Wu, Josef Kittler, Tianyang Xu, Sara Atito, Muhammad Awais, Zhenhua Feng</author><pubDate>Mon, 11 Sep 2023 22:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05834v1</guid></item><item><title>DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition</title><link>http://arxiv.org/abs/2308.12501v1</link><description>Graph Convolutional Networks (GCNs) have been widely used in skeleton-basedhuman action recognition. In GCN-based methods, the spatio-temporal graph isfundamental for capturing motion patterns. However, existing approaches ignorethe physical dependency and synchronized spatio-temporal correlations betweenjoints, which limits the representation capability of GCNs. To solve theseproblems, we construct the directed diffusion graph for action modeling andintroduce the activity partition strategy to optimize the weight sharingmechanism of graph convolution kernels. In addition, we present thespatio-temporal synchronization encoder to embed synchronized spatio-temporalsemantics. Finally, we propose Directed Diffusion Graph Convolutional Network(DD-GCN) for action recognition, and the experiments on three public datasets:NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-artperformance of our method.</description><author>Chang Li, Qian Huang, Yingchi Mao</author><pubDate>Thu, 24 Aug 2023 02:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12501v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v1</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Wed, 30 Aug 2023 14:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v2</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Mon, 04 Sep 2023 08:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v2</guid></item><item><title>SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network</title><link>http://arxiv.org/abs/2306.17574v1</link><description>Recent advancements in technology have expanded the possibilities of humanaction recognition by leveraging 3D data, which offers a richer representationof actions through the inclusion of depth information, enabling more accurateanalysis of spatial and temporal characteristics. However, 3D human actionrecognition is a challenging task due to the irregularity and Disarrangement ofthe data points in action sequences. In this context, we present our novelmodel for human action recognition from fixed topology mesh sequences based onSpiral Auto-encoder and Transformer Network, namely SpATr. The proposed methodfirst disentangles space and time in the mesh sequences. Then, an auto-encoderis utilized to extract spatial geometrical features, and tiny transformer isused to capture the temporal evolution of the sequence. Previous methods eitheruse 2D depth images, sample skeletons points or they require a huge amount ofmemory leading to the ability to process short sequences only. In this work, weshow competitive recognition rate and high memory efficiency by building ourauto-encoder based on spiral convolutions, which are light weight convolutiondirectly applied to mesh data with fixed topologies, and by modeling temporalevolution using a attention, that can handle large sequences. The proposedmethod is evaluated on on two 3D human action datasets: MoVi and BMLrub fromthe Archive of Motion Capture As Surface Shapes (AMASS). The results analysisshows the effectiveness of our method in 3D human action recognition whilemaintaining high memory efficiency. The code will soon be made publiclyavailable.</description><author>Hamza Bouzid, Lahoucine Ballihi</author><pubDate>Fri, 30 Jun 2023 12:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17574v1</guid></item><item><title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</title><link>http://arxiv.org/abs/2211.13466v3</link><description>Contrastive learning has been proven beneficial for self-supervisedskeleton-based action recognition. Most contrastive learning methods utilizecarefully designed augmentations to generate different movement patterns ofskeletons for the same semantics. However, it is still a pending issue to applystrong augmentations, which distort the images/skeletons' structures and causesemantic loss, due to their resulting unstable training. In this paper, weinvestigate the potential of adopting strong augmentations and propose ageneral hierarchical consistent contrastive learning framework (HiCLR) forskeleton-based action recognition. Specifically, we first design a gradualgrowing augmentation policy to generate multiple ordered positive pairs, whichguide to achieve the consistency of the learned representation from differentviews. Then, an asymmetric loss is proposed to enforce the hierarchicalconsistency via a directional clustering operation in the feature space,pulling the representations from strongly augmented views closer to those fromweakly augmented views for better generalizability. Meanwhile, we propose andevaluate three kinds of strong augmentations for 3D skeletons to demonstratethe effectiveness of our method. Extensive experiments show that HiCLRoutperforms the state-of-the-art methods notably on three large-scale datasets,i.e., NTU60, NTU120, and PKUMMD.</description><author>Jiahang Zhang, Lilang Lin, Jiaying Liu</author><pubDate>Mon, 10 Jul 2023 11:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13466v3</guid></item><item><title>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</title><link>http://arxiv.org/abs/2308.07571v1</link><description>This paper presents Ske2Grid, a new representation learning framework forimproved skeleton-based action recognition. In Ske2Grid, we define a regularconvolution operation upon a novel grid representation of human skeleton, whichis a compact image-like grid patch constructed and learned through three noveldesigns. Specifically, we propose a graph-node index transform (GIT) toconstruct a regular grid patch through assigning the nodes in the skeletongraph one by one to the desired grid cells. To ensure that GIT is a bijectionand enrich the expressiveness of the grid representation, an up-samplingtransform (UPT) is learned to interpolate the skeleton graph nodes for fillingthe grid patch to the full. To resolve the problem when the one-step UPT isaggressive and further exploit the representation capability of the grid patchwith increasing spatial size, a progressive learning strategy (PLS) is proposedwhich decouples the UPT into multiple steps and aligns them to multiple pairedGITs through a compact cascaded design learned progressively. We constructnetworks upon prevailing graph convolution networks and conduct experiments onsix mainstream skeleton-based action recognition datasets. Experiments showthat our Ske2Grid significantly outperforms existing GCN-based solutions underdifferent benchmark settings, without bells and whistles. Code and models areavailable at https://github.com/OSVAI/Ske2Grid</description><author>Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</author><pubDate>Tue, 15 Aug 2023 05:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07571v1</guid></item><item><title>Taylor Videos for Action Recognition</title><link>http://arxiv.org/abs/2402.03019v3</link><description>Effectively extracting motions from video is a critical and long-standingproblem for action recognition. This problem is very challenging becausemotions (i) do not have an explicit form, (ii) have various concepts such asdisplacement, velocity, and acceleration, and (iii) often contain noise causedby unstable pixels. Addressing these challenges, we propose the Taylor video, anew video format that highlights the dominate motions (e.g., a waving hand) ineach of its frames named the Taylor frame. Taylor video is named after Taylorseries, which approximates a function at a given point using important terms.In the scenario of videos, we define an implicit motion-extraction functionwhich aims to extract motions from video temporal block. In this block, usingthe frames, the difference frames, and higher-order difference frames, weperform Taylor expansion to approximate this function at the starting frame. Weshow the summation of the higher-order terms in the Taylor series gives usdominant motion patterns, where static objects, small and unstable motions areremoved. Experimentally we show that Taylor videos are effective inputs topopular architectures including 2D CNNs, 3D CNNs, and transformers. When usedindividually, Taylor videos yield competitive action recognition accuracycompared to RGB videos and optical flow. When fused with RGB or optical flowvideos, further accuracy improvement is achieved.</description><author>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng</author><pubDate>Thu, 08 Feb 2024 02:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03019v3</guid></item><item><title>Taylor Videos for Action Recognition</title><link>http://arxiv.org/abs/2402.03019v2</link><description>Effectively extracting motions from video is a critical and long-standingproblem for action recognition. This problem is very challenging becausemotions (i) do not have an explicit form, (ii) have various concepts such asdisplacement, velocity, and acceleration, and (iii) often contain noise causedby unstable pixels. Addressing these challenges, we propose the Taylor video, anew video format that highlights the dominate motions (e.g., a waving hand) ineach of its frames named the Taylor frame. Taylor video is named after Taylorseries, which approximates a function at a given point using important terms.In the scenario of videos, we define an implicit motion-extraction functionwhich aims to extract motions from video temporal block. In this block, usingthe frames, the difference frames, and higher-order difference frames, weperform Taylor expansion to approximate this function at the starting frame. Weshow the summation of the higher-order terms in the Taylor series gives usdominant motion patterns, where static objects, small and unstable motions areremoved. Experimentally we show that Taylor videos are effective inputs topopular architectures including 2D CNNs, 3D CNNs, and transformers. When usedindividually, Taylor videos yield competitive action recognition accuracycompared to RGB videos and optical flow. When fused with RGB or optical flowvideos, further accuracy improvement is achieved.</description><author>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng</author><pubDate>Wed, 07 Feb 2024 05:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03019v2</guid></item><item><title>Taylor Videos for Action Recognition</title><link>http://arxiv.org/abs/2402.03019v1</link><description>Effectively extracting motions from video is a critical and long-standingproblem for action recognition. This problem is very challenging becausemotions (i) do not have an explicit form, (ii) have various concepts such asdisplacement, velocity, and acceleration, and (iii) often contain noise causedby unstable pixels. Addressing these challenges, we propose the Taylor video, anew video format that highlights the dominate motions (e.g., a waving hand) ineach of its frames named the Taylor frame. Taylor video is named after Taylorseries, which approximates a function at a given point using important terms.In the scenario of videos, we define an implicit motion-extraction functionwhich aims to extract motions from video temporal block. In this block, usingthe frames, the difference frames, and higher-order difference frames, weperform Taylor expansion to approximate this function at the starting frame. Weshow the summation of the higher-order terms in the Taylor series gives usdominant motion patterns, where static objects, small and unstable motions areremoved. Experimentally we show that Taylor videos are effective inputs topopular architectures including 2D CNNs, 3D CNNs, and transformers. When usedindividually, Taylor videos yield competitive action recognition accuracycompared to RGB videos and optical flow. When fused with RGB or optical flowvideos, further accuracy improvement is achieved.</description><author>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng</author><pubDate>Mon, 05 Feb 2024 14:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03019v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>SkateboardAI: The Coolest Video Action Recognition for Skateboarding</title><link>http://arxiv.org/abs/2311.11467v2</link><description>Impressed by the coolest skateboarding sports program from 2021 Tokyo OlympicGames, we are the first to curate the original real-world video datasets"SkateboardAI" in the wild, even self-design and implement diverse uni-modaland multi-modal video action recognition approaches to recognize differenttricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)Transformer-based action recognition pipeline. Transferred to the multi-modalconditions, we investigated the two-stream Inflated-3D architecture on"SkateboardAI" datasets to compare its performance with uni-modal cases. Insum, our objective is developing an excellent AI sport referee for the coolestskateboarding competitions.</description><author>Hanxiao Chen</author><pubDate>Wed, 03 Jan 2024 12:09:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11467v2</guid></item><item><title>Selective Volume Mixup for Video Action Recognition</title><link>http://arxiv.org/abs/2309.09534v1</link><description>The recent advances in Convolutional Neural Networks (CNNs) and VisionTransformers have convincingly demonstrated high learning capability for videoaction recognition on large datasets. Nevertheless, deep models often sufferfrom the overfitting effect on small-scale datasets with a limited number oftraining videos. A common solution is to exploit the existing imageaugmentation strategies for each frame individually including Mixup, Cutmix,and RandAugment, which are not particularly optimized for video data. In thispaper, we propose a novel video augmentation strategy named Selective VolumeMixup (SV-Mix) to improve the generalization ability of deep models withlimited training videos. SV-Mix devises a learnable selective module to choosethe most informative volumes from two videos and mixes the volumes up toachieve a new training video. Technically, we propose two new modules, i.e., aspatial selective module to select the local patches for each spatial position,and a temporal selective module to mix the entire frames for each timestamp andmaintain the spatial pattern. At each time, we randomly choose one of the twomodules to expand the diversity of training samples. The selective modules arejointly optimized with the video action recognition framework to find theoptimal augmentation strategy. We empirically demonstrate the merits of theSV-Mix augmentation on a wide range of video action recognition benchmarks andconsistently boot the performances of both CNN-based and transformer-basedmodels.</description><author>Yi Tan, Zhaofan Qiu, Yanbin Hao, Ting Yao, Xiangnan He, Tao Mei</author><pubDate>Mon, 18 Sep 2023 08:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09534v1</guid></item><item><title>FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition</title><link>http://arxiv.org/abs/2305.18479v1</link><description>3D Convolutional Neural Networks are gaining increasing attention fromresearchers and practitioners and have found applications in many domains, suchas surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval. However, their widespread adoption is hindered by their highcomputational and memory requirements, especially when resource-constrainedsystems are targeted. This paper addresses the problem of mapping X3D, astate-of-the-art model in Human Action Recognition that achieves accuracy of95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflowgenerates an optimised stream-based hardware system, taking into account theavailable resources and off-chip memory characteristics of the FPGA device. Thegenerated designs push further the current performance-accuracy pareto front,and enable for the first time the targeting of such complex model architecturesfor the Human Action Recognition task.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18479v1</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition</title><link>http://arxiv.org/abs/2310.17421v1</link><description>Human action recognition from skeletal data is an important and active areaof research in which the state of the art has not yet achieved near-perfectaccuracy on many well-known datasets. In this paper, we introduce theDistribution of Action Movements Descriptor, a novel action descriptor based onthe distribution of the directions of the motions of the joints between frames,over the set of all possible motions in the dataset. The descriptor is computedas a normalized histogram over a set of representative directions of thejoints, which are in turn obtained via clustering. While the descriptor isglobal in the sense that it represents the overall distribution of movementdirections of an action, it is able to partially retain its temporal structureby applying a windowing scheme. The descriptor, together with a standard classifier, outperforms severalstate-of-the-art techniques on many well-known datasets.</description><author>Facundo Manuel Quiroga, Franco Ronchetti, Laura Lanzarini, Cesar Eestrebou</author><pubDate>Thu, 26 Oct 2023 15:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17421v1</guid></item><item><title>TransNet: A Transfer Learning-Based Network for Human Action Recognition</title><link>http://arxiv.org/abs/2309.06951v1</link><description>Human action recognition (HAR) is a high-level and significant research areain computer vision due to its ubiquitous applications. The main limitations ofthe current HAR models are their complex structures and lengthy training time.In this paper, we propose a simple yet versatile and effective end-to-end deeplearning architecture, coined as TransNet, for HAR. TransNet decomposes thecomplex 3D-CNNs into 2D- and 1D-CNNs, where the 2D- and 1D-CNN componentsextract spatial features and temporal patterns in videos, respectively.Benefiting from its concise architecture, TransNet is ideally compatible withany pretrained state-of-the-art 2D-CNN models in other fields, beingtransferred to serve the HAR task. In other words, it naturally leverages thepower and success of transfer learning for HAR, bringing huge advantages interms of efficiency and effectiveness. Extensive experimental results and thecomparison with the state-of-the-art models demonstrate the superiorperformance of the proposed TransNet in HAR in terms of flexibility, modelcomplexity, training speed and classification accuracy.</description><author>K. Alomar, X. Cai</author><pubDate>Wed, 13 Sep 2023 14:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06951v1</guid></item><item><title>Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization</title><link>http://arxiv.org/abs/2304.08799v3</link><description>3D Skeleton-based human action recognition has attracted increasing attentionin recent years. Most of the existing work focuses on supervised learning whichrequires a large number of labeled action sequences that are often expensiveand time-consuming to annotate. In this paper, we address self-supervised 3Daction representation learning for skeleton-based action recognition. Weinvestigate self-supervised representation learning and design a novel skeletoncloud colorization technique that is capable of learning spatial and temporalskeleton representations from unlabeled skeleton sequence data. We represent askeleton action sequence as a 3D skeleton cloud and colorize each point in thecloud according to its temporal and spatial orders in the original(unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud,we design an auto-encoder framework that can learn spatial-temporal featuresfrom the artificial color labels of skeleton joints effectively. Specifically,we design a two-steam pretraining network that leverages fine-grained andcoarse-grained colorization to learn multi-scale spatial-temporal features. Inaddition, we design a Masked Skeleton Cloud Repainting task that can pretrainthe designed auto-encoder framework to learn informative representations. Weevaluate our skeleton cloud colorization approach with linear classifierstrained under different configurations, including unsupervised,semi-supervised, fully-supervised, and transfer learning settings. Extensiveexperiments on NTU RGB+D, NTU RGB+D 120, PKU-MMD, NW-UCLA, and UWA3D datasetsshow that the proposed method outperforms existing unsupervised andsemi-supervised 3D action recognition methods by large margins and achievescompetitive performance in supervised 3D action recognition as well.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Yongjian Hu, Alex C. Kot</author><pubDate>Mon, 16 Oct 2023 09:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08799v3</guid></item><item><title>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</title><link>http://arxiv.org/abs/2308.03741v1</link><description>In line with the human capacity to perceive the world by simultaneouslyprocessing and integrating high-dimensional inputs from multiple modalitieslike vision and audio, we propose a novel model, MAiVAR-T (MultimodalAudio-Image to Video Action Recognition Transformer). This model employs anintuitive approach for the combination of audio-image and video modalities,with a primary aim to escalate the effectiveness of multimodal human actionrecognition (MHAR). At the core of MAiVAR-T lies the significance of distillingsubstantial representations from the audio modality and transmuting these intothe image domain. Subsequently, this audio-image depiction is fused with thevideo modality to formulate a unified representation. This concerted approachstrives to exploit the contextual richness inherent in both audio and videomodalities, thereby promoting action recognition. In contrast to existingstate-of-the-art strategies that focus solely on audio or video modalities,MAiVAR-T demonstrates superior performance. Our extensive empirical evaluationsconducted on a benchmark action recognition dataset corroborate the model'sremarkable performance. This underscores the potential enhancements derivedfrom integrating audio and video modalities for action recognition purposes.</description><author>Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</author><pubDate>Tue, 01 Aug 2023 12:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03741v1</guid></item><item><title>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</title><link>http://arxiv.org/abs/2309.03989v1</link><description>Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. %a schedule that helpswith this transition. We evaluate our method on several challenging benchmarkdatasets and demonstrate that our approach outperforms existing cross-domainfew-shot learning techniques. Our code is available at\hyperlink{https://github.com/Sarinda251/CDFSL-V}{https://github.com/Sarinda251/CDFSL-V}</description><author>Sarinda Samarasinghe, Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah</author><pubDate>Thu, 07 Sep 2023 20:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03989v1</guid></item><item><title>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</title><link>http://arxiv.org/abs/2309.03989v2</link><description>Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. We evaluate our methodon several challenging benchmark datasets and demonstrate that our approachoutperforms existing cross-domain few-shot learning techniques. Our code isavailable at https://github.com/Sarinda251/CDFSL-V</description><author>Sarinda Samarasinghe, Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah</author><pubDate>Fri, 15 Sep 2023 18:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03989v2</guid></item><item><title>ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition</title><link>http://arxiv.org/abs/2401.11654v1</link><description>Zero-shot action recognition (ZSAR) aims to learn an alignment model betweenvideos and class descriptions of seen actions that is transferable to unseenactions. The text queries (class descriptions) used in existing ZSAR works,however, are often short action names that fail to capture the rich semanticsin the videos, leading to misalignment. With the intuition that video contentdescriptions (e.g., video captions) can provide rich contextual information ofvisual concepts in videos, we propose to utilize human annotated videodescriptions to enrich the semantics of the class descriptions of each action.However, all existing action video description datasets are limited in terms ofthe number of actions, the semantics of video descriptions, etc. To this end,we collect a large-scale action video descriptions dataset named ActionHub,which covers a total of 1,211 common actions and provides 3.6 million actionvideo descriptions. With the proposed ActionHub dataset, we further propose anovel Cross-modality and Cross-action Modeling (CoCo) framework for ZSAR, whichconsists of a Dual Cross-modality Alignment module and a Cross-actionInvariance Mining module. Specifically, the Dual Cross-modality Alignmentmodule utilizes both action labels and video descriptions from ActionHub toobtain rich class semantic features for feature alignment. The Cross-actionInvariance Mining module exploits a cycle-reconstruction process between theclass semantic feature spaces of seen actions and unseen actions, aiming toguide the model to learn cross-action invariant representations. Extensiveexperimental results demonstrate that our CoCo framework significantlyoutperforms the state-of-the-art on three popular ZSAR benchmarks (i.e.,Kinetics-ZSAR, UCF101 and HMDB51) under two different learning protocols inZSAR. We will release our code, models, and the proposed ActionHub dataset.</description><author>Jiaming Zhou, Junwei Liang, Kun-Yu Lin, Jinrui Yang, Wei-Shi Zheng</author><pubDate>Mon, 22 Jan 2024 02:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11654v1</guid></item><item><title>Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2402.02210v1</link><description>Skeleton-based action recognition has attracted much attention, benefitingfrom its succinctness and robustness. However, the minimal inter-classvariation in similar action sequences often leads to confusion. The inherentspatiotemporal coupling characteristics make it challenging to mine the subtledifferences in joint motion trajectories, which is critical for distinguishingconfusing fine-grained actions. To alleviate this problem, we propose aWavelet-Attention Decoupling (WAD) module that utilizes discrete wavelettransform to effectively disentangle salient and subtle motion features in thetime-frequency domain. Then, the decoupling attention adaptively recalibratestheir temporal responses. To further amplify the discrepancies in these subtlemotion features, we propose a Fine-grained Contrastive Enhancement (FCE) moduleto enhance attention towards trajectory features by contrastive learning.Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D andthe fine-grained dataset FineGYM. Our methods perform competitively compared tostate-of-the-art methods and can discriminate confusing fine-grained actionswell.</description><author>Haochen Chang, Jing Chen, Yilin Li, Jixiang Chen, Xiaofeng Zhang</author><pubDate>Sat, 03 Feb 2024 16:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02210v1</guid></item><item><title>Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition</title><link>http://arxiv.org/abs/2312.15144v3</link><description>Skeleton-based action recognition is a central task in human-computerinteraction. However, most previous methods suffer from two issues: (i)semantic ambiguity arising from spatial-temporal information mixture; and (ii)overlooking the explicit exploitation of the latent data distributions (i.e.,the intra-class variations and inter-class relations), thereby leading tosub-optimum solutions of the skeleton encoders. To mitigate this, we propose aspatial-temporal decoupling contrastive learning (STD-CL) framework to obtaindiscriminative and semantically distinct representations from the sequences,which can be incorporated into various previous skeleton encoders and can beremoved when testing. Specifically, we decouple the global features intospatial-specific and temporal-specific features to reduce the spatial-temporalcoupling of features. Furthermore, to explicitly exploit the latent datadistributions, we employ the attentive features to contrastive learning, whichmodels the cross-sequence semantic relations by pulling together the featuresfrom the positive pairs and pushing away the negative pairs. Extensiveexperiments show that STD-CL with four various skeleton encoders (HCN, 2S-AGCN,CTR-GCN, and Hyperformer) achieves solid improvements on NTU60, NTU120, andNW-UCLA benchmarks. The code will be released soon.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang</author><pubDate>Thu, 18 Jan 2024 14:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15144v3</guid></item><item><title>Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition</title><link>http://arxiv.org/abs/2312.15144v2</link><description>Skeleton-based action recognition is a central task of human-computerinteraction. However, most of the previous methods suffer from two issues: (i)semantic ambiguity arising from spatiotemporal information mixture; and (ii)overlooking the explicit exploitation of the latent data distributions (i.e.,the intra-class variations and inter-class relations), thereby leading to localoptimum solutions of the skeleton encoders. To mitigate this, we propose aspatial-temporal decoupling contrastive learning (STD-CL) framework to obtaindiscriminative and semantically distinct representations from the sequences,which can be incorporated into almost all previous skeleton encoders and haveno impact on the skeleton encoders when testing. Specifically, we decouple theglobal features into spatial-specific and temporal-specific features to reducethe spatiotemporal coupling of features. Furthermore, to explicitly exploit thelatent data distributions, we employ the attentive features to contrastivelearning, which models the cross-sequence semantic relations by pullingtogether the features from the positive pairs and pushing away the negativepairs. Extensive experiments show that STD-CL with four various skeletonencoders (HCN, 2S-AGCN, CTR-GCN, and Hyperformer) achieves solid improvement onNTU60, NTU120, and NW-UCLA benchmarks. The code will be released.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang</author><pubDate>Tue, 09 Jan 2024 08:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15144v2</guid></item><item><title>CAST: Cross-Attention in Space and Time for Video Action Recognition</title><link>http://arxiv.org/abs/2311.18825v1</link><description>Recognizing human actions in videos requires spatial and temporalunderstanding. Most existing action recognition models lack a balancedspatio-temporal understanding of videos. In this work, we propose a noveltwo-stream architecture, called Cross-Attention in Space and Time (CAST), thatachieves a balanced spatio-temporal understanding of videos using only RGBinput. Our proposed bottleneck cross-attention mechanism enables the spatialand temporal expert models to exchange information and make synergisticpredictions, leading to improved performance. We validate the proposed methodwith extensive experiments on public benchmarks with different characteristics:EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our methodconsistently shows favorable performance across these datasets, while theperformance of existing methods fluctuates depending on the datasetcharacteristics.</description><author>Dongho Lee, Jongseo Lee, Jinwoo Choi</author><pubDate>Thu, 30 Nov 2023 18:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18825v1</guid></item><item><title>Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer</title><link>http://arxiv.org/abs/2302.09187v3</link><description>Recognizing human actions in video sequences, known as Human ActionRecognition (HAR), is a challenging task in pattern recognition. WhileConvolutional Neural Networks (ConvNets) have shown remarkable success in imagerecognition, they are not always directly applicable to HAR, as temporalfeatures are critical for accurate classification. In this paper, we propose anovel dynamic PSO-ConvNet model for learning actions in videos, building on ourrecent work in image recognition. Our approach leverages a framework where theweight vector of each neural network represents the position of a particle inphase space, and particles share their current weight vectors and gradientestimates of the Loss function. To extend our approach to video, we integrateConvNets with state-of-the-art temporal methods such as Transformer andRecurrent Neural Networks. Our experimental results on the UCF-101 datasetdemonstrate substantial improvements of up to 9% in accuracy, which confirmsthe effectiveness of our proposed method. In addition, we conducted experimentson larger and more variety of datasets including Kinetics-400 and HMDB-51 andobtained preference for Collaborative Learning in comparison withNon-Collaborative Learning (Individual Learning). Overall, our dynamicPSO-ConvNet model provides a promising direction for improving HAR by bettercapturing the spatio-temporal dynamics of human actions in videos. The code isavailable athttps://github.com/leonlha/Video-Action-Recognition-Collaborative-Learning-with-Dynamics-via-PSO-ConvNet-Transformer.</description><author>Nguyen Huu Phong, Bernardete Ribeiro</author><pubDate>Thu, 21 Sep 2023 09:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09187v3</guid></item><item><title>MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition</title><link>http://arxiv.org/abs/2303.02575v2</link><description>We present a novel approach for action recognition in UAV videos. Ourformulation is designed to handle occlusion and viewpoint changes caused by themovement of a UAV. We use the concept of mutual information to compute andalign the regions corresponding to human action or motion in the temporaldomain. This enables our recognition model to learn from the key featuresassociated with the motion. We also propose a novel frame sampling method thatuses joint mutual information to acquire the most informative frame sequence inUAV videos. We have integrated our approach with X3D and evaluated theperformance on multiple datasets. In practice, we achieve 18.9% improvement inTop-1 accuracy over current state-of-the-art methods on UAV-Human(Li et al.,2021), 7.3% improvement on Drone-Action(Perera et al., 2019), and 7.16%improvement on NEC Drones(Choi et al., 2020).</description><author>Ruiqi Xian, Xijun Wang, Dinesh Manocha</author><pubDate>Wed, 15 Nov 2023 23:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02575v2</guid></item></channel></rss>