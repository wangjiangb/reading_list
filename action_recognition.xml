<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 14 Aug 2023 06:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>M$^3$Net: Multi-view Encoding, Matching, and Fusion for Few-shot Fine-grained Action Recognition</title><link>http://arxiv.org/abs/2308.03063v1</link><description>Due to the scarcity of manually annotated data required for fine-grainedvideo understanding, few-shot fine-grained (FS-FG) action recognition hasgained significant attention, with the aim of classifying novel fine-grainedaction categories with only a few labeled instances. Despite the progress madein FS coarse-grained action recognition, current approaches encounter twochallenges when dealing with the fine-grained action categories: the inabilityto capture subtle action details and the insufficiency of learning from limiteddata that exhibit high intra-class variance and inter-class similarity. Toaddress these limitations, we propose M$^3$Net, a matching-based framework forFS-FG action recognition, which incorporates \textit{multi-view encoding},\textit{multi-view matching}, and \textit{multi-view fusion} to facilitateembedding encoding, similarity matching, and decision making across multipleviewpoints. \textit{Multi-view encoding} captures rich contextual details fromthe intra-frame, intra-video, and intra-episode perspectives, generatingcustomized higher-order embeddings for fine-grained data. \textit{Multi-viewmatching} integrates various matching functions enabling flexible relationmodeling within limited samples to handle multi-scale spatio-temporalvariations by leveraging the instance-specific, category-specific, andtask-specific perspectives. \textit{Multi-view fusion} consists ofmatching-predictions fusion and matching-losses fusion over the above views,where the former promotes mutual complementarity and the latter enhancesembedding generalizability by employing multi-task collaborative learning.Explainable visualizations and experimental results on three challengingbenchmarks demonstrate the superiority of M$^3$Net in capturing fine-grainedaction details and achieving state-of-the-art performance for FS-FG actionrecognition.</description><author>Hao Tang, Jun Liu, Shuanglin Yan, Rui Yan, Zechao Li, Jinhui Tang</author><pubDate>Sun, 06 Aug 2023 10:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03063v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v1</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 29 May 2023 09:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v1</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Improving Zero-Shot Action Recognition using Human Instruction with Text Description</title><link>http://arxiv.org/abs/2301.08874v2</link><description>Zero-shot action recognition, which recognizes actions in videos withouthaving received any training examples, is gaining wide attention considering itcan save labor costs and training time. Nevertheless, the performance ofzero-shot learning is still unsatisfactory, which limits its practicalapplication. To solve this problem, this study proposes a framework to improvezero-shot action recognition using human instructions with text descriptions.The proposed framework manually describes video contents, which incurs somelabor costs; in many situations, the labor costs are worth it. We manuallyannotate text features for each action, which can be a word, phrase, orsentence. Then by computing the matching degrees between the video and all textfeatures, we can predict the class of the video. Furthermore, the proposedmodel can also be combined with other models to improve its accuracy. Inaddition, our model can be continuously optimized to improve the accuracy byrepeating human instructions. The results with UCF101 and HMDB51 showed thatour model achieved the best accuracy and improved the accuracies of othermodels.</description><author>Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 12 Jun 2023 09:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08874v2</guid></item><item><title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2212.04761v2</link><description>Skeleton-based action recognition has attracted considerable attention due toits compact representation of the human body's skeletal sructure. Many recentmethods have achieved remarkable performance using graph convolutional networks(GCNs) and convolutional neural networks (CNNs), which extract spatial andtemporal features, respectively. Although spatial and temporal dependencies inthe human skeleton have been explored separately, spatio-temporal dependency israrely considered. In this paper, we propose the Spatio-Temporal Curve Network(STC-Net) to effectively leverage the spatio-temporal dependency of the humanskeleton. Our proposed network consists of two novel elements: 1) TheSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for GraphConvolution (DK-GC). The STC module dynamically adjusts the receptive field byidentifying meaningful node connections between every adjacent frame andgenerating spatio-temporal curves based on the identified node connections,providing an adaptive spatio-temporal coverage. In addition, we propose DK-GCto consider long-range dependencies, which results in a large receptive fieldwithout any additional parameters by applying an extended kernel to the givenadjacency matrices of the graph. Our STC-Net combines these two modules andachieves state-of-the-art performance on four skeleton-based action recognitionbenchmarks.</description><author>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 03:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04761v2</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</title><link>http://arxiv.org/abs/2308.03950v1</link><description>Zero-shot skeleton-based action recognition aims to recognize actions ofunseen categories after training on data of seen categories. The key is tobuild the connection between visual and semantic space from seen to unseenclasses. Previous studies have primarily focused on encoding sequences into asingular feature vector, with subsequent mapping the features to an identicalanchor point within the embedded space. Their performance is hindered by 1) theignorance of the global visual/semantic distribution alignment, which resultsin a limitation to capture the true interdependence between the two spaces. 2)the negligence of temporal information since the frame-wise features with richaction clues are directly pooled into a single feature vector. We propose a newzero-shot skeleton-based action recognition method via mutual information (MI)estimation and maximization. Specifically, 1) we maximize the MI between visualand semantic space for distribution alignment; 2) we leverage the temporalinformation for estimating the MI by encouraging MI to increase as more framesare observed. Extensive experiments on three large-scale skeleton actiondatasets confirm the effectiveness of our method. Code:https://github.com/YujieOuO/SMIE.</description><author>Yujie Zhou, Wenwen Qiang, Anyi Rao, Ning Lin, Bing Su, Jiaqi Wang</author><pubDate>Tue, 08 Aug 2023 00:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03950v1</guid></item><item><title>Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2302.13434v2</link><description>Recently, skeleton-based human action has become a hot research topic becausethe compact representation of human skeletons brings new blood to this researchdomain. As a result, researchers began to notice the importance of using RGB orother sensors to analyze human action by extracting skeleton information.Leveraging the rapid development of deep learning (DL), a significant number ofskeleton-based human action approaches have been presented with fine-designedDL structures recently. However, a well-trained DL model always demandshigh-quality and sufficient data, which is hard to obtain without costing highexpenses and human labor. In this paper, we introduce a novel data augmentationmethod for skeleton-based action recognition tasks, which can effectivelygenerate high-quality and diverse sequential actions. In order to obtainnatural and realistic action sequences, we propose denoising diffusionprobabilistic models (DDPMs) that can generate a series of synthetic actionsequences, and their generation process is precisely guided by aspatial-temporal transformer (ST-Trans). Experimental results show that ourmethod outperforms the state-of-the-art (SOTA) motion generation approaches ondifferent naturality and diversity metrics. It proves that its high-qualitysynthetic data can also be effectively deployed to existing action recognitionmodels with significant performance improvement.</description><author>Yifan Jiang, Han Chen, Hanseok Ko</author><pubDate>Tue, 25 Jul 2023 03:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13434v2</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>Graph Contrastive Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2301.10900v2</link><description>In the field of skeleton-based action recognition, current top-performinggraph convolutional networks (GCNs) exploit intra-sequence context to constructadaptive graphs for feature aggregation. However, we argue that such context isstill \textit{local} since the rich cross-sequence relations have not beenexplicitly investigated. In this paper, we propose a graph contrastive learningframework for skeleton-based action recognition (\textit{SkeletonGCL}) toexplore the \textit{global} context across all sequences. In specific,SkeletonGCL associates graph learning across sequences by enforcing graphs tobe class-discriminative, \emph{i.e.,} intra-class compact and inter-classdispersed, which improves the GCN capacity to distinguish various actionpatterns. Besides, two memory banks are designed to enrich cross-sequencecontext from two complementary levels, \emph{i.e.,} instance and semanticlevels, enabling graph contrastive learning in multiple context scales.Consequently, SkeletonGCL establishes a new training paradigm, and it can beseamlessly incorporated into current GCNs. Without loss of generality, wecombine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), andachieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. Thesource code will be available at\url{https://github.com/OliverHxh/SkeletonGCL}.</description><author>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</author><pubDate>Sat, 10 Jun 2023 11:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10900v2</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</title><link>http://arxiv.org/abs/2307.09238v1</link><description>As collaborative robots (cobots) continue to gain popularity in industrialmanufacturing, effective human-robot collaboration becomes crucial. Cobotsshould be able to recognize human actions to assist with assembly tasks and actautonomously. To achieve this, skeleton-based approaches are often used due totheir ability to generalize across various people and environments. Althoughbody skeleton approaches are widely used for action recognition, they may notbe accurate enough for assembly actions where the worker's fingers and handsplay a significant role. To address this limitation, we propose a method inwhich less detailed body skeletons are combined with highly detailed handskeletons. We investigate CNNs and transformers, the latter of which areparticularly adept at extracting and combining important information from bothskeleton types using attention. This paper demonstrates the effectiveness ofour proposed approach in enhancing action recognition in assembly scenarios.</description><author>Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Tue, 18 Jul 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09238v1</guid></item><item><title>How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</title><link>http://arxiv.org/abs/2306.05844v1</link><description>As the use of collaborative robots (cobots) in industrial manufacturingcontinues to grow, human action recognition for effective human-robotcollaboration becomes increasingly important. This ability is crucial forcobots to act autonomously and assist in assembly tasks. Recently,skeleton-based approaches are often used as they tend to generalize better todifferent people and environments. However, when processing skeletons alone,information about the objects a human interacts with is lost. Therefore, wepresent a novel approach of integrating object information into skeleton-basedaction recognition. We enhance two state-of-the-art methods by treating objectcenters as further skeleton joints. Our experiments on the assembly datasetIKEA ASM show that our approach improves the performance of thesestate-of-the-art methods to a large extent when combining skeleton joints withobjects predicted by a state-of-the-art instance segmentation model. Ourresearch sheds light on the benefits of combining skeleton joints with objectinformation for human action recognition in assembly tasks. We analyze theeffect of the object detector on the combination for action classification anddiscuss the important factors that must be taken into account.</description><author>Dustin Aganian, Mona Köhler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Fri, 09 Jun 2023 13:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05844v1</guid></item><item><title>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</title><link>http://arxiv.org/abs/2306.11046v1</link><description>Existing skeleton-based action recognition methods typically follow acentralized learning paradigm, which can pose privacy concerns when exposinghuman-related videos. Federated Learning (FL) has attracted much attention dueto its outstanding advantages in privacy-preserving. However, directly applyingFL approaches to skeleton videos suffers from unstable training. In this paper,we investigate and discover that the heterogeneous human topology graphstructure is the crucial factor hindering training stability. To address thislimitation, we pioneer a novel Federated Skeleton-based Action Recognition(FSAR) paradigm, which enables the construction of a globally generalized modelwithout accessing local sensitive data. Specifically, we introduce an AdaptiveTopology Structure (ATS), separating generalization and personalization bylearning a domain-invariant topology shared across clients and adomain-specific topology decoupled from global model aggregation.Furthermore,we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancybetween clients and server caused by distinct updating patterns throughaligning shallow block-wise motion features. Extensive experiments on multipledatasets demonstrate that FSAR outperforms state-of-the-art FL-based methodswhile inherently protecting privacy.</description><author>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si</author><pubDate>Mon, 19 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11046v1</guid></item><item><title>SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network</title><link>http://arxiv.org/abs/2306.17574v1</link><description>Recent advancements in technology have expanded the possibilities of humanaction recognition by leveraging 3D data, which offers a richer representationof actions through the inclusion of depth information, enabling more accurateanalysis of spatial and temporal characteristics. However, 3D human actionrecognition is a challenging task due to the irregularity and Disarrangement ofthe data points in action sequences. In this context, we present our novelmodel for human action recognition from fixed topology mesh sequences based onSpiral Auto-encoder and Transformer Network, namely SpATr. The proposed methodfirst disentangles space and time in the mesh sequences. Then, an auto-encoderis utilized to extract spatial geometrical features, and tiny transformer isused to capture the temporal evolution of the sequence. Previous methods eitheruse 2D depth images, sample skeletons points or they require a huge amount ofmemory leading to the ability to process short sequences only. In this work, weshow competitive recognition rate and high memory efficiency by building ourauto-encoder based on spiral convolutions, which are light weight convolutiondirectly applied to mesh data with fixed topologies, and by modeling temporalevolution using a attention, that can handle large sequences. The proposedmethod is evaluated on on two 3D human action datasets: MoVi and BMLrub fromthe Archive of Motion Capture As Surface Shapes (AMASS). The results analysisshows the effectiveness of our method in 3D human action recognition whilemaintaining high memory efficiency. The code will soon be made publiclyavailable.</description><author>Hamza Bouzid, Lahoucine Ballihi</author><pubDate>Fri, 30 Jun 2023 12:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17574v1</guid></item><item><title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</title><link>http://arxiv.org/abs/2211.13466v3</link><description>Contrastive learning has been proven beneficial for self-supervisedskeleton-based action recognition. Most contrastive learning methods utilizecarefully designed augmentations to generate different movement patterns ofskeletons for the same semantics. However, it is still a pending issue to applystrong augmentations, which distort the images/skeletons' structures and causesemantic loss, due to their resulting unstable training. In this paper, weinvestigate the potential of adopting strong augmentations and propose ageneral hierarchical consistent contrastive learning framework (HiCLR) forskeleton-based action recognition. Specifically, we first design a gradualgrowing augmentation policy to generate multiple ordered positive pairs, whichguide to achieve the consistency of the learned representation from differentviews. Then, an asymmetric loss is proposed to enforce the hierarchicalconsistency via a directional clustering operation in the feature space,pulling the representations from strongly augmented views closer to those fromweakly augmented views for better generalizability. Meanwhile, we propose andevaluate three kinds of strong augmentations for 3D skeletons to demonstratethe effectiveness of our method. Extensive experiments show that HiCLRoutperforms the state-of-the-art methods notably on three large-scale datasets,i.e., NTU60, NTU120, and PKUMMD.</description><author>Jiahang Zhang, Lilang Lin, Jiaying Liu</author><pubDate>Mon, 10 Jul 2023 11:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13466v3</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition</title><link>http://arxiv.org/abs/2305.18479v1</link><description>3D Convolutional Neural Networks are gaining increasing attention fromresearchers and practitioners and have found applications in many domains, suchas surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval. However, their widespread adoption is hindered by their highcomputational and memory requirements, especially when resource-constrainedsystems are targeted. This paper addresses the problem of mapping X3D, astate-of-the-art model in Human Action Recognition that achieves accuracy of95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflowgenerates an optimised stream-based hardware system, taking into account theavailable resources and off-chip memory characteristics of the FPGA device. Thegenerated designs push further the current performance-accuracy pareto front,and enable for the first time the targeting of such complex model architecturesfor the Human Action Recognition task.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18479v1</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</title><link>http://arxiv.org/abs/2308.03741v1</link><description>In line with the human capacity to perceive the world by simultaneouslyprocessing and integrating high-dimensional inputs from multiple modalitieslike vision and audio, we propose a novel model, MAiVAR-T (MultimodalAudio-Image to Video Action Recognition Transformer). This model employs anintuitive approach for the combination of audio-image and video modalities,with a primary aim to escalate the effectiveness of multimodal human actionrecognition (MHAR). At the core of MAiVAR-T lies the significance of distillingsubstantial representations from the audio modality and transmuting these intothe image domain. Subsequently, this audio-image depiction is fused with thevideo modality to formulate a unified representation. This concerted approachstrives to exploit the contextual richness inherent in both audio and videomodalities, thereby promoting action recognition. In contrast to existingstate-of-the-art strategies that focus solely on audio or video modalities,MAiVAR-T demonstrates superior performance. Our extensive empirical evaluationsconducted on a benchmark action recognition dataset corroborate the model'sremarkable performance. This underscores the potential enhancements derivedfrom integrating audio and video modalities for action recognition purposes.</description><author>Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</author><pubDate>Tue, 01 Aug 2023 12:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03741v1</guid></item><item><title>Universal Prototype Transport for Zero-Shot Action Recognition and Localization</title><link>http://arxiv.org/abs/2203.03971v2</link><description>This work addresses the problem of recognizing action categories in videoswhen no training examples are available. The current state-of-the-art enablessuch a zero-shot recognition by learning universal mappings from videos to asemantic space, either trained on large-scale seen actions or on objects. Whileeffective, we find that universal action and object mappings are biased tospecific regions in the semantic space. These biases lead to a fundamentalproblem: many unseen action categories are simply never inferred duringtesting. For example on UCF-101, a quarter of the unseen actions are out ofreach with a state-of-the-art universal action model. To that end, this paperintroduces universal prototype transport for zero-shot action recognition. Themain idea is to re-position the semantic prototypes of unseen actions bymatching them to the distribution of all test videos. For universal actionmodels, we propose to match distributions through a hyperspherical optimaltransport from unseen action prototypes to the set of all projected testvideos. The resulting transport couplings in turn determine the targetprototype for each unseen action. Rather than directly using the targetprototype as final result, we re-position unseen action prototypes along thegeodesic spanned by the original and target prototypes as a form of semanticregularization. For universal object models, we outline a variant that definestarget prototypes based on an optimal transport between unseen actionprototypes and object prototypes. Empirically, we show that universal prototypetransport diminishes the biased selection of unseen action prototypes andboosts both universal action and object models for zero-shot classification andspatio-temporal localization.</description><author>Pascal Mettes</author><pubDate>Tue, 01 Aug 2023 10:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03971v2</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v1</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Wed, 14 Jun 2023 20:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v1</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Action Recognition with Multi-stream Motion Modeling and Mutual Information Maximization</title><link>http://arxiv.org/abs/2306.07576v1</link><description>Action recognition has long been a fundamental and intriguing problem inartificial intelligence. The task is challenging due to the high dimensionalitynature of an action, as well as the subtle motion details to be considered.Current state-of-the-art approaches typically learn from articulated motionsequences in the straightforward 3D Euclidean space. However, the vanillaEuclidean space is not efficient for modeling important motion characteristicssuch as the joint-wise angular acceleration, which reveals the driving forcebehind the motion. Moreover, current methods typically attend to each channelequally and lack theoretical constrains on extracting task-relevant featuresfrom the input. In this paper, we seek to tackle these challenges from three aspects: (1) Wepropose to incorporate an acceleration representation, explicitly modeling thehigher-order variations in motion. (2) We introduce a novel Stream-GCN networkequipped with multi-stream components and channel attention, where differentrepresentations (i.e., streams) supplement each other towards a more preciseaction recognition while attention capitalizes on those important channels. (3)We explore feature-level supervision for maximizing the extraction oftask-relevant information and formulate this into a mutual information loss.Empirically, our approach sets the new state-of-the-art performance on threebenchmark datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our code isanonymously released at https://github.com/ActionR-Group/Stream-GCN, hoping toinspire the community.</description><author>Yuheng Yang, Haipeng Chen, Zhenguang Liu, Yingda Lyu, Beibei Zhang, Shuang Wu, Zhibo Wang, Kui Ren</author><pubDate>Tue, 13 Jun 2023 07:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07576v1</guid></item><item><title>FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes</title><link>http://arxiv.org/abs/2306.10858v1</link><description>A typical task in the field of video understanding is hand actionrecognition, which has a wide range of applications. Existing works eithermainly focus on full-body actions, or the defined action categories arerelatively coarse-grained. In this paper, we propose FHA-Kitchens, a noveldataset of fine-grained hand actions in kitchen scenes. In particular, we focuson human hand interaction regions and perform deep excavation to further refinehand action information and interaction regions. Our FHA-Kitchens datasetconsists of 2,377 video clips and 30,047 images collected from 8 differenttypes of dishes, and all hand interaction regions in each image are labeledwith high-quality fine-grained action classes and bounding boxes. We representthe action information in each hand interaction region as a triplet, resultingin a total of 878 action triplets. Based on the constructed dataset, webenchmark representative action recognition and detection models on thefollowing three tracks: (1) supervised learning for hand interaction region andobject detection, (2) supervised learning for fine-grained hand actionrecognition, and (3) intra- and inter-class domain generalization for handinteraction region detection. The experimental results offer compellingempirical evidence that highlights the challenges inherent in fine-grained handaction recognition, while also shedding light on potential avenues for futureresearch, particularly in relation to pre-training strategy, model design, anddomain generalization. The dataset will be released athttps://github.com/tingZ123/FHA-Kitchens.</description><author>Ting Zhe, Yongqian Li, Jing Zhang, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao</author><pubDate>Mon, 19 Jun 2023 12:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10858v1</guid></item><item><title>Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration</title><link>http://arxiv.org/abs/2307.14866v1</link><description>Training an effective video action recognition model poses significantcomputational challenges, particularly under limited resource budgets. Currentmethods primarily aim to either reduce model size or utilize pre-trainedmodels, limiting their adaptability to various backbone architectures. Thispaper investigates the issue of over-sampled frames, a prevalent problem inmany approaches yet it has received relatively little attention. Despite theuse of fewer frames being a potential solution, this approach often results ina substantial decline in performance. To address this issue, we propose a novelmethod to restore the intermediate features for two sparsely sampled andadjacent video frames. This feature restoration technique brings a negligibleincrease in computational requirements compared to resource-intensive imageencoders, such as ViT. To evaluate the effectiveness of our method, we conductextensive experiments on four public datasets, including Kinetics-400,ActivityNet, UCF-101, and HMDB-51. With the integration of our method, theefficiency of three commonly used baselines has been improved by over 50%, witha mere 0.5% reduction in recognition accuracy. In addition, our method alsosurprisingly helps improve the generalization ability of the models underzero-shot settings.</description><author>Harry Cheng, Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Mohan Kankanhalli</author><pubDate>Thu, 27 Jul 2023 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14866v1</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v1</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Si-Fan Zhang, Wen-Yue Chen, Ning Zhou, Hao Liu, Gui-Hong Lao</author><pubDate>Thu, 06 Jul 2023 03:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v1</guid></item><item><title>A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023</title><link>http://arxiv.org/abs/2307.06569v1</link><description>In this technical report, we present our findings from a study conducted onthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for ActionRecognition. Our research focuses on the innovative application of adifferentiable logic loss in the training to leverage the co-occurrencerelations between verb and noun, as well as the pre-trained Large LanguageModels (LLMs) to generate the logic rules for the adaptation to unseen actionlabels. Specifically, the model's predictions are treated as the truthassignment of a co-occurrence logic formula to compute the logic loss, whichmeasures the consistency between the predictions and the logic constraints. Byusing the verb-noun co-occurrence matrix generated from the dataset, we observea moderate improvement in model performance compared to our baseline framework.To further enhance the model's adaptability to novel action labels, weexperiment with rules generated using GPT-3.5, which leads to a slight decreasein performance. These findings shed light on the potential and challenges ofincorporating differentiable logic and LLMs for knowledge extraction inunsupervised domain adaptation for action recognition. Our final submission(entitled `NS-LLM') achieved the first place in terms of top-1 actionrecognition accuracy.</description><author>Yi Cheng, Ziwei Xu, Fen Fang, Dongyun Lin, Hehe Fan, Yongkang Wong, Ying Sun, Mohan Kankanhalli</author><pubDate>Thu, 13 Jul 2023 06:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06569v1</guid></item><item><title>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2208.10741v3</link><description>Graph convolutional networks (GCNs) are the most commonly used methods forskeleton-based action recognition and have achieved remarkable performance.Generating adjacency matrices with semantically meaningful edges isparticularly important for this task, but extracting such edges is challengingproblem. To solve this, we propose a hierarchically decomposed graphconvolutional network (HD-GCN) architecture with a novel hierarchicallydecomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes everyjoint node into several sets to extract major structurally adjacent and distantedges, and uses them to construct an HD-Graph containing those edges in thesame semantic spaces of a human skeleton. In addition, we introduce anattention-guided hierarchy aggregation (A-HA) module to highlight the dominanthierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-wayensemble method, which uses only joint and bone stream without any motionstream. The proposed model is evaluated and achieves state-of-the-artperformance on four large, popular datasets. Finally, we demonstrate theeffectiveness of our model with various comparative experiments.</description><author>Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 10:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10741v3</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</title><link>http://arxiv.org/abs/2303.08914v2</link><description>Large scale Vision-Language (VL) models have shown tremendous success inaligning representations between visual and text modalities. This enablesremarkable progress in zero-shot recognition, image generation &amp; editing, andmany other exciting tasks. However, VL models tend to over-represent objectswhile paying much less attention to verbs, and require additional tuning onvideo data for best zero-shot action recognition performance. While previouswork relied on large-scale, fully-annotated data, in this work we propose anunsupervised approach. We adapt a VL model for zero-shot and few-shot actionrecognition using a collection of unlabeled videos and an unpaired actiondictionary. Based on that, we leverage Large Language Models and VL models tobuild a text bag for each unlabeled video via matching, text expansion andcaptioning. We use those bags in a Multiple Instance Learning setup to adapt animage-text backbone to video data. Although finetuned on unlabeled video data,our resulting models demonstrate high transferability to numerous unseenzero-shot downstream tasks, improving the base VL model performance by up to14\%, and even comparing favorably to fully-supervised baselines in bothzero-shot and few-shot video recognition transfer. The code will be releasedlater at \url{https://github.com/wlin-at/MAXI}.</description><author>Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, Horst Bischof</author><pubDate>Sat, 22 Jul 2023 10:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08914v2</guid></item><item><title>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2307.07791v1</link><description>Considering the instance-level discriminative ability, contrastive learningmethods, including MoCo and SimCLR, have been adapted from the original imagerepresentation learning task to solve the self-supervised skeleton-based actionrecognition task. These methods usually use multiple data streams (i.e., joint,motion, and bone) for ensemble learning, meanwhile, how to construct adiscriminative feature space within a single stream and effectively aggregatethe information from multiple streams remains an open problem. To this end, wefirst apply a new contrastive learning method called BYOL to learn fromskeleton data and formulate SkeletonBYOL as a simple yet effective baseline forself-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, wefurther present a joint Adversarial and Collaborative Learning (ACL) framework,which combines Cross-Model Adversarial Learning (CMAL) and Cross-StreamCollaborative Learning (CSCL). Specifically, CMAL learns single-streamrepresentation by cross-model adversarial loss to obtain more discriminativefeatures. To aggregate and interact with multi-stream information, CSCL isdesigned by generating similarity pseudo label of ensemble learning assupervision and guiding feature generation for individual streams. Exhaustiveexperiments on three datasets verify the complementary properties between CMALand CSCL and also verify that our method can perform favorably againststate-of-the-art methods using various evaluation protocols. Our code andmodels are publicly available at \url{https://github.com/Levigty/ACL}.</description><author>Tianyu Guo, Mengyuan Liu, Hong Liu, Wenhao Li, Jingwen Guo, Tao Wang, Yidi Li</author><pubDate>Sat, 15 Jul 2023 13:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07791v1</guid></item><item><title>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2307.01985v1</link><description>In the research field of few-shot learning, the main difference betweenimage-based and video-based is the additional temporal dimension for videos. Inrecent years, many approaches for few-shot action recognition have followed themetric-based methods, especially, since some works use the Transformer to getthe cross-attention feature of the videos or the enhanced prototype, and theresults are competitive. However, they do not mine enough information from theTransformer because they only focus on the feature of a single level. In ourpaper, we have addressed this problem. We propose an end-to-end method named"Task-Specific Alignment and Multiple Level Transformer Network (TSA-MLT)". Inour model, the Multiple Level Transformer focuses on the multiple-level featureof the support video and query video. Especially before Multiple LevelTransformer, we use task-specific TSA to filter unimportant or misleadingframes as a pre-processing. Furthermore, we adopt a fusion loss using two kindsof distance, the first is L2 sequence distance, which focuses on temporal orderalignment. The second one is Optimal transport distance, which focuses onmeasuring the gap between the appearance and semantics of the videos. Using asimple fusion network, we fuse the two distances element-wise, then use thecross-entropy loss as our fusion loss. Extensive experiments show our methodachieves state-of-the-art results on the HMDB51 and UCF101 datasets and acompetitive result on the benchmark of Kinetics and something-2-something V2datasets. Our code will be available at the URL:https://github.com/cofly2014/tsa-mlt.git</description><author>Fei Guo, Li Zhu, YiWang Wang</author><pubDate>Wed, 05 Jul 2023 03:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01985v1</guid></item><item><title>Human Action Recognition in Still Images Using ConViT</title><link>http://arxiv.org/abs/2307.08994v1</link><description>Understanding the relationship between different parts of the image plays acrucial role in many visual recognition tasks. Despite the fact thatConvolutional Neural Networks (CNNs) have demonstrated impressive results indetecting single objects, they lack the capability to extract the relationshipbetween various regions of an image, which is a crucial factor in human actionrecognition. To address this problem, this paper proposes a new module thatfunctions like a convolutional layer using Vision Transformer (ViT). Theproposed action recognition model comprises two components: the first part is adeep convolutional network that extracts high-level spatial features from theimage, and the second component of the model utilizes a Vision Transformer thatextracts the relationship between various regions of the image using thefeature map generated by the CNN output. The proposed model has been evaluatedon the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%mAP and 91.5% mAP results, respectively, which are promising compared to otherstate-of-the-art methods.</description><author>Seyed Rohollah Hosseyni, Hasan Taheri, Sanaz Seyedin, Ali Ahmad Rahmani</author><pubDate>Tue, 18 Jul 2023 07:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08994v1</guid></item><item><title>Optimizing ViViT Training: Time and Memory Reduction for Action Recognition</title><link>http://arxiv.org/abs/2306.04822v1</link><description>In this paper, we address the challenges posed by the substantial trainingtime and memory consumption associated with video transformers, focusing on theViViT (Video Vision Transformer) model, in particular the Factorised Encoderversion, as our baseline for action recognition tasks. The factorised encodervariant follows the late-fusion approach that is adopted by many state of theart approaches. Despite standing out for its favorable speed/accuracy tradeoffsamong the different variants of ViViT, its considerable training time andmemory requirements still pose a significant barrier to entry. Our method isdesigned to lower this barrier and is based on the idea of freezing the spatialtransformer during training. This leads to a low accuracy model if naivelydone. But we show that by (1) appropriately initializing the temporaltransformer (a module responsible for processing temporal information) (2)introducing a compact adapter model connecting frozen spatial representations((a module that selectively focuses on regions of the input image) to thetemporal transformer, we can enjoy the benefits of freezing the spatialtransformer without sacrificing accuracy. Through extensive experimentationover 6 benchmarks, we demonstrate that our proposed training strategysignificantly reduces training costs (by $\sim 50\%$) and memory consumptionwhile maintaining or slightly improving performance by up to 1.79\% compared tothe baseline model. Our approach additionally unlocks the capability to utilizelarger image transformer models as our spatial transformer and access moreframes with the same memory consumption.</description><author>Shreyank N Gowda, Anurag Arnab, Jonathan Huang</author><pubDate>Thu, 08 Jun 2023 00:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04822v1</guid></item><item><title>Multimodal Distillation for Egocentric Action Recognition</title><link>http://arxiv.org/abs/2307.07483v2</link><description>The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views. We release our code athttps://github.com/gorjanradevski/multimodal-distillation.</description><author>Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</author><pubDate>Tue, 18 Jul 2023 10:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07483v2</guid></item><item><title>Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</title><link>http://arxiv.org/abs/2306.01075v1</link><description>Accurate understanding and prediction of human behaviors are criticalprerequisites for autonomous vehicles, especially in highly dynamic andinteractive scenarios such as intersections in dense urban areas. In this work,we aim at identifying crossing pedestrians and predicting their futuretrajectories. To achieve these goals, we not only need the context informationof road geometry and other traffic participants but also need fine-grainedinformation of the human pose, motion and activity, which can be inferred fromhuman keypoints. In this paper, we propose a novel multi-task learningframework for pedestrian crossing action recognition and trajectory prediction,which utilizes 3D human keypoints extracted from raw sensor data to capturerich information on human pose and activity. Moreover, we propose to apply twoauxiliary tasks and contrastive learning to enable auxiliary supervisions toimprove the learned keypoints representation, which further enhances theperformance of major tasks. We validate our approach on a large-scale in-housedataset, as well as a public benchmark dataset, and show that our approachachieves state-of-the-art performance on a wide range of evaluation metrics.The effectiveness of each model component is validated in a detailed ablationstudy.</description><author>Jiachen Li, Xinwei Shi, Feiyu Chen, Jonathan Stroud, Zhishuai Zhang, Tian Lan, Junhua Mao, Jeonhyung Kang, Khaled S. Refaat, Weilong Yang, Eugene Ie, Congcong Li</author><pubDate>Thu, 01 Jun 2023 19:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01075v1</guid></item><item><title>Spiking Neural Networks for event-based action recognition: A new task to understand their advantage</title><link>http://arxiv.org/abs/2209.14915v2</link><description>Spiking Neural Networks (SNN) are characterised by their unique temporaldynamics, but the properties and advantages of such computations are still notwell understood. In order to provide answers, in this work we demonstrate howSpiking neurons can enable temporal feature extraction in feed-forward neuralnetworks without the need for recurrent synapses, showing how theirbio-inspired computing principles can be successfully exploited beyond energyefficiency gains and evidencing their differences with respect to conventionalneurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain(DVS-GC), which allows, for the first time, to evaluate the perception oftemporal dependencies in a real event-based action recognition dataset. Ourstudy proves how the widely used DVS Gesture benchmark could be solved bynetworks without temporal feature extraction, unlike the new DVS-GC whichdemands an understanding of the ordering of the events. Furthermore, this setupallowed us to unveil the role of the leakage rate in spiking neurons fortemporal processing tasks and demonstrated the benefits of "hard reset"mechanisms. Additionally, we also show how time-dependent weights andnormalization can lead to understanding order by means of temporal attention.</description><author>Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl</author><pubDate>Tue, 08 Aug 2023 11:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14915v2</guid></item><item><title>High-Performance Inference Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2305.18710v1</link><description>Recently, significant achievements have been made in skeleton-based humanaction recognition with the emergence of graph convolutional networks (GCNs).However, the state-of-the-art (SOTA) models used for this task focus onconstructing more complex higher-order connections between joint nodes todescribe skeleton information, which leads to complex inference processes andhigh computational costs, resulting in reduced model's practicality. To addressthe slow inference speed caused by overly complex model structures, weintroduce re-parameterization and over-parameterization techniques to GCNs, andpropose two novel high-performance inference graph convolutional networks,namely HPI-GCN-RP and HPI-GCN-OP. HPI-GCN-RP uses re-parameterization techniqueto GCNs to achieve a higher inference speed with competitive model performance.HPI-GCN-OP further utilizes over-parameterization technique to bringsignificant performance improvement with inference speed slightly decreased.Experimental results on the two skeleton-based action recognition datasetsdemonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves anaccuracy of 93% on the cross-subject split of the NTU-RGB+D 60 dataset, and90.1% on the cross-subject benchmark of the NTU-RGB+D 120 dataset and is 4.5times faster than HD-GCN at the same accuracy.</description><author>Ziao Li, Junyi Wang, Guhong Nie</author><pubDate>Tue, 30 May 2023 04:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18710v1</guid></item><item><title>High-order Tensor Pooling with Attention for Action Recognition</title><link>http://arxiv.org/abs/2110.05216v2</link><description>We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.</description><author>Piotr Koniusz, Lei Wang, Ke Sun</author><pubDate>Thu, 20 Jul 2023 15:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05216v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v1</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Wed, 24 May 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v1</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v2</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Mon, 19 Jun 2023 09:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v2</guid></item><item><title>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</title><link>http://arxiv.org/abs/2307.07469v1</link><description>Recognizing interactive action plays an important role in human-robotinteraction and collaboration. Previous methods use late fusion andco-attention mechanism to capture interactive relations, which have limitedlearning capability or inefficiency to adapt to more interacting entities. Withassumption that priors of each entity are already known, they also lackevaluations on a more general setting addressing the diversity of subjects. Toaddress these problems, we propose an Interactive Spatiotemporal TokenAttention Network (ISTA-Net), which simultaneously model spatial, temporal, andinteractive relations. Specifically, our network contains a tokenizer topartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way torepresent motions of multiple diverse entities. By extending the entitydimension, ISTs provide better interactive representations. To jointly learnalong three dimensions in ISTs, multi-head self-attention blocks integratedwith 3D convolutions are designed to capture inter-token correlations. Whenmodeling correlations, a strict entity ordering is usually irrelevant forrecognizing interactive actions. To this end, Entity Rearrangement is proposedto eliminate the orderliness in ISTs for interchangeable entities. Extensiveexperiments on four datasets verify the effectiveness of ISTA-Net byoutperforming state-of-the-art methods. Our code is publicly available athttps://github.com/Necolizer/ISTA-Net</description><author>Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu</author><pubDate>Fri, 14 Jul 2023 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07469v1</guid></item><item><title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</title><link>http://arxiv.org/abs/2307.07768v1</link><description>Classifying player actions from soccer videos is a challenging problem, whichhas become increasingly important in sports analytics over the years. Moststate-of-the-art methods employ highly complex offline networks, which makes itdifficult to deploy such models in resource constrained scenarios. Here, inthis paper we propose a novel end-to-end knowledge distillation based transferlearning network pre-trained on the Kinetics400 dataset and then performextensive analysis on the learned framework by introducing a unique lossparameterization. We also introduce a new dataset named SoccerDB1 containing448 videos and consisting of 4 diverse classes each of players playing soccer.Furthermore, we introduce an unique loss parameter that help us linearly weighthe extent to which the predictions of each network are utilized. Finally, wealso perform a thorough performance study using various changedhyperparameters. We also benchmark the first classification results on the newSoccerDB1 dataset obtaining 67.20% validation accuracy. Apart fromoutperforming prior arts significantly, our model also generalizes to newdatasets easily. The dataset has been made publicly available at:https://bit.ly/soccerdb1</description><author>Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</author><pubDate>Sat, 15 Jul 2023 11:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07768v1</guid></item><item><title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</title><link>http://arxiv.org/abs/2307.07768v2</link><description>Classifying player actions from soccer videos is a challenging problem, whichhas become increasingly important in sports analytics over the years. Moststate-of-the-art methods employ highly complex offline networks, which makes itdifficult to deploy such models in resource constrained scenarios. Here, inthis paper we propose a novel end-to-end knowledge distillation based transferlearning network pre-trained on the Kinetics400 dataset and then performextensive analysis on the learned framework by introducing a unique lossparameterization. We also introduce a new dataset named SoccerDB1 containing448 videos and consisting of 4 diverse classes each of players playing soccer.Furthermore, we introduce an unique loss parameter that help us linearly weighthe extent to which the predictions of each network are utilized. Finally, wealso perform a thorough performance study using various changedhyperparameters. We also benchmark the first classification results on the newSoccerDB1 dataset obtaining 67.20% validation accuracy. Apart fromoutperforming prior arts significantly, our model also generalizes to newdatasets easily. The dataset has been made publicly available at:https://bit.ly/soccerdb1</description><author>Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</author><pubDate>Sat, 22 Jul 2023 05:47:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07768v2</guid></item><item><title>Multi-Dimensional Refinement Graph Convolutional Network with Robust Decouple Loss for Fine-Grained Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2306.15321v1</link><description>Graph convolutional networks have been widely used in skeleton-based actionrecognition. However, existing approaches are limited in fine-grained actionrecognition due to the similarity of inter-class data. Moreover, the noisy datafrom pose extraction increases the challenge of fine-grained recognition. Inthis work, we propose a flexible attention block called Channel-VariableSpatial-Temporal Attention (CVSTA) to enhance the discriminative power ofspatial-temporal joints and obtain a more compact intra-class featuredistribution. Based on CVSTA, we construct a Multi-Dimensional Refinement GraphConvolutional Network (MDR-GCN), which can improve the discrimination amongchannel-, joint- and frame-level features for fine-grained actions.Furthermore, we propose a Robust Decouple Loss (RDL), which significantlyboosts the effect of the CVSTA and reduces the impact of noise. The proposedmethod combining MDR-GCN with RDL outperforms the known state-of-the-artskeleton-based approaches on fine-grained datasets, FineGym99 and FSD-10, andalso on the coarse dataset NTU-RGB+D X-view version.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Jin-Rong Zhang, Kai-Yuan Liu, Si-Fan Zhang, Fei-Long Wang, Gao Huang</author><pubDate>Tue, 27 Jun 2023 10:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15321v1</guid></item><item><title>Ensemble Modeling for Multimodal Visual Action Recognition</title><link>http://arxiv.org/abs/2308.05430v1</link><description>In this work, we propose an ensemble modeling approach for multimodal actionrecognition. We independently train individual modality models using a variantof focal loss tailored to handle the long-tailed distribution of the MECCANO[21] dataset. Based on the underlying principle of focal loss, which capturesthe relationship between tail (scarce) classes and their predictiondifficulties, we propose an exponentially decaying variant of focal loss forour current task. It initially emphasizes learning from the hard misclassifiedexamples and gradually adapts to the entire range of examples in the dataset.This annealing process encourages the model to strike a balance betweenfocusing on the sparse set of hard samples, while still leveraging theinformation provided by the easier ones. Additionally, we opt for the latefusion strategy to combine the resultant probability distributions from RGB andDepth modalities for final action prediction. Experimental evaluations on theMECCANO dataset demonstrate the effectiveness of our approach.</description><author>Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah</author><pubDate>Thu, 10 Aug 2023 09:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05430v1</guid></item><item><title>Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective</title><link>http://arxiv.org/abs/2208.07365v2</link><description>Unsupervised video domain adaptation is a practical yet challenging task. Inthis work, for the first time, we tackle it from a disentanglement view. Ourkey idea is to handle the spatial and temporal domain divergence separatelythrough disentanglement. Specifically, we consider the generation ofcross-domain videos from two sets of latent factors, one encoding the staticinformation and another encoding the dynamic information. A Transfer SequentialVAE (TranSVAE) framework is then developed to model such generation. To betterserve for adaptation, we propose several objectives to constrain the latentfactors. With these constraints, the spatial divergence can be readily removedby disentangling the static domain-specific information out, and the temporaldivergence is further reduced from both frame- and video-levels throughadversarial learning. Extensive experiments on the UCF-HMDB, Jester, andEpic-Kitchens datasets verify the effectiveness and superiority of TranSVAEcompared with several state-of-the-art methods. The code with reproducibleresults is publicly accessible.</description><author>Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin</author><pubDate>Fri, 09 Jun 2023 16:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07365v2</guid></item><item><title>How can objects help action recognition?</title><link>http://arxiv.org/abs/2306.11726v1</link><description>Current state-of-the-art video models process a video clip as a long sequenceof spatio-temporal tokens. However, they do not explicitly model objects, theirinteractions across the video, and instead process all the tokens in the video.In this paper, we investigate how we can use knowledge of objects to designbetter video models, namely to process fewer tokens and to improve recognitionaccuracy. This is in contrast to prior works which either drop tokens at thecost of accuracy, or increase accuracy whilst also increasing the computationrequired. First, we propose an object-guided token sampling strategy thatenables us to retain a small fraction of the input tokens with minimal impacton accuracy. And second, we propose an object-aware attention module thatenriches our feature representation with object information and improvesoverall accuracy. Our resulting framework achieves better performance whenusing fewer tokens than strong baselines. In particular, we match our baselinewith 30%, 40%, and 60% of the input tokens on SomethingElse,Something-something v2, and Epic-Kitchens, respectively. When we use our modelto process the same number of tokens as our baseline, we improve by 0.6 to 4.2points on these datasets.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11726v1</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title><link>http://arxiv.org/abs/2308.05681v1</link><description>Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.</description><author>Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</author><pubDate>Thu, 10 Aug 2023 17:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05681v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v1</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 13 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v2</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Sun, 16 Jul 2023 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Humans in 4D: Reconstructing and Tracking Humans with Transformers</title><link>http://arxiv.org/abs/2305.20091v2</link><description>We present an approach to reconstruct humans and track them over time. At thecore of our approach, we propose a fully "transformerized" version of a networkfor human mesh recovery. This network, HMR 2.0, advances the state of the artand shows the capability to analyze unusual poses that have in the past beendifficult to reconstruct from single images. To analyze video, we use 3Dreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.This enables us to deal with multiple people and maintain identities throughocclusion events. Our complete approach, 4DHumans, achieves state-of-the-artresults for tracking people from monocular video. Furthermore, we demonstratethe effectiveness of HMR 2.0 on the downstream task of action recognition,achieving significant improvements over previous pose-based action recognitionapproaches. Our code and models are available on the project website:https://shubham-goel.github.io/4dhumans/.</description><author>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Thu, 29 Jun 2023 06:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20091v2</guid></item><item><title>TS-RGBD Dataset: a Novel Dataset for Theatre Scenes Description for People with Visual Impairments</title><link>http://arxiv.org/abs/2308.01035v1</link><description>Computer vision was long a tool used for aiding visually impaired people tomove around their environment and avoid obstacles and falls. Solutions arelimited to either indoor or outdoor scenes, which limits the kind of places andscenes visually disabled people can be in, including entertainment places suchas theatres. Furthermore, most of the proposed computer-vision-based methodsrely on RGB benchmarks to train their models resulting in a limited performancedue to the absence of the depth modality. In this paper, we propose a novel RGB-D dataset containing theatre sceneswith ground truth human actions and dense captions annotations for imagecaptioning and human action recognition: TS-RGBD dataset. It includes threetypes of data: RGB, depth, and skeleton sequences, captured by MicrosoftKinect. We test image captioning models on our dataset as well as some skeleton-basedhuman action recognition models in order to extend the range of environmenttypes where a visually disabled person can be, by detecting human actions andtextually describing appearances of regions of interest in theatre scenes.</description><author>Leyla Benhamida, Khadidja Delloul, Slimane Larabi</author><pubDate>Wed, 02 Aug 2023 10:28:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01035v1</guid></item><item><title>Volterra Neural Networks (VNNs)</title><link>http://arxiv.org/abs/1910.09616v5</link><description>The importance of inference in Machine Learning (ML) has led to an explosivenumber of different proposals in ML, and particularly in Deep Learning. In anattempt to reduce the complexity of Convolutional Neural Networks, we propose aVolterra filter-inspired Network architecture. This architecture introducescontrolled non-linearities in the form of interactions between the delayedinput samples of data. We propose a cascaded implementation of VolterraFiltering so as to significantly reduce the number of parameters required tocarry out the same classification task as that of a conventional NeuralNetwork. We demonstrate an efficient parallel implementation of this VolterraNeural Network (VNN), along with its remarkable performance while retaining arelatively simpler and potentially more tractable structure. Furthermore, weshow a rather sophisticated adaptation of this network to nonlinearly fuse theRGB (spatial) information and the Optical Flow (temporal) information of avideo sequence for action recognition. The proposed approach is evaluated onUCF-101 and HMDB-51 datasets for action recognition, and is shown to outperformstate of the art CNN approaches.</description><author>Siddharth Roheda, Hamid Krim</author><pubDate>Thu, 15 Jun 2023 17:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.09616v5</guid></item><item><title>Is end-to-end learning enough for fitness activity recognition?</title><link>http://arxiv.org/abs/2305.08191v1</link><description>End-to-end learning has taken hold of many computer vision tasks, inparticular, related to still images, with task-specific optimization yieldingvery strong performance. Nevertheless, human-centric action recognition isstill largely dominated by hand-crafted pipelines, and only individualcomponents are replaced by neural networks that typically operate on individualframes. As a testbed to study the relevance of such pipelines, we present a newfully annotated video dataset of fitness activities. Any recognitioncapabilities in this domain are almost exclusively a function of human posesand their temporal dynamics, so pose-based solutions should perform well. Weshow that, with this labelled data, end-to-end learning on raw pixels cancompete with state-of-the-art action recognition pipelines based on poseestimation. We also show that end-to-end learning can support temporallyfine-grained tasks such as real-time repetition counting.</description><author>Antoine Mercier, Guillaume Berger, Sunny Panchal, Florian Letsch, Cornelius Boehm, Nahua Kang, Ingo Bax, Roland Memisevic</author><pubDate>Sun, 14 May 2023 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08191v1</guid></item><item><title>Human-centric Scene Understanding for 3D Large-scale Scenarios</title><link>http://arxiv.org/abs/2307.14392v1</link><description>Human-centric scene understanding is significant for real-world applications,but it is extremely challenging due to the existence of diverse human poses andactions, complex human-environment interactions, severe occlusions in crowds,etc. In this paper, we present a large-scale multi-modal dataset forhuman-centric scene understanding, dubbed HuCenLife, which is collected indiverse daily-life scenarios with rich and fine-grained annotations. OurHuCenLife can benefit many 3D perception tasks, such as segmentation,detection, action recognition, etc., and we also provide benchmarks for thesetasks to facilitate related research. In addition, we design novel modules forLiDAR-based segmentation and action recognition, which are more applicable forlarge-scale human-centric scenarios and achieve state-of-the-art performance.</description><author>Yiteng Xu, Peishan Cong, Yichen Yao, Runnan Chen, Yuenan Hou, Xinge Zhu, Xuming He, Jingyi Yu, Yuexin Ma</author><pubDate>Wed, 26 Jul 2023 09:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14392v1</guid></item><item><title>EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video</title><link>http://arxiv.org/abs/2307.05784v1</link><description>In egocentric action recognition a single population model is typicallytrained and subsequently embodied on a head-mounted device, such as anaugmented reality headset. While this model remains static for new users andenvironments, we introduce an adaptive paradigm of two phases, where afterpretraining a population model, the model adapts on-device and online to theuser's experience. This setting is highly challenging due to the change frompopulation to user domain and the distribution shifts in the user's datastream. Coping with the latter in-stream distribution shifts is the focus ofcontinual learning, where progress has been rooted in controlled benchmarks butchallenges faced in real-world applications often remain unaddressed. Weintroduce EgoAdapt, a benchmark for real-world egocentric action recognitionthat facilitates our two-phased adaptive paradigm, and real-world challengesnaturally occur in the egocentric video streams from Ego4d, such as long-tailedaction distributions and large-scale classification over 2740 actions. Weintroduce an evaluation framework that directly exploits the user's data streamwith new metrics to measure the adaptation gain over the population model,online generalization, and hindsight performance. In contrast to single-streamevaluation in existing works, our framework proposes a meta-evaluation thataggregates the results from 50 independent user streams. We provide anextensive empirical study for finetuning and experience replay.</description><author>Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway</author><pubDate>Tue, 11 Jul 2023 21:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05784v1</guid></item><item><title>HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding</title><link>http://arxiv.org/abs/2307.05721v1</link><description>Understanding comprehensive assembly knowledge from videos is critical forfuturistic ultra-intelligent industry. To enable technological breakthrough, wepresent HA-ViD - the first human assembly video dataset that featuresrepresentative industrial assembly scenarios, natural procedural knowledgeacquisition process, and consistent human-robot shared annotations.Specifically, HA-ViD captures diverse collaboration patterns of real-worldassembly, natural human behaviors and learning progression during assembly, andgranulate action annotations to subject, action verb, manipulated object,target object, and tool. We provide 3222 multi-view, multi-modality videos(each video contains one assembly task), 1.5M frames, 96K temporal labels and2M spatial labels. We benchmark four foundational video understanding tasks:action recognition, action segmentation, object detection and multi-objecttracking. Importantly, we analyze their performance for comprehending knowledgein assembly progress, process efficiency, task collaboration, skill parametersand human intention. Details of HA-ViD is available at:https://iai-hrc.github.io/ha-vid.</description><author>Hao Zheng, Regina Lee, Yuqian Lu</author><pubDate>Sun, 09 Jul 2023 09:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05721v1</guid></item><item><title>Bullying10K: A Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition</title><link>http://arxiv.org/abs/2306.11546v1</link><description>The prevalence of violence in daily life poses significant threats toindividuals' physical and mental well-being. Using surveillance cameras inpublic spaces has proven effective in proactively deterring and preventing suchincidents. However, concerns regarding privacy invasion have emerged due totheir widespread deployment. To address the problem, we leverage Dynamic VisionSensors (DVS) cameras to detect violent incidents and preserve privacy since itcaptures pixel brightness variations instead of static imagery. We introducethe Bullying10K dataset, encompassing various actions, complex movements, andocclusions from real-life scenarios. It provides three benchmarks forevaluating different tasks: action recognition, temporal action localization,and pose estimation. With 10,000 event segments, totaling 12 billion events and255 GB of data, Bullying10K contributes significantly by balancing violencedetection and personal privacy persevering. And it also poses a challenge tothe neuromorphic dataset. It will serve as a valuable resource for training anddeveloping privacy-protecting video systems. The Bullying10K opens newpossibilities for innovative approaches in these domains.</description><author>Yiting Dong, Yang Li, Dongcheng Zhao, Guobin Shen, Yi Zeng</author><pubDate>Tue, 20 Jun 2023 14:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11546v1</guid></item><item><title>GoferBot: A Visual Guided Human-Robot Collaborative Assembly System</title><link>http://arxiv.org/abs/2304.08840v2</link><description>The current transformation towards smart manufacturing has led to a growingdemand for human-robot collaboration (HRC) in the manufacturing process.Perceiving and understanding the human co-worker's behaviour introduceschallenges for collaborative robots to efficiently and effectively performtasks in unstructured and dynamic environments. Integrating recent data-drivenmachine vision capabilities into HRC systems is a logical next step inaddressing these challenges. However, in these cases, off-the-shelf componentsstruggle due to generalisation limitations. Real-world evaluation is requiredin order to fully appreciate the maturity and robustness of these approaches.Furthermore, understanding the pure-vision aspects is a crucial first stepbefore combining multiple modalities in order to understand the limitations. Inthis paper, we propose GoferBot, a novel vision-based semantic HRC system for areal-world assembly task. It is composed of a visual servoing module thatreaches and grasps assembly parts in an unstructured multi-instance and dynamicenvironment, an action recognition module that performs human action predictionfor implicit communication, and a visual handover module that uses theperceptual understanding of human behaviour to produce an intuitive andefficient collaborative assembly experience. GoferBot is a novel assemblysystem that seamlessly integrates all sub-modules by utilising implicitsemantic information purely from visual perception.</description><author>Zheyu Zhuang, Yizhak Ben-Shabat, Jiahao Zhang, Stephen Gould, Robert Mahony</author><pubDate>Wed, 17 May 2023 08:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08840v2</guid></item><item><title>Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition</title><link>http://arxiv.org/abs/2307.11973v1</link><description>As a fundamental aspect of human life, two-person interactions containmeaningful information about people's activities, relationships, and socialsettings. Human action recognition serves as the foundation for many smartapplications, with a strong focus on personal privacy. However, recognizingtwo-person interactions poses more challenges due to increased body occlusionand overlap compared to single-person actions. In this paper, we propose apoint cloud-based network named Two-stream Multi-level Dynamic PointTransformer for two-person interaction recognition. Our model addresses thechallenge of recognizing two-person interactions by incorporating local-regionspatial information, appearance information, and motion information. To achievethis, we introduce a designed frame selection method named Interval FrameSampling (IFS), which efficiently samples frames from videos, capturing morediscriminative information in a relatively short processing time. Subsequently,a frame features learning module and a two-stream multi-level featureaggregation module extract global and partial features from the sampled frames,effectively representing the local-region spatial information, appearanceinformation, and motion information related to the interactions. Finally, weapply a transformer to perform self-attention on the learned features for thefinal classification. Extensive experiments are conducted on two large-scaledatasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. Theresults show that our network outperforms state-of-the-art approaches acrossall standard evaluation settings.</description><author>Yao Liu, Gangfeng Cui, Jiahui Luo, Lina Yao, Xiaojun Chang</author><pubDate>Sat, 22 Jul 2023 04:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11973v1</guid></item><item><title>Theater Aid System for the Visually Impaired Through Transfer Learning of Spatio-Temporal Graph Convolution Networks</title><link>http://arxiv.org/abs/2306.16357v1</link><description>The aim of this research is to recognize human actions performed on stage toaid visually impaired and blind individuals. To achieve this, we have created atheatre human action recognition system that uses skeleton data captured bydepth image as input. We collected new samples of human actions in a theatreenvironment, and then tested the transfer learning technique with threepre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based humanaction recognition: the spatio-temporal graph convolution network, thetwo-stream adaptive graph convolution network, and the multi-scale disentangledunified graph convolution network. We selected the NTU-RGBD human actionbenchmark as the source domain and used our collected dataset as the targetdomain. We analyzed the transferability of the pre-trained models and proposedtwo configurations to apply and adapt the transfer learning technique to thediversity between the source and target domains. The use of transfer learninghelped to improve the performance of the human action system within the contextof theatre. The results indicate that Spatio-Temporal Graph ConvolutionNetworks is positively transferred, and there was an improvement in performancecompared to the baseline without transfer learning.</description><author>Leyla Benhamida, Slimane Larabi</author><pubDate>Wed, 28 Jun 2023 17:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16357v1</guid></item><item><title>Learning Higher-order Object Interactions for Keypoint-based Video Understanding</title><link>http://arxiv.org/abs/2305.09539v1</link><description>Action recognition is an important problem that requires identifying actionsin video by learning complex interactions across scene actors and objects.However, modern deep-learning based networks often require significantcomputation, and may capture scene context using various modalities thatfurther increases compute costs. Efficient methods such as those used for AR/VRoften only use human-keypoint information but suffer from a loss of scenecontext that hurts accuracy. In this paper, we describe an action-localizationmethod, KeyNet, that uses only the keypoint data for tracking and actionrecognition. Specifically, KeyNet introduces the use of object based keypointinformation to capture context in the scene. Our method illustrates how tobuild a structured intermediate representation that allows modelinghigher-order interactions in the scene from object and human keypoints withoutusing any RGB information. We find that KeyNet is able to track and classifyhuman actions at just 5 FPS. More importantly, we demonstrate that objectkeypoints can be modeled to recover any loss in context from using keypointinformation over AVA action and Kinetics datasets.</description><author>Yi Huang, Asim Kadav, Farley Lai, Deep Patel, Hans Peter Graf</author><pubDate>Tue, 16 May 2023 16:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09539v1</guid></item><item><title>HomE: Homography-Equivariant Video Representation Learning</title><link>http://arxiv.org/abs/2306.01623v1</link><description>Recent advances in self-supervised representation learning have enabled moreefficient and robust model performance without relying on extensive labeleddata. However, most works are still focused on images, with few working onvideos and even fewer on multi-view videos, where more powerful inductivebiases can be leveraged for self-supervision. In this work, we propose a novelmethod for representation learning of multi-view videos, where we explicitlymodel the representation space to maintain Homography Equivariance (HomE). Ourmethod learns an implicit mapping between different views, culminating in arepresentation space that maintains the homography relationship betweenneighboring views. We evaluate our HomE representation via action recognitionand pedestrian intent prediction as downstream tasks. On action classification,our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better thanmost state-of-the-art self-supervised learning methods. Similarly, on the STIPdataset, we outperform the state-of-the-art by 6% for pedestrian intentprediction one second into the future while also obtaining an accuracy of 91.2%for pedestrian action (cross vs. not-cross) classification. Code is availableat https://github.com/anirudhs123/HomE.</description><author>Anirudh Sriram, Adrien Gaidon, Jiajun Wu, Juan Carlos Niebles, Li Fei-Fei, Ehsan Adeli</author><pubDate>Fri, 02 Jun 2023 16:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01623v1</guid></item><item><title>CMD: Self-supervised 3D Action Representation Learning with Cross-modal Mutual Distillation</title><link>http://arxiv.org/abs/2208.12448v3</link><description>In 3D action recognition, there exists rich complementary information betweenskeleton modalities. Nevertheless, how to model and utilize this informationremains a challenging problem for self-supervised 3D action representationlearning. In this work, we formulate the cross-modal interaction as abidirectional knowledge distillation problem. Different from classicdistillation solutions that transfer the knowledge of a fixed and pre-trainedteacher to the student, in this work, the knowledge is continuously updated andbidirectionally distilled between modalities. To this end, we propose a newCross-modal Mutual Distillation (CMD) framework with the following designs. Onthe one hand, the neighboring similarity distribution is introduced to modelthe knowledge learned in each modality, where the relational information isnaturally suitable for the contrastive frameworks. On the other hand,asymmetrical configurations are used for teacher and student to stabilize thedistillation process and to transfer high-confidence information betweenmodalities. By derivation, we find that the cross-modal positive mining inprevious works can be regarded as a degenerated version of our CMD. We performextensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets.Our approach outperforms existing self-supervised methods and sets a series ofnew records. The code is available at: https://github.com/maoyunyao/CMD</description><author>Yunyao Mao, Wengang Zhou, Zhenbo Lu, Jiajun Deng, Houqiang Li</author><pubDate>Thu, 25 May 2023 15:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12448v3</guid></item><item><title>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events</title><link>http://arxiv.org/abs/2005.04490v6</link><description>Along with the development of modern smart cities, human-centric videoanalysis has been encountering the challenge of analyzing diverse and complexevents in real scenes. A complex event relates to dense crowds, anomalousindividuals, or collective behaviors. However, limited by the scale andcoverage of existing video datasets, few human analysis approaches havereported their performances on such complex events. To this end, we present anew large-scale dataset with comprehensive annotations, named Human-in-Eventsor HiEve (Human-centric video analysis in complex Events), for theunderstanding of human motions, poses, and actions in a variety of realisticevents, especially in crowd &amp; complex events. It contains a record number ofposes (&gt;1M), the largest number of action instances (&gt;56k) under complexevents, as well as one of the largest numbers of trajectories lasting forlonger time (with an average trajectory length of &gt;480 frames). Based on itsdiverse annotation, we present two simple baselines for action recognition andpose estimation, respectively. They leverage cross-label information duringtraining to enhance the feature learning in corresponding visual tasks.Experiments show that they could boost the performance of existing actionrecognition and pose estimation pipelines. More importantly, they prove thewidely ranged annotations in HiEve can improve various video tasks.Furthermore, we conduct extensive experiments to benchmark recent videoanalysis approaches together with our baseline methods, demonstrating HiEve isa challenging dataset for human-centric video analysis. We expect that thedataset will advance the development of cutting-edge techniques inhuman-centric analysis and the understanding of complex events. The dataset isavailable at http://humaninevents.org</description><author>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe</author><pubDate>Thu, 13 Jul 2023 14:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.04490v6</guid></item><item><title>Physical Adversarial Attacks for Surveillance: A Survey</title><link>http://arxiv.org/abs/2305.01074v1</link><description>Modern automated surveillance techniques are heavily reliant on deep learningmethods. Despite the superior performance, these learning systems areinherently vulnerable to adversarial attacks - maliciously crafted inputs thatare designed to mislead, or trick, models into making incorrect predictions. Anadversary can physically change their appearance by wearing adversarialt-shirts, glasses, or hats or by specific behavior, to potentially avoidvarious forms of detection, tracking and recognition of surveillance systems;and obtain unauthorized access to secure properties and assets. This poses asevere threat to the security and safety of modern surveillance systems. Thispaper reviews recent attempts and findings in learning and designing physicaladversarial attacks for surveillance applications. In particular, we propose aframework to analyze physical adversarial attacks and provide a comprehensivesurvey of physical adversarial attacks on four key surveillance tasks:detection, identification, tracking, and action recognition under thisframework. Furthermore, we review and analyze strategies to defend against thephysical adversarial attacks and the methods for evaluating the strengths ofthe defense. The insights in this paper present an important step in buildingresilience within surveillance systems to physical adversarial attacks.</description><author>Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan</author><pubDate>Mon, 01 May 2023 21:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01074v1</guid></item><item><title>VideoGLUE: Video General Understanding Evaluation of Foundation Models</title><link>http://arxiv.org/abs/2307.03166v1</link><description>We evaluate existing foundation models video understanding capabilities usinga carefully designed experiment protocol consisting of three hallmark tasks(action recognition, temporal localization, and spatiotemporal localization),eight datasets well received by the community, and four adaptation methodstailoring a foundation model (FM) for a downstream task. Moreover, we propose ascalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency whenadapting to general video understanding tasks. Our main findings are asfollows. First, task-specialized models significantly outperform the six FMsstudied in this work, in sharp contrast to what FMs have achieved in naturallanguage and image understanding. Second,video-native FMs, whose pretrainingdata contains the video modality, are generally better than image-native FMs inclassifying motion-rich videos, localizing actions in time, and understanding avideo of more than one action. Third, the video-native FMs can perform well onvideo tasks under light adaptations to downstream tasks(e.g., freezing the FMbackbones), while image-native FMs win in full end-to-end finetuning. The firsttwo observations reveal the need and tremendous opportunities to conductresearch on video-focused FMs, and the last confirms that both tasks andadaptation methods matter when it comes to the evaluation of FMs.</description><author>Liangzhe Yuan, Nitesh Bharadwaj Gundavarapu, Long Zhao, Hao Zhou, Yin Cui, Lu Jiang, Xuan Yang, Menglin Jia, Tobias Weyand, Luke Friedman, Mikhail Sirotenko, Huisheng Wang, Florian Schroff, Hartwig Adam, Ming-Hsuan Yang, Ting Liu, Boqing Gong</author><pubDate>Thu, 06 Jul 2023 18:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03166v1</guid></item><item><title>Dynamic Perceiver for Efficient Visual Recognition</title><link>http://arxiv.org/abs/2306.11248v1</link><description>Early exiting has become a promising approach to improving the inferenceefficiency of deep networks. By structuring models with multiple classifiers(exits), predictions for ``easy'' samples can be generated at earlier exits,negating the need for executing deeper layers. Current multi-exit networkstypically implement linear classifiers at intermediate layers, compellinglow-level features to encapsulate high-level semantics. This sub-optimal designinvariably undermines the performance of later exits. In this paper, we proposeDynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedureand the early classification task with a novel dual-branch architecture. Afeature branch serves to extract image features, while a classification branchprocesses a latent code assigned for classification tasks. Bi-directionalcross-attention layers are established to progressively fuse the information ofboth branches. Early exits are placed exclusively within the classificationbranch, thus eliminating the need for linear separability in low-levelfeatures. Dyn-Perceiver constitutes a versatile and adaptable framework thatcan be built upon various architectures. Experiments on image classification,action recognition, and object detection demonstrate that our methodsignificantly improves the inference efficiency of different backbones,outperforming numerous competitive approaches across a broad range ofcomputational budgets. Evaluation on both CPU and GPU platforms substantiatethe superior practical efficiency of Dyn-Perceiver. Code is available athttps://www.github.com/LeapLabTHU/Dynamic_Perceiver.</description><author>Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, Gao Huang</author><pubDate>Tue, 20 Jun 2023 04:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11248v1</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v2</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Mon, 03 Jul 2023 08:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v2</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v1</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Fri, 26 May 2023 01:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v1</guid></item><item><title>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</title><link>http://arxiv.org/abs/2307.08476v1</link><description>Skeleton sequence representation learning has shown great advantages foraction recognition due to its promising ability to model human joints andtopology. However, the current methods usually require sufficient labeled datafor training computationally expensive models, which is labor-intensive andtime-consuming. Moreover, these methods ignore how to utilize the fine-graineddependencies among different skeleton joints to pre-train an efficient skeletonsequence learning model that can generalize well across different datasets. Inthis paper, we propose an efficient skeleton sequence learning framework, namedSkeleton Sequence Learning (SSL). To comprehensively capture the human pose andobtain discriminative skeleton sequence representation, we build an asymmetricgraph-based encoder-decoder pre-training architecture named SkeletonMAE, whichembeds skeleton joint sequence into Graph Convolutional Network (GCN) andreconstructs the masked skeleton joints and edges based on the prior humantopology knowledge. Then, the pre-trained SkeletonMAE encoder is integratedwith the Spatial-Temporal Representation Learning (STRL) module to build theSSL framework. Extensive experimental results show that our SSL generalizeswell across different datasets and outperforms the state-of-the-artself-supervised skeleton-based action recognition methods on FineGym, Diving48,NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance tosome fully supervised methods. The code is avaliable athttps://github.com/HongYan1123/SkeletonMAE.</description><author>Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin</author><pubDate>Mon, 17 Jul 2023 14:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08476v1</guid></item><item><title>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</title><link>http://arxiv.org/abs/2304.07775v2</link><description>Cross-modal distillation has been widely used to transfer knowledge acrossdifferent modalities, enriching the representation of the target unimodal one.Recent studies highly relate the temporal synchronization between vision andsound to the semantic consistency for cross-modal distillation. However, suchsemantic consistency from the synchronization is hard to guarantee inunconstrained videos, due to the irrelevant modality noise and differentiatedsemantic correlation. To this end, we first propose a \textit{Modality NoiseFilter} (MNF) module to erase the irrelevant noise in teacher modality withcross-modal context. After this purification, we then design a\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distilluseful knowledge for target modality, by referring to the differentiatedsample-wise semantic correlation in a contrastive fashion. Extensiveexperiments show that our method could bring a performance boost compared withother distillation methods in both visual action recognition and videoretrieval task. We also extend to the audio tagging task to prove thegeneralization of our method. The source code is available at\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.</description><author>Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu</author><pubDate>Thu, 27 Apr 2023 05:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07775v2</guid></item></channel></rss>