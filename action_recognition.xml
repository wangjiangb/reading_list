<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 02 Sep 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Balanced Representation Learning for Long-tailed Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.14024v1</link><description>Skeleton-based action recognition has recently made significant progress.However, data imbalance is still a great challenge in real-world scenarios. Theperformance of current action recognition algorithms declines sharply whentraining data suffers from heavy class imbalance. The imbalanced data actuallydegrades the representations learned by these methods and becomes thebottleneck for action recognition. How to learn unbiased representations fromimbalanced action data is the key to long-tailed action recognition. In thispaper, we propose a novel balanced representation learning method to addressthe long-tailed problem in action recognition. Firstly, a spatial-temporalaction exploration strategy is presented to expand the sample spaceeffectively, generating more valuable samples in a rebalanced manner. Secondly,we design a detached action-aware learning schedule to further mitigate thebias in the representation space. The schedule detaches the representationlearning of tail classes from training and proposes an action-aware loss toimpose more effective constraints. Additionally, a skip-modal representation isproposed to provide complementary structural information. The proposed methodis validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA,and Kinetics. It not only achieves consistently large improvement compared tothe state-of-the-art (SOTA) methods, but also demonstrates a superiorgeneralization capacity through extensive experiments. Our code is available athttps://github.com/firework8/BRL.</description><author>Hongda Liu, Yunlong Wang, Min Ren, Junxing Hu, Zhengquan Luo, Guangqi Hou, Zhenan Sun</author><pubDate>Sun, 27 Aug 2023 08:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14024v1</guid></item><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition</title><link>http://arxiv.org/abs/2308.10557v1</link><description>Hand action recognition is essential. Communication, human-robotinteractions, and gesture control are dependent on it. Skeleton-based actionrecognition traditionally includes hands, which belong to the classes whichremain challenging to correctly recognize to date. We propose a methodspecifically designed for hand action recognition which uses relative angularembeddings and local Spherical Harmonics to create novel hand representations.The use of Spherical Harmonics creates rotation-invariant representations whichmake hand action recognition even more robust against inter-subject differencesand viewpoint changes. We conduct extensive experiments on the hand joints inthe First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand PoseAnnotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit ofusing Local Spherical Harmonics Representations. Our code is available athttps://github.com/KathPra/LSHR_LSHT.</description><author>Katharina Prasse, Steffen Jung, Yuxuan Zhou, Margret Keuper</author><pubDate>Mon, 21 Aug 2023 09:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10557v1</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>Cross-Modal Learning with 3D Deformable Attention for Action Recognition</title><link>http://arxiv.org/abs/2212.05638v3</link><description>An important challenge in vision-based action recognition is the embedding ofspatiotemporal features with two or more heterogeneous modalities into a singlefeature. In this study, we propose a new 3D deformable transformer for actionrecognition with adaptive spatiotemporal receptive fields and a cross-modallearning scheme. The 3D deformable transformer consists of three attentionmodules: 3D deformability, local joint stride, and temporal stride attention.The two cross-modal tokens are input into the 3D deformable attention module tocreate a cross-attention token with a reflected spatiotemporal correlation.Local joint stride attention is applied to spatially combine attention and posetokens. Temporal stride attention temporally reduces the number of input tokensin the attention module and supports temporal expression learning without thesimultaneous use of all tokens. The deformable transformer iterates L-times andcombines the last cross-modal token for classification. The proposed 3Ddeformable transformer was tested on the NTU60, NTU120, FineGYM, and PennActiondatasets, and showed results better than or similar to pre-trainedstate-of-the-art methods even without a pre-training process. In addition, byvisualizing important joints and correlations during action recognition throughspatial joint and temporal stride attention, the possibility of achieving anexplainable potential for action recognition is presented.</description><author>Sangwon Kim, Dasom Ahn, Byoung Chul Ko</author><pubDate>Thu, 17 Aug 2023 08:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05638v3</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>M$^3$Net: Multi-view Encoding, Matching, and Fusion for Few-shot Fine-grained Action Recognition</title><link>http://arxiv.org/abs/2308.03063v1</link><description>Due to the scarcity of manually annotated data required for fine-grainedvideo understanding, few-shot fine-grained (FS-FG) action recognition hasgained significant attention, with the aim of classifying novel fine-grainedaction categories with only a few labeled instances. Despite the progress madein FS coarse-grained action recognition, current approaches encounter twochallenges when dealing with the fine-grained action categories: the inabilityto capture subtle action details and the insufficiency of learning from limiteddata that exhibit high intra-class variance and inter-class similarity. Toaddress these limitations, we propose M$^3$Net, a matching-based framework forFS-FG action recognition, which incorporates \textit{multi-view encoding},\textit{multi-view matching}, and \textit{multi-view fusion} to facilitateembedding encoding, similarity matching, and decision making across multipleviewpoints. \textit{Multi-view encoding} captures rich contextual details fromthe intra-frame, intra-video, and intra-episode perspectives, generatingcustomized higher-order embeddings for fine-grained data. \textit{Multi-viewmatching} integrates various matching functions enabling flexible relationmodeling within limited samples to handle multi-scale spatio-temporalvariations by leveraging the instance-specific, category-specific, andtask-specific perspectives. \textit{Multi-view fusion} consists ofmatching-predictions fusion and matching-losses fusion over the above views,where the former promotes mutual complementarity and the latter enhancesembedding generalizability by employing multi-task collaborative learning.Explainable visualizations and experimental results on three challengingbenchmarks demonstrate the superiority of M$^3$Net in capturing fine-grainedaction details and achieving state-of-the-art performance for FS-FG actionrecognition.</description><author>Hao Tang, Jun Liu, Shuanglin Yan, Rui Yan, Zechao Li, Jinhui Tang</author><pubDate>Sun, 06 Aug 2023 10:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03063v1</guid></item><item><title>On the Importance of Spatial Relations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2308.07119v1</link><description>Deep learning has achieved great success in video recognition, yet stillstruggles to recognize novel actions when faced with only a few examples. Totackle this challenge, few-shot action recognition methods have been proposedto transfer knowledge from a source dataset to a novel target dataset with onlyone or a few labeled videos. However, existing methods mainly focus on modelingthe temporal relations between the query and support videos while ignoring thespatial relations. In this paper, we find that the spatial misalignment betweenobjects also occurs in videos, notably more common than the temporalinconsistency. We are thus motivated to investigate the importance of spatialrelations and propose a more accurate few-shot action recognition method thatleverages both spatial and temporal information. Particularly, a novel SpatialAlignment Cross Transformer (SA-CT) which learns to re-adjust the spatialrelations and incorporates the temporal information is contributed. Experimentsreveal that, even without using any temporal information, the performance ofSA-CT is comparable to temporal based methods on 3/4 benchmarks. To furtherincorporate the temporal information, we propose a simple yet effectiveTemporal Mixer module. The Temporal Mixer enhances the video representation andimproves the performance of the full SA-CT model, achieving very competitiveresults. In this work, we also exploit large-scale pretrained models forfew-shot action recognition, providing useful insights for this researchdirection.</description><author>Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 14 Aug 2023 13:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07119v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v1</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 29 May 2023 09:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v1</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Improving Zero-Shot Action Recognition using Human Instruction with Text Description</title><link>http://arxiv.org/abs/2301.08874v2</link><description>Zero-shot action recognition, which recognizes actions in videos withouthaving received any training examples, is gaining wide attention considering itcan save labor costs and training time. Nevertheless, the performance ofzero-shot learning is still unsatisfactory, which limits its practicalapplication. To solve this problem, this study proposes a framework to improvezero-shot action recognition using human instructions with text descriptions.The proposed framework manually describes video contents, which incurs somelabor costs; in many situations, the labor costs are worth it. We manuallyannotate text features for each action, which can be a word, phrase, orsentence. Then by computing the matching degrees between the video and all textfeatures, we can predict the class of the video. Furthermore, the proposedmodel can also be combined with other models to improve its accuracy. Inaddition, our model can be continuously optimized to improve the accuracy byrepeating human instructions. The results with UCF101 and HMDB51 showed thatour model achieved the best accuracy and improved the accuracies of othermodels.</description><author>Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 12 Jun 2023 09:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08874v2</guid></item><item><title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2212.04761v2</link><description>Skeleton-based action recognition has attracted considerable attention due toits compact representation of the human body's skeletal sructure. Many recentmethods have achieved remarkable performance using graph convolutional networks(GCNs) and convolutional neural networks (CNNs), which extract spatial andtemporal features, respectively. Although spatial and temporal dependencies inthe human skeleton have been explored separately, spatio-temporal dependency israrely considered. In this paper, we propose the Spatio-Temporal Curve Network(STC-Net) to effectively leverage the spatio-temporal dependency of the humanskeleton. Our proposed network consists of two novel elements: 1) TheSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for GraphConvolution (DK-GC). The STC module dynamically adjusts the receptive field byidentifying meaningful node connections between every adjacent frame andgenerating spatio-temporal curves based on the identified node connections,providing an adaptive spatio-temporal coverage. In addition, we propose DK-GCto consider long-range dependencies, which results in a large receptive fieldwithout any additional parameters by applying an extended kernel to the givenadjacency matrices of the graph. Our STC-Net combines these two modules andachieves state-of-the-art performance on four skeleton-based action recognitionbenchmarks.</description><author>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 03:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04761v2</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</title><link>http://arxiv.org/abs/2308.03950v1</link><description>Zero-shot skeleton-based action recognition aims to recognize actions ofunseen categories after training on data of seen categories. The key is tobuild the connection between visual and semantic space from seen to unseenclasses. Previous studies have primarily focused on encoding sequences into asingular feature vector, with subsequent mapping the features to an identicalanchor point within the embedded space. Their performance is hindered by 1) theignorance of the global visual/semantic distribution alignment, which resultsin a limitation to capture the true interdependence between the two spaces. 2)the negligence of temporal information since the frame-wise features with richaction clues are directly pooled into a single feature vector. We propose a newzero-shot skeleton-based action recognition method via mutual information (MI)estimation and maximization. Specifically, 1) we maximize the MI between visualand semantic space for distribution alignment; 2) we leverage the temporalinformation for estimating the MI by encouraging MI to increase as more framesare observed. Extensive experiments on three large-scale skeleton actiondatasets confirm the effectiveness of our method. Code:https://github.com/YujieOuO/SMIE.</description><author>Yujie Zhou, Wenwen Qiang, Anyi Rao, Ning Lin, Bing Su, Jiaqi Wang</author><pubDate>Tue, 08 Aug 2023 00:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03950v1</guid></item><item><title>Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2302.13434v2</link><description>Recently, skeleton-based human action has become a hot research topic becausethe compact representation of human skeletons brings new blood to this researchdomain. As a result, researchers began to notice the importance of using RGB orother sensors to analyze human action by extracting skeleton information.Leveraging the rapid development of deep learning (DL), a significant number ofskeleton-based human action approaches have been presented with fine-designedDL structures recently. However, a well-trained DL model always demandshigh-quality and sufficient data, which is hard to obtain without costing highexpenses and human labor. In this paper, we introduce a novel data augmentationmethod for skeleton-based action recognition tasks, which can effectivelygenerate high-quality and diverse sequential actions. In order to obtainnatural and realistic action sequences, we propose denoising diffusionprobabilistic models (DDPMs) that can generate a series of synthetic actionsequences, and their generation process is precisely guided by aspatial-temporal transformer (ST-Trans). Experimental results show that ourmethod outperforms the state-of-the-art (SOTA) motion generation approaches ondifferent naturality and diversity metrics. It proves that its high-qualitysynthetic data can also be effectively deployed to existing action recognitionmodels with significant performance improvement.</description><author>Yifan Jiang, Han Chen, Hanseok Ko</author><pubDate>Tue, 25 Jul 2023 03:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13434v2</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>Graph Contrastive Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2301.10900v2</link><description>In the field of skeleton-based action recognition, current top-performinggraph convolutional networks (GCNs) exploit intra-sequence context to constructadaptive graphs for feature aggregation. However, we argue that such context isstill \textit{local} since the rich cross-sequence relations have not beenexplicitly investigated. In this paper, we propose a graph contrastive learningframework for skeleton-based action recognition (\textit{SkeletonGCL}) toexplore the \textit{global} context across all sequences. In specific,SkeletonGCL associates graph learning across sequences by enforcing graphs tobe class-discriminative, \emph{i.e.,} intra-class compact and inter-classdispersed, which improves the GCN capacity to distinguish various actionpatterns. Besides, two memory banks are designed to enrich cross-sequencecontext from two complementary levels, \emph{i.e.,} instance and semanticlevels, enabling graph contrastive learning in multiple context scales.Consequently, SkeletonGCL establishes a new training paradigm, and it can beseamlessly incorporated into current GCNs. Without loss of generality, wecombine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), andachieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. Thesource code will be available at\url{https://github.com/OliverHxh/SkeletonGCL}.</description><author>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</author><pubDate>Sat, 10 Jun 2023 11:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10900v2</guid></item><item><title>Video BagNet: short temporal receptive fields increase robustness in long-term action recognition</title><link>http://arxiv.org/abs/2308.11249v1</link><description>Previous work on long-term video action recognition relies on deep3D-convolutional models that have a large temporal receptive field (RF). Weargue that these models are not always the best choice for temporal modeling invideos. A large temporal receptive field allows the model to encode the exactsub-action order of a video, which causes a performance decrease when testingvideos have a different sub-action order. In this work, we investigate whetherwe can improve the model robustness to the sub-action order by shrinking thetemporal receptive field of action recognition models. For this, we designVideo BagNet, a variant of the 3D ResNet-50 model with the temporal receptivefield size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet onsynthetic and real-world video datasets and experimentally compare models withvarying temporal receptive fields. We find that short receptive fields arerobust to sub-action order changes, while larger temporal receptive fields aresensitive to the sub-action order.</description><author>Ombretta Strafforello, Xin Liu, Klamer Schutte, Jan van Gemert</author><pubDate>Tue, 22 Aug 2023 08:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11249v1</guid></item><item><title>IndGIC: Supervised Action Recognition under Low Illumination</title><link>http://arxiv.org/abs/2308.15345v1</link><description>Technologies of human action recognition in the dark are gaining more andmore attention as huge demand in surveillance, motion control andhuman-computer interaction. However, because of limitation in image enhancementmethod and low-lighting video datasets, e.g. labeling cost, existing methodsmeet some problems. Some video-based approached are effect and efficient inspecific datasets but cannot generalize to most cases while others methodsusing multiple sensors rely heavily to prior knowledge to deal with noisynature from video stream. In this paper, we proposes action recognition methodusing deep multi-input network. Furthermore, we proposed a Independent GammaIntensity Corretion (Ind-GIC) to enhance poor-illumination video, generatingone gamma for one frame to increase enhancement performance. To prove ourmethod is effective, there is some evaluation and comparison between our methodand existing methods. Experimental results show that our model achieves highaccuracy in on ARID dataset.</description><author>Jingbo Zeng</author><pubDate>Tue, 29 Aug 2023 15:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15345v1</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</title><link>http://arxiv.org/abs/2307.09238v1</link><description>As collaborative robots (cobots) continue to gain popularity in industrialmanufacturing, effective human-robot collaboration becomes crucial. Cobotsshould be able to recognize human actions to assist with assembly tasks and actautonomously. To achieve this, skeleton-based approaches are often used due totheir ability to generalize across various people and environments. Althoughbody skeleton approaches are widely used for action recognition, they may notbe accurate enough for assembly actions where the worker's fingers and handsplay a significant role. To address this limitation, we propose a method inwhich less detailed body skeletons are combined with highly detailed handskeletons. We investigate CNNs and transformers, the latter of which areparticularly adept at extracting and combining important information from bothskeleton types using attention. This paper demonstrates the effectiveness ofour proposed approach in enhancing action recognition in assembly scenarios.</description><author>Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Tue, 18 Jul 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09238v1</guid></item><item><title>How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</title><link>http://arxiv.org/abs/2306.05844v1</link><description>As the use of collaborative robots (cobots) in industrial manufacturingcontinues to grow, human action recognition for effective human-robotcollaboration becomes increasingly important. This ability is crucial forcobots to act autonomously and assist in assembly tasks. Recently,skeleton-based approaches are often used as they tend to generalize better todifferent people and environments. However, when processing skeletons alone,information about the objects a human interacts with is lost. Therefore, wepresent a novel approach of integrating object information into skeleton-basedaction recognition. We enhance two state-of-the-art methods by treating objectcenters as further skeleton joints. Our experiments on the assembly datasetIKEA ASM show that our approach improves the performance of thesestate-of-the-art methods to a large extent when combining skeleton joints withobjects predicted by a state-of-the-art instance segmentation model. Ourresearch sheds light on the benefits of combining skeleton joints with objectinformation for human action recognition in assembly tasks. We analyze theeffect of the object detector on the combination for action classification anddiscuss the important factors that must be taken into account.</description><author>Dustin Aganian, Mona Köhler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Fri, 09 Jun 2023 13:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05844v1</guid></item><item><title>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</title><link>http://arxiv.org/abs/2306.11046v1</link><description>Existing skeleton-based action recognition methods typically follow acentralized learning paradigm, which can pose privacy concerns when exposinghuman-related videos. Federated Learning (FL) has attracted much attention dueto its outstanding advantages in privacy-preserving. However, directly applyingFL approaches to skeleton videos suffers from unstable training. In this paper,we investigate and discover that the heterogeneous human topology graphstructure is the crucial factor hindering training stability. To address thislimitation, we pioneer a novel Federated Skeleton-based Action Recognition(FSAR) paradigm, which enables the construction of a globally generalized modelwithout accessing local sensitive data. Specifically, we introduce an AdaptiveTopology Structure (ATS), separating generalization and personalization bylearning a domain-invariant topology shared across clients and adomain-specific topology decoupled from global model aggregation.Furthermore,we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancybetween clients and server caused by distinct updating patterns throughaligning shallow block-wise motion features. Extensive experiments on multipledatasets demonstrate that FSAR outperforms state-of-the-art FL-based methodswhile inherently protecting privacy.</description><author>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si</author><pubDate>Mon, 19 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11046v1</guid></item><item><title>DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition</title><link>http://arxiv.org/abs/2308.12501v1</link><description>Graph Convolutional Networks (GCNs) have been widely used in skeleton-basedhuman action recognition. In GCN-based methods, the spatio-temporal graph isfundamental for capturing motion patterns. However, existing approaches ignorethe physical dependency and synchronized spatio-temporal correlations betweenjoints, which limits the representation capability of GCNs. To solve theseproblems, we construct the directed diffusion graph for action modeling andintroduce the activity partition strategy to optimize the weight sharingmechanism of graph convolution kernels. In addition, we present thespatio-temporal synchronization encoder to embed synchronized spatio-temporalsemantics. Finally, we propose Directed Diffusion Graph Convolutional Network(DD-GCN) for action recognition, and the experiments on three public datasets:NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-artperformance of our method.</description><author>Chang Li, Qian Huang, Yingchi Mao</author><pubDate>Thu, 24 Aug 2023 02:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12501v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v1</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Wed, 30 Aug 2023 14:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v1</guid></item><item><title>SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network</title><link>http://arxiv.org/abs/2306.17574v1</link><description>Recent advancements in technology have expanded the possibilities of humanaction recognition by leveraging 3D data, which offers a richer representationof actions through the inclusion of depth information, enabling more accurateanalysis of spatial and temporal characteristics. However, 3D human actionrecognition is a challenging task due to the irregularity and Disarrangement ofthe data points in action sequences. In this context, we present our novelmodel for human action recognition from fixed topology mesh sequences based onSpiral Auto-encoder and Transformer Network, namely SpATr. The proposed methodfirst disentangles space and time in the mesh sequences. Then, an auto-encoderis utilized to extract spatial geometrical features, and tiny transformer isused to capture the temporal evolution of the sequence. Previous methods eitheruse 2D depth images, sample skeletons points or they require a huge amount ofmemory leading to the ability to process short sequences only. In this work, weshow competitive recognition rate and high memory efficiency by building ourauto-encoder based on spiral convolutions, which are light weight convolutiondirectly applied to mesh data with fixed topologies, and by modeling temporalevolution using a attention, that can handle large sequences. The proposedmethod is evaluated on on two 3D human action datasets: MoVi and BMLrub fromthe Archive of Motion Capture As Surface Shapes (AMASS). The results analysisshows the effectiveness of our method in 3D human action recognition whilemaintaining high memory efficiency. The code will soon be made publiclyavailable.</description><author>Hamza Bouzid, Lahoucine Ballihi</author><pubDate>Fri, 30 Jun 2023 12:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17574v1</guid></item><item><title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</title><link>http://arxiv.org/abs/2211.13466v3</link><description>Contrastive learning has been proven beneficial for self-supervisedskeleton-based action recognition. Most contrastive learning methods utilizecarefully designed augmentations to generate different movement patterns ofskeletons for the same semantics. However, it is still a pending issue to applystrong augmentations, which distort the images/skeletons' structures and causesemantic loss, due to their resulting unstable training. In this paper, weinvestigate the potential of adopting strong augmentations and propose ageneral hierarchical consistent contrastive learning framework (HiCLR) forskeleton-based action recognition. Specifically, we first design a gradualgrowing augmentation policy to generate multiple ordered positive pairs, whichguide to achieve the consistency of the learned representation from differentviews. Then, an asymmetric loss is proposed to enforce the hierarchicalconsistency via a directional clustering operation in the feature space,pulling the representations from strongly augmented views closer to those fromweakly augmented views for better generalizability. Meanwhile, we propose andevaluate three kinds of strong augmentations for 3D skeletons to demonstratethe effectiveness of our method. Extensive experiments show that HiCLRoutperforms the state-of-the-art methods notably on three large-scale datasets,i.e., NTU60, NTU120, and PKUMMD.</description><author>Jiahang Zhang, Lilang Lin, Jiaying Liu</author><pubDate>Mon, 10 Jul 2023 11:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13466v3</guid></item><item><title>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</title><link>http://arxiv.org/abs/2308.07571v1</link><description>This paper presents Ske2Grid, a new representation learning framework forimproved skeleton-based action recognition. In Ske2Grid, we define a regularconvolution operation upon a novel grid representation of human skeleton, whichis a compact image-like grid patch constructed and learned through three noveldesigns. Specifically, we propose a graph-node index transform (GIT) toconstruct a regular grid patch through assigning the nodes in the skeletongraph one by one to the desired grid cells. To ensure that GIT is a bijectionand enrich the expressiveness of the grid representation, an up-samplingtransform (UPT) is learned to interpolate the skeleton graph nodes for fillingthe grid patch to the full. To resolve the problem when the one-step UPT isaggressive and further exploit the representation capability of the grid patchwith increasing spatial size, a progressive learning strategy (PLS) is proposedwhich decouples the UPT into multiple steps and aligns them to multiple pairedGITs through a compact cascaded design learned progressively. We constructnetworks upon prevailing graph convolution networks and conduct experiments onsix mainstream skeleton-based action recognition datasets. Experiments showthat our Ske2Grid significantly outperforms existing GCN-based solutions underdifferent benchmark settings, without bells and whistles. Code and models areavailable at https://github.com/OSVAI/Ske2Grid</description><author>Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</author><pubDate>Tue, 15 Aug 2023 05:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07571v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition</title><link>http://arxiv.org/abs/2305.18479v1</link><description>3D Convolutional Neural Networks are gaining increasing attention fromresearchers and practitioners and have found applications in many domains, suchas surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval. However, their widespread adoption is hindered by their highcomputational and memory requirements, especially when resource-constrainedsystems are targeted. This paper addresses the problem of mapping X3D, astate-of-the-art model in Human Action Recognition that achieves accuracy of95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflowgenerates an optimised stream-based hardware system, taking into account theavailable resources and off-chip memory characteristics of the FPGA device. Thegenerated designs push further the current performance-accuracy pareto front,and enable for the first time the targeting of such complex model architecturesfor the Human Action Recognition task.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18479v1</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</title><link>http://arxiv.org/abs/2308.03741v1</link><description>In line with the human capacity to perceive the world by simultaneouslyprocessing and integrating high-dimensional inputs from multiple modalitieslike vision and audio, we propose a novel model, MAiVAR-T (MultimodalAudio-Image to Video Action Recognition Transformer). This model employs anintuitive approach for the combination of audio-image and video modalities,with a primary aim to escalate the effectiveness of multimodal human actionrecognition (MHAR). At the core of MAiVAR-T lies the significance of distillingsubstantial representations from the audio modality and transmuting these intothe image domain. Subsequently, this audio-image depiction is fused with thevideo modality to formulate a unified representation. This concerted approachstrives to exploit the contextual richness inherent in both audio and videomodalities, thereby promoting action recognition. In contrast to existingstate-of-the-art strategies that focus solely on audio or video modalities,MAiVAR-T demonstrates superior performance. Our extensive empirical evaluationsconducted on a benchmark action recognition dataset corroborate the model'sremarkable performance. This underscores the potential enhancements derivedfrom integrating audio and video modalities for action recognition purposes.</description><author>Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</author><pubDate>Tue, 01 Aug 2023 12:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03741v1</guid></item><item><title>Universal Prototype Transport for Zero-Shot Action Recognition and Localization</title><link>http://arxiv.org/abs/2203.03971v2</link><description>This work addresses the problem of recognizing action categories in videoswhen no training examples are available. The current state-of-the-art enablessuch a zero-shot recognition by learning universal mappings from videos to asemantic space, either trained on large-scale seen actions or on objects. Whileeffective, we find that universal action and object mappings are biased tospecific regions in the semantic space. These biases lead to a fundamentalproblem: many unseen action categories are simply never inferred duringtesting. For example on UCF-101, a quarter of the unseen actions are out ofreach with a state-of-the-art universal action model. To that end, this paperintroduces universal prototype transport for zero-shot action recognition. Themain idea is to re-position the semantic prototypes of unseen actions bymatching them to the distribution of all test videos. For universal actionmodels, we propose to match distributions through a hyperspherical optimaltransport from unseen action prototypes to the set of all projected testvideos. The resulting transport couplings in turn determine the targetprototype for each unseen action. Rather than directly using the targetprototype as final result, we re-position unseen action prototypes along thegeodesic spanned by the original and target prototypes as a form of semanticregularization. For universal object models, we outline a variant that definestarget prototypes based on an optimal transport between unseen actionprototypes and object prototypes. Empirically, we show that universal prototypetransport diminishes the biased selection of unseen action prototypes andboosts both universal action and object models for zero-shot classification andspatio-temporal localization.</description><author>Pascal Mettes</author><pubDate>Tue, 01 Aug 2023 10:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03971v2</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v1</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Wed, 14 Jun 2023 20:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v1</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v2</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Thu, 24 Aug 2023 11:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v2</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Action Recognition with Multi-stream Motion Modeling and Mutual Information Maximization</title><link>http://arxiv.org/abs/2306.07576v1</link><description>Action recognition has long been a fundamental and intriguing problem inartificial intelligence. The task is challenging due to the high dimensionalitynature of an action, as well as the subtle motion details to be considered.Current state-of-the-art approaches typically learn from articulated motionsequences in the straightforward 3D Euclidean space. However, the vanillaEuclidean space is not efficient for modeling important motion characteristicssuch as the joint-wise angular acceleration, which reveals the driving forcebehind the motion. Moreover, current methods typically attend to each channelequally and lack theoretical constrains on extracting task-relevant featuresfrom the input. In this paper, we seek to tackle these challenges from three aspects: (1) Wepropose to incorporate an acceleration representation, explicitly modeling thehigher-order variations in motion. (2) We introduce a novel Stream-GCN networkequipped with multi-stream components and channel attention, where differentrepresentations (i.e., streams) supplement each other towards a more preciseaction recognition while attention capitalizes on those important channels. (3)We explore feature-level supervision for maximizing the extraction oftask-relevant information and formulate this into a mutual information loss.Empirically, our approach sets the new state-of-the-art performance on threebenchmark datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our code isanonymously released at https://github.com/ActionR-Group/Stream-GCN, hoping toinspire the community.</description><author>Yuheng Yang, Haipeng Chen, Zhenguang Liu, Yingda Lyu, Beibei Zhang, Shuang Wu, Zhibo Wang, Kui Ren</author><pubDate>Tue, 13 Jun 2023 07:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07576v1</guid></item><item><title>FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes</title><link>http://arxiv.org/abs/2306.10858v1</link><description>A typical task in the field of video understanding is hand actionrecognition, which has a wide range of applications. Existing works eithermainly focus on full-body actions, or the defined action categories arerelatively coarse-grained. In this paper, we propose FHA-Kitchens, a noveldataset of fine-grained hand actions in kitchen scenes. In particular, we focuson human hand interaction regions and perform deep excavation to further refinehand action information and interaction regions. Our FHA-Kitchens datasetconsists of 2,377 video clips and 30,047 images collected from 8 differenttypes of dishes, and all hand interaction regions in each image are labeledwith high-quality fine-grained action classes and bounding boxes. We representthe action information in each hand interaction region as a triplet, resultingin a total of 878 action triplets. Based on the constructed dataset, webenchmark representative action recognition and detection models on thefollowing three tracks: (1) supervised learning for hand interaction region andobject detection, (2) supervised learning for fine-grained hand actionrecognition, and (3) intra- and inter-class domain generalization for handinteraction region detection. The experimental results offer compellingempirical evidence that highlights the challenges inherent in fine-grained handaction recognition, while also shedding light on potential avenues for futureresearch, particularly in relation to pre-training strategy, model design, anddomain generalization. The dataset will be released athttps://github.com/tingZ123/FHA-Kitchens.</description><author>Ting Zhe, Yongqian Li, Jing Zhang, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao</author><pubDate>Mon, 19 Jun 2023 12:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10858v1</guid></item><item><title>Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration</title><link>http://arxiv.org/abs/2307.14866v1</link><description>Training an effective video action recognition model poses significantcomputational challenges, particularly under limited resource budgets. Currentmethods primarily aim to either reduce model size or utilize pre-trainedmodels, limiting their adaptability to various backbone architectures. Thispaper investigates the issue of over-sampled frames, a prevalent problem inmany approaches yet it has received relatively little attention. Despite theuse of fewer frames being a potential solution, this approach often results ina substantial decline in performance. To address this issue, we propose a novelmethod to restore the intermediate features for two sparsely sampled andadjacent video frames. This feature restoration technique brings a negligibleincrease in computational requirements compared to resource-intensive imageencoders, such as ViT. To evaluate the effectiveness of our method, we conductextensive experiments on four public datasets, including Kinetics-400,ActivityNet, UCF-101, and HMDB-51. With the integration of our method, theefficiency of three commonly used baselines has been improved by over 50%, witha mere 0.5% reduction in recognition accuracy. In addition, our method alsosurprisingly helps improve the generalization ability of the models underzero-shot settings.</description><author>Harry Cheng, Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Mohan Kankanhalli</author><pubDate>Thu, 27 Jul 2023 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14866v1</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v1</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Si-Fan Zhang, Wen-Yue Chen, Ning Zhou, Hao Liu, Gui-Hong Lao</author><pubDate>Thu, 06 Jul 2023 03:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v1</guid></item><item><title>A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023</title><link>http://arxiv.org/abs/2307.06569v1</link><description>In this technical report, we present our findings from a study conducted onthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for ActionRecognition. Our research focuses on the innovative application of adifferentiable logic loss in the training to leverage the co-occurrencerelations between verb and noun, as well as the pre-trained Large LanguageModels (LLMs) to generate the logic rules for the adaptation to unseen actionlabels. Specifically, the model's predictions are treated as the truthassignment of a co-occurrence logic formula to compute the logic loss, whichmeasures the consistency between the predictions and the logic constraints. Byusing the verb-noun co-occurrence matrix generated from the dataset, we observea moderate improvement in model performance compared to our baseline framework.To further enhance the model's adaptability to novel action labels, weexperiment with rules generated using GPT-3.5, which leads to a slight decreasein performance. These findings shed light on the potential and challenges ofincorporating differentiable logic and LLMs for knowledge extraction inunsupervised domain adaptation for action recognition. Our final submission(entitled `NS-LLM') achieved the first place in terms of top-1 actionrecognition accuracy.</description><author>Yi Cheng, Ziwei Xu, Fen Fang, Dongyun Lin, Hehe Fan, Yongkang Wong, Ying Sun, Mohan Kankanhalli</author><pubDate>Thu, 13 Jul 2023 06:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06569v1</guid></item><item><title>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</title><link>http://arxiv.org/abs/2308.11070v1</link><description>Deep neural networks (DNNs) have achieved tremendous success in variousapplications including video action recognition, yet remain vulnerable tobackdoor attacks (Trojans). The backdoor-compromised model will mis-classify tothe target class chosen by the attacker when a test instance (from a non-targetclass) is embedded with a specific trigger, while maintaining high accuracy onattack-free instances. Although there are extensive studies on backdoor attacksagainst image data, the susceptibility of video-based systems under backdoorattacks remains largely unexplored. Current studies are direct extensions ofapproaches proposed for image data, e.g., the triggers are\textbf{independently} embedded within the frames, which tend to be detectableby existing defenses. In this paper, we introduce a \textit{simple} yet\textit{effective} backdoor attack against video data. Our proposed attack,adding perturbations in a transformed domain, plants an \textbf{imperceptible,temporally distributed} trigger across the video frames, and is shown to beresilient to existing defensive strategies. The effectiveness of the proposedattack is demonstrated by extensive experiments with various well-known modelson two video recognition benchmarks, UCF101 and HMDB51, and a sign languagerecognition benchmark, Greek Sign Language (GSL) dataset. We delve into theimpact of several influential factors on our proposed attack and identify anintriguing effect termed "collateral damage" through extensive studies.</description><author>Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</author><pubDate>Mon, 21 Aug 2023 23:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11070v1</guid></item><item><title>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2208.10741v3</link><description>Graph convolutional networks (GCNs) are the most commonly used methods forskeleton-based action recognition and have achieved remarkable performance.Generating adjacency matrices with semantically meaningful edges isparticularly important for this task, but extracting such edges is challengingproblem. To solve this, we propose a hierarchically decomposed graphconvolutional network (HD-GCN) architecture with a novel hierarchicallydecomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes everyjoint node into several sets to extract major structurally adjacent and distantedges, and uses them to construct an HD-Graph containing those edges in thesame semantic spaces of a human skeleton. In addition, we introduce anattention-guided hierarchy aggregation (A-HA) module to highlight the dominanthierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-wayensemble method, which uses only joint and bone stream without any motionstream. The proposed model is evaluated and achieves state-of-the-artperformance on four large, popular datasets. Finally, we demonstrate theeffectiveness of our model with various comparative experiments.</description><author>Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 10:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10741v3</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</title><link>http://arxiv.org/abs/2303.08914v2</link><description>Large scale Vision-Language (VL) models have shown tremendous success inaligning representations between visual and text modalities. This enablesremarkable progress in zero-shot recognition, image generation &amp; editing, andmany other exciting tasks. However, VL models tend to over-represent objectswhile paying much less attention to verbs, and require additional tuning onvideo data for best zero-shot action recognition performance. While previouswork relied on large-scale, fully-annotated data, in this work we propose anunsupervised approach. We adapt a VL model for zero-shot and few-shot actionrecognition using a collection of unlabeled videos and an unpaired actiondictionary. Based on that, we leverage Large Language Models and VL models tobuild a text bag for each unlabeled video via matching, text expansion andcaptioning. We use those bags in a Multiple Instance Learning setup to adapt animage-text backbone to video data. Although finetuned on unlabeled video data,our resulting models demonstrate high transferability to numerous unseenzero-shot downstream tasks, improving the base VL model performance by up to14\%, and even comparing favorably to fully-supervised baselines in bothzero-shot and few-shot video recognition transfer. The code will be releasedlater at \url{https://github.com/wlin-at/MAXI}.</description><author>Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, Horst Bischof</author><pubDate>Sat, 22 Jul 2023 10:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08914v2</guid></item><item><title>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2307.07791v1</link><description>Considering the instance-level discriminative ability, contrastive learningmethods, including MoCo and SimCLR, have been adapted from the original imagerepresentation learning task to solve the self-supervised skeleton-based actionrecognition task. These methods usually use multiple data streams (i.e., joint,motion, and bone) for ensemble learning, meanwhile, how to construct adiscriminative feature space within a single stream and effectively aggregatethe information from multiple streams remains an open problem. To this end, wefirst apply a new contrastive learning method called BYOL to learn fromskeleton data and formulate SkeletonBYOL as a simple yet effective baseline forself-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, wefurther present a joint Adversarial and Collaborative Learning (ACL) framework,which combines Cross-Model Adversarial Learning (CMAL) and Cross-StreamCollaborative Learning (CSCL). Specifically, CMAL learns single-streamrepresentation by cross-model adversarial loss to obtain more discriminativefeatures. To aggregate and interact with multi-stream information, CSCL isdesigned by generating similarity pseudo label of ensemble learning assupervision and guiding feature generation for individual streams. Exhaustiveexperiments on three datasets verify the complementary properties between CMALand CSCL and also verify that our method can perform favorably againststate-of-the-art methods using various evaluation protocols. Our code andmodels are publicly available at \url{https://github.com/Levigty/ACL}.</description><author>Tianyu Guo, Mengyuan Liu, Hong Liu, Wenhao Li, Jingwen Guo, Tao Wang, Yidi Li</author><pubDate>Sat, 15 Jul 2023 13:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07791v1</guid></item><item><title>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2307.01985v1</link><description>In the research field of few-shot learning, the main difference betweenimage-based and video-based is the additional temporal dimension for videos. Inrecent years, many approaches for few-shot action recognition have followed themetric-based methods, especially, since some works use the Transformer to getthe cross-attention feature of the videos or the enhanced prototype, and theresults are competitive. However, they do not mine enough information from theTransformer because they only focus on the feature of a single level. In ourpaper, we have addressed this problem. We propose an end-to-end method named"Task-Specific Alignment and Multiple Level Transformer Network (TSA-MLT)". Inour model, the Multiple Level Transformer focuses on the multiple-level featureof the support video and query video. Especially before Multiple LevelTransformer, we use task-specific TSA to filter unimportant or misleadingframes as a pre-processing. Furthermore, we adopt a fusion loss using two kindsof distance, the first is L2 sequence distance, which focuses on temporal orderalignment. The second one is Optimal transport distance, which focuses onmeasuring the gap between the appearance and semantics of the videos. Using asimple fusion network, we fuse the two distances element-wise, then use thecross-entropy loss as our fusion loss. Extensive experiments show our methodachieves state-of-the-art results on the HMDB51 and UCF101 datasets and acompetitive result on the benchmark of Kinetics and something-2-something V2datasets. Our code will be available at the URL:https://github.com/cofly2014/tsa-mlt.git</description><author>Fei Guo, Li Zhu, YiWang Wang</author><pubDate>Wed, 05 Jul 2023 03:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01985v1</guid></item><item><title>Human Action Recognition in Still Images Using ConViT</title><link>http://arxiv.org/abs/2307.08994v1</link><description>Understanding the relationship between different parts of the image plays acrucial role in many visual recognition tasks. Despite the fact thatConvolutional Neural Networks (CNNs) have demonstrated impressive results indetecting single objects, they lack the capability to extract the relationshipbetween various regions of an image, which is a crucial factor in human actionrecognition. To address this problem, this paper proposes a new module thatfunctions like a convolutional layer using Vision Transformer (ViT). Theproposed action recognition model comprises two components: the first part is adeep convolutional network that extracts high-level spatial features from theimage, and the second component of the model utilizes a Vision Transformer thatextracts the relationship between various regions of the image using thefeature map generated by the CNN output. The proposed model has been evaluatedon the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%mAP and 91.5% mAP results, respectively, which are promising compared to otherstate-of-the-art methods.</description><author>Seyed Rohollah Hosseyni, Hasan Taheri, Sanaz Seyedin, Ali Ahmad Rahmani</author><pubDate>Tue, 18 Jul 2023 07:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08994v1</guid></item><item><title>Optimizing ViViT Training: Time and Memory Reduction for Action Recognition</title><link>http://arxiv.org/abs/2306.04822v1</link><description>In this paper, we address the challenges posed by the substantial trainingtime and memory consumption associated with video transformers, focusing on theViViT (Video Vision Transformer) model, in particular the Factorised Encoderversion, as our baseline for action recognition tasks. The factorised encodervariant follows the late-fusion approach that is adopted by many state of theart approaches. Despite standing out for its favorable speed/accuracy tradeoffsamong the different variants of ViViT, its considerable training time andmemory requirements still pose a significant barrier to entry. Our method isdesigned to lower this barrier and is based on the idea of freezing the spatialtransformer during training. This leads to a low accuracy model if naivelydone. But we show that by (1) appropriately initializing the temporaltransformer (a module responsible for processing temporal information) (2)introducing a compact adapter model connecting frozen spatial representations((a module that selectively focuses on regions of the input image) to thetemporal transformer, we can enjoy the benefits of freezing the spatialtransformer without sacrificing accuracy. Through extensive experimentationover 6 benchmarks, we demonstrate that our proposed training strategysignificantly reduces training costs (by $\sim 50\%$) and memory consumptionwhile maintaining or slightly improving performance by up to 1.79\% compared tothe baseline model. Our approach additionally unlocks the capability to utilizelarger image transformer models as our spatial transformer and access moreframes with the same memory consumption.</description><author>Shreyank N Gowda, Anurag Arnab, Jonathan Huang</author><pubDate>Thu, 08 Jun 2023 00:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04822v1</guid></item><item><title>Multimodal Distillation for Egocentric Action Recognition</title><link>http://arxiv.org/abs/2307.07483v2</link><description>The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views. We release our code athttps://github.com/gorjanradevski/multimodal-distillation.</description><author>Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</author><pubDate>Tue, 18 Jul 2023 10:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07483v2</guid></item><item><title>Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</title><link>http://arxiv.org/abs/2306.01075v1</link><description>Accurate understanding and prediction of human behaviors are criticalprerequisites for autonomous vehicles, especially in highly dynamic andinteractive scenarios such as intersections in dense urban areas. In this work,we aim at identifying crossing pedestrians and predicting their futuretrajectories. To achieve these goals, we not only need the context informationof road geometry and other traffic participants but also need fine-grainedinformation of the human pose, motion and activity, which can be inferred fromhuman keypoints. In this paper, we propose a novel multi-task learningframework for pedestrian crossing action recognition and trajectory prediction,which utilizes 3D human keypoints extracted from raw sensor data to capturerich information on human pose and activity. Moreover, we propose to apply twoauxiliary tasks and contrastive learning to enable auxiliary supervisions toimprove the learned keypoints representation, which further enhances theperformance of major tasks. We validate our approach on a large-scale in-housedataset, as well as a public benchmark dataset, and show that our approachachieves state-of-the-art performance on a wide range of evaluation metrics.The effectiveness of each model component is validated in a detailed ablationstudy.</description><author>Jiachen Li, Xinwei Shi, Feiyu Chen, Jonathan Stroud, Zhishuai Zhang, Tian Lan, Junhua Mao, Jeonhyung Kang, Khaled S. Refaat, Weilong Yang, Eugene Ie, Congcong Li</author><pubDate>Thu, 01 Jun 2023 19:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01075v1</guid></item><item><title>Spiking Neural Networks for event-based action recognition: A new task to understand their advantage</title><link>http://arxiv.org/abs/2209.14915v2</link><description>Spiking Neural Networks (SNN) are characterised by their unique temporaldynamics, but the properties and advantages of such computations are still notwell understood. In order to provide answers, in this work we demonstrate howSpiking neurons can enable temporal feature extraction in feed-forward neuralnetworks without the need for recurrent synapses, showing how theirbio-inspired computing principles can be successfully exploited beyond energyefficiency gains and evidencing their differences with respect to conventionalneurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain(DVS-GC), which allows, for the first time, to evaluate the perception oftemporal dependencies in a real event-based action recognition dataset. Ourstudy proves how the widely used DVS Gesture benchmark could be solved bynetworks without temporal feature extraction, unlike the new DVS-GC whichdemands an understanding of the ordering of the events. Furthermore, this setupallowed us to unveil the role of the leakage rate in spiking neurons fortemporal processing tasks and demonstrated the benefits of "hard reset"mechanisms. Additionally, we also show how time-dependent weights andnormalization can lead to understanding order by means of temporal attention.</description><author>Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl</author><pubDate>Tue, 08 Aug 2023 11:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14915v2</guid></item><item><title>High-Performance Inference Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2305.18710v1</link><description>Recently, significant achievements have been made in skeleton-based humanaction recognition with the emergence of graph convolutional networks (GCNs).However, the state-of-the-art (SOTA) models used for this task focus onconstructing more complex higher-order connections between joint nodes todescribe skeleton information, which leads to complex inference processes andhigh computational costs, resulting in reduced model's practicality. To addressthe slow inference speed caused by overly complex model structures, weintroduce re-parameterization and over-parameterization techniques to GCNs, andpropose two novel high-performance inference graph convolutional networks,namely HPI-GCN-RP and HPI-GCN-OP. HPI-GCN-RP uses re-parameterization techniqueto GCNs to achieve a higher inference speed with competitive model performance.HPI-GCN-OP further utilizes over-parameterization technique to bringsignificant performance improvement with inference speed slightly decreased.Experimental results on the two skeleton-based action recognition datasetsdemonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves anaccuracy of 93% on the cross-subject split of the NTU-RGB+D 60 dataset, and90.1% on the cross-subject benchmark of the NTU-RGB+D 120 dataset and is 4.5times faster than HD-GCN at the same accuracy.</description><author>Ziao Li, Junyi Wang, Guhong Nie</author><pubDate>Tue, 30 May 2023 04:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18710v1</guid></item><item><title>High-order Tensor Pooling with Attention for Action Recognition</title><link>http://arxiv.org/abs/2110.05216v2</link><description>We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.</description><author>Piotr Koniusz, Lei Wang, Ke Sun</author><pubDate>Thu, 20 Jul 2023 15:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05216v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v2</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Mon, 19 Jun 2023 09:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v1</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Wed, 24 May 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v1</guid></item><item><title>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</title><link>http://arxiv.org/abs/2307.07469v1</link><description>Recognizing interactive action plays an important role in human-robotinteraction and collaboration. Previous methods use late fusion andco-attention mechanism to capture interactive relations, which have limitedlearning capability or inefficiency to adapt to more interacting entities. Withassumption that priors of each entity are already known, they also lackevaluations on a more general setting addressing the diversity of subjects. Toaddress these problems, we propose an Interactive Spatiotemporal TokenAttention Network (ISTA-Net), which simultaneously model spatial, temporal, andinteractive relations. Specifically, our network contains a tokenizer topartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way torepresent motions of multiple diverse entities. By extending the entitydimension, ISTs provide better interactive representations. To jointly learnalong three dimensions in ISTs, multi-head self-attention blocks integratedwith 3D convolutions are designed to capture inter-token correlations. Whenmodeling correlations, a strict entity ordering is usually irrelevant forrecognizing interactive actions. To this end, Entity Rearrangement is proposedto eliminate the orderliness in ISTs for interchangeable entities. Extensiveexperiments on four datasets verify the effectiveness of ISTA-Net byoutperforming state-of-the-art methods. Our code is publicly available athttps://github.com/Necolizer/ISTA-Net</description><author>Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu</author><pubDate>Fri, 14 Jul 2023 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07469v1</guid></item><item><title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</title><link>http://arxiv.org/abs/2307.07768v2</link><description>Classifying player actions from soccer videos is a challenging problem, whichhas become increasingly important in sports analytics over the years. Moststate-of-the-art methods employ highly complex offline networks, which makes itdifficult to deploy such models in resource constrained scenarios. Here, inthis paper we propose a novel end-to-end knowledge distillation based transferlearning network pre-trained on the Kinetics400 dataset and then performextensive analysis on the learned framework by introducing a unique lossparameterization. We also introduce a new dataset named SoccerDB1 containing448 videos and consisting of 4 diverse classes each of players playing soccer.Furthermore, we introduce an unique loss parameter that help us linearly weighthe extent to which the predictions of each network are utilized. Finally, wealso perform a thorough performance study using various changedhyperparameters. We also benchmark the first classification results on the newSoccerDB1 dataset obtaining 67.20% validation accuracy. Apart fromoutperforming prior arts significantly, our model also generalizes to newdatasets easily. The dataset has been made publicly available at:https://bit.ly/soccerdb1</description><author>Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</author><pubDate>Sat, 22 Jul 2023 05:47:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07768v2</guid></item><item><title>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</title><link>http://arxiv.org/abs/2307.07768v1</link><description>Classifying player actions from soccer videos is a challenging problem, whichhas become increasingly important in sports analytics over the years. Moststate-of-the-art methods employ highly complex offline networks, which makes itdifficult to deploy such models in resource constrained scenarios. Here, inthis paper we propose a novel end-to-end knowledge distillation based transferlearning network pre-trained on the Kinetics400 dataset and then performextensive analysis on the learned framework by introducing a unique lossparameterization. We also introduce a new dataset named SoccerDB1 containing448 videos and consisting of 4 diverse classes each of players playing soccer.Furthermore, we introduce an unique loss parameter that help us linearly weighthe extent to which the predictions of each network are utilized. Finally, wealso perform a thorough performance study using various changedhyperparameters. We also benchmark the first classification results on the newSoccerDB1 dataset obtaining 67.20% validation accuracy. Apart fromoutperforming prior arts significantly, our model also generalizes to newdatasets easily. The dataset has been made publicly available at:https://bit.ly/soccerdb1</description><author>Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</author><pubDate>Sat, 15 Jul 2023 11:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07768v1</guid></item><item><title>Multi-Dimensional Refinement Graph Convolutional Network with Robust Decouple Loss for Fine-Grained Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2306.15321v1</link><description>Graph convolutional networks have been widely used in skeleton-based actionrecognition. However, existing approaches are limited in fine-grained actionrecognition due to the similarity of inter-class data. Moreover, the noisy datafrom pose extraction increases the challenge of fine-grained recognition. Inthis work, we propose a flexible attention block called Channel-VariableSpatial-Temporal Attention (CVSTA) to enhance the discriminative power ofspatial-temporal joints and obtain a more compact intra-class featuredistribution. Based on CVSTA, we construct a Multi-Dimensional Refinement GraphConvolutional Network (MDR-GCN), which can improve the discrimination amongchannel-, joint- and frame-level features for fine-grained actions.Furthermore, we propose a Robust Decouple Loss (RDL), which significantlyboosts the effect of the CVSTA and reduces the impact of noise. The proposedmethod combining MDR-GCN with RDL outperforms the known state-of-the-artskeleton-based approaches on fine-grained datasets, FineGym99 and FSD-10, andalso on the coarse dataset NTU-RGB+D X-view version.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Jin-Rong Zhang, Kai-Yuan Liu, Si-Fan Zhang, Fei-Long Wang, Gao Huang</author><pubDate>Tue, 27 Jun 2023 10:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15321v1</guid></item><item><title>Ensemble Modeling for Multimodal Visual Action Recognition</title><link>http://arxiv.org/abs/2308.05430v1</link><description>In this work, we propose an ensemble modeling approach for multimodal actionrecognition. We independently train individual modality models using a variantof focal loss tailored to handle the long-tailed distribution of the MECCANO[21] dataset. Based on the underlying principle of focal loss, which capturesthe relationship between tail (scarce) classes and their predictiondifficulties, we propose an exponentially decaying variant of focal loss forour current task. It initially emphasizes learning from the hard misclassifiedexamples and gradually adapts to the entire range of examples in the dataset.This annealing process encourages the model to strike a balance betweenfocusing on the sparse set of hard samples, while still leveraging theinformation provided by the easier ones. Additionally, we opt for the latefusion strategy to combine the resultant probability distributions from RGB andDepth modalities for final action prediction. Experimental evaluations on theMECCANO dataset demonstrate the effectiveness of our approach.</description><author>Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah</author><pubDate>Thu, 10 Aug 2023 09:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05430v1</guid></item><item><title>Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching</title><link>http://arxiv.org/abs/2308.09346v1</link><description>Class prototype construction and matching are core aspects of few-shot actionrecognition. Previous methods mainly focus on designing spatiotemporal relationmodeling modules or complex temporal alignment algorithms. Despite thepromising results, they ignored the value of class prototype construction andmatching, leading to unsatisfactory performance in recognizing similarcategories in every task. In this paper, we propose GgHM, a new framework withGraph-guided Hybrid Matching. Concretely, we learn task-oriented features bythe guidance of a graph neural network during class prototype construction,optimizing the intra- and inter-class feature correlation explicitly. Next, wedesign a hybrid matching strategy, combining frame-level and tuple-levelmatching to classify videos with multivariate styles. We additionally propose alearnable dense temporal modeling module to enhance the video feature temporalrepresentation to build a more solid foundation for the matching process. GgHMshows consistent improvements over other challenging baselines on severalfew-shot datasets, demonstrating the effectiveness of our method. The code willbe publicly available at https://github.com/jiazheng-xing/GgHM.</description><author>Jiazheng Xing, Mengmeng Wang, Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu</author><pubDate>Fri, 18 Aug 2023 08:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09346v1</guid></item><item><title>Unlimited Knowledge Distillation for Action Recognition in the Dark</title><link>http://arxiv.org/abs/2308.09327v1</link><description>Dark videos often lose essential information, which causes the knowledgelearned by networks is not enough to accurately recognize actions. Existingknowledge assembling methods require massive GPU memory to distill theknowledge from multiple teacher models into a student model. In actionrecognition, this drawback becomes serious due to much computation required byvideo process. Constrained by limited computation source, these approaches areinfeasible. To address this issue, we propose an unlimited knowledgedistillation (UKD) in this paper. Compared with existing knowledge assemblingmethods, our UKD can effectively assemble different knowledge withoutintroducing high GPU memory consumption. Thus, the number of teaching modelsfor distillation is unlimited. With our UKD, the network's learned knowledgecan be remarkably enriched. Our experiments show that the single stream networkdistilled with our UKD even surpasses a two-stream network. Extensiveexperiments are conducted on the ARID dataset.</description><author>Ruibing Jin, Guosheng Lin, Min Wu, Jie Lin, Zhengguo Li, Xiaoli Li, Zhenghua Chen</author><pubDate>Fri, 18 Aug 2023 07:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09327v1</guid></item><item><title>Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition</title><link>http://arxiv.org/abs/2308.12006v1</link><description>RGB-D action and gesture recognition remain an interesting topic inhuman-centered scene understanding, primarily due to the multiple granularitiesand large variation in human motion. Although many RGB-D based action andgesture recognition approaches have demonstrated remarkable results byutilizing highly integrated spatio-temporal representations across multiplemodalities (i.e., RGB and depth data), they still encounter several challenges.Firstly, vanilla 3D convolution makes it hard to capture fine-grained motiondifferences between local clips under different modalities. Secondly, theintricate nature of highly integrated spatio-temporal modeling can lead tooptimization difficulties. Thirdly, duplicate and unnecessary information canadd complexity and complicate entangled spatio-temporal modeling. To addressthe above issues, we propose an innovative heuristic architecture calledMulti-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesturerecognition. The proposed MFST model comprises a 3D Central DifferenceConvolution Stem (CDC-Stem) module and multiple factorized spatio-temporalstages. The CDC-Stem enriches fine-grained temporal perception, and themultiple hierarchical spatio-temporal stages construct dimension-independenthigher-order semantic primitives. Specifically, the CDC-Stem module capturesbottom-level spatio-temporal features and passes them successively to thefollowing spatio-temporal factored stages to capture the hierarchical spatialand temporal features through the Multi- Scale Convolution and Transformer(MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans)block. The seamless integration of these innovative designs results in a robustspatio-temporal representation that outperforms state-of-the-art approaches onRGB-D action and gesture recognition datasets.</description><author>Yujun Ma, Benjia Zhou, Ruili Wang, Pichao Wang</author><pubDate>Wed, 23 Aug 2023 09:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12006v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v1</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 13 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v1</guid></item><item><title>How can objects help action recognition?</title><link>http://arxiv.org/abs/2306.11726v1</link><description>Current state-of-the-art video models process a video clip as a long sequenceof spatio-temporal tokens. However, they do not explicitly model objects, theirinteractions across the video, and instead process all the tokens in the video.In this paper, we investigate how we can use knowledge of objects to designbetter video models, namely to process fewer tokens and to improve recognitionaccuracy. This is in contrast to prior works which either drop tokens at thecost of accuracy, or increase accuracy whilst also increasing the computationrequired. First, we propose an object-guided token sampling strategy thatenables us to retain a small fraction of the input tokens with minimal impacton accuracy. And second, we propose an object-aware attention module thatenriches our feature representation with object information and improvesoverall accuracy. Our resulting framework achieves better performance whenusing fewer tokens than strong baselines. In particular, we match our baselinewith 30%, 40%, and 60% of the input tokens on SomethingElse,Something-something v2, and Epic-Kitchens, respectively. When we use our modelto process the same number of tokens as our baseline, we improve by 0.6 to 4.2points on these datasets.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11726v1</guid></item><item><title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title><link>http://arxiv.org/abs/2308.05681v2</link><description>Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.</description><author>Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</author><pubDate>Fri, 18 Aug 2023 16:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05681v2</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v3</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on five large-scale datasets(Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lowercomputational cost. Our code/models are released athttps://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 24 Aug 2023 17:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v3</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective</title><link>http://arxiv.org/abs/2208.07365v2</link><description>Unsupervised video domain adaptation is a practical yet challenging task. Inthis work, for the first time, we tackle it from a disentanglement view. Ourkey idea is to handle the spatial and temporal domain divergence separatelythrough disentanglement. Specifically, we consider the generation ofcross-domain videos from two sets of latent factors, one encoding the staticinformation and another encoding the dynamic information. A Transfer SequentialVAE (TranSVAE) framework is then developed to model such generation. To betterserve for adaptation, we propose several objectives to constrain the latentfactors. With these constraints, the spatial divergence can be readily removedby disentangling the static domain-specific information out, and the temporaldivergence is further reduced from both frame- and video-levels throughadversarial learning. Extensive experiments on the UCF-HMDB, Jester, andEpic-Kitchens datasets verify the effectiveness and superiority of TranSVAEcompared with several state-of-the-art methods. The code with reproducibleresults is publicly accessible.</description><author>Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin</author><pubDate>Fri, 09 Jun 2023 16:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07365v2</guid></item><item><title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title><link>http://arxiv.org/abs/2308.05681v1</link><description>Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.</description><author>Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</author><pubDate>Thu, 10 Aug 2023 17:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05681v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v2</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Sun, 16 Jul 2023 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v2</guid></item><item><title>Are current long-term video understanding datasets long-term?</title><link>http://arxiv.org/abs/2308.11244v1</link><description>Many real-world applications, from sport analysis to surveillance, benefitfrom automatic long-term action recognition. In the current deep learningparadigm for automatic action recognition, it is imperative that models aretrained and tested on datasets and tasks that evaluate if such models actuallylearn and reason over long-term information. In this work, we propose a methodto evaluate how suitable a video dataset is to evaluate models for long-termaction recognition. To this end, we define a long-term action as excluding allthe videos that can be correctly recognized using solely short-terminformation. We test this definition on existing long-term classification taskson three popular real-world datasets, namely Breakfast, CrossTask and LVU, todetermine if these datasets are truly evaluating long-term recognition. Ourstudy reveals that these datasets can be effectively solved using shortcutsbased on short-term information. Following this finding, we encourage long-termaction recognition researchers to make use of datasets that need long-terminformation to be solved.</description><author>Ombretta Strafforello, Klamer Schutte, Jan van Gemert</author><pubDate>Tue, 22 Aug 2023 08:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11244v1</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Humans in 4D: Reconstructing and Tracking Humans with Transformers</title><link>http://arxiv.org/abs/2305.20091v2</link><description>We present an approach to reconstruct humans and track them over time. At thecore of our approach, we propose a fully "transformerized" version of a networkfor human mesh recovery. This network, HMR 2.0, advances the state of the artand shows the capability to analyze unusual poses that have in the past beendifficult to reconstruct from single images. To analyze video, we use 3Dreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.This enables us to deal with multiple people and maintain identities throughocclusion events. Our complete approach, 4DHumans, achieves state-of-the-artresults for tracking people from monocular video. Furthermore, we demonstratethe effectiveness of HMR 2.0 on the downstream task of action recognition,achieving significant improvements over previous pose-based action recognitionapproaches. Our code and models are available on the project website:https://shubham-goel.github.io/4dhumans/.</description><author>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Thu, 29 Jun 2023 06:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20091v2</guid></item><item><title>Humans in 4D: Reconstructing and Tracking Humans with Transformers</title><link>http://arxiv.org/abs/2305.20091v3</link><description>We present an approach to reconstruct humans and track them over time. At thecore of our approach, we propose a fully "transformerized" version of a networkfor human mesh recovery. This network, HMR 2.0, advances the state of the artand shows the capability to analyze unusual poses that have in the past beendifficult to reconstruct from single images. To analyze video, we use 3Dreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.This enables us to deal with multiple people and maintain identities throughocclusion events. Our complete approach, 4DHumans, achieves state-of-the-artresults for tracking people from monocular video. Furthermore, we demonstratethe effectiveness of HMR 2.0 on the downstream task of action recognition,achieving significant improvements over previous pose-based action recognitionapproaches. Our code and models are available on the project website:https://shubham-goel.github.io/4dhumans/.</description><author>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Thu, 31 Aug 2023 17:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20091v3</guid></item><item><title>TS-RGBD Dataset: a Novel Dataset for Theatre Scenes Description for People with Visual Impairments</title><link>http://arxiv.org/abs/2308.01035v1</link><description>Computer vision was long a tool used for aiding visually impaired people tomove around their environment and avoid obstacles and falls. Solutions arelimited to either indoor or outdoor scenes, which limits the kind of places andscenes visually disabled people can be in, including entertainment places suchas theatres. Furthermore, most of the proposed computer-vision-based methodsrely on RGB benchmarks to train their models resulting in a limited performancedue to the absence of the depth modality. In this paper, we propose a novel RGB-D dataset containing theatre sceneswith ground truth human actions and dense captions annotations for imagecaptioning and human action recognition: TS-RGBD dataset. It includes threetypes of data: RGB, depth, and skeleton sequences, captured by MicrosoftKinect. We test image captioning models on our dataset as well as some skeleton-basedhuman action recognition models in order to extend the range of environmenttypes where a visually disabled person can be, by detecting human actions andtextually describing appearances of regions of interest in theatre scenes.</description><author>Leyla Benhamida, Khadidja Delloul, Slimane Larabi</author><pubDate>Wed, 02 Aug 2023 10:28:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01035v1</guid></item><item><title>Volterra Neural Networks (VNNs)</title><link>http://arxiv.org/abs/1910.09616v5</link><description>The importance of inference in Machine Learning (ML) has led to an explosivenumber of different proposals in ML, and particularly in Deep Learning. In anattempt to reduce the complexity of Convolutional Neural Networks, we propose aVolterra filter-inspired Network architecture. This architecture introducescontrolled non-linearities in the form of interactions between the delayedinput samples of data. We propose a cascaded implementation of VolterraFiltering so as to significantly reduce the number of parameters required tocarry out the same classification task as that of a conventional NeuralNetwork. We demonstrate an efficient parallel implementation of this VolterraNeural Network (VNN), along with its remarkable performance while retaining arelatively simpler and potentially more tractable structure. Furthermore, weshow a rather sophisticated adaptation of this network to nonlinearly fuse theRGB (spatial) information and the Optical Flow (temporal) information of avideo sequence for action recognition. The proposed approach is evaluated onUCF-101 and HMDB-51 datasets for action recognition, and is shown to outperformstate of the art CNN approaches.</description><author>Siddharth Roheda, Hamid Krim</author><pubDate>Thu, 15 Jun 2023 17:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.09616v5</guid></item><item><title>Is end-to-end learning enough for fitness activity recognition?</title><link>http://arxiv.org/abs/2305.08191v1</link><description>End-to-end learning has taken hold of many computer vision tasks, inparticular, related to still images, with task-specific optimization yieldingvery strong performance. Nevertheless, human-centric action recognition isstill largely dominated by hand-crafted pipelines, and only individualcomponents are replaced by neural networks that typically operate on individualframes. As a testbed to study the relevance of such pipelines, we present a newfully annotated video dataset of fitness activities. Any recognitioncapabilities in this domain are almost exclusively a function of human posesand their temporal dynamics, so pose-based solutions should perform well. Weshow that, with this labelled data, end-to-end learning on raw pixels cancompete with state-of-the-art action recognition pipelines based on poseestimation. We also show that end-to-end learning can support temporallyfine-grained tasks such as real-time repetition counting.</description><author>Antoine Mercier, Guillaume Berger, Sunny Panchal, Florian Letsch, Cornelius Boehm, Nahua Kang, Ingo Bax, Roland Memisevic</author><pubDate>Sun, 14 May 2023 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08191v1</guid></item><item><title>Human-centric Scene Understanding for 3D Large-scale Scenarios</title><link>http://arxiv.org/abs/2307.14392v1</link><description>Human-centric scene understanding is significant for real-world applications,but it is extremely challenging due to the existence of diverse human poses andactions, complex human-environment interactions, severe occlusions in crowds,etc. In this paper, we present a large-scale multi-modal dataset forhuman-centric scene understanding, dubbed HuCenLife, which is collected indiverse daily-life scenarios with rich and fine-grained annotations. OurHuCenLife can benefit many 3D perception tasks, such as segmentation,detection, action recognition, etc., and we also provide benchmarks for thesetasks to facilitate related research. In addition, we design novel modules forLiDAR-based segmentation and action recognition, which are more applicable forlarge-scale human-centric scenarios and achieve state-of-the-art performance.</description><author>Yiteng Xu, Peishan Cong, Yichen Yao, Runnan Chen, Yuenan Hou, Xinge Zhu, Xuming He, Jingyi Yu, Yuexin Ma</author><pubDate>Wed, 26 Jul 2023 09:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14392v1</guid></item><item><title>EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video</title><link>http://arxiv.org/abs/2307.05784v1</link><description>In egocentric action recognition a single population model is typicallytrained and subsequently embodied on a head-mounted device, such as anaugmented reality headset. While this model remains static for new users andenvironments, we introduce an adaptive paradigm of two phases, where afterpretraining a population model, the model adapts on-device and online to theuser's experience. This setting is highly challenging due to the change frompopulation to user domain and the distribution shifts in the user's datastream. Coping with the latter in-stream distribution shifts is the focus ofcontinual learning, where progress has been rooted in controlled benchmarks butchallenges faced in real-world applications often remain unaddressed. Weintroduce EgoAdapt, a benchmark for real-world egocentric action recognitionthat facilitates our two-phased adaptive paradigm, and real-world challengesnaturally occur in the egocentric video streams from Ego4d, such as long-tailedaction distributions and large-scale classification over 2740 actions. Weintroduce an evaluation framework that directly exploits the user's data streamwith new metrics to measure the adaptation gain over the population model,online generalization, and hindsight performance. In contrast to single-streamevaluation in existing works, our framework proposes a meta-evaluation thataggregates the results from 50 independent user streams. We provide anextensive empirical study for finetuning and experience replay.</description><author>Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway</author><pubDate>Tue, 11 Jul 2023 21:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05784v1</guid></item></channel></rss>