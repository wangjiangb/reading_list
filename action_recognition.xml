<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 04 May 2023 19:20:44 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v1</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Mon, 01 May 2023 06:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v1</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>Physical Adversarial Attacks for Surveillance: A Survey</title><link>http://arxiv.org/abs/2305.01074v1</link><description>Modern automated surveillance techniques are heavily reliant on deep learningmethods. Despite the superior performance, these learning systems areinherently vulnerable to adversarial attacks - maliciously crafted inputs thatare designed to mislead, or trick, models into making incorrect predictions. Anadversary can physically change their appearance by wearing adversarialt-shirts, glasses, or hats or by specific behavior, to potentially avoidvarious forms of detection, tracking and recognition of surveillance systems;and obtain unauthorized access to secure properties and assets. This poses asevere threat to the security and safety of modern surveillance systems. Thispaper reviews recent attempts and findings in learning and designing physicaladversarial attacks for surveillance applications. In particular, we propose aframework to analyze physical adversarial attacks and provide a comprehensivesurvey of physical adversarial attacks on four key surveillance tasks:detection, identification, tracking, and action recognition under thisframework. Furthermore, we review and analyze strategies to defend against thephysical adversarial attacks and the methods for evaluating the strengths ofthe defense. The insights in this paper present an important step in buildingresilience within surveillance systems to physical adversarial attacks.</description><author>Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan</author><pubDate>Mon, 01 May 2023 21:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01074v1</guid></item><item><title>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</title><link>http://arxiv.org/abs/2304.07775v2</link><description>Cross-modal distillation has been widely used to transfer knowledge acrossdifferent modalities, enriching the representation of the target unimodal one.Recent studies highly relate the temporal synchronization between vision andsound to the semantic consistency for cross-modal distillation. However, suchsemantic consistency from the synchronization is hard to guarantee inunconstrained videos, due to the irrelevant modality noise and differentiatedsemantic correlation. To this end, we first propose a \textit{Modality NoiseFilter} (MNF) module to erase the irrelevant noise in teacher modality withcross-modal context. After this purification, we then design a\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distilluseful knowledge for target modality, by referring to the differentiatedsample-wise semantic correlation in a contrastive fashion. Extensiveexperiments show that our method could bring a performance boost compared withother distillation methods in both visual action recognition and videoretrieval task. We also extend to the audio tagging task to prove thegeneralization of our method. The source code is available at\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.</description><author>Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu</author><pubDate>Thu, 27 Apr 2023 05:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07775v2</guid></item><item><title>Egocentric Audio-Visual Noise Suppression</title><link>http://arxiv.org/abs/2211.03643v2</link><description>This paper studies audio-visual noise suppression for egocentric videos --where the speaker is not captured in the video. Instead, potential noisesources are visible on screen with the camera emulating the off-screenspeaker's view of the outside world. This setting is different from prior workin audio-visual speech enhancement that relies on lip and facial visuals. Inthis paper, we first demonstrate that egocentric visual information is helpfulfor noise suppression. We compare object recognition and actionclassification-based visual feature extractors and investigate methods to alignaudio and visual representations. Then, we examine different fusion strategiesfor the aligned features, and locations within the noise suppression model toincorporate visual information. Experiments demonstrate that visual featuresare most helpful when used to generate additive correction masks. Finally, inorder to ensure that the visual features are discriminative with respect todifferent noise types, we introduce a multi-task learning framework thatjointly optimizes audio-visual noise suppression and video-based acoustic eventdetection. This proposed multi-task framework outperforms the audio-onlybaseline on all metrics, including a 0.16 PESQ improvement. Extensive ablationsreveal the improved performance of the proposed model with multiple activedistractors, overall noise types, and across different SNRs.</description><author>Roshan Sharma, Weipeng He, Ju Lin, Egor Lakomkin, Yang Liu, Kaustubh Kalgaonkar</author><pubDate>Wed, 03 May 2023 03:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03643v2</guid></item><item><title>Deep Graph Reprogramming</title><link>http://arxiv.org/abs/2304.14593v1</link><description>In this paper, we explore a novel model reusing task tailored for graphneural networks (GNNs), termed as "deep graph reprogramming". We strive toreprogram a pre-trained GNN, without amending raw node features nor modelparameters, to handle a bunch of cross-level downstream tasks in variousdomains. To this end, we propose an innovative Data Reprogramming paradigmalongside a Model Reprogramming paradigm. The former one aims to address thechallenge of diversified graph feature dimensions for various tasks on theinput side, while the latter alleviates the dilemma of fixed per-task-per-modelbehavior on the model side. For data reprogramming, we specifically devise anelaborated Meta-FeatPadding method to deal with heterogeneous input dimensions,and also develop a transductive Edge-Slimming as well as an inductiveMeta-GraPadding approach for diverse homogenous samples. Meanwhile, for modelreprogramming, we propose a novel task-adaptive Reprogrammable-Aggregator, toendow the frozen model with larger expressive capacities in handlingcross-domain tasks. Experiments on fourteen datasets across node/graphclassification/regression, 3D object recognition, and distributed actionrecognition, demonstrate that the proposed methods yield gratifying results, onpar with those by re-training from scratch.</description><author>Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, Dacheng Tao</author><pubDate>Fri, 28 Apr 2023 03:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14593v1</guid></item><item><title>A Survey on Multi-Resident Activity Recognition in Smart Environments</title><link>http://arxiv.org/abs/2304.12304v1</link><description>Human activity recognition (HAR) is a rapidly growing field that utilizessmart devices, sensors, and algorithms to automatically classify and identifythe actions of individuals within a given environment. These systems have awide range of applications, including assisting with caring tasks, increasingsecurity, and improving energy efficiency. However, there are severalchallenges that must be addressed in order to effectively utilize HAR systemsin multi-resident environments. One of the key challenges is accuratelyassociating sensor observations with the identities of the individualsinvolved, which can be particularly difficult when residents are engaging incomplex and collaborative activities. This paper provides a brief overview ofthe design and implementation of HAR systems, including a summary of thevarious data collection devices and approaches used for human activityidentification. It also reviews previous research on the use of these systemsin multi-resident environments and offers conclusions on the current state ofthe art in the field.</description><author>Farhad MortezaPour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed, Mohd Anuaruddin Bin Ahmadon, Shingo Yamaguchi</author><pubDate>Mon, 24 Apr 2023 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12304v1</guid></item><item><title>Model-free Motion Planning of Autonomous Agents for Complex Tasks in Partially Observable Environments</title><link>http://arxiv.org/abs/2305.00561v1</link><description>Motion planning of autonomous agents in partially known environments withincomplete information is a challenging problem, particularly for complextasks. This paper proposes a model-free reinforcement learning approach toaddress this problem. We formulate motion planning as a probabilistic-labeledpartially observable Markov decision process (PL-POMDP) problem and use lineartemporal logic (LTL) to express the complex task. The LTL formula is thenconverted to a limit-deterministic generalized B\"uchi automaton (LDGBA). Theproblem is redefined as finding an optimal policy on the product of PL-POMDPwith LDGBA based on model-checking techniques to satisfy the complex task. Weimplement deep Q learning with long short-term memory (LSTM) to process theobservation history and task recognition. Our contributions include theproposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Qlearning. We demonstrate the applicability of the proposed method by conductingsimulations in various environments, including grid worlds, a virtual office,and a multi-agent warehouse. The simulation results demonstrate that ourproposed method effectively addresses environment, action, and observationuncertainties. This indicates its potential for real-world applications,including the control of unmanned aerial vehicles (UAVs).</description><author>Junchao Li, Mingyu Cai, Zhen Kan, Shaoping Xiao</author><pubDate>Sun, 30 Apr 2023 20:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00561v1</guid></item></channel></rss>