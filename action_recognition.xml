<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 12 May 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>Physical Adversarial Attacks for Surveillance: A Survey</title><link>http://arxiv.org/abs/2305.01074v1</link><description>Modern automated surveillance techniques are heavily reliant on deep learningmethods. Despite the superior performance, these learning systems areinherently vulnerable to adversarial attacks - maliciously crafted inputs thatare designed to mislead, or trick, models into making incorrect predictions. Anadversary can physically change their appearance by wearing adversarialt-shirts, glasses, or hats or by specific behavior, to potentially avoidvarious forms of detection, tracking and recognition of surveillance systems;and obtain unauthorized access to secure properties and assets. This poses asevere threat to the security and safety of modern surveillance systems. Thispaper reviews recent attempts and findings in learning and designing physicaladversarial attacks for surveillance applications. In particular, we propose aframework to analyze physical adversarial attacks and provide a comprehensivesurvey of physical adversarial attacks on four key surveillance tasks:detection, identification, tracking, and action recognition under thisframework. Furthermore, we review and analyze strategies to defend against thephysical adversarial attacks and the methods for evaluating the strengths ofthe defense. The insights in this paper present an important step in buildingresilience within surveillance systems to physical adversarial attacks.</description><author>Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan</author><pubDate>Mon, 01 May 2023 21:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01074v1</guid></item><item><title>Learning Video-Conditioned Policies for Unseen Manipulation Tasks</title><link>http://arxiv.org/abs/2305.06289v1</link><description>The ability to specify robot commands by a non-expert user is critical forbuilding generalist agents capable of solving a large variety of tasks. Oneconvenient way to specify the intended robot goal is by a video of a persondemonstrating the target task. While prior work typically aims to imitate humandemonstrations performed in robot environments, here we focus on a morerealistic and challenging setup with demonstrations recorded in natural anddiverse human environments. We propose Video-conditioned Policy learning (ViP),a data-driven approach that maps human demonstrations of previously unseentasks to robot manipulation skills. To this end, we learn our policy togenerate appropriate actions given current scene observations and a video ofthe target task. To encourage generalization to new tasks, we avoid particulartasks during training and learn our policy from unlabelled robot trajectoriesand corresponding robot videos. Both robot and human videos in our frameworkare represented by video embeddings pre-trained for human action recognition.At test time we first translate human videos to robot videos in the commonvideo embedding space, and then use resulting embeddings to condition ourpolicies. Notably, our approach enables robot control by human demonstrationsin a zero-shot manner, i.e., without using robot trajectories paired with humaninstructions during training. We validate our approach on a set of challengingmulti-task robot manipulation environments and outperform state of the art. Ourmethod also demonstrates excellent performance in a new challenging zero-shotsetup where no paired data is used during training.</description><author>Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev</author><pubDate>Wed, 10 May 2023 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06289v1</guid></item><item><title>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</title><link>http://arxiv.org/abs/2304.07775v2</link><description>Cross-modal distillation has been widely used to transfer knowledge acrossdifferent modalities, enriching the representation of the target unimodal one.Recent studies highly relate the temporal synchronization between vision andsound to the semantic consistency for cross-modal distillation. However, suchsemantic consistency from the synchronization is hard to guarantee inunconstrained videos, due to the irrelevant modality noise and differentiatedsemantic correlation. To this end, we first propose a \textit{Modality NoiseFilter} (MNF) module to erase the irrelevant noise in teacher modality withcross-modal context. After this purification, we then design a\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distilluseful knowledge for target modality, by referring to the differentiatedsample-wise semantic correlation in a contrastive fashion. Extensiveexperiments show that our method could bring a performance boost compared withother distillation methods in both visual action recognition and videoretrieval task. We also extend to the audio tagging task to prove thegeneralization of our method. The source code is available at\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.</description><author>Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu</author><pubDate>Thu, 27 Apr 2023 05:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07775v2</guid></item><item><title>Egocentric Audio-Visual Noise Suppression</title><link>http://arxiv.org/abs/2211.03643v2</link><description>This paper studies audio-visual noise suppression for egocentric videos --where the speaker is not captured in the video. Instead, potential noisesources are visible on screen with the camera emulating the off-screenspeaker's view of the outside world. This setting is different from prior workin audio-visual speech enhancement that relies on lip and facial visuals. Inthis paper, we first demonstrate that egocentric visual information is helpfulfor noise suppression. We compare object recognition and actionclassification-based visual feature extractors and investigate methods to alignaudio and visual representations. Then, we examine different fusion strategiesfor the aligned features, and locations within the noise suppression model toincorporate visual information. Experiments demonstrate that visual featuresare most helpful when used to generate additive correction masks. Finally, inorder to ensure that the visual features are discriminative with respect todifferent noise types, we introduce a multi-task learning framework thatjointly optimizes audio-visual noise suppression and video-based acoustic eventdetection. This proposed multi-task framework outperforms the audio-onlybaseline on all metrics, including a 0.16 PESQ improvement. Extensive ablationsreveal the improved performance of the proposed model with multiple activedistractors, overall noise types, and across different SNRs.</description><author>Roshan Sharma, Weipeng He, Ju Lin, Egor Lakomkin, Yang Liu, Kaustubh Kalgaonkar</author><pubDate>Wed, 03 May 2023 03:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03643v2</guid></item><item><title>Deep Graph Reprogramming</title><link>http://arxiv.org/abs/2304.14593v1</link><description>In this paper, we explore a novel model reusing task tailored for graphneural networks (GNNs), termed as "deep graph reprogramming". We strive toreprogram a pre-trained GNN, without amending raw node features nor modelparameters, to handle a bunch of cross-level downstream tasks in variousdomains. To this end, we propose an innovative Data Reprogramming paradigmalongside a Model Reprogramming paradigm. The former one aims to address thechallenge of diversified graph feature dimensions for various tasks on theinput side, while the latter alleviates the dilemma of fixed per-task-per-modelbehavior on the model side. For data reprogramming, we specifically devise anelaborated Meta-FeatPadding method to deal with heterogeneous input dimensions,and also develop a transductive Edge-Slimming as well as an inductiveMeta-GraPadding approach for diverse homogenous samples. Meanwhile, for modelreprogramming, we propose a novel task-adaptive Reprogrammable-Aggregator, toendow the frozen model with larger expressive capacities in handlingcross-domain tasks. Experiments on fourteen datasets across node/graphclassification/regression, 3D object recognition, and distributed actionrecognition, demonstrate that the proposed methods yield gratifying results, onpar with those by re-training from scratch.</description><author>Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, Dacheng Tao</author><pubDate>Fri, 28 Apr 2023 03:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14593v1</guid></item><item><title>HeteroEdge: Addressing Asymmetry in Heterogeneous Collaborative Autonomous Systems</title><link>http://arxiv.org/abs/2305.03252v1</link><description>Gathering knowledge about surroundings and generating situational awarenessfor IoT devices is of utmost importance for systems developed for smart urbanand uncontested environments. For example, a large-area surveillance system istypically equipped with multi-modal sensors such as cameras and LIDARs and isrequired to execute deep learning algorithms for action, face, behavior, andobject recognition. However, these systems face power and memory constraintsdue to their ubiquitous nature, making it crucial to optimize data processing,deep learning algorithm input, and model inference communication. In thispaper, we propose a self-adaptive optimization framework for a testbedcomprising two Unmanned Ground Vehicles (UGVs) and two NVIDIA Jetson devices.This framework efficiently manages multiple tasks (storage, processing,computation, transmission, inference) on heterogeneous nodes concurrently. Itinvolves compressing and masking input image frames, identifying similarframes, and profiling devices to obtain boundary conditions for optimization..Finally, we propose and optimize a novel parameter split-ratio, which indicatesthe proportion of the data required to be offloaded to another device whileconsidering the networking bandwidth, busy factor, memory (CPU, GPU, RAM), andpower constraints of the devices in the testbed. Our evaluations captured whileexecuting multiple tasks (e.g., PoseNet, SegNet, ImageNet, DetectNet, DepthNet)simultaneously, reveal that executing 70% (split-ratio=70%) of the data on theauxiliary node minimizes the offloading latency by approx. 33% (18.7 ms/imageto 12.5 ms/image) and the total operation time by approx. 47% (69.32s to36.43s) compared to the baseline configuration (executing on the primary node).</description><author>Mohammad Saeid Anwar, Emon Dey, Maloy Kumar Devnath, Indrajeet Ghosh, Naima Khan, Jade Freeman, Timothy Gregory, Niranjan Suri, Kasthuri Jayaraja, Sreenivasan Ramasamy Ramamurthy, Nirmalya Roy</author><pubDate>Fri, 05 May 2023 03:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03252v1</guid></item><item><title>A Survey on Multi-Resident Activity Recognition in Smart Environments</title><link>http://arxiv.org/abs/2304.12304v1</link><description>Human activity recognition (HAR) is a rapidly growing field that utilizessmart devices, sensors, and algorithms to automatically classify and identifythe actions of individuals within a given environment. These systems have awide range of applications, including assisting with caring tasks, increasingsecurity, and improving energy efficiency. However, there are severalchallenges that must be addressed in order to effectively utilize HAR systemsin multi-resident environments. One of the key challenges is accuratelyassociating sensor observations with the identities of the individualsinvolved, which can be particularly difficult when residents are engaging incomplex and collaborative activities. This paper provides a brief overview ofthe design and implementation of HAR systems, including a summary of thevarious data collection devices and approaches used for human activityidentification. It also reviews previous research on the use of these systemsin multi-resident environments and offers conclusions on the current state ofthe art in the field.</description><author>Farhad MortezaPour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed, Mohd Anuaruddin Bin Ahmadon, Shingo Yamaguchi</author><pubDate>Mon, 24 Apr 2023 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12304v1</guid></item><item><title>Group Activity Recognition via Dynamic Composition and Interaction</title><link>http://arxiv.org/abs/2305.05583v1</link><description>Previous group activity recognition approaches were limited to reasoningusing human relations or finding important subgroups and tended to ignoreindispensable group composition and human-object interactions. This absencemakes a partial interpretation of the scene and increases the interference ofirrelevant actions on the results. Therefore, we propose our DynamicFormer withDynamic composition Module (DcM) and Dynamic interaction Module (DiM) to modelrelations and locations of persons and discriminate the contribution ofparticipants, respectively. Our findings on group composition and human-objectinteraction inspire our core idea. Group composition tells us the location ofpeople and their relations inside the group, while interaction reflects therelation between humans and objects outside the group. We utilize spatial andtemporal encoders in DcM to model our dynamic composition and build DiM toexplore interaction with a novel GCN, which has a transformer inside toconsider the temporal neighbors of human/object. Also, a Multi-level DynamicIntegration is employed to integrate features from different levels. We conductextensive experiments on two public datasets and show that our method achievesstate-of-the-art.</description><author>Youliang Zhang, Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang</author><pubDate>Tue, 09 May 2023 17:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05583v1</guid></item><item><title>Model-free Motion Planning of Autonomous Agents for Complex Tasks in Partially Observable Environments</title><link>http://arxiv.org/abs/2305.00561v1</link><description>Motion planning of autonomous agents in partially known environments withincomplete information is a challenging problem, particularly for complextasks. This paper proposes a model-free reinforcement learning approach toaddress this problem. We formulate motion planning as a probabilistic-labeledpartially observable Markov decision process (PL-POMDP) problem and use lineartemporal logic (LTL) to express the complex task. The LTL formula is thenconverted to a limit-deterministic generalized B\"uchi automaton (LDGBA). Theproblem is redefined as finding an optimal policy on the product of PL-POMDPwith LDGBA based on model-checking techniques to satisfy the complex task. Weimplement deep Q learning with long short-term memory (LSTM) to process theobservation history and task recognition. Our contributions include theproposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Qlearning. We demonstrate the applicability of the proposed method by conductingsimulations in various environments, including grid worlds, a virtual office,and a multi-agent warehouse. The simulation results demonstrate that ourproposed method effectively addresses environment, action, and observationuncertainties. This indicates its potential for real-world applications,including the control of unmanned aerial vehicles (UAVs).</description><author>Junchao Li, Mingyu Cai, Zhen Kan, Shaoping Xiao</author><pubDate>Sun, 30 Apr 2023 20:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00561v1</guid></item></channel></rss>