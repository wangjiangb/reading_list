<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 11 Jul 2024 18:35:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EA-VTR: Event-Aware Video-Text Retrieval</title><link>http://arxiv.org/abs/2407.07478v1</link><description>Understanding the content of events occurring in the video and their inherenttemporal logic is crucial for video-text retrieval. However, web-crawledpre-training datasets often lack sufficient event information, and the widelyadopted video-level cross-modal contrastive learning also struggles to capturedetailed and complex video-text event alignment. To address these challenges,we make improvements from both data and model perspectives. In terms ofpre-training data, we focus on supplementing the missing specific event contentand event temporal transitions with the proposed event augmentation strategies.Based on the event-augmented data, we construct a novel Event-Aware Video-TextRetrieval model, ie, EA-VTR, which achieves powerful video-text retrievalability through superior video event awareness. EA-VTR can efficiently encodeframe-level and video-level visual representations simultaneously, enablingdetailed event content and complex event temporal cross-modal alignment,ultimately enhancing the comprehensive understanding of video events. Ourmethod not only significantly outperforms existing approaches on multipledatasets for Text-to-Video Retrieval and Video Action Recognition tasks, butalso demonstrates superior event content perceive ability on Multi-eventVideo-Text Retrieval and Video Moment Retrieval tasks, as well as outstandingevent temporal logic understanding ability on Test of Time task.</description><author>Zongyang Ma, Ziqi Zhang, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Yingmin Luo, Xu Li, Xiaojuan Qi, Ying Shan, Weiming Hu</author><pubDate>Wed, 10 Jul 2024 09:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07478v1</guid></item></channel></rss>