<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 05 Mar 2024 06:00:26 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2402.12706v1</link><description>Few-shot action recognition aims at quickly adapting a pre-trained model tothe novel data with a distribution shift using only a limited number ofsamples. Key challenges include how to identify and leverage the transferableknowledge learned by the pre-trained model. Our central hypothesis is thattemporal invariance in the dynamic system between latent variables lends itselfto transferability (domain-invariance). We therefore propose DITeD, orDomain-Invariant Temporal Dynamics for knowledge transfer. To detect thetemporal invariance part, we propose a generative framework with a two-stagetraining strategy during pre-training. Specifically, we explicitly modelinvariant dynamics including temporal dynamic generation and transitions, andthe variant visual and domain encoders. Then we pre-train the model with theself-supervised signals to learn the representation. After that, we fix thewhole representation model and tune the classifier. During adaptation, we fixthe transferable temporal dynamics and update the image encoder. The efficacyof our approach is revealed by the superior accuracy of DITeD over leadingalternatives across standard few-shot action recognition datasets. Moreover, wevalidate that the learned temporal dynamic transition and temporal dynamicgeneration modules possess transferable qualities.</description><author>Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei</author><pubDate>Tue, 20 Feb 2024 04:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12706v1</guid></item><item><title>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</title><link>http://arxiv.org/abs/2402.08875v1</link><description>The increasing variety and quantity of tagged multimedia content on platformssuch as TikTok provides an opportunity to advance computer vision modeling. Wehave curated a distinctive dataset of 283,582 unique video clips categorizedunder 386 hashtags relating to modern human actions. We release this dataset asa valuable resource for building domain-specific foundation models for humanmovement modeling tasks such as action recognition. To validate this dataset,which we name TikTokActions, we perform two sets of experiments. First, wepretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone onTikTokActions subset, and then fine-tune and evaluate on popular datasets suchas UCF101 and the HMDB51. We find that the performance of the model pre-trainedusing our Tik-Tok dataset is comparable to models trained on larger actionrecognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, ourinvestigation into the relationship between pre-training dataset size andfine-tuning performance reveals that beyond a certain threshold, theincremental benefit of larger training sets diminishes. This work introduces auseful TikTok video dataset that is available for public use and providesinsights into the marginal benefit of increasing pre-training dataset sizes forvideo-based foundation models.</description><author>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington</author><pubDate>Wed, 14 Feb 2024 00:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08875v1</guid></item><item><title>Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability</title><link>http://arxiv.org/abs/2402.10510v1</link><description>Goal recognition is a fundamental cognitive process that enables individualsto infer intentions based on available cues. Current goal recognitionalgorithms often take only observed actions as input, but here we use aBayesian framework to explore the role of actions, timing, and goal solvabilityin goal recognition. We analyze human responses to goal-recognition problems inthe Sokoban domain, and find that actions are assigned most importance, butthat timing and solvability also influence goal recognition in some cases,especially when actions are uninformative. We leverage these findings todevelop a goal recognition model that matches human inferences more closelythan do existing algorithms. Our work provides new insight into human goalrecognition and takes a step towards more human-like AI models.</description><author>Chenyuan Zhang, Charles Kemp, Nir Lipovetzky</author><pubDate>Fri, 16 Feb 2024 08:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10510v1</guid></item><item><title>Dynamic 3D Point Cloud Sequences as 2D Videos</title><link>http://arxiv.org/abs/2403.01129v1</link><description>Dynamic 3D point cloud sequences serve as one of the most common andpractical representation modalities of dynamic real-world environments.However, their unstructured nature in both spatial and temporal domains posessignificant challenges to effective and efficient processing. Existing deeppoint cloud sequence modeling approaches imitate the mature 2D video learningmechanisms by developing complex spatio-temporal point neighbor grouping andfeature aggregation schemes, often resulting in methods lacking effectiveness,efficiency, and expressive power. In this paper, we propose a novel genericrepresentation called \textit{Structured Point Cloud Videos} (SPCVs).Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2Dmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatialsmoothness and temporal consistency, where the pixel values correspond to the3D coordinates of points. The structured nature of our SPCV representationallows for the seamless adaptation of well-established 2D image/videotechniques, enabling efficient and effective processing and analysis of 3Dpoint cloud sequences. To achieve such re-organization, we design aself-supervised learning pipeline that is geometrically regularized and drivenby self-reconstructive and deformation field learning objectives. Additionally,we construct SPCV-based frameworks for both low-level and high-level 3D pointcloud sequence processing and analysis tasks, including action recognition,temporal interpolation, and compression. Extensive experiments demonstrate theversatility and superiority of the proposed SPCV, which has the potential tooffer new possibilities for deep learning on unstructured 3D point cloudsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</description><author>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang</author><pubDate>Sat, 02 Mar 2024 08:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01129v1</guid></item><item><title>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection</title><link>http://arxiv.org/abs/2307.09465v2</link><description>Given that approximately half of science, technology, engineering, andmathematics (STEM) undergraduate students in U.S. colleges and universitiesleave by the end of the first year [15], it is crucial to improve the qualityof classroom environments. This study focuses on monitoring students' emotionsin the classroom as an indicator of their engagement and proposes an approachto address this issue. The impact of different facial parts on the performanceof an emotional recognition model is evaluated through experimentation. To testthe proposed model under partial occlusion, an artificially occluded dataset isintroduced. The novelty of this work lies in the proposal of an occlusion-awarearchitecture for facial action units (AUs) extraction, which employs attentionmechanism and adaptive feature learning. The AUs can be used later to classifyfacial expressions in classroom settings. This research paper's findings provide valuable insights into handlingocclusion in analyzing facial images for emotional engagement analysis. Theproposed experiments demonstrate the significance of considering occlusion andenhancing the reliability of facial analysis models in classroom environments.These findings can also be extended to other settings where occlusions areprevalent.</description><author>Shrouk Wally, Ahmed Elsayed, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Thu, 29 Feb 2024 03:17:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09465v2</guid></item><item><title>Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks</title><link>http://arxiv.org/abs/2403.00366v1</link><description>Multimodal data analysis and validation based on streams fromstate-of-the-art sensor technology such as eye-tracking or emotion recognitionusing the Facial Action Coding System (FACTs) with deep learning allowseducational researchers to study multifaceted learning and problem-solvingprocesses and to improve educational experiences. This study aims toinvestigate the correlation between two continuous sensor streams, pupildiameter as an indicator of cognitive workload and FACTs with deep learning asan indicator of emotional arousal (RQ 1a), specifically for epochs of high,medium, and low arousal (RQ 1b). Furthermore, the time lag between emotionalarousal and pupil diameter data will be analyzed (RQ 2). 28 participants workedon three cognitively demanding and emotionally engaging everyday moral dilemmaswhile eye-tracking and emotion recognition data were collected. The data werepre-processed in Phyton (synchronization, blink control, downsampling) andanalyzed using correlation analysis and Granger causality tests. The resultsshow negative and statistically significant correlations between the datastreams for emotional arousal and pupil diameter. However, the correlation isnegative and significant only for epochs of high arousal, while positive butnon-significant relationships were found for epochs of medium or low arousal.The average time lag for the relationship between arousal and pupil diameterwas 2.8 ms. In contrast to previous findings without a multimodal approachsuggesting a positive correlation between the constructs, the resultscontribute to the state of research by highlighting the importance ofmultimodal data validation and research on convergent vagility. Future researchshould consider emotional regulation strategies and emotional valence.</description><author>C. Kosel, S. Michel, T. Seidel, M. Foerster</author><pubDate>Fri, 01 Mar 2024 08:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00366v1</guid></item><item><title>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</title><link>http://arxiv.org/abs/2402.19344v1</link><description>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)Competition, which is part of the respective Workshop held in conjunction withIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges inunderstanding human emotions and behaviors, crucial for the development ofhuman-centered technologies. In more detail, the Competition focuses on affectrelated benchmarking tasks and comprises of five sub-challenges: i)Valence-Arousal Estimation (the target is to estimate two continuous affectdimensions, valence and arousal), ii) Expression Recognition (the target is torecognise between the mutually exclusive classes of the 7 basic expressions and'other'), iii) Action Unit Detection (the target is to detect 12 action units),iv) Compound Expression Recognition (the target is to recognise between the 7mutually exclusive compound expression classes), and v) Emotional MimicryIntensity Estimation (the target is to estimate six continuous emotiondimensions). In the paper, we present these Challenges, describe theirrespective datasets and challenge protocols (we outline the evaluation metrics)and present the baseline systems as well as their obtained performance. Moreinformation for the Competition can be found in:\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.</description><author>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu</author><pubDate>Thu, 29 Feb 2024 16:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19344v1</guid></item><item><title>ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis</title><link>http://arxiv.org/abs/2402.15952v1</link><description>The immense popularity of racket sports has fueled substantial demand intactical analysis with broadcast videos. However, existing manual methodsrequire laborious annotation, and recent attempts leveraging video perceptionmodels are limited to low-level annotations like ball trajectories, overlookingtactics that necessitate an understanding of stroke techniques.State-of-the-art action segmentation models also struggle with techniquerecognition due to frequent occlusions and motion-induced blurring in racketsports videos. To address these challenges, We propose ViSTec, a Video-basedSports Technique recognition model inspired by human cognition that synergizessparse visual data with rich contextual insights. Our approach integrates agraph to explicitly model strategic knowledge in stroke sequences and enhancetechnique recognition with contextual inductive bias. A two-stage actionperception model is jointly trained to align with the contextual knowledge inthe graph. Experiments demonstrate that our method outperforms existing modelsby a significant margin. Case studies with experts from the Chinese nationaltable tennis team validate our model's capacity to automate analysis fortechnical actions and tactical strategies. More details are available at:https://ViSTec2024.github.io/.</description><author>Yuchen He, Zeqing Yuan, Yihong Wu, Liqi Cheng, Dazhen Deng, Yingcai Wu</author><pubDate>Sun, 25 Feb 2024 02:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15952v1</guid></item><item><title>Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru</title><link>http://arxiv.org/abs/2402.11571v1</link><description>Social robots aim to establish long-term bonds with humans through engagingconversation. However, traditional conversational approaches, reliant onscripted interactions, often fall short in maintaining engaging conversations.This paper addresses this limitation by integrating large language models(LLMs) into social robots to achieve more dynamic and expressive conversations.We introduce a fully-automated conversation system that leverages LLMs togenerate robot responses with expressive behaviors, congruent with the robot'spersonality. We incorporate robot behavior with two modalities: 1) atext-to-speech (TTS) engine capable of various delivery styles, and 2) alibrary of physical actions for the robot. We develop a custom,state-of-the-art emotion recognition model to dynamically select the robot'stone of voice and utilize emojis from LLM output as cues for generating robotactions. A demo of our system is available here. To illuminate design andimplementation issues, we conduct a pilot study where volunteers chat with asocial robot using our proposed system, and we analyze their feedback,conducting a rigorous error analysis of chat transcripts. Feedback wasoverwhelmingly positive, with participants commenting on the robot's empathy,helpfulness, naturalness, and entertainment. Most negative feedback was due toautomatic speech recognition (ASR) errors which had limited impact onconversations. However, we observed a small class of errors, such as the LLMrepeating itself or hallucinating fictitious information and human responses,that have the potential to derail conversations, raising important issues forLLM application.</description><author>Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez</author><pubDate>Sun, 18 Feb 2024 12:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11571v1</guid></item><item><title>Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2402.14569v1</link><description>Robot navigation has transitioned from prioritizing obstacle avoidance toadopting socially aware navigation strategies that accommodate human presence.As a result, the recognition of socially aware navigation within dynamichuman-centric environments has gained prominence in the field of robotics.Although reinforcement learning technique has fostered the advancement ofsocially aware navigation, defining appropriate reward functions, especially incongested environments, has posed a significant challenge. These rewards,crucial in guiding robot actions, demand intricate human-crafted design due totheir complex nature and inability to be automatically set. The multitude ofmanually designed rewards poses issues with hyperparameter redundancy,imbalance, and inadequate representation of unique object characteristics. Toaddress these challenges, we introduce a transformable gaussian reward function(TGRF). The TGRF significantly reduces the burden of hyperparameter tuning,displays adaptability across various reward functions, and demonstratesaccelerated learning rates, particularly excelling in crowded environmentsutilizing deep reinforcement learning (DRL). We introduce and validate TGRFthrough sections highlighting its conceptual background, characteristics,experiments, and real-world application, paving the way for a more effectiveand adaptable approach in robotics.The complete source code is available onhttps://github.com/JinnnK/TGRF</description><author>Jinyeob Kim, Sumin Kang, Sungwoo Yang, Beomjoon Kim, Jargalbaatar Yura, Donghan Kim</author><pubDate>Thu, 22 Feb 2024 14:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14569v1</guid></item></channel></rss>