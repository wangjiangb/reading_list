<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EA-VTR: Event-Aware Video-Text Retrieval</title><link>http://arxiv.org/abs/2407.07478v1</link><description>Understanding the content of events occurring in the video and their inherenttemporal logic is crucial for video-text retrieval. However, web-crawledpre-training datasets often lack sufficient event information, and the widelyadopted video-level cross-modal contrastive learning also struggles to capturedetailed and complex video-text event alignment. To address these challenges,we make improvements from both data and model perspectives. In terms ofpre-training data, we focus on supplementing the missing specific event contentand event temporal transitions with the proposed event augmentation strategies.Based on the event-augmented data, we construct a novel Event-Aware Video-TextRetrieval model, ie, EA-VTR, which achieves powerful video-text retrievalability through superior video event awareness. EA-VTR can efficiently encodeframe-level and video-level visual representations simultaneously, enablingdetailed event content and complex event temporal cross-modal alignment,ultimately enhancing the comprehensive understanding of video events. Ourmethod not only significantly outperforms existing approaches on multipledatasets for Text-to-Video Retrieval and Video Action Recognition tasks, butalso demonstrates superior event content perceive ability on Multi-eventVideo-Text Retrieval and Video Moment Retrieval tasks, as well as outstandingevent temporal logic understanding ability on Test of Time task.</description><author>Zongyang Ma, Ziqi Zhang, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Yingmin Luo, Xu Li, Xiaojuan Qi, Ying Shan, Weiming Hu</author><pubDate>Wed, 10 Jul 2024 09:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07478v1</guid></item><item><title>Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space</title><link>http://arxiv.org/abs/2407.08572v1</link><description>Skeletal motion plays a pivotal role in human activity recognition (HAR).Recently, attack methods have been proposed to identify the universalvulnerability of skeleton-based HAR(S-HAR). However, the research ofadversarial transferability on S-HAR is largely missing. More importantly,existing attacks all struggle in transfer across unknown S-HAR models. Weobserved that the key reason is that the loss landscape of the actionrecognizers is rugged and sharp. Given the established correlation in priorstudies~\cite{qin2022boosting,wu2020towards} between loss landscape andadversarial transferability, we assume and empirically validate that smoothingthe loss landscape could potentially improve adversarial transferability onS-HAR. This is achieved by proposing a new post-train Dual Bayesian strategy,which can effectively explore the model posterior space for a collection ofsurrogates without the need for re-training. Furthermore, to craft adversarialexamples along the motion manifold, we incorporate the attack gradient withinformation of the motion dynamics in a Bayesian manner. Evaluated on benchmarkdatasets, e.g. HDM05 and NTU 60, the average transfer success rate can reach ashigh as 35.9\% and 45.5\% respectively. In comparison, current state-of-the-artskeletal attacks achieve only 3.6\% and 9.8\%. The high adversarialtransferability remains consistent across various surrogate, victim, and evendefense models. Through a comprehensive analysis of the results, we provideinsights on what surrogates are more likely to exhibit transferability, to shedlight on future research.</description><author>Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Xun Yang, Meng Wang, He Wang</author><pubDate>Thu, 11 Jul 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08572v1</guid></item><item><title>Facial Affective Behavior Analysis with Instruction Tuning</title><link>http://arxiv.org/abs/2404.05052v2</link><description>Facial affective behavior analysis (FABA) is crucial for understanding humanmental states from images. However, traditional approaches primarily deploymodels to discriminate among discrete emotion categories, and lack the finegranularity and reasoning capability for complex facial behaviors. The adventof Multi-modal Large Language Models (MLLMs) has been proven successful ingeneral visual understanding tasks. However, directly harnessing MLLMs for FABAis challenging due to the scarcity of datasets and benchmarks, neglectingfacial prior knowledge, and low training efficiency. To address thesechallenges, we introduce (i) an instruction-following dataset for two FABAtasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Benchwith a new metric considering both recognition and generation ability, and(iii) a new MLLM "EmoLA" as a strong baseline to the community. Our initiativeon the dataset and benchmarks reveal the nature and rationale of facialaffective behaviors, i.e., fine-grained facial movement, interpretability, andreasoning. Moreover, to build an effective and efficient FABA MLLM, weintroduce a facial prior expert module with face structure knowledge and alow-rank adaptation module into pre-trained MLLM. We conduct extensiveexperiments on FABA-Bench and four commonly-used FABA datasets. The resultsdemonstrate that the proposed facial prior expert can boost the performance andEmoLA achieves the best results on our FABA-Bench. On commonly-used FABAdatasets, EmoLA is competitive rivaling task-specific state-of-the-art models.</description><author>Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong</author><pubDate>Fri, 12 Jul 2024 17:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05052v2</guid></item><item><title>Improving Alignment and Robustness with Circuit Breakers</title><link>http://arxiv.org/abs/2406.04313v4</link><description>AI systems can take harmful actions and are highly vulnerable to adversarialattacks. We present an approach, inspired by recent advances in representationengineering, that interrupts the models as they respond with harmful outputswith "circuit breakers." Existing techniques aimed at improving alignment, suchas refusal training, are often bypassed. Techniques such as adversarialtraining try to plug these holes by countering specific attacks. As analternative to refusal training and adversarial training, circuit-breakingdirectly controls the representations that are responsible for harmful outputsin the first place. Our technique can be applied to both text-only andmultimodal language models to prevent the generation of harmful outputs withoutsacrificing utility -- even in the presence of powerful unseen attacks.Notably, while adversarial robustness in standalone image recognition remainsan open challenge, circuit breakers allow the larger multimodal system toreliably withstand image "hijacks" that aim to produce harmful content.Finally, we extend our approach to AI agents, demonstrating considerablereductions in the rate of harmful actions when they are under attack. Ourapproach represents a significant step forward in the development of reliablesafeguards to harmful behavior and adversarial attacks.</description><author>Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks</author><pubDate>Fri, 12 Jul 2024 16:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04313v4</guid></item></channel></rss>