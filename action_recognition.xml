<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 May 2023 06:01:22 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v1</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Wed, 24 May 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v1</guid></item><item><title>A baseline on continual learning methods for video action recognition</title><link>http://arxiv.org/abs/2304.10335v2</link><description>Continual learning has recently attracted attention from the researchcommunity, as it aims to solve long-standing limitations of classicsupervisedly-trained models. However, most research on this subject has tackledcontinual learning in simple image classification scenarios. In this paper, wepresent a benchmark of state-of-the-art continual learning methods on videoaction recognition. Besides the increased complexity due to the temporaldimension, the video setting imposes stronger requirements on computingresources for top-performing rehearsal methods. To counteract the increasedmemory requirements, we present two method-agnostic variants for rehearsalmethods, exploiting measures of either model confidence or data information toselect memorable samples. Our experiments show that, as expected from theliterature, rehearsal methods outperform other approaches; moreover, theproposed memory-efficient variants are shown to be effective at retaining acertain level of performance with a smaller buffer size.</description><author>Giulia Castagnolo, Concetto Spampinato, Francesco Rundo, Daniela Giordano, Simone Palazzo</author><pubDate>Wed, 26 Apr 2023 10:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10335v2</guid></item><item><title>Is end-to-end learning enough for fitness activity recognition?</title><link>http://arxiv.org/abs/2305.08191v1</link><description>End-to-end learning has taken hold of many computer vision tasks, inparticular, related to still images, with task-specific optimization yieldingvery strong performance. Nevertheless, human-centric action recognition isstill largely dominated by hand-crafted pipelines, and only individualcomponents are replaced by neural networks that typically operate on individualframes. As a testbed to study the relevance of such pipelines, we present a newfully annotated video dataset of fitness activities. Any recognitioncapabilities in this domain are almost exclusively a function of human posesand their temporal dynamics, so pose-based solutions should perform well. Weshow that, with this labelled data, end-to-end learning on raw pixels cancompete with state-of-the-art action recognition pipelines based on poseestimation. We also show that end-to-end learning can support temporallyfine-grained tasks such as real-time repetition counting.</description><author>Antoine Mercier, Guillaume Berger, Sunny Panchal, Florian Letsch, Cornelius Boehm, Nahua Kang, Ingo Bax, Roland Memisevic</author><pubDate>Sun, 14 May 2023 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08191v1</guid></item><item><title>GoferBot: A Visual Guided Human-Robot Collaborative Assembly System</title><link>http://arxiv.org/abs/2304.08840v2</link><description>The current transformation towards smart manufacturing has led to a growingdemand for human-robot collaboration (HRC) in the manufacturing process.Perceiving and understanding the human co-worker's behaviour introduceschallenges for collaborative robots to efficiently and effectively performtasks in unstructured and dynamic environments. Integrating recent data-drivenmachine vision capabilities into HRC systems is a logical next step inaddressing these challenges. However, in these cases, off-the-shelf componentsstruggle due to generalisation limitations. Real-world evaluation is requiredin order to fully appreciate the maturity and robustness of these approaches.Furthermore, understanding the pure-vision aspects is a crucial first stepbefore combining multiple modalities in order to understand the limitations. Inthis paper, we propose GoferBot, a novel vision-based semantic HRC system for areal-world assembly task. It is composed of a visual servoing module thatreaches and grasps assembly parts in an unstructured multi-instance and dynamicenvironment, an action recognition module that performs human action predictionfor implicit communication, and a visual handover module that uses theperceptual understanding of human behaviour to produce an intuitive andefficient collaborative assembly experience. GoferBot is a novel assemblysystem that seamlessly integrates all sub-modules by utilising implicitsemantic information purely from visual perception.</description><author>Zheyu Zhuang, Yizhak Ben-Shabat, Jiahao Zhang, Stephen Gould, Robert Mahony</author><pubDate>Wed, 17 May 2023 08:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08840v2</guid></item><item><title>Learning Higher-order Object Interactions for Keypoint-based Video Understanding</title><link>http://arxiv.org/abs/2305.09539v1</link><description>Action recognition is an important problem that requires identifying actionsin video by learning complex interactions across scene actors and objects.However, modern deep-learning based networks often require significantcomputation, and may capture scene context using various modalities thatfurther increases compute costs. Efficient methods such as those used for AR/VRoften only use human-keypoint information but suffer from a loss of scenecontext that hurts accuracy. In this paper, we describe an action-localizationmethod, KeyNet, that uses only the keypoint data for tracking and actionrecognition. Specifically, KeyNet introduces the use of object based keypointinformation to capture context in the scene. Our method illustrates how tobuild a structured intermediate representation that allows modelinghigher-order interactions in the scene from object and human keypoints withoutusing any RGB information. We find that KeyNet is able to track and classifyhuman actions at just 5 FPS. More importantly, we demonstrate that objectkeypoints can be modeled to recover any loss in context from using keypointinformation over AVA action and Kinetics datasets.</description><author>Yi Huang, Asim Kadav, Farley Lai, Deep Patel, Hans Peter Graf</author><pubDate>Tue, 16 May 2023 16:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09539v1</guid></item><item><title>CMD: Self-supervised 3D Action Representation Learning with Cross-modal Mutual Distillation</title><link>http://arxiv.org/abs/2208.12448v3</link><description>In 3D action recognition, there exists rich complementary information betweenskeleton modalities. Nevertheless, how to model and utilize this informationremains a challenging problem for self-supervised 3D action representationlearning. In this work, we formulate the cross-modal interaction as abidirectional knowledge distillation problem. Different from classicdistillation solutions that transfer the knowledge of a fixed and pre-trainedteacher to the student, in this work, the knowledge is continuously updated andbidirectionally distilled between modalities. To this end, we propose a newCross-modal Mutual Distillation (CMD) framework with the following designs. Onthe one hand, the neighboring similarity distribution is introduced to modelthe knowledge learned in each modality, where the relational information isnaturally suitable for the contrastive frameworks. On the other hand,asymmetrical configurations are used for teacher and student to stabilize thedistillation process and to transfer high-confidence information betweenmodalities. By derivation, we find that the cross-modal positive mining inprevious works can be regarded as a degenerated version of our CMD. We performextensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets.Our approach outperforms existing self-supervised methods and sets a series ofnew records. The code is available at: https://github.com/maoyunyao/CMD</description><author>Yunyao Mao, Wengang Zhou, Zhenbo Lu, Jiajun Deng, Houqiang Li</author><pubDate>Thu, 25 May 2023 15:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12448v3</guid></item><item><title>Physical Adversarial Attacks for Surveillance: A Survey</title><link>http://arxiv.org/abs/2305.01074v1</link><description>Modern automated surveillance techniques are heavily reliant on deep learningmethods. Despite the superior performance, these learning systems areinherently vulnerable to adversarial attacks - maliciously crafted inputs thatare designed to mislead, or trick, models into making incorrect predictions. Anadversary can physically change their appearance by wearing adversarialt-shirts, glasses, or hats or by specific behavior, to potentially avoidvarious forms of detection, tracking and recognition of surveillance systems;and obtain unauthorized access to secure properties and assets. This poses asevere threat to the security and safety of modern surveillance systems. Thispaper reviews recent attempts and findings in learning and designing physicaladversarial attacks for surveillance applications. In particular, we propose aframework to analyze physical adversarial attacks and provide a comprehensivesurvey of physical adversarial attacks on four key surveillance tasks:detection, identification, tracking, and action recognition under thisframework. Furthermore, we review and analyze strategies to defend against thephysical adversarial attacks and the methods for evaluating the strengths ofthe defense. The insights in this paper present an important step in buildingresilience within surveillance systems to physical adversarial attacks.</description><author>Kien Nguyen, Tharindu Fernando, Clinton Fookes, Sridha Sridharan</author><pubDate>Mon, 01 May 2023 21:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01074v1</guid></item><item><title>CVB: A Video Dataset of Cattle Visual Behaviors</title><link>http://arxiv.org/abs/2305.16555v1</link><description>Existing image/video datasets for cattle behavior recognition are mostlysmall, lack well-defined labels, or are collected in unrealistic controlledenvironments. This limits the utility of machine learning (ML) models learnedfrom them. Therefore, we introduce a new dataset, called Cattle VisualBehaviors (CVB), that consists of 502 video clips, each fifteen seconds long,captured in natural lighting conditions, and annotated with eleven visuallyperceptible behaviors of grazing cattle. We use the Computer Vision AnnotationTool (CVAT) to collect our annotations. To make the procedure more efficient,we perform an initial detection and tracking of cattle in the videos usingappropriate pre-trained models. The results are corrected by domain expertsalong with cattle behavior labeling in CVAT. The pre-hoc detection and trackingstep significantly reduces the manual annotation time and effort. Moreover, weconvert CVB to the atomic visual action (AVA) format and train and evaluate thepopular SlowFast action recognition model on it. The associated preliminaryresults confirm that we can localize the cattle and recognize their frequentlyoccurring behaviors with confidence. By creating and sharing CVB, our aim is todevelop improved models capable of recognizing all important behaviorsaccurately and to assist other researchers and practitioners in developing andevaluating new ML models for cattle behavior classification using video data.</description><author>Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham</author><pubDate>Fri, 26 May 2023 01:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16555v1</guid></item><item><title>Learning Video-Conditioned Policies for Unseen Manipulation Tasks</title><link>http://arxiv.org/abs/2305.06289v1</link><description>The ability to specify robot commands by a non-expert user is critical forbuilding generalist agents capable of solving a large variety of tasks. Oneconvenient way to specify the intended robot goal is by a video of a persondemonstrating the target task. While prior work typically aims to imitate humandemonstrations performed in robot environments, here we focus on a morerealistic and challenging setup with demonstrations recorded in natural anddiverse human environments. We propose Video-conditioned Policy learning (ViP),a data-driven approach that maps human demonstrations of previously unseentasks to robot manipulation skills. To this end, we learn our policy togenerate appropriate actions given current scene observations and a video ofthe target task. To encourage generalization to new tasks, we avoid particulartasks during training and learn our policy from unlabelled robot trajectoriesand corresponding robot videos. Both robot and human videos in our frameworkare represented by video embeddings pre-trained for human action recognition.At test time we first translate human videos to robot videos in the commonvideo embedding space, and then use resulting embeddings to condition ourpolicies. Notably, our approach enables robot control by human demonstrationsin a zero-shot manner, i.e., without using robot trajectories paired with humaninstructions during training. We validate our approach on a set of challengingmulti-task robot manipulation environments and outperform state of the art. Ourmethod also demonstrates excellent performance in a new challenging zero-shotsetup where no paired data is used during training.</description><author>Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev</author><pubDate>Wed, 10 May 2023 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06289v1</guid></item><item><title>Full-Body Articulated Human-Object Interaction</title><link>http://arxiv.org/abs/2212.10621v2</link><description>Fine-grained capturing of 3D HOI boosts human activity understanding andfacilitates downstream visual tasks, including action recognition, holisticscene reconstruction, and human motion synthesis. Despite its significance,existing works mostly assume that humans interact with rigid objects using onlya few body parts, limiting their scope. In this paper, we address thechallenging problem of f-AHOI, wherein the whole human bodies interact witharticulated objects, whose parts are connected by movable joints. We presentCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hoursof versatile interactions between 46 participants and 81 articulated and rigidsittable objects. CHAIRS provides 3D meshes of both humans and articulatedobjects during the entire interactive process, as well as realistic andphysically plausible full-body interactions. We show the value of CHAIRS withobject pose estimation. By learning the geometrical relationships in HOI, wedevise the very first model that leverage human pose estimation to tackle theestimation of articulated object poses and shapes during whole-bodyinteractions. Given an image and an estimated human pose, our model firstreconstructs the pose and shape of the object, then optimizes thereconstruction according to a learned interaction prior. Under both evaluationsettings (e.g., with or without the knowledge of objects'geometries/structures), our model significantly outperforms baselines. We hopeCHAIRS will promote the community towards finer-grained interactionunderstanding. We will make the data/code publicly available.</description><author>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang</author><pubDate>Tue, 16 May 2023 20:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10621v2</guid></item><item><title>The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose</title><link>http://arxiv.org/abs/2007.00394v2</link><description>The availability of a large labeled dataset is a key requirement for applyingdeep learning methods to solve various computer vision tasks. In the context ofunderstanding human activities, existing public datasets, while large in size,are often limited to a single RGB camera and provide only per-frame or per-clipaction annotations. To enable richer analysis and understanding of humanactivities, we introduce IKEA ASM -- a three million frame, multi-view,furniture assembly video dataset that includes depth, atomic actions, objectsegmentation, and human pose. Additionally, we benchmark prominent methods forvideo action recognition, object segmentation and human pose estimation taskson this challenging dataset. The dataset enables the development of holisticmethods, which integrate multi-modal and multi-view data to better perform onthese tasks.</description><author>Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould</author><pubDate>Wed, 17 May 2023 08:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.00394v2</guid></item><item><title>Lightweight Delivery Detection on Doorbell Cameras</title><link>http://arxiv.org/abs/2305.07812v1</link><description>Despite recent advances in video-based action recognition and robustspatio-temporal modeling, most of the proposed approaches rely on the abundanceof computational resources to afford running huge and computation-intensiveconvolutional or transformer-based neural networks to obtain satisfactoryresults. This limits the deployment of such models on edge devices with limitedpower and computing resources. In this work we investigate an important smarthome application, video based delivery detection, and present a simple andlightweight pipeline for this task that can run on resource-constraineddoorbell cameras. Our proposed pipeline relies on motion cues to generate a setof coarse activity proposals followed by their classification with amobile-friendly 3DCNN network. For training we design a novel semi-supervisedattention module that helps the network to learn robust spatio-temporalfeatures and adopt an evidence-based optimization objective that allows forquantifying the uncertainty of predictions made by the network. Experimentalresults on our curated delivery dataset shows the significant effectiveness ofour pipeline compared to alternatives and highlights the benefits of ourtraining phase novelties to achieve free and considerable inference-timeperformance gains.</description><author>Pirazh Khorramshahi, Zhe Wu, Tianchen Wang, Luke Deluccia, Hongcheng Wang</author><pubDate>Sat, 13 May 2023 02:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07812v1</guid></item><item><title>Robust Cross-Modal Knowledge Distillation for Unconstrained Videos</title><link>http://arxiv.org/abs/2304.07775v2</link><description>Cross-modal distillation has been widely used to transfer knowledge acrossdifferent modalities, enriching the representation of the target unimodal one.Recent studies highly relate the temporal synchronization between vision andsound to the semantic consistency for cross-modal distillation. However, suchsemantic consistency from the synchronization is hard to guarantee inunconstrained videos, due to the irrelevant modality noise and differentiatedsemantic correlation. To this end, we first propose a \textit{Modality NoiseFilter} (MNF) module to erase the irrelevant noise in teacher modality withcross-modal context. After this purification, we then design a\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distilluseful knowledge for target modality, by referring to the differentiatedsample-wise semantic correlation in a contrastive fashion. Extensiveexperiments show that our method could bring a performance boost compared withother distillation methods in both visual action recognition and videoretrieval task. We also extend to the audio tagging task to prove thegeneralization of our method. The source code is available at\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.</description><author>Wenke Xia, Xingjian Li, Andong Deng, Haoyi Xiong, Dejing Dou, Di Hu</author><pubDate>Thu, 27 Apr 2023 05:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07775v2</guid></item><item><title>Contrastive Predictive Autoencoders for Dynamic Point Cloud Self-Supervised Learning</title><link>http://arxiv.org/abs/2305.12959v1</link><description>We present a new self-supervised paradigm on point cloud sequenceunderstanding. Inspired by the discriminative and generative self-supervisedmethods, we design two tasks, namely point cloud sequence based ContrastivePrediction and Reconstruction (CPR), to collaboratively learn morecomprehensive spatiotemporal representations. Specifically, dense point cloudsegments are first input into an encoder to extract embeddings. All but thelast ones are then aggregated by a context-aware autoregressor to makepredictions for the last target segment. Towards the goal of modelingmulti-granularity structures, local and global contrastive learning areperformed between predictions and targets. To further improve thegeneralization of representations, the predictions are also utilized toreconstruct raw point cloud sequences by a decoder, where point cloudcolorization is employed to discriminate against different frames. By combiningclassic contrast and reconstruction paradigms, it makes the learnedrepresentations with both global discrimination and local perception. Weconduct experiments on four point cloud sequence benchmarks, and report theresults on action recognition and gesture recognition under multipleexperimental settings. The performances are comparable with supervised methodsand show powerful transferability.</description><author>Xiaoxiao Sheng, Zhiqiang Shen, Gang Xiao</author><pubDate>Mon, 22 May 2023 13:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12959v1</guid></item><item><title>Egocentric Audio-Visual Noise Suppression</title><link>http://arxiv.org/abs/2211.03643v2</link><description>This paper studies audio-visual noise suppression for egocentric videos --where the speaker is not captured in the video. Instead, potential noisesources are visible on screen with the camera emulating the off-screenspeaker's view of the outside world. This setting is different from prior workin audio-visual speech enhancement that relies on lip and facial visuals. Inthis paper, we first demonstrate that egocentric visual information is helpfulfor noise suppression. We compare object recognition and actionclassification-based visual feature extractors and investigate methods to alignaudio and visual representations. Then, we examine different fusion strategiesfor the aligned features, and locations within the noise suppression model toincorporate visual information. Experiments demonstrate that visual featuresare most helpful when used to generate additive correction masks. Finally, inorder to ensure that the visual features are discriminative with respect todifferent noise types, we introduce a multi-task learning framework thatjointly optimizes audio-visual noise suppression and video-based acoustic eventdetection. This proposed multi-task framework outperforms the audio-onlybaseline on all metrics, including a 0.16 PESQ improvement. Extensive ablationsreveal the improved performance of the proposed model with multiple activedistractors, overall noise types, and across different SNRs.</description><author>Roshan Sharma, Weipeng He, Ju Lin, Egor Lakomkin, Yang Liu, Kaustubh Kalgaonkar</author><pubDate>Wed, 03 May 2023 03:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03643v2</guid></item><item><title>Deep Graph Reprogramming</title><link>http://arxiv.org/abs/2304.14593v1</link><description>In this paper, we explore a novel model reusing task tailored for graphneural networks (GNNs), termed as "deep graph reprogramming". We strive toreprogram a pre-trained GNN, without amending raw node features nor modelparameters, to handle a bunch of cross-level downstream tasks in variousdomains. To this end, we propose an innovative Data Reprogramming paradigmalongside a Model Reprogramming paradigm. The former one aims to address thechallenge of diversified graph feature dimensions for various tasks on theinput side, while the latter alleviates the dilemma of fixed per-task-per-modelbehavior on the model side. For data reprogramming, we specifically devise anelaborated Meta-FeatPadding method to deal with heterogeneous input dimensions,and also develop a transductive Edge-Slimming as well as an inductiveMeta-GraPadding approach for diverse homogenous samples. Meanwhile, for modelreprogramming, we propose a novel task-adaptive Reprogrammable-Aggregator, toendow the frozen model with larger expressive capacities in handlingcross-domain tasks. Experiments on fourteen datasets across node/graphclassification/regression, 3D object recognition, and distributed actionrecognition, demonstrate that the proposed methods yield gratifying results, onpar with those by re-training from scratch.</description><author>Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, Dacheng Tao</author><pubDate>Fri, 28 Apr 2023 03:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14593v1</guid></item><item><title>Paxion: Patching Action Knowledge in Video-Language Foundation Models</title><link>http://arxiv.org/abs/2305.10683v3</link><description>Action knowledge involves the understanding of textual, visual, and temporalaspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)containing two carefully designed probing tasks: Action Antonym and VideoReversal, which targets multimodal alignment capabilities and temporalunderstanding skills of the model, respectively. Despite recent video-languagemodels' (VidLM) impressive performance on various benchmark tasks, ourdiagnostic tasks reveal their surprising deficiency (near-random performance)in action knowledge, suggesting that current models rely on object recognitionabilities as a shortcut for action understanding. To remedy this, we propose anovel framework, Paxion, along with a new Discriminative Video DynamicsModeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patchernetwork to encode new action knowledge and a Knowledge Fuser component tointegrate the Patcher into frozen VidLMs without compromising their existingcapabilities. Due to limitations of the widely-used Video-Text Contrastive(VTC) loss for learning action knowledge, we introduce the DVDM objective totrain the Knowledge Patcher. DVDM forces the model to encode the correlationbetween the action text and the correct ordering of video frames. Our extensiveanalyses show that Paxion and DVDM together effectively fill the gap in actionknowledge understanding (~50% to 80%), while maintaining or improvingperformance on a wide spectrum of both object- and action-centric downstreamtasks.</description><author>Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu, Jaemin Cho, Zineng Tang, Mohit Bansal, Heng Ji</author><pubDate>Fri, 26 May 2023 01:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10683v3</guid></item><item><title>Discovering Novel Actions in an Open World with Object-Grounded Visual Commonsense Reasoning</title><link>http://arxiv.org/abs/2305.16602v1</link><description>Learning to infer labels in an open world, i.e., in an environment where thetarget ``labels'' are unknown, is an important characteristic for achievingautonomy. Foundation models pre-trained on enormous amounts of data have shownremarkable generalization skills through prompting, particularly in zero-shotinference. However, their performance is restricted to the correctness of thetarget label's search space. In an open world where these labels are unknown,the search space can be exceptionally large. It can require reasoning overseveral combinations of elementary concepts to arrive at an inference, whichseverely restricts the performance of such models. To tackle this challengingproblem, we propose a neuro-symbolic framework called ALGO - novel ActionLearning with Grounded Object recognition that can use symbolic knowledgestored in large-scale knowledge bases to infer activities (verb-nouncombinations) in egocentric videos with limited supervision using two steps.First, we propose a novel neuro-symbolic prompting approach that usesobject-centric vision-language foundation models as a noisy oracle to groundobjects in the video through evidence-based reasoning. Second, driven by priorcommonsense knowledge, we discover plausible activities through an energy-basedsymbolic pattern theory framework and learn to ground knowledge-based action(verb) concepts in the video. Extensive experiments on two publicly availabledatasets (GTEA Gaze and GTEA Gaze Plus) demonstrate its performance onopen-world activity inference and its generalization to unseen actions in anunknown search space. We show that ALGO can be extended to zero-shot settingsand demonstrate its competitive performance to multimodal foundation models.</description><author>Sathyanarayanan N. Aakur, Sanjoy Kundu, Shubham Trehan</author><pubDate>Fri, 26 May 2023 04:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16602v1</guid></item><item><title>HeteroEdge: Addressing Asymmetry in Heterogeneous Collaborative Autonomous Systems</title><link>http://arxiv.org/abs/2305.03252v1</link><description>Gathering knowledge about surroundings and generating situational awarenessfor IoT devices is of utmost importance for systems developed for smart urbanand uncontested environments. For example, a large-area surveillance system istypically equipped with multi-modal sensors such as cameras and LIDARs and isrequired to execute deep learning algorithms for action, face, behavior, andobject recognition. However, these systems face power and memory constraintsdue to their ubiquitous nature, making it crucial to optimize data processing,deep learning algorithm input, and model inference communication. In thispaper, we propose a self-adaptive optimization framework for a testbedcomprising two Unmanned Ground Vehicles (UGVs) and two NVIDIA Jetson devices.This framework efficiently manages multiple tasks (storage, processing,computation, transmission, inference) on heterogeneous nodes concurrently. Itinvolves compressing and masking input image frames, identifying similarframes, and profiling devices to obtain boundary conditions for optimization..Finally, we propose and optimize a novel parameter split-ratio, which indicatesthe proportion of the data required to be offloaded to another device whileconsidering the networking bandwidth, busy factor, memory (CPU, GPU, RAM), andpower constraints of the devices in the testbed. Our evaluations captured whileexecuting multiple tasks (e.g., PoseNet, SegNet, ImageNet, DetectNet, DepthNet)simultaneously, reveal that executing 70% (split-ratio=70%) of the data on theauxiliary node minimizes the offloading latency by approx. 33% (18.7 ms/imageto 12.5 ms/image) and the total operation time by approx. 47% (69.32s to36.43s) compared to the baseline configuration (executing on the primary node).</description><author>Mohammad Saeid Anwar, Emon Dey, Maloy Kumar Devnath, Indrajeet Ghosh, Naima Khan, Jade Freeman, Timothy Gregory, Niranjan Suri, Kasthuri Jayaraja, Sreenivasan Ramasamy Ramamurthy, Nirmalya Roy</author><pubDate>Fri, 05 May 2023 03:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03252v1</guid></item><item><title>Learning Action Changes by Measuring Verb-Adverb Textual Relationships</title><link>http://arxiv.org/abs/2303.15086v2</link><description>The goal of this work is to understand the way actions are performed invideos. That is, given a video, we aim to predict an adverb indicating amodification applied to the action (e.g. cut "finely"). We cast this problem asa regression task. We measure textual relationships between verbs and adverbsto generate a regression target representing the action change we aim to learn.We test our approach on a range of datasets and achieve state-of-the-artresults on both adverb prediction and antonym classification. Furthermore, weoutperform previous work when we lift two commonly assumed conditions: theavailability of action labels during testing and the pairing of adverbs asantonyms. Existing datasets for adverb recognition are either noisy, whichmakes learning difficult, or contain actions whose appearance is not influencedby adverbs, which makes evaluation less reliable. To address this, we collect anew high quality dataset: Adverbs in Recipes (AIR). We focus on instructionalrecipes videos, curating a set of actions that exhibit meaningful visualchanges when performed differently. Videos in AIR are more tightly trimmed andwere manually reviewed by multiple annotators to ensure high labelling quality.Results show that models learn better from AIR given its cleaner videos. At thesame time, adverb prediction on AIR is challenging, demonstrating that there isconsiderable room for improvement.</description><author>Davide Moltisanti, Frank Keller, Hakan Bilen, Laura Sevilla-Lara</author><pubDate>Tue, 23 May 2023 13:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15086v2</guid></item><item><title>Explainable Activity Recognition for Smart Home Systems</title><link>http://arxiv.org/abs/2105.09787v2</link><description>Smart home environments are designed to provide services that help improvethe quality of life for the occupant via a variety of sensors and actuatorsinstalled throughout the space. Many automated actions taken by a smart homeare governed by the output of an underlying activity recognition system.However, activity recognition systems may not be perfectly accurate andtherefore inconsistencies in smart home operations can lead users reliant onsmart home predictions to wonder "why did the smart home do that?" In thiswork, we build on insights from Explainable Artificial Intelligence (XAI)techniques and introduce an explainable activity recognition framework in whichwe leverage leading XAI methods to generate natural language explanations thatexplain what about an activity led to the given classification. Within thecontext of remote caregiver monitoring, we perform a two-step evaluation: (a)utilize ML experts to assess the sensibility of explanations, and (b) recruitnon-experts in two user remote caregiver monitoring scenarios, synchronous andasynchronous, to assess the effectiveness of explanations generated via ourframework. Our results show that the XAI approach, SHAP, has a 92% success ratein generating sensible explanations. Moreover, in 83% of sampled scenariosusers preferred natural language explanations over a simple activity label,underscoring the need for explainable activity recognition systems. Finally, weshow that explanations generated by some XAI methods can lead users to loseconfidence in the accuracy of the underlying activity recognition model. Wemake a recommendation regarding which existing XAI method leads to the bestperformance in the domain of smart home automation, and discuss a range oftopics for future work to further improve explainable activity recognition.</description><author>Devleena Das, Yasutaka Nishimura, Rajan P. Vivek, Naoto Takeda, Sean T. Fish, Thomas Ploetz, Sonia Chernova</author><pubDate>Fri, 26 May 2023 17:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.09787v2</guid></item><item><title>MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling</title><link>http://arxiv.org/abs/2305.08264v1</link><description>We present MatSci-NLP, a natural language benchmark for evaluating theperformance of natural language processing (NLP) models on materials sciencetext. We construct the benchmark from publicly available materials science textdata to encompass seven different NLP tasks, including conventional NLP taskslike named entity recognition and relation classification, as well as NLP tasksspecific to materials science, such as synthesis action retrieval which relatesto creating synthesis procedures for materials. We study various BERT-basedmodels pretrained on different scientific text corpora on MatSci-NLP tounderstand the impact of pretraining strategies on understanding materialsscience text. Given the scarcity of high-quality annotated data in thematerials science domain, we perform our fine-tuning experiments with limitedtraining data to encourage the generalize across MatSci-NLP tasks. Ourexperiments in this low-resource training setting show that language modelspretrained on scientific text outperform BERT trained on general text. MatBERT,a model pretrained specifically on materials science journals, generallyperforms best for most tasks. Moreover, we propose a unified text-to-schema formultitask learning on \benchmark and compare its performance with traditionalfine-tuning methods. In our analysis of different training methods, we findthat our proposed text-to-schema methods inspired by question-answeringconsistently outperform single and multitask NLP fine-tuning methods. The codeand datasets are publicly available at\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.</description><author>Yu Song, Santiago Miret, Bang Liu</author><pubDate>Sun, 14 May 2023 23:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08264v1</guid></item><item><title>A Survey on Multi-Resident Activity Recognition in Smart Environments</title><link>http://arxiv.org/abs/2304.12304v1</link><description>Human activity recognition (HAR) is a rapidly growing field that utilizessmart devices, sensors, and algorithms to automatically classify and identifythe actions of individuals within a given environment. These systems have awide range of applications, including assisting with caring tasks, increasingsecurity, and improving energy efficiency. However, there are severalchallenges that must be addressed in order to effectively utilize HAR systemsin multi-resident environments. One of the key challenges is accuratelyassociating sensor observations with the identities of the individualsinvolved, which can be particularly difficult when residents are engaging incomplex and collaborative activities. This paper provides a brief overview ofthe design and implementation of HAR systems, including a summary of thevarious data collection devices and approaches used for human activityidentification. It also reviews previous research on the use of these systemsin multi-resident environments and offers conclusions on the current state ofthe art in the field.</description><author>Farhad MortezaPour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed, Mohd Anuaruddin Bin Ahmadon, Shingo Yamaguchi</author><pubDate>Mon, 24 Apr 2023 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12304v1</guid></item><item><title>Group Activity Recognition via Dynamic Composition and Interaction</title><link>http://arxiv.org/abs/2305.05583v1</link><description>Previous group activity recognition approaches were limited to reasoningusing human relations or finding important subgroups and tended to ignoreindispensable group composition and human-object interactions. This absencemakes a partial interpretation of the scene and increases the interference ofirrelevant actions on the results. Therefore, we propose our DynamicFormer withDynamic composition Module (DcM) and Dynamic interaction Module (DiM) to modelrelations and locations of persons and discriminate the contribution ofparticipants, respectively. Our findings on group composition and human-objectinteraction inspire our core idea. Group composition tells us the location ofpeople and their relations inside the group, while interaction reflects therelation between humans and objects outside the group. We utilize spatial andtemporal encoders in DcM to model our dynamic composition and build DiM toexplore interaction with a novel GCN, which has a transformer inside toconsider the temporal neighbors of human/object. Also, a Multi-level DynamicIntegration is employed to integrate features from different levels. We conductextensive experiments on two public datasets and show that our method achievesstate-of-the-art.</description><author>Youliang Zhang, Zhuo Zhou, Wenxuan Liu, Danni Xu, Zheng Wang</author><pubDate>Tue, 09 May 2023 17:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05583v1</guid></item><item><title>Self-Supervised Video Representation Learning via Latent Time Navigation</title><link>http://arxiv.org/abs/2305.06437v1</link><description>Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.</description><author>Di Yang, Yaohui Wang, Quan Kong, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 10 May 2023 21:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06437v1</guid></item><item><title>Model-free Motion Planning of Autonomous Agents for Complex Tasks in Partially Observable Environments</title><link>http://arxiv.org/abs/2305.00561v1</link><description>Motion planning of autonomous agents in partially known environments withincomplete information is a challenging problem, particularly for complextasks. This paper proposes a model-free reinforcement learning approach toaddress this problem. We formulate motion planning as a probabilistic-labeledpartially observable Markov decision process (PL-POMDP) problem and use lineartemporal logic (LTL) to express the complex task. The LTL formula is thenconverted to a limit-deterministic generalized B\"uchi automaton (LDGBA). Theproblem is redefined as finding an optimal policy on the product of PL-POMDPwith LDGBA based on model-checking techniques to satisfy the complex task. Weimplement deep Q learning with long short-term memory (LSTM) to process theobservation history and task recognition. Our contributions include theproposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Qlearning. We demonstrate the applicability of the proposed method by conductingsimulations in various environments, including grid worlds, a virtual office,and a multi-agent warehouse. The simulation results demonstrate that ourproposed method effectively addresses environment, action, and observationuncertainties. This indicates its potential for real-world applications,including the control of unmanned aerial vehicles (UAVs).</description><author>Junchao Li, Mingyu Cai, Zhen Kan, Shaoping Xiao</author><pubDate>Sun, 30 Apr 2023 20:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00561v1</guid></item></channel></rss>