<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 10 Jun 2024 06:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition</title><link>http://arxiv.org/abs/2404.09308v1</link><description>Action recognition is essential for egocentric video understanding, allowingautomatic and continuous monitoring of Activities of Daily Living (ADLs)without user effort. Existing literature focuses on 3D hand pose input, whichrequires computationally intensive depth estimation networks or wearing anuncomfortable depth sensor. In contrast, there has been insufficient researchin understanding 2D hand pose for egocentric action recognition, despite theavailability of user-friendly smart glasses in the market capable of capturinga single RGB image. Our study aims to fill this research gap by exploring thefield of 2D hand pose estimation for egocentric action recognition, making twocontributions. Firstly, we introduce two novel approaches for 2D hand poseestimation, namely EffHandNet for single-hand estimation and EffHandEgoNet,tailored for an egocentric perspective, capturing interactions between handsand objects. Both methods outperform state-of-the-art models on H2O and FPHApublic benchmarks. Secondly, we present a robust action recognitionarchitecture from 2D hand and object poses. This method incorporatesEffHandEgoNet, and a transformer-based action recognition method. Evaluated onH2O and FPHA datasets, our architecture has a faster inference time andachieves an accuracy of 91.32% and 94.43%, respectively, surpassing state ofthe art, including 3D-based methods. Our work demonstrates that using 2Dskeletal data is a robust approach for egocentric action understanding.Extensive evaluation and ablation studies show the impact of the hand poseestimation approach, and how each input affects the overall performance.</description><author>Wiktor Mucha, Martin Kampel</author><pubDate>Sun, 14 Apr 2024 18:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09308v1</guid></item><item><title>Hypergraph-based Multi-View Action Recognition using Event Cameras</title><link>http://arxiv.org/abs/2403.19316v1</link><description>Action recognition from video data forms a cornerstone with wide-rangingapplications. Single-view action recognition faces limitations due to itsreliance on a single viewpoint. In contrast, multi-view approaches capturecomplementary information from various viewpoints for improved accuracy.Recently, event cameras have emerged as innovative bio-inspired sensors,leading to advancements in event-based action recognition. However, existingworks predominantly focus on single-view scenarios, leaving a gap in multi-viewevent data exploitation, particularly in challenges like information deficitand semantic misalignment. To bridge this gap, we introduce HyperMV, amulti-view event-based action recognition framework. HyperMV converts discreteevent data into frame-like representations and extracts view-related featuresusing a shared convolutional network. By treating segments as vertices andconstructing hyperedges using rule-based and KNN-based strategies, a multi-viewhypergraph neural network that captures relationships across viewpoint andtemporal features is established. The vertex attention hypergraph propagationis also introduced for enhanced feature fusion. To prompt research in thisarea, we present the largest multi-view event-based action dataset$\text{THU}^{\text{MV-EACT}}\text{-50}$, comprising 50 actions from 6viewpoints, which surpasses existing datasets by over tenfold. Experimentalresults show that HyperMV significantly outperforms baselines in bothcross-subject and cross-view scenarios, and also exceeds the state-of-the-artsin frame-based multi-view action recognition.</description><author>Yue Gao, Jiaxuan Lu, Siqi Li, Yipeng Li, Shaoyi Du</author><pubDate>Thu, 28 Mar 2024 12:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19316v1</guid></item><item><title>A Survey on Backbones for Deep Video Action Recognition</title><link>http://arxiv.org/abs/2405.05584v1</link><description>Action recognition is a key technology in building interactive metaverses.With the rapid development of deep learning, methods in action recognition havealso achieved great advancement. Researchers design and implement the backbonesreferring to multiple standpoints, which leads to the diversity of methods andencountering new challenges. This paper reviews several action recognitionmethods based on deep neural networks. We introduce these methods in threeparts: 1) Two-Streams networks and their variants, which, specifically in thispaper, use RGB video frame and optical flow modality as input; 2) 3Dconvolutional networks, which make efforts in taking advantage of RGB modalitydirectly while extracting different motion information is no longer necessary;3) Transformer-based methods, which introduce the model from natural languageprocessing into computer vision and video understanding. We offer objectivesights in this review and hopefully provide a reference for future research.</description><author>Zixuan Tang, Youjun Zhao, Yuhang Wen, Mengyuan Liu</author><pubDate>Thu, 09 May 2024 08:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05584v1</guid></item><item><title>Multi-view Action Recognition via Directed Gromov-Wasserstein Discrepancy</title><link>http://arxiv.org/abs/2405.01337v1</link><description>Action recognition has become one of the popular research topics in computervision. There are various methods based on Convolutional Networks andself-attention mechanisms as Transformers to solve both spatial and temporaldimensions problems of action recognition tasks that achieve competitiveperformances. However, these methods lack a guarantee of the correctness of theaction subject that the models give attention to, i.e., how to ensure an actionrecognition model focuses on the proper action subject to make a reasonableaction prediction. In this paper, we propose a multi-view attention consistencymethod that computes the similarity between two attentions from two differentviews of the action videos using Directed Gromov-Wasserstein Discrepancy.Furthermore, our approach applies the idea of Neural Radiance Field toimplicitly render the features from novel views when training on single-viewdatasets. Therefore, the contributions in this work are three-fold. Firstly, weintroduce the multi-view attention consistency to solve the problem ofreasonable prediction in action recognition. Secondly, we define a new metricfor multi-view consistent attention using Directed Gromov-WassersteinDiscrepancy. Thirdly, we built an action recognition model based on VideoTransformers and Neural Radiance Fields. Compared to the recent actionrecognition methods, the proposed approach achieves state-of-the-art results onthree large-scale datasets, i.e., Jester, Something-Something V2, andKinetics-400.</description><author>Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 02 May 2024 15:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01337v1</guid></item><item><title>Exploring Explainability in Video Action Recognition</title><link>http://arxiv.org/abs/2404.09067v1</link><description>Image Classification and Video Action Recognition are perhaps the two mostfoundational tasks in computer vision. Consequently, explaining the innerworkings of trained deep neural networks is of prime importance. While numerousefforts focus on explaining the decisions of trained deep neural networks inimage classification, exploration in the domain of its temporal version, videoaction recognition, has been scant. In this work, we take a deeper look at thisproblem. We begin by revisiting Grad-CAM, one of the popular featureattribution methods for Image Classification, and its extension to Video ActionRecognition tasks and examine the method's limitations. To address these, weintroduce Video-TCAV, by building on TCAV for Image Classification tasks, whichaims to quantify the importance of specific concepts in the decision-makingprocess of Video Action Recognition models. As the scalable generation ofconcepts is still an open problem, we propose a machine-assisted approach togenerate spatial and spatiotemporal concepts relevant to Video ActionRecognition for testing Video-TCAV. We then establish the importance oftemporally-varying concepts by demonstrating the superiority of dynamicspatiotemporal concepts over trivial spatial concepts. In conclusion, weintroduce a framework for investigating hypotheses in action recognition andquantitatively testing them, thus advancing research in the explainability ofdeep neural networks used in video action recognition.</description><author>Avinab Saha, Shashank Gupta, Sravan Kumar Ankireddy, Karl Chahine, Joydeep Ghosh</author><pubDate>Sat, 13 Apr 2024 20:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09067v1</guid></item><item><title>SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition</title><link>http://arxiv.org/abs/2403.09508v1</link><description>Skeleton-based action recognition, which classifies human actions based onthe coordinates of joints and their connectivity within skeleton data, iswidely utilized in various scenarios. While Graph Convolutional Networks (GCNs)have been proposed for skeleton data represented as graphs, they suffer fromlimited receptive fields constrained by joint connectivity. To address thislimitation, recent advancements have introduced transformer-based methods.However, capturing correlations between all joints in all frames requiressubstantial memory resources. To alleviate this, we propose a novel approachcalled Skeletal-Temporal Transformer (SkateFormer) that partitions joints andframes based on different types of skeletal-temporal relation (Skate-Type) andperforms skeletal-temporal self-attention (Skate-MSA) within each partition. Wecategorize the key skeletal-temporal relations for action recognition into atotal of four distinct types. These types combine (i) two skeletal relationtypes based on physically neighboring and distant joints, and (ii) two temporalrelation types based on neighboring and distant frames. Through thispartition-specific attention strategy, our SkateFormer can selectively focus onkey joints and frames crucial for action recognition in an action-adaptivemanner with efficient computation. Extensive experiments on various benchmarkdatasets validate that our SkateFormer outperforms recent state-of-the-artmethods.</description><author>Jeonghyeok Do, Munchurl Kim</author><pubDate>Thu, 14 Mar 2024 16:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09508v1</guid></item><item><title>ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos</title><link>http://arxiv.org/abs/2404.06243v1</link><description>Human action or activity recognition in videos is a fundamental task incomputer vision with applications in surveillance and monitoring, self-drivingcars, sports analytics, human-robot interaction and many more. Traditionalsupervised methods require large annotated datasets for training, which areexpensive and time-consuming to acquire. This work proposes a novel approachusing Cross-Architecture Pseudo-Labeling with contrastive learning forsemi-supervised action recognition. Our framework leverages both labeled andunlabelled data to robustly learn action representations in videos, combiningpseudo-labeling with contrastive learning for effective learning from bothtypes of samples. We introduce a novel cross-architecture approach where 3DConvolutional Neural Networks (3D CNNs) and video transformers (VIT) areutilised to capture different aspects of action representations; hence we callit ActNetFormer. The 3D CNNs excel at capturing spatial features and localdependencies in the temporal domain, while VIT excels at capturing long-rangedependencies across frames. By integrating these complementary architectureswithin the ActNetFormer framework, our approach can effectively capture bothlocal and global contextual information of an action. This comprehensiverepresentation learning enables the model to achieve better performance insemi-supervised action recognition tasks by leveraging the strengths of each ofthese architectures. Experimental results on standard action recognitiondatasets demonstrate that our approach performs better than the existingmethods, achieving state-of-the-art performance with only a fraction of labeleddata. The official website of this work is available at:https://github.com/rana2149/ActNetFormer.</description><author>Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan</author><pubDate>Tue, 09 Apr 2024 13:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06243v1</guid></item><item><title>Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos</title><link>http://arxiv.org/abs/2404.07645v1</link><description>Skeleton Action Recognition (SAR) involves identifying human actions usingskeletal joint coordinates and their interconnections. While plain Transformershave been attempted for this task, they still fall short compared to thecurrent leading methods, which are rooted in Graph Convolutional Networks(GCNs) due to the absence of structural priors. Recently, a novel selectivestate space model, Mamba, has surfaced as a compelling alternative to theattention mechanism in Transformers, offering efficient modeling of longsequences. In this work, to the utmost extent of our awareness, we present thefirst SAR framework incorporating Mamba. Each fundamental block of our modeladopts a novel U-ShiftGCN architecture with Mamba as its core component. Theencoder segment of the U-ShiftGCN is devised to extract spatial features fromthe skeletal data using downsampling vanilla Shift S-GCN blocks. These spatialfeatures then undergo intermediate temporal modeling facilitated by the Mambablock before progressing to the encoder section, which comprises vanillaupsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporalmodeling unit is employed before the exit of each fundamental block to refinetemporal representations. This particular integration of downsampling spatial,intermediate temporal, upsampling spatial, and ultimate temporal subunitsyields promising results for skeleton action recognition. We dub the resultingmodel \textbf{Simba}, which attains state-of-the-art performance across threewell-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba withoutIntermediate Mamba Block) by itself is capable of performing reasonably welland surpasses our baseline.</description><author>Soumyabrata Chaudhuri, Saumik Bhattacharya</author><pubDate>Thu, 11 Apr 2024 12:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07645v1</guid></item><item><title>Analysis and Evaluation of Kinect-based Action Recognition Algorithms</title><link>http://arxiv.org/abs/2112.08626v3</link><description>Human action recognition still exists many challenging problems such asdifferent viewpoints, occlusion, lighting conditions, human body size and thespeed of action execution, although it has been widely used in different areas.To tackle these challenges, the Kinect depth sensor has been developed torecord real time depth sequences, which are insensitive to the color of humanclothes and illumination conditions. Many methods on recognizing human actionhave been reported in the literature such as HON4D, HOPC, RBD and HDG, whichuse the 4D surface normals, pointclouds, skeleton-based model and depthgradients respectively to capture discriminative information from depth videosor skeleton data. In this research project, the performance of fouraforementioned algorithms will be analyzed and evaluated using five benchmarkdatasets, which cover challenging issues such as noise, change of viewpoints,background clutters and occlusions. We also implemented and improved the HDGalgorithm, and applied it in cross-view action recognition using the UWA3DMultiview Activity dataset. Moreover, we used different combinations ofindividual feature vectors in HDG for performance evaluation. The experimentalresults show that our improvement of HDG outperforms other threestate-of-the-art algorithms for cross-view action recognition.</description><author>Lei Wang</author><pubDate>Fri, 07 Jun 2024 10:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.08626v3</guid></item><item><title>Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition</title><link>http://arxiv.org/abs/2403.01560v2</link><description>Building upon the impressive success of CLIP (Contrastive Language-ImagePretraining), recent pioneer works have proposed to adapt the powerful CLIP tovideo data, leading to efficient and effective video learners foropen-vocabulary action recognition. Inspired by that humans perform actions indiverse environments, our work delves into an intriguing question: CanCLIP-based video learners effectively generalize to video domains they have notencountered during training? To answer this, we establish a CROSS-domainOpen-Vocabulary Action recognition benchmark named XOV-Action, and conduct acomprehensive evaluation of five state-of-the-art CLIP-based video learnersunder various types of domain gaps. The evaluation demonstrates that previousmethods exhibit limited action recognition performance in unseen video domains,revealing potential challenges of the cross-domain open-vocabulary actionrecognition task. In this paper, we focus on one critical challenge of thetask, namely scene bias, and accordingly contribute a novel scene-awarevideo-text alignment method. Our key idea is to distinguish videorepresentations apart from scene-encoded text representations, aiming to learnscene-agnostic video representations for recognizing actions across domains.Extensive experiments demonstrate the effectiveness of our method. Thebenchmark and code will be available athttps://github.com/KunyuLin/XOV-Action/.</description><author>Kun-Yu Lin, Henghui Ding, Jiaming Zhou, Yu-Ming Tang, Yi-Xing Peng, Zhilin Zhao, Chen Change Loy, Wei-Shi Zheng</author><pubDate>Fri, 24 May 2024 15:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01560v2</guid></item><item><title>SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System</title><link>http://arxiv.org/abs/2311.08094v2</link><description>Skeleton-based action recognition receives the attention of many researchersas it is robust to viewpoint and illumination changes, and its processing ismuch more efficient than the processing of video frames. With the emergence ofdeep learning models, it has become very popular to represent the skeleton datain pseudo-image form and apply CNN for action recognition. Thereafter, studiesconcentrated on finding effective methods for forming pseudo-images. Recently,attention networks, more specifically transformers have provided promisingresults in various vision problems. In this study, the effectiveness of VIT forskeleton-based action recognition is examined and its robustness on thepseudo-image representation scheme is investigated. To this end, a three-levelarchitecture, SkelVit is proposed, which forms a set of pseudo images, appliesa classifier on each of the representations, and combines their results to findthe final action class. The performance of SkelVit is examined thoroughly via aset of experiments. First, the sensitivity of the system to representation isinvestigated by comparing it with two of the state-of-the-art pseudo-imagerepresentation methods. Then, the classifiers of SkelVit are realized in twoexperimental setups by CNNs and VITs, and their performances are compared. Inthe final experimental setup, the contribution of combining classifiers isexamined by applying the model with a different number of classifiers.Experimental studies reveal that the proposed system with its lightweightrepresentation scheme achieves better results than the state-of-the-artmethods. It is also observed that the vision transformer is less sensitive tothe initial pseudo-image representation compared to CNN. Nevertheless, evenwith the vision transformer, the recognition performance can be furtherimproved by the consensus of classifiers.</description><author>Ozge Oztimur Karadag</author><pubDate>Thu, 07 Mar 2024 07:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08094v2</guid></item><item><title>Emotion Recognition from the perspective of Activity Recognition</title><link>http://arxiv.org/abs/2403.16263v1</link><description>Applications of an efficient emotion recognition system can be found inseveral domains such as medicine, driver fatigue surveillance, social robotics,and human-computer interaction. Appraising human emotional states, behaviors,and reactions displayed in real-world settings can be accomplished using latentcontinuous dimensions. Continuous dimensional models of human affect, such asthose based on valence and arousal are more accurate in describing a broadrange of spontaneous everyday emotions than more traditional models of discretestereotypical emotion categories (e.g. happiness, surprise). Most of the priorwork on estimating valence and arousal considers laboratory settings and acteddata. But, for emotion recognition systems to be deployed and integrated intoreal-world mobile and computing devices, we need to consider data collected inthe world. Action recognition is a domain of Computer Vision that involvescapturing complementary information on appearance from still frames and motionbetween frames. In this paper, we treat emotion recognition from theperspective of action recognition by exploring the application of deep learningarchitectures specifically designed for action recognition, for continuousaffect recognition. We propose a novel three-stream end-to-end deep learningregression pipeline with an attention mechanism, which is an ensemble designbased on sub-modules of multiple state-of-the-art action recognition systems.The pipeline constitutes a novel data pre-processing approach with a spatialself-attention mechanism to extract keyframes. The optical flow ofhigh-attention regions of the face is extracted to capture temporal context.AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.Quantitative analysis shows that the proposed model outperforms multiplestandard baselines of both emotion recognition and action recognition models.</description><author>Savinay Nagendra, Prapti Panigrahi</author><pubDate>Sun, 24 Mar 2024 19:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16263v1</guid></item><item><title>The impact of Compositionality in Zero-shot Multi-label action recognition for Object-based tasks</title><link>http://arxiv.org/abs/2405.08695v1</link><description>Addressing multi-label action recognition in videos represents a significantchallenge for robotic applications in dynamic environments, especially when therobot is required to cooperate with humans in tasks that involve objects.Existing methods still struggle to recognize unseen actions or requireextensive training data. To overcome these problems, we propose Dual-VCLIP, aunified approach for zero-shot multi-label action recognition. Dual-VCLIPenhances VCLIP, a zero-shot action recognition method, with the DualCoOp methodfor multi-label image classification. The strength of our method is that attraining time it only learns two prompts, and it is therefore much simpler thanother methods. We validate our method on the Charades dataset that includes amajority of object-based actions, demonstrating that -- despite its simplicity-- our method performs favorably with respect to existing methods on thecomplete dataset, and promising performance when tested on unseen actions. Ourcontribution emphasizes the impact of verb-object class-splits during robots'training for new cooperative tasks, highlighting the influence on theperformance and giving insights into mitigating biases.</description><author>Carmela Calabrese, Stefano Berti, Giulia Pasquale, Lorenzo Natale</author><pubDate>Tue, 14 May 2024 16:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08695v1</guid></item><item><title>Enhancing Action Recognition from Low-Quality Skeleton Data via Part-Level Knowledge Distillation</title><link>http://arxiv.org/abs/2404.18206v1</link><description>Skeleton-based action recognition is vital for comprehending human-centricvideos and has applications in diverse domains. One of the challenges ofskeleton-based action recognition is dealing with low-quality data, such asskeletons that have missing or inaccurate joints. This paper addresses theissue of enhancing action recognition using low-quality skeletons through ageneral knowledge distillation framework. The proposed framework employs ateacher-student model setup, where a teacher model trained on high-qualityskeletons guides the learning of a student model that handles low-qualityskeletons. To bridge the gap between heterogeneous high-quality and lowqualityskeletons, we present a novel part-based skeleton matching strategy, whichexploits shared body parts to facilitate local action pattern learning. Anaction-specific part matrix is developed to emphasize critical parts fordifferent actions, enabling the student model to distill discriminativepart-level knowledge. A novel part-level multi-sample contrastive loss achievesknowledge transfer from multiple high-quality skeletons to low-quality ones,which enables the proposed knowledge distillation framework to include traininglow-quality skeletons that lack corresponding high-quality matches.Comprehensive experiments conducted on the NTU-RGB+D, Penn Action, and SYSU 3DHOI datasets demonstrate the effectiveness of the proposed knowledgedistillation framework.</description><author>Cuiwei Liu, Youzhi Jiang, Chong Du, Zhaokui Li</author><pubDate>Sun, 28 Apr 2024 15:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18206v1</guid></item><item><title>D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2312.01431v2</link><description>Adapting large pre-trained image models to few-shot action recognition hasproven to be an effective and efficient strategy for learning robust featureextractors, which is essential for few-shot learning. Typical fine-tuning basedadaptation paradigm is prone to overfitting in the few-shot learning scenariosand offers little modeling flexibility for learning temporal features in videodata. In this work we present the Disentangled-and-Deformable Spatio-TemporalAdapter (D$^2$ST-Adapter), which is a novel adapter tuning frameworkwell-suited for few-shot action recognition due to lightweight design and lowparameter-learning overhead. It is designed in a dual-pathway architecture toencode spatial and temporal features in a disentangled manner. In particular,we devise the anisotropic Deformable Spatio-Temporal Attention module as thecore component of D$^2$ST-Adapter, which can be tailored with anisotropicsampling densities along spatial and temporal domains to learn spatial andtemporal features specifically in corresponding pathways, allowing ourD$^2$ST-Adapter to encode features in a global view in 3D spatio-temporal spacewhile maintaining a lightweight design. Extensive experiments withinstantiations of our method on both pre-trained ResNet and ViT demonstrate thesuperiority of our method over state-of-the-art methods for few-shot actionrecognition. Our method is particularly well-suited to challenging scenarioswhere temporal dynamics are critical for action recognition.</description><author>Wenjie Pei, Qizhong Tan, Guangming Lu, Jiandong Tian</author><pubDate>Wed, 17 Apr 2024 13:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01431v2</guid></item><item><title>HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2404.15719v2</link><description>Skeleton-based action recognition has gained considerable traction thanks toits utilization of succinct and robust skeletal representations. Nonetheless,current methodologies often lean towards utilizing a solitary backbone to modelskeleton modality, which can be limited by inherent flaws in the networkbackbone. To address this and fully leverage the complementary characteristicsof various network architectures, we propose a novel Hybrid Dual-Branch Network(HDBN) for robust skeleton-based action recognition, which benefits from thegraph convolutional network's proficiency in handling graph-structured data andthe powerful modeling capabilities of Transformers for global information. Indetail, our proposed HDBN is divided into two trunk branches: MixGCN andMixFormer. The two branches utilize GCNs and Transformers to model both 2D and3D skeletal modalities respectively. Our proposed HDBN emerged as one of thetop solutions in the Multi-Modal Video Reasoning and Analyzing Competition(MMVRAC) of 2024 ICME Grand Challenge, achieving accuracies of 47.95% and75.36% on two benchmarks of the UAV-Human dataset by outperforming mostexisting methods. Our code will be publicly available at:https://github.com/liujf69/ICMEW2024-Track10.</description><author>Jinfu Liu, Baiqiao Yin, Jiaying Lin, Jiajun Wen, Yue Li, Mengyuan Liu</author><pubDate>Thu, 25 Apr 2024 09:27:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15719v2</guid></item><item><title>Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2402.12706v1</link><description>Few-shot action recognition aims at quickly adapting a pre-trained model tothe novel data with a distribution shift using only a limited number ofsamples. Key challenges include how to identify and leverage the transferableknowledge learned by the pre-trained model. Our central hypothesis is thattemporal invariance in the dynamic system between latent variables lends itselfto transferability (domain-invariance). We therefore propose DITeD, orDomain-Invariant Temporal Dynamics for knowledge transfer. To detect thetemporal invariance part, we propose a generative framework with a two-stagetraining strategy during pre-training. Specifically, we explicitly modelinvariant dynamics including temporal dynamic generation and transitions, andthe variant visual and domain encoders. Then we pre-train the model with theself-supervised signals to learn the representation. After that, we fix thewhole representation model and tune the classifier. During adaptation, we fixthe transferable temporal dynamics and update the image encoder. The efficacyof our approach is revealed by the superior accuracy of DITeD over leadingalternatives across standard few-shot action recognition datasets. Moreover, wevalidate that the learned temporal dynamic transition and temporal dynamicgeneration modules possess transferable qualities.</description><author>Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei</author><pubDate>Tue, 20 Feb 2024 04:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12706v1</guid></item><item><title>Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition</title><link>http://arxiv.org/abs/2404.07487v2</link><description>Skeleton-based zero-shot action recognition aims to recognize unknown humanactions based on the learned priors of the known skeleton-based actions and asemantic descriptor space shared by both known and unknown categories. However,previous works focus on establishing the bridges between the known skeletonrepresentation space and semantic descriptions space at the coarse-grainedlevel for recognizing unknown action categories, ignoring the fine-grainedalignment of these two spaces, resulting in suboptimal performance indistinguishing high-similarity action categories. To address these challenges,we propose a novel method via Side information and dual-prompts learning forskeleton-based zero-shot action recognition (STAR) at the fine-grained level.Specifically, 1) we decompose the skeleton into several parts based on itstopology structure and introduce the side information concerning multi-partdescriptions of human body movements for alignment between the skeleton and thesemantic space at the fine-grained level; 2) we design the visual-attribute andsemantic-part prompts to improve the intra-class compactness within theskeleton space and inter-class separability within the semantic space,respectively, to distinguish the high-similarity actions. Extensive experimentsshow that our method achieves state-of-the-art performance in ZSL and GZSLsettings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.</description><author>Yang Chen, Jingcai Guo, Tian He, Ling Wang</author><pubDate>Mon, 15 Apr 2024 03:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07487v2</guid></item><item><title>Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition</title><link>http://arxiv.org/abs/2404.07487v1</link><description>Skeleton-based zero-shot action recognition aims to recognize unknown humanactions based on the learned priors of the known skeleton-based actions and asemantic descriptor space shared by both known and unknown categories. However,previous works focus on establishing the bridges between the known skeletonrepresentation space and semantic descriptions space at the coarse-grainedlevel for recognizing unknown action categories, ignoring the fine-grainedalignment of these two spaces, resulting in suboptimal performance indistinguishing high-similarity action categories. To address these challenges,we propose a novel method via Side information and dual-prompts learning forskeleton-based zero-shot action recognition (STAR) at the fine-grained level.Specifically, 1) we decompose the skeleton into several parts based on itstopology structure and introduce the side information concerning multi-partdescriptions of human body movements for alignment between the skeleton and thesemantic space at the fine-grained level; 2) we design the visual-attribute andsemantic-part prompts to improve the intra-class compactness within theskeleton space and inter-class separability within the semantic space,respectively, to distinguish the high-similarity actions. Extensive experimentsshow that our method achieves state-of-the-art performance in ZSL and GZSLsettings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.</description><author>Yang Chen, Jingcai Guo, Tian He, Ling Wang</author><pubDate>Thu, 11 Apr 2024 06:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07487v1</guid></item><item><title>Benchmarking Micro-action Recognition: Dataset, Methods, and Applications</title><link>http://arxiv.org/abs/2403.05234v1</link><description>Micro-action is an imperceptible non-verbal behaviour characterised bylow-intensity movement. It offers insights into the feelings and intentions ofindividuals and is important for human-oriented applications such as emotionrecognition and psychological assessment. However, the identification,differentiation, and understanding of micro-actions pose challenges due to theimperceptible and inaccessible nature of these subtle human behaviors ineveryday life. In this study, we innovatively collect a new micro-actiondataset designated as Micro-action-52 (MA-52), and propose a benchmark namedmicro-action network (MANet) for micro-action recognition (MAR) task. Uniquely,MA-52 provides the whole-body perspective including gestures, upper- andlower-limb movements, attempting to reveal comprehensive micro-action cues. Indetail, MA-52 contains 52 micro-action categories along with seven body partlabels, and encompasses a full array of realistic and natural micro-actions,accounting for 205 participants and 22,422 video instances collated from thepsychological interviews. Based on the proposed dataset, we assess MANet andother nine prevalent action recognition methods. MANet incorporates squeeze-andexcitation (SE) and temporal shift module (TSM) into the ResNet architecturefor modeling the spatiotemporal characteristics of micro-actions. Then ajoint-embedding loss is designed for semantic matching between video and actionlabels; the loss is used to better distinguish between visually similar yetdistinct micro-action categories. The extended application in emotionrecognition has demonstrated one of the important values of our proposeddataset and method. In the future, further exploration of human behaviour,emotion, and psychological assessment will be conducted in depth. The datasetand source code are released at https://github.com/VUT-HFUT/Micro-Action.</description><author>Dan Guo, Kun Li, Bin Hu, Yan Zhang, Meng Wang</author><pubDate>Fri, 08 Mar 2024 11:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05234v1</guid></item><item><title>DL-KDD: Dual-Light Knowledge Distillation for Action Recognition in the Dark</title><link>http://arxiv.org/abs/2406.02468v1</link><description>Human action recognition in dark videos is a challenging task for computervision. Recent research focuses on applying dark enhancement methods to improvethe visibility of the video. However, such video processing results in the lossof critical information in the original (un-enhanced) video. Conversely,traditional two-stream methods are capable of learning information from bothoriginal and processed videos, but it can lead to a significant increase in thecomputational cost during the inference phase in the task of videoclassification. To address these challenges, we propose a novel teacher-studentvideo classification framework, named Dual-Light KnowleDge Distillation forAction Recognition in the Dark (DL-KDD). This framework enables the model tolearn from both original and enhanced video without introducing additionalcomputational cost during inference. Specifically, DL-KDD utilizes the strategyof knowledge distillation during training. The teacher model is trained withenhanced video, and the student model is trained with both the original videoand the soft target generated by the teacher model. This teacher-studentframework allows the student model to predict action using only the originalinput video during inference. In our experiments, the proposed DL-KDD frameworkoutperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48datasets. We achieve the best performance on each dataset and up to a 4.18%improvement on Dark-48, using only original video inputs, thus avoiding the useof two-stream framework or enhancement modules for inference. We furthervalidate the effectiveness of the distillation strategy in ablativeexperiments. The results highlight the advantages of our knowledge distillationframework in dark human action recognition.</description><author>Chi-Jui Chang, Oscar Tai-Yuan Chen, Vincent S. Tseng</author><pubDate>Tue, 04 Jun 2024 17:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02468v1</guid></item><item><title>Cross-Block Fine-Grained Semantic Cascade for Skeleton-Based Sports Action Recognition</title><link>http://arxiv.org/abs/2404.19383v1</link><description>Human action video recognition has recently attracted more attention inapplications such as video security and sports posture correction. Popularsolutions, including graph convolutional networks (GCNs) that model the humanskeleton as a spatiotemporal graph, have proven very effective. GCNs-basedmethods with stacked blocks usually utilize top-layer semantics forclassification/annotation purposes. Although the global features learnedthrough the procedure are suitable for the general classification, they havedifficulty capturing fine-grained action change across adjacent frames --decisive factors in sports actions. In this paper, we propose a novel``Cross-block Fine-grained Semantic Cascade (CFSC)'' module to overcome thischallenge. In summary, the proposed CFSC progressively integrates shallowvisual knowledge into high-level blocks to allow networks to focus on actiondetails. In particular, the CFSC module utilizes the GCN feature maps producedat different levels, as well as aggregated features from proceeding levels toconsolidate fine-grained features. In addition, a dedicated temporalconvolution is applied at each level to learn short-term temporal features,which will be carried over from shallow to deep layers to maximize the leverageof low-level details. This cross-block feature aggregation methodology, capableof mitigating the loss of fine-grained information, has resulted in improvedperformance. Last, FD-7, a new action recognition dataset for fencing sports,was collected and will be made publicly available. Experimental results andempirical analysis on public benchmarks (FSD-10) and self-collected (FD-7)demonstrate the advantage of our CFSC module on learning discriminativepatterns for action classification over others.</description><author>Zhendong Liu, Haifeng Xia, Tong Guo, Libo Sun, Ming Shao, Siyu Xia</author><pubDate>Tue, 30 Apr 2024 10:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19383v1</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v2</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the selfish view. First, we present a novelgeometric-based constraint into the self-attention mechanism in Transformerbased on analyzing the camera positions between two views. Then, we propose anew cross-view self-attention loss learned on unpaired cross-view data toenforce the self-attention mechanism learning to transfer knowledge acrossviews. Finally, to further improve the performance of our cross-view learningapproach, we present the metrics to measure the correlations in videos andattention maps effectively. Experimental results on standard egocentric actionrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Wed, 15 May 2024 18:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v2</guid></item><item><title>Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition</title><link>http://arxiv.org/abs/2403.12710v1</link><description>Concerns for the privacy of individuals captured in public imagery have ledto privacy-preserving action recognition. Existing approaches often suffer fromissues arising through obfuscation being applied globally and a lack ofinterpretability. Global obfuscation hides privacy sensitive regions, but alsocontextual regions important for action recognition. Lack of interpretabilityerodes trust in these new technologies. We highlight the limitations of currentparadigms and propose a solution: Human selected privacy templates that yieldinterpretability by design, an obfuscation scheme that selectively hidesattributes and also induces temporal consistency, which is important in actionrecognition. Our approach is architecture agnostic and directly modifies inputimagery, while existing approaches generally require architecture training. Ourapproach offers more flexibility, as no retraining is required, and outperformsalternatives on three widely used datasets.</description><author>Filip Ilic, He Zhao, Thomas Pock, Richard P. Wildes</author><pubDate>Tue, 19 Mar 2024 14:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12710v1</guid></item><item><title>On the Utility of 3D Hand Poses for Action Recognition</title><link>http://arxiv.org/abs/2403.09805v1</link><description>3D hand poses are an under-explored modality for action recognition. Posesare compact yet informative and can greatly benefit applications with limitedcompute budgets. However, poses alone offer an incomplete understanding ofactions, as they cannot fully capture objects and environments with whichhumans interact. To efficiently model hand-object interactions, we proposeHandFormer, a novel multimodal transformer. HandFormer combines 3D hand posesat a high temporal resolution for fine-grained motion modeling with sparselysampled RGB frames for encoding scene semantics. Observing the uniquecharacteristics of hand poses, we temporally factorize hand modeling andrepresent each joint by its short-term trajectories. This factorized poserepresentation combined with sparse RGB samples is remarkably efficient andachieves high accuracy. Unimodal HandFormer with only hand poses outperformsexisting skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve newstate-of-the-art performance on Assembly101 and H2O with significantimprovements in egocentric action recognition.</description><author>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Thu, 14 Mar 2024 19:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09805v1</guid></item><item><title>Taylor Videos for Action Recognition</title><link>http://arxiv.org/abs/2402.03019v4</link><description>Effectively extracting motions from video is a critical and long-standingproblem for action recognition. This problem is very challenging becausemotions (i) do not have an explicit form, (ii) have various concepts such asdisplacement, velocity, and acceleration, and (iii) often contain noise causedby unstable pixels. Addressing these challenges, we propose the Taylor video, anew video format that highlights the dominate motions (e.g., a waving hand) ineach of its frames named the Taylor frame. Taylor video is named after Taylorseries, which approximates a function at a given point using important terms.In the scenario of videos, we define an implicit motion-extraction functionwhich aims to extract motions from video temporal block. In this block, usingthe frames, the difference frames, and higher-order difference frames, weperform Taylor expansion to approximate this function at the starting frame. Weshow the summation of the higher-order terms in the Taylor series gives usdominant motion patterns, where static objects, small and unstable motions areremoved. Experimentally we show that Taylor videos are effective inputs topopular architectures including 2D CNNs, 3D CNNs, and transformers. When usedindividually, Taylor videos yield competitive action recognition accuracycompared to RGB videos and optical flow. When fused with RGB or optical flowvideos, further accuracy improvement is achieved. Additionally, we apply Taylorvideo computation to human skeleton sequences, resulting in Taylor skeletonsequences that outperform the use of original skeletons for skeleton-basedaction recognition.</description><author>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng</author><pubDate>Fri, 10 May 2024 15:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03019v4</guid></item><item><title>Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines</title><link>http://arxiv.org/abs/2403.09056v1</link><description>On modern industrial assembly lines, many intelligent algorithms have beendeveloped to replace or supervise workers. However, we found that there werebottlenecks in both training datasets and real-time performance when deployingalgorithms on actual assembly line. Therefore, we developed a promisingstrategy for expanding industrial datasets, which utilized large models withstrong generalization abilities to achieve efficient, high-quality, andlarge-scale dataset expansion, solving the problem of insufficient andlow-quality industrial datasets. We also applied this strategy to video actionrecognition. We proposed a method of converting hand action recognitionproblems into hand skeletal trajectory classification problems, which solvedthe real-time performance problem of industrial algorithms. In the "handmovements during wire insertion" scenarios on the actual assembly line, theaccuracy of hand action recognition reached 98.8\%. We conducted detailedexperimental analysis to demonstrate the effectiveness and superiority of themethod, and deployed the entire process on Midea's actual assembly line.</description><author>Liang Wu, X. -G. Ma</author><pubDate>Thu, 14 Mar 2024 03:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09056v1</guid></item><item><title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title><link>http://arxiv.org/abs/2404.05559v2</link><description>Diverse actions give rise to rich audio-visual signals in long videos. Recentworks showcase that the two modalities of audio and video exhibit differenttemporal extents of events and distinct labels. We address the interplaybetween the two modalities in long videos by explicitly modelling the temporalextents of audio and visual events. We propose the Time Interval Machine (TIM)where a modality-specific time interval poses as a query to a transformerencoder that ingests a long video input. The encoder then attends to thespecified interval, as well as the surrounding context in both modalities, inorder to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. OnEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantlylarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, weshow that TIM can be adapted for action detection, using dense multi-scaleinterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, andshowing strong performance on the Perception Test. Our ablations show thecritical role of integrating the two modalities and modelling their timeintervals in achieving this performance. Code and models at:https://github.com/JacobChalk/TIM</description><author>Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen</author><pubDate>Tue, 09 Apr 2024 08:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05559v2</guid></item><item><title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title><link>http://arxiv.org/abs/2404.05559v1</link><description>Diverse actions give rise to rich audio-visual signals in long videos. Recentworks showcase that the two modalities of audio and video exhibit differenttemporal extents of events and distinct labels. We address the interplaybetween the two modalities in long videos by explicitly modelling the temporalextents of audio and visual events. We propose the Time Interval Machine (TIM)where a modality-specific time interval poses as a query to a transformerencoder that ingests a long video input. The encoder then attends to thespecified interval, as well as the surrounding context in both modalities, inorder to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. OnEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantlylarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, weshow that TIM can be adapted for action detection, using dense multi-scaleinterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, andshowing strong performance on the Perception Test. Our ablations show thecritical role of integrating the two modalities and modelling their timeintervals in achieving this performance. Code and models at:https://github.com/JacobChalk/TIM</description><author>Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen</author><pubDate>Mon, 08 Apr 2024 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05559v1</guid></item><item><title>Towards Weakly Supervised End-to-end Learning for Long-video Action Recognition</title><link>http://arxiv.org/abs/2311.17118v2</link><description>Developing end-to-end action recognition models on long videos is fundamentaland crucial for long-video action understanding. Due to the unaffordable costof end-to-end training on the whole long videos, existing works generally trainmodels on short clips trimmed from long videos. However, this``trimming-then-training'' practice requires action interval annotations forclip-level supervision, i.e., knowing which actions are trimmed into the clips.Unfortunately, collecting such annotations is very expensive and prevents modeltraining at scale. To this end, this work aims to build a weakly supervisedend-to-end framework for training recognition models on long videos, with onlyvideo-level action category labels. Without knowing the precise temporallocations of actions in long videos, our proposed weakly supervised framework,namely AdaptFocus, estimates where and how likely the actions will occur toadaptively focus on informative action clips for end-to-end training. Theeffectiveness of the proposed AdaptFocus framework is demonstrated on threelong-video datasets. Furthermore, for downstream long-video tasks, ourAdaptFocus framework provides a weakly supervised feature extraction pipelinefor extracting more robust long-video features, such that the state-of-the-artmethods on downstream tasks are significantly advanced. We will release thecode and models.</description><author>Jiaming Zhou, Hanjun Li, Kun-Yu Lin, Junwei Liang</author><pubDate>Fri, 24 May 2024 16:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17118v2</guid></item><item><title>Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition</title><link>http://arxiv.org/abs/2404.16416v1</link><description>Semi-supervised action recognition aims to improve spatio-temporal reasoningability with a few labeled data in conjunction with a large amount of unlabeleddata. Albeit recent advancements, existing powerful methods are still prone tomaking ambiguous predictions under scarce labeled data, embodied as thelimitation of distinguishing different actions with similar spatio-temporalinformation. In this paper, we approach this problem by empowering the modeltwo aspects of capability, namely discriminative spatial modeling and temporalstructure modeling for learning discriminative spatio-temporal representations.Specifically, we propose an Adaptive Contrastive Learning~(ACL) strategy. Itassesses the confidence of all unlabeled samples by the class prototypes of thelabeled data, and adaptively selects positive-negative samples from apseudo-labeled sample bank to construct contrastive learning. Additionally, weintroduce a Multi-scale Temporal Learning~(MTL) strategy. It could highlightinformative semantics from long-term clips and integrate them into theshort-term clip while suppressing noisy information. Afterwards, both of thesetwo new techniques are integrated in a unified framework to encourage the modelto make accurate predictions. Extensive experiments on UCF101, HMDB51 andKinetics400 show the superiority of our method over prior state-of-the-artapproaches.</description><author>Yu Wang, Sanping Zhou, Kun Xia, Le Wang</author><pubDate>Thu, 25 Apr 2024 09:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16416v1</guid></item><item><title>GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2403.15212v1</link><description>Skeleton-based action recognition (SAR) in videos is an important butchallenging task in computer vision. The recent state-of-the-art models for SARare primarily based on graph convolutional neural networks (GCNs), which arepowerful in extracting the spatial information of skeleton data. However, it isyet clear that such GCN-based models can effectively capture the temporaldynamics of human action sequences. To this end, we propose the DevLSTM module,which exploits the path development -- a principled and parsimoniousrepresentation for sequential data by leveraging the Lie group structure. Thepath development, originated from Rough path theory, can effectively capturethe order of events in high-dimensional stream data with massive dimensionreduction and consequently enhance the LSTM module substantially. Our proposedG-DevLSTM module can be conveniently plugged into the temporal graph,complementing existing advanced GCN-based models. Our empirical studies on theNTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybridmodel significantly outperforms the current best-performing methods in SARtasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.</description><author>Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni</author><pubDate>Fri, 22 Mar 2024 14:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15212v1</guid></item><item><title>Skeleton-Based Human Action Recognition with Noisy Labels</title><link>http://arxiv.org/abs/2403.09975v1</link><description>Understanding human actions from body poses is critical for assistive robotssharing space with humans in order to make informed and safe decisions aboutthe next interaction. However, precise temporal localization and annotation ofactivity sequences is time-consuming and the resulting labels are often noisy.If not effectively addressed, label noise negatively affects the model'straining, resulting in lower recognition quality. Despite its importance,addressing label noise for skeleton-based action recognition has beenoverlooked so far. In this study, we bridge this gap by implementing aframework that augments well-established skeleton-based human actionrecognition methods with label-denoising strategies from various research areasto serve as the initial benchmark. Observations reveal that these baselinesyield only marginal performance when dealing with sparse skeleton data.Consequently, we introduce a novel methodology, NoiseEraSAR, which integratesglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.Our proposed approach demonstrates better performance on the establishedbenchmark, setting new state-of-the-art standards. The source code for thisstudy will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.</description><author>Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen</author><pubDate>Fri, 15 Mar 2024 03:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09975v1</guid></item><item><title>MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2405.02077v1</link><description>Recent few-shot action recognition (FSAR) methods achieve promisingperformance by performing semantic matching on learned discriminative features.However, most FSAR methods focus on single-scale (e.g., frame-level,segment-level, \etc) feature alignment, which ignores that human actions withthe same semantic may appear at different velocities. To this end, we develop anovel Multi-Velocity Progressive-alignment (MVP-Shot) framework toprogressively learn and align semantic-related action features atmulti-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA)module is designed to measure the similarity between features from support andquery videos with different velocity scales and then merge all similarityscores in a residual fashion. To avoid the multiple velocity features deviatingfrom the underlying motion semantic, our proposed Progressive Semantic-TailoredInteraction (PSTI) module injects velocity-tailored text information into thevideo feature via feature interaction on channel and temporal domains atdifferent velocities. The above two modules compensate for each other topredict query categories more accurately under the few-shot settings.Experimental results show our method outperforms current state-of-the-artmethods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101,Kinetics, and SSv2-small).</description><author>Hongyu Qu, Rui Yan, Xiangbo Shu, Haoliang Gao, Peng Huang, Guo-Sen Xie</author><pubDate>Fri, 03 May 2024 14:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02077v1</guid></item><item><title>MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2405.02077v2</link><description>Recent few-shot action recognition (FSAR) methods achieve promisingperformance by performing semantic matching on learned discriminative features.However, most FSAR methods focus on single-scale (e.g., frame-level,segment-level, \etc) feature alignment, which ignores that human actions withthe same semantic may appear at different velocities. To this end, we develop anovel Multi-Velocity Progressive-alignment (MVP-Shot) framework toprogressively learn and align semantic-related action features atmulti-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA)module is designed to measure the similarity between features from support andquery videos with different velocity scales and then merge all similarityscores in a residual fashion. To avoid the multiple velocity features deviatingfrom the underlying motion semantic, our proposed Progressive Semantic-TailoredInteraction (PSTI) module injects velocity-tailored text information into thevideo feature via feature interaction on channel and temporal domains atdifferent velocities. The above two modules compensate for each other topredict query categories more accurately under the few-shot settings.Experimental results show our method outperforms current state-of-the-artmethods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101,Kinetics, and SSv2-small).</description><author>Hongyu Qu, Rui Yan, Xiangbo Shu, Hailiang Gao, Peng Huang, Guo-Sen Xie</author><pubDate>Sat, 11 May 2024 16:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02077v2</guid></item><item><title>Deep Learning Approaches for Human Action Recognition in Video Data</title><link>http://arxiv.org/abs/2403.06810v1</link><description>Human action recognition in videos is a critical task with significantimplications for numerous applications, including surveillance, sportsanalytics, and healthcare. The challenge lies in creating models that are bothprecise in their recognition capabilities and efficient enough for practicaluse. This study conducts an in-depth analysis of various deep learning modelsto address this challenge. Utilizing a subset of the UCF101 Videos dataset, wefocus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks(RNNs), and Two-Stream ConvNets. The research reveals that while CNNseffectively capture spatial features and RNNs encode temporal sequences,Two-Stream ConvNets exhibit superior performance by integrating spatial andtemporal dimensions. These insights are distilled from the evaluation metricsof accuracy, precision, recall, and F1-score. The results of this studyunderscore the potential of composite models in achieving robust human actionrecognition and suggest avenues for future research in optimizing these modelsfor real-world deployment.</description><author>Yufei Xie</author><pubDate>Mon, 11 Mar 2024 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06810v1</guid></item><item><title>MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2404.10210v1</link><description>In recent years, skeleton-based action recognition, leveraging multimodalGraph Convolutional Networks (GCN), has achieved remarkable results. However,due to their deep structure and reliance on continuous floating-pointoperations, GCN-based methods are energy-intensive. To address this issue, wepropose an innovative Spiking Graph Convolutional Network with MultimodalFusion and Knowledge Distillation (MK-SGN). By merging the energy efficiency ofSpiking Neural Network (SNN) with the graph representation capability of GCN,the proposed MK-SGN reduces energy consumption while maintaining recognitionaccuracy. Firstly, we convert GCN into Spiking Graph Convolutional Network(SGN) and construct a foundational Base-SGN for skeleton-based actionrecognition, establishing a new benchmark and paving the way for futureresearch exploration. Secondly, we further propose a Spiking Multimodal Fusionmodule (SMF), leveraging mutual information to process multimodal data moreefficiently. Additionally, we introduce a spiking attention mechanism anddesign a Spatio Graph Convolution module with a Spatial Global SpikingAttention mechanism (SA-SGC), enhancing feature learning capability.Furthermore, we delve into knowledge distillation methods from multimodal GCNto SGN and propose a novel, integrated method that simultaneously focuses onboth intermediate layer distillation and soft label distillation to improve theperformance of SGN. On two challenging datasets for skeleton-based actionrecognition, MK-SGN outperforms the state-of-the-art GCN-like frameworks inreducing computational load and energy consumption. In contrast, typical GCNmethods typically consume more than 35mJ per action sample, while MK-SGNreduces energy consumption by more than 98%.</description><author>Naichuan Zheng, Hailun Xia, Zeyu Liang</author><pubDate>Tue, 16 Apr 2024 02:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10210v1</guid></item><item><title>ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More</title><link>http://arxiv.org/abs/2403.12534v1</link><description>Event cameras have recently been shown beneficial for practical vision tasks,such as action recognition, thanks to their high temporal resolution, powerefficiency, and reduced privacy concerns. However, current research is hinderedby 1) the difficulty in processing events because of their prolonged durationand dynamic actions with complex and ambiguous semantics and 2) the redundantaction depiction of the event frame representation with fixed stacks. We findlanguage naturally conveys abundant semantic information, rendering itstunningly superior in reducing semantic uncertainty. In light of this, wepropose ExACT, a novel approach that, for the first time, tackles event-basedaction recognition from a cross-modal conceptualizing perspective. Our ExACTbrings two technical contributions. Firstly, we propose an adaptivefine-grained event (AFE) representation to adaptively filter out the repeatedevents for the stationary objects while preserving dynamic ones. This subtlyenhances the performance of ExACT without extra computational cost. Then, wepropose a conceptual reasoning-based uncertainty estimation module, whichsimulates the recognition process to enrich the semantic representation. Inparticular, conceptual reasoning builds the temporal relation based on theaction semantics, and uncertainty estimation tackles the semantic uncertaintyof actions based on the distributional representation. Experiments show thatour ExACT achieves superior recognition accuracy of 94.83%(+2.23%),90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.</description><author>Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang</author><pubDate>Tue, 19 Mar 2024 09:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12534v1</guid></item><item><title>Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2212.04873v3</link><description>Current methods for few-shot action recognition mainly fall into the metriclearning framework following ProtoNet, which demonstrates the importance ofprototypes. Although they achieve relatively good performance, the effect ofmultimodal information is ignored, e.g. label texts. In this work, we propose anovel MultimOdal PRototype-ENhanced Network (MORN), which uses the semanticinformation of label texts as multimodal information to enhance prototypes. ACLIP visual encoder and a frozen CLIP text encoder are introduced to obtainfeatures with good multimodal initialization. Then in the visual flow, visualprototypes are computed by a visual prototype-computed module. In the textflow, a semantic-enhanced (SE) module and an inflating operation are used toobtain text prototypes. The final multimodal prototypes are then computed by amultimodal prototype-enhanced (MPE) module. Besides, we define a PRototypeSImilarity DiffErence (PRIDE) to evaluate the quality of prototypes, which isused to verify our improvement on the prototype level and effectiveness ofMORN. We conduct extensive experiments on four popular few-shot actionrecognition datasets: HMDB51, UCF101, Kinetics and SSv2, and MORN achievesstate-of-the-art results. When plugging PRIDE into the training stage, theperformance can be further improved.</description><author>Xinzhe Ni, Yong Liu, Hao Wen, Yatai Ji, Jing Xiao, Yujiu Yang</author><pubDate>Tue, 21 May 2024 07:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04873v3</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v3</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Gang Yan, Si-Fan Zhang, Jin-Rong Zhang, Wen-Yue Chen, Xue-Hai Xu</author><pubDate>Tue, 09 Apr 2024 14:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v3</guid></item><item><title>SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v4</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, previous GCN-based methods rely onelaborate human priors excessively and construct complex feature aggregationmechanisms, which limits the generalizability and effectiveness of networks. Tosolve these problems, we propose a novel Spatial Topology Gating Unit (STGU),an MLP-based variant without extra priors, to capture the co-occurrencetopology features that encode the spatial dependency across all joints. InSTGU, to learn the point-wise topology features, a new gate-based featureinteraction mechanism is introduced to activate the features point-to-point bythe attention map generated from the input sample. Based on the STGU, wepropose the first MLP-based model, SiT-MLP, for skeleton-based actionrecognition in this work. Compared with previous methods on three large-scaledatasets, SiT-MLP achieves competitive performance. In addition, SiT-MLPreduces the parameters significantly with favorable results. The code will beavailable at https://github.com/BUPTSJZhang/SiT?MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Mon, 08 Apr 2024 15:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v4</guid></item><item><title>CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner</title><link>http://arxiv.org/abs/2403.10082v1</link><description>Most existing one-shot skeleton-based action recognition focuses on rawlow-level information (e.g., joint location), and may suffer from localinformation loss and low generalization ability. To alleviate these, we proposeto leverage text description generated from large language models (LLM) thatcontain high-level human knowledge, to guide feature learning, in aglobal-local-global way. Particularly, during training, we design $2$ promptsto gain global and local text descriptions of each action from an LLM. We firstutilize the global text description to guide the skeleton encoder focus oninformative joints (i.e.,global-to-local). Then we build non-local interactionbetween local text and joint features, to form the final global representation(i.e., local-to-global). To mitigate the asymmetry issue between the trainingand inference phases, we further design a dual-branch architecture that allowsthe model to perform novel class inference without any text input, also makingthe additional inference cost neglectable compared with the base skeletonencoder. Extensive experiments on three different benchmarks show that CrossGLGconsistently outperforms the existing SOTA methods with large margins, and theinference cost (model size) is only $2.8$\% than the previous SOTA. CrossGLGcan also serve as a plug-and-play module that can substantially enhance theperformance of different SOTA skeleton encoders with a neglectable cost duringinference. The source code will be released soon.</description><author>Tingbing Yan, Wenzheng Zeng, Yang Xiao, Xingyu Tong, Bo Tan, Zhiwen Fang, Zhiguo Cao, Joey Tianyi Zhou</author><pubDate>Fri, 15 Mar 2024 08:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10082v1</guid></item><item><title>Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery</title><link>http://arxiv.org/abs/2404.01571v1</link><description>In this article, we explore the potential of zero-shot Large MultimodalModels (LMMs) in the domain of drone perception. We focus on person detectionand action recognition tasks and evaluate two prominent LMMs, namely YOLO-Worldand GPT-4V(ision) using a publicly available dataset captured from aerialviews. Traditional deep learning approaches rely heavily on large andhigh-quality training datasets. However, in certain robotic settings, acquiringsuch datasets can be resource-intensive or impractical within a reasonabletimeframe. The flexibility of prompt-based Large Multimodal Models (LMMs) andtheir exceptional generalization capabilities have the potential torevolutionize robotics applications in these scenarios. Our findings suggestthat YOLO-World demonstrates good detection performance. GPT-4V struggles withaccurately classifying action classes but delivers promising results infiltering out unwanted region proposals and in providing a general descriptionof the scenery. This research represents an initial step in leveraging LMMs fordrone perception and establishes a foundation for future investigations in thisarea.</description><author>Christian Limberg, Artur Gonçalves, Bastien Rigault, Helmut Prendinger</author><pubDate>Tue, 02 Apr 2024 03:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01571v1</guid></item><item><title>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</title><link>http://arxiv.org/abs/2402.08875v1</link><description>The increasing variety and quantity of tagged multimedia content on platformssuch as TikTok provides an opportunity to advance computer vision modeling. Wehave curated a distinctive dataset of 283,582 unique video clips categorizedunder 386 hashtags relating to modern human actions. We release this dataset asa valuable resource for building domain-specific foundation models for humanmovement modeling tasks such as action recognition. To validate this dataset,which we name TikTokActions, we perform two sets of experiments. First, wepretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone onTikTokActions subset, and then fine-tune and evaluate on popular datasets suchas UCF101 and the HMDB51. We find that the performance of the model pre-trainedusing our Tik-Tok dataset is comparable to models trained on larger actionrecognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, ourinvestigation into the relationship between pre-training dataset size andfine-tuning performance reveals that beyond a certain threshold, theincremental benefit of larger training sets diminishes. This work introduces auseful TikTok video dataset that is available for public use and providesinsights into the marginal benefit of increasing pre-training dataset sizes forvideo-based foundation models.</description><author>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington</author><pubDate>Wed, 14 Feb 2024 00:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08875v1</guid></item><item><title>Leveraging Temporal Contextualization for Video Action Recognition</title><link>http://arxiv.org/abs/2404.09490v1</link><description>Pretrained vision-language models have shown effectiveness in videounderstanding. However, recent studies have not sufficiently leveragedessential temporal information from videos, simply averaging frame-wiserepresentations or referencing consecutive frames. We introduce TemporallyContextualized CLIP (TC-CLIP), a pioneering framework for video understandingthat effectively and efficiently leverages comprehensive video information. Wepropose Temporal Contextualization (TC), a novel layer-wise temporalinformation infusion mechanism for video that extracts core information fromeach frame, interconnects relevant information across the video to summarizeinto context tokens, and ultimately leverages the context tokens during thefeature encoding process. Furthermore, our Video-conditional Prompting (VP)module manufactures context tokens to generate informative prompts in textmodality. We conduct extensive experiments in zero-shot, few-shot,base-to-novel, and fully-supervised action recognition to validate thesuperiority of our TC-CLIP. Ablation studies for TC and VP guarantee our designchoices. Code is available at https://github.com/naver-ai/tc-clip</description><author>Minji Kim, Dongyoon Han, Taekyung Kim, Bohyung Han</author><pubDate>Mon, 15 Apr 2024 07:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09490v1</guid></item><item><title>Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models</title><link>http://arxiv.org/abs/2403.08420v1</link><description>Industrial managements, including quality control, cost and safetyoptimization, etc., heavily rely on high quality industrial human actionrecognitions (IHARs) which were hard to be implemented in large-scaleindustrial scenes due to their high costs and poor real-time performance. Inthis paper, we proposed a large-scale foundation model(LSFM)-based IHAR method,wherein various LSFMs and lightweight methods were jointly used, for the firsttime, to fulfill low-cost dataset establishment and real-time IHARs.Comprehensive tests on in-situ large-scale industrial manufacturing lineselucidated that the proposed method realized great reduction on employmentcosts, superior real-time performance, and satisfactory accuracy andgeneralization capabilities, indicating its great potential as a backbone IHARmethod, especially for large-scale industrial applications.</description><author>Wensheng Liang, Ruiyan Zhuang, Xianwei Shi, Shuai Li, Zhicheng Wang, Xiaoguang Ma</author><pubDate>Wed, 13 Mar 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08420v1</guid></item><item><title>POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2405.08036v2</link><description>Value function factorization methods are commonly used in cooperativemulti-agent reinforcement learning, with QMIX receiving significant attention.Many QMIX-based methods introduce monotonicity constraints between the jointaction value and individual action values to achieve decentralized execution.However, such constraints limit the representation capacity of valuefactorization, restricting the joint action values it can represent andhindering the learning of the optimal policy. To address this challenge, wepropose the Potentially Optimal joint actions Weighted QMIX (POWQMIX)algorithm, which recognizes the potentially optimal joint actions and assignshigher weights to the corresponding losses of these joint actions duringtraining. We theoretically prove that with such a weighted training approachthe optimal policy is guaranteed to be recovered. Experiments in matrix games,predator-prey, and StarCraft II Multi-Agent Challenge environments demonstratethat our algorithm outperforms the state-of-the-art value-based multi-agentreinforcement learning methods.</description><author>Chang Huang, Junqiao Zhao, Shatong Zhu, Hongtu Zhou, Chen Ye, Tiantian Feng, Changjun Jiang</author><pubDate>Wed, 15 May 2024 06:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08036v2</guid></item><item><title>Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2404.02624v1</link><description>Skeleton-based gesture recognition methods have achieved high success usingGraph Convolutional Network (GCN). In addition, context-dependent adaptivetopology as a neighborhood vertex information and attention mechanism leveragesa model to better represent actions. In this paper, we propose self-attentionGCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN toeffectively improve modeling ability to achieve state-of-the-art results onseveral datasets. We utilize spatial self-attention module with adaptivetopology to understand intra-frame interactions within a frame among differentbody parts, and temporal self-attention module to examine correlations betweenframes of a node. These two are followed by multi-scale convolution networkwith dilations, which not only captures the long-range temporal dependencies ofjoints but also the long-range spatial dependencies (i.e., long-distancedependencies) of node temporal behaviors. They are combined into high-levelspatial-temporal representations and output the predicted action with thesoftmax classifier.</description><author>Ikuo Nakamura</author><pubDate>Wed, 03 Apr 2024 11:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02624v1</guid></item><item><title>Spiking Neural Networks for event-based action recognition: A new task to understand their advantage</title><link>http://arxiv.org/abs/2209.14915v3</link><description>Spiking Neural Networks (SNN) are characterised by their unique temporaldynamics, but the properties and advantages of such computations are still notwell understood. In order to provide answers, in this work we demonstrate howSpiking neurons can enable temporal feature extraction in feed-forward neuralnetworks without the need for recurrent synapses, and how recurrent SNNs canachieve comparable results to LSTM with a smaller number of parameters. Thisshows how their bio-inspired computing principles can be successfully exploitedbeyond energy efficiency gains and evidences their differences with respect toconventional artificial neural networks. These results are obtained through anew task, DVS-Gesture-Chain (DVS-GC), which allows, for the first time, toevaluate the perception of temporal dependencies in a real event-based actionrecognition dataset. Our study proves how the widely used DVS Gesture benchmarkcan be solved by networks without temporal feature extraction when its eventsare accumulated in frames, unlike the new DVS-GC which demands an understandingof the order in which events happen. Furthermore, this setup allowed us toreveal the role of the leakage rate in spiking neurons for temporal processingtasks and demonstrated the benefits of "hard reset" mechanisms. Additionally,we also show how time-dependent weights and normalization can lead tounderstanding order by means of temporal attention.</description><author>Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl</author><pubDate>Fri, 07 Jun 2024 15:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14915v3</guid></item><item><title>Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition</title><link>http://arxiv.org/abs/2311.15619v2</link><description>Large-scale visual-language pre-trained models have achieved significantsuccess in various video tasks. However, most existing methods follow an "adaptthen align" paradigm, which adapts pre-trained image encoders to modelvideo-level representations and utilizes one-hot or text embedding of theaction labels for supervision. This paradigm overlooks the challenge of mappingfrom static images to complicated activity concepts. In this paper, we proposea novel "Align before Adapt" (ALT) paradigm. Prior to adapting to videorepresentation learning, we exploit the entity-to-region alignments for eachframe. The alignments are fulfilled by matching the region-aware imageembeddings to an offline-constructed text corpus. With the aligned entities, wefeed their text embeddings to a transformer-based video adapter as the queries,which can help extract the semantics of the most important entities from avideo to a vector. This paradigm reuses the visual-language alignment of VLPduring adaptation and tries to explain an action by the underlying entities.This helps understand actions by bridging the gap with complex activitysemantics, particularly when facing unfamiliar or unseen categories. ALTdemonstrates competitive performance while maintaining remarkably lowcomputational costs. In fully supervised experiments, it achieves 88.1% top-1accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms theprevious state-of-the-art methods in both zero-shot and few-shot experiments,emphasizing its superior generalizability across various learning scenarios.</description><author>Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng</author><pubDate>Tue, 19 Mar 2024 18:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15619v2</guid></item><item><title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation</title><link>http://arxiv.org/abs/2311.08393v3</link><description>The learn-from-observation (LfO) paradigm is a human-inspired mode for arobot to learn to perform a task simply by watching it being performed. LfO canfacilitate robot integration on factory floors by minimizing disruption andreducing tedious programming. A key component of the LfO pipeline is atransformation of the depth camera frames to the corresponding task state andaction pairs, which are then relayed to learning techniques such as imitationor inverse reinforcement learning for understanding the task parameters. Whileseveral existing computer vision models analyze videos for activityrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,SA-Net and many other models analyze frame data captured from a singleviewpoint. Their analysis is therefore highly sensitive to occlusions of theobserved task, which are frequent in deployments. An obvious way of reducingocclusions is to simultaneously observe the task from multiple viewpoints andsynchronously fuse the multiple streams in the model. Toward this, we presentmulti-view SA-Net, which generalizes the SA-Net model to allow the perceptionof multiple viewpoints of the task activity, integrate them, and betterrecognize the state and action in each frame. Performance evaluations on twodistinct domains establish that MVSA-Net recognizes the state-action pairsunder occlusion more accurately compared to single-view MVSA-Net and otherbaselines. Our ablation studies further evaluate its performance underdifferent ambient conditions and establish the contribution of the architecturecomponents. As such, MVSA-Net offers a significantly more robust and deployablestate-action trajectory generation compared to previous methods.</description><author>Ehsan Asali, Prashant Doshi, Jin Sun</author><pubDate>Mon, 08 Apr 2024 03:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08393v3</guid></item><item><title>DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition</title><link>http://arxiv.org/abs/2404.14890v1</link><description>As one of the fundamental video tasks in computer vision, Open-VocabularyAction Recognition (OVAR) recently gains increasing attention, with thedevelopment of vision-language pre-trainings. To enable generalization ofarbitrary classes, existing methods treat class labels as text descriptions,then formulate OVAR as evaluating embedding similarity between visual samplesand textual classes. However, one crucial issue is completely ignored: theclass descriptions given by users may be noisy, e.g., misspellings and typos,limiting the real-world practicality of vanilla OVAR. To fill the research gap,this paper pioneers to evaluate existing methods by simulating multi-levelnoises of various types, and reveals their poor robustness. To tackle the noisyOVAR task, we further propose one novel DENOISER framework, covering two parts:generation and discrimination. Concretely, the generative part denoises noisyclass-text names via one decoding process, i.e., propose text candidates, thenutilize inter-modal and intra-modal information to vote for the best. At thediscriminative part, we use vanilla OVAR models to assign visual samples toclass-text names, thus obtaining more semantics. For optimization, wealternately iterate between generative and discriminative parts for progressiverefinements. The denoised text classes help OVAR models classify visual samplesmore accurately; in return, classified visual samples help better denoising. Onthree datasets, we carry out extensive experiments to show our superiorrobustness, and thorough ablations to dissect the effectiveness of eachcomponent.</description><author>Haozhe Cheng, Cheng Ju, Haicheng Wang, Jinxiang Liu, Mengting Chen, Qiang Hu, Xiaoyun Zhang, Yanfeng Wang</author><pubDate>Tue, 23 Apr 2024 11:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14890v1</guid></item><item><title>An Improved Graph Pooling Network for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2404.16359v1</link><description>Pooling is a crucial operation in computer vision, yet the unique structureof skeletons hinders the application of existing pooling strategies to skeletongraph modelling. In this paper, we propose an Improved Graph Pooling Network,referred to as IGPN. The main innovations include: Our method incorporates aregion-awareness pooling strategy based on structural partitioning. Thecorrelation matrix of the original feature is used to adaptively adjust theweight of information in different regions of the newly generated features,resulting in more flexible and effective processing. To prevent theirreversible loss of discriminative information, we propose a cross fusionmodule and an information supplement module to provide block-level andinput-level information respectively. As a plug-and-play structure, theproposed operation can be seamlessly combined with existing GCN-based models.We conducted extensive evaluations on several challenging benchmarks, and theexperimental results indicate the effectiveness of our proposed solutions. Forexample, in the cross-subject evaluation of the NTU-RGB+D 60 dataset, IGPNachieves a significant improvement in accuracy compared to the baseline whilereducing Flops by nearly 70%; a heavier version has also been introduced tofurther boost accuracy.</description><author>Cong Wu, Xiao-Jun Wu, Tianyang Xu, Josef Kittler</author><pubDate>Thu, 25 Apr 2024 07:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16359v1</guid></item><item><title>GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action Recognition</title><link>http://arxiv.org/abs/2401.10039v2</link><description>Vision-Language Models (VLMs), pre-trained on large-scale datasets, haveshown impressive performance in various visual recognition tasks. Thisadvancement paves the way for notable performance in Zero-Shot EgocentricAction Recognition (ZS-EAR). Typically, VLMs handle ZS-EAR as a globalvideo-text matching task, which often leads to suboptimal alignment of visionand linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs,emphasizing fine-grained concept-description alignment that capitalizes on therich semantic and contextual details in egocentric videos. In this paper, weintroduce GPT4Ego, a straightforward yet remarkably potent VLM framework forZS-EAR, designed to enhance the fine-grained alignment of concept anddescription between vision and language. Extensive experiments demonstrateGPT4Ego significantly outperforms existing VLMs on three large-scale egocentricvideo benchmarks, i.e., EPIC-KITCHENS-100 (33.2%, +9.4%), EGTEA (39.6%, +5.5%),and CharadesEgo (31.5%, +2.6%).</description><author>Guangzhao Dai, Xiangbo Shu, Wenhao Wu, Rui Yan, Jiachao Zhang</author><pubDate>Sat, 11 May 2024 19:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10039v2</guid></item><item><title>Density-Guided Label Smoothing for Temporal Localization of Driving Actions</title><link>http://arxiv.org/abs/2403.06616v1</link><description>Temporal localization of driving actions plays a crucial role in advanceddriver-assistance systems and naturalistic driving studies. However, this is achallenging task due to strict requirements for robustness, reliability andaccurate localization. In this work, we focus on improving the overallperformance by efficiently utilizing video action recognition networks andadapting these to the problem of action localization. To this end, we firstdevelop a density-guided label smoothing technique based on label probabilitydistributions to facilitate better learning from boundary video-segments thattypically include multiple labels. Second, we design a post-processing step toefficiently fuse information from video-segments and multiple camera views intoscene-level predictions, which facilitates elimination of false positives. Ourmethodology yields a competitive performance on the A2 test set of thenaturalistic driving action recognition track of the 2022 NVIDIA AI CityChallenge with an F1 score of 0.271.</description><author>Tunc Alkanat, Erkut Akdag, Egor Bondarev, Peter H. N. De With</author><pubDate>Mon, 11 Mar 2024 12:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06616v1</guid></item><item><title>Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition</title><link>http://arxiv.org/abs/2204.07935v2</link><description>Subject-invariant facial action unit (AU) recognition remains challenging forthe reason that the data distribution varies among subjects. In this paper, wepropose a causal inference framework for subject-invariant facial action unitrecognition. To illustrate the causal effect existing in AU recognition task,we formulate the causalities among facial images, subjects, latent AU semanticrelations, and estimated AU occurrence probabilities via a structural causalmodel. By constructing such a causal diagram, we clarify the causal effectamong variables and propose a plug-in causal intervention module, CIS, todeconfound the confounder \emph{Subject} in the causal diagram. Extensiveexperiments conducted on two commonly used AU benchmark datasets, BP4D andDISFA, show the effectiveness of our CIS, and the model with CIS inserted,CISNet, has achieved state-of-the-art performance.</description><author>Yingjie Chen, Diqi Chen, Tao Wang, Yizhou Wang, Yun Liang</author><pubDate>Wed, 03 Apr 2024 03:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07935v2</guid></item><item><title>Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception</title><link>http://arxiv.org/abs/2403.11616v1</link><description>For training a video-based action recognition model that accepts multi-viewvideo, annotating frame-level labels is tedious and difficult. However, it isrelatively easy to annotate sequence-level labels. This kind of coarseannotations are called as weak labels. However, training a multi-viewvideo-based action recognition model with weak labels for frame-levelperception is challenging. In this paper, we propose a novel learningframework, where the weak labels are first used to train a multi-viewvideo-based base model, which is subsequently used for downstream frame-levelperception tasks. The base model is trained to obtain individual latentembeddings for each view in the multi-view input. For training the model usingthe weak labels, we propose a novel latent loss function. We also propose amodel that uses the view-specific latent embeddings for downstream frame-levelaction recognition and detection tasks. The proposed framework is evaluatedusing the MM Office dataset by comparing several baseline algorithms. Theresults show that the proposed base model is effectively trained using weaklabels and the latent embeddings help the downstream models improve accuracy.</description><author>Vijay John, Yasutomo Kawanishi</author><pubDate>Mon, 18 Mar 2024 10:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11616v1</guid></item><item><title>Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception</title><link>http://arxiv.org/abs/2403.11616v2</link><description>For training a video-based action recognition model that accepts multi-viewvideo, annotating frame-level labels is tedious and difficult. However, it isrelatively easy to annotate sequence-level labels. This kind of coarseannotations are called as weak labels. However, training a multi-viewvideo-based action recognition model with weak labels for frame-levelperception is challenging. In this paper, we propose a novel learningframework, where the weak labels are first used to train a multi-viewvideo-based base model, which is subsequently used for downstream frame-levelperception tasks. The base model is trained to obtain individual latentembeddings for each view in the multi-view input. For training the model usingthe weak labels, we propose a novel latent loss function. We also propose amodel that uses the view-specific latent embeddings for downstream frame-levelaction recognition and detection tasks. The proposed framework is evaluatedusing the MM Office dataset by comparing several baseline algorithms. Theresults show that the proposed base model is effectively trained using weaklabels and the latent embeddings help the downstream models improve accuracy.</description><author>Vijay John, Yasutomo Kawanishi</author><pubDate>Tue, 19 Mar 2024 06:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11616v2</guid></item><item><title>Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition</title><link>http://arxiv.org/abs/2404.06443v1</link><description>Human facial action units (AUs) are mutually related in a hierarchicalmanner, as not only they are associated with each other in both spatial andtemporal domains but also AUs located in the same/close facial regions showstronger relationships than those of different facial regions. While none ofexisting approach thoroughly model such hierarchical inter-dependencies amongAUs, this paper proposes to comprehensively model multi-scale AU-relateddynamic and hierarchical spatio-temporal relationship among AUs for theiroccurrences recognition. Specifically, we first propose a novel multi-scaletemporal differencing network with an adaptive weighting block to explicitlycapture facial dynamics across frames at different spatial scales, whichspecifically considers the heterogeneity of range and magnitude in differentAUs' activation. Then, a two-stage strategy is introduced to hierarchicallymodel the relationship among AUs based on their spatial distribution (i.e.,local and cross-region AU relationship modelling). Experimental resultsachieved on BP4D and DISFA show that our approach is the new state-of-the-artin the field of AU occurrence recognition. Our code is publicly available athttps://github.com/CVI-SZU/MDHR.</description><author>Zihan Wang, Siyang Song, Cheng Luo, Songhe Deng, Weicheng Xie, Linlin Shen</author><pubDate>Tue, 09 Apr 2024 17:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06443v1</guid></item><item><title>Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability</title><link>http://arxiv.org/abs/2402.10510v1</link><description>Goal recognition is a fundamental cognitive process that enables individualsto infer intentions based on available cues. Current goal recognitionalgorithms often take only observed actions as input, but here we use aBayesian framework to explore the role of actions, timing, and goal solvabilityin goal recognition. We analyze human responses to goal-recognition problems inthe Sokoban domain, and find that actions are assigned most importance, butthat timing and solvability also influence goal recognition in some cases,especially when actions are uninformative. We leverage these findings todevelop a goal recognition model that matches human inferences more closelythan do existing algorithms. Our work provides new insight into human goalrecognition and takes a step towards more human-like AI models.</description><author>Chenyuan Zhang, Charles Kemp, Nir Lipovetzky</author><pubDate>Fri, 16 Feb 2024 08:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10510v1</guid></item><item><title>MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for Dynamic Facial Expression Recognition</title><link>http://arxiv.org/abs/2404.08433v1</link><description>Unlike typical video action recognition, Dynamic Facial ExpressionRecognition (DFER) does not involve distinct moving targets but relies onlocalized changes in facial muscles. Addressing this distinctive attribute, wepropose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Ourapproach takes spatial features of different scales extracted by CNN and feedsthem into a Multi-scale Embedding Layer (MELayer). The MELayer extractsmulti-scale spatial information and encodes these features before sending theminto a Temporal Transformer (T-Former). The T-Former simultaneously extractstemporal information while continually integrating multi-scale spatialinformation. This process culminates in the generation of multi-scalespatio-temporal features that are utilized for the final classification. Ourmethod achieves state-of-the-art results on two in-the-wild datasets.Furthermore, a series of ablation experiments and visualizations providefurther validation of our approach's proficiency in leveraging spatio-temporalinformation within DFER.</description><author>Linhuang Wang, Xin Kang, Fei Ding, Satoshi Nakagawa, Fuji Ren</author><pubDate>Fri, 12 Apr 2024 13:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08433v1</guid></item><item><title>3DInAction: Understanding Human Actions in 3D Point Clouds</title><link>http://arxiv.org/abs/2303.06346v2</link><description>We propose a novel method for 3D point cloud action recognition.Understanding human actions in RGB videos has been widely studied in recentyears, however, its 3D point cloud counterpart remains under-explored. This ismostly due to the inherent limitation of the point cloud data modality -- lackof structure, permutation invariance, and varying number of points -- whichmakes it difficult to learn a spatio-temporal representation. To address thislimitation, we propose the 3DinAction pipeline that first estimates patchesmoving in time (t-patches) as a key building block, alongside a hierarchicalarchitecture that learns an informative spatio-temporal representation. We showthat our method achieves improved performance on existing datasets, includingDFAUST and IKEA ASM. Code is publicly available athttps://github.com/sitzikbs/3dincaction.</description><author>Yizhak Ben-Shabat, Oren Shrout, Stephen Gould</author><pubDate>Fri, 29 Mar 2024 16:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06346v2</guid></item><item><title>PREGO: online mistake detection in PRocedural EGOcentric videos</title><link>http://arxiv.org/abs/2404.01933v2</link><description>Promptly identifying procedural errors from egocentric videos in an onlinesetting is highly challenging and valuable for detecting mistakes as soon asthey happen. This capability has a wide range of applications across variousfields, such as manufacturing and healthcare. The nature of procedural mistakesis open-set since novel types of failures might occur, which calls forone-class classifiers trained on correctly executed procedures. However, notechnique can currently detect open-set procedural mistakes online. We proposePREGO, the first online one-class classification model for mistake detection inPRocedural EGOcentric videos. PREGO is based on an online action recognitioncomponent to model the current action, and a symbolic reasoning module topredict the next actions. Mistake detection is performed by comparing therecognized current action with the expected future one. We evaluate PREGO ontwo procedural egocentric video datasets, Assembly101 and Epic-tent, which weadapt for online benchmarking of procedural mistake detection to establishsuitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,respectively.</description><author>Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso</author><pubDate>Fri, 17 May 2024 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01933v2</guid></item><item><title>EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?</title><link>http://arxiv.org/abs/2405.17719v2</link><description>Egocentric video-language pretraining is a crucial paradigm to advance thelearning of egocentric hand-object interactions (EgoHOI). Despite the greatsuccess on existing testbeds, these benchmarks focus more on closed-set visualconcepts or limited scenarios. Due to the occurrence of diverse EgoHOIs in thereal world, we propose an open-vocabulary benchmark named EgoHOIBench to revealthe diminished performance of current egocentric video-language models (EgoVLM)on fined-grained concepts, indicating that these models still lack a fullspectrum of egocentric understanding. We attribute this performance gap toinsufficient fine-grained supervision and strong bias towards understandingobjects rather than temporal dynamics in current methods. To tackle theseissues, we introduce a novel asymmetric contrastive objective for EgoHOI namedEgoNCE++. For video-to-text loss, we enhance text supervision through thegeneration of negative captions by leveraging the in-context learning of largelanguage models to perform HOI-related word substitution. For text-to-videoloss, we propose an object-centric positive video sampling strategy thataggregates video representations by the same nouns. Our extensive experimentsdemonstrate that EgoNCE++ significantly boosts open-vocabulary HOI recognition,multi-instance retrieval, and action recognition tasks across variousegocentric models, with improvements of up to +26.55%. Our code is available athttps://github.com/xuboshen/EgoNCEpp.</description><author>Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, Sipeng Zheng, Qin Jin</author><pubDate>Mon, 03 Jun 2024 08:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17719v2</guid></item><item><title>PREGO: online mistake detection in PRocedural EGOcentric videos</title><link>http://arxiv.org/abs/2404.01933v1</link><description>Promptly identifying procedural errors from egocentric videos in an onlinesetting is highly challenging and valuable for detecting mistakes as soon asthey happen. This capability has a wide range of applications across variousfields, such as manufacturing and healthcare. The nature of procedural mistakesis open-set since novel types of failures might occur, which calls forone-class classifiers trained on correctly executed procedures. However, notechnique can currently detect open-set procedural mistakes online. We proposePREGO, the first online one-class classification model for mistake detection inPRocedural EGOcentric videos. PREGO is based on an online action recognitioncomponent to model the current action, and a symbolic reasoning module topredict the next actions. Mistake detection is performed by comparing therecognized current action with the expected future one. We evaluate PREGO ontwo procedural egocentric video datasets, Assembly101 and Epic-tent, which weadapt for online benchmarking of procedural mistake detection to establishsuitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,respectively.</description><author>Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso</author><pubDate>Tue, 02 Apr 2024 14:27:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01933v1</guid></item><item><title>Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition</title><link>http://arxiv.org/abs/2307.11973v2</link><description>As a fundamental aspect of human life, two-person interactions containmeaningful information about people's activities, relationships, and socialsettings. Human action recognition serves as the foundation for many smartapplications, with a strong focus on personal privacy. However, recognizingtwo-person interactions poses more challenges due to increased body occlusionand overlap compared to single-person actions. In this paper, we propose apoint cloud-based network named Two-stream Multi-level Dynamic PointTransformer for two-person interaction recognition. Our model addresses thechallenge of recognizing two-person interactions by incorporating local-regionspatial information, appearance information, and motion information. To achievethis, we introduce a designed frame selection method named Interval FrameSampling (IFS), which efficiently samples frames from videos, capturing morediscriminative information in a relatively short processing time. Subsequently,a frame features learning module and a two-stream multi-level featureaggregation module extract global and partial features from the sampled frames,effectively representing the local-region spatial information, appearanceinformation, and motion information related to the interactions. Finally, weapply a transformer to perform self-attention on the learned features for thefinal classification. Extensive experiments are conducted on two large-scaledatasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. Theresults show that our network outperforms state-of-the-art approaches in moststandard evaluation settings.</description><author>Yao Liu, Gangfeng Cui, Jiahui Luo, Xiaojun Chang, Lina Yao</author><pubDate>Tue, 14 May 2024 06:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11973v2</guid></item><item><title>Affective Behaviour Analysis via Integrating Multi-Modal Knowledge</title><link>http://arxiv.org/abs/2403.10825v1</link><description>Affective Behavior Analysis aims to facilitate technology emotionally smart,creating a world where devices can understand and react to our emotions ashumans do. To comprehensively evaluate the authenticity and applicability ofemotional behavior analysis techniques in natural environments, the 6thcompetition on Affective Behavior Analysis in-the-wild (ABAW) utilizes theAff-Wild2, Hume-Vidmimic2, and C-EXPR-DB datasets to set up five competitivetracks, i.e., Valence-Arousal (VA) Estimation, Expression (EXPR) Recognition,Action Unit (AU) Detection, Compound Expression (CE) Recognition, and EmotionalMimicry Intensity (EMI) Estimation. In this paper, we present our methoddesigns for the five tasks. Specifically, our design mainly includes threeaspects: 1) Utilizing a transformer-based feature fusion module to fullyintegrate emotional information provided by audio signals, visual images, andtranscripts, offering high-quality expression features for the downstreamtasks. 2) To achieve high-quality facial feature representations, we employMasked-Auto Encoder as the visual features extraction model and fine-tune itwith our facial dataset. 3) Considering the complexity of the video collectionscenes, we conduct a more detailed dataset division based on scenecharacteristics and train the classifier for each scene. Extensive experimentsdemonstrate the superiority of our designs.</description><author>Wei Zhang, Feng Qiu, Chen Liu, Lincheng Li, Heming Du, Tiancheng Guo, Xin Yu</author><pubDate>Sat, 16 Mar 2024 07:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10825v1</guid></item><item><title>Real-Time Multimodal Cognitive Assistant for Emergency Medical Services</title><link>http://arxiv.org/abs/2403.06734v1</link><description>Emergency Medical Services (EMS) responders often operate undertime-sensitive conditions, facing cognitive overload and inherent risks,requiring essential skills in critical thinking and rapid decision-making. Thispaper presents CognitiveEMS, an end-to-end wearable cognitive assistant systemthat can act as a collaborative virtual partner engaging in the real-timeacquisition and analysis of multimodal data from an emergency scene andinteracting with EMS responders through Augmented Reality (AR) smart glasses.CognitiveEMS processes the continuous streams of data in real-time andleverages edge computing to provide assistance in EMS protocol selection andintervention recognition. We address key technical challenges in real-timecognitive assistance by introducing three novel components: (i) a SpeechRecognition model that is fine-tuned for real-world medical emergencyconversations using simulated EMS audio recordings, augmented with syntheticdata generated by large language models (LLMs); (ii) an EMS Protocol Predictionmodel that combines state-of-the-art (SOTA) tiny language models with EMSdomain knowledge using graph-based attention mechanisms; (iii) an EMS ActionRecognition module which leverages multimodal audio and video data and protocolpredictions to infer the intervention/treatment actions taken by the respondersat the incident scene. Our results show that for speech recognition we achievesuperior performance compared to SOTA (WER of 0.290 vs. 0.618) onconversational data. Our protocol prediction component also significantlyoutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognitionachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78sfor protocol prediction on the edge and 0.31s on the server.</description><author>Keshara Weerasinghe, Saahith Janapati, Xueren Ge, Sion Kim, Sneha Iyer, John A. Stankovic, Homa Alemzadeh</author><pubDate>Mon, 11 Mar 2024 14:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06734v1</guid></item><item><title>OmniVid: A Generative Framework for Universal Video Understanding</title><link>http://arxiv.org/abs/2403.17935v1</link><description>The core of video understanding tasks, such as recognition, captioning, andtracking, is to automatically detect objects or actions in a video and analyzetheir temporal evolution. Despite sharing a common goal, different tasks oftenrely on distinct model architectures and annotation formats. In contrast,natural language processing benefits from a unified output space, i.e., textsequences, which simplifies the training of powerful foundational languagemodels, such as GPT-3, with extensive training corpora. Inspired by this, weseek to unify the output space of video understanding tasks by using languagesas labels and additionally introducing time and box tokens. In this way, avariety of video tasks could be formulated as video-grounded token generation.This enables us to address various types of video tasks, includingclassification (such as action recognition), captioning (covering clipcaptioning, video question answering, and dense video captioning), andlocalization tasks (such as visual object tracking) within a fully sharedencoder-decoder architecture, following a generative framework. Throughcomprehensive experiments, we demonstrate such a simple and straightforwardidea is quite effective and can achieve state-of-the-art or competitive resultson seven video benchmarks, providing a novel perspective for more universalvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.</description><author>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 26 Mar 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17935v1</guid></item><item><title>Aligning Actions and Walking to LLM-Generated Textual Descriptions</title><link>http://arxiv.org/abs/2404.12192v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities invarious domains, including data augmentation and synthetic data generation.This work explores the use of LLMs to generate rich textual descriptions formotion sequences, encompassing both actions and walking patterns. We leveragethe expressive power of LLMs to align motion representations with high-levellinguistic cues, addressing two distinct tasks: action recognition andretrieval of walking sequences based on appearance attributes. For actionrecognition, we employ LLMs to generate textual descriptions of actions in theBABEL-60 dataset, facilitating the alignment of motion sequences withlinguistic representations. In the domain of gait analysis, we investigate theimpact of appearance attributes on walking patterns by generating textualdescriptions of motion sequences from the DenseGait dataset using LLMs. Thesedescriptions capture subtle variations in walking styles influenced by factorssuch as clothing choices and footwear. Our approach demonstrates the potentialof LLMs in augmenting structured motion attributes and aligning multi-modalrepresentations. The findings contribute to the advancement of comprehensivemotion understanding and open up new avenues for leveraging LLMs in multi-modalalignment and data augmentation for motion analysis. We make the code publiclyavailable at https://github.com/Radu1999/WalkAndText</description><author>Radu Chivereanu, Adrian Cosma, Andy Catruna, Razvan Rughinis, Emilian Radoi</author><pubDate>Thu, 18 Apr 2024 14:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12192v1</guid></item><item><title>Language Model Guided Interpretable Video Action Reasoning</title><link>http://arxiv.org/abs/2404.01591v1</link><description>While neural networks have excelled in video action recognition tasks, theirblack-box nature often obscures the understanding of their decision-makingprocesses. Recent approaches used inherently interpretable models to analyzevideo actions in a manner akin to human reasoning. These models, however,usually fall short in performance compared to their black-box counterparts. Inthis work, we present a new framework named Language-guided InterpretableAction Recognition framework (LaIAR). LaIAR leverages knowledge from languagemodels to enhance both the recognition capabilities and the interpretability ofvideo models. In essence, we redefine the problem of understanding video modeldecisions as a task of aligning video and language models. Using the logicalreasoning captured by the language model, we steer the training of the videomodel. This integrated approach not only improves the video model'sadaptability to different domains but also boosts its overall performance.Extensive experiments on two complex video action datasets, Charades &amp; CAD-120,validates the improved performance and interpretability of our LaIAR framework.The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.</description><author>Ning Wang, Guangming Zhu, HS Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun</author><pubDate>Tue, 02 Apr 2024 03:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01591v1</guid></item><item><title>Frozen Transformers in Language Models Are Effective Visual Encoder Layers</title><link>http://arxiv.org/abs/2310.12973v2</link><description>This paper reveals that large language models (LLMs), despite being trainedsolely on textual data, are surprisingly strong encoders for purely visualtasks in the absence of language. Even more intriguingly, this can be achievedby a simple yet previously overlooked strategy -- employing a frozentransformer block from pre-trained LLMs as a constituent encoder layer todirectly process visual tokens. Our work pushes the boundaries of leveragingLLMs for computer vision tasks, significantly departing from conventionalpractices that typically necessitate a multi-modal vision-language setup withassociated language prompts, inputs, or outputs. We demonstrate that ourapproach consistently enhances performance across a diverse range of tasks,encompassing pure 2D and 3D visual recognition tasks (e.g., image and pointcloud classification), temporal modeling tasks (e.g., action recognition),non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,2D/3D visual question answering and image-text retrieval). Such improvementsare a general phenomenon, applicable to various types of LLMs (e.g., LLaMA andOPT) and different LLM transformer blocks. We additionally propose theinformation filtering hypothesis to explain the effectiveness of pre-trainedLLMs in visual encoding -- the pre-trained LLM transformer blocks discerninformative visual tokens and further amplify their effect. This hypothesis isempirically supported by the observation that the feature activation, aftertraining with LLM transformer blocks, exhibits a stronger focus on relevantregions. We hope that our work inspires new perspectives on utilizing LLMs anddeepening our understanding of their underlying mechanisms. Code is availableat https://github.com/ziqipang/LM4VisualEncoding.</description><author>Ziqi Pang, Ziyang Xie, Yunze Man, Yu-Xiong Wang</author><pubDate>Mon, 06 May 2024 16:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12973v2</guid></item><item><title>LLMs are Good Action Recognizers</title><link>http://arxiv.org/abs/2404.00532v1</link><description>Skeleton-based action recognition has attracted lots of research attention.Recently, to build an accurate skeleton-based action recognizer, a variety ofworks have been proposed. Among them, some works use large model architecturesas backbones of their recognizers to boost the skeleton data representationcapability, while some other works pre-train their recognizers on external datato enrich the knowledge. In this work, we observe that large language modelswhich have been extensively used in various natural language processing tasksgenerally hold both large model architectures and rich implicit knowledge.Motivated by this, we propose a novel LLM-AR framework, in which we investigatetreating the Large Language Model as an Action Recognizer. In our framework, wepropose a linguistic projection process to project each input action signal(i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``actionsentence''). Moreover, we also incorporate our framework with several designsto further facilitate this linguistic projection process. Extensive experimentsdemonstrate the efficacy of our proposed framework.</description><author>Haoxuan Qu, Yujun Cai, Jun Liu</author><pubDate>Sun, 31 Mar 2024 03:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00532v1</guid></item><item><title>EventRPG: Event Data Augmentation with Relevance Propagation Guidance</title><link>http://arxiv.org/abs/2403.09274v1</link><description>Event camera, a novel bio-inspired vision sensor, has drawn a lot ofattention for its low latency, low power consumption, and high dynamic range.Currently, overfitting remains a critical problem in event-based classificationtasks for Spiking Neural Network (SNN) due to its relatively weak spatialrepresentation capability. Data augmentation is a simple but efficient methodto alleviate overfitting and improve the generalization ability of neuralnetworks, and saliency-based augmentation methods are proven to be effective inthe image processing field. However, there is no approach available forextracting saliency maps from SNNs. Therefore, for the first time, we presentSpiking Layer-Time-wise Relevance Propagation rule (SLTRP) and SpikingLayer-wise Relevance Propagation rule (SLRP) in order for SNN to generatestable and accurate CAMs and saliency maps. Based on this, we propose EventRPG,which leverages relevance propagation on the spiking neural network for moreefficient augmentation. Our proposed method has been evaluated on several SNNstructures, achieving state-of-the-art performance in object recognition tasksincluding N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, aswell as action recognition task SL-Animals with an accuracy of 91.59%. Our codeis available at https://github.com/myuansun/EventRPG.</description><author>Mingyuan Sun, Donghao Zhang, Zongyuan Ge, Jiaxu Wang, Jia Li, Zheng Fang, Renjing Xu</author><pubDate>Thu, 14 Mar 2024 11:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09274v1</guid></item><item><title>VG4D: Vision-Language Model Goes 4D Video Recognition</title><link>http://arxiv.org/abs/2404.11605v1</link><description>Understanding the real world through point cloud video is a crucial aspect ofrobotics and autonomous driving systems. However, prevailing methods for 4Dpoint cloud recognition have limitations due to sensor resolution, which leadsto a lack of detailed information. Recent advances have shown thatVision-Language Models (VLM) pre-trained on web-scale text-image datasets canlearn fine-grained visual concepts that can be transferred to variousdownstream tasks. However, effectively integrating VLM into the domain of 4Dpoint clouds remains an unresolved problem. In this work, we propose theVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge fromvisual-text pre-trained models to a 4D point cloud network. Our approachinvolves aligning the 4D encoder's representation with a VLM to learn a sharedvisual and text space from training on large-scale image-text pairs. Bytransferring the knowledge of the VLM to the 4D encoder and combining the VLM,our VG4D achieves improved recognition performance. To enhance the 4D encoder,we modernize the classic dynamic point cloud backbone and propose an improvedversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.Experiments demonstrate that our method achieves state-of-the-art performancefor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120dataset. Code is available at \url{https://github.com/Shark0-0/VG4D}.</description><author>Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, Mengyuan Liu</author><pubDate>Wed, 17 Apr 2024 18:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11605v1</guid></item><item><title>From Forest to Zoo: Great Ape Behavior Recognition with ChimpBehave</title><link>http://arxiv.org/abs/2405.20025v1</link><description>This paper addresses the significant challenge of recognizing behaviors innon-human primates, specifically focusing on chimpanzees. Automated behaviorrecognition is crucial for both conservation efforts and the advancement ofbehavioral research. However, it is significantly hindered by thelabor-intensive process of manual video annotation. Despite the availability oflarge-scale animal behavior datasets, the effective application of machinelearning models across varied environmental settings poses a criticalchallenge, primarily due to the variability in data collection contexts and thespecificity of annotations. In this paper, we introduce ChimpBehave, a novel dataset featuring over 2hours of video (approximately 193,000 video frames) of zoo-housed chimpanzees,meticulously annotated with bounding boxes and behavior labels for actionrecognition. ChimpBehave uniquely aligns its behavior classes with existingdatasets, allowing for the study of domain adaptation and cross-datasetgeneralization methods between different visual settings. Furthermore, webenchmark our dataset using a state-of-the-art CNN-based action recognitionmodel, providing the first baseline results for both within and cross-datasetsettings. The dataset, models, and code can be accessed at:https://github.com/MitchFuchs/ChimpBehave</description><author>Michael Fuchs, Emilie Genty, Adrian Bangerter, Klaus Zuberbühler, Paul Cotofrei</author><pubDate>Thu, 30 May 2024 14:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20025v1</guid></item><item><title>AutoGCN -- Towards Generic Human Activity Recognition with Neural Architecture Search</title><link>http://arxiv.org/abs/2402.01313v3</link><description>This paper introduces AutoGCN, a generic Neural Architecture Search (NAS)algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks(GCNs). HAR has gained attention due to advances in deep learning, increaseddata availability, and enhanced computational capabilities. At the same time,GCNs have shown promising results in modeling relationships between body keypoints in a skeletal graph. While domain experts often craft dataset-specificGCN-based methods, their applicability beyond this specific context is severelylimited. AutoGCN seeks to address this limitation by simultaneously searchingfor the ideal hyperparameters and architecture combination within a versatilesearch space using a reinforcement controller while balancing optimalexploration and exploitation behavior with a knowledge reservoir during thesearch process. We conduct extensive experiments on two large-scale datasetsfocused on skeleton-based action recognition to assess the proposed algorithm'sperformance. Our experimental results underscore the effectiveness of AutoGCNin constructing optimal GCN architectures for HAR, outperforming conventionalNAS and GCN methods, as well as random search. These findings highlight thesignificance of a diverse search space and an expressive input representationto enhance the network performance and generalizability.</description><author>Felix Tempel, Inga Strümke, Espen Alexander F. Ihlen</author><pubDate>Tue, 12 Mar 2024 11:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01313v3</guid></item><item><title>Exploring Missing Modality in Multimodal Egocentric Datasets</title><link>http://arxiv.org/abs/2401.11470v2</link><description>Multimodal video understanding is crucial for analyzing egocentric videos,where integrating multiple sensory signals significantly enhances actionrecognition and moment localization. However, practical applications oftengrapple with incomplete modalities due to factors like privacy concerns,efficiency demands, or hardware malfunctions. Addressing this, our study delvesinto the impact of missing modalities on egocentric action recognition,particularly within transformer-based models. We introduce a novel concept-Missing Modality Token (MMT)-to maintain performance even when modalities areabsent, a strategy that proves effective in the Ego4D, Epic-Kitchens, andEpic-Sounds datasets. Our method mitigates the performance loss, reducing itfrom its original $\sim 30\%$ drop to only $\sim 10\%$ when half of the testset is modal-incomplete. Through extensive experimentation, we demonstrate theadaptability of MMT to different training scenarios and its superiority inhandling missing modalities compared to current methods. Our researchcontributes a comprehensive analysis and an innovative approach, openingavenues for more resilient multimodal systems in real-world settings.</description><author>Merey Ramazanova, Alejandro Pardo, Humam Alwassel, Bernard Ghanem</author><pubDate>Wed, 17 Apr 2024 14:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11470v2</guid></item><item><title>Dynamic 3D Point Cloud Sequences as 2D Videos</title><link>http://arxiv.org/abs/2403.01129v1</link><description>Dynamic 3D point cloud sequences serve as one of the most common andpractical representation modalities of dynamic real-world environments.However, their unstructured nature in both spatial and temporal domains posessignificant challenges to effective and efficient processing. Existing deeppoint cloud sequence modeling approaches imitate the mature 2D video learningmechanisms by developing complex spatio-temporal point neighbor grouping andfeature aggregation schemes, often resulting in methods lacking effectiveness,efficiency, and expressive power. In this paper, we propose a novel genericrepresentation called \textit{Structured Point Cloud Videos} (SPCVs).Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2Dmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatialsmoothness and temporal consistency, where the pixel values correspond to the3D coordinates of points. The structured nature of our SPCV representationallows for the seamless adaptation of well-established 2D image/videotechniques, enabling efficient and effective processing and analysis of 3Dpoint cloud sequences. To achieve such re-organization, we design aself-supervised learning pipeline that is geometrically regularized and drivenby self-reconstructive and deformation field learning objectives. Additionally,we construct SPCV-based frameworks for both low-level and high-level 3D pointcloud sequence processing and analysis tasks, including action recognition,temporal interpolation, and compression. Extensive experiments demonstrate theversatility and superiority of the proposed SPCV, which has the potential tooffer new possibilities for deep learning on unstructured 3D point cloudsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</description><author>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang</author><pubDate>Sat, 02 Mar 2024 08:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01129v1</guid></item><item><title>Video Relationship Detection Using Mixture of Experts</title><link>http://arxiv.org/abs/2403.03994v1</link><description>Machine comprehension of visual information from images and videos by neuralnetworks faces two primary challenges. Firstly, there exists a computationaland inference gap in connecting vision and language, making it difficult toaccurately determine which object a given agent acts on and represent itthrough language. Secondly, classifiers trained by a single, monolithic neuralnetwork often lack stability and generalization. To overcome these challenges,we introduce MoE-VRD, a novel approach to visual relationship detectionutilizing a mixture of experts. MoE-VRD identifies language triplets in theform of &lt; subject, predicate, object&gt; tuples to extract relationships fromvisual processing. Leveraging recent advancements in visual relationshipdetection, MoE-VRD addresses the requirement for action recognition inestablishing relationships between subjects (acting) and objects (being actedupon). In contrast to single monolithic networks, MoE-VRD employs multiplesmall models as experts, whose outputs are aggregated. Each expert in MoE-VRDspecializes in visual relationship learning and object tagging. By utilizing asparsely-gated mixture of experts, MoE-VRD enables conditional computation andsignificantly enhances neural network capacity without increasing computationalcomplexity. Our experimental results demonstrate that the conditionalcomputation capabilities and scalability of the mixture-of-experts approachlead to superior performance in visual relationship detection compared tostate-of-the-art methods.</description><author>Ala Shaabana, Zahra Gharaee, Paul Fieguth</author><pubDate>Wed, 06 Mar 2024 19:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03994v1</guid></item><item><title>Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling</title><link>http://arxiv.org/abs/2403.06978v1</link><description>In this paper, we introduce Attention Prompt Tuning (APT) - a computationallyefficient variant of prompt tuning for video-based applications such as actionrecognition. Prompt tuning approaches involve injecting a set of learnableprompts along with data tokens during fine-tuning while keeping the backbonefrozen. This approach greatly reduces the number of learnable parameterscompared to full tuning. For image-based downstream tasks, normally a couple oflearnable prompts achieve results close to those of full tuning. However,videos, which contain more complex spatiotemporal information, require hundredsof tunable prompts to achieve reasonably good results. This reduces theparameter efficiency observed in images and significantly increases latency andthe number of floating-point operations (FLOPs) during inference. To tacklethese issues, we directly inject the prompts into the keys and values of thenon-local attention mechanism within the transformer block. Additionally, weintroduce a novel prompt reparameterization technique to make APT more robustagainst hyperparameter selection. The proposed APT approach greatly reduces thenumber of FLOPs and latency while achieving a significant performance boostover the existing parameter-efficient tuning methods on UCF101, HMDB51, andSSv2 datasets for action recognition. The code and pre-trained models areavailable at https://github.com/wgcban/apt</description><author>Wele Gedara Chaminda Bandara, Vishal M. Patel</author><pubDate>Mon, 11 Mar 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06978v1</guid></item><item><title>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</title><link>http://arxiv.org/abs/2310.05737v2</link><description>While Large Language Models (LLMs) are the dominant models for generativetasks in language, they do not perform as well as diffusion models on image andvideo generation. To effectively use LLMs for visual generation, one crucialcomponent is the visual tokenizer that maps pixel-space inputs to discretetokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, avideo tokenizer designed to generate concise and expressive tokens for bothvideos and images using a common token vocabulary. Equipped with this newtokenizer, we show that LLMs outperform diffusion models on standard image andvideo generation benchmarks including ImageNet and Kinetics. In addition, wedemonstrate that our tokenizer surpasses the previously top-performing videotokenizer on two more tasks: (1) video compression comparable to thenext-generation video codec (VCC) according to human evaluations, and (2)learning effective representations for action recognition tasks.</description><author>Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</author><pubDate>Wed, 13 Mar 2024 06:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05737v2</guid></item><item><title>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</title><link>http://arxiv.org/abs/2402.05892v3</link><description>In recent years, Transformers have become the de-facto architecture forsequence modeling on text and a variety of multi-dimensional data, such asimages and video. However, the use of self-attention layers in a Transformerincurs prohibitive compute and memory complexity that scales quadraticallyw.r.t. the sequence length. A recent architecture, Mamba, based on state spacemodels has been shown to achieve comparable performance for modeling textsequences, while scaling linearly with the sequence length. In this work, wepresent Mamba-ND, a generalized design extending the Mamba architecture toarbitrary multi-dimensional data. Our design alternatively unravels the inputdata across different dimensions following row-major orderings. We provide asystematic comparison of Mamba-ND with several other alternatives, based onprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.Empirically, we show that Mamba-ND demonstrates performance competitive withthe state-of-the-art on a variety of multi-dimensional benchmarks, includingImageNet-1K classification, HMDB-51 action recognition, and ERA5 weatherforecasting.</description><author>Shufan Li, Harkanwar Singh, Aditya Grover</author><pubDate>Thu, 14 Mar 2024 17:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05892v3</guid></item><item><title>STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.16221v2</link><description>The capability to accurately estimate 3D human poses is crucial for diversefields such as action recognition, gait recognition, and virtual/augmentedreality. However, a persistent and significant challenge within this field isthe accurate prediction of human poses under conditions of severe occlusion.Traditional image-based estimators struggle with heavy occlusions due to a lackof temporal context, resulting in inconsistent predictions. While video-basedmodels benefit from processing temporal data, they encounter limitations whenfaced with prolonged occlusions that extend over multiple frames. Thischallenge arises because these models struggle to generalize beyond theirtraining datasets, and the variety of occlusions is hard to capture in thetraining data. Addressing these challenges, we propose STRIDE (Single-videobased TempoRally contInuous occlusion Robust 3D Pose Estimation), a novelTest-Time Training (TTT) approach to fit a human motion prior for each video.This approach specifically handles occlusions that were not encountered duringthe model's training. By employing STRIDE, we can refine a sequence of noisyinitial pose estimates into accurate, temporally coherent poses during testtime, effectively overcoming the limitations of prior methods. Our frameworkdemonstrates flexibility by being model-agnostic, allowing us to use anyoff-the-shelf 3D pose estimation method for improving robustness and temporalconsistency. We validate STRIDE's efficacy through comprehensive experiments onchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where itnot only outperforms existing single-image and video-based pose estimationmodels but also showcases superior handling of substantial occlusions,achieving fast, robust, accurate, and temporally consistent 3D pose estimates.</description><author>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury</author><pubDate>Thu, 14 Mar 2024 04:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16221v2</guid></item><item><title>Hierarchical NeuroSymbolic Approach for Action Quality Assessment</title><link>http://arxiv.org/abs/2403.13798v1</link><description>Action quality assessment (AQA) applies computer vision to quantitativelyassess the performance or execution of a human action. Current AQA approachesare end-to-end neural models, which lack transparency and tend to be biasedbecause they are trained on subjective human judgements as ground-truth. Toaddress these issues, we introduce a neuro-symbolic paradigm for AQA, whichuses neural networks to abstract interpretable symbols from video data andmakes quality assessments by applying rules to those symbols. We take diving asthe case study. We found that domain experts prefer our system and find it moreinformative than purely neural approaches to AQA in diving. Our system alsoachieves state-of-the-art action recognition and temporal segmentation, andautomatically generates a detailed report that breaks the dive down into itselements and provides objective scoring with visual evidence. As verified by agroup of domain experts, this report may be used to assist judges in scoring,help train judges, and provide feedback to divers. We will open-source all ofour annotated training data and code for ease of reproducibility.</description><author>Lauren Okamoto, Paritosh Parmar</author><pubDate>Wed, 20 Mar 2024 18:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13798v1</guid></item><item><title>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</title><link>http://arxiv.org/abs/2402.05892v4</link><description>In recent years, Transformers have become the de-facto architecture forsequence modeling on text and a variety of multi-dimensional data, such asimages and video. However, the use of self-attention layers in a Transformerincurs prohibitive compute and memory complexity that scales quadraticallyw.r.t. the sequence length. A recent architecture, Mamba, based on state spacemodels has been shown to achieve comparable performance for modeling textsequences, while scaling linearly with the sequence length. In this work, wepresent Mamba-ND, a generalized design extending the Mamba architecture toarbitrary multi-dimensional data. Our design alternatively unravels the inputdata across different dimensions following row-major orderings. We provide asystematic comparison of Mamba-ND with several other alternatives, based onprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.Empirically, we show that Mamba-ND demonstrates performance competitive withthe state-of-the-art on a variety of multi-dimensional benchmarks, includingImageNet-1K classification, HMDB-51 action recognition, and ERA5 weatherforecasting.</description><author>Shufan Li, Harkanwar Singh, Aditya Grover</author><pubDate>Wed, 20 Mar 2024 01:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05892v4</guid></item><item><title>Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training</title><link>http://arxiv.org/abs/2312.02914v3</link><description>In this work, we tackle the problem of unsupervised domain adaptation (UDA)for video action recognition. Our approach, which we call UNITE, uses an imageteacher model to adapt a video student model to the target domain. UNITE firstemploys self-supervised pre-training to promote discriminative feature learningon target domain videos using a teacher-guided masked distillation objective.We then perform self-training on masked target data, using the video studentmodel and image teacher model together to generate improved pseudolabels forunlabeled target videos. Our self-training process successfully leverages thestrengths of both models to achieve strong transfer performance across domains.We evaluate our approach on multiple video domain adaptation benchmarks andobserve significant improvements upon previously reported results.</description><author>Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa</author><pubDate>Thu, 21 Mar 2024 14:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02914v3</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment</title><link>http://arxiv.org/abs/2402.04599v2</link><description>Video sequences exhibit significant nuisance variations (undesired effects)of speed of actions, temporal locations, and subjects' poses, leading totemporal-viewpoint misalignment when comparing two sets of frames or evaluatingthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmeraviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3Dskeleton sequences whose camera and subjects' poses can be easily manipulatedin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), wherematching well temporal blocks (temporal chunks that make up a sequence) ofsupport-query sequence pairs (by factoring out nuisance variations) isessential due to limited samples of novel classes. Given a query sequence, wecreate its several views by simulating several camera locations. For a supportsequence, we match it with view-simulated query sequences, as in the popularDynamic Time Warping (DTW). Specifically, each support temporal block can bematched to the query temporal block with the same or adjacent (next) temporalindex, and adjacent camera views to achieve joint local temporal-viewpointwarping. JEANIE selects the smallest distance among matching paths withdifferent temporal-viewpoint warping patterns, an advantage over DTW which onlyperforms temporal alignment. We also propose an unsupervised FSAR akin toclustering of sequences with JEANIE as a distance measure. JEANIE achievesstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3DMultiview Activity II on supervised and unsupervised FSAR, and theirmeta-learning inspired fusion.</description><author>Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz</author><pubDate>Mon, 25 Mar 2024 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04599v2</guid></item><item><title>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</title><link>http://arxiv.org/abs/2403.16428v1</link><description>We interact with the world with our hands and see it through our own(egocentric) perspective. A holistic 3D understanding of such interactions fromegocentric views is important for tasks in robotics, AR/VR, action recognitionand motion generation. Accurately reconstructing such interactions in 3D ischallenging due to heavy occlusion, viewpoint bias, camera distortion, andmotion blur from the head movement. To this end, we designed the HANDS23challenge based on the AssemblyHands and ARCTIC datasets with carefullydesigned training and testing splits. Based on the results of the top submittedmethods and more recent baselines on the leaderboards, we perform a thoroughanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstratesthe effectiveness of addressing distortion specific to egocentric cameras,adopting high-capacity transformers to learn complex hand-object interactions,and fusing predictions from different views. Our study further revealschallenging scenarios intractable with state-of-the-art methods, such as fasthand motion, object reconstruction from narrow egocentric views, and closecontact between two hands and objects. Our efforts will enrich the community'sknowledge foundation and facilitate future hand studies on egocentrichand-object interactions.</description><author>Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao</author><pubDate>Mon, 25 Mar 2024 06:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16428v1</guid></item><item><title>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</title><link>http://arxiv.org/abs/2310.05737v3</link><description>While Large Language Models (LLMs) are the dominant models for generativetasks in language, they do not perform as well as diffusion models on image andvideo generation. To effectively use LLMs for visual generation, one crucialcomponent is the visual tokenizer that maps pixel-space inputs to discretetokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, avideo tokenizer designed to generate concise and expressive tokens for bothvideos and images using a common token vocabulary. Equipped with this newtokenizer, we show that LLMs outperform diffusion models on standard image andvideo generation benchmarks including ImageNet and Kinetics. In addition, wedemonstrate that our tokenizer surpasses the previously top-performing videotokenizer on two more tasks: (1) video compression comparable to thenext-generation video codec (VCC) according to human evaluations, and (2)learning effective representations for action recognition tasks.</description><author>Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</author><pubDate>Fri, 29 Mar 2024 18:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05737v3</guid></item><item><title>Disentangled Pre-training for Human-Object Interaction Detection</title><link>http://arxiv.org/abs/2404.01725v1</link><description>Detecting human-object interaction (HOI) has long been limited by the amountof supervised data available. Recent approaches address this issue bypre-training according to pseudo-labels, which align object regions with HOItriplets parsed from image captions. However, pseudo-labeling is tricky andnoisy, making HOI pre-training a complex process. Therefore, we propose anefficient disentangled pre-training method for HOI detection (DP-HOI) toaddress this problem. First, DP-HOI utilizes object detection and actionrecognition datasets to pre-train the detection and interaction decoder layers,respectively. Then, we arrange these decoder layers so that the pre-trainingarchitecture is consistent with the downstream HOI detection task. Thisfacilitates efficient knowledge transfer. Specifically, the detection decoderidentifies reliable human instances in each action recognition dataset image,generates one corresponding query, and feeds it into the interaction decoderfor verb classification. Next, we combine the human instance verb predictionsin the same image and impose image-level supervision. The DP-HOI structure canbe easily adapted to the HOI detection task, enabling effective model parameterinitialization. Therefore, it significantly enhances the performance ofexisting HOI detection models on a broad range of rare categories. The code andpre-trained weight are available at https://github.com/xingaoli/DP-HOI.</description><author>Zhuolong Li, Xingao Li, Changxing Ding, Xiangmin Xu</author><pubDate>Tue, 02 Apr 2024 09:21:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01725v1</guid></item><item><title>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</title><link>http://arxiv.org/abs/2404.04430v1</link><description>While current methods have shown promising progress on estimating 3D humanmotion from monocular videos, their motion estimates are often physicallyunrealistic because they mainly consider kinematics. In this paper, weintroduce Physics-aware Pretrained Transformer (PhysPT), which improveskinematics-based motion estimates and infers motion forces. PhysPT exploits aTransformer encoder-decoder backbone to effectively learn human dynamics in aself-supervised manner. Moreover, it incorporates physics principles governinghuman motion. Specifically, we build a physics-based body representation andcontact force model. We leverage them to impose novel physics-inspired traininglosses (i.e., force loss, contact loss, and Euler-Lagrange loss), enablingPhysPT to capture physical properties of the human body and the forces itexperiences. Experiments demonstrate that, once trained, PhysPT can be directlyapplied to kinematics-based estimates to significantly enhance their physicalplausibility and generate favourable motion forces. Furthermore, we show thatthese physically meaningful quantities translate into improved accuracy of animportant downstream task: human action recognition.</description><author>Yufei Zhang, Jeffrey O. Kephart, Zijun Cui, Qiang Ji</author><pubDate>Fri, 05 Apr 2024 23:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04430v1</guid></item><item><title>Koala: Key frame-conditioned long video-LLM</title><link>http://arxiv.org/abs/2404.04346v1</link><description>Long video question answering is a challenging task that involves recognizingshort-term activities and reasoning about their fine-grained relationships.State-of-the-art video Large Language Models (vLLMs) hold promise as a viablesolution due to their demonstrated emergent capabilities on new tasks. However,despite being trained on millions of short seconds-long videos, vLLMs areunable to understand minutes-long videos and accurately answer questions aboutthem. To address this limitation, we propose a lightweight and self-supervisedapproach, Key frame-conditioned long video-LLM (Koala), that introduceslearnable spatiotemporal queries to adapt pretrained vLLMs for generalizing tolonger videos. Our approach introduces two new tokenizers that condition onvisual tokens computed from sparse video key frames for understanding short andlong video moments. We train our proposed approach on HowTo100M and demonstrateits effectiveness on zero-shot long video understanding benchmarks, where itoutperforms state-of-the-art large models by 3 - 6% in absolute accuracy acrossall tasks. Surprisingly, we also empirically show that our approach not onlyhelps a pretrained vLLM to understand long videos but also improves itsaccuracy on short-term action recognition.</description><author>Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko</author><pubDate>Fri, 05 Apr 2024 19:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04346v1</guid></item><item><title>Understanding Video Transformers via Universal Concept Discovery</title><link>http://arxiv.org/abs/2401.10831v3</link><description>This paper studies the problem of concept-based interpretability oftransformer representations for videos. Concretely, we seek to explain thedecision-making process of video transformers based on high-level,spatiotemporal concepts that are automatically discovered. Prior research onconcept-based interpretability has concentrated solely on image-level tasks.Comparatively, video models deal with the added temporal dimension, increasingcomplexity and posing challenges in identifying dynamic concepts over time. Inthis work, we systematically address these challenges by introducing the firstVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we proposean efficient approach for unsupervised identification of units of videotransformer representations - concepts, and ranking their importance to theoutput of a model. The resulting concepts are highly interpretable, revealingspatio-temporal reasoning mechanisms and object-centric representations inunstructured video models. Performing this analysis jointly over a diverse setof supervised and self-supervised representations, we discover that some ofthese mechanism are universal in video transformers. Finally, we show that VTCDcan be used for fine-grained action recognition and video object segmentation.</description><author>Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov</author><pubDate>Wed, 10 Apr 2024 16:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10831v3</guid></item><item><title>Learning to Score Sign Language with Two-stage Method</title><link>http://arxiv.org/abs/2404.10383v1</link><description>Human action recognition and performance assessment have been hot researchtopics in recent years. Recognition problems have mature solutions in the fieldof sign language, but past research in performance analysis has focused oncompetitive sports and medical training, overlooking the scoring assessment,which is an important part of sign language teaching digitalization. In thispaper, we analyze the existing technologies for performance assessment andadopt methods that perform well in human pose reconstruction tasks combinedwith motion rotation embedded expressions, proposing a two-stage sign languageperformance evaluation pipeline. Our analysis shows that choosingreconstruction tasks in the first stage can provide more expressive features,and using smoothing methods can provide an effective reference for assessment.Experiments show that our method provides good score feedback mechanisms andhigh consistency with professional assessments compared to end-to-endevaluations.</description><author>Wen Hongli, Xu Yang</author><pubDate>Tue, 16 Apr 2024 09:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10383v1</guid></item><item><title>Learning to Score Sign Language with Two-stage Method</title><link>http://arxiv.org/abs/2404.10383v2</link><description>Human action recognition and performance assessment have been hot researchtopics in recent years. Recognition problems have mature solutions in the fieldof sign language, but past research in performance analysis has focused oncompetitive sports and medical training, overlooking the scoring assessment,which is an important part of sign language teaching digitalization. In thispaper, we analyze the existing technologies for performance assessment andadopt methods that perform well in human pose reconstruction tasks combinedwith motion rotation embedded expressions, proposing a two-stage sign languageperformance evaluation pipeline. Our analysis shows that choosingreconstruction tasks in the first stage can provide more expressive features,and using smoothing methods can provide an effective reference for assessment.Experiments show that our method provides good score feedback mechanisms andhigh consistency with professional assessments compared to end-to-endevaluations.</description><author>Hongli Wen, Yang Xu</author><pubDate>Wed, 17 Apr 2024 02:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10383v2</guid></item><item><title>Koala: Key frame-conditioned long video-LLM</title><link>http://arxiv.org/abs/2404.04346v2</link><description>Long video question answering is a challenging task that involves recognizingshort-term activities and reasoning about their fine-grained relationships.State-of-the-art video Large Language Models (vLLMs) hold promise as a viablesolution due to their demonstrated emergent capabilities on new tasks. However,despite being trained on millions of short seconds-long videos, vLLMs areunable to understand minutes-long videos and accurately answer questions aboutthem. To address this limitation, we propose a lightweight and self-supervisedapproach, Key frame-conditioned long video-LLM (Koala), that introduceslearnable spatiotemporal queries to adapt pretrained vLLMs for generalizing tolonger videos. Our approach introduces two new tokenizers that condition onvisual tokens computed from sparse video key frames for understanding short andlong video moments. We train our proposed approach on HowTo100M and demonstrateits effectiveness on zero-shot long video understanding benchmarks, where itoutperforms state-of-the-art large models by 3 - 6% in absolute accuracy acrossall tasks. Surprisingly, we also empirically show that our approach not onlyhelps a pretrained vLLM to understand long videos but also improves itsaccuracy on short-term action recognition.</description><author>Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko</author><pubDate>Fri, 19 Apr 2024 13:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04346v2</guid></item><item><title>CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment</title><link>http://arxiv.org/abs/2404.13999v1</link><description>Action Quality Assessment (AQA) is pivotal for quantifying actions acrossdomains like sports and medical care. Existing methods often rely onpre-trained backbones from large-scale action recognition datasets to boostperformance on smaller AQA datasets. However, this common strategy yieldssuboptimal results due to the inherent struggle of these backbones to capturethe subtle cues essential for AQA. Moreover, fine-tuning on smaller datasetsrisks overfitting. To address these issues, we propose Coarse-to-FineInstruction Alignment (CoFInAl). Inspired by recent advances in large languagemodel tuning, CoFInAl aligns AQA with broader pre-trained tasks byreformulating it as a coarse-to-fine classification task. Initially, it learnsgrade prototypes for coarse assessment and then utilizes fixed sub-gradeprototypes for fine-grained assessment. This hierarchical approach mirrors thejudging process, enhancing interpretability within the AQA framework.Experimental results on two long-term AQA datasets demonstrate CoFInAl achievesstate-of-the-art performance with significant correlation gains of 5.49% and3.55% on Rhythmic Gymnastics and Fis-V, respectively. Our code is available athttps://github.com/ZhouKanglei/CoFInAl_AQA.</description><author>Kanglei Zhou, Junlin Li, Ruizhi Cai, Liyuan Wang, Xingxing Zhang, Xiaohui Liang</author><pubDate>Mon, 22 Apr 2024 10:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13999v1</guid></item><item><title>Combating Missing Modalities in Egocentric Videos at Test Time</title><link>http://arxiv.org/abs/2404.15161v1</link><description>Understanding videos that contain multiple modalities is crucial, especiallyin egocentric videos, where combining various sensory inputs significantlyimproves tasks like action recognition and moment localization. However,real-world applications often face challenges with incomplete modalities due toprivacy concerns, efficiency needs, or hardware issues. Current methods, whileeffective, often necessitate retraining the model entirely to handle missingmodalities, making them computationally intensive, particularly with largetraining datasets. In this study, we propose a novel approach to address thisissue at test time without requiring retraining. We frame the problem as atest-time adaptation task, where the model adjusts to the available unlabeleddata at test time. Our method, MiDl~(Mutual information withself-Distillation), encourages the model to be insensitive to the specificmodality source present during testing by minimizing the mutual informationbetween the prediction and the available modality. Additionally, we incorporateself-distillation to maintain the model's original performance when bothmodalities are available. MiDl represents the first self-supervised, onlinesolution for handling missing modalities exclusively at test time. Throughexperiments with various pretrained models and datasets, MiDl demonstratessubstantial performance improvement without the need for retraining.</description><author>Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra</author><pubDate>Tue, 23 Apr 2024 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15161v1</guid></item></channel></rss>