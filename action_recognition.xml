<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 19 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</title><link>http://arxiv.org/abs/2402.08875v1</link><description>The increasing variety and quantity of tagged multimedia content on platformssuch as TikTok provides an opportunity to advance computer vision modeling. Wehave curated a distinctive dataset of 283,582 unique video clips categorizedunder 386 hashtags relating to modern human actions. We release this dataset asa valuable resource for building domain-specific foundation models for humanmovement modeling tasks such as action recognition. To validate this dataset,which we name TikTokActions, we perform two sets of experiments. First, wepretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone onTikTokActions subset, and then fine-tune and evaluate on popular datasets suchas UCF101 and the HMDB51. We find that the performance of the model pre-trainedusing our Tik-Tok dataset is comparable to models trained on larger actionrecognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, ourinvestigation into the relationship between pre-training dataset size andfine-tuning performance reveals that beyond a certain threshold, theincremental benefit of larger training sets diminishes. This work introduces auseful TikTok video dataset that is available for public use and providesinsights into the marginal benefit of increasing pre-training dataset sizes forvideo-based foundation models.</description><author>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington</author><pubDate>Wed, 14 Feb 2024 00:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08875v1</guid></item><item><title>Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability</title><link>http://arxiv.org/abs/2402.10510v1</link><description>Goal recognition is a fundamental cognitive process that enables individualsto infer intentions based on available cues. Current goal recognitionalgorithms often take only observed actions as input, but here we use aBayesian framework to explore the role of actions, timing, and goal solvabilityin goal recognition. We analyze human responses to goal-recognition problems inthe Sokoban domain, and find that actions are assigned most importance, butthat timing and solvability also influence goal recognition in some cases,especially when actions are uninformative. We leverage these findings todevelop a goal recognition model that matches human inferences more closelythan do existing algorithms. Our work provides new insight into human goalrecognition and takes a step towards more human-like AI models.</description><author>Chenyuan Zhang, Charles Kemp, Nir Lipovetzky</author><pubDate>Fri, 16 Feb 2024 08:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10510v1</guid></item></channel></rss>