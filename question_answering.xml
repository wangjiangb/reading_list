<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 11 Jul 2024 18:35:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison</title><link>http://arxiv.org/abs/2407.07840v1</link><description>Despite tremendous advancements, current state-of-the-art Vision-LanguageModels (VLMs) are still far from perfect. They tend to hallucinate and maygenerate biased responses. In such circumstances, having a way to assess thereliability of a given response generated by a VLM is quite useful. Existingmethods, such as estimating uncertainty using answer likelihoods orprompt-based confidence generation, often suffer from overconfidence. Othermethods use self-consistency comparison but are affected by confirmationbiases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparingthe consistency between the direct answer generated using the VLM's internalreasoning process, and the indirect answers obtained by decomposing thequestion into sub-questions and reasoning over the sub-answers produced by theVLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experimentsacross six vision-language tasks with three VLMs show \texttt{DeCC}'sreliability estimation achieves better correlation with task accuracy comparedto the existing methods.</description><author>Qian Yang, Weixiang Yan, Aishwarya Agrawal</author><pubDate>Wed, 10 Jul 2024 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07840v1</guid></item><item><title>Ramsey Theorems for Trees and a General 'Private Learning Implies Online Learning' Theorem</title><link>http://arxiv.org/abs/2407.07765v1</link><description>This work continues to investigate the link between differentially private(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed thatfor binary concept classes, DP learnability of a given class implies that ithas a finite Littlestone dimension (equivalently, that it is online learnable).Their proof relies on a model-theoretic result by Hodges (1997), whichdemonstrates that any binary concept class with a large Littlestone dimensioncontains a large subclass of thresholds. In a follow-up work, Jung, Kim, andTewari (2020) extended this proof to multiclass PAC learning with a boundednumber of labels. Unfortunately, Hodges's result does not apply in othernatural settings such as multiclass PAC learning with an unbounded label space,and PAC learning of partial concept classes. This naturally raises the question of whether DP learnability continues toimply online learnability in more general scenarios: indeed, Alon, Hanneke,Holzman, and Moran (2021) explicitly leave it as an open question in thecontext of partial concept classes, and the same question is open in thegeneral multiclass setting. In this work, we give a positive answer to thesequestions showing that for general classification tasks, DP learnabilityimplies online learnability. Our proof reasons directly about Littlestonetrees, without relying on thresholds. We achieve this by establishing severalRamsey-type theorems for trees, which might be of independent interest.</description><author>Simone Fioravanti, Steve Hanneke, Shay Moran, Hilla Schefler, Iska Tsubari</author><pubDate>Wed, 10 Jul 2024 15:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07765v1</guid></item><item><title>MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</title><link>http://arxiv.org/abs/2407.01523v2</link><description>Understanding documents with rich layouts and multi-modal components is along-standing and practical task. Recent Large Vision-Language Models (LVLMs)have made remarkable strides in various tasks, particularly in single-pagedocument understanding (DU). However, their abilities on long-context DU remainan open problem. This work presents MMLongBench-Doc, a long-context,multi-modal benchmark comprising 1,062 expert-annotated questions. Distinctfrom previous datasets, it is constructed upon 130 lengthy PDF-formatteddocuments with an average of 49.4 pages and 20,971 textual tokens. Towardscomprehensive evaluation, answers to these questions rely on pieces of evidencefrom (1) different sources (text, image, chart, table, and layout structure)and (2) various locations (i.e. page number). Moreover, 33.2% of the questionsare cross-page questions requiring evidence across multiple pages. 22.8% of thequestions are designed to be unanswerable for detecting potentialhallucinations. Experiments on 14 LVLMs demonstrate that long-context DUgreatly challenges current models. Notably, the best-performing model, GPT-4o,achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worseperformance than their LLM counterparts which are fed with lossy-parsed OCRdocuments. These results validate the necessity of future research toward morecapable long-context LVLMs. Project Page:https://mayubo2333.github.io/MMLongBench-Doc</description><author>Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun</author><pubDate>Wed, 10 Jul 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01523v2</guid></item><item><title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title><link>http://arxiv.org/abs/2403.01858v2</link><description>We present TMMLU+, a new benchmark designed for Traditional Chinese languageunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66subjects from elementary to professional level. It is six times larger andboasts a more balanced subject distribution than its predecessor, TaiwanMassive Multitask Language Understanding (TMMLU). We also benchmarkclosed-source models and 26 open-weight Chinese large language models (LLMs) ofparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings revealthat (1.) Traditional Chinese models still trail behind their SimplifiedChinese counterparts, highlighting a need for more focused advancements in LLMscatering to Traditional Chinese. (2.) Current LLMs still fall short of humanperformance in average scores, indicating a potential need for future researchto delve deeper into social science and humanities subjects. (3.) Among all thetokenization compression metrics examined, we identify that only the fertilityscore uniquely demonstrates strong correlations with our benchmark results. Weforesee that TMMLU+ will pinpoint areas for future model improvement, therebynarrowing the gap between machine and human linguistic capabilities andsupporting researchers in developing Traditional Chinese LLMs. Our dataset,along with the benchmark source code, is accessible athuggingface.co/datasets/ikala/tmmluplus.</description><author>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</author><pubDate>Wed, 10 Jul 2024 15:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01858v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</title><link>http://arxiv.org/abs/2407.07577v1</link><description>The rapid advancement of Large Vision-Language models (LVLMs) hasdemonstrated a spectrum of emergent capabilities. Nevertheless, current modelsonly focus on the visual content of a single scenario, while their ability toassociate instances across different scenes has not yet been explored, which isessential for understanding complex visual content, such as movies withmultiple characters and intricate plots. Towards movie understanding, acritical initial step for LVLMs is to unleash the potential of characteridentities memory and recognition across multiple visual scenarios. To achievethe goal, we propose visual instruction tuning with ID reference and develop anID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our researchintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory andrecognition across four dimensions: matching, location, question-answering, andcaptioning. Our findings highlight the limitations of existing LVLMs inrecognizing and associating instance identities with ID reference. This paperpaves the way for future artificial intelligence systems to possessmulti-identity visual inputs, thereby facilitating the comprehension of complexvisual narratives like movies.</description><author>Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo</author><pubDate>Wed, 10 Jul 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07577v1</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item></channel></rss>