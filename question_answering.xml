<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 20 Feb 2024 06:01:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Unsupervised LLM Adaptation for Question Answering</title><link>http://arxiv.org/abs/2402.12170v1</link><description>Large language models (LLM) learn diverse knowledge present in thelarge-scale training dataset via self-supervised training. Followed byinstruction-tuning, LLM acquires the ability to return correct information fordiverse questions. However, adapting these pre-trained LLMs to new targetdomains, such as different organizations or periods, for the question-answering(QA) task incurs a substantial annotation cost. To tackle this challenge, wepropose a novel task, unsupervised LLM adaptation for question answering. Inthis task, we leverage a pre-trained LLM, a publicly available QA dataset(source data), and unlabeled documents from the target domain. Our goal is tolearn LLM that can answer questions about the target domain. We introduce onesynthetic and two real datasets to evaluate models fine-tuned on the source andtarget data, and reveal intriguing insights; (i) fine-tuned models exhibit theability to provide correct answers for questions about the target domain eventhough they do not see any questions about the information described in theunlabeled documents, but (ii) they have difficulties in accessing informationlocated in the middle or at the end of documents, and (iii) this challenge canbe partially mitigated by replacing input tokens with random ones duringadaptation.</description><author>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</author><pubDate>Fri, 16 Feb 2024 06:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12170v1</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v3</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Sat, 17 Feb 2024 07:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v3</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Question Answering Over Spatio-Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2402.11542v1</link><description>Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledgegraphs (KGs) by incorporating time and location information. While the researchcommunity's focus on Knowledge Graph Question Answering (KGQA), the field ofanswering questions incorporating both spatio-temporal information based onSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasetsalso has hindered progress in this area. To address this issue, we presentSTQAD, a dataset comprising 10,000 natural language questions forspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,various state-of-the-art KGQA approaches fall far short of achievingsatisfactory performance on our dataset. In response, we propose STCQA, a newspatio-temporal KGQA approach that utilizes a novel STKG embedding method namedSTComplEx. By extracting temporal and spatial information from a question, ourQA model can better comprehend the question and retrieve accurate answers fromthe STKG. Through extensive experiments, we demonstrate the quality of ourdataset and the effectiveness of our STKGQA method.</description><author>Xinbang Dai, Huiying Li, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11542v1</guid></item><item><title>PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title><link>http://arxiv.org/abs/2402.11034v1</link><description>Existing work on Temporal Question Answering (TQA) has predominantly focusedon questions anchored to specific timestamps or events (e.g. "Who was the USpresident in 1970?"). Little work has studied questions whose temporal contextis relative to the present time (e.g. "Who was the previous US president?"). Werefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA posesunique challenges: (1) large language models (LLMs) may have outdatedknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') arehard to reason, (3) multi-hop reasoning may be required, and (4) the goldanswers of benchmarks must be continuously updated. To address thesechallenges, we introduce the PAT-Questions benchmark, which includes single andmulti-hop temporal questions. The answers in PAT-Questions can be automaticallyrefreshed by re-running SPARQL queries on a knowledge graph, if available. Weevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model(TEMPREASON-T5) on PAT-Questions through direct prompting andretrieval-augmented generation (RAG). The results highlight the limitations ofexisting solutions in PATQA and motivate the need for new methods to improvePATQA reasoning capabilities.</description><author>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</author><pubDate>Fri, 16 Feb 2024 19:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11034v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v1</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 16 Feb 2024 20:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v1</guid></item><item><title>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</title><link>http://arxiv.org/abs/2402.11161v1</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current answer correctness (AC) metrics do not align with human judgments,particularly verbose, free form answers from large language models (LLM). Thereare two challenges: a lack of data and that models are too big. LLM basedscorers correlate better with humans, but this expensive task has only beentested on limited QA datasets. We rectify these issues by providing clearguidelines for evaluating machine QA adopted from human QA contests. We alsointroduce Precise ANswer correctness Determination and Adjudication (PANDA), asmall, efficient, deterministic AC classifier (812 KB) that more accuratelyevaluates answer correctness.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</author><pubDate>Sat, 17 Feb 2024 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11161v1</guid></item><item><title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges</title><link>http://arxiv.org/abs/2307.11471v2</link><description>Visual question answering requires a system to provide an accurate naturallanguage answer given an image and a natural language question. However, it iswidely recognized that previous generic VQA methods often exhibit a tendency tomemorize biases present in the training data rather than learning properbehaviors, such as grounding images before predicting answers. Therefore, thesemethods usually achieve high in-distribution but poor out-of-distributionperformance. In recent years, various datasets and debiasing methods have beenproposed to evaluate and enhance the VQA robustness, respectively. This paperprovides the first comprehensive survey focused on this emerging fashion.Specifically, we first provide an overview of the development process ofdatasets from in-distribution and out-of-distribution perspectives. Then, weexamine the evaluation metrics employed by these datasets. Thirdly, we proposea typology that presents the development process, similarities and differences,robustness comparison, and technical features of existing debiasing methods.Furthermore, we analyze and discuss the robustness of representativevision-and-language pre-training models on VQA. Finally, through a thoroughreview of the available literature and experimental analysis, we discuss thekey areas for future research from various viewpoints.</description><author>Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao</author><pubDate>Sun, 18 Feb 2024 08:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11471v2</guid></item><item><title>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</title><link>http://arxiv.org/abs/2402.11083v1</link><description>Visual Question Answering (VQA) is a fundamental task in computer vision andnatural language process fields. Although the ``pre-training &amp; finetuning''learning paradigm significantly improves the VQA performance, the adversarialrobustness of such a learning paradigm has not been explored. In this paper, wedelve into a new problem: using a pre-trained multimodal source model to createadversarial image-text pairs and then transferring them to attack the targetVQA models. Correspondingly, we propose a novel VQAttack model, which caniteratively generate both image and text perturbations with the designedmodules: the large language model (LLM)-enhanced image attack and thecross-modal joint attack module. At each iteration, the LLM-enhanced imageattack module first optimizes the latent representation-based loss to generatefeature-level image perturbations. Then it incorporates an LLM to furtherenhance the image perturbations by optimizing the designed masked answeranti-recovery loss. The cross-modal joint attack module will be triggered at aspecific iteration, which updates the image and text perturbationssequentially. Notably, the text perturbation updates are based on both thelearned gradients in the word embedding space and word synonym-basedsubstitution. Experimental results on two VQA datasets with five validatedmodels demonstrate the effectiveness of the proposed VQAttack in thetransferable attack setting, compared with state-of-the-art baselines. Thiswork reveals a significant blind spot in the ``pre-training &amp; fine-tuning''paradigm on VQA tasks. Source codes will be released.</description><author>Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</author><pubDate>Fri, 16 Feb 2024 21:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11083v1</guid></item><item><title>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation</title><link>http://arxiv.org/abs/2310.13505v3</link><description>Models for conversational question answering (ConvQA) over knowledge graphs(KGs) are usually trained and tested on benchmarks of gold QA pairs. Thisimplies that training is limited to surface forms seen in the respectivedatasets, and evaluation is on a small set of held-out questions. Through ourproposed framework REIGN, we take several steps to remedy this restrictedlearning setup. First, we systematically generate reformulations of trainingquestions to increase robustness of models to surface form variations. This isa particularly challenging problem, given the incomplete nature of suchquestions. Second, we guide ConvQA models towards higher performance by feedingit only those reformulations that help improve their answering quality, usingdeep reinforcement learning. Third, we demonstrate the viability of trainingmajor model components on one benchmark and applying them zero-shot to another.Finally, for a rigorous evaluation of robustness for trained models, we use andrelease large numbers of diverse reformulations generated by prompting GPT forbenchmark test sets (resulting in 20x increase in sizes). Our findings showthat ConvQA models with robust training via reformulations, significantlyoutperform those with standard training from gold QA pairs only.</description><author>Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum</author><pubDate>Fri, 16 Feb 2024 19:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13505v3</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction</title><link>http://arxiv.org/abs/2402.11177v1</link><description>Electronic health records (EHRs) hold significant value for research andapplications. As a new way of information extraction, question answering (QA)can extract more flexible information than conventional methods and is moreaccessible to clinical researchers, but its progress is impeded by the scarcityof annotated data. In this paper, we propose a novel approach thatautomatically generates training data for transfer learning of QA models. Ourpipeline incorporates a preprocessing module to handle challenges posed byextraction types that are not readily compatible with extractive QA frameworks,including cases with discontinuous answers and many-to-one relationships. Theobtained QA model exhibits excellent performance on subtasks of informationextraction in EHRs, and it can effectively handle few-shot or zero-shotsettings involving yes-no questions. Case studies and ablation studiesdemonstrate the necessity of each component in our design, and the resultingmodel is deemed suitable for practical use.</description><author>Huaiyuan Ying, Sheng Yu</author><pubDate>Sat, 17 Feb 2024 02:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11177v1</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering</title><link>http://arxiv.org/abs/2402.11194v1</link><description>Large Language Models (LLMs), excel in natural language understanding, buttheir capability for complex mathematical reasoning with an amalgamation ofstructured tables and unstructured text is uncertain. This study explores LLMs'mathematical reasoning on four financial tabular question-answering datasets:TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments withvarious models and prompting techniques, we assess how LLMs adapt to complextables and mathematical tasks. We focus on sensitivity to table complexity andperformance variations with an increasing number of arithmetic reasoning steps.The results provide insights into LLMs' capabilities and limitations inhandling complex mathematical scenarios for semi-structured tables. Ultimately,we introduce a novel prompting technique tailored to semi-structured documents,matching or outperforming other baselines in performance while providing anuanced understanding of LLMs abilities for such a task.</description><author>Pragya Srivastava, Manuj Malik, Tanuja Ganu</author><pubDate>Sat, 17 Feb 2024 05:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11194v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v2</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Thu, 15 Feb 2024 10:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v2</guid></item><item><title>Zero-shot sampling of adversarial entities in biomedical question answering</title><link>http://arxiv.org/abs/2402.10527v1</link><description>The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.</description><author>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</author><pubDate>Fri, 16 Feb 2024 09:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10527v1</guid></item><item><title>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</title><link>http://arxiv.org/abs/2402.08277v2</link><description>Advances towards more faithful and traceable answers of Large Language Models(LLMs) are crucial for various research and practical endeavors. One avenue inreaching this goal is basing the answers on reliable sources. However, thisEvidence-Based QA has proven to work insufficiently with LLMs in terms ofciting the correct sources (source quality) and truthfully representing theinformation within sources (answer attributability). In this work, wesystematically investigate how to robustly fine-tune LLMs for better sourcequality and answer attributability. Specifically, we introduce a datageneration pipeline with automated data quality filters, which can synthesizediversified high-quality training and testing data at scale. We furtherintroduce four test sets to benchmark the robustness of fine-tuned specialistmodels. Extensive evaluation shows that fine-tuning on synthetic data improvesperformance on both in- and out-of-distribution. Furthermore, we show that dataquality, which can be drastically improved by proposed quality filters, mattersmore than quantity in improving Evidence-Based QA.</description><author>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</author><pubDate>Fri, 16 Feb 2024 11:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08277v2</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title><link>http://arxiv.org/abs/2402.09642v1</link><description>This work aims to build a text embedder that can capture characteristics oftexts specified by user instructions. Despite its tremendous potential todeploy user-oriented embeddings, none of previous approaches provides aconcrete solution for it. This paper offers a new viewpoint, which treats theinstruction as a question about the input text and encodes the expected answersto obtain the representation accordingly. Intuitively, texts with the same(implicit) semantics would share similar answers following the instruction,thus leading to more similar embeddings. Specifically, we propose InBedder thatinstantiates this embed-via-answering idea by only fine-tuning language modelson abstractive question answering tasks. InBedder demonstrates significantlyimproved instruction-following capabilities according to our proposedinstruction awareness tests and instruction robustness tests, when applied toboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-basedLMs (e.g., roberta-large). Additionally, our qualitative analysis of clusteringoutcomes, achieved by applying different instructions to the same corpus,demonstrates a high degree of interpretability.</description><author>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</author><pubDate>Thu, 15 Feb 2024 01:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09642v1</guid></item><item><title>An Evaluation of GPT-4V and Gemini in Online VQA</title><link>http://arxiv.org/abs/2312.10637v2</link><description>While there is much excitement about the potential of large multimodal models(LMM), a comprehensive evaluation is critical to establish their truecapabilities and limitations. In support of this aim, we evaluate twostate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answeringdataset sourced from an authentic online question answering community. Weconduct fine-grained analysis by generating seven types of metadata for nearly2,000 visual questions, such as image type and the required image processingcapabilities. Our zero-shot performance analysis highlights the types ofquestions that are most challenging for both models, including questionsrelated to "puzzling" topic, with "Identification" user intention, with "SheetMusic" image type, or labeled as "hard" by GPT-4.</description><author>Mengchen Liu, Chongyan Chen, Danna Gurari</author><pubDate>Wed, 14 Feb 2024 03:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10637v2</guid></item><item><title>Debating with More Persuasive LLMs Leads to More Truthful Answers</title><link>http://arxiv.org/abs/2402.06782v2</link><description>Common methods for aligning large language models (LLMs) with desiredbehaviour heavily rely on human-labelled data. However, as models growincreasingly sophisticated, they will surpass human expertise, and the role ofhuman evaluation will evolve into non-experts overseeing experts. Inanticipation of this, we ask: can weaker models assess the correctness ofstronger models? We investigate this question in an analogous setting, wherestronger models (experts) possess the necessary information to answer questionsand weaker models (non-experts) lack this information. The method we evaluateis \textit{debate}, where two LLM experts each argue for a different answer,and a non-expert selects the answer. We find that debate consistently helpsboth non-expert models and humans answer questions, achieving 76\% and 88\%accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,optimising expert debaters for persuasiveness in an unsupervised mannerimproves non-expert ability to identify the truth in debates. Our resultsprovide encouraging empirical evidence for the viability of aligning modelswith debate in the absence of ground truth.</description><author>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez</author><pubDate>Thu, 15 Feb 2024 22:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06782v2</guid></item><item><title>SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning</title><link>http://arxiv.org/abs/2401.13246v2</link><description>Elucidating the reasoning process with structured explanations from questionto answer is crucial, as it significantly enhances the interpretability,traceability, and trustworthiness of question-answering (QA) systems. However,structured explanations demand models to perform intricately structuredreasoning, which poses great challenges. Most existing methods focus onsingle-step reasoning through supervised learning, ignoring logicaldependencies between steps. Moreover, existing reinforcement learning (RL)based methods overlook the structured relationships, underutilizing thepotential of RL in structured reasoning. In this paper, we propose SEER, anovel method that maximizes a structure-based return to facilitate structuredreasoning and explanation. Our proposed structure-based return preciselydescribes the hierarchical and branching structure inherent in structuredreasoning, effectively capturing the intricate relationships between differentreasoning steps. In addition, we introduce a fine-grained reward function tometiculously delineate diverse reasoning steps. Extensive experiments show thatSEER significantly outperforms state-of-the-art methods, achieving an absoluteimprovement of 6.9% over RL-based methods on EntailmentBank, a 4.4% averageimprovement on STREET benchmark, and exhibiting outstanding efficiency andcross-dataset generalization performance.</description><author>Guoxin Chen, Kexin Tang, Chao Yang, Fuying Ye, Yu Qiao, Yiming Qian</author><pubDate>Fri, 16 Feb 2024 14:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13246v2</guid></item><item><title>MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning</title><link>http://arxiv.org/abs/2402.11260v1</link><description>Adapting large language models (LLMs) to new domains/tasks and enabling themto be efficient lifelong learners is a pivotal challenge. In this paper, wepropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation forLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with thefine-tuning abilities of LoRA for effective life-long learning of LLMs. Incontrast to the conventional approaches that use factual triplets as inputsMoRAL relies on simple question-answer pairs, which is a more practical andeffective strategy for robust and efficient learning. Owing to new datasettings, we introduce a new evaluation benchmark namely: Life Long Learning ofLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,and a set of evaluation metrics for rigorous evaluation of MoRAL in open-bookand closed-book settings. Experimental evaluation shows (i) LLMs learn fast inopen-book settings with up to 30.15% improvement in "RA" for Phi-2-2.7Bcompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL showshigher performance improvement for models with a greater number of parameters;(iii) MoRAL is robust to catastrophic forgetting offering better knowledgeretention compared to baselines.</description><author>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang</author><pubDate>Sat, 17 Feb 2024 12:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11260v1</guid></item><item><title>ScreenAI: A Vision-Language Model for UI and Infographics Understanding</title><link>http://arxiv.org/abs/2402.04615v2</link><description>Screen user interfaces (UIs) and infographics, sharing similar visuallanguage and design principles, play important roles in human communication andhuman-machine interaction. We introduce ScreenAI, a vision-language model thatspecializes in UI and infographics understanding. Our model improves upon thePaLI architecture with the flexible patching strategy of pix2struct and istrained on a unique mixture of datasets. At the heart of this mixture is anovel screen annotation task in which the model has to identify the type andlocation of UI elements. We use these text annotations to describe screens toLarge Language Models and automatically generate question-answering (QA), UInavigation, and summarization training datasets at scale. We run ablationstudies to demonstrate the impact of these design choices. At only 5Bparameters, ScreenAI achieves new state-of-the-artresults on UI- andinfographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and WidgetCaptioning), and new best-in-class performance on others (Chart QA, DocVQA, andInfographicVQA) compared to models of similar size. Finally, we release threenew datasets: one focused on the screen annotation task and two others focusedon question answering.</description><author>Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cărbune, Jason Lin, Jindong Chen, Abhanshu Sharma</author><pubDate>Mon, 19 Feb 2024 17:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04615v2</guid></item><item><title>GenDec: A robust generative Question-decomposition method for Multi-hop reasoning</title><link>http://arxiv.org/abs/2402.11166v1</link><description>Multi-hop QA (MHQA) involves step-by-step reasoning to answer complexquestions and find multiple relevant supporting facts. However, Existing largelanguage models'(LLMs) reasoning ability in multi-hop question answeringremains exploration, which is inadequate in answering multi-hop questions.Moreover, it is unclear whether LLMs follow a desired reasoning chain to reachthe right final answer. In this paper, we propose a \textbf{gen}erativequestion \textbf{dec}omposition method (GenDec) from the perspective ofexplainable QA by generating independent and complete sub-questions based onincorporating additional extracted evidence for enhancing LLMs' reasoningability in RAG. To demonstrate the impact, generalization, and robustness ofGendec, we conduct two experiments, the first is combining GenDec with small QAsystems on paragraph retrieval and QA tasks. We secondly examine the reasoningcapabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,MuSiQue, and PokeMQA datasets.</description><author>Jian Wu, Linyi Yang, Yuliang Ji, Wenhao Huang, Börje F. Karlsson, Manabu Okumura</author><pubDate>Sat, 17 Feb 2024 02:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11166v1</guid></item><item><title>MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition</title><link>http://arxiv.org/abs/2402.11924v1</link><description>Although Large Language Models (LLMs) have shown strong performance inMulti-hop Question Answering (MHQA) tasks, their real reasoning ability remainsexploration. Current LLM QA evaluation benchmarks have shown limitations,including 1) data contamination, the evaluation data are potentially exposed toLLMs during the pretraining stage; and 2) ignoration of the reasoning chainevaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QAbenchmark based on the new, unprecedented knowledge by editing theoff-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate thereasoning chain in the form of sub-questions and intermediate answerscorresponding to the multi-hop questions. Specifically, based on theobservation, 1) LLMs show a performance gap between the original HotpotQA andour edited data, deeming that current MHQA benchmarks have the potential riskof data contamination that hard to evaluate LLMs' performance objectively andscientifically; 2) LLMs only get a small percentage of the right reasoningchain, e.g. GPT-4 only gets 36.3\% right reasoning chain. We believe this newMulti-hop QA evaluation benchmark and novel evaluation methods will facilitatethe development of trustworthy LLM evaluation on the MHQA task.</description><author>Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang</author><pubDate>Mon, 19 Feb 2024 08:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11924v1</guid></item><item><title>KnowGPT: Black-Box Knowledge Injection for Large Language Models</title><link>http://arxiv.org/abs/2312.06185v2</link><description>Generative Large Language Models (LLMs), such as ChatGPT, offer interactiveAPIs that can answer common questions at a human-expert level. However, thesemodels often give inaccurate or incorrect responses when faced with questionsrequiring domain-specific or professional-specific knowledge not covered intheir training corpus. Furthermore, many state-of-the-art LLMs are notopen-source, making it challenging to inject knowledge with model APIs only. Inthis work, we introduce KnowGPT, a black-box knowledge injection framework forLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-ArmedBandit (MAB) to construct the most suitable prompt for each question. Ourextensive experiments on three benchmark datasets showcase that KnowGPTsignificantly enhances the existing methods. Notably, KnowGPT achieves anaverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%over GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQAofficial leaderboard, which is comparable to human-level performance.</description><author>Qinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang, Daochen Zha, Zailiang Yu</author><pubDate>Mon, 19 Feb 2024 09:28:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06185v2</guid></item><item><title>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</title><link>http://arxiv.org/abs/2402.11651v1</link><description>Large language models (LLMs) have achieved success in acting as agents, whichinteract with environments through tools like search engines. However, LLMs arenot optimized specifically for tool use during training or alignment, limitingtheir effectiveness as agents. To resolve this problem, previous work hascollected interaction trajectories between GPT-4 and environments, andfine-tuned smaller models with them. As part of this, the standard approach hasbeen to simply discard trajectories that do not finish the task successfully,which, on the one hand, leads to a significant waste of data and resources, andon the other hand, has the potential to limit the possible optimization pathsduring fine-tuning. In this paper, we contend that large language models canlearn from failures through appropriate data cleaning and fine-tuningstrategies. We conduct experiments on mathematical reasoning, multi-hopquestion answering, and strategic question answering tasks. Experimentalresults demonstrate that compared to solely using positive examples,incorporating negative examples enhances model performance by a large margin.</description><author>Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin</author><pubDate>Sun, 18 Feb 2024 17:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11651v1</guid></item><item><title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title><link>http://arxiv.org/abs/2402.08479v3</link><description>The increasing use of complex and opaque black box models requires theadoption of interpretable measures, one such option is extractive rationalizingmodels, which serve as a more interpretable alternative. These models, alsoknown as Explain-Then-Predict models, employ an explainer model to extractrationales and subsequently condition the predictor with the extractedinformation. Their primary objective is to provide precise and faithfulexplanations, represented by the extracted rationales. In this paper, we take asemi-supervised approach to optimize for the plausibility of extractedrationales. We adopt a pre-trained natural language inference (NLI) model andfurther fine-tune it on a small set of supervised rationales ($10\%$). The NLIpredictor is leveraged as a source of supervisory signals to the explainer viaentailment alignment. We show that, by enforcing the alignment agreementbetween the explanation and answer in a question-answering task, theperformance can be improved without access to ground truth labels. We evaluateour approach on the ERASER dataset and show that our approach achievescomparable results with supervised extractive models and outperformsunsupervised approaches by $&gt; 100\%$.</description><author>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria</author><pubDate>Fri, 16 Feb 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08479v3</guid></item><item><title>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</title><link>http://arxiv.org/abs/2402.10150v1</link><description>In self-supervised contrastive learning, a widely-adopted objective functionis InfoNCE, which uses the heuristic cosine similarity for the representationcomparison, and is closely related to maximizing the Kullback-Leibler(KL)-based mutual information. In this paper, we aim at answering twointriguing questions: (1) Can we go beyond the KL-based objective? (2) Besidesthe popular cosine similarity, can we design a better similarity function? Weprovide answers to both questions by generalizing the KL-based mutualinformation to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)using the $f$-divergences. To answer the first question, we provide a widerange of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,alignment and uniformity), and meanwhile result in similar or even superiorperformance. For the second question, assuming that the joint featuredistribution is proportional to the Gaussian kernel, we derive an $f$-Gaussiansimilarity with better interpretability and empirical performance. Finally, weidentify close relationships between the $f$-MICL objective and several popularInfoNCE-based objectives. Using benchmark tasks from both vision and naturallanguage, we empirically evaluate $f$-MICL with different $f$-divergences onvarious architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that$f$-MICL generally outperforms the benchmarks and the best-performing$f$-divergence is task and dataset dependent.</description><author>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 17:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10150v1</guid></item><item><title>Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA</title><link>http://arxiv.org/abs/2401.15847v2</link><description>Multipanel images, commonly seen as web screenshots, posters, etc., pervadeour daily lives. These images, characterized by their composition of multiplesubfigures in distinct layouts, effectively convey information to people.Toward building advanced multimodal AI applications, such as agents thatunderstand complex scenes and navigate through webpages, the skill ofmultipanel visual reasoning is essential, and a comprehensive evaluation ofmodels in this regard is important. Therefore, we introduce Multipanel VisualQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 tripletsof questions, answers, and multipanel images that specifically challenge modelsin comprehending multipanel images. Our evaluation shows that questions in theMultipanelVQA benchmark pose significant challenges to the state-of-the-artLarge Vision Language Models (LVLMs) tested, even though humans can attainapproximately 99\% accuracy on these questions. Distinctively, theMultipanelVQA benchmark features synthetically generated multipanel imagesspecifically crafted to isolate and assess the impact of various factors, suchas the layout, on LVLMs' multipanel image comprehension abilities. As a result,in addition to benchmarking the capabilities of LVLMs in understandingmultipanel images, we analyze the potential causes for LVLMs' performance andoffer insights for enhancement with the synthetic data. Code and data arereleased at https://sites.google.com/view/multipanelvqa/home.</description><author>Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang</author><pubDate>Mon, 19 Feb 2024 05:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15847v2</guid></item><item><title>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</title><link>http://arxiv.org/abs/2402.06764v2</link><description>Integrating large language models (LLMs) with knowledge graphs derived fromdomain-specific data represents an important advancement towards more powerfuland factual reasoning. As these models grow more capable, it is crucial toenable them to perform multi-step inferences over real-world knowledge graphswhile minimizing hallucination. While large language models excel atconversation and text generation, their ability to reason overdomain-specialized graphs of interconnected entities remains limited. Forexample, can we query a LLM to identify the optimal contact in a professionalnetwork for a specific goal, based on relationships and attributes in a privatedatabase? The answer is no--such capabilities lie beyond current methods.However, this question underscores a critical technical gap that must beaddressed. Many high-value applications in areas such as science, security, ande-commerce rely on proprietary knowledge graphs encoding unique structures,relationships, and logical constraints. We introduce a fine-tuning frameworkfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledgegraph into an alternate text representation with labeled question-answer pairs.We demonstrate that grounding the models in specific graph-based knowledgeexpands the models' capacity for structure-based reasoning. Our methodologyleverages the large-language model's generative capabilities to create thedataset and proposes an efficient alternate to retrieval-augmented generationstyled methods.</description><author>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</author><pubDate>Fri, 16 Feb 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06764v2</guid></item><item><title>Tokenization Preference for Human and Machine Learning Model: An Annotation Study</title><link>http://arxiv.org/abs/2304.10813v3</link><description>Is preferred tokenization for humans also preferred for machine-learning (ML)models? This study examines the relations between preferred tokenization forhumans (appropriateness and readability) and one for ML models (performance onan NLP task). The question texts of the Japanese commonsense question-answeringdataset are tokenized with six different tokenizers, and the performances ofhuman annotators and ML models were compared. Furthermore, we analyze relationsamong performance of answers by human and ML model, the appropriateness oftokenization for human, and response time to questions by human. This studyprovides a quantitative investigation result that shows that preferredtokenizations for humans and ML models are not necessarily always the same. Theresult also implies that existing methods using language models fortokenization could be a good compromise both for human and ML models.</description><author>Tatsuya Hiraoka, Tomoya Iwakura</author><pubDate>Fri, 16 Feb 2024 07:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10813v3</guid></item><item><title>Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation</title><link>http://arxiv.org/abs/2402.11493v1</link><description>In recent years, substantial advancements have been made in the developmentof large language models, achieving remarkable performance across diversetasks. To evaluate the knowledge ability of language models, previous studieshave proposed lots of benchmarks based on question-answering pairs. We arguethat it is not reliable and comprehensive to evaluate language models with afixed question or limited paraphrases as the query, since language models aresensitive to prompt. Therefore, we introduce a novel concept named knowledgeboundary to encompass both prompt-agnostic and prompt-sensitive knowledgewithin language models. Knowledge boundary avoids prompt sensitivity inlanguage model evaluations, rendering them more dependable and robust. Toexplore the knowledge boundary for a given model, we propose projected gradientdescent method with semantic constraints, a new algorithm designed to identifythe optimal prompt for each piece of knowledge. Experiments demonstrate asuperior performance of our algorithm in computing the knowledge boundarycompared to existing methods. Furthermore, we evaluate the ability of multiplelanguage models in several domains with knowledge boundary.</description><author>Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan</author><pubDate>Sun, 18 Feb 2024 07:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11493v1</guid></item><item><title>Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs</title><link>http://arxiv.org/abs/2402.11199v1</link><description>Large language models (LLMs) demonstrate strong reasoning abilities whenprompted to generate chain-of-thought (CoT) explanations alongside answers.However, previous research on evaluating LLMs has solely focused on answeraccuracy, neglecting the correctness of the generated CoT. In this paper, wedelve deeper into the CoT reasoning capabilities of LLMs in multi-hop questionanswering by utilizing knowledge graphs (KGs). We propose a noveldiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledgeof reasoning and the accuracy of the generated CoT. Through experimentsconducted on 5 different families of LLMs across 2 multi-hop question-answeringdatasets, we find that LLMs possess sufficient knowledge to perform reasoning.However, there exists a significant disparity between answer accuracy andfaithfulness of the CoT reasoning generated by LLMs, indicating that they oftenarrive at correct answers through incorrect reasoning.</description><author>Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari</author><pubDate>Sat, 17 Feb 2024 05:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11199v1</guid></item><item><title>BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence</title><link>http://arxiv.org/abs/2402.12174v1</link><description>Retrieval-augmented large language models (LLMs) have demonstrated efficacyin knowledge-intensive tasks such as open-domain QA, addressing inherentchallenges in knowledge update and factual inadequacy. However, inconsistenciesbetween retrieval knowledge and the necessary knowledge for LLMs, leading to adecline in LLM's answer quality. This paper introduces BIDER, an approach thatrefines retrieval documents into Key Supporting Evidence (KSE) throughknowledge synthesis, supervised fine-tuning (SFT), and preference alignment. Wetrain BIDER by learning from crafting KSE, while maximizing its output to alignwith LLM's information acquisition preferences through reinforcement learning.Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7%while reducing input content length in retrieval documents by 80%,outperforming existing methods. The proposed KSE simulation effectively equipsLLMs with essential information for accurate question answering.</description><author>Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou</author><pubDate>Mon, 19 Feb 2024 14:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12174v1</guid></item><item><title>OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models</title><link>http://arxiv.org/abs/2310.07637v3</link><description>Information Technology (IT) Operations (Ops), particularly ArtificialIntelligence for IT Operations (AIOps), is the guarantee for maintaining theorderly and stable operation of existing information systems. According toGartner's prediction, the use of AI technology for automated IT operations hasbecome a new trend. Large language models (LLMs) that have exhibited remarkablecapabilities in NLP-related tasks, are showing great potential in the field ofAIOps, such as in aspects of root cause analysis of failures, generation ofoperations and maintenance scripts, and summarizing of alert information.Nevertheless, the performance of current LLMs in Ops tasks is yet to bedetermined. In this paper, we present OpsEval, a comprehensive task-orientedOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'proficiency in various crucial scenarios at different ability levels. Thebenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)formats in English and Chinese. By conducting a comprehensive performanceevaluation of the current leading large language models, we show how variousLLM techniques can affect the performance of Ops, and discussed findingsrelated to various topics, including model quantification, QA evaluation, andhallucination issues. To ensure the credibility of our evaluation, we invitedozens of domain experts to manually review our questions. At the same time, wehave open-sourced 20% of the test QA to assist current researchers inpreliminary evaluations of their OpsLLM models. The remaining 80% of the data,which is not disclosed, is used to eliminate the issue of the test set leakage.Additionally, we have constructed an online leaderboard that is updated inreal-time and will continue to be updated, ensuring that any newly emergingLLMs will be evaluated promptly. Both our dataset and leaderboard have beenmade public.</description><author>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei</author><pubDate>Fri, 16 Feb 2024 08:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07637v3</guid></item><item><title>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2401.17043v2</link><description>Retrieval-Augmented Generation (RAG) is a technique that enhances thecapabilities of large language models (LLMs) by incorporating externalknowledge sources. This method addresses common LLM limitations, includingoutdated information and the tendency to produce inaccurate "hallucinated"content. However, the evaluation of RAG systems is challenging, as existingbenchmarks are limited in scope and diversity. Most of the current benchmarkspredominantly assess question-answering applications, overlooking the broaderspectrum of situations where RAG could prove advantageous. Moreover, they onlyevaluate the performance of the LLM component of the RAG pipeline in theexperiments, and neglect the influence of the retrieval component and theexternal knowledge database. To address these issues, this paper constructs alarge-scale and more comprehensive benchmark, and evaluates all the componentsof RAG systems in various RAG application scenarios. Specifically, we havecategorized the range of RAG applications into four distinct types-Create,Read, Update, and Delete (CRUD), each representing a unique use case. "Create"refers to scenarios requiring the generation of original, varied content."Read" involves responding to intricate questions in knowledge-intensivesituations. "Update" focuses on revising and rectifying inaccuracies orinconsistencies in pre-existing texts. "Delete" pertains to the task ofsummarizing extensive texts into more concise forms. For each of these CRUDcategories, we have developed comprehensive datasets to evaluate theperformance of RAG systems. We also analyze the effects of various componentsof the RAG system, such as the retriever, the context length, the knowledgebase construction, and the LLM. Finally, we provide useful insights foroptimizing the RAG technology for different scenarios.</description><author>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen, Yi Luo, Peng Cheng, Haiying Deng, Zhonghao Wang, Zijia Lu</author><pubDate>Mon, 19 Feb 2024 03:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17043v2</guid></item><item><title>Learning to Edit: Aligning LLMs with Knowledge Editing</title><link>http://arxiv.org/abs/2402.11905v1</link><description>Knowledge editing techniques, aiming to efficiently modify a minor proportionof knowledge in large language models (LLMs) without negatively impactingperformance across other inputs, have garnered widespread attention. However,existing methods predominantly rely on memorizing the updated knowledge,impeding LLMs from effectively combining the new knowledge with their inherentknowledge when answering questions. To this end, we propose a Learning to Edit(LTE) framework, focusing on teaching LLMs to apply updated knowledge intoinput questions, inspired by the philosophy of "Teach a man to fish." LTEfeatures a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs ona meticulously curated parallel dataset to make reliable, in-scope edits whilepreserving out-of-scope information and linguistic proficiency; and (ii) theInference Phase, which employs a retrieval-based mechanism for real-time andmass knowledge editing. By comparing our approach with seven advanced baselinesacross four popular knowledge editing benchmarks and two LLM architectures, wedemonstrate LTE's superiority in knowledge editing performance, robustness inboth batch and sequential editing, minimal interference on general tasks, andrapid editing speeds. The data and code are available athttps://github.com/YJiangcm/LTE.</description><author>Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang</author><pubDate>Mon, 19 Feb 2024 07:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11905v1</guid></item><item><title>Uncovering the Full Potential of Visual Grounding Methods in VQA</title><link>http://arxiv.org/abs/2401.07803v2</link><description>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt toimprove VQA performance by strengthening a model's reliance onquestion-relevant visual information. The presence of such relevant informationin the visual input is typically assumed in training and testing. Thisassumption, however, is inherently flawed when dealing with imperfect imagerepresentations common in large-scale VQA, where the information carried byvisual features frequently deviates from expected ground-truth contents. As aresult, training and testing of VG-methods is performed with largely inaccuratedata, which obstructs proper assessment of their potential benefits. In thisstudy, we demonstrate that current evaluation schemes for VG-methods areproblematic due to the flawed assumption of availability of relevant visualinformation. Our experiments show that these methods can be much more effectivewhen evaluation conditions are corrected. Code is provided on GitHub.</description><author>Daniel Reich, Tanja Schultz</author><pubDate>Thu, 15 Feb 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07803v2</guid></item><item><title>AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis</title><link>http://arxiv.org/abs/2402.09742v1</link><description>The incorporation of Large Language Models (LLMs) in healthcare marks asignificant advancement. However, the application has predominantly beenlimited to discriminative and question-answering tasks, which does not fullyleverage their interactive potential. To address this limitation, our paperpresents AI Hospital, a framework designed to build a real-time interactivediagnosis environment. To simulate the procedure, we collect high-qualitymedical records to create patient, examiner, and medical director agents. AIHospital is then utilized for the interactive evaluation and collaboration ofLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmarkwhere various LLMs serve as intern doctors for interactive diagnosis.Subsequently, to improve diagnostic accuracy, we introduce a collaborativemechanism that involves iterative discussions and a dispute resolution processunder the supervision of the medical director. In our experiments, we validatethe reliability of AI Hospital. The results not only explore the feasibility ofapply LLMs in clinical consultation but also confirm the effectiveness of thedispute resolution focused collaboration method.</description><author>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</author><pubDate>Thu, 15 Feb 2024 06:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09742v1</guid></item><item><title>Probabilistic Reasoning in Generative Large Language Models</title><link>http://arxiv.org/abs/2402.09614v1</link><description>This paper considers the challenges that Large Language Models (LLMs) facewhen reasoning over text that includes information involving uncertaintyexplicitly quantified via probability values. This type of reasoning isrelevant to a variety of contexts ranging from everyday conversations tomedical decision-making. Despite improvements in the mathematical reasoningcapabilities of LLMs, they still exhibit significant difficulties when it comesto probabilistic reasoning. To deal with this problem, we first introduce theBayesian Linguistic Inference Dataset (BLInD), a new dataset specificallydesigned to test the probabilistic reasoning capabilities of LLMs. We thenleverage this new dataset to thoroughly illustrate the specific limitations ofLLMs for tasks involving probabilistic reasoning and present several strategiesthat map the problem to different formal representations, including Pythoncode, probabilistic inference algorithms, and probabilistic logicalprogramming. We conclude by providing an evaluation of our methods on BLInD andon an adaptation of a causal reasoning question-answering dataset, whichfurther shows their practical effectiveness.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Wed, 14 Feb 2024 23:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09614v1</guid></item><item><title>OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM</title><link>http://arxiv.org/abs/2402.09181v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in various multimodal tasks. However, their potential in themedical domain remains largely unexplored. A significant challenge arises fromthe scarcity of diverse medical images spanning various modalities andanatomical regions, which is essential in real-world medical applications. Tosolve this problem, in this paper, we introduce OmniMedVQA, a novelcomprehensive medical Visual Question Answering (VQA) benchmark. This benchmarkis collected from 75 different medical datasets, including 12 differentmodalities and covering more than 20 distinct anatomical regions. Importantly,all images in this benchmark are sourced from authentic medical scenarios,ensuring alignment with the requirements of the medical field and suitabilityfor evaluating LVLMs. Through our extensive experiments, we have found thatexisting LVLMs struggle to address these medical VQA problems effectively.Moreover, what surprises us is that medical-specialized LVLMs even exhibitinferior performance to those general-domain models, calling for a moreversatile and robust LVLM in the biomedical field. The evaluation results notonly reveal the current limitations of LVLM in understanding real medicalimages but also highlight our dataset's significance. Our dataset will be madepublicly available.</description><author>Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo</author><pubDate>Wed, 14 Feb 2024 13:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09181v1</guid></item><item><title>Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks</title><link>http://arxiv.org/abs/2402.09177v1</link><description>Large Language Models (LLMs) are susceptible to Jailbreaking attacks, whichaim to extract harmful information by subtly modifying the attack query. Asdefense mechanisms evolve, directly obtaining harmful information becomesincreasingly challenging for Jailbreaking attacks. In this work, inspired byhuman practices of indirect context to elicit harmful information, we focus ona new attack form called Contextual Interaction Attack. The idea relies on theautoregressive nature of the generation process in LLMs. We contend that theprior context--the information preceding the attack query--plays a pivotal rolein enabling potent Jailbreaking attacks. Specifically, we propose an approachthat leverages preliminary question-answer pairs to interact with the LLM. Bydoing so, we guide the responses of the model toward revealing the 'desired'harmful information. We conduct experiments on four different LLMs anddemonstrate the efficacy of this attack, which is black-box and can alsotransfer across LLMs. We believe this can lead to further developments andunderstanding of the context vector in LLMs.</description><author>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos</author><pubDate>Wed, 14 Feb 2024 13:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09177v1</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>Multi-modal preference alignment remedies regression of visual instruction tuning on language model</title><link>http://arxiv.org/abs/2402.10884v1</link><description>In production, multi-modal large language models (MLLMs) are expected tosupport multi-turn queries of interchanging image and text modalities. However,the current MLLMs trained with visual-question-answering (VQA) datasets couldsuffer from degradation, as VQA datasets lack the diversity and complexity ofthe original text instruction datasets which the underlying language model hadbeen trained with. To address this challenging degradation, we first collect alightweight (6k entries) VQA preference dataset where answers were annotated byGemini for 5 quality metrics in a granular fashion, and investigate standardSupervised Fine-tuning, rejection sampling, Direct Preference Optimization(DPO), and SteerLM. Our findings indicate that the with DPO we are able tosurpass instruction-following capabilities of the language model, achieving a6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despitesmall data scale. This enhancement in textual instruction proficiencycorrelates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarkscompared to previous RLHF approach. In conclusion, we propose adistillation-based multi-modal alignment model with fine-grained annotations ona small dataset that reconciles the textual and visual performance of MLLMs,restoring and boosting language capability after visual instruction tuning.</description><author>Shengzhi Li, Rongyu Lin, Shichao Pei</author><pubDate>Fri, 16 Feb 2024 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10884v1</guid></item><item><title>Anchor-based Large Language Models</title><link>http://arxiv.org/abs/2402.07616v2</link><description>Large language models (LLMs) predominantly employ decoder-only transformerarchitectures, necessitating the retention of keys/values information forhistorical tokens to provide contextual information and avoid redundantcomputation. However, the substantial size and parameter volume of these LLMsrequire massive GPU memory. This memory demand increases with the length of theinput text, leading to an urgent need for more efficient methods of informationstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), whichutilize an innovative anchor-based self-attention network (AnSAN) and also ananchor-based inference strategy. This approach enables LLMs to compresssequence information into an anchor token, reducing the keys/values cache andenhancing inference efficiency. Experiments on question-answering benchmarksreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%keys/values cache reduction and up to 3.5 times faster inference. Despite aminor compromise in accuracy, the substantial enhancements of AnLLMs employingthe AnSAN technique in resource utilization and computational efficiencyunderscore their potential for practical LLM applications.</description><author>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</author><pubDate>Fri, 16 Feb 2024 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07616v2</guid></item><item><title>Inference to the Best Explanation in Large Language Models</title><link>http://arxiv.org/abs/2402.10767v1</link><description>While Large Language Models (LLMs) have found success in real-worldapplications, their underlying explanatory process is still poorly understood.This paper proposes IBE-Eval, a framework inspired by philosophical accounts onInference to the Best Explanation (IBE) to advance the interpretation andevaluation of LLMs' explanations. IBE-Eval estimates the plausibility ofnatural language explanations through a combination of explicit logical andlinguistic features including: consistency, parsimony, coherence, anduncertainty. Extensive experiments are conducted on Causal Question Answering(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causalexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama2). The experiments reveal that IBE-Eval can successfully identify the bestexplanation with up to 77\% accuracy ($\approx 27\%$ above random), improvingupon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsicallymore efficient and interpretable. Additional analyses suggest that, despitemodel-specific variances, LLM-generated explanations tend to conform to IBEcriteria and that IBE-Eval is significantly correlated with human judgment,opening up opportunities for future development of automated explanationverification tools.</description><author>Dhairya Dalal, Marco Valentino, André Freitas, Paul Buitelaar</author><pubDate>Fri, 16 Feb 2024 15:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10767v1</guid></item><item><title>Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research</title><link>http://arxiv.org/abs/2402.10743v1</link><description>Entity and relationship extraction is a crucial component in natural languageprocessing tasks such as knowledge graph construction, question answeringsystem design, and semantic analysis. Most of the information of the Yishuischool of traditional Chinese Medicine (TCM) is stored in the form ofunstructured classical Chinese text. The key information extraction of TCMtexts plays an important role in mining and studying the academic schools ofTCM. In order to solve these problems efficiently using artificial intelligencemethods, this study constructs a word segmentation and entity relationshipextraction model based on conditional random fields under the framework ofnatural language processing technology to identify and extract the entityrelationship of traditional Chinese medicine texts, and uses the commonweighting technology of TF-IDF information retrieval and data mining to extractimportant key entity information in different ancient books. The dependencysyntactic parser based on neural network is used to analyze the grammaticalrelationship between entities in each ancient book article, and it isrepresented as a tree structure visualization, which lays the foundation forthe next construction of the knowledge graph of Yishui school and the use ofartificial intelligence methods to carry out the research of TCM academicschools.</description><author>Hanqing Zhao, Yuehan Li</author><pubDate>Fri, 16 Feb 2024 14:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10743v1</guid></item><item><title>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</title><link>http://arxiv.org/abs/2305.11738v3</link><description>Recent developments in large language models (LLMs) have been impressive.However, these models sometimes show inconsistencies and problematic behavior,such as hallucinating facts, generating flawed code, or creating offensive andtoxic content. Unlike these models, humans typically utilize external tools tocross-check and refine their initial content, like using a search engine forfact-checking, or a code interpreter for debugging. Inspired by thisobservation, we introduce a framework called CRITIC that allows LLMs, which areessentially "black boxes" to validate and progressively amend their own outputsin a manner similar to human interaction with tools. More specifically,starting with an initial output, CRITIC interacts with appropriate tools toevaluate certain aspects of the text, and then revises the output based on thefeedback obtained during this validation process. Comprehensive evaluationsinvolving free-form question answering, mathematical program synthesis, andtoxicity reduction demonstrate that CRITIC consistently enhances theperformance of LLMs. Meanwhile, our research highlights the crucial importanceof external feedback in promoting the ongoing self-improvement of LLMs.</description><author>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</author><pubDate>Fri, 16 Feb 2024 08:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11738v3</guid></item><item><title>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</title><link>http://arxiv.org/abs/2402.10373v1</link><description>Large Language Models (LLMs) have demonstrated remarkable versatility inrecent years, offering potential applications across specialized domains suchas healthcare and medicine. Despite the availability of various open-sourceLLMs tailored for health contexts, adapting general-purpose LLMs to the medicaldomain presents significant challenges. In this paper, we introduce BioMistral,an open-source LLM tailored for the biomedical domain, utilizing Mistral as itsfoundation model and further pre-trained on PubMed Central. We conduct acomprehensive evaluation of BioMistral on a benchmark comprising 10 establishedmedical question-answering (QA) tasks in English. We also explore lightweightmodels obtained through quantization and model merging approaches. Our resultsdemonstrate BioMistral's superior performance compared to existing open-sourcemedical models and its competitive edge against proprietary counterparts.Finally, to address the limited availability of data beyond English and toassess the multilingual generalization of medical LLMs, we automaticallytranslated and evaluated this benchmark into 7 other languages. This marks thefirst large-scale multilingual evaluation of LLMs in the medical domain.Datasets, multilingual evaluation benchmarks, scripts, and all the modelsobtained during our experiments are freely released.</description><author>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour</author><pubDate>Thu, 15 Feb 2024 23:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10373v1</guid></item><item><title>Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge</title><link>http://arxiv.org/abs/2402.12352v1</link><description>Large language models (LLMs) are transforming the way information isretrieved with vast amounts of knowledge being summarized and presented vianatural language conversations. Yet, LLMs are prone to highlight the mostfrequently seen pieces of information from the training set and to neglect therare ones. In the field of biomedical research, latest discoveries are key toacademic and industrial actors and are obscured by the abundance of anever-increasing literature corpus (the information overload problem). Surfacingnew associations between biomedical entities, e.g., drugs, genes, diseases,with LLMs becomes a challenge of capturing the long-tail knowledge of thebiomedical scientific production. To overcome this challenge, RetrievalAugmented Generation (RAG) has been proposed to alleviate some of theshortcomings of LLMs by augmenting the prompts with context retrieved fromexternal datasets. RAG methods typically select the context via maximumsimilarity search over text embeddings. In this study, we show that RAG methodsleave out a significant proportion of relevant information due to clusters ofover-represented concepts in the biomedical literature. We introduce a novelinformation-retrieval method that leverages a knowledge graph to downsamplethese clusters and mitigate the information overload problem. Its retrievalperformance is about twice better than embedding similarity alternatives onboth precision and recall. Finally, we demonstrate that both embeddingsimilarity and knowledge graph retrieval methods can be advantageously combinedinto a hybrid model that outperforms both, enabling potential improvements tobiomedical question-answering models.</description><author>Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov</author><pubDate>Mon, 19 Feb 2024 18:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12352v1</guid></item><item><title>Survey of Hallucination in Natural Language Generation</title><link>http://arxiv.org/abs/2202.03629v6</link><description>Natural Language Generation (NLG) has improved exponentially in recent yearsthanks to the development of sequence-to-sequence deep learning technologiessuch as Transformer-based language models. This advancement has led to morefluent and coherent NLG, leading to improved development in downstream taskssuch as abstractive summarization, dialogue generation and data-to-textgeneration. However, it is also apparent that deep learning based generation isprone to hallucinate unintended text, which degrades the system performance andfails to meet user expectations in many real-world scenarios. To address thisissue, many studies have been presented in measuring and mitigatinghallucinated texts, but these have never been reviewed in a comprehensivemanner before. In this survey, we thus provide a broad overview of the researchprogress and challenges in the hallucination problem in NLG. The survey isorganized into two parts: (1) a general overview of metrics, mitigationmethods, and future directions; (2) an overview of task-specific researchprogress on hallucinations in the following downstream tasks, namelyabstractive summarization, dialogue generation, generative question answering,data-to-text generation, machine translation, and visual-language generation;and (3) hallucinations in large language models (LLMs). This survey serves tofacilitate collaborative efforts among researchers in tackling the challenge ofhallucinated texts in NLG.</description><author>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Delong Chen, Ho Shu Chan, Wenliang Dai, Andrea Madotto, Pascale Fung</author><pubDate>Mon, 19 Feb 2024 14:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.03629v6</guid></item><item><title>Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.12048v1</link><description>Catastrophic forgetting emerges as a critical challenge when fine-tuningmulti-modal large language models (MLLMs), where improving performance onunseen tasks often leads to a significant performance drop on the originaltasks. This paper presents a comprehensive analysis of catastrophic forgettingin MLLMs and introduces a post-training adjustment method called Model Tailor.Our method primarily preserves the pre-trained parameters while replacing asmall number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\%effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\%on new tasks compared to standard fine-tuning. Specifically, we derive a sparsemask to identify the "model patch", based on a fusion strategy that integratessalience and sensitivity analysis. Subsequently, a compensation mechanism isintroduced to "decorate the patch", enhancing the model's performance on bothtarget and original tasks. Additionally, our method is adaptable to multi-taskscenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in bothimage captioning and visual question answering tasks, our approach demonstratessignificant task adaptability while preserving inherent pre-trainedcapabilities.</description><author>Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, Chao Wu</author><pubDate>Mon, 19 Feb 2024 11:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12048v1</guid></item><item><title>LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation</title><link>http://arxiv.org/abs/2402.11485v1</link><description>Adapting English-based large language models (LLMs) to other languages hasbecome increasingly popular due to the efficiency and potential ofcross-lingual transfer. However, existing language adaptation methods oftenoverlook the benefits of cross-lingual supervision. In this study, we introduceLEIA, a language adaptation tuning method that utilizes Wikipedia entity namesaligned across languages. This method involves augmenting the target languagecorpus with English entity names and training the model using left-to-rightlanguage modeling. We assess LEIA on diverse question answering datasets using7B-parameter LLMs, demonstrating significant performance gains across variousnon-English languages. The source code is available athttps://github.com/studio-ousia/leia.</description><author>Ikuya Yamada, Ryokan Ri</author><pubDate>Sun, 18 Feb 2024 07:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11485v1</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v1</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo</author><pubDate>Mon, 19 Feb 2024 01:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v1</guid></item><item><title>MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs</title><link>http://arxiv.org/abs/2402.11756v1</link><description>Generative Large Language Models (LLMs) are widely utilized for theirexcellence in various tasks. However, their tendency to produce inaccurate ormisleading outputs poses a potential risk, particularly in high-stakesenvironments. Therefore, estimating the correctness of generative LLM outputsis an important task for enhanced reliability. Uncertainty Estimation (UE) ingenerative LLMs is an evolving domain, where SOTA probability-based methodscommonly employ length-normalized scoring. In this work, we proposeMeaning-Aware Response Scoring (MARS) as an alternative to length-normalizedscoring for UE methods. MARS is a novel scoring function that considers thesemantic contribution of each token in the generated sequence in the context ofthe question. We demonstrate that integrating MARS into UE methods results in auniversal and significant improvement in UE performance. We conduct experimentsusing three distinct closed-book question-answering datasets across fivepopular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a MedicalQA dataset. Code can be foundhttps://anonymous.4open.science/r/LLM_Uncertainity-309B.</description><author>Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr</author><pubDate>Mon, 19 Feb 2024 01:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11756v1</guid></item><item><title>Solving Data-centric Tasks using Large Language Models</title><link>http://arxiv.org/abs/2402.11734v1</link><description>Large language models (LLMs) are rapidly replacing help forums likeStackOverflow, and are especially helpful for non-professional programmers andend users. These users are often interested in data-centric tasks, such asspreadsheet manipulation and data wrangling, which are hard to solve if theintent is only communicated using a natural-language description, withoutincluding the data. But how do we decide how much data and which data toinclude in the prompt? This paper makes two contributions towards answeringthis question. First, we create a dataset of real-world NL-to-code tasksmanipulating tabular data, mined from StackOverflow posts. Second, we introducea cluster-then-select prompting technique, which adds the most representativerows from the input data to the LLM prompt. Our experiments show that LLMperformance is indeed sensitive to the amount of data passed in the prompt, andthat for tasks with a lot of syntactic variation in the input table, ourcluster-then-select technique outperforms a random selection baseline.</description><author>Shraddha Barke, Christian Poelitz, Carina Suzana Negreanu, Benjamin Zorn, José Cambronero, Andrew D. Gordon, Vu Le, Elnaz Nouri, Nadia Polikarpova, Advait Sarkar, Brian Slininger, Neil Toronto, Jack Williams</author><pubDate>Sun, 18 Feb 2024 23:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11734v1</guid></item><item><title>LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2402.11550v1</link><description>Large language models (LLMs) have demonstrated impressive performance inunderstanding language and executing complex reasoning tasks. However, LLMswith long context windows have been notorious for their expensive trainingcosts and high inference latency. Even the most advanced models such as GPT-4and Claude2 often make mistakes when processing inputs of over $100k$ tokens, aphenomenon also known as \textit{lost in the middle}. In this paper, we propose\textsc{LongAgent}, a method based on multi-agent collaboration, which scalesLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiorityin long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader isresponsible for understanding user intent and directing team members to acquireinformation from documents. Due to members' hallucinations, it is non-trivialfor a leader to obtain accurate information from the responses of dozens tohundreds of members. To address this, we develop an \textit{inter-membercommunication} mechanism to resolve response conflicts caused by hallucinationsthrough information sharing. Our experimental results indicate that\textsc{LongAgent} offers a promising alternative for long-text processing. Theagent team instantiated with LLaMA-7B achieves significant improvements intasks such as 128k-long text retrieval, multi-hop question answering, comparedto GPT-4.</description><author>Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Sun, 18 Feb 2024 11:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11550v1</guid></item><item><title>Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought</title><link>http://arxiv.org/abs/2402.11541v1</link><description>Although the method of enhancing large language models' (LLMs') reasoningability and reducing their hallucinations through the use of knowledge graphs(KGs) has received widespread attention, the exploration of how to enable LLMsto integrate the structured knowledge in KGs on-the-fly remains inadequate.Researchers often co-train KG embeddings and LLM parameters to equip LLMs withthe ability of comprehending KG knowledge. However, this resource-hungrytraining paradigm significantly increases the model learning cost and is alsounsuitable for non-open-source, black-box LLMs. In this paper, we employcomplex question answering (CQA) as a task to assess the LLM's ability ofcomprehending KG knowledge. We conducted a comprehensive comparison of KGknowledge injection methods (from triples to natural language text), aiming toexplore the optimal prompting method for supplying KG knowledge to LLMs,thereby enhancing their comprehension of KG. Contrary to our initialexpectations, our analysis revealed that LLMs effectively handle messy, noisy,and linearized KG knowledge, outperforming methods that employ well-designednatural language (NL) textual prompts. This counter-intuitive finding providessubstantial insights for future research on LLMs' comprehension of structuredknowledge.</description><author>Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11541v1</guid></item><item><title>SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</title><link>http://arxiv.org/abs/2311.01740v2</link><description>Hallucination detection is a critical step toward understanding thetrustworthiness of modern language models (LMs). To achieve this goal, were-examine existing detection approaches based on the self-consistency of LMsand uncover two types of hallucinations resulting from 1) question-level and 2)model-level, which cannot be effectively identified through self-consistencycheck alone. Building upon this discovery, we propose a novel sampling-basedmethod, i.e., semantic-aware cross-check consistency (SAC3) that expands on theprinciple of self-consistency checking. Our SAC3 approach incorporatesadditional mechanisms to detect both question-level and model-levelhallucinations by leveraging advances including semantically equivalentquestion perturbation and cross-model response consistency checking. Throughextensive and systematic empirical analysis, we demonstrate that SAC3outperforms the state of the art in detecting both non-factual and factualstatements across multiple question-answering and open-domain generationbenchmarks.</description><author>Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Sricharan Kumar</author><pubDate>Sun, 18 Feb 2024 06:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01740v2</guid></item><item><title>Faithful Knowledge Graph Explanations for Commonsense Reasoning</title><link>http://arxiv.org/abs/2310.04910v3</link><description>While fusing language models and knowledge graphs has become common incommonsense question answering research, enabling faithful chain-of-thoughtexplanations in these models remains an open problem. Our analysis reveals thatone major weakness of current KG-based explanation methodologies lies inoverlooking the faithfulness of path decoding during evaluation. This oversightleads to the distribution of the graph encoder often diverging from theoriginal model predictions. To address this gap, we present two maincontributions: (1) We propose and validate Text-GNN Fidelity in this specificcontext, to assess the reliability of the graph representation. (2) Weintroduce TeGDA (Text-Graph Distribution-aware Alignment), a novel algorithmthat aligns the graph encoder with the target model to improve the faithfulnessof subsequent explanations and that can be easily integrated into existingapproaches. Our experiments and analysis show its potential to produce morefaithful systems. Concretely, our work emphasises the neglected distributionalmisalignment problem in LM-KG reasoning models, which has been a latent sourceof spurious explanations.</description><author>Weihe Zhai, Arkaitz Zubiaga, Bingquan Liu, Chengjie Sun, Yalong Zhao</author><pubDate>Sat, 17 Feb 2024 14:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04910v3</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v1</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Tuo Zhao, Jing Gao</author><pubDate>Fri, 16 Feb 2024 23:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v1</guid></item><item><title>Forward-Backward Reasoning in Large Language Models for Mathematical Verification</title><link>http://arxiv.org/abs/2308.07758v5</link><description>Self-Consistency samples diverse reasoning chains with answers and choosesthe final answer by majority voting. It is based on forward reasoning andcannot further improve performance by sampling more reasoning chains whensaturated. To further boost performance, we introduce backward reasoning toverify candidate answers. Specifically, for mathematical tasks, we mask anumber in the question and ask the LLM to answer a backward question created bya simple template, i.e., to predict the masked number when a candidate answeris provided. Instead of using forward or backward reasoning alone, we proposeFOBAR to combine FOrward and BAckward Reasoning for verification. Extensiveexperiments on six standard mathematical data sets and three LLMs show thatFOBAR achieves state-of-the-art performance. In particular, FOBAR outperformsSelf-Consistency, which uses forward reasoning alone, demonstrating thatcombining forward and forward reasoning is better. In addition, FOBAR performsbetter than existing verification methods, showing the effectiveness of thesimple template used in backward reasoning and the proposed combination.Extensions to non-mathematical problems are also discussed and validatedempirically.</description><author>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</author><pubDate>Sat, 17 Feb 2024 04:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07758v5</guid></item><item><title>Pruning vs Quantization: Which is Better?</title><link>http://arxiv.org/abs/2307.02973v2</link><description>Neural network pruning and quantization techniques are almost as old asneural networks themselves. However, to date only ad-hoc comparisons betweenthe two have been published. In this paper, we set out to answer the questionon which is better: neural network quantization or pruning? By answering thisquestion, we hope to inform design decisions made on neural network hardwaregoing forward. We provide an extensive comparison between the two techniquesfor compressing deep neural networks. First, we give an analytical comparisonof expected quantization and pruning error for general data distributions.Then, we provide lower bounds for the per-layer pruning and quantization errorin trained networks, and compare these to empirical error after optimization.Finally, we provide an extensive experimental comparison for training 8large-scale models on 3 tasks. Our results show that in most cases quantizationoutperforms pruning. Only in some scenarios with very high compression ratio,pruning might be beneficial from an accuracy standpoint.</description><author>Andrey Kuzmin, Markus Nagel, Mart van Baalen, Arash Behboodi, Tijmen Blankevoort</author><pubDate>Fri, 16 Feb 2024 09:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02973v2</guid></item><item><title>Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios</title><link>http://arxiv.org/abs/2311.08154v2</link><description>Although chain-of-thought (CoT) prompting combined with language models hasachieved encouraging results on complex reasoning tasks, the naive greedydecoding used in CoT prompting usually causes the repetitiveness and localoptimality. To address this shortcoming, ensemble-optimization tries to obtainmultiple reasoning paths to get the final answer assembly. However, currentensemble-optimization methods either simply employ rule-based post-processingsuch as \textit{self-consistency}, or train an additional model based onseveral task-related human annotations to select the best one among multiplereasoning paths, yet fail to generalize to realistic settings where the type ofinput questions is unknown or the answer format of reasoning paths is unknown.To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizableensemble-optimization method applying in almost all scenarios where the type ofinput questions and the answer format of reasoning paths may be known orunknown. Self-agreement firstly samples from language model's decoder togenerate a \textit{diverse} set of reasoning paths, and subsequently promptsthe language model \textit{one more time} to determine the optimal answer byselecting the most \textit{agreed} answer among the sampled reasoning paths.Self-agreement simultaneously achieves remarkable performance on six publicreasoning benchmarks and superior generalization capabilities.</description><author>Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai</author><pubDate>Sun, 18 Feb 2024 07:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08154v2</guid></item><item><title>Optimal Differentially Private Model Training with Public Data</title><link>http://arxiv.org/abs/2306.15056v2</link><description>Differential privacy (DP) ensures that training a machine learning model doesnot leak private data. In practice, we may have access to auxiliary public datathat is free of privacy concerns. In this work, we assume access to a givenamount of public data and settle the following fundamental open questions: 1.What is the optimal (worst-case) error of a DP model trained over a privatedata set while having access to side public data? 2. How can we harness publicdata to improve DP model training in practice? We consider these questions inboth the local and central models of pure and approximate DP. To answer thefirst question, we prove tight (up to log factors) lower and upper bounds thatcharacterize the optimal error rates of three fundamental problems: meanestimation, empirical risk minimization, and stochastic convex optimization. Weshow that the optimal error rates can be attained (up to log factors) by eitherdiscarding private data and training a public model, or treating public datalike it is private and using an optimal DP algorithm. To address the secondquestion, we develop novel algorithms that are "even more optimal" (i.e. betterconstants) than the asymptotically optimal approaches described above. Forlocal DP mean estimation, our algorithm is \ul{optimal including constants}.Empirically, our algorithms show benefits over the state-of-the-art.</description><author>Andrew Lowy, Zeman Li, Tianjian Huang, Meisam Razaviyayn</author><pubDate>Wed, 14 Feb 2024 04:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15056v2</guid></item><item><title>Gender Bias in News Summarization: Measures, Pitfalls and Corpora</title><link>http://arxiv.org/abs/2309.08047v2</link><description>Summarization is an important application of large language models (LLMs).Most previous evaluation of summarization models has focused on theirperformance in content selection, faithfulness, grammaticality and coherence.However, it is well known that LLMs reproduce and reinforce harmful socialbiases. This raises the question: Do these biases affect model outputs in arelatively constrained setting like summarization? To help answer this question, we first motivate and introduce a number ofdefinitions for biased behaviours in summarization models, along with practicaloperationalizations. Since we find that biases inherent to input documents canconfound bias analysis in summaries, we propose a method to generate inputdocuments with carefully controlled demographic attributes. This allows us tostudy summarizer behavior in a controlled setting, while still working withrealistic input documents. Finally, we measure gender bias in English summaries generated by bothpurpose-built summarization models and general purpose chat models as a casestudy. We find content selection in single document summarization to be largelyunaffected by gender bias, while hallucinations exhibit evidence of downstreambiases in summarization.</description><author>Julius Steen, Katja Markert</author><pubDate>Fri, 16 Feb 2024 12:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08047v2</guid></item><item><title>Pushing The Limit of LLM Capacity for Text Classification</title><link>http://arxiv.org/abs/2402.07470v2</link><description>The value of text classification's future research has encountered challengesand uncertainties, due to the extraordinary efficacy demonstrated by largelanguage models (LLMs) across numerous downstream NLP tasks. In this era ofopen-ended language modeling, where task boundaries are gradually fading, anurgent question emerges: have we made significant advances in textclassification under the full benefit of LLMs? To answer this question, wepropose RGPT, an adaptive boosting framework tailored to produce a specializedtext classification LLM by recurrently ensembling a pool of strong baselearners. The base learners are constructed by adaptively adjusting thedistribution of training samples and iteratively fine-tuning LLMs with them.Such base learners are then ensembled to be a specialized text classificationLLM, by recurrently incorporating the historical predictions from the previouslearners. Through a comprehensive empirical comparison, we show that RGPTsignificantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by1.36% on average. Further evaluation experiments show a clear surpassing ofRGPT over human classification.</description><author>Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin</author><pubDate>Fri, 16 Feb 2024 15:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07470v2</guid></item><item><title>Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs</title><link>http://arxiv.org/abs/2402.12052v1</link><description>The integration of large language models (LLMs) and search engines representsa significant evolution in knowledge acquisition methodologies. However,determining the knowledge that an LLM already possesses and the knowledge thatrequires the help of a search engine remains an unresolved issue. Most existingmethods solve this problem through the results of preliminary answers orreasoning done by the LLM itself, but this incurs excessively highcomputational costs. This paper introduces a novel collaborative approach,namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model,to enhance the LLM's knowledge acquisition process. We employ a proxy modelwhich has far fewer parameters, and take its answers as heuristic answers.Heuristic answers are then utilized to predict the knowledge required to answerthe user question, as well as the known and unknown knowledge within the LLM.We only conduct retrieval for the missing knowledge in questions that the LLMdoes not know. Extensive experimental results on five datasets with two LLMsdemonstrate a notable improvement in the end-to-end performance of LLMs inquestion-answering tasks, achieving or surpassing current state-of-the-artmodels with lower LLM inference costs.</description><author>Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen</author><pubDate>Mon, 19 Feb 2024 11:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12052v1</guid></item><item><title>What Evidence Do Language Models Find Convincing?</title><link>http://arxiv.org/abs/2402.11782v1</link><description>Retrieval-augmented language models are being increasingly tasked withsubjective, contentious, and conflicting queries such as "is aspartame linkedto cancer". To resolve these ambiguous queries, one must search through a largerange of websites and consider "which, if any, of this evidence do I findconvincing?". In this work, we study how LLMs answer this question. Inparticular, we construct ConflictingQA, a dataset that pairs controversialqueries with a series of real-world evidence documents that contain differentfacts (e.g., quantitative results), argument styles (e.g., appeals toauthority), and answers (Yes or No). We use this dataset to perform sensitivityand counterfactual analyses to explore which text features most affect LLMpredictions. Overall, we find that current models rely heavily on the relevanceof a website to the query, while largely ignoring stylistic features thathumans find important such as whether a text contains scientific references oris written with a neutral tone. Taken together, these results highlight theimportance of RAG corpus quality (e.g., the need to filter misinformation), andpossibly even a shift in how LLMs are trained to better align with humanjudgements.</description><author>Alexander Wan, Eric Wallace, Dan Klein</author><pubDate>Mon, 19 Feb 2024 02:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11782v1</guid></item><item><title>(Ir)rationality and Cognitive Biases in Large Language Models</title><link>http://arxiv.org/abs/2402.09193v2</link><description>Do large language models (LLMs) display rational reasoning? LLMs have beenshown to contain human biases due to the data they have been trained on;whether this is reflected in rational reasoning remains less clear. In thispaper, we answer this question by evaluating seven language models using tasksfrom the cognitive psychology literature. We find that, like humans, LLMsdisplay irrationality in these tasks. However, the way this irrationality isdisplayed does not reflect that shown by humans. When incorrect answers aregiven by LLMs to these tasks, they are often incorrect in ways that differ fromhuman-like biases. On top of this, the LLMs reveal an additional layer ofirrationality in the significant inconsistency of the responses. Aside from theexperimental results, this paper seeks to make a methodological contribution byshowing how we can assess and compare different capabilities of these types ofmodels, in this case with respect to rational reasoning.</description><author>Olivia Macmillan-Scott, Mirco Musolesi</author><pubDate>Thu, 15 Feb 2024 11:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09193v2</guid></item><item><title>Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States</title><link>http://arxiv.org/abs/2402.09733v1</link><description>Large Language Models (LLMs) can make up answers that are not real, and thisis known as hallucination. This research aims to see if, how, and to whatextent LLMs are aware of hallucination. More specifically, we check whether andhow an LLM reacts differently in its hidden states when it answers a questionright versus when it hallucinates. To do this, we introduce an experimentalframework which allows examining LLM's hidden states in different hallucinationsituations. Building upon this framework, we conduct a series of experimentswith language models in the LLaMA family (Touvron et al., 2023). Our empiricalfindings suggest that LLMs react differently when processing a genuine responseversus a fabricated one. We then apply various model interpretation techniquesto help understand and explain the findings better. Moreover, informed by theempirical observations, we show great potential of using the guidance derivedfrom LLM's hidden representation space to mitigate hallucination. We believethis work provides insights into how LLMs produce hallucinated answers and howto make them occur less often.</description><author>Hanyu Duan, Yi Yang, Kar Yan Tam</author><pubDate>Thu, 15 Feb 2024 06:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09733v1</guid></item><item><title>Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty</title><link>http://arxiv.org/abs/2402.10423v1</link><description>Our work focuses on understanding the underpinning mechanism of datasetcondensation by drawing connections with ($\epsilon$, $\delta$)-differentialprivacy where the optimal noise, $\epsilon$, is chosen by adversarialuncertainty \cite{Grining2017}. We can answer the question about the innerworkings of the dataset condensation procedure. Previous work \cite{dong2022}proved the link between dataset condensation (DC) and ($\epsilon$,$\delta$)-differential privacy. However, it is unclear from existing works onablating DC to obtain a lower-bound estimate of $\epsilon$ that will sufficefor creating high-fidelity synthetic data. We suggest that adversarialuncertainty is the most appropriate method to achieve an optimal noise level,$\epsilon$. As part of the internal dynamics of dataset condensation, we adopta satisfactory scheme for noise estimation that guarantees high-fidelity datawhile providing privacy.</description><author>Kenneth Odoh</author><pubDate>Fri, 16 Feb 2024 03:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10423v1</guid></item><item><title>Can Transformers Predict Vibrations?</title><link>http://arxiv.org/abs/2402.10511v1</link><description>Highly accurate time-series vibration prediction is an important researchissue for electric vehicles (EVs). EVs often experience vibrations when drivingon rough terrains, known as torsional resonance. This resonance, caused by theinteraction between motor and tire vibrations, puts excessive loads on thevehicle's drive shaft. However, current damping technologies only detectresonance after the vibration amplitude of the drive shaft torque reaches acertain threshold, leading to significant loads on the shaft at the time ofdetection. In this study, we propose a novel approach to address this issue byintroducing Resoformer, a transformer-based model for predicting torsionalresonance. Resoformer utilizes time-series of the motor rotation speed as inputand predicts the amplitude of torsional vibration at a specified quantileoccurring in the shaft after the input series. By calculating the attentionbetween recursive and convolutional features extracted from the measured datapoints, Resoformer improves the accuracy of vibration forecasting. To evaluatethe model, we use a vibration dataset called VIBES (Dataset for ForecastingVibration Transition in EVs), consisting of 2,600 simulator-generated vibrationsequences. Our experiments, conducted on strong baselines built on the VIBESdataset, demonstrate that Resoformer achieves state-of-the-art results. Inconclusion, our study answers the question "Can Transformers ForecastVibrations?" While traditional transformer architectures show low performancein forecasting torsional resonance waves, our findings indicate that combiningrecurrent neural network and temporal convolutional network using thetransformer architecture improves the accuracy of long-term vibrationforecasting.</description><author>Fusataka Kuniyoshi, Yoshihide Sawada</author><pubDate>Fri, 16 Feb 2024 08:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10511v1</guid></item><item><title>Towards 3D VR-Sketch to 3D Shape Retrieval</title><link>http://arxiv.org/abs/2209.10020v2</link><description>Growing free online 3D shapes collections dictated research on 3D retrieval.Active debate has however been had on (i) what the best input modality is totrigger retrieval, and (ii) the ultimate usage scenario for such retrieval. Inthis paper, we offer a different perspective towards answering these questions-- we study the use of 3D sketches as an input modality and advocate aVR-scenario where retrieval is conducted. Thus, the ultimate vision is thatusers can freely retrieve a 3D model by air-doodling in a VR environment. As afirst stab at this new 3D VR-sketch to 3D shape retrieval problem, we make fourcontributions. First, we code a VR utility to collect 3D VR-sketches andconduct retrieval. Second, we collect the first set of $167$ 3D VR-sketches ontwo shape categories from ModelNet. Third, we propose a novel approach togenerate a synthetic dataset of human-like 3D sketches of different abstractlevels to train deep networks. At last, we compare the common multi-view andvolumetric approaches: We show that, in contrast to 3D shape to 3D shaperetrieval, volumetric point-based approaches exhibit superior performance on 3Dsketch to 3D shape retrieval due to the sparse and abstract nature of 3DVR-sketches. We believe these contributions will collectively serve as enablersfor future attempts at this problem. The VR interface, code and datasets areavailable at https://tinyurl.com/3DSketch3DV.</description><author>Ling Luo, Yulia Gryaditskaya, Yongxin Yang, Tao Xiang, Yi-Zhe Song</author><pubDate>Sun, 18 Feb 2024 11:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10020v2</guid></item><item><title>Investigating White-Box Attacks for On-Device Models</title><link>http://arxiv.org/abs/2402.05493v2</link><description>Numerous mobile apps have leveraged deep learning capabilities. However,on-device models are vulnerable to attacks as they can be easily extracted fromtheir corresponding mobile apps. Existing on-device attacking approaches onlygenerate black-box attacks, which are far less effective and efficient thanwhite-box strategies. This is because mobile deep learning frameworks likeTFLite do not support gradient computing, which is necessary for white-boxattacking algorithms. Thus, we argue that existing findings may underestimatethe harmfulness of on-device attacks. To this end, we conduct a study to answerthis research question: Can on-device models be directly attacked via white-boxstrategies? We first systematically analyze the difficulties of transformingthe on-device model to its debuggable version, and propose a ReverseEngineering framework for On-device Models (REOM), which automatically reversesthe compiled on-device TFLite model to the debuggable model. Specifically, REOMfirst transforms compiled on-device models into Open Neural Network Exchangeformat, then removes the non-debuggable parts, and converts them to thedebuggable DL models format that allows attackers to exploit in a white-boxsetting. Our experimental results show that our approach is effective inachieving automated transformation among 244 TFLite models. Compared withprevious attacks using surrogate models, REOM enables attackers to achievehigher attack success rates with a hundred times smaller attack perturbations.In addition, because the ONNX platform has plenty of tools for model formatexchanging, the proposed method based on the ONNX platform can be adapted toother model formats. Our findings emphasize the need for developers tocarefully consider their model deployment strategies, and use white-box methodsto evaluate the vulnerability of on-device models.</description><author>Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li</author><pubDate>Sat, 17 Feb 2024 13:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05493v2</guid></item><item><title>Robust agents learn causal world models</title><link>http://arxiv.org/abs/2402.10877v1</link><description>It has long been hypothesised that causal reasoning plays a fundamental rolein robust and general intelligence. However, it is not known if agents mustlearn causal models in order to generalise to new domains, or if otherinductive biases are sufficient. We answer this question, showing that anyagent capable of satisfying a regret bound under a large set of distributionalshifts must have learned an approximate causal model of the data generatingprocess, which converges to the true causal model for optimal agents. Wediscuss the implications of this result for several research areas includingtransfer learning and causal inference.</description><author>Jonathan Richens, Tom Everitt</author><pubDate>Fri, 16 Feb 2024 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10877v1</guid></item><item><title>Model approximation in MDPs with unbounded per-step cost</title><link>http://arxiv.org/abs/2402.08813v1</link><description>We consider the problem of designing a control policy for an infinite-horizondiscounted cost Markov decision process $\mathcal{M}$ when we only have accessto an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy$\hat{\pi}^{\star}$ of the approximate model perform when used in the originalmodel $\mathcal{M}$? We answer this question by bounding a weighted norm of thedifference between the value function of $\hat{\pi}^\star $ when used in$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extendour results and obtain potentially tighter upper bounds by considering affinetransformations of the per-step cost. We further provide upper bounds thatexplicitly depend on the weighted distance between cost functions and weighteddistance between transition kernels of the original and approximate models. Wepresent examples to illustrate our results.</description><author>Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang</author><pubDate>Tue, 13 Feb 2024 21:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08813v1</guid></item><item><title>KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph</title><link>http://arxiv.org/abs/2402.11163v1</link><description>In this paper, we aim to improve the reasoning ability of large languagemodels (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspiredby existing methods that design the interaction strategy between LLMs and KG,we propose an autonomous LLM-based agent framework, called KG-Agent, whichenables a small LLM to actively make decisions until finishing the reasoningprocess over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox,KG-based executor, and knowledge memory, and develop an iteration mechanismthat autonomously selects the tool then updates the memory for reasoning overKG. To guarantee the effectiveness, we leverage program language to formulatethe multi-hop reasoning process over the KG, and synthesize a code-basedinstruction dataset to fine-tune the base LLM. Extensive experimentsdemonstrate that only using 10K samples for tuning LLaMA-7B can outperformstate-of-the-art methods using larger LLMs or more data, on both in-domain andout-domain datasets. Our code and data will be publicly released.</description><author>Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, Ji-Rong Wen</author><pubDate>Sat, 17 Feb 2024 02:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11163v1</guid></item><item><title>AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies</title><link>http://arxiv.org/abs/2402.12370v1</link><description>Humans regularly engage in analogical thinking, relating personal experiencesto current situations ($X$ is analogous to $Y$ because of $Z$). Analogicalthinking allows humans to solve problems in creative ways, grasp difficultconcepts, and articulate ideas more effectively. Can language models (LMs) dothe same? To answer this question, we propose ANALOBENCH, a benchmark todetermine analogical reasoning ability in LMs. Our benchmarking approachfocuses on aspects of this ability that are common among humans: (i) recallingrelated experiences from a large amount of information, and (ii) applyinganalogical reasoning to complex and lengthy scenarios. We test a broadcollection of proprietary models (e.g., GPT family, Claude V2) and open sourcemodels such as LLaMA2. As in prior results, scaling up LMs results in someperformance boosts. Surprisingly, scale offers minimal gains when, (i)analogies involve lengthy scenarios, or (ii) recalling relevant scenarios froma large pool of information, a process analogous to finding a needle in ahaystack. We hope these observations encourage further research in this field.</description><author>Xiao Ye, Andrew Wang, Jacob Choi, Yining Lu, Shreya Sharma, Lingfeng Shen, Vijay Tiyyala, Nicholas Andrews, Daniel Khashabi</author><pubDate>Mon, 19 Feb 2024 18:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12370v1</guid></item><item><title>ProtChatGPT: Towards Understanding Proteins with Large Language Models</title><link>http://arxiv.org/abs/2402.09649v1</link><description>Protein research is crucial in various fundamental disciplines, butunderstanding their intricate structure-function relationships remainschallenging. Recent Large Language Models (LLMs) have made significant stridesin comprehending task-specific knowledge, suggesting the potential forChatGPT-like systems specialized in protein to facilitate basic research. Inthis work, we introduce ProtChatGPT, which aims at learning and understandingprotein structures via natural languages. ProtChatGPT enables users to uploadproteins, ask questions, and engage in interactive conversations to producecomprehensive answers. The system comprises protein encoders, aProtein-Language Pertaining Transformer (PLP-former), a projection adapter, andan LLM. The protein first undergoes protein encoders and PLP-former to produceprotein embeddings, which are then projected by the adapter to conform with theLLM. The LLM finally combines user questions with projected embeddings togenerate informative answers. Experiments show that ProtChatGPT can producepromising responses to proteins and their corresponding questions. We hope thatProtChatGPT could form the basis for further exploration and application inprotein research. Code and our pre-trained model will be publicly available.</description><author>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang</author><pubDate>Thu, 15 Feb 2024 01:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09649v1</guid></item><item><title>Private PAC Learning May be Harder than Online Learning</title><link>http://arxiv.org/abs/2402.11119v1</link><description>We continue the study of the computational complexity of differentiallyprivate PAC learning and how it is situated within the foundations of machinelearning. A recent line of work uncovered a qualitative equivalence between theprivate PAC model and Littlestone's mistake-bounded model of online learning,in particular, showing that any concept class of Littlestone dimension $d$ canbe privately PAC learned using $\mathrm{poly}(d)$ samples. This raises thenatural question of whether there might be a generic conversion from onlinelearners to private PAC learners that also preserves computational efficiency. We give a negative answer to this question under reasonable cryptographicassumptions (roughly, those from which it is possible to buildindistinguishability obfuscation for all circuits). We exhibit a concept classthat admits an online learner running in polynomial time with a polynomialmistake bound, but for which there is no computationally-efficientdifferentially private PAC learner. Our construction and analysis strengthensand generalizes that of Bun and Zhandry (TCC 2016-A), who established such aseparation between private and non-private PAC learner.</description><author>Mark Bun, Aloni Cohen, Rathin Desai</author><pubDate>Fri, 16 Feb 2024 22:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11119v1</guid></item><item><title>Explaining generative diffusion models via visual analysis for interpretable decision-making process</title><link>http://arxiv.org/abs/2402.10404v1</link><description>Diffusion models have demonstrated remarkable performance in generationtasks. Nevertheless, explaining the diffusion process remains challenging dueto it being a sequence of denoising noisy images that are difficult for expertsto interpret. To address this issue, we propose the three research questions tointerpret the diffusion process from the perspective of the visual conceptsgenerated by the model and the region where the model attends in each timestep. We devise tools for visualizing the diffusion process and answering theaforementioned research questions to render the diffusion processhuman-understandable. We show how the output is progressively generated in thediffusion process by explaining the level of denoising and highlightingrelationships to foundational visual concepts at each time step through theresults of experiments with various visual analyses using the tools. Throughoutthe training of the diffusion model, the model learns diverse visual conceptscorresponding to each time-step, enabling the model to predict varying levelsof visual concepts at different stages. We substantiate our tools using AreaUnder Cover (AUC) score, correlation quantification, and cross-attentionmapping. Our findings provide insights into the diffusion process and pave theway for further research into explainable diffusion mechanisms.</description><author>Ji-Hoon Park, Yeong-Joon Ju, Seong-Whan Lee</author><pubDate>Fri, 16 Feb 2024 02:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10404v1</guid></item><item><title>Can We Verify Step by Step for Incorrect Answer Detection?</title><link>http://arxiv.org/abs/2402.10528v1</link><description>Chain-of-Thought (CoT) prompting has marked a significant advancement inenhancing the reasoning capabilities of large language models (LLMs). Previousstudies have developed various extensions of CoT, which focus primarily onenhancing end-task performance. In addition, there has been research onassessing the quality of reasoning chains in CoT. This raises an intriguingquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizingthe reasoning chains they generate? To answer this research question, weintroduce a benchmark, R2PE, designed specifically to explore the relationshipbetween reasoning chains and performance in various reasoning tasks spanningfive different domains. This benchmark aims to measure the falsehood of thefinal output of LLMs based on the reasoning steps. To make full use ofinformation in multiple reasoning chains, we propose the process discernibilityscore (PDS) framework that beats the answer-checking baseline by a largemargin. Concretely, this resulted in an average of 5.1% increase in the F1score across all 45 subsets within R2PE. We further demonstrate our PDS'sefficacy in advancing open-domain QA accuracy. Data and code are available athttps://github.com/XinXU-USTC/R2PE.</description><author>Xin Xu, Shizhe Diao, Can Yang, Yang Wang</author><pubDate>Fri, 16 Feb 2024 09:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10528v1</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v1</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Wed, 14 Feb 2024 17:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v1</guid></item><item><title>Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation</title><link>http://arxiv.org/abs/2302.03038v2</link><description>Spatially resolved transcriptomics brings exciting breakthroughs tosingle-cell analysis by providing physical locations along with geneexpression. However, as a cost of the extremely high spatial resolution, thecellular level spatial transcriptomic data suffer significantly from missingvalues. While a standard solution is to perform imputation on the missingvalues, most existing methods either overlook spatial information or onlyincorporate localized spatial context without the ability to capture long-rangespatial information. Using multi-head self-attention mechanisms and positionalencoding, transformer models can readily grasp the relationship between tokensand encode location information. In this paper, by treating single cells asspatial tokens, we study how to leverage transformers to facilitate spatialtanscriptomics imputation. In particular, investigate the following two keyquestions: (1) $\textit{how to encode spatial information of cells intransformers}$, and (2) $\textit{ how to train a transformer for transcriptomicimputation}$. By answering these two questions, we present a transformer-basedimputation framework, SpaFormer, for cellular-level spatial transcriptomicdata. Extensive experiments demonstrate that SpaFormer outperforms existingstate-of-the-art imputation algorithms on three large-scale datasets whilemaintaining superior computational efficiency.</description><author>Hongzhi Wen, Wenzhuo Tang, Wei Jin, Jiayuan Ding, Renming Liu, Xinnan Dai, Feng Shi, Lulu Shang, Hui Liu, Yuying Xie</author><pubDate>Fri, 16 Feb 2024 17:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03038v2</guid></item><item><title>Language Models as Science Tutors</title><link>http://arxiv.org/abs/2402.11111v1</link><description>NLP has recently made exciting progress toward training language models (LMs)with strong scientific problem-solving skills. However, model development hasnot focused on real-life use-cases of LMs for science, including applicationsin education that require processing long scientific documents. To addressthis, we introduce TutorEval and TutorChat. TutorEval is a diversequestion-answering benchmark consisting of questions about long chapters fromSTEM textbooks, written by experts. TutorEval helps measure real-life usabilityof LMs as scientific assistants, and it is the first benchmark combining longcontexts, free-form generation, and multi-disciplinary scientific knowledge.Moreover, we show that fine-tuning base models with existing dialogue datasetsleads to poor performance on TutorEval. Therefore, we create TutorChat, adataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat tofine-tune Llemma models with 7B and 34B parameters. These LM tutors specializedin math have a 32K-token context window, and they excel at TutorEval whileperforming strongly on GSM8K and MATH. Our datasets build on open-sourcematerials, and we release our models, data, and evaluations.</description><author>Alexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Jameson Aragon, Arturo Rodríguez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen T. Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Jia, Jiatong Yu, Jun-Jie Zhu, Zhiyong Jason Ren, Sanjeev Arora, Danqi Chen</author><pubDate>Fri, 16 Feb 2024 22:24:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11111v1</guid></item><item><title>PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</title><link>http://arxiv.org/abs/2309.07759v2</link><description>Interactive Object Grasping (IOG) is the task of identifying and grasping thedesired object via human-robot natural language interaction. Current IOGsystems assume that a human user initially specifies the target object'scategory (e.g., bottle). Inspired by pragmatics, where humans often conveytheir intentions by relying on context to achieve goals, we introduce a new IOGtask, Pragmatic-IOG, and the corresponding dataset, Intention-orientedMulti-modal Dialogue (IM-Dial). In our proposed task scenario, anintention-oriented utterance (e.g., "I am thirsty") is initially given to therobot. The robot should then identify the target object by interacting with ahuman user. Based on the task setup, we propose a new robotic system that caninterpret the user's intention and pick up the target object, Pragmatic ObjectGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modulesfor visual grounding, question asking, object grasping, and most importantly,answer interpretation for pragmatic inference. Experimental results show thatPROGrasp is effective in offline (i.e., target object discovery) and online(i.e., IOG with a physical robot arm) settings. Code and data are available athttps://github.com/gicheonkang/prograsp.</description><author>Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang</author><pubDate>Sun, 18 Feb 2024 10:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07759v2</guid></item><item><title>ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions</title><link>http://arxiv.org/abs/2402.08801v1</link><description>Since its release in November 2022, ChatGPT has shaken up Stack Overflow, thepremier platform for developers' queries on programming and softwaredevelopment. Demonstrating an ability to generate instant, human-like responsesto technical questions, ChatGPT has ignited debates within the developercommunity about the evolving role of human-driven platforms in the age ofgenerative AI. Two months after ChatGPT's release, Meta released its answerwith its own Large Language Model (LLM) called LLaMA: the race was on. Weconducted an empirical study analyzing questions from Stack Overflow and usingthese LLMs to address them. This way, we aim to (ii) measure user engagementevolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'answers and their potential to replace Stack Overflow in the long term; (iii)identify and understand why LLMs fails; and (iv) compare LLMs together. Ourempirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,yet do not outperform it for some domains, while a significant decline in userposting activity has been observed. Furthermore, we also discuss the impact ofour findings regarding the usage and development of new LLMs.</description><author>Leuson Da Silva, Jordan Samhi, Foutse Khomh</author><pubDate>Tue, 13 Feb 2024 21:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08801v1</guid></item><item><title>Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4</title><link>http://arxiv.org/abs/2402.10083v1</link><description>Purpose: To assess the alignment of GPT-4-based evaluation to human clinicianexperts, for the evaluation of responses to ophthalmology-related patientqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmologyquestions and paired answers were created by ophthalmologists to representcommonly asked patient questions, divided into fine-tuning (368; 92%), andtesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,additional 8 glaucoma QnA pairs were included. 200 responses to the testingdataset were generated by 5 fine-tuned LLMs for evaluation. A customizedclinical evaluation rubric was used to guide GPT-4 evaluation, grounded onclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4evaluation was then compared against ranking by 5 clinicians for clinicalalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4evaluation demonstrated significant agreement with human clinician rankings,with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80respectively; while correlation based on Cohen Kappa was more modest at 0.50.Notably, qualitative analysis and the glaucoma sub-analysis revealed clinicalinaccuracies in the LLM-generated responses, which were appropriatelyidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignmentof GPT-4 evaluation highlighted its potential to streamline the clinicalevaluation of LLM chatbot responses to healthcare-related queries. Bycomplementing the existing clinician-dependent manual grading, this efficientand automated evaluation could assist the validation of future developments inLLM applications for healthcare.</description><author>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</author><pubDate>Thu, 15 Feb 2024 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10083v1</guid></item><item><title>Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?</title><link>http://arxiv.org/abs/2401.11911v3</link><description>While auxiliary information has become a key to enhancing Large LanguageModels (LLMs), relatively little is known about how LLMs merge these contexts,specifically contexts generated by LLMs and those retrieved from externalsources. To investigate this, we formulate a systematic framework to identifywhether LLMs' responses, derived from the integration of generated andretrieved contexts, are attributed to either generated or retrieved contexts.To easily trace the origin of the response, we construct datasets withconflicting contexts, i.e., each question is paired with both generated andretrieved contexts, yet only one of them contains the correct answer. Ourexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) tofavor generated contexts, even when they provide incorrect information. Wefurther identify two key factors contributing to this bias: i) contextsgenerated by LLMs typically show greater similarity to the questions,increasing their likelihood of being selected; ii) the segmentation processused in retrieved contexts disrupts their completeness, thereby hindering theirfull utilization in LLMs. Our analysis enhances the understanding of how LLMsmerge diverse contexts, offering valuable insights for advancing currentaugmentation methods for LLMs.</description><author>Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng</author><pubDate>Sat, 17 Feb 2024 15:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11911v3</guid></item><item><title>It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning</title><link>http://arxiv.org/abs/2311.07532v2</link><description>Chain-of-thought (COT) prompting can help large language models (LLMs) reasontoward correct answers, but its efficacy in reasoning toward incorrect answersis unexplored. This process of elimination (PoE), when used with COT, canenhance self-consistency, interpretability, and tasks such as medical diagnosesof exclusion. Thus, we propose PoE with COT, where LLMs must reason towardincorrect options on multiple-choice questions. We evaluate the ability ofGPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of fourcommonsense and scientific reasoning datasets. We find that the strategy of PoEalways underperforms the strategy of choosing the correct answer. The agreementof these strategies is also lower than the self-consistency of each strategy.To study these issues further, we conduct error analyses and give suggestionsfor future work.</description><author>Nishant Balepur, Shramay Palta, Rachel Rudinger</author><pubDate>Mon, 19 Feb 2024 16:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07532v2</guid></item><item><title>KnowTuning: Knowledge-aware Fine-tuning for Large Language Models</title><link>http://arxiv.org/abs/2402.11176v1</link><description>Despite their success at many natural language processing (NLP) tasks, largelanguage models (LLMs) still struggle to effectively leverage knowledge forknowledge-intensive tasks, manifesting limitations such as generatingincomplete, non-factual, or illogical answers. These limitations stem frominadequate knowledge awareness of LLMs during vanilla fine-tuning. To addressthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method toexplicitly and implicitly improve the knowledge awareness of LLMs. We devise anexplicit knowledge-aware generation stage to train LLMs to explicitly identifyknowledge triples in answers. We also propose an implicit knowledge-awarecomparison stage to train LLMs to implicitly distinguish between reliable andunreliable knowledge, in three aspects: completeness, factuality, andlogicality. Extensive experiments on both generic and medical questionanswering (QA) datasets confirm the effectiveness of KnowTuning, throughautomatic and human evaluations, across various sizes of LLMs. Finally, wedemonstrate that the improvements of KnowTuning generalize to unseen QAdatasets.</description><author>Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren</author><pubDate>Sat, 17 Feb 2024 02:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11176v1</guid></item></channel></rss>