<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 21 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>JDocQA: Japanese Document Question Answering Dataset for Generative Language Models</title><link>http://arxiv.org/abs/2403.19454v1</link><description>Document question answering is a task of question answering on givendocuments such as reports, slides, pamphlets, and websites, and it is a trulydemanding task as paper and electronic forms of documents are so common in oursociety. This is known as a quite challenging task because it requires not onlytext understanding but also understanding of figures and tables, and hencevisual question answering (VQA) methods are often examined in addition totextual approaches. We introduce Japanese Document Question Answering (JDocQA),a large-scale document-based QA dataset, essentially requiring both visual andtextual information to answer questions, which comprises 5,504 documents in PDFformat and annotated 11,600 question-and-answer instances in Japanese. Each QAinstance includes references to the document pages and bounding boxes for theanswer clues. We incorporate multiple categories of questions and unanswerablequestions from the document for realistic question-answering applications. Weempirically evaluate the effectiveness of our dataset with text-based largelanguage models (LLMs) and multimodal models. Incorporating unanswerablequestions in finetuning may contribute to harnessing the so-calledhallucination generation.</description><author>Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe</author><pubDate>Thu, 28 Mar 2024 15:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19454v1</guid></item><item><title>EVJVQA Challenge: Multilingual Visual Question Answering</title><link>http://arxiv.org/abs/2302.11752v5</link><description>Visual Question Answering (VQA) is a challenging task of natural languageprocessing (NLP) and computer vision (CV), attracting significant attentionfrom researchers. English is a resource-rich language that has witnessedvarious developments in datasets and models for visual question answering.Visual question answering in other languages also would be developed forresources and models. In addition, there is no multilingual dataset targetingthe visual content of a particular country with its own objects and culturalcharacteristics. To address the weakness, we provide the research communitywith a benchmark dataset named EVJVQA, including 33,000+ pairs ofquestion-answer over three languages: Vietnamese, English, and Japanese, onapproximately 5,000 images taken from Vietnam for evaluating multilingual VQAsystems or models. EVJVQA is used as a benchmark dataset for the challenge ofmultilingual visual question answering at the 9th Workshop on VietnameseLanguage and Speech Processing (VLSP 2022). This task attracted 62 participantteams from various universities and organizations. In this article, we presentdetails of the organization of the challenge, an overview of the methodsemployed by shared-task participants, and the results. The highest performancesare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. Themultilingual QA systems proposed by the top 2 teams use ViT for the pre-trainedvision model and mT5 for the pre-trained language model, a powerful pre-trainedlanguage model based on the transformer architecture. EVJVQA is a challengingdataset that motivates NLP and CV researchers to further explore themultilingual models or systems for visual question answering systems. Wereleased the challenge on the Codalab evaluation system for further research.</description><author>Ngan Luu-Thuy Nguyen, Nghia Hieu Nguyen, Duong T. D Vo, Khanh Quoc Tran, Kiet Van Nguyen</author><pubDate>Wed, 17 Apr 2024 05:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11752v5</guid></item><item><title>Interactive Question Answering Systems: Literature Review</title><link>http://arxiv.org/abs/2209.01621v3</link><description>Question answering systems are recognized as popular and frequently effectivemeans of information seeking on the web. In such systems, information seekerscan receive a concise response to their query by presenting their questions innatural language. Interactive question answering is a recently proposed andincreasingly popular solution that resides at the intersection of questionanswering and dialogue systems. On the one hand, the user can ask questions innormal language and locate the actual response to her inquiry; on the otherhand, the system can prolong the question-answering session into a dialogue ifthere are multiple probable replies, very few, or ambiguities in the initialrequest. By permitting the user to ask more questions, interactive questionanswering enables users to dynamically interact with the system and receivemore precise results. This survey offers a detailed overview of the interactivequestion-answering methods that are prevalent in current literature. It beginsby explaining the foundational principles of question-answering systems, hencedefining new notations and taxonomies to combine all identified works inside aunified framework. The reviewed published work on interactivequestion-answering systems is then presented and examined in terms of itsproposed methodology, evaluation approaches, and dataset/application domain. Wealso describe trends surrounding specific tasks and issues raised by thecommunity, so shedding light on the future interests of scholars. Our work isfurther supported by a GitHub page with a synthesis of all the major topicscovered in this literature study.https://sisinflab.github.io/interactive-question-answering-systems-survey/</description><author>Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci</author><pubDate>Fri, 19 Apr 2024 14:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01621v3</guid></item><item><title>Interactive Question Answering Systems: Literature Review</title><link>http://arxiv.org/abs/2209.01621v2</link><description>Question answering systems are recognized as popular and frequently effectivemeans of information seeking on the web. In such systems, information seekerscan receive a concise response to their query by presenting their questions innatural language. Interactive question answering is a recently proposed andincreasingly popular solution that resides at the intersection of questionanswering and dialogue systems. On the one hand, the user can ask questions innormal language and locate the actual response to her inquiry; on the otherhand, the system can prolong the question-answering session into a dialogue ifthere are multiple probable replies, very few, or ambiguities in the initialrequest. By permitting the user to ask more questions, interactive questionanswering enables users to dynamically interact with the system and receivemore precise results. This survey offers a detailed overview of the interactivequestion-answering methods that are prevalent in current literature. It beginsby explaining the foundational principles of question-answering systems, hencedefining new notations and taxonomies to combine all identified works inside aunified framework. The reviewed published work on interactivequestion-answering systems is then presented and examined in terms of itsproposed methodology, evaluation approaches, and dataset/application domain. Wealso describe trends surrounding specific tasks and issues raised by thecommunity, so shedding light on the future interests of scholars. Our work isfurther supported by a GitHub page with a synthesis of all the major topicscovered in this literature study.https://sisinflab.github.io/interactive-question-answering-systems-survey/</description><author>Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci</author><pubDate>Wed, 06 Mar 2024 21:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01621v2</guid></item><item><title>Retrieval Augmented Generation for Domain-specific Question Answering</title><link>http://arxiv.org/abs/2404.14760v2</link><description>Question answering (QA) has become an important application in the advanceddevelopment of large language models. General pre-trained large language modelsfor question-answering are not trained to properly understand the knowledge orterminology for a specific domain, such as finance, healthcare, education, andcustomer service for a product. To better cater to domain-specificunderstanding, we build an in-house question-answering system for Adobeproducts. We propose a novel framework to compile a large question-answerdatabase and develop the approach for retrieval-aware finetuning of a LargeLanguage model. We showcase that fine-tuning the retriever leads to majorimprovements in the final generation. Our overall approach reduceshallucinations during generation while keeping in context the latest retrievalinformation for contextual grounding.</description><author>Sanat Sharma, David Seunghyun Yoon, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</author><pubDate>Wed, 29 May 2024 17:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14760v2</guid></item><item><title>Retrieval Augmented Generation for Domain-specific Question Answering</title><link>http://arxiv.org/abs/2404.14760v1</link><description>Question answering (QA) has become an important application in the advanceddevelopment of large language models. General pre-trained large language modelsfor question-answering are not trained to properly understand the knowledge orterminology for a specific domain, such as finance, healthcare, education, andcustomer service for a product. To better cater to domain-specificunderstanding, we build an in-house question-answering system for Adobeproducts. We propose a novel framework to compile a large question-answerdatabase and develop the approach for retrieval-aware finetuning of a LargeLanguage model. We showcase that fine-tuning the retriever leads to majorimprovements in the final generation. Our overall approach reduceshallucinations during generation while keeping in context the latest retrievalinformation for contextual grounding.</description><author>Sanat Sharma, David Seunghyun Yoon, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</author><pubDate>Tue, 23 Apr 2024 06:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14760v1</guid></item><item><title>ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages</title><link>http://arxiv.org/abs/2403.17859v1</link><description>Question answering (QA) and Machine Reading Comprehension (MRC) tasks havesignificantly advanced in recent years due to the rapid development of deeplearning techniques and, more recently, large language models. At the sametime, many benchmark datasets have become available for QA and MRC tasks.However, most existing large-scale benchmark datasets have been createdpredominantly using synchronous document collections like Wikipedia or the Web.Archival document collections, such as historical newspapers, contain valuableinformation from the past that is still not widely used to train large languagemodels. To further contribute to advancing QA and MRC tasks and to overcome thelimitation of previous datasets, we introduce ChroniclingAmericaQA, alarge-scale dataset with 485K question-answer pairs created based on thehistorical newspaper collection Chronicling America. Our dataset is constructedfrom a subset of the Chronicling America newspaper collection spanning 120years. One of the significant challenges for utilizing digitized historicalnewspaper collections is the low quality of OCR text. Therefore, to enablerealistic testing of QA models, our dataset can be used in three differentways: answering questions from raw and noisy content, answering questions fromcleaner, corrected version of the content, as well as answering questions fromscanned images of newspaper pages. This and the fact that ChroniclingAmericaQAspans the longest time period among available QA datasets make it quite aunique and useful resource.</description><author>Bhawna Piryani, Jamshid Mozafari, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17859v1</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v3</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Thu, 07 Mar 2024 06:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v3</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v2</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Wed, 06 Mar 2024 12:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v2</guid></item><item><title>Prompting-based Synthetic Data Generation for Few-Shot Question Answering</title><link>http://arxiv.org/abs/2405.09335v1</link><description>Although language models (LMs) have boosted the performance of QuestionAnswering, they still need plenty of data. Data annotation, in contrast, is atime-consuming process. This especially applies to Question Answering, wherepossibly large documents have to be parsed and annotated with questions andtheir corresponding answers. Furthermore, Question Answering models often onlywork well for the domain they were trained on. Since annotation is costly, weargue that domain-agnostic knowledge from LMs, such as linguisticunderstanding, is sufficient to create a well-curated dataset. With thismotivation, we show that using large language models can improve QuestionAnswering performance on various datasets in the few-shot setting compared tostate-of-the-art approaches. For this, we perform data generation leveragingthe Prompting framework, suggesting that language models contain valuabletask-agnostic knowledge that can be used beyond the commonpre-training/fine-tuning scheme. As a result, we consistently outperformprevious approaches on few-shot Question Answering.</description><author>Maximilian Schmidt, Andrea Bartezzaghi, Ngoc Thang Vu</author><pubDate>Wed, 15 May 2024 14:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09335v1</guid></item><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Unsupervised LLM Adaptation for Question Answering</title><link>http://arxiv.org/abs/2402.12170v1</link><description>Large language models (LLM) learn diverse knowledge present in thelarge-scale training dataset via self-supervised training. Followed byinstruction-tuning, LLM acquires the ability to return correct information fordiverse questions. However, adapting these pre-trained LLMs to new targetdomains, such as different organizations or periods, for the question-answering(QA) task incurs a substantial annotation cost. To tackle this challenge, wepropose a novel task, unsupervised LLM adaptation for question answering. Inthis task, we leverage a pre-trained LLM, a publicly available QA dataset(source data), and unlabeled documents from the target domain. Our goal is tolearn LLM that can answer questions about the target domain. We introduce onesynthetic and two real datasets to evaluate models fine-tuned on the source andtarget data, and reveal intriguing insights; (i) fine-tuned models exhibit theability to provide correct answers for questions about the target domain eventhough they do not see any questions about the information described in theunlabeled documents, but (ii) they have difficulties in accessing informationlocated in the middle or at the end of documents, and (iii) this challenge canbe partially mitigated by replacing input tokens with random ones duringadaptation.</description><author>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</author><pubDate>Fri, 16 Feb 2024 06:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12170v1</guid></item><item><title>ArabicaQA: A Comprehensive Dataset for Arabic Question Answering</title><link>http://arxiv.org/abs/2403.17848v1</link><description>In this paper, we address the significant gap in Arabic natural languageprocessing (NLP) resources by introducing ArabicaQA, the first large-scaledataset for machine reading comprehension and open-domain question answering inArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701unanswerable questions created by crowdworkers to look similar to answerableones, along with additional labels of open-domain questions marks a crucialadvancement in Arabic NLP resources. We also present AraDPR, the first densepassage retrieval model trained on the Arabic Wikipedia corpus, specificallydesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,our study includes extensive benchmarking of large language models (LLMs) forArabic question answering, critically evaluating their performance in theArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarkingof LLMs in Arabic question answering offer significant advancements in thefield of Arabic NLP. The dataset and code are publicly accessible for furtherresearch https://github.com/DataScienceUIBK/ArabicaQA.</description><author>Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17848v1</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2404.14464v1</link><description>Multi-hop question answering is a knowledge-intensive complex problem. LargeLanguage Models (LLMs) use their Chain of Thoughts (CoT) capability to reasoncomplex problems step by step, and retrieval-augmentation can effectivelyalleviate factual errors caused by outdated and unknown knowledge in LLMs.Recent works have introduced retrieval-augmentation in the CoT reasoning tosolve multi-hop question answering. However, these chain methods have thefollowing problems: 1) Retrieved irrelevant paragraphs may mislead thereasoning; 2) An error in the chain structure may lead to a cascade of errors. In this paper, we propose a dynamic retrieval framework called Tree ofReviews (ToR), where the root node is the question, and the other nodes areparagraphs from retrieval, extending different reasoning paths from the rootnode to other nodes. Our framework dynamically decides to initiate a newsearch, reject, or accept based on the paragraphs on the reasoning paths.Compared to related work, we introduce a tree structure to handle eachretrieved paragraph separately, alleviating the misleading effect of irrelevantparagraphs on the reasoning path; the diversity of reasoning path extensionreduces the impact of a single reasoning error on the whole. We conductedexperiments on three different multi-hop question answering datasets. Theresults show that compared to the baseline methods, ToR achievesstate-of-the-art performance in both retrieval and response generation. Inaddition, we propose two tree-based search optimization strategies, pruning andeffective expansion, to reduce time overhead and increase the diversity of pathextension. We will release our code.</description><author>Li Jiapeng, Liu Runze, Li Yabo, Zhou Tong, Li Mingling, Chen Xiang</author><pubDate>Mon, 22 Apr 2024 10:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14464v1</guid></item><item><title>ViOCRVQA: Novel Benchmark Dataset and Vision Reader for Visual Question Answering by Understanding Vietnamese Text in Images</title><link>http://arxiv.org/abs/2404.18397v1</link><description>Optical Character Recognition - Visual Question Answering (OCR-VQA) is thetask of answering text information contained in images that have just beensignificantly developed in the English language in recent years. However, thereare limited studies of this task in low-resource languages such as Vietnamese.To this end, we introduce a novel dataset, ViOCRVQA (Vietnamese OpticalCharacter Recognition - Visual Question Answering dataset), consisting of28,000+ images and 120,000+ question-answer pairs. In this dataset, all theimages contain text and questions about the information relevant to the text inthe images. We deploy ideas from state-of-the-art methods proposed for Englishto conduct experiments on our dataset, revealing the challenges anddifficulties inherent in a Vietnamese dataset. Furthermore, we introduce anovel approach, called VisionReader, which achieved 0.4116 in EM and 0.6990 inthe F1-score on the test set. Through the results, we found that the OCR systemplays a very important role in VQA models on the ViOCRVQA dataset. In addition,the objects in the image also play a role in improving model performance. Weopen access to our dataset at link (https://github.com/qhnhynmm/ViOCRVQA.git)for further research in OCR-VQA task in Vietnamese.</description><author>Huy Quang Pham, Thang Kien-Bao Nguyen, Quan Van Nguyen, Dan Quang Tran, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</author><pubDate>Mon, 29 Apr 2024 04:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18397v1</guid></item><item><title>Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts</title><link>http://arxiv.org/abs/2404.08589v1</link><description>Visual question answering (VQA) is known as an AI-complete task as itrequires understanding, reasoning, and inferring about the vision and thelanguage content. Over the past few years, numerous neural architectures havebeen suggested for the VQA problem. However, achieving success in zero-shot VQAremains a challenge due to its requirement for advanced generalization andreasoning skills. This study explores the impact of incorporating imagecaptioning as an intermediary process within the VQA pipeline. Specifically, weexplore the efficacy of utilizing image captions instead of images andleveraging large language models (LLMs) to establish a zero-shot setting. Sinceimage captioning is the most crucial step in this process, we compare theimpact of state-of-the-art image captioning models on VQA performance acrossvarious question types in terms of structure and semantics. We propose astraightforward and efficient question-driven image captioning approach withinthis pipeline to transfer contextual information into the question-answering(QA) model. This method involves extracting keywords from the question,generating a caption for each image-question pair using the keywords, andincorporating the question-driven caption into the LLM prompt. We evaluate theefficacy of using general-purpose and question-driven image captions in the VQApipeline. Our study highlights the potential of employing image captions andharnessing the capabilities of LLMs to achieve competitive performance on GQAunder the zero-shot setting. Our code is available at\url{https://github.com/ovguyo/captions-in-VQA}.</description><author>Övgü Özdemir, Erdem Akagündüz</author><pubDate>Fri, 12 Apr 2024 17:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08589v1</guid></item><item><title>ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages</title><link>http://arxiv.org/abs/2403.17859v2</link><description>Question answering (QA) and Machine Reading Comprehension (MRC) tasks havesignificantly advanced in recent years due to the rapid development of deeplearning techniques and, more recently, large language models. At the sametime, many benchmark datasets have become available for QA and MRC tasks.However, most existing large-scale benchmark datasets have been createdpredominantly using synchronous document collections like Wikipedia or the Web.Archival document collections, such as historical newspapers, contain valuableinformation from the past that is still not widely used to train large languagemodels. To further contribute to advancing QA and MRC tasks and to overcome thelimitation of previous datasets, we introduce ChroniclingAmericaQA, alarge-scale temporal QA dataset with 487K question-answer pairs created basedon the historical newspaper collection Chronicling America. Our dataset isconstructed from a subset of the Chronicling America newspaper collectionspanning 120 years. One of the significant challenges for utilizing digitizedhistorical newspaper collections is the low quality of OCR text. Therefore, toenable realistic testing of QA models, our dataset can be used in threedifferent ways: answering questions from raw and noisy content, answeringquestions from cleaner, corrected version of the content, as well as answeringquestions from scanned images of newspaper pages. This and the fact thatChroniclingAmericaQA spans the longest time period among available QA datasetsmake it quite a unique and useful resource.</description><author>Bhawna Piryani, Jamshid Mozafari, Adam Jatowt</author><pubDate>Fri, 10 May 2024 18:15:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17859v2</guid></item><item><title>Quantum Neural Network with Density Matrix for Question Answering and Classical Image Classification</title><link>http://arxiv.org/abs/2203.11155v4</link><description>Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.</description><author>X. Q. Zhao, T. L. Chen</author><pubDate>Wed, 06 Mar 2024 08:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11155v4</guid></item><item><title>RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions</title><link>http://arxiv.org/abs/2403.10849v1</link><description>State-of-the-art KBQA models assume answerability of questions. Recentresearch has shown that while these can be adapted to detect unaswerabilitywith suitable training and thresholding, this comes at the expense of accuracyfor answerable questions, and no single model is able to handle all categoriesof unanswerability. We propose a new model for KBQA named RetinaQA that isrobust against unaswerability. It complements KB-traversal based logical formretrieval with sketch-filling based logical form construction. This helps withquestions that have valid logical forms but no data paths in the KB leading toan answer. Additionally, it uses discrimination instead of generation to betteridentify questions that do not have valid logical forms. We demonstrate thatRetinaQA significantly outperforms adaptations of state-of-the-art KBQA modelsacross answerable and unanswerable questions, while showing robustness acrossunanswerability categories. Remarkably, it also establishes a new state-of-theart for answerable KBQA by surpassing existing models</description><author>Prayushi Faldu, Indrajit Bhattacharya, Mausam</author><pubDate>Sat, 16 Mar 2024 09:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10849v1</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v3</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Sat, 17 Feb 2024 07:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v3</guid></item><item><title>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</title><link>http://arxiv.org/abs/2404.02022v1</link><description>In the era of large language models, applying techniques such as RetrievalAugmented Generation can better address Open-Domain Question-Answeringproblems. Due to constraints including model sizes and computing resources, thelength of context is often limited, and it becomes challenging to empower themodel to cover overlong contexts while answering questions from open domains.This paper proposes a general and convenient method to covering longer contextsin Open-Domain Question-Answering tasks. It leverages a small encoder languagemodel that effectively encodes contexts, and the encoding appliescross-attention with origin inputs. With our method, the origin language modelscan cover several times longer contexts while keeping the computingrequirements close to the baseline. Our experiments demonstrate that afterfine-tuning, there is improved performance across two held-in datasets, fourheld-out datasets, and also in two In Context Learning settings.</description><author>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</author><pubDate>Tue, 02 Apr 2024 16:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02022v1</guid></item><item><title>JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability</title><link>http://arxiv.org/abs/2402.17887v2</link><description>With the explosive growth of medical data and the rapid development ofartificial intelligence technology, precision medicine has emerged as a key toenhancing the quality and efficiency of healthcare services. In this context,Large Language Models (LLMs) play an increasingly vital role in medicalknowledge acquisition and question-answering systems. To further improve theperformance of these systems in the medical domain, we introduce an innovativemethod that jointly trains an Information Retrieval (IR) system and an LLMduring the fine-tuning phase. This approach, which we call Joint Medical LLMand Retrieval Training (JMLR), is designed to overcome the challenges faced bytraditional models in handling medical question-answering tasks. By employing asynchronized training mechanism, JMLR reduces the demand for computationalresources and enhances the model's ability to leverage medical knowledge forreasoning and answering questions. Our experimental results demonstrate thatJMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models usingconventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3%on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% onMedQA) significantly outperforms other public models (Meditron-7B: 50.1%,47.9%), proving its superiority in terms of cost (our training time: 37 hours,traditional method: 144 hours), efficiency, and effectiveness in medicalquestion-answering tasks. Through this work, we provide a new and efficientknowledge enhancement tool for healthcare, demonstrating the great potential ofintegrating IR and LLM training in precision medical information retrieval andquestion-answering systems.</description><author>Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu</author><pubDate>Sat, 02 Mar 2024 09:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17887v2</guid></item><item><title>Self-Improvement Programming for Temporal Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2404.01720v1</link><description>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questionswith temporal intent over Temporal Knowledge Graphs (TKGs). The core challengeof this task lies in understanding the complex semantic information regardingmultiple types of time constraints (e.g., before, first) in questions. Existingend-to-end methods implicitly model the time constraints by learning time-awareembeddings of questions and candidate answers, which is far from understandingthe question comprehensively. Motivated by semantic-parsing-based approachesthat explicitly model constraints in questions by generating logical forms withsymbolic operators, we design fundamental temporal operators for timeconstraints and introduce a novel self-improvement Programming method for TKGQA(Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability ofLarge Language Models (LLMs) to understand the combinatory time constraints inthe questions and generate corresponding program drafts with a few examplesgiven. Then, it aligns these drafts to TKGs with the linking module andsubsequently executes them to generate the answers. To enhance the ability tounderstand questions, Prog-TQA is further equipped with a self-improvementstrategy to effectively bootstrap LLMs using high-quality self-generateddrafts. Extensive experiments demonstrate the superiority of the proposedProg-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1metric.</description><author>Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin, Yongjun Xu</author><pubDate>Tue, 02 Apr 2024 09:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01720v1</guid></item><item><title>Causal Question Answering with Reinforcement Learning</title><link>http://arxiv.org/abs/2311.02760v2</link><description>Causal questions inquire about causal relationships between different eventsor phenomena. They are important for a variety of use cases, including virtualassistants and search engines. However, many current approaches to causalquestion answering cannot provide explanations or evidence for their answers.Hence, in this paper, we aim to answer causal questions with a causality graph,a large-scale dataset of causal relations between noun phrases along with therelations' provenance data. Inspired by recent, successful applications ofreinforcement learning to knowledge graph tasks, such as link prediction andfact-checking, we explore the application of reinforcement learning on acausality graph for causal question answering. We introduce anActor-Critic-based agent which learns to search through the graph to answercausal questions. We bootstrap the agent with a supervised learning procedureto deal with large action spaces and sparse rewards. Our evaluation shows thatthe agent successfully prunes the search space to answer binary causalquestions by visiting less than 30 nodes per question compared to over 3,000nodes by a naive breadth-first search. Our ablation study indicates that oursupervised learning strategy provides a strong foundation upon which ourreinforcement learning agent improves. The paths returned by our agent explainthe mechanisms by which a cause produces an effect. Moreover, for each edge ona path, our causality graph provides its original source allowing for easyverification of paths.</description><author>Lukas Blübaum, Stefan Heindorf</author><pubDate>Mon, 25 Mar 2024 09:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02760v2</guid></item><item><title>EEE-QA: Exploring Effective and Efficient Question-Answer Representations</title><link>http://arxiv.org/abs/2403.02176v1</link><description>Current approaches to question answering rely on pre-trained language models(PLMs) like RoBERTa. This work challenges the existing question-answer encodingconvention and explores finer representations. We begin with testing variouspooling methods compared to using the begin-of-sentence token as a questionrepresentation for better quality. Next, we explore opportunities tosimultaneously embed all answer candidates with the question. This enablescross-reference between answer choices and improves inference throughput viareduced memory usage. Despite their simplicity and effectiveness, these methodshave yet to be widely studied in current frameworks. We experiment withdifferent PLMs, and with and without the integration of knowledge graphs.Results prove that the memory efficacy of the proposed techniques with littlesacrifice in performance. Practically, our work enhances 38-100% throughputwith 26-65% speedups on consumer-grade GPUs by allowing for considerably largerbatch sizes. Our work sends a message to the community with promisingdirections in both representation quality and efficiency for thequestion-answering task in natural language processing.</description><author>Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen</author><pubDate>Mon, 04 Mar 2024 16:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02176v1</guid></item><item><title>CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</title><link>http://arxiv.org/abs/2403.03203v1</link><description>The integration of learning and reasoning is high on the research agenda inAI. Nevertheless, there is only a little attention to use existing backgroundknowledge for reasoning about partially observed scenes to answer questionsabout the scene. Yet, we as humans use such knowledge frequently to inferplausible answers to visual questions (by eliminating all inconsistent ones).Such knowledge often comes in the form of constraints about objects and ittends to be highly domain or environment-specific. We contribute a novelbenchmark called CLEVR-POC for reasoning-intensive visual question answering(VQA) in partially observable environments under constraints. In CLEVR-POC,knowledge in the form of logical constraints needs to be leveraged to generateplausible answers to questions about a hidden object in a given partial scene.For instance, if one has the knowledge that all cups are colored either red,green or blue and that there is only one green cup, it becomes possible todeduce the color of an occluded cup as either red or blue, provided that allother cups, including the green one, are observed. Through experiments, weobserve that the low performance of pre-trained vision language models likeCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POCascertains the necessity for frameworks that can handle reasoning-intensivetasks where environment-specific background knowledge is available and crucial.Furthermore, our demonstration illustrates that a neuro-symbolic model, whichintegrates an LLM like GPT-4 with a visual perception network and a formallogical reasoner, exhibits exceptional performance on CLEVR-POC.</description><author>Savitha Sam Abraham, Marjan Alirezaie, Luc De Raedt</author><pubDate>Tue, 05 Mar 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03203v1</guid></item><item><title>TraveLER: A Multi-LMM Agent Framework for Video Question-Answering</title><link>http://arxiv.org/abs/2404.01476v1</link><description>Recently, Large Multimodal Models (LMMs) have made significant progress invideo question-answering using a frame-wise approach by leveraging large-scale,image-based pretraining in a zero-shot manner. While image-based methods forvideos have shown impressive performance, a current limitation is that theyoften overlook how key timestamps are selected and cannot adjust when incorrecttimestamps are identified. Moreover, they are unable to extract detailsrelevant to the question, instead providing general descriptions of the frame.To overcome this, we design a multi-LMM agent framework that travels along thevideo, iteratively collecting relevant information from keyframes throughinteractive question-asking until there is sufficient information to answer thequestion. Specifically, we propose TraveLER, a model that can create a plan to"Traverse" through the video, ask questions about individual frames to "Locate"and store key information, and then "Evaluate" if there is enough informationto answer the question. Finally, if there is not enough information, our methodis able to "Replan" based on its collected knowledge. Through extensiveexperiments, we find that the proposed TraveLER approach improves performanceon several video question-answering benchmarks, such as NExT-QA, STAR, andPerception Test, without the need to fine-tune on specific datasets.</description><author>Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig</author><pubDate>Mon, 01 Apr 2024 21:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01476v1</guid></item><item><title>Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models</title><link>http://arxiv.org/abs/2406.09994v1</link><description>In the realm of multimodal tasks, Visual Question Answering (VQA) plays acrucial role by addressing natural language questions grounded in visualcontent. Knowledge-Based Visual Question Answering (KBVQA) advances thisconcept by adding external knowledge along with images to respond to questions.We introduce an approach for KBVQA, augmenting the existing vision-languagetransformer encoder-decoder (OFA) model. Our main contribution involvesenhancing questions by incorporating relevant external knowledge extracted fromknowledge graphs, using a dynamic triple extraction method. We supply aflexible number of triples from the knowledge graph as context, tailored tomeet the requirements for answering the question. Our model, enriched withknowledge, demonstrates an average improvement of 4.75\% in Exact Match Scoreover the state-of-the-art on three different KBVQA datasets. Throughexperiments and analysis, we demonstrate that furnishing variable triples foreach question improves the reasoning capabilities of the language model incontrast to supplying a fixed number of triples. This is illustrated even forrecent large language models. Additionally, we highlight the model'sgeneralization capability by showcasing its SOTA-beating performance on a smalldataset, achieved through straightforward fine-tuning.</description><author>Manas Jhalani, Annervaz K M, Pushpak Bhattacharyya</author><pubDate>Fri, 14 Jun 2024 14:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09994v1</guid></item><item><title>Efficient Medical Question Answering with Knowledge-Augmented Question Generation</title><link>http://arxiv.org/abs/2405.14654v1</link><description>In the expanding field of language model applications, medical knowledgerepresentation remains a significant challenge due to the specialized nature ofthe domain. Large language models, such as GPT-4, obtain reasonable scores onmedical question answering tasks, but smaller models are far behind. In thiswork, we introduce a method to improve the proficiency of a small languagemodel in the medical domain by employing a two-fold approach. We firstfine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 togenerate questions similar to the downstream task, prompted with textbookknowledge, and use them to fine-tune the model. Additionally, we introduceECN-QA, a novel medical question answering dataset containing ``progressivequestions'' composed of related sequential questions. We show the benefits ofour training strategy on this dataset. The study's findings highlight thepotential of small language models in the medical domain when appropriatelyfine-tuned. The code and weights are available athttps://github.com/raidium-med/MQG.</description><author>Julien Khlaut, Corentin Dancette, Elodie Ferreres, Alaedine Bennani, Paul Hérent, Pierre Manceron</author><pubDate>Thu, 23 May 2024 15:53:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14654v1</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v3</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Fri, 01 Mar 2024 15:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v3</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v2</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Tue, 20 Feb 2024 19:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v2</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Multi-Image Visual Question Answering for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2404.07622v1</link><description>Unsupervised anomaly detection enables the identification of potentialpathological areas by juxtaposing original images with their pseudo-healthyreconstructions generated by models trained exclusively on normal images.However, the clinical interpretation of resultant anomaly maps presents achallenge due to a lack of detailed, understandable explanations. Recentadvancements in language models have shown the capability of mimickinghuman-like understanding and providing detailed descriptions. This raises aninteresting question: \textit{How can language models be employed to make theanomaly maps more explainable?} To the best of our knowledge, we are the firstto leverage a language model for unsupervised anomaly detection, for which weconstruct a dataset with different questions and answers. Additionally, wepresent a novel multi-image visual question answering framework tailored foranomaly detection, incorporating diverse feature fusion strategies to enhancevisual knowledge extraction. Our experiments reveal that the framework,augmented by our new Knowledge Q-Former module, adeptly answers questions onthe anomaly detection dataset. Besides, integrating anomaly maps as inputsdistinctly aids in improving the detection of unseen pathologies.</description><author>Jun Li, Cosmin I. Bercea, Philip Müller, Lina Felsner, Suhwan Kim, Daniel Rueckert, Benedikt Wiestler, Julia A. Schnabel</author><pubDate>Thu, 11 Apr 2024 11:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07622v1</guid></item><item><title>Question Answering Over Spatio-Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2402.11542v1</link><description>Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledgegraphs (KGs) by incorporating time and location information. While the researchcommunity's focus on Knowledge Graph Question Answering (KGQA), the field ofanswering questions incorporating both spatio-temporal information based onSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasetsalso has hindered progress in this area. To address this issue, we presentSTQAD, a dataset comprising 10,000 natural language questions forspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,various state-of-the-art KGQA approaches fall far short of achievingsatisfactory performance on our dataset. In response, we propose STCQA, a newspatio-temporal KGQA approach that utilizes a novel STKG embedding method namedSTComplEx. By extracting temporal and spatial information from a question, ourQA model can better comprehend the question and retrieve accurate answers fromthe STKG. Through extensive experiments, we demonstrate the quality of ourdataset and the effectiveness of our STKGQA method.</description><author>Xinbang Dai, Huiying Li, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11542v1</guid></item><item><title>CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark</title><link>http://arxiv.org/abs/2406.05967v1</link><description>Visual Question Answering (VQA) is an important task in multimodal AI, and itis often used to test the ability of vision-language models to understand andreason on knowledge present in both visual and textual data. However, most ofthe current VQA models use datasets that are primarily focused on English and afew major world languages, with images that are typically Western-centric.While recent efforts have tried to increase the number of languages covered onVQA datasets, they still lack diversity in low-resource languages. Moreimportantly, although these datasets often extend their linguistic range viatranslation or some other approaches, they usually keep images the same,resulting in narrow cultural representation. To address these limitations, weconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answeringbenchmark, designed to cover a rich set of languages and cultures, where weengage native speakers and cultural experts in the data collection process. Asa result, CVQA includes culturally-driven images and questions from across 28countries on four continents, covering 26 languages with 11 scripts, providinga total of 9k questions. We then benchmark several Multimodal Large LanguageModels (MLLMs) on CVQA, and show that the dataset is challenging for thecurrent state-of-the-art models. This benchmark can serve as a probingevaluation suite for assessing the cultural capability and bias of multimodalmodels and hopefully encourage more research efforts toward increasing culturalawareness and linguistic diversity in this field.</description><author>David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mari</author><pubDate>Mon, 10 Jun 2024 02:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05967v1</guid></item><item><title>Conversational Question Answering with Reformulations over Knowledge Graph</title><link>http://arxiv.org/abs/2312.17269v2</link><description>Conversational question answering (convQA) over knowledge graphs (KGs)involves answering multi-turn natural language questions about informationcontained in a KG. State-of-the-art methods of ConvQA often struggle withinexplicit question-answer pairs. These inputs are easy for human beings tounderstand given a conversation history, but hard for a machine to interpret,which can degrade ConvQA performance. To address this problem, we propose areinforcement learning (RL) based model, CornNet, which utilizes questionreformulations generated by large language models (LLMs) to improve ConvQAperformance. CornNet adopts a teacher-student architecture where a teachermodel learns question representations using human writing reformulations, and astudent model to mimic the teacher model's output via reformulations generatedby LLMs. The learned question representation is then used by an RL model tolocate the correct answer in a KG. Extensive experimental results show thatCornNet outperforms state-of-the-art convQA models.</description><author>Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, Hanghang Tong</author><pubDate>Fri, 29 Mar 2024 07:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17269v2</guid></item><item><title>EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings</title><link>http://arxiv.org/abs/2402.16040v1</link><description>This study introduces EHRNoteQA, a novel patient-specific question answeringbenchmark tailored for evaluating Large Language Models (LLMs) in clinicalenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of threemedical professionals has curated the dataset comprising 962 unique questions,each linked to a specific patient's EHR clinical notes. What makes EHRNoteQAdistinct from existing EHR-based benchmarks is as follows: Firstly, it is thefirst dataset to adopt a multi-choice question answering format, a designchoice that effectively evaluates LLMs with reliable scores in the context ofautomatic evaluation, compared to other formats. Secondly, it requires ananalysis of multiple clinical notes to answer a single question, reflecting thecomplex nature of real-world clinical decision-making where clinicians reviewextensive records of patient histories. Our comprehensive evaluation on variouslarge language models showed that their scores on EHRNoteQA correlate moreclosely with their performance in addressing real-world medical questionsevaluated by clinicians than their scores from other LLM benchmarks. Thisunderscores the significance of EHRNoteQA in evaluating LLMs for medicalapplications and highlights its crucial role in facilitating the integration ofLLMs into healthcare systems. The dataset will be made available to the publicunder PhysioNet credential access, promoting further research in this vitalfield.</description><author>Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Seunghyun Won, Edward Choi</author><pubDate>Sun, 25 Feb 2024 09:41:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16040v1</guid></item><item><title>TANQ: An open domain dataset of table answered questions</title><link>http://arxiv.org/abs/2405.07765v1</link><description>Language models, potentially augmented with tool usage such as retrieval arebecoming the go-to means of answering questions. Understanding and answeringquestions in real-world settings often requires retrieving information fromdifferent sources, processing and aggregating data to extract insights, andpresenting complex findings in form of structured artifacts such as noveltables, charts, or infographics. In this paper, we introduce TANQ, the firstopen domain question answering dataset where the answers require buildingtables from information across multiple sources. We release the full sourceattribution for every cell in the resulting table and benchmarkstate-of-the-art language models in open, oracle, and closed book setups. Ourbest-performing baseline, GPT4 reaches an overall F1 score of 29.1, laggingbehind human performance by 19.7 points. We analyse baselines' performanceacross different dataset attributes such as different skills required for thistask, including multi-hop reasoning, math operations, and unit conversions. Wefurther discuss common failures in model-generated answers, suggesting thatTANQ is a complex task with many challenges ahead.</description><author>Mubashara Akhtar, Chenxi Pang, Andreea Marzoca, Yasemin Altun, Julian Martin Eisenschlos</author><pubDate>Mon, 13 May 2024 15:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07765v1</guid></item><item><title>ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering</title><link>http://arxiv.org/abs/2406.04866v1</link><description>We introduce ComplexTempQA,a large-scale dataset consisting of over 100million question-answer pairs designed to tackle the challenges in temporalquestion answering. ComplexTempQA significantly surpasses existing benchmarkslike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data fromWikipedia and Wikidata, the dataset covers questions spanning over two decadesand offers an unmatched breadth of topics. We introduce a unique taxonomy thatcategorizes questions as attributes, comparisons, and counting questions, eachrevolving around events, entities, and time periods. One standout feature ofComplexTempQA is the high complexity of its questions, which demand effectivecapabilities for answering such as across-time comparison, temporalaggregation, and multi-hop reasoning involving temporal event ordering andentity recognition. Additionally, each question is accompanied by detailedmetadata, including specific time scopes, allowing for comprehensive evaluationand enhancement of the temporal reasoning abilities of large language models.ComplexTempQA serves both as a testing ground for developing sophisticated AImodels and as a foundation for advancing research in question answering,information retrieval, and language understanding. Dataset and code are freelyavailable at: https://github.com/DataScienceUIBK/ComplexTempQA.</description><author>Raphael Gruber, Abdelrahman Abdallah, Michael Färber, Adam Jatowt</author><pubDate>Fri, 07 Jun 2024 13:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04866v1</guid></item><item><title>Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval</title><link>http://arxiv.org/abs/2404.08359v1</link><description>In today's digital world, seeking answers to health questions on the Internetis a common practice. However, existing question answering (QA) systems oftenrely on using pre-selected and annotated evidence documents, thus making theminadequate for addressing novel questions. Our study focuses on the open-domainQA setting, where the key challenge is to first uncover relevant evidence inlarge knowledge bases. By utilizing the common retrieve-then-read QA pipelineand PubMed as a trustworthy collection of medical research documents, we answerhealth questions from three diverse datasets. We modify different retrievalsettings to observe their influence on the QA pipeline's performance, includingthe number of retrieved documents, sentence selection process, the publicationyear of articles, and their number of citations. Our results reveal thatcutting down on the amount of retrieved documents and favoring more recent andhighly cited documents can improve the final macro F1 score up to 10%. Wediscuss the results, highlight interesting examples, and outline challenges forfuture research, like managing evidence disagreement and crafting user-friendlyexplanations.</description><author>Juraj Vladika, Florian Matthes</author><pubDate>Fri, 12 Apr 2024 10:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08359v1</guid></item><item><title>MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering</title><link>http://arxiv.org/abs/2405.11985v1</link><description>Text-Centric Visual Question Answering (TEC-VQA) in its proper format notonly facilitates human-machine interaction in text-centric visual environmentsbut also serves as a de facto gold proxy to evaluate AI models in the domain oftext-centric scene understanding. However, most TEC-VQA benchmarks have focusedon high-resource languages like English and Chinese. Despite pioneering worksto expand multilingual QA pairs in non-text-centric VQA datasets usingtranslation engines, the translation-based protocol encounters a substantial``Visual-textual misalignment'' problem when applied to TEC-VQA. Specifically,it prioritizes the text in question-answer pairs while disregarding the visualtext present in images. Furthermore, it does not adequately tackle challengesrelated to nuanced meaning, contextual distortion, language bias, andquestion-type diversity. In this work, we address the task of multilingualTEC-VQA and provide a benchmark with high-quality human expert annotations in 9diverse languages, called MTVQA. To our knowledge, MTVQA is the firstmultilingual TEC-VQA benchmark to provide human expert annotations fortext-centric scenarios. Further, by evaluating several state-of-the-artMultimodal Large Language Models (MLLMs), including GPT-4V, on our MTVQAdataset, it is evident that there is still room for performance improvement,underscoring the value of our dataset. We hope this dataset will provideresearchers with fresh perspectives and inspiration within the community. TheMTVQA dataset will be available athttps://huggingface.co/datasets/ByteDance/MTVQA.</description><author>Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, Can Huang</author><pubDate>Mon, 20 May 2024 13:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11985v1</guid></item><item><title>On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension</title><link>http://arxiv.org/abs/2404.02800v1</link><description>Question Generation aims to automatically generate questions based on a giveninput provided as context. A controllable question generation scheme focuses ongenerating questions with specific attributes, allowing better control. In thisstudy, we propose a few-shot prompting strategy for controlling the generationof question-answer pairs from children's narrative texts. We aim to control twoattributes: the question's explicitness and underlying narrative elements. Withempirical evaluation, we show the effectiveness of controlling the generationprocess by employing few-shot prompting side by side with a reference model.Our experiments highlight instances where the few-shot strategy surpasses thereference model, particularly in scenarios such as semantic closenessevaluation and the diversity and coherency of question-answer pairs. However,these improvements are not always statistically significant. The code ispublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.</description><author>Bernardo Leite, Henrique Lopes Cardoso</author><pubDate>Wed, 03 Apr 2024 16:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02800v1</guid></item><item><title>Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering</title><link>http://arxiv.org/abs/2403.09288v1</link><description>Scene-Text Visual Question Answering (ST-VQA) aims to understand scene textin images and answer questions related to the text content. Most existingmethods heavily rely on the accuracy of Optical Character Recognition (OCR)systems, and aggressive fine-tuning based on limited spatial locationinformation and erroneous OCR text information often leads to inevitableoverfitting. In this paper, we propose a multimodal adversarial trainingarchitecture with spatial awareness capabilities. Specifically, we introduce anAdversarial OCR Enhancement (AOE) module, which leverages adversarial trainingin the embedding space of OCR modality to enhance fault-tolerant representationof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, Weadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model bettercapture the spatial relationships among OCR tokens. Various experimentsdemonstrate that our method achieves significant performance improvements onboth the ST-VQA and TextVQA datasets and provides a novel paradigm formultimodal adversarial training.</description><author>Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li</author><pubDate>Thu, 14 Mar 2024 12:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09288v1</guid></item><item><title>PitVQA: Image-grounded Text Embedding LLM for Visual Question Answering in Pituitary Surgery</title><link>http://arxiv.org/abs/2405.13949v1</link><description>Visual Question Answering (VQA) within the surgical domain, utilizing LargeLanguage Models (LLMs), offers a distinct opportunity to improveintra-operative decision-making and facilitate intuitive surgeon-AIinteraction. However, the development of LLMs for surgical VQA is hindered bythe scarcity of diverse and extensive datasets with complex reasoning tasks.Moreover, contextual fusion of the image and text modalities remains an openresearch challenge due to the inherent differences between these two types ofinformation and the complexity involved in aligning them. This paper introducesPitVQA, a novel dataset specifically designed for VQA in endonasal pituitarysurgery and PitVQA-Net, an adaptation of the GPT2 with a novel image-groundedtext embedding for surgical VQA. PitVQA comprises 25 procedural videos and arich collection of question-answer pairs spanning crucial surgical aspects suchas phase and step recognition, context understanding, tool detection andlocalization, and tool-tissue interactions. PitVQA-Net consists of a novelimage-grounded text embedding that projects image and text features into ashared embedding space and GPT2 Backbone with an excitation blockclassification head to generate contextually relevant answers within thecomplex domain of endonasal pituitary surgery. Our image-grounded textembedding leverages joint embedding, cross-attention and contextualrepresentation to understand the contextual relationship between questions andsurgical images. We demonstrate the effectiveness of PitVQA-Net on both thePitVQA and the publicly available EndoVis18-VQA dataset, achieving improvementsin balanced accuracy of 8% and 9% over the most recent baselines, respectively.Our code and dataset is available at https://github.com/mobarakol/PitVQA.</description><author>Runlong He, Mengya Xu, Adrito Das, Danyal Z. Khan, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarakol Islam</author><pubDate>Wed, 22 May 2024 20:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13949v1</guid></item><item><title>Multi-hop Question Answering</title><link>http://arxiv.org/abs/2204.09140v2</link><description>The task of Question Answering (QA) has attracted significant researchinterest for long. Its relevance to language understanding and knowledgeretrieval tasks, along with the simple setting makes the task of QA crucial forstrong AI systems. Recent success on simple QA tasks has shifted the focus tomore complex settings. Among these, Multi-Hop QA (MHQA) is one of the mostresearched tasks over the recent years. In broad terms, MHQA is the task ofanswering natural language questions that involve extracting and combiningmultiple pieces of information and doing multiple steps of reasoning. Anexample of a multi-hop question would be "The Argentine PGA Championship recordholder has won how many tournaments worldwide?". Answering the question wouldneed two pieces of information: "Who is the record holder for Argentine PGAChampionship tournaments?" and "How many tournaments did [Answer of Sub Q1]win?". The ability to answer multi-hop questions and perform multi stepreasoning can significantly improve the utility of NLP systems. Consequently,the field has seen a surge with high quality datasets, models and evaluationstrategies. The notion of 'multiple hops' is somewhat abstract which results ina large variety of tasks that require multi-hop reasoning. This leads todifferent datasets and models that differ significantly from each other andmakes the field challenging to generalize and survey. We aim to provide ageneral and formal definition of the MHQA task, and organize and summarizeexisting MHQA frameworks. We also outline some best practices for building MHQAdatasets. This book provides a systematic and thorough introduction as well asthe structuring of the existing attempts to this highly interesting, yet quitechallenging task.</description><author>Vaibhav Mavi, Anubhav Jangra, Adam Jatowt</author><pubDate>Fri, 31 May 2024 15:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.09140v2</guid></item><item><title>PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title><link>http://arxiv.org/abs/2402.11034v1</link><description>Existing work on Temporal Question Answering (TQA) has predominantly focusedon questions anchored to specific timestamps or events (e.g. "Who was the USpresident in 1970?"). Little work has studied questions whose temporal contextis relative to the present time (e.g. "Who was the previous US president?"). Werefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA posesunique challenges: (1) large language models (LLMs) may have outdatedknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') arehard to reason, (3) multi-hop reasoning may be required, and (4) the goldanswers of benchmarks must be continuously updated. To address thesechallenges, we introduce the PAT-Questions benchmark, which includes single andmulti-hop temporal questions. The answers in PAT-Questions can be automaticallyrefreshed by re-running SPARQL queries on a knowledge graph, if available. Weevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model(TEMPREASON-T5) on PAT-Questions through direct prompting andretrieval-augmented generation (RAG). The results highlight the limitations ofexisting solutions in PATQA and motivate the need for new methods to improvePATQA reasoning capabilities.</description><author>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</author><pubDate>Fri, 16 Feb 2024 19:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11034v1</guid></item><item><title>VideoDistill: Language-aware Vision Distillation for Video Question Answering</title><link>http://arxiv.org/abs/2404.00973v1</link><description>Significant advancements in video question answering (VideoQA) have been madethanks to thriving large image-language pretraining frameworks. Although theseimage-language models can efficiently represent both video and languagebranches, they typically employ a goal-free vision perception process and donot interact vision with language well during the answer generation, thusomitting crucial visual cues. In this paper, we are inspired by the humanrecognition and learning pattern and propose VideoDistill, a framework withlanguage-aware (i.e., goal-driven) behavior in both vision perception andanswer generation process. VideoDistill generates answers only fromquestion-related visual embeddings and follows a thinking-observing-answeringapproach that closely resembles human behavior, distinguishing it from previousresearch. Specifically, we develop a language-aware gating mechanism to replacethe standard cross-attention, avoiding language's direct fusion into visualrepresentations. We incorporate this mechanism into two key components of theentire framework. The first component is a differentiable sparse samplingmodule, which selects frames containing the necessary dynamics and semanticsrelevant to the questions. The second component is a vision refinement modulethat merges existing spatial-temporal attention layers to ensure the extractionof multi-grained visual semantics associated with the questions. We conductexperimental evaluations on various challenging video question-answeringbenchmarks, and VideoDistill achieves state-of-the-art performance in bothgeneral and long-form VideoQA datasets. In Addition, we verify thatVideoDistill can effectively alleviate the utilization of language shortcutsolutions in the EgoTaskQA dataset.</description><author>Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao</author><pubDate>Mon, 01 Apr 2024 08:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00973v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>MedREQAL: Examining Medical Knowledge Recall of Large Language Models via Question Answering</title><link>http://arxiv.org/abs/2406.05845v1</link><description>In recent years, Large Language Models (LLMs) have demonstrated an impressiveability to encode knowledge during pre-training on large text corpora. They canleverage this knowledge for downstream tasks like question answering (QA), evenin complex areas involving health topics. Considering their high potential forfacilitating clinical work in the future, understanding the quality of encodedmedical knowledge and its recall in LLMs is an important step forward. In thisstudy, we examine the capability of LLMs to exhibit medical knowledge recall byconstructing a novel dataset derived from systematic reviews -- studiessynthesizing evidence-based answers for specific medical questions. Throughexperiments on the new MedREQAL dataset, comprising question-answer pairsextracted from rigorous systematic reviews, we assess six LLMs, such as GPT andMixtral, analyzing their classification and generation performance. Ourexperimental insights into LLM performance on the novel biomedical QA datasetreveal the still challenging nature of this task.</description><author>Juraj Vladika, Phillip Schneider, Florian Matthes</author><pubDate>Sun, 09 Jun 2024 17:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05845v1</guid></item><item><title>Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering</title><link>http://arxiv.org/abs/2403.11129v1</link><description>Document-level Event Causality Identification (DECI) aims to identify causalrelations between two events in documents. Recent research tends to usepre-trained language models to generate the event causal relations. Whereas,these methods are prone to the errors of sequential generation due to multipleevents in a document. Moreover, the potential structures such as eventcoreference and related causal chain are neglected. In this paper, we propose amulti-task learning framework to enhance event causality identification withrationale and structure-aware causal question answering. Specifically, the DECItask is transformed into multiple-choice question answering, and the causes andeffects of the questioned event are generated with large language models. Inaddition, we generate the rationales to explain why these events have causalrelations. Moreover, we construct an event structure graph, which models themulti-hop potential relations for causal reasoning of the current event.Experiments on two benchmark datasets show the great advantages of our proposedapproach compared to the state-of-the-art methods. Moreover, we conduct bothquantitative and qualitative analyses, which shed light on why each componentof our approach can lead to great improvements.</description><author>Baiyan Zhang, Qin Chen, Jie Zhou, Jian Jin, Liang He</author><pubDate>Sun, 17 Mar 2024 08:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11129v1</guid></item><item><title>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</title><link>http://arxiv.org/abs/2402.13188v1</link><description>Many models that leverage knowledge graphs (KGs) have recently demonstratedremarkable success in question answering (QA) tasks. In the real world, manyfacts contained in KGs are time-constrained thus temporal KGQA has receivedincreasing attention. Despite the fruitful efforts of previous models intemporal KGQA, they still have several limitations. (I) They adopt pre-trainedlanguage models (PLMs) to obtain question representations, while PLMs tend tofocus on entity information and ignore entity transfer caused by temporalconstraints, and finally fail to learn specific temporal representations ofentities. (II) They neither emphasize the graph structure between entities norexplicitly model the multi-hop relationship in the graph, which will make itdifficult to solve complex multi-hop question answering. To alleviate thisproblem, we propose a novel Question Calibration and Multi-Hop Modeling(QC-MHM) approach. Specifically, We first calibrate the question representationby fusing the question and the time-constrained concepts in KG. Then, weconstruct the GNN layer to complete multi-hop message passing. Finally, thequestion representation is combined with the embedding output by the GNN togenerate the final prediction. Empirical results verify that the proposed modelachieves better performance than the state-of-the-art models in the benchmarkdataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestionsdataset's complex questions are absolutely improved by 5.1% and 1.2% comparedto the best-performing baseline. Moreover, QC-MHM can generate interpretableand trustworthy predictions.</description><author>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang</author><pubDate>Tue, 20 Feb 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13188v1</guid></item><item><title>Explore until Confident: Efficient Exploration for Embodied Question Answering</title><link>http://arxiv.org/abs/2403.15941v2</link><description>We consider the problem of Embodied Question Answering (EQA), which refers tosettings where an embodied agent such as a robot needs to actively explore anenvironment to gather information until it is confident about the answer to aquestion. In this work, we leverage the strong semantic reasoning capabilitiesof large vision-language models (VLMs) to efficiently explore and answer suchquestions. However, there are two main challenges when using VLMs in EQA: theydo not have an internal memory for mapping the scene to be able to plan how toexplore over time, and their confidence can be miscalibrated and can cause therobot to prematurely stop exploration or over-explore. We propose a method thatfirst builds a semantic map of the scene based on depth information and viavisual prompting of a VLM - leveraging its vast knowledge of relevant regionsof the scene for exploration. Next, we use conformal prediction to calibratethe VLM's question answering confidence, allowing the robot to know when tostop exploration - leading to a more calibrated and efficient explorationstrategy. To test our framework in simulation, we also contribute a new EQAdataset with diverse, realistic human-robot scenarios and scenes built upon theHabitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robotexperiments show our proposed approach improves the performance and efficiencyover baselines that do no leverage VLM for exploration or do not calibrate itsconfidence. Webpage with experiment videos and code:https://explore-eqa.github.io/</description><author>Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh</author><pubDate>Mon, 27 May 2024 04:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15941v2</guid></item><item><title>Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check</title><link>http://arxiv.org/abs/2403.18243v1</link><description>Retrieval-Augmented Generation (RAG) aims to generate more reliable andaccurate responses, by augmenting large language models (LLMs) with theexternal vast and dynamic knowledge. Most previous work focuses on using RAGfor single-round question answering, while how to adapt RAG to the complexconversational setting wherein the question is interdependent on the precedingcontext is not well studied. In this paper, we propose a conversation-level RAGapproach, which incorporates fine-grained retrieval augmentation and self-checkfor conversational question answering (CQA). In particular, our approachconsists of three components, namely conversational question refiner,fine-grained retriever and self-check based response generator, which workcollaboratively for question understanding and relevant information acquisitionin conversational settings. Extensive experiments demonstrate the greatadvantages of our approach over the state-of-the-art baselines. Moreover, wealso release a Chinese CQA dataset with new features including reformulatedquestion, extracted keyword, retrieved paragraphs and their helpfulness, whichfacilitates further researches in RAG enhanced CQA.</description><author>Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He</author><pubDate>Wed, 27 Mar 2024 05:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18243v1</guid></item><item><title>UQA: Corpus for Urdu Question Answering</title><link>http://arxiv.org/abs/2405.01458v1</link><description>This paper introduces UQA, a novel dataset for question answering and textcomprehension in Urdu, a low-resource language with over 70 million nativespeakers. UQA is generated by translating the Stanford Question AnsweringDataset (SQuAD2.0), a large-scale English QA dataset, using a technique calledEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans inthe translated context paragraphs. The paper describes the process of selectingand evaluating the best translation model among two candidates: GoogleTranslator and Seamless M4T. The paper also benchmarks several state-of-the-artmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, andreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and74.56 EM. UQA is a valuable resource for developing and testing multilingualNLP systems for Urdu and for enhancing the cross-lingual transferability ofexisting models. Further, the paper demonstrates the effectiveness of EATS forcreating high-quality datasets for other languages and domains. The UQA datasetand the code are publicly available at www.github.com/sameearif/UQA.</description><author>Samee Arif, Sualeha Farid, Awais Athar, Agha Ali Raza</author><pubDate>Thu, 02 May 2024 17:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01458v1</guid></item><item><title>ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search</title><link>http://arxiv.org/abs/2403.16702v1</link><description>Retrieval-based code question answering seeks to match user queries innatural language to relevant code snippets. Previous approaches typically relyon pretraining models using crafted bi-modal and uni-modal datasets to aligntext and code representations. In this paper, we introduce ProCQA, alarge-scale programming question answering dataset extracted from theStackOverflow community, offering naturally structured mixed-modal QA pairs. Tovalidate its effectiveness, we propose a modality-agnostic contrastivepre-training approach to improve the alignment of text and code representationsof current code language models. Compared to previous models that primarilyemploy bimodal and unimodal pairs extracted from CodeSearchNet forpre-training, our model exhibits significant performance improvements across awide range of code retrieval benchmarks.</description><author>Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong</author><pubDate>Mon, 25 Mar 2024 13:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16702v1</guid></item><item><title>MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale</title><link>http://arxiv.org/abs/2404.12372v1</link><description>Medical Visual Question Answering (MedVQA), which offers language responsesto image-based medical inquiries, represents a challenging task and significantadvancement in healthcare. It assists medical experts to swiftly interpretmedical images, thereby enabling faster and more accurate diagnoses. However,the model interpretability and transparency of existing MedVQA solutions areoften limited, posing challenges in understanding their decision-makingprocesses. To address this issue, we devise a semi-automated annotation processto streamlining data preparation and build new benchmark MedVQA datasets R-RADand R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medicaldecision-making rationales generated by multimodal large language models andhuman annotations for question-answering pairs in existing MedVQA datasets,i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetuneslightweight pretrained generative models by incorporating medicaldecision-making rationales into the training process. The framework includesthree distinct strategies to generate decision outcomes and correspondingrationales, thereby clearly showcasing the medical decision-making processduring reasoning. Extensive experiments demonstrate that our method can achievean accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperformingexisting state-of-the-art baselines. Dataset and code will be released.</description><author>Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng, Jian Wu, Zuozhu Liu</author><pubDate>Thu, 18 Apr 2024 18:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12372v1</guid></item><item><title>Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2404.14741v1</link><description>To address the issue of insufficient knowledge and the tendency to generatehallucination in Large Language Models (LLMs), numerous studies have endeavoredto integrate LLMs with Knowledge Graphs (KGs). However, all these methods areevaluated on conventional Knowledge Graph Question Answering (KGQA) withcomplete KGs, where the factual triples involved in each question are entirelycovered by the given KG. In this situation, LLM mainly acts as an agent to findanswer entities by exploring the KG, rather than effectively integratinginternal and external knowledge sources. However, in real-world scenarios, KGsare often incomplete to cover all the knowledge required to answer questions.To simulate real-world scenarios and evaluate the ability of LLMs to integrateinternal and external knowledge, in this paper, we propose leveraging LLMs forQA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't includeall the factual triples involved in each question. To handle IKGQA, we proposea training-free method called Generate-on-Graph (GoG) that can generate newfactual triples while exploring on KGs. Specifically, we propose aselecting-generating-answering framework, which not only treat the LLM as anagent to explore on KGs, but also treat it as a KG to generate new facts basedon the explored subgraph and its inherent knowledge. Experimental results ontwo datasets demonstrate that our GoG can solve IKGQA to a certain extent,while almost all previous methods cannot perform well on IKGQA.</description><author>Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Kang Liu, Jun Zhao</author><pubDate>Tue, 23 Apr 2024 05:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14741v1</guid></item><item><title>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction</title><link>http://arxiv.org/abs/2307.04642v2</link><description>When applied to open-domain question answering, large language models (LLMs)frequently generate incorrect responses based on made-up facts, which arecalled $\textit{hallucinations}$. Retrieval augmented generation (RAG) is apromising strategy to avoid hallucinations, but it does not provide guaranteeson its correctness. To address this challenge, we propose the TrustworthyRetrieval Augmented Question Answering, or $\textit{TRAQ}$, which provides thefirst end-to-end statistical correctness guarantee for RAG. TRAQ uses conformalprediction, a statistical technique for constructing prediction sets that areguaranteed to contain the semantically correct response with high probability.Additionally, TRAQ leverages Bayesian optimization to minimize the size of theconstructed sets. In an extensive experimental evaluation, we demonstrate thatTRAQ provides the desired correctness guarantee while reducing prediction setsize by 16.2% on average compared to an ablation. The implementation isavailable at $\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.</description><author>Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani</author><pubDate>Fri, 05 Apr 2024 21:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04642v2</guid></item><item><title>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering</title><link>http://arxiv.org/abs/2404.04007v1</link><description>Compositional spatio-temporal reasoning poses a significant challenge in thefield of video question answering (VideoQA). Existing approaches struggle toestablish effective symbolic reasoning structures, which are crucial foranswering compositional spatio-temporal questions. To address this challenge,we propose a neural-symbolic framework called Neural-Symbolic VideoQA(NS-VideoQA), specifically designed for real-world VideoQA tasks. Theuniqueness and superiority of NS-VideoQA are two-fold: 1) It proposes a SceneParser Network (SPN) to transform static-dynamic video scenes into SymbolicRepresentation (SR), structuralizing persons, objects, relations, and actionchronologies. 2) A Symbolic Reasoning Machine (SRM) is designed for top-downquestion decompositions and bottom-up compositional reasonings. Specifically, apolymorphic program executor is constructed for internally consistent reasoningfrom SR to the final answer. As a result, Our NS-VideoQA not only improves thecompositional spatio-temporal reasoning in real-world VideoQA task, but alsoenables step-by-step error analysis by tracing the intermediate results.Experimental evaluations on the AGQA Decomp benchmark demonstrate theeffectiveness of the proposed NS-VideoQA framework. Empirical studies furtherconfirm that NS-VideoQA exhibits internal consistency in answeringcompositional questions and significantly improves the capability ofspatio-temporal and logical inference for VideoQA tasks.</description><author>Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang</author><pubDate>Fri, 05 Apr 2024 11:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04007v1</guid></item><item><title>BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining</title><link>http://arxiv.org/abs/2401.06443v2</link><description>The current research direction in generative models, such as the recentlydeveloped GPT4, aims to find relevant knowledge information for multimodal andmultilingual inputs to provide answers. Under these research circumstances, thedemand for multilingual evaluation of visual question answering (VQA) tasks, arepresentative task of multimodal systems, has increased. Accordingly, wepropose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study thatcan be extended to multilingualism. The proposed data include 17K images, 17Kquestion-answer pairs for both Korean and English and 280K instances ofknowledge information related to question-answer content. We also present aframework that can effectively inject knowledge information into a VQA systemby pretraining the knowledge information of BOK-VQA data in the form of graphembeddings. Finally, through in-depth analysis, we demonstrated the actualeffect of the knowledge information contained in the constructed training dataon VQA.</description><author>Minjun Kim, Seungwoo Song, Youhan Lee, Haneol Jang, Kyungtae Lim</author><pubDate>Fri, 15 Mar 2024 08:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06443v2</guid></item><item><title>Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models</title><link>http://arxiv.org/abs/2403.19631v1</link><description>Large Language Models (LLMs) have shown proficiency in question-answeringtasks but often struggle to integrate real-time knowledge updates, leading topotentially outdated or inaccurate responses. This problem becomes even morechallenging when dealing with multi-hop questions since they require LLMs toupdate and integrate multiple knowledge pieces relevant to the questions. Totackle the problem, we propose the Retrieval-Augmented model Editing (RAE)framework tailored for multi-hop question answering. RAE first retrieves editedfacts and then refines the language model through in-context learning.Specifically, our retrieval approach, based on mutual information maximization,leverages the reasoning abilities of LLMs to identify chain facts that na\"ivesimilarity-based searches might miss. Additionally, our framework incorporatesa pruning strategy to eliminate redundant information from the retrieved facts,which enhances the editing accuracy and mitigates the hallucination problem.Our framework is supported by theoretical justification for its fact retrievalefficacy. Finally, comprehensive evaluation across various LLMs validates RAE'sability in providing accurate answers with updated knowledge.</description><author>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</author><pubDate>Thu, 28 Mar 2024 18:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19631v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering</title><link>http://arxiv.org/abs/2403.17647v2</link><description>The large success of deep learning based methods in Visual Question Answering(VQA) has concurrently increased the demand for explainable methods. Mostmethods in Explainable Artificial Intelligence (XAI) focus on generatingpost-hoc explanations rather than taking an intrinsic approach, the lattercharacterizing an interpretable model. In this work, we introduce aninterpretable approach for graph-based VQA and demonstrate competitiveperformance on the GQA dataset. This approach bridges the gap betweeninterpretability and performance. Our model is designed to intrinsicallyproduce a subgraph during the question-answering process as its explanation,providing insight into the decision making. To evaluate the quality of thesegenerated subgraphs, we compare them against established post-hocexplainability methods for graph neural networks, and perform a humanevaluation. Moreover, we present quantitative metrics that correlate with theevaluations of human assessors, acting as automatic metrics for the generatedexplanatory subgraphs. Our implementation is available athttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.</description><author>Pascal Tilli, Ngoc Thang Vu</author><pubDate>Wed, 27 Mar 2024 11:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17647v2</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v2</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, etc. Our proposed framework, named PromptGIP, unifiesthese diverse image processing tasks within a universal framework. Inspired byNLP question answering (QA) techniques, we employ a visual prompting questionanswering paradigm. Specifically, we treat the input-output image pair as astructured question-answer sentence, thereby reprogramming the image processingtask as a prompting QA problem. PromptGIP can undertake diverse cross-domaintasks using provided visual prompts, eliminating the need for task-specificfinetuning. Our methodology offers a universal and adaptive solution to generalimage processing. While PromptGIP has demonstrated a certain degree ofout-of-domain task generalization capability, further research is expected tofully explore its more powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Wed, 21 Feb 2024 03:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v2</guid></item><item><title>Faithful Temporal Question Answering over Heterogeneous Sources</title><link>http://arxiv.org/abs/2402.15400v1</link><description>Temporal question answering (QA) involves time constraints, with phrases suchas "... in 2019" or "... before COVID". In the former, time is an explicitcondition, in the latter it is implicit. State-of-the-art methods havelimitations along three dimensions. First, with neural inference, timeconstraints are merely soft-matched, giving room to invalid or inexplicableanswers. Second, questions with implicit time are poorly supported. Third,answers come from a single source: either a knowledge base (KB) or a textcorpus. We propose a temporal QA system that addresses these shortcomings.First, it enforces temporal constraints for faithful answering with tangibleevidence. Second, it properly handles implicit questions. Third, it operatesover heterogeneous sources, covering KB, text and web tables in a unifiedmanner. The method has three stages: (i) understanding the question and itstemporal conditions, (ii) retrieving evidence from all sources, and (iii)faithfully answering the question. As implicit questions are sparse in priorbenchmarks, we introduce a principled method for generating diverse questions.Experiments show superior performance over a suite of baselines.</description><author>Zhen Jia, Philipp Christmann, Gerhard Weikum</author><pubDate>Fri, 23 Feb 2024 16:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15400v1</guid></item><item><title>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</title><link>http://arxiv.org/abs/2405.20654v1</link><description>Effective passage retrieval and reranking methods have been widely utilizedto identify suitable candidates in open-domain question answering tasks, recentstudies have resorted to LLMs for reranking the retrieved passages by thelog-likelihood of the question conditioned on each passage. Although thesemethods have demonstrated promising results, the performance is notablysensitive to the human-written prompt (or hard prompt), and fine-tuning LLMscan be computationally intensive and time-consuming. Furthermore, this approachlimits the leverage of question-passage relevance pairs and passage-specificknowledge to enhance the ranking capabilities of LLMs. In this paper, wepropose passage-specific prompt tuning for reranking in open-domain questionanswering (PSPT): a parameter-efficient method that fine-tunes learnablepassage-specific soft prompts, incorporating passage-specific knowledge from alimited set of question-passage relevance pairs. The method involves rankingretrieved passages based on the log-likelihood of the model generating thequestion conditioned on each passage and the learned soft prompt. We conductedextensive experiments utilizing the Llama-2-chat-7B model across three publiclyavailable open-domain question answering datasets and the results demonstratethe effectiveness of the proposed approach.</description><author>Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, Yi Fang</author><pubDate>Fri, 31 May 2024 08:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20654v1</guid></item><item><title>Asking and Answering Questions to Extract Event-Argument Structures</title><link>http://arxiv.org/abs/2404.16413v1</link><description>This paper presents a question-answering approach to extract document-levelevent-argument structures. We automatically ask and answer questions for eachargument type an event may have. Questions are generated using manually definedtemplates and generative transformers. Template-based questions are generatedusing predefined role-specific wh-words and event triggers from the contextdocument. Transformer-based questions are generated using large language modelstrained to formulate questions based on a passage and the expected answer.Additionally, we develop novel data augmentation strategies specialized ininter-sentential event-argument relations. We use a simple span-swappingtechnique, coreference resolution, and large language models to augment thetraining instances. Our approach enables transfer learning without anycorpora-specific modifications and yields competitive results with the RAMSdataset. It outperforms previous work, and it is especially beneficial toextract arguments that appear in different sentences than the event trigger. Wealso present detailed quantitative and qualitative analyses shedding light onthe most common errors made by our best model.</description><author>Md Nayem Uddin, Enfa Rose George, Eduardo Blanco, Steven Corman</author><pubDate>Thu, 25 Apr 2024 09:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16413v1</guid></item><item><title>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</title><link>http://arxiv.org/abs/2403.04640v1</link><description>This paper focuses on the challenge of answering questions in scenarios thatare composed of rich and complex dynamic audio-visual components. Althoughexisting Multimodal Large Language Models (MLLMs) can respond to audio-visualcontent, these responses are sometimes ambiguous and fail to describe specificaudio-visual events. To overcome this limitation, we introduce the CAT, whichenhances MLLM in three ways: 1) besides straightforwardly bridging audio andvideo, we design a clue aggregator that aggregates question-related clues indynamic audio-visual scenarios to enrich the detailed knowledge required forlarge language models. 2) CAT is trained on a mixed multimodal dataset,allowing direct application in audio-visual scenarios. Notably, we collect anaudio-visual joint instruction dataset named AVinstruct, to further enhance thecapacity of CAT to model cross-semantic correlations. 3) we propose AI-assistedambiguity-aware direct preference optimization, a strategy specialized inretraining the model to favor the non-ambiguity response and improve theability to localize specific audio-visual objects. Extensive experimentalresults demonstrate that CAT outperforms existing methods on multimodal tasks,especially in Audio-Visual Question Answering (AVQA) tasks. The codes and thecollected instructions are released at https://github.com/rikeilong/Bay-CAT.</description><author>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</author><pubDate>Thu, 07 Mar 2024 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04640v1</guid></item><item><title>Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering</title><link>http://arxiv.org/abs/2401.10711v3</link><description>Video Question Answering (VideoQA) aims to answer natural language questionsbased on the information observed in videos. Despite the recent success ofLarge Multimodal Models (LMMs) in image-language understanding and reasoning,they deal with VideoQA insufficiently, by simply taking uniformly sampledframes as visual inputs, which ignores question-relevant visual clues.Moreover, there are no human annotations for question-critical timestamps inexisting VideoQA datasets. In light of this, we propose a novel weaklysupervised framework to enforce the LMMs to reason out the answers withquestion-critical moments as visual inputs. Specifically, we first fuse thequestion and answer pairs as event descriptions to find multiple keyframes astarget moments and pseudo-labels, with the visual-language alignment capabilityof the CLIP models. With these pseudo-labeled keyframes as additionally weaksupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)module. GCG learns multiple Gaussian functions to characterize the temporalstructure of the video, and sample question-critical frames as positive momentsto be the visual inputs of LMMs. Extensive experiments on several benchmarksverify the effectiveness of our framework, and we achieve substantialimprovements compared to previous state-of-the-art methods.</description><author>Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge</author><pubDate>Fri, 26 Apr 2024 10:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10711v3</guid></item><item><title>ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</title><link>http://arxiv.org/abs/2209.08199v2</link><description>We present a new task and dataset, ScreenQA, for screen content understandingvia question answering. The existing screen datasets are focused either onstructure and component-level understanding, or on a much higher-levelcomposite task such as navigation and task completion. We attempt to bridge thegap between these two by annotating 86K question-answer pairs over the RICOdataset in hope to benchmark the screen reading comprehension capacity.</description><author>Yu-Chung Hsiao, Fedir Zubach, Maria Wang, Jindong Chen</author><pubDate>Thu, 22 Feb 2024 08:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08199v2</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v1</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 16 Feb 2024 20:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v1</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v2</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 31 May 2024 18:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v2</guid></item><item><title>FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering</title><link>http://arxiv.org/abs/2404.18585v1</link><description>Table Question Answering (TQA) aims at composing an answer to a questionbased on tabular data. While prior research has shown that TQA models lackrobustness, understanding the underlying cause and nature of this issue remainspredominantly unclear, posing a significant obstacle to the development ofrobust TQA systems. In this paper, we formalize three major desiderata for afine-grained evaluation of robustness of TQA systems. They should (i) answerquestions regardless of alterations in table structure, (ii) base theirresponses on the content of relevant cells rather than on biases, and (iii)demonstrate robust numerical reasoning capabilities. To investigate theseaspects, we create and publish a novel TQA evaluation benchmark in English. Ourextensive experimental analysis reveals that none of the examinedstate-of-the-art TQA systems consistently excels in these three aspects. Ourbenchmark is a crucial instrument for monitoring the behavior of TQA systemsand paves the way for the development of robust TQA systems. We release ourbenchmark publicly.</description><author>Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich</author><pubDate>Mon, 29 Apr 2024 11:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18585v1</guid></item><item><title>TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</title><link>http://arxiv.org/abs/2404.03574v1</link><description>Traditional machine learning models often require powerful hardware, makingthem unsuitable for deployment on resource-limited devices. Tiny MachineLearning (tinyML) has emerged as a promising approach for running machinelearning models on these devices, but integrating multiple data modalities intotinyML models still remains a challenge due to increased complexity, latency,and power consumption. This paper proposes TinyVQA, a novel multimodal deepneural network for visual question answering tasks that can be deployed onresource-constrained tinyML hardware. TinyVQA leverages a supervisedattention-based model to learn how to answer questions about images using bothvision and language modalities. Distilled knowledge from the supervisedattention-based VQA model trains the memory aware compact TinyVQA model and lowbit-width quantization technique is employed to further compress the model fordeployment on tinyML devices. The TinyVQA model was evaluated on the FloodNetdataset, which is used for post-disaster damage assessment. The compact modelachieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA forreal-world applications. Additionally, the model was deployed on a Crazyflie2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA modelachieved low latencies of 56 ms and consumes 693 mW power while deployed on thetiny drone, showcasing its suitability for resource-constrained embeddedsystems.</description><author>Hasib-Al Rashid, Argho Sarkar, Aryya Gangopadhyay, Maryam Rahnemoonfar, Tinoosh Mohsenin</author><pubDate>Thu, 04 Apr 2024 17:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03574v1</guid></item><item><title>Multi-Frame, Lightweight &amp; Efficient Vision-Language Models for Question Answering in Autonomous Driving</title><link>http://arxiv.org/abs/2403.19838v1</link><description>Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) havebecome prominent in autonomous driving research, as these models can provideinterpretable textual reasoning and responses for end-to-end autonomous drivingsafety tasks using traffic scene images and other data modalities. However,current approaches to these systems use expensive large language model (LLM)backbones and image encoders, making such systems unsuitable for real-timeautonomous driving systems where tight memory constraints exist and fastinference time is necessary. To address these previous issues, we developEM-VLM4AD, an efficient, lightweight, multi-frame vision language model whichperforms Visual Question Answering for autonomous driving. In comparison toprevious approaches, EM-VLM4AD requires at least 10 times less memory andfloating point operations, while also achieving higher BLEU-4, METEOR, CIDEr,and ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4ADalso exhibits the ability to extract relevant information from traffic viewsrelated to prompts and can answer questions for various autonomous drivingsubtasks. We release our code to train and evaluate our model athttps://github.com/akshaygopalkr/EM-VLM4AD.</description><author>Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi</author><pubDate>Thu, 28 Mar 2024 22:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19838v1</guid></item><item><title>Multi-Frame, Lightweight &amp; Efficient Vision-Language Models for Question Answering in Autonomous Driving</title><link>http://arxiv.org/abs/2403.19838v2</link><description>Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) havebecome prominent in autonomous driving research, as these models can provideinterpretable textual reasoning and responses for end-to-end autonomous drivingsafety tasks using traffic scene images and other data modalities. However,current approaches to these systems use expensive large language model (LLM)backbones and image encoders, making such systems unsuitable for real-timeautonomous driving systems where tight memory constraints exist and fastinference time is necessary. To address these previous issues, we developEM-VLM4AD, an efficient, lightweight, multi-frame vision language model whichperforms Visual Question Answering for autonomous driving. In comparison toprevious approaches, EM-VLM4AD requires at least 10 times less memory andfloating point operations, while also achieving higher CIDEr and ROUGE-L scoresthan the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits theability to extract relevant information from traffic views related to promptsand can answer questions for various autonomous driving subtasks. We releaseour code to train and evaluate our model athttps://github.com/akshaygopalkr/EM-VLM4AD.</description><author>Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi</author><pubDate>Thu, 09 May 2024 04:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19838v2</guid></item><item><title>KazQAD: Kazakh Open-Domain Question Answering Dataset</title><link>http://arxiv.org/abs/2404.04487v1</link><description>We introduce KazQAD -- a Kazakh open-domain question answering (ODQA) dataset-- that can be used in both reading comprehension and full ODQA settings, aswell as for information retrieval experiments. KazQAD contains just under 6,000unique questions with extracted short answers and nearly 12,000 passage-levelrelevance judgements. We use a combination of machine translation, Wikipediasearch, and in-house manual annotation to ensure annotation efficiency and dataquality. The questions come from two sources: translated items from the NaturalQuestions (NQ) dataset (only for training) and the original Kazakh UnifiedNational Testing (UNT) exam (for development and testing). The accompanyingtext corpus contains more than 800,000 passages from the Kazakh Wikipedia. As asupplementary dataset, we release around 61,000 question-passage-answer triplesfrom the NQ dataset that have been machine-translated into Kazakh. We developbaseline retrievers and readers that achieve reasonable scores in retrieval(NDCG@10 = 0.389 MRR = 0.382), reading comprehension (EM = 38.5 F1 = 54.2), andfull ODQA (EM = 17.8 F1 = 28.7) settings. Nevertheless, these results aresubstantially lower than state-of-the-art results for English QA collections,and we think that there should still be ample room for improvement. We alsoshow that the current OpenAI's ChatGPTv3.5 is not able to answer KazQAD testquestions in the closed-book setting with acceptable quality. The dataset isfreely available under the Creative Commons licence (CC BY-SA) athttps://github.com/IS2AI/KazQAD.</description><author>Rustem Yeshpanov, Pavel Efimov, Leonid Boytsov, Ardak Shalkarbayuli, Pavel Braslavski</author><pubDate>Sat, 06 Apr 2024 04:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04487v1</guid></item><item><title>End-to-End Beam Retrieval for Multi-Hop Question Answering</title><link>http://arxiv.org/abs/2308.08973v2</link><description>Multi-hop question answering (QA) involves finding multiple relevant passagesand step-by-step reasoning to answer complex questions, indicating aretrieve-and-read paradigm. However, previous retrievers were customized fortwo-hop questions, and most of them were trained separately across differenthops, resulting in a lack of supervision over the entire multi-hop retrievalprocess and leading to poor performance in complicated scenarios beyond twohops. In this work, we introduce Beam Retrieval, an end-to-end beam retrievalframework for multi-hop QA. This approach models the multi-hop retrievalprocess in an end-to-end manner by jointly optimizing an encoder and twoclassification heads across all hops. Moreover, Beam Retrieval maintainsmultiple partial hypotheses of relevant passages at each step, expanding thesearch space and reducing the risk of missing relevant passages. To establish acomplete QA system, we incorporate a supervised reader or a large languagemodel (LLM). Experimental results demonstrate that Beam Retrieval achieves anearly 50% improvement compared with baselines on challenging MuSiQue-Ans, andit also surpasses all previous retrievers on HotpotQA and achieves 99.9%precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrievalhelps our supervised reader achieve new state-of-the-art performance andsubstantially improves the few-shot QA performance of LLMs.</description><author>Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, Shen Huang</author><pubDate>Mon, 01 Apr 2024 09:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08973v2</guid></item><item><title>MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering</title><link>http://arxiv.org/abs/2403.19116v1</link><description>In today's fast-paced industry, professionals face the challenge ofsummarizing a large number of documents and extracting vital information fromthem on a daily basis. These metrics are frequently hidden away in tablesand/or their nested hyperlinks. To address this challenge, the approach ofTable Question Answering (QA) has been developed to extract the relevantinformation. However, traditional Table QA training tasks that provide a tableand an answer(s) from a gold cell coordinate(s) for a question may not alwaysensure extracting the accurate answer(s). Recent advancements in Large LanguageModels (LLMs) have opened up new possibilities for extracting information fromtabular data using prompts. In this paper, we introduce the Multi-hop Few-shotOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. Thefirst step involves Few-Shot Learning (FSL), where relevant tables andassociated contexts of hyperlinks are retrieved based on a given question. Theretrieved content is then used to construct few-shot prompts as inputs to anLLM, such as ChatGPT. To tackle the challenge of answering complex questions,the second step leverages Chain-of-thought (CoT) prompting to decompose thecomplex question into a sequential chain of questions and reasoning thoughts ina multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this processby retrieving relevant tables and contexts of hyperlinks that are relevant tothe resulting reasoning thoughts and questions. These additional contexts arethen used to supplement the prompt used in the first step, resulting in moreaccurate answers from an LLM. Empirical results from OTT-QA demonstrate thatour abstractive QA approach significantly improves the accuracy of extractiveTable QA methods.</description><author>Che Guan, Mengyu Huang, Peng Zhang</author><pubDate>Thu, 28 Mar 2024 04:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19116v1</guid></item><item><title>Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering</title><link>http://arxiv.org/abs/2402.12728v1</link><description>Knowledge-based visual question answering (KVQA) has been extensively studiedto answer visual questions with external knowledge, e.g., knowledge graphs(KGs). While several attempts have been proposed to leverage large languagemodels (LLMs) as an implicit knowledge source, it remains challenging sinceLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,images, KGs and LLMs, cannot be readily aligned for complex scenarios. Totackle these, we present a novel modality-aware integration with LLMs for KVQA(MAIL). It carefully leverages multimodal knowledge for both imageunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stageprompting strategy with LLMs to densely embody the image into a scene graphwith detailed visual features; (ii) We construct a coupled concept graph bylinking the mentioned entities with external facts. (iii) A tailoredpseudo-siamese graph medium fusion is designed for sufficient multimodalfusion. We utilize the shared mentioned entities in two graphs as mediums tobridge a tight inter-modal exchange, while maximally preserving insightfulintra-modal learning by constraining the fusion within mediums. Extensiveexperiments on two benchmark datasets show the superiority of MAIL with 24xless resources.</description><author>Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang</author><pubDate>Tue, 20 Feb 2024 05:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12728v1</guid></item><item><title>Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering</title><link>http://arxiv.org/abs/2309.08469v2</link><description>Modern open-domain question answering systems often rely on accurate andefficient retrieval components to find passages containing the facts necessaryto answer the question. Recently, neural retrievers have gained popularity overlexical alternatives due to their superior performance. However, most of thework concerns popular languages such as English or Chinese. For others, such asPolish, few models are available. In this work, we present Silver Retriever, aneural retriever for Polish trained on a diverse collection of manually orweakly labeled datasets. Silver Retriever achieves much better results thanother Polish models and is competitive with larger multilingual models.Together with the model, we open-source five new passage retrieval datasets.</description><author>Piotr Rybak, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08469v2</guid></item><item><title>CommVQA: Situating Visual Question Answering in Communicative Contexts</title><link>http://arxiv.org/abs/2402.15002v1</link><description>Current visual question answering (VQA) models tend to be trained andevaluated on image-question pairs in isolation. However, the questions peopleask are dependent on their informational needs and prior knowledge about theimage content. To evaluate how situating images within naturalistic contextsshapes visual questions, we introduce CommVQA, a VQA dataset consisting ofimages, image descriptions, real-world communicative scenarios where the imagemight appear (e.g., a travel website), and follow-up questions and answersconditioned on the scenario. We show that CommVQA poses a challenge for currentmodels. Providing contextual information to VQA models improves performancebroadly, highlighting the relevance of situating systems within a communicativescenario.</description><author>Nandita Shankar Naik, Christopher Potts, Elisa Kreiss</author><pubDate>Thu, 22 Feb 2024 22:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15002v1</guid></item><item><title>LaPA: Latent Prompt Assist Model For Medical Visual Question Answering</title><link>http://arxiv.org/abs/2404.13039v1</link><description>Medical visual question answering (Med-VQA) aims to automate the predictionof correct answers for medical images and questions, thereby assistingphysicians in reducing repetitive tasks and alleviating their workload.Existing approaches primarily focus on pre-training models using additional andcomprehensive datasets, followed by fine-tuning to enhance performance indownstream tasks. However, there is also significant value in exploringexisting models to extract clinically relevant information. In this paper, wepropose the Latent Prompt Assist model (LaPA) for medical visual questionanswering. Firstly, we design a latent prompt generation module to generate thelatent prompt with the constraint of the target answer. Subsequently, wepropose a multi-modal fusion block with latent prompt fusion module thatutilizes the latent prompt to extract clinical-relevant information fromuni-modal and multi-modal features. Additionally, we introduce a priorknowledge fusion module to integrate the relationship between diseases andorgans with the clinical-relevant information. Finally, we combine the finalintegrated information with image-language cross-modal information to predictthe final answers. Experimental results on three publicly available Med-VQAdatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,achieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, andVQA-2019, respectively. The code is publicly available athttps://github.com/GaryGuTC/LaPA_model.</description><author>Tiancheng Gu, Kaicheng Yang, Dongnan Liu, Weidong Cai</author><pubDate>Fri, 19 Apr 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13039v1</guid></item><item><title>Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering</title><link>http://arxiv.org/abs/2311.17331v2</link><description>Recently, several methods have been proposed to augment large Vision LanguageModels (VLMs) for Visual Question Answering (VQA) simplicity by incorporatingexternal knowledge from knowledge bases or visual clues derived from questiondecomposition. Although having achieved promising results, these methods stillsuffer from the challenge that VLMs cannot inherently understand theincorporated knowledge and might fail to generate the optimal answers.Contrarily, human cognition engages visual questions through a top-downreasoning process, systematically exploring relevant issues to derive acomprehensive answer. This not only facilitates an accurate answer but alsoprovides a transparent rationale for the decision-making pathway. Motivated bythis cognitive mechanism, we introduce a novel, explainable multi-agentcollaboration framework designed to imitate human-like top-down reasoning byleveraging the expansive knowledge of Large Language Models (LLMs). Ourframework comprises three agents, i.e., Responder, Seeker, and Integrator, eachcontributing uniquely to the top-down reasoning process. The VLM-basedResponder generates the answer candidates for the question and gives responsesto other issues. The Seeker, primarily based on LLM, identifies relevant issuesrelated to the question to inform the Responder and constructs a Multi-ViewKnowledge Base (MVKB) for the given visual scene by leveraging theunderstanding capabilities of LLM. The Integrator agent combines informationfrom the Seeker and the Responder to produce the final VQA answer. Through thiscollaboration mechanism, our framework explicitly constructs an MVKB for aspecific visual scene and reasons answers in a top-down reasoning process.Extensive and comprehensive evaluations on diverse VQA datasets and VLMsdemonstrate the superior applicability and interpretability of our frameworkover the existing compared methods.</description><author>Zeqing Wang, Wentao Wan, Qiqing Lao, Runmeng Chen, Minjie Lang, Keze Wang, Liang Lin</author><pubDate>Tue, 14 May 2024 07:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17331v2</guid></item><item><title>PDF-MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering</title><link>http://arxiv.org/abs/2404.12720v1</link><description>Document Question Answering (QA) presents a challenge in understandingvisually-rich documents (VRD), particularly those dominated by lengthy textualcontent like research journal articles. Existing studies primarily focus onreal-world documents with sparse text, while challenges persist incomprehending the hierarchical semantic relations among multiple pages tolocate multimodal components. To address this gap, we propose PDF-MVQA, whichis tailored for research journal articles, encompassing multiple pages andmultimodal information retrieval. Unlike traditional machine readingcomprehension (MRC) tasks, our approach aims to retrieve entire paragraphscontaining answers or visually rich document entities like tables and figures.Our contributions include the introduction of a comprehensive PDF Document VQAdataset, allowing the examination of semantically hierarchical layoutstructures in text-dominant documents. We also present new VRD-QA frameworksdesigned to grasp textual contents and relations among document layoutssimultaneously, extending page-level understanding to the entire multi-pagedocument. Through this work, we aim to enhance the capabilities of existingvision-and-language models in handling challenges posed by text-dominantdocuments in VRD-QA.</description><author>Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, Soyeon Caren Han</author><pubDate>Fri, 19 Apr 2024 10:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12720v1</guid></item><item><title>SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering</title><link>http://arxiv.org/abs/2401.13463v2</link><description>Spoken Question Answering (SQA) is essential for machines to reply to user'squestion by finding the answer span within a given spoken passage. SQA has beenpreviously achieved without ASR to avoid recognition errors andOut-of-Vocabulary (OOV) problems. However, the real-world problem ofOpen-domain SQA (openSQA), in which the machine needs to first retrievepassages that possibly contain the answer from a spoken archive in addition,was never considered. This paper proposes the first known end-to-end framework,Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of theopenSQA problem. SpeechDPR learns a sentence-level semantic representation bydistilling knowledge from the cascading model of unsupervised ASR (UASR) andtext dense retriever (TDR). No manually transcribed speech data is needed.Initial experiments showed performance comparable to the cascading model ofUASR and TDR, and significantly better when UASR was poor, verifying thisapproach is more robust to speech recognition errors.</description><author>Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee</author><pubDate>Mon, 18 Mar 2024 07:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13463v2</guid></item><item><title>Evaluating the Retrieval Component in LLM-Based Question Answering Systems</title><link>http://arxiv.org/abs/2406.06458v1</link><description>Question answering systems (QA) utilizing Large Language Models (LLMs)heavily depend on the retrieval component to provide them with domain-specificinformation and reduce the risk of generating inaccurate responses orhallucinations. Although the evaluation of retrievers dates back to the earlyresearch in Information Retrieval, assessing their performance within LLM-basedchatbots remains a challenge. This study proposes a straightforward baseline for evaluating retrievers inRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstratethat this evaluation framework provides a better image of how the retrieverperforms and is more aligned with the overall performance of the QA system.Although conventional metrics such as precision, recall, and F1 score may notfully capture LLMs' capabilities - as they can yield accurate responses despiteimperfect retrievers - our method considers LLMs' strengths to ignoreirrelevant contexts, as well as potential errors and hallucinations in theirresponses.</description><author>Ashkan Alinejad, Krtin Kumar, Ali Vahdat</author><pubDate>Mon, 10 Jun 2024 17:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06458v1</guid></item><item><title>Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</title><link>http://arxiv.org/abs/2404.01652v1</link><description>Open-domain Question Answering (OpenQA) aims at answering factual questionswith an external large-scale knowledge corpus. However, real-world knowledge isnot static; it updates and evolves continually. Such a dynamic characteristicof knowledge poses a vital challenge for these models, as the trained modelsneed to constantly adapt to the latest information to make sure that theanswers remain accurate. In addition, it is still unclear how well an OpenQAmodel can transfer to completely new knowledge domains. In this paper, weinvestigate the generalization performance of a retrieval-augmented QA model intwo specific scenarios: 1) adapting to updated versions of the same knowledgecorpus; 2) switching to completely different knowledge domains. We observe thatthe generalization challenges of OpenQA models stem from the reader'sover-reliance on memorizing the knowledge from the external corpus, whichhinders the model from generalizing to a new knowledge corpus. We introduceCorpus-Invariant Tuning (CIT), a simple but effective training strategy, tomitigate the knowledge over-memorization by controlling the likelihood ofretrieved contexts during training. Extensive experimental results on multipleOpenQA benchmarks show that CIT achieves significantly better generalizabilitywithout compromising the model's performance in its original corpus and domain.</description><author>Zixuan Zhang, Revanth Gangi Reddy, Kevin Small, Tong Zhang, Heng Ji</author><pubDate>Tue, 02 Apr 2024 06:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01652v1</guid></item><item><title>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering</title><link>http://arxiv.org/abs/2402.16313v1</link><description>Open-ended question answering requires models to find appropriate evidence toform well-reasoned, comprehensive and helpful answers. In practicalapplications, models also need to engage in extended discussions on potentialscenarios closely relevant to the question. With augmentation of retrievalmodule, open-source Large Language Models (LLMs) can produce coherent answersoften with different focuses, but are still sub-optimal in terms of reliableevidence selection and in-depth question analysis. In this paper, we propose anovel Chain-of-Discussion framework to leverage the synergy among multipleopen-source LLMs aiming to provide \textbf{more correct} and \textbf{morecomprehensive} answers for open-ended QA, although they are not strong enoughindividually. Our experiments show that discussions among multiple LLMs play avital role in enhancing the quality of answers. We release our data and code at\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.</description><author>Mingxu Tao, Dongyan Zhao, Yansong Feng</author><pubDate>Mon, 26 Feb 2024 05:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16313v1</guid></item><item><title>ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images</title><link>http://arxiv.org/abs/2404.10652v1</link><description>Visual Question Answering (VQA) is a complicated task that requires thecapability of simultaneously processing natural language and images. Initially,this task was researched, focusing on methods to help machines understandobjects and scene contexts in images. However, some text appearing in the imagethat carries explicit information about the full content of the image is notmentioned. Along with the continuous development of the AI era, there have beenmany studies on the reading comprehension ability of VQA models in the world.As a developing country, conditions are still limited, and this task is stillopen in Vietnam. Therefore, we introduce the first large-scale dataset inVietnamese specializing in the ability to understand text appearing in images,we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual\textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over16,000} images and \textbf{over 50,000} questions with answers. Throughmeticulous experiments with various state-of-the-art models, we uncover thesignificance of the order in which tokens in OCR text are processed andselected to formulate answers. This finding helped us significantly improve theperformance of the baseline models on the ViTextVQA dataset. Our dataset isavailable at this\href{https://github.com/minhquan6203/ViTextVQA-Dataset}{link} for researchpurposes.</description><author>Quan Van Nguyen, Dan Quang Tran, Huy Quang Pham, Thang Kien-Bao Nguyen, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</author><pubDate>Tue, 16 Apr 2024 16:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10652v1</guid></item><item><title>Answering Diverse Questions via Text Attached with Key Audio-Visual Clues</title><link>http://arxiv.org/abs/2403.06679v1</link><description>Audio-visual question answering (AVQA) requires reference to video contentand auditory information, followed by correlating the question to predict themost precise answer. Although mining deeper layers of audio-visual informationto interact with questions facilitates the multimodal fusion process, theredundancy of audio-visual parameters tends to reduce the generalization of theinference engine to multiple question-answer pairs in a single video. Indeed,the natural heterogeneous relationship between audiovisuals and text makes theperfect fusion challenging, to prevent high-level audio-visual semantics fromweakening the network's adaptability to diverse question types, we propose aframework for performing mutual correlation distillation (MCD) to aid questioninference. MCD is divided into three main steps: 1) firstly, the residualstructure is utilized to enhance the audio-visual soft associations based onself-attention, then key local audio-visual features relevant to the questioncontext are captured hierarchically by shared aggregators and coupled in theform of clues with specific question vectors. 2) Secondly, knowledgedistillation is enforced to align audio-visual-text pairs in a shared latentspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visualdependencies are decoupled by discarding the decision-level integrations. Weevaluate the proposed method on two publicly available datasets containingmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments showthat our method outperforms other state-of-the-art methods, and one interestingfinding behind is that removing deep audio-visual features during inference caneffectively mitigate overfitting. The source code is released athttp://github.com/rikeilong/MCD-forAVQA.</description><author>Qilang Ye, Zitong Yu, Xin Liu</author><pubDate>Mon, 11 Mar 2024 13:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06679v1</guid></item><item><title>Can I Trust Your Answer? Visually Grounded Video Question Answering</title><link>http://arxiv.org/abs/2309.01327v2</link><description>We study visually grounded VideoQA in response to the emerging trends ofutilizing pretraining techniques for video-language understanding.Specifically, by forcing vision-language models (VLMs) to answer questions andsimultaneously provide visual evidence, we seek to ascertain the extent towhich the predictions of such techniques are genuinely anchored in relevantvideo content, versus spurious correlations from language or irrelevant visualcontext. Towards this, we construct NExT-GQA -- an extension of NExT-QA with10.5$K$ temporal grounding (or location) labels tied to the original QA pairs.With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Throughpost-hoc attention analysis, we find that these models are extremely weak insubstantiating the answers despite their strong QA performance. This exposesthe limitation of current VLMs in making reliable predictions. As a remedy, wefurther explore and propose a grounded-QA method via Gaussian mask optimizationand cross-modal learning. Experiments with different backbones demonstrate thatthis grounding mechanism improves both grounding and QA. With these efforts, weaim to push towards trustworthy VLMs in VQA systems. Our dataset and code areavailable at https://github.com/doc-doc/NExT-GQA.</description><author>Junbin Xiao, Angela Yao, Yicong Li, Tat Seng Chua</author><pubDate>Sat, 30 Mar 2024 07:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01327v2</guid></item><item><title>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</title><link>http://arxiv.org/abs/2402.11161v1</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current answer correctness (AC) metrics do not align with human judgments,particularly verbose, free form answers from large language models (LLM). Thereare two challenges: a lack of data and that models are too big. LLM basedscorers correlate better with humans, but this expensive task has only beentested on limited QA datasets. We rectify these issues by providing clearguidelines for evaluating machine QA adopted from human QA contests. We alsointroduce Precise ANswer correctness Determination and Adjudication (PANDA), asmall, efficient, deterministic AC classifier (812 KB) that more accuratelyevaluates answer correctness.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</author><pubDate>Sat, 17 Feb 2024 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11161v1</guid></item><item><title>Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education</title><link>http://arxiv.org/abs/2402.14293v1</link><description>In the domain of Natural Language Processing (NLP), Large Language Models(LLMs) have demonstrated promise in text-generation tasks. However, theireducational applications, particularly for domain-specific queries, remainunderexplored. This study investigates LLMs' capabilities in educationalscenarios, focusing on concept graph recovery and question-answering (QA). Weassess LLMs' zero-shot performance in creating domain-specific concept graphsand introduce TutorQA, a new expert-verified NLP-focused benchmark forscientific graph reasoning and QA. TutorQA consists of five tasks with 500 QApairs. To tackle TutorQA queries, we present CGLLM, a pipeline integratingconcept graphs with LLMs for answering diverse questions. Our results indicatethat LLMs' zero-shot concept graph recovery is competitive with supervisedmethods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMsachieve up to 26% F1 score enhancement. Moreover, human evaluation and analysisshow that CGLLM generates answers with more fine-grained concepts.</description><author>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</author><pubDate>Thu, 22 Feb 2024 05:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14293v1</guid></item><item><title>Generator-Retriever-Generator Approach for Open-Domain Question Answering</title><link>http://arxiv.org/abs/2307.11278v3</link><description>Open-domain question answering (QA) tasks usually require the retrieval ofrelevant information from a large corpus to generate accurate answers. Wepropose a novel approach called Generator-Retriever-Generator (GRG) thatcombines document retrieval techniques with a large language model (LLM), byfirst prompting the model to generate contextual documents based on a givenquestion. In parallel, a dual-encoder network retrieves documents that arerelevant to the question from an external corpus. The generated and retrieveddocuments are then passed to the second LLM, which generates the final answer.By combining document retrieval and LLM generation, our approach addresses thechallenges of open-domain QA, such as generating informative and contextuallyrelevant answers. GRG outperforms the state-of-the-art generate-then-read andretrieve-then-read pipelines (GENREAD and RFiD) improving their performance byat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,respectively. We provide code, datasets, and checkpoints athttps://github.com/abdoelsayed2016/GRG.</description><author>Abdelrahman Abdallah, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11278v3</guid></item><item><title>VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images</title><link>http://arxiv.org/abs/2405.03852v1</link><description>While Vector Symbolic Architectures (VSAs) are promising for modellingspatial cognition, their application is currently limited to artificiallygenerated images and simple spatial queries. We propose VSA4VQA - a novel 4Dimplementation of VSAs that implements a mental representation of naturalimages for the challenging task of Visual Question Answering (VQA). VSA4VQA isthe first model to scale a VSA to complex spatial queries. Our method is basedon the Semantic Pointer Architecture (SPA) to encode objects in ahyperdimensional vector space. To encode natural images, we extend the SPA toinclude dimensions for object's width and height in addition to their spatiallocation. To perform spatial queries we further introduce learned spatial querymasks and integrate a pre-trained vision-language model for answeringattribute-related questions. We evaluate our method on the GQA benchmarkdataset and show that it can effectively encode natural images, achievingcompetitive performance to state-of-the-art deep learning methods for zero-shotVQA.</description><author>Anna Penzkofer, Lei Shi, Andreas Bulling</author><pubDate>Mon, 06 May 2024 21:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03852v1</guid></item><item><title>FinTextQA: A Dataset for Long-form Financial Question Answering</title><link>http://arxiv.org/abs/2405.09980v1</link><description>Accurate evaluation of financial question answering (QA) systems necessitatesa comprehensive dataset encompassing diverse question types and contexts.However, current financial QA datasets lack scope diversity and questioncomplexity. This work introduces FinTextQA, a novel dataset for long-formquestion answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,source-attributed QA pairs extracted and selected from finance textbooks andgovernment agency websites.Moreover, we developed a Retrieval-AugmentedGeneration (RAG)-based LFQA system, comprising an embedder, retriever,reranker, and generator. A multi-faceted evaluation approach, including humanranking, automatic metrics, and GPT-4 scoring, was employed to benchmark theperformance of different LFQA system configurations under heightened noisyconditions. The results indicate that: (1) Among all compared generators,Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) Themost effective system configuration on our dataset involved setting theembedder, retriever, reranker, and generator as Ada2, Automated MergedRetrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models areless susceptible to noise after the length of contexts reaching a specificthreshold.</description><author>Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, Junwei Liang</author><pubDate>Thu, 16 May 2024 11:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09980v1</guid></item><item><title>MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental Healthcare</title><link>http://arxiv.org/abs/2405.12619v1</link><description>Mental health disorders significantly impact people globally, regardless ofbackground, education, or socioeconomic status. However, access to adequatecare remains a challenge, particularly for underserved communities with limitedresources. Text mining tools offer immense potential to support mentalhealthcare by assisting professionals in diagnosing and treating patients. Thisstudy addresses the scarcity of Arabic mental health resources for developingsuch tools. We introduce MentalQA, a novel Arabic dataset featuringconversational-style question-and-answer (QA) interactions. To ensure dataquality, we conducted a rigorous annotation process using a well-defined schemawith quality control measures. Data was collected from a question-answeringmedical platform. The annotation schema for mental health questions andcorresponding answers draws upon existing classification schemes with somemodifications. Question types encompass six distinct categories: diagnosis,treatment, anatomy \&amp; physiology, epidemiology, healthy lifestyle, and providerchoice. Answer strategies include information provision, direct guidance, andemotional support. Three experienced annotators collaboratively annotated thedata to ensure consistency. Our findings demonstrate high inter-annotatoragreement, with Fleiss' Kappa of $0.61$ for question types and $0.98$ foranswer strategies. In-depth analysis revealed insightful patterns, includingvariations in question preferences across age groups and a strong correlationbetween question types and answer strategies. MentalQA offers a valuablefoundation for developing Arabic text mining tools capable of supporting mentalhealth professionals and individuals seeking information.</description><author>Hassan Alhuzali, Ashwag Alasmari, Hamad Alsaleh</author><pubDate>Tue, 21 May 2024 10:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12619v1</guid></item><item><title>Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering</title><link>http://arxiv.org/abs/2403.05217v1</link><description>Open-domain question answering (ODQA) has emerged as a pivotal researchspotlight in information systems. Existing methods follow two main paradigms tocollect evidence: (1) The \textit{retrieve-then-read} paradigm retrievespertinent documents from an external corpus; and (2) the\textit{generate-then-read} paradigm employs large language models (LLMs) togenerate relevant documents. However, neither can fully address multifacetedrequirements for evidence. To this end, we propose LLMQA, a generalizedframework that formulates the ODQA process into three basic steps: queryexpansion, document selection, and answer generation, combining the superiorityof both retrieval-based and generation-based evidence. Since LLMs exhibit theirexcellent capabilities to accomplish various tasks, we instruct LLMs to playmultiple roles as generators, rerankers, and evaluators within our framework,integrating them to collaborate in the ODQA process. Furthermore, we introducea novel prompt optimization algorithm to refine role-playing prompts and steerLLMs to produce higher-quality evidence and answers. Extensive experimentalresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate thatLLMQA achieves the best performance in terms of both answer accuracy andevidence quality, showcasing its potential for advancing ODQA research andapplications.</description><author>Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan</author><pubDate>Fri, 08 Mar 2024 11:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05217v1</guid></item></channel></rss>