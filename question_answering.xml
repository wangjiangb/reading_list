<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>JDocQA: Japanese Document Question Answering Dataset for Generative Language Models</title><link>http://arxiv.org/abs/2403.19454v1</link><description>Document question answering is a task of question answering on givendocuments such as reports, slides, pamphlets, and websites, and it is a trulydemanding task as paper and electronic forms of documents are so common in oursociety. This is known as a quite challenging task because it requires not onlytext understanding but also understanding of figures and tables, and hencevisual question answering (VQA) methods are often examined in addition totextual approaches. We introduce Japanese Document Question Answering (JDocQA),a large-scale document-based QA dataset, essentially requiring both visual andtextual information to answer questions, which comprises 5,504 documents in PDFformat and annotated 11,600 question-and-answer instances in Japanese. Each QAinstance includes references to the document pages and bounding boxes for theanswer clues. We incorporate multiple categories of questions and unanswerablequestions from the document for realistic question-answering applications. Weempirically evaluate the effectiveness of our dataset with text-based largelanguage models (LLMs) and multimodal models. Incorporating unanswerablequestions in finetuning may contribute to harnessing the so-calledhallucination generation.</description><author>Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe</author><pubDate>Thu, 28 Mar 2024 15:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19454v1</guid></item><item><title>Interactive Question Answering Systems: Literature Review</title><link>http://arxiv.org/abs/2209.01621v2</link><description>Question answering systems are recognized as popular and frequently effectivemeans of information seeking on the web. In such systems, information seekerscan receive a concise response to their query by presenting their questions innatural language. Interactive question answering is a recently proposed andincreasingly popular solution that resides at the intersection of questionanswering and dialogue systems. On the one hand, the user can ask questions innormal language and locate the actual response to her inquiry; on the otherhand, the system can prolong the question-answering session into a dialogue ifthere are multiple probable replies, very few, or ambiguities in the initialrequest. By permitting the user to ask more questions, interactive questionanswering enables users to dynamically interact with the system and receivemore precise results. This survey offers a detailed overview of the interactivequestion-answering methods that are prevalent in current literature. It beginsby explaining the foundational principles of question-answering systems, hencedefining new notations and taxonomies to combine all identified works inside aunified framework. The reviewed published work on interactivequestion-answering systems is then presented and examined in terms of itsproposed methodology, evaluation approaches, and dataset/application domain. Wealso describe trends surrounding specific tasks and issues raised by thecommunity, so shedding light on the future interests of scholars. Our work isfurther supported by a GitHub page with a synthesis of all the major topicscovered in this literature study.https://sisinflab.github.io/interactive-question-answering-systems-survey/</description><author>Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci</author><pubDate>Wed, 06 Mar 2024 21:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01621v2</guid></item><item><title>ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages</title><link>http://arxiv.org/abs/2403.17859v1</link><description>Question answering (QA) and Machine Reading Comprehension (MRC) tasks havesignificantly advanced in recent years due to the rapid development of deeplearning techniques and, more recently, large language models. At the sametime, many benchmark datasets have become available for QA and MRC tasks.However, most existing large-scale benchmark datasets have been createdpredominantly using synchronous document collections like Wikipedia or the Web.Archival document collections, such as historical newspapers, contain valuableinformation from the past that is still not widely used to train large languagemodels. To further contribute to advancing QA and MRC tasks and to overcome thelimitation of previous datasets, we introduce ChroniclingAmericaQA, alarge-scale dataset with 485K question-answer pairs created based on thehistorical newspaper collection Chronicling America. Our dataset is constructedfrom a subset of the Chronicling America newspaper collection spanning 120years. One of the significant challenges for utilizing digitized historicalnewspaper collections is the low quality of OCR text. Therefore, to enablerealistic testing of QA models, our dataset can be used in three differentways: answering questions from raw and noisy content, answering questions fromcleaner, corrected version of the content, as well as answering questions fromscanned images of newspaper pages. This and the fact that ChroniclingAmericaQAspans the longest time period among available QA datasets make it quite aunique and useful resource.</description><author>Bhawna Piryani, Jamshid Mozafari, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17859v1</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v3</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Thu, 07 Mar 2024 06:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v3</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v2</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Wed, 06 Mar 2024 12:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v2</guid></item><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Unsupervised LLM Adaptation for Question Answering</title><link>http://arxiv.org/abs/2402.12170v1</link><description>Large language models (LLM) learn diverse knowledge present in thelarge-scale training dataset via self-supervised training. Followed byinstruction-tuning, LLM acquires the ability to return correct information fordiverse questions. However, adapting these pre-trained LLMs to new targetdomains, such as different organizations or periods, for the question-answering(QA) task incurs a substantial annotation cost. To tackle this challenge, wepropose a novel task, unsupervised LLM adaptation for question answering. Inthis task, we leverage a pre-trained LLM, a publicly available QA dataset(source data), and unlabeled documents from the target domain. Our goal is tolearn LLM that can answer questions about the target domain. We introduce onesynthetic and two real datasets to evaluate models fine-tuned on the source andtarget data, and reveal intriguing insights; (i) fine-tuned models exhibit theability to provide correct answers for questions about the target domain eventhough they do not see any questions about the information described in theunlabeled documents, but (ii) they have difficulties in accessing informationlocated in the middle or at the end of documents, and (iii) this challenge canbe partially mitigated by replacing input tokens with random ones duringadaptation.</description><author>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</author><pubDate>Fri, 16 Feb 2024 06:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12170v1</guid></item><item><title>ArabicaQA: A Comprehensive Dataset for Arabic Question Answering</title><link>http://arxiv.org/abs/2403.17848v1</link><description>In this paper, we address the significant gap in Arabic natural languageprocessing (NLP) resources by introducing ArabicaQA, the first large-scaledataset for machine reading comprehension and open-domain question answering inArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701unanswerable questions created by crowdworkers to look similar to answerableones, along with additional labels of open-domain questions marks a crucialadvancement in Arabic NLP resources. We also present AraDPR, the first densepassage retrieval model trained on the Arabic Wikipedia corpus, specificallydesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,our study includes extensive benchmarking of large language models (LLMs) forArabic question answering, critically evaluating their performance in theArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarkingof LLMs in Arabic question answering offer significant advancements in thefield of Arabic NLP. The dataset and code are publicly accessible for furtherresearch https://github.com/DataScienceUIBK/ArabicaQA.</description><author>Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17848v1</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts</title><link>http://arxiv.org/abs/2404.08589v1</link><description>Visual question answering (VQA) is known as an AI-complete task as itrequires understanding, reasoning, and inferring about the vision and thelanguage content. Over the past few years, numerous neural architectures havebeen suggested for the VQA problem. However, achieving success in zero-shot VQAremains a challenge due to its requirement for advanced generalization andreasoning skills. This study explores the impact of incorporating imagecaptioning as an intermediary process within the VQA pipeline. Specifically, weexplore the efficacy of utilizing image captions instead of images andleveraging large language models (LLMs) to establish a zero-shot setting. Sinceimage captioning is the most crucial step in this process, we compare theimpact of state-of-the-art image captioning models on VQA performance acrossvarious question types in terms of structure and semantics. We propose astraightforward and efficient question-driven image captioning approach withinthis pipeline to transfer contextual information into the question-answering(QA) model. This method involves extracting keywords from the question,generating a caption for each image-question pair using the keywords, andincorporating the question-driven caption into the LLM prompt. We evaluate theefficacy of using general-purpose and question-driven image captions in the VQApipeline. Our study highlights the potential of employing image captions andharnessing the capabilities of LLMs to achieve competitive performance on GQAunder the zero-shot setting. Our code is available at\url{https://github.com/ovguyo/captions-in-VQA}.</description><author>Övgü Özdemir, Erdem Akagündüz</author><pubDate>Fri, 12 Apr 2024 17:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08589v1</guid></item><item><title>Quantum Neural Network with Density Matrix for Question Answering and Classical Image Classification</title><link>http://arxiv.org/abs/2203.11155v4</link><description>Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.</description><author>X. Q. Zhao, T. L. Chen</author><pubDate>Wed, 06 Mar 2024 08:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11155v4</guid></item><item><title>RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions</title><link>http://arxiv.org/abs/2403.10849v1</link><description>State-of-the-art KBQA models assume answerability of questions. Recentresearch has shown that while these can be adapted to detect unaswerabilitywith suitable training and thresholding, this comes at the expense of accuracyfor answerable questions, and no single model is able to handle all categoriesof unanswerability. We propose a new model for KBQA named RetinaQA that isrobust against unaswerability. It complements KB-traversal based logical formretrieval with sketch-filling based logical form construction. This helps withquestions that have valid logical forms but no data paths in the KB leading toan answer. Additionally, it uses discrimination instead of generation to betteridentify questions that do not have valid logical forms. We demonstrate thatRetinaQA significantly outperforms adaptations of state-of-the-art KBQA modelsacross answerable and unanswerable questions, while showing robustness acrossunanswerability categories. Remarkably, it also establishes a new state-of-theart for answerable KBQA by surpassing existing models</description><author>Prayushi Faldu, Indrajit Bhattacharya, Mausam</author><pubDate>Sat, 16 Mar 2024 09:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10849v1</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v3</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Sat, 17 Feb 2024 07:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v3</guid></item><item><title>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</title><link>http://arxiv.org/abs/2404.02022v1</link><description>In the era of large language models, applying techniques such as RetrievalAugmented Generation can better address Open-Domain Question-Answeringproblems. Due to constraints including model sizes and computing resources, thelength of context is often limited, and it becomes challenging to empower themodel to cover overlong contexts while answering questions from open domains.This paper proposes a general and convenient method to covering longer contextsin Open-Domain Question-Answering tasks. It leverages a small encoder languagemodel that effectively encodes contexts, and the encoding appliescross-attention with origin inputs. With our method, the origin language modelscan cover several times longer contexts while keeping the computingrequirements close to the baseline. Our experiments demonstrate that afterfine-tuning, there is improved performance across two held-in datasets, fourheld-out datasets, and also in two In Context Learning settings.</description><author>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</author><pubDate>Tue, 02 Apr 2024 16:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02022v1</guid></item><item><title>JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability</title><link>http://arxiv.org/abs/2402.17887v2</link><description>With the explosive growth of medical data and the rapid development ofartificial intelligence technology, precision medicine has emerged as a key toenhancing the quality and efficiency of healthcare services. In this context,Large Language Models (LLMs) play an increasingly vital role in medicalknowledge acquisition and question-answering systems. To further improve theperformance of these systems in the medical domain, we introduce an innovativemethod that jointly trains an Information Retrieval (IR) system and an LLMduring the fine-tuning phase. This approach, which we call Joint Medical LLMand Retrieval Training (JMLR), is designed to overcome the challenges faced bytraditional models in handling medical question-answering tasks. By employing asynchronized training mechanism, JMLR reduces the demand for computationalresources and enhances the model's ability to leverage medical knowledge forreasoning and answering questions. Our experimental results demonstrate thatJMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models usingconventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3%on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% onMedQA) significantly outperforms other public models (Meditron-7B: 50.1%,47.9%), proving its superiority in terms of cost (our training time: 37 hours,traditional method: 144 hours), efficiency, and effectiveness in medicalquestion-answering tasks. Through this work, we provide a new and efficientknowledge enhancement tool for healthcare, demonstrating the great potential ofintegrating IR and LLM training in precision medical information retrieval andquestion-answering systems.</description><author>Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu</author><pubDate>Sat, 02 Mar 2024 09:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17887v2</guid></item><item><title>Self-Improvement Programming for Temporal Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2404.01720v1</link><description>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questionswith temporal intent over Temporal Knowledge Graphs (TKGs). The core challengeof this task lies in understanding the complex semantic information regardingmultiple types of time constraints (e.g., before, first) in questions. Existingend-to-end methods implicitly model the time constraints by learning time-awareembeddings of questions and candidate answers, which is far from understandingthe question comprehensively. Motivated by semantic-parsing-based approachesthat explicitly model constraints in questions by generating logical forms withsymbolic operators, we design fundamental temporal operators for timeconstraints and introduce a novel self-improvement Programming method for TKGQA(Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability ofLarge Language Models (LLMs) to understand the combinatory time constraints inthe questions and generate corresponding program drafts with a few examplesgiven. Then, it aligns these drafts to TKGs with the linking module andsubsequently executes them to generate the answers. To enhance the ability tounderstand questions, Prog-TQA is further equipped with a self-improvementstrategy to effectively bootstrap LLMs using high-quality self-generateddrafts. Extensive experiments demonstrate the superiority of the proposedProg-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1metric.</description><author>Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin, Yongjun Xu</author><pubDate>Tue, 02 Apr 2024 09:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01720v1</guid></item><item><title>Causal Question Answering with Reinforcement Learning</title><link>http://arxiv.org/abs/2311.02760v2</link><description>Causal questions inquire about causal relationships between different eventsor phenomena. They are important for a variety of use cases, including virtualassistants and search engines. However, many current approaches to causalquestion answering cannot provide explanations or evidence for their answers.Hence, in this paper, we aim to answer causal questions with a causality graph,a large-scale dataset of causal relations between noun phrases along with therelations' provenance data. Inspired by recent, successful applications ofreinforcement learning to knowledge graph tasks, such as link prediction andfact-checking, we explore the application of reinforcement learning on acausality graph for causal question answering. We introduce anActor-Critic-based agent which learns to search through the graph to answercausal questions. We bootstrap the agent with a supervised learning procedureto deal with large action spaces and sparse rewards. Our evaluation shows thatthe agent successfully prunes the search space to answer binary causalquestions by visiting less than 30 nodes per question compared to over 3,000nodes by a naive breadth-first search. Our ablation study indicates that oursupervised learning strategy provides a strong foundation upon which ourreinforcement learning agent improves. The paths returned by our agent explainthe mechanisms by which a cause produces an effect. Moreover, for each edge ona path, our causality graph provides its original source allowing for easyverification of paths.</description><author>Lukas Blübaum, Stefan Heindorf</author><pubDate>Mon, 25 Mar 2024 09:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02760v2</guid></item><item><title>EEE-QA: Exploring Effective and Efficient Question-Answer Representations</title><link>http://arxiv.org/abs/2403.02176v1</link><description>Current approaches to question answering rely on pre-trained language models(PLMs) like RoBERTa. This work challenges the existing question-answer encodingconvention and explores finer representations. We begin with testing variouspooling methods compared to using the begin-of-sentence token as a questionrepresentation for better quality. Next, we explore opportunities tosimultaneously embed all answer candidates with the question. This enablescross-reference between answer choices and improves inference throughput viareduced memory usage. Despite their simplicity and effectiveness, these methodshave yet to be widely studied in current frameworks. We experiment withdifferent PLMs, and with and without the integration of knowledge graphs.Results prove that the memory efficacy of the proposed techniques with littlesacrifice in performance. Practically, our work enhances 38-100% throughputwith 26-65% speedups on consumer-grade GPUs by allowing for considerably largerbatch sizes. Our work sends a message to the community with promisingdirections in both representation quality and efficiency for thequestion-answering task in natural language processing.</description><author>Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen</author><pubDate>Mon, 04 Mar 2024 16:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02176v1</guid></item><item><title>CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</title><link>http://arxiv.org/abs/2403.03203v1</link><description>The integration of learning and reasoning is high on the research agenda inAI. Nevertheless, there is only a little attention to use existing backgroundknowledge for reasoning about partially observed scenes to answer questionsabout the scene. Yet, we as humans use such knowledge frequently to inferplausible answers to visual questions (by eliminating all inconsistent ones).Such knowledge often comes in the form of constraints about objects and ittends to be highly domain or environment-specific. We contribute a novelbenchmark called CLEVR-POC for reasoning-intensive visual question answering(VQA) in partially observable environments under constraints. In CLEVR-POC,knowledge in the form of logical constraints needs to be leveraged to generateplausible answers to questions about a hidden object in a given partial scene.For instance, if one has the knowledge that all cups are colored either red,green or blue and that there is only one green cup, it becomes possible todeduce the color of an occluded cup as either red or blue, provided that allother cups, including the green one, are observed. Through experiments, weobserve that the low performance of pre-trained vision language models likeCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POCascertains the necessity for frameworks that can handle reasoning-intensivetasks where environment-specific background knowledge is available and crucial.Furthermore, our demonstration illustrates that a neuro-symbolic model, whichintegrates an LLM like GPT-4 with a visual perception network and a formallogical reasoner, exhibits exceptional performance on CLEVR-POC.</description><author>Savitha Sam Abraham, Marjan Alirezaie, Luc De Raedt</author><pubDate>Tue, 05 Mar 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03203v1</guid></item><item><title>TraveLER: A Multi-LMM Agent Framework for Video Question-Answering</title><link>http://arxiv.org/abs/2404.01476v1</link><description>Recently, Large Multimodal Models (LMMs) have made significant progress invideo question-answering using a frame-wise approach by leveraging large-scale,image-based pretraining in a zero-shot manner. While image-based methods forvideos have shown impressive performance, a current limitation is that theyoften overlook how key timestamps are selected and cannot adjust when incorrecttimestamps are identified. Moreover, they are unable to extract detailsrelevant to the question, instead providing general descriptions of the frame.To overcome this, we design a multi-LMM agent framework that travels along thevideo, iteratively collecting relevant information from keyframes throughinteractive question-asking until there is sufficient information to answer thequestion. Specifically, we propose TraveLER, a model that can create a plan to"Traverse" through the video, ask questions about individual frames to "Locate"and store key information, and then "Evaluate" if there is enough informationto answer the question. Finally, if there is not enough information, our methodis able to "Replan" based on its collected knowledge. Through extensiveexperiments, we find that the proposed TraveLER approach improves performanceon several video question-answering benchmarks, such as NExT-QA, STAR, andPerception Test, without the need to fine-tune on specific datasets.</description><author>Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig</author><pubDate>Mon, 01 Apr 2024 21:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01476v1</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v2</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Tue, 20 Feb 2024 19:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v2</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v3</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Fri, 01 Mar 2024 15:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v3</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Multi-Image Visual Question Answering for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2404.07622v1</link><description>Unsupervised anomaly detection enables the identification of potentialpathological areas by juxtaposing original images with their pseudo-healthyreconstructions generated by models trained exclusively on normal images.However, the clinical interpretation of resultant anomaly maps presents achallenge due to a lack of detailed, understandable explanations. Recentadvancements in language models have shown the capability of mimickinghuman-like understanding and providing detailed descriptions. This raises aninteresting question: \textit{How can language models be employed to make theanomaly maps more explainable?} To the best of our knowledge, we are the firstto leverage a language model for unsupervised anomaly detection, for which weconstruct a dataset with different questions and answers. Additionally, wepresent a novel multi-image visual question answering framework tailored foranomaly detection, incorporating diverse feature fusion strategies to enhancevisual knowledge extraction. Our experiments reveal that the framework,augmented by our new Knowledge Q-Former module, adeptly answers questions onthe anomaly detection dataset. Besides, integrating anomaly maps as inputsdistinctly aids in improving the detection of unseen pathologies.</description><author>Jun Li, Cosmin I. Bercea, Philip Müller, Lina Felsner, Suhwan Kim, Daniel Rueckert, Benedikt Wiestler, Julia A. Schnabel</author><pubDate>Thu, 11 Apr 2024 11:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07622v1</guid></item><item><title>Question Answering Over Spatio-Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2402.11542v1</link><description>Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledgegraphs (KGs) by incorporating time and location information. While the researchcommunity's focus on Knowledge Graph Question Answering (KGQA), the field ofanswering questions incorporating both spatio-temporal information based onSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasetsalso has hindered progress in this area. To address this issue, we presentSTQAD, a dataset comprising 10,000 natural language questions forspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,various state-of-the-art KGQA approaches fall far short of achievingsatisfactory performance on our dataset. In response, we propose STCQA, a newspatio-temporal KGQA approach that utilizes a novel STKG embedding method namedSTComplEx. By extracting temporal and spatial information from a question, ourQA model can better comprehend the question and retrieve accurate answers fromthe STKG. Through extensive experiments, we demonstrate the quality of ourdataset and the effectiveness of our STKGQA method.</description><author>Xinbang Dai, Huiying Li, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11542v1</guid></item><item><title>Conversational Question Answering with Reformulations over Knowledge Graph</title><link>http://arxiv.org/abs/2312.17269v2</link><description>Conversational question answering (convQA) over knowledge graphs (KGs)involves answering multi-turn natural language questions about informationcontained in a KG. State-of-the-art methods of ConvQA often struggle withinexplicit question-answer pairs. These inputs are easy for human beings tounderstand given a conversation history, but hard for a machine to interpret,which can degrade ConvQA performance. To address this problem, we propose areinforcement learning (RL) based model, CornNet, which utilizes questionreformulations generated by large language models (LLMs) to improve ConvQAperformance. CornNet adopts a teacher-student architecture where a teachermodel learns question representations using human writing reformulations, and astudent model to mimic the teacher model's output via reformulations generatedby LLMs. The learned question representation is then used by an RL model tolocate the correct answer in a KG. Extensive experimental results show thatCornNet outperforms state-of-the-art convQA models.</description><author>Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, Hanghang Tong</author><pubDate>Fri, 29 Mar 2024 07:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17269v2</guid></item><item><title>EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings</title><link>http://arxiv.org/abs/2402.16040v1</link><description>This study introduces EHRNoteQA, a novel patient-specific question answeringbenchmark tailored for evaluating Large Language Models (LLMs) in clinicalenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of threemedical professionals has curated the dataset comprising 962 unique questions,each linked to a specific patient's EHR clinical notes. What makes EHRNoteQAdistinct from existing EHR-based benchmarks is as follows: Firstly, it is thefirst dataset to adopt a multi-choice question answering format, a designchoice that effectively evaluates LLMs with reliable scores in the context ofautomatic evaluation, compared to other formats. Secondly, it requires ananalysis of multiple clinical notes to answer a single question, reflecting thecomplex nature of real-world clinical decision-making where clinicians reviewextensive records of patient histories. Our comprehensive evaluation on variouslarge language models showed that their scores on EHRNoteQA correlate moreclosely with their performance in addressing real-world medical questionsevaluated by clinicians than their scores from other LLM benchmarks. Thisunderscores the significance of EHRNoteQA in evaluating LLMs for medicalapplications and highlights its crucial role in facilitating the integration ofLLMs into healthcare systems. The dataset will be made available to the publicunder PhysioNet credential access, promoting further research in this vitalfield.</description><author>Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Seunghyun Won, Edward Choi</author><pubDate>Sun, 25 Feb 2024 09:41:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16040v1</guid></item><item><title>Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval</title><link>http://arxiv.org/abs/2404.08359v1</link><description>In today's digital world, seeking answers to health questions on the Internetis a common practice. However, existing question answering (QA) systems oftenrely on using pre-selected and annotated evidence documents, thus making theminadequate for addressing novel questions. Our study focuses on the open-domainQA setting, where the key challenge is to first uncover relevant evidence inlarge knowledge bases. By utilizing the common retrieve-then-read QA pipelineand PubMed as a trustworthy collection of medical research documents, we answerhealth questions from three diverse datasets. We modify different retrievalsettings to observe their influence on the QA pipeline's performance, includingthe number of retrieved documents, sentence selection process, the publicationyear of articles, and their number of citations. Our results reveal thatcutting down on the amount of retrieved documents and favoring more recent andhighly cited documents can improve the final macro F1 score up to 10%. Wediscuss the results, highlight interesting examples, and outline challenges forfuture research, like managing evidence disagreement and crafting user-friendlyexplanations.</description><author>Juraj Vladika, Florian Matthes</author><pubDate>Fri, 12 Apr 2024 10:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08359v1</guid></item><item><title>On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension</title><link>http://arxiv.org/abs/2404.02800v1</link><description>Question Generation aims to automatically generate questions based on a giveninput provided as context. A controllable question generation scheme focuses ongenerating questions with specific attributes, allowing better control. In thisstudy, we propose a few-shot prompting strategy for controlling the generationof question-answer pairs from children's narrative texts. We aim to control twoattributes: the question's explicitness and underlying narrative elements. Withempirical evaluation, we show the effectiveness of controlling the generationprocess by employing few-shot prompting side by side with a reference model.Our experiments highlight instances where the few-shot strategy surpasses thereference model, particularly in scenarios such as semantic closenessevaluation and the diversity and coherency of question-answer pairs. However,these improvements are not always statistically significant. The code ispublicly available at github.com/bernardoleite/few-shot-prompting-qg-control.</description><author>Bernardo Leite, Henrique Lopes Cardoso</author><pubDate>Wed, 03 Apr 2024 16:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02800v1</guid></item><item><title>Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering</title><link>http://arxiv.org/abs/2403.09288v1</link><description>Scene-Text Visual Question Answering (ST-VQA) aims to understand scene textin images and answer questions related to the text content. Most existingmethods heavily rely on the accuracy of Optical Character Recognition (OCR)systems, and aggressive fine-tuning based on limited spatial locationinformation and erroneous OCR text information often leads to inevitableoverfitting. In this paper, we propose a multimodal adversarial trainingarchitecture with spatial awareness capabilities. Specifically, we introduce anAdversarial OCR Enhancement (AOE) module, which leverages adversarial trainingin the embedding space of OCR modality to enhance fault-tolerant representationof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, Weadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model bettercapture the spatial relationships among OCR tokens. Various experimentsdemonstrate that our method achieves significant performance improvements onboth the ST-VQA and TextVQA datasets and provides a novel paradigm formultimodal adversarial training.</description><author>Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li</author><pubDate>Thu, 14 Mar 2024 12:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09288v1</guid></item><item><title>PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title><link>http://arxiv.org/abs/2402.11034v1</link><description>Existing work on Temporal Question Answering (TQA) has predominantly focusedon questions anchored to specific timestamps or events (e.g. "Who was the USpresident in 1970?"). Little work has studied questions whose temporal contextis relative to the present time (e.g. "Who was the previous US president?"). Werefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA posesunique challenges: (1) large language models (LLMs) may have outdatedknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') arehard to reason, (3) multi-hop reasoning may be required, and (4) the goldanswers of benchmarks must be continuously updated. To address thesechallenges, we introduce the PAT-Questions benchmark, which includes single andmulti-hop temporal questions. The answers in PAT-Questions can be automaticallyrefreshed by re-running SPARQL queries on a knowledge graph, if available. Weevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model(TEMPREASON-T5) on PAT-Questions through direct prompting andretrieval-augmented generation (RAG). The results highlight the limitations ofexisting solutions in PATQA and motivate the need for new methods to improvePATQA reasoning capabilities.</description><author>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</author><pubDate>Fri, 16 Feb 2024 19:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11034v1</guid></item><item><title>VideoDistill: Language-aware Vision Distillation for Video Question Answering</title><link>http://arxiv.org/abs/2404.00973v1</link><description>Significant advancements in video question answering (VideoQA) have been madethanks to thriving large image-language pretraining frameworks. Although theseimage-language models can efficiently represent both video and languagebranches, they typically employ a goal-free vision perception process and donot interact vision with language well during the answer generation, thusomitting crucial visual cues. In this paper, we are inspired by the humanrecognition and learning pattern and propose VideoDistill, a framework withlanguage-aware (i.e., goal-driven) behavior in both vision perception andanswer generation process. VideoDistill generates answers only fromquestion-related visual embeddings and follows a thinking-observing-answeringapproach that closely resembles human behavior, distinguishing it from previousresearch. Specifically, we develop a language-aware gating mechanism to replacethe standard cross-attention, avoiding language's direct fusion into visualrepresentations. We incorporate this mechanism into two key components of theentire framework. The first component is a differentiable sparse samplingmodule, which selects frames containing the necessary dynamics and semanticsrelevant to the questions. The second component is a vision refinement modulethat merges existing spatial-temporal attention layers to ensure the extractionof multi-grained visual semantics associated with the questions. We conductexperimental evaluations on various challenging video question-answeringbenchmarks, and VideoDistill achieves state-of-the-art performance in bothgeneral and long-form VideoQA datasets. In Addition, we verify thatVideoDistill can effectively alleviate the utilization of language shortcutsolutions in the EgoTaskQA dataset.</description><author>Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao</author><pubDate>Mon, 01 Apr 2024 08:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00973v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering</title><link>http://arxiv.org/abs/2403.11129v1</link><description>Document-level Event Causality Identification (DECI) aims to identify causalrelations between two events in documents. Recent research tends to usepre-trained language models to generate the event causal relations. Whereas,these methods are prone to the errors of sequential generation due to multipleevents in a document. Moreover, the potential structures such as eventcoreference and related causal chain are neglected. In this paper, we propose amulti-task learning framework to enhance event causality identification withrationale and structure-aware causal question answering. Specifically, the DECItask is transformed into multiple-choice question answering, and the causes andeffects of the questioned event are generated with large language models. Inaddition, we generate the rationales to explain why these events have causalrelations. Moreover, we construct an event structure graph, which models themulti-hop potential relations for causal reasoning of the current event.Experiments on two benchmark datasets show the great advantages of our proposedapproach compared to the state-of-the-art methods. Moreover, we conduct bothquantitative and qualitative analyses, which shed light on why each componentof our approach can lead to great improvements.</description><author>Baiyan Zhang, Qin Chen, Jie Zhou, Jian Jin, Liang He</author><pubDate>Sun, 17 Mar 2024 08:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11129v1</guid></item><item><title>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</title><link>http://arxiv.org/abs/2402.13188v1</link><description>Many models that leverage knowledge graphs (KGs) have recently demonstratedremarkable success in question answering (QA) tasks. In the real world, manyfacts contained in KGs are time-constrained thus temporal KGQA has receivedincreasing attention. Despite the fruitful efforts of previous models intemporal KGQA, they still have several limitations. (I) They adopt pre-trainedlanguage models (PLMs) to obtain question representations, while PLMs tend tofocus on entity information and ignore entity transfer caused by temporalconstraints, and finally fail to learn specific temporal representations ofentities. (II) They neither emphasize the graph structure between entities norexplicitly model the multi-hop relationship in the graph, which will make itdifficult to solve complex multi-hop question answering. To alleviate thisproblem, we propose a novel Question Calibration and Multi-Hop Modeling(QC-MHM) approach. Specifically, We first calibrate the question representationby fusing the question and the time-constrained concepts in KG. Then, weconstruct the GNN layer to complete multi-hop message passing. Finally, thequestion representation is combined with the embedding output by the GNN togenerate the final prediction. Empirical results verify that the proposed modelachieves better performance than the state-of-the-art models in the benchmarkdataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestionsdataset's complex questions are absolutely improved by 5.1% and 1.2% comparedto the best-performing baseline. Moreover, QC-MHM can generate interpretableand trustworthy predictions.</description><author>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang</author><pubDate>Tue, 20 Feb 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13188v1</guid></item><item><title>Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check</title><link>http://arxiv.org/abs/2403.18243v1</link><description>Retrieval-Augmented Generation (RAG) aims to generate more reliable andaccurate responses, by augmenting large language models (LLMs) with theexternal vast and dynamic knowledge. Most previous work focuses on using RAGfor single-round question answering, while how to adapt RAG to the complexconversational setting wherein the question is interdependent on the precedingcontext is not well studied. In this paper, we propose a conversation-level RAGapproach, which incorporates fine-grained retrieval augmentation and self-checkfor conversational question answering (CQA). In particular, our approachconsists of three components, namely conversational question refiner,fine-grained retriever and self-check based response generator, which workcollaboratively for question understanding and relevant information acquisitionin conversational settings. Extensive experiments demonstrate the greatadvantages of our approach over the state-of-the-art baselines. Moreover, wealso release a Chinese CQA dataset with new features including reformulatedquestion, extracted keyword, retrieved paragraphs and their helpfulness, whichfacilitates further researches in RAG enhanced CQA.</description><author>Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He</author><pubDate>Wed, 27 Mar 2024 05:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18243v1</guid></item><item><title>ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search</title><link>http://arxiv.org/abs/2403.16702v1</link><description>Retrieval-based code question answering seeks to match user queries innatural language to relevant code snippets. Previous approaches typically relyon pretraining models using crafted bi-modal and uni-modal datasets to aligntext and code representations. In this paper, we introduce ProCQA, alarge-scale programming question answering dataset extracted from theStackOverflow community, offering naturally structured mixed-modal QA pairs. Tovalidate its effectiveness, we propose a modality-agnostic contrastivepre-training approach to improve the alignment of text and code representationsof current code language models. Compared to previous models that primarilyemploy bimodal and unimodal pairs extracted from CodeSearchNet forpre-training, our model exhibits significant performance improvements across awide range of code retrieval benchmarks.</description><author>Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong</author><pubDate>Mon, 25 Mar 2024 13:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16702v1</guid></item><item><title>TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction</title><link>http://arxiv.org/abs/2307.04642v2</link><description>When applied to open-domain question answering, large language models (LLMs)frequently generate incorrect responses based on made-up facts, which arecalled $\textit{hallucinations}$. Retrieval augmented generation (RAG) is apromising strategy to avoid hallucinations, but it does not provide guaranteeson its correctness. To address this challenge, we propose the TrustworthyRetrieval Augmented Question Answering, or $\textit{TRAQ}$, which provides thefirst end-to-end statistical correctness guarantee for RAG. TRAQ uses conformalprediction, a statistical technique for constructing prediction sets that areguaranteed to contain the semantically correct response with high probability.Additionally, TRAQ leverages Bayesian optimization to minimize the size of theconstructed sets. In an extensive experimental evaluation, we demonstrate thatTRAQ provides the desired correctness guarantee while reducing prediction setsize by 16.2% on average compared to an ablation. The implementation isavailable at $\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.</description><author>Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani</author><pubDate>Fri, 05 Apr 2024 21:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04642v2</guid></item><item><title>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering</title><link>http://arxiv.org/abs/2404.04007v1</link><description>Compositional spatio-temporal reasoning poses a significant challenge in thefield of video question answering (VideoQA). Existing approaches struggle toestablish effective symbolic reasoning structures, which are crucial foranswering compositional spatio-temporal questions. To address this challenge,we propose a neural-symbolic framework called Neural-Symbolic VideoQA(NS-VideoQA), specifically designed for real-world VideoQA tasks. Theuniqueness and superiority of NS-VideoQA are two-fold: 1) It proposes a SceneParser Network (SPN) to transform static-dynamic video scenes into SymbolicRepresentation (SR), structuralizing persons, objects, relations, and actionchronologies. 2) A Symbolic Reasoning Machine (SRM) is designed for top-downquestion decompositions and bottom-up compositional reasonings. Specifically, apolymorphic program executor is constructed for internally consistent reasoningfrom SR to the final answer. As a result, Our NS-VideoQA not only improves thecompositional spatio-temporal reasoning in real-world VideoQA task, but alsoenables step-by-step error analysis by tracing the intermediate results.Experimental evaluations on the AGQA Decomp benchmark demonstrate theeffectiveness of the proposed NS-VideoQA framework. Empirical studies furtherconfirm that NS-VideoQA exhibits internal consistency in answeringcompositional questions and significantly improves the capability ofspatio-temporal and logical inference for VideoQA tasks.</description><author>Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang</author><pubDate>Fri, 05 Apr 2024 11:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04007v1</guid></item><item><title>BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining</title><link>http://arxiv.org/abs/2401.06443v2</link><description>The current research direction in generative models, such as the recentlydeveloped GPT4, aims to find relevant knowledge information for multimodal andmultilingual inputs to provide answers. Under these research circumstances, thedemand for multilingual evaluation of visual question answering (VQA) tasks, arepresentative task of multimodal systems, has increased. Accordingly, wepropose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study thatcan be extended to multilingualism. The proposed data include 17K images, 17Kquestion-answer pairs for both Korean and English and 280K instances ofknowledge information related to question-answer content. We also present aframework that can effectively inject knowledge information into a VQA systemby pretraining the knowledge information of BOK-VQA data in the form of graphembeddings. Finally, through in-depth analysis, we demonstrated the actualeffect of the knowledge information contained in the constructed training dataon VQA.</description><author>Minjun Kim, Seungwoo Song, Youhan Lee, Haneol Jang, Kyungtae Lim</author><pubDate>Fri, 15 Mar 2024 08:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06443v2</guid></item><item><title>Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models</title><link>http://arxiv.org/abs/2403.19631v1</link><description>Large Language Models (LLMs) have shown proficiency in question-answeringtasks but often struggle to integrate real-time knowledge updates, leading topotentially outdated or inaccurate responses. This problem becomes even morechallenging when dealing with multi-hop questions since they require LLMs toupdate and integrate multiple knowledge pieces relevant to the questions. Totackle the problem, we propose the Retrieval-Augmented model Editing (RAE)framework tailored for multi-hop question answering. RAE first retrieves editedfacts and then refines the language model through in-context learning.Specifically, our retrieval approach, based on mutual information maximization,leverages the reasoning abilities of LLMs to identify chain facts that na\"ivesimilarity-based searches might miss. Additionally, our framework incorporatesa pruning strategy to eliminate redundant information from the retrieved facts,which enhances the editing accuracy and mitigates the hallucination problem.Our framework is supported by theoretical justification for its fact retrievalefficacy. Finally, comprehensive evaluation across various LLMs validates RAE'sability in providing accurate answers with updated knowledge.</description><author>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</author><pubDate>Thu, 28 Mar 2024 18:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19631v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering</title><link>http://arxiv.org/abs/2403.17647v2</link><description>The large success of deep learning based methods in Visual Question Answering(VQA) has concurrently increased the demand for explainable methods. Mostmethods in Explainable Artificial Intelligence (XAI) focus on generatingpost-hoc explanations rather than taking an intrinsic approach, the lattercharacterizing an interpretable model. In this work, we introduce aninterpretable approach for graph-based VQA and demonstrate competitiveperformance on the GQA dataset. This approach bridges the gap betweeninterpretability and performance. Our model is designed to intrinsicallyproduce a subgraph during the question-answering process as its explanation,providing insight into the decision making. To evaluate the quality of thesegenerated subgraphs, we compare them against established post-hocexplainability methods for graph neural networks, and perform a humanevaluation. Moreover, we present quantitative metrics that correlate with theevaluations of human assessors, acting as automatic metrics for the generatedexplanatory subgraphs. Our implementation is available athttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.</description><author>Pascal Tilli, Ngoc Thang Vu</author><pubDate>Wed, 27 Mar 2024 11:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17647v2</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v2</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, etc. Our proposed framework, named PromptGIP, unifiesthese diverse image processing tasks within a universal framework. Inspired byNLP question answering (QA) techniques, we employ a visual prompting questionanswering paradigm. Specifically, we treat the input-output image pair as astructured question-answer sentence, thereby reprogramming the image processingtask as a prompting QA problem. PromptGIP can undertake diverse cross-domaintasks using provided visual prompts, eliminating the need for task-specificfinetuning. Our methodology offers a universal and adaptive solution to generalimage processing. While PromptGIP has demonstrated a certain degree ofout-of-domain task generalization capability, further research is expected tofully explore its more powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Wed, 21 Feb 2024 03:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v2</guid></item><item><title>Faithful Temporal Question Answering over Heterogeneous Sources</title><link>http://arxiv.org/abs/2402.15400v1</link><description>Temporal question answering (QA) involves time constraints, with phrases suchas "... in 2019" or "... before COVID". In the former, time is an explicitcondition, in the latter it is implicit. State-of-the-art methods havelimitations along three dimensions. First, with neural inference, timeconstraints are merely soft-matched, giving room to invalid or inexplicableanswers. Second, questions with implicit time are poorly supported. Third,answers come from a single source: either a knowledge base (KB) or a textcorpus. We propose a temporal QA system that addresses these shortcomings.First, it enforces temporal constraints for faithful answering with tangibleevidence. Second, it properly handles implicit questions. Third, it operatesover heterogeneous sources, covering KB, text and web tables in a unifiedmanner. The method has three stages: (i) understanding the question and itstemporal conditions, (ii) retrieving evidence from all sources, and (iii)faithfully answering the question. As implicit questions are sparse in priorbenchmarks, we introduce a principled method for generating diverse questions.Experiments show superior performance over a suite of baselines.</description><author>Zhen Jia, Philipp Christmann, Gerhard Weikum</author><pubDate>Fri, 23 Feb 2024 16:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15400v1</guid></item><item><title>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</title><link>http://arxiv.org/abs/2403.04640v1</link><description>This paper focuses on the challenge of answering questions in scenarios thatare composed of rich and complex dynamic audio-visual components. Althoughexisting Multimodal Large Language Models (MLLMs) can respond to audio-visualcontent, these responses are sometimes ambiguous and fail to describe specificaudio-visual events. To overcome this limitation, we introduce the CAT, whichenhances MLLM in three ways: 1) besides straightforwardly bridging audio andvideo, we design a clue aggregator that aggregates question-related clues indynamic audio-visual scenarios to enrich the detailed knowledge required forlarge language models. 2) CAT is trained on a mixed multimodal dataset,allowing direct application in audio-visual scenarios. Notably, we collect anaudio-visual joint instruction dataset named AVinstruct, to further enhance thecapacity of CAT to model cross-semantic correlations. 3) we propose AI-assistedambiguity-aware direct preference optimization, a strategy specialized inretraining the model to favor the non-ambiguity response and improve theability to localize specific audio-visual objects. Extensive experimentalresults demonstrate that CAT outperforms existing methods on multimodal tasks,especially in Audio-Visual Question Answering (AVQA) tasks. The codes and thecollected instructions are released at https://github.com/rikeilong/Bay-CAT.</description><author>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</author><pubDate>Thu, 07 Mar 2024 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04640v1</guid></item><item><title>ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</title><link>http://arxiv.org/abs/2209.08199v2</link><description>We present a new task and dataset, ScreenQA, for screen content understandingvia question answering. The existing screen datasets are focused either onstructure and component-level understanding, or on a much higher-levelcomposite task such as navigation and task completion. We attempt to bridge thegap between these two by annotating 86K question-answer pairs over the RICOdataset in hope to benchmark the screen reading comprehension capacity.</description><author>Yu-Chung Hsiao, Fedir Zubach, Maria Wang, Jindong Chen</author><pubDate>Thu, 22 Feb 2024 08:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08199v2</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v1</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 16 Feb 2024 20:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v1</guid></item><item><title>TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</title><link>http://arxiv.org/abs/2404.03574v1</link><description>Traditional machine learning models often require powerful hardware, makingthem unsuitable for deployment on resource-limited devices. Tiny MachineLearning (tinyML) has emerged as a promising approach for running machinelearning models on these devices, but integrating multiple data modalities intotinyML models still remains a challenge due to increased complexity, latency,and power consumption. This paper proposes TinyVQA, a novel multimodal deepneural network for visual question answering tasks that can be deployed onresource-constrained tinyML hardware. TinyVQA leverages a supervisedattention-based model to learn how to answer questions about images using bothvision and language modalities. Distilled knowledge from the supervisedattention-based VQA model trains the memory aware compact TinyVQA model and lowbit-width quantization technique is employed to further compress the model fordeployment on tinyML devices. The TinyVQA model was evaluated on the FloodNetdataset, which is used for post-disaster damage assessment. The compact modelachieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA forreal-world applications. Additionally, the model was deployed on a Crazyflie2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA modelachieved low latencies of 56 ms and consumes 693 mW power while deployed on thetiny drone, showcasing its suitability for resource-constrained embeddedsystems.</description><author>Hasib-Al Rashid, Argho Sarkar, Aryya Gangopadhyay, Maryam Rahnemoonfar, Tinoosh Mohsenin</author><pubDate>Thu, 04 Apr 2024 17:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03574v1</guid></item><item><title>Multi-Frame, Lightweight &amp; Efficient Vision-Language Models for Question Answering in Autonomous Driving</title><link>http://arxiv.org/abs/2403.19838v1</link><description>Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) havebecome prominent in autonomous driving research, as these models can provideinterpretable textual reasoning and responses for end-to-end autonomous drivingsafety tasks using traffic scene images and other data modalities. However,current approaches to these systems use expensive large language model (LLM)backbones and image encoders, making such systems unsuitable for real-timeautonomous driving systems where tight memory constraints exist and fastinference time is necessary. To address these previous issues, we developEM-VLM4AD, an efficient, lightweight, multi-frame vision language model whichperforms Visual Question Answering for autonomous driving. In comparison toprevious approaches, EM-VLM4AD requires at least 10 times less memory andfloating point operations, while also achieving higher BLEU-4, METEOR, CIDEr,and ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4ADalso exhibits the ability to extract relevant information from traffic viewsrelated to prompts and can answer questions for various autonomous drivingsubtasks. We release our code to train and evaluate our model athttps://github.com/akshaygopalkr/EM-VLM4AD.</description><author>Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi</author><pubDate>Thu, 28 Mar 2024 22:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19838v1</guid></item><item><title>KazQAD: Kazakh Open-Domain Question Answering Dataset</title><link>http://arxiv.org/abs/2404.04487v1</link><description>We introduce KazQAD -- a Kazakh open-domain question answering (ODQA) dataset-- that can be used in both reading comprehension and full ODQA settings, aswell as for information retrieval experiments. KazQAD contains just under 6,000unique questions with extracted short answers and nearly 12,000 passage-levelrelevance judgements. We use a combination of machine translation, Wikipediasearch, and in-house manual annotation to ensure annotation efficiency and dataquality. The questions come from two sources: translated items from the NaturalQuestions (NQ) dataset (only for training) and the original Kazakh UnifiedNational Testing (UNT) exam (for development and testing). The accompanyingtext corpus contains more than 800,000 passages from the Kazakh Wikipedia. As asupplementary dataset, we release around 61,000 question-passage-answer triplesfrom the NQ dataset that have been machine-translated into Kazakh. We developbaseline retrievers and readers that achieve reasonable scores in retrieval(NDCG@10 = 0.389 MRR = 0.382), reading comprehension (EM = 38.5 F1 = 54.2), andfull ODQA (EM = 17.8 F1 = 28.7) settings. Nevertheless, these results aresubstantially lower than state-of-the-art results for English QA collections,and we think that there should still be ample room for improvement. We alsoshow that the current OpenAI's ChatGPTv3.5 is not able to answer KazQAD testquestions in the closed-book setting with acceptable quality. The dataset isfreely available under the Creative Commons licence (CC BY-SA) athttps://github.com/IS2AI/KazQAD.</description><author>Rustem Yeshpanov, Pavel Efimov, Leonid Boytsov, Ardak Shalkarbayuli, Pavel Braslavski</author><pubDate>Sat, 06 Apr 2024 04:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04487v1</guid></item><item><title>End-to-End Beam Retrieval for Multi-Hop Question Answering</title><link>http://arxiv.org/abs/2308.08973v2</link><description>Multi-hop question answering (QA) involves finding multiple relevant passagesand step-by-step reasoning to answer complex questions, indicating aretrieve-and-read paradigm. However, previous retrievers were customized fortwo-hop questions, and most of them were trained separately across differenthops, resulting in a lack of supervision over the entire multi-hop retrievalprocess and leading to poor performance in complicated scenarios beyond twohops. In this work, we introduce Beam Retrieval, an end-to-end beam retrievalframework for multi-hop QA. This approach models the multi-hop retrievalprocess in an end-to-end manner by jointly optimizing an encoder and twoclassification heads across all hops. Moreover, Beam Retrieval maintainsmultiple partial hypotheses of relevant passages at each step, expanding thesearch space and reducing the risk of missing relevant passages. To establish acomplete QA system, we incorporate a supervised reader or a large languagemodel (LLM). Experimental results demonstrate that Beam Retrieval achieves anearly 50% improvement compared with baselines on challenging MuSiQue-Ans, andit also surpasses all previous retrievers on HotpotQA and achieves 99.9%precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrievalhelps our supervised reader achieve new state-of-the-art performance andsubstantially improves the few-shot QA performance of LLMs.</description><author>Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, Shen Huang</author><pubDate>Mon, 01 Apr 2024 09:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08973v2</guid></item><item><title>MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering</title><link>http://arxiv.org/abs/2403.19116v1</link><description>In today's fast-paced industry, professionals face the challenge ofsummarizing a large number of documents and extracting vital information fromthem on a daily basis. These metrics are frequently hidden away in tablesand/or their nested hyperlinks. To address this challenge, the approach ofTable Question Answering (QA) has been developed to extract the relevantinformation. However, traditional Table QA training tasks that provide a tableand an answer(s) from a gold cell coordinate(s) for a question may not alwaysensure extracting the accurate answer(s). Recent advancements in Large LanguageModels (LLMs) have opened up new possibilities for extracting information fromtabular data using prompts. In this paper, we introduce the Multi-hop Few-shotOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. Thefirst step involves Few-Shot Learning (FSL), where relevant tables andassociated contexts of hyperlinks are retrieved based on a given question. Theretrieved content is then used to construct few-shot prompts as inputs to anLLM, such as ChatGPT. To tackle the challenge of answering complex questions,the second step leverages Chain-of-thought (CoT) prompting to decompose thecomplex question into a sequential chain of questions and reasoning thoughts ina multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this processby retrieving relevant tables and contexts of hyperlinks that are relevant tothe resulting reasoning thoughts and questions. These additional contexts arethen used to supplement the prompt used in the first step, resulting in moreaccurate answers from an LLM. Empirical results from OTT-QA demonstrate thatour abstractive QA approach significantly improves the accuracy of extractiveTable QA methods.</description><author>Che Guan, Mengyu Huang, Peng Zhang</author><pubDate>Thu, 28 Mar 2024 04:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19116v1</guid></item><item><title>Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering</title><link>http://arxiv.org/abs/2402.12728v1</link><description>Knowledge-based visual question answering (KVQA) has been extensively studiedto answer visual questions with external knowledge, e.g., knowledge graphs(KGs). While several attempts have been proposed to leverage large languagemodels (LLMs) as an implicit knowledge source, it remains challenging sinceLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,images, KGs and LLMs, cannot be readily aligned for complex scenarios. Totackle these, we present a novel modality-aware integration with LLMs for KVQA(MAIL). It carefully leverages multimodal knowledge for both imageunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stageprompting strategy with LLMs to densely embody the image into a scene graphwith detailed visual features; (ii) We construct a coupled concept graph bylinking the mentioned entities with external facts. (iii) A tailoredpseudo-siamese graph medium fusion is designed for sufficient multimodalfusion. We utilize the shared mentioned entities in two graphs as mediums tobridge a tight inter-modal exchange, while maximally preserving insightfulintra-modal learning by constraining the fusion within mediums. Extensiveexperiments on two benchmark datasets show the superiority of MAIL with 24xless resources.</description><author>Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang</author><pubDate>Tue, 20 Feb 2024 05:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12728v1</guid></item><item><title>Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering</title><link>http://arxiv.org/abs/2309.08469v2</link><description>Modern open-domain question answering systems often rely on accurate andefficient retrieval components to find passages containing the facts necessaryto answer the question. Recently, neural retrievers have gained popularity overlexical alternatives due to their superior performance. However, most of thework concerns popular languages such as English or Chinese. For others, such asPolish, few models are available. In this work, we present Silver Retriever, aneural retriever for Polish trained on a diverse collection of manually orweakly labeled datasets. Silver Retriever achieves much better results thanother Polish models and is competitive with larger multilingual models.Together with the model, we open-source five new passage retrieval datasets.</description><author>Piotr Rybak, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08469v2</guid></item><item><title>CommVQA: Situating Visual Question Answering in Communicative Contexts</title><link>http://arxiv.org/abs/2402.15002v1</link><description>Current visual question answering (VQA) models tend to be trained andevaluated on image-question pairs in isolation. However, the questions peopleask are dependent on their informational needs and prior knowledge about theimage content. To evaluate how situating images within naturalistic contextsshapes visual questions, we introduce CommVQA, a VQA dataset consisting ofimages, image descriptions, real-world communicative scenarios where the imagemight appear (e.g., a travel website), and follow-up questions and answersconditioned on the scenario. We show that CommVQA poses a challenge for currentmodels. Providing contextual information to VQA models improves performancebroadly, highlighting the relevance of situating systems within a communicativescenario.</description><author>Nandita Shankar Naik, Christopher Potts, Elisa Kreiss</author><pubDate>Thu, 22 Feb 2024 22:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15002v1</guid></item><item><title>SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering</title><link>http://arxiv.org/abs/2401.13463v2</link><description>Spoken Question Answering (SQA) is essential for machines to reply to user'squestion by finding the answer span within a given spoken passage. SQA has beenpreviously achieved without ASR to avoid recognition errors andOut-of-Vocabulary (OOV) problems. However, the real-world problem ofOpen-domain SQA (openSQA), in which the machine needs to first retrievepassages that possibly contain the answer from a spoken archive in addition,was never considered. This paper proposes the first known end-to-end framework,Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of theopenSQA problem. SpeechDPR learns a sentence-level semantic representation bydistilling knowledge from the cascading model of unsupervised ASR (UASR) andtext dense retriever (TDR). No manually transcribed speech data is needed.Initial experiments showed performance comparable to the cascading model ofUASR and TDR, and significantly better when UASR was poor, verifying thisapproach is more robust to speech recognition errors.</description><author>Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee</author><pubDate>Mon, 18 Mar 2024 07:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13463v2</guid></item><item><title>Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</title><link>http://arxiv.org/abs/2404.01652v1</link><description>Open-domain Question Answering (OpenQA) aims at answering factual questionswith an external large-scale knowledge corpus. However, real-world knowledge isnot static; it updates and evolves continually. Such a dynamic characteristicof knowledge poses a vital challenge for these models, as the trained modelsneed to constantly adapt to the latest information to make sure that theanswers remain accurate. In addition, it is still unclear how well an OpenQAmodel can transfer to completely new knowledge domains. In this paper, weinvestigate the generalization performance of a retrieval-augmented QA model intwo specific scenarios: 1) adapting to updated versions of the same knowledgecorpus; 2) switching to completely different knowledge domains. We observe thatthe generalization challenges of OpenQA models stem from the reader'sover-reliance on memorizing the knowledge from the external corpus, whichhinders the model from generalizing to a new knowledge corpus. We introduceCorpus-Invariant Tuning (CIT), a simple but effective training strategy, tomitigate the knowledge over-memorization by controlling the likelihood ofretrieved contexts during training. Extensive experimental results on multipleOpenQA benchmarks show that CIT achieves significantly better generalizabilitywithout compromising the model's performance in its original corpus and domain.</description><author>Zixuan Zhang, Revanth Gangi Reddy, Kevin Small, Tong Zhang, Heng Ji</author><pubDate>Tue, 02 Apr 2024 06:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01652v1</guid></item><item><title>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering</title><link>http://arxiv.org/abs/2402.16313v1</link><description>Open-ended question answering requires models to find appropriate evidence toform well-reasoned, comprehensive and helpful answers. In practicalapplications, models also need to engage in extended discussions on potentialscenarios closely relevant to the question. With augmentation of retrievalmodule, open-source Large Language Models (LLMs) can produce coherent answersoften with different focuses, but are still sub-optimal in terms of reliableevidence selection and in-depth question analysis. In this paper, we propose anovel Chain-of-Discussion framework to leverage the synergy among multipleopen-source LLMs aiming to provide \textbf{more correct} and \textbf{morecomprehensive} answers for open-ended QA, although they are not strong enoughindividually. Our experiments show that discussions among multiple LLMs play avital role in enhancing the quality of answers. We release our data and code at\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.</description><author>Mingxu Tao, Dongyan Zhao, Yansong Feng</author><pubDate>Mon, 26 Feb 2024 05:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16313v1</guid></item><item><title>Answering Diverse Questions via Text Attached with Key Audio-Visual Clues</title><link>http://arxiv.org/abs/2403.06679v1</link><description>Audio-visual question answering (AVQA) requires reference to video contentand auditory information, followed by correlating the question to predict themost precise answer. Although mining deeper layers of audio-visual informationto interact with questions facilitates the multimodal fusion process, theredundancy of audio-visual parameters tends to reduce the generalization of theinference engine to multiple question-answer pairs in a single video. Indeed,the natural heterogeneous relationship between audiovisuals and text makes theperfect fusion challenging, to prevent high-level audio-visual semantics fromweakening the network's adaptability to diverse question types, we propose aframework for performing mutual correlation distillation (MCD) to aid questioninference. MCD is divided into three main steps: 1) firstly, the residualstructure is utilized to enhance the audio-visual soft associations based onself-attention, then key local audio-visual features relevant to the questioncontext are captured hierarchically by shared aggregators and coupled in theform of clues with specific question vectors. 2) Secondly, knowledgedistillation is enforced to align audio-visual-text pairs in a shared latentspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visualdependencies are decoupled by discarding the decision-level integrations. Weevaluate the proposed method on two publicly available datasets containingmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments showthat our method outperforms other state-of-the-art methods, and one interestingfinding behind is that removing deep audio-visual features during inference caneffectively mitigate overfitting. The source code is released athttp://github.com/rikeilong/MCD-forAVQA.</description><author>Qilang Ye, Zitong Yu, Xin Liu</author><pubDate>Mon, 11 Mar 2024 13:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06679v1</guid></item><item><title>Can I Trust Your Answer? Visually Grounded Video Question Answering</title><link>http://arxiv.org/abs/2309.01327v2</link><description>We study visually grounded VideoQA in response to the emerging trends ofutilizing pretraining techniques for video-language understanding.Specifically, by forcing vision-language models (VLMs) to answer questions andsimultaneously provide visual evidence, we seek to ascertain the extent towhich the predictions of such techniques are genuinely anchored in relevantvideo content, versus spurious correlations from language or irrelevant visualcontext. Towards this, we construct NExT-GQA -- an extension of NExT-QA with10.5$K$ temporal grounding (or location) labels tied to the original QA pairs.With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Throughpost-hoc attention analysis, we find that these models are extremely weak insubstantiating the answers despite their strong QA performance. This exposesthe limitation of current VLMs in making reliable predictions. As a remedy, wefurther explore and propose a grounded-QA method via Gaussian mask optimizationand cross-modal learning. Experiments with different backbones demonstrate thatthis grounding mechanism improves both grounding and QA. With these efforts, weaim to push towards trustworthy VLMs in VQA systems. Our dataset and code areavailable at https://github.com/doc-doc/NExT-GQA.</description><author>Junbin Xiao, Angela Yao, Yicong Li, Tat Seng Chua</author><pubDate>Sat, 30 Mar 2024 07:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01327v2</guid></item><item><title>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</title><link>http://arxiv.org/abs/2402.11161v1</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current answer correctness (AC) metrics do not align with human judgments,particularly verbose, free form answers from large language models (LLM). Thereare two challenges: a lack of data and that models are too big. LLM basedscorers correlate better with humans, but this expensive task has only beentested on limited QA datasets. We rectify these issues by providing clearguidelines for evaluating machine QA adopted from human QA contests. We alsointroduce Precise ANswer correctness Determination and Adjudication (PANDA), asmall, efficient, deterministic AC classifier (812 KB) that more accuratelyevaluates answer correctness.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</author><pubDate>Sat, 17 Feb 2024 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11161v1</guid></item><item><title>Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education</title><link>http://arxiv.org/abs/2402.14293v1</link><description>In the domain of Natural Language Processing (NLP), Large Language Models(LLMs) have demonstrated promise in text-generation tasks. However, theireducational applications, particularly for domain-specific queries, remainunderexplored. This study investigates LLMs' capabilities in educationalscenarios, focusing on concept graph recovery and question-answering (QA). Weassess LLMs' zero-shot performance in creating domain-specific concept graphsand introduce TutorQA, a new expert-verified NLP-focused benchmark forscientific graph reasoning and QA. TutorQA consists of five tasks with 500 QApairs. To tackle TutorQA queries, we present CGLLM, a pipeline integratingconcept graphs with LLMs for answering diverse questions. Our results indicatethat LLMs' zero-shot concept graph recovery is competitive with supervisedmethods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMsachieve up to 26% F1 score enhancement. Moreover, human evaluation and analysisshow that CGLLM generates answers with more fine-grained concepts.</description><author>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</author><pubDate>Thu, 22 Feb 2024 05:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14293v1</guid></item><item><title>Generator-Retriever-Generator Approach for Open-Domain Question Answering</title><link>http://arxiv.org/abs/2307.11278v3</link><description>Open-domain question answering (QA) tasks usually require the retrieval ofrelevant information from a large corpus to generate accurate answers. Wepropose a novel approach called Generator-Retriever-Generator (GRG) thatcombines document retrieval techniques with a large language model (LLM), byfirst prompting the model to generate contextual documents based on a givenquestion. In parallel, a dual-encoder network retrieves documents that arerelevant to the question from an external corpus. The generated and retrieveddocuments are then passed to the second LLM, which generates the final answer.By combining document retrieval and LLM generation, our approach addresses thechallenges of open-domain QA, such as generating informative and contextuallyrelevant answers. GRG outperforms the state-of-the-art generate-then-read andretrieve-then-read pipelines (GENREAD and RFiD) improving their performance byat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,respectively. We provide code, datasets, and checkpoints athttps://github.com/abdoelsayed2016/GRG.</description><author>Abdelrahman Abdallah, Adam Jatowt</author><pubDate>Tue, 26 Mar 2024 17:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11278v3</guid></item><item><title>Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering</title><link>http://arxiv.org/abs/2403.05217v1</link><description>Open-domain question answering (ODQA) has emerged as a pivotal researchspotlight in information systems. Existing methods follow two main paradigms tocollect evidence: (1) The \textit{retrieve-then-read} paradigm retrievespertinent documents from an external corpus; and (2) the\textit{generate-then-read} paradigm employs large language models (LLMs) togenerate relevant documents. However, neither can fully address multifacetedrequirements for evidence. To this end, we propose LLMQA, a generalizedframework that formulates the ODQA process into three basic steps: queryexpansion, document selection, and answer generation, combining the superiorityof both retrieval-based and generation-based evidence. Since LLMs exhibit theirexcellent capabilities to accomplish various tasks, we instruct LLMs to playmultiple roles as generators, rerankers, and evaluators within our framework,integrating them to collaborate in the ODQA process. Furthermore, we introducea novel prompt optimization algorithm to refine role-playing prompts and steerLLMs to produce higher-quality evidence and answers. Extensive experimentalresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate thatLLMQA achieves the best performance in terms of both answer accuracy andevidence quality, showcasing its potential for advancing ODQA research andapplications.</description><author>Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan</author><pubDate>Fri, 08 Mar 2024 11:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05217v1</guid></item><item><title>Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian</title><link>http://arxiv.org/abs/2404.08617v1</link><description>In this paper, we focus on generating a synthetic question answering (QA)dataset using an adapted Translate-Align-Retrieve method. Using this method, wecreated the largest Serbian QA dataset of more than 87K samples, which we nameSQuAD-sr. To acknowledge the script duality in Serbian, we generated bothCyrillic and Latin versions of the dataset. We investigate the dataset qualityand use it to fine-tune several pre-trained QA models. Best results wereobtained by fine-tuning the BERTi\'c model on our Latin SQuAD-sr dataset,achieving 73.91% Exact Match and 82.97% F1 score on the benchmark XQuADdataset, which we translated into Serbian for the purpose of evaluation. Theresults show that our model exceeds zero-shot baselines, but fails to go beyondhuman performance. We note the advantage of using a monolingual pre-trainedmodel over multilingual, as well as the performance increase gained by usingLatin over Cyrillic. By performing additional analysis, we show that questionsabout numeric values or dates are more likely to be answered correctly thanother types of questions. Finally, we conclude that SQuAD-sr is of sufficientquality for fine-tuning a Serbian QA model, in the absence of a manuallycrafted and annotated dataset.</description><author>Aleksa Cvetanović, Predrag Tadić</author><pubDate>Fri, 12 Apr 2024 18:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08617v1</guid></item><item><title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges</title><link>http://arxiv.org/abs/2307.11471v2</link><description>Visual question answering requires a system to provide an accurate naturallanguage answer given an image and a natural language question. However, it iswidely recognized that previous generic VQA methods often exhibit a tendency tomemorize biases present in the training data rather than learning properbehaviors, such as grounding images before predicting answers. Therefore, thesemethods usually achieve high in-distribution but poor out-of-distributionperformance. In recent years, various datasets and debiasing methods have beenproposed to evaluate and enhance the VQA robustness, respectively. This paperprovides the first comprehensive survey focused on this emerging fashion.Specifically, we first provide an overview of the development process ofdatasets from in-distribution and out-of-distribution perspectives. Then, weexamine the evaluation metrics employed by these datasets. Thirdly, we proposea typology that presents the development process, similarities and differences,robustness comparison, and technical features of existing debiasing methods.Furthermore, we analyze and discuss the robustness of representativevision-and-language pre-training models on VQA. Finally, through a thoroughreview of the available literature and experimental analysis, we discuss thekey areas for future research from various viewpoints.</description><author>Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao</author><pubDate>Sun, 18 Feb 2024 08:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11471v2</guid></item><item><title>mForms : Multimodal Form-Filling with Question Answering</title><link>http://arxiv.org/abs/2011.12340v3</link><description>This paper presents a new approach to form-filling by reformulating the taskas multimodal natural language Question Answering (QA). The reformulation isachieved by first translating the elements on the GUI form (text fields,buttons, icons, etc.) to natural language questions, where these questionscapture the element's multimodal semantics. After a match is determined betweenthe form element (Question) and the user utterance (Answer), the form elementis filled through a pre-trained extractive QA system. By leveraging pre-trainedQA models and not requiring form-specific training, this approach toform-filling is zero-shot. The paper also presents an approach to furtherrefine the form-filling by using multi-task training to incorporate apotentially large number of successive tasks. Finally, the paper introduces amultimodal natural language form-filling dataset Multimodal Forms (mForms), aswell as a multimodal extension of the popular ATIS dataset to support futureresearch and experimentation. Results show the new approach not only maintainsrobust accuracy for sparse training conditions but achieves state-of-the-art F1of 0.97 on ATIS with approximately 1/10th of the training data.</description><author>Larry Heck, Simon Heck, Anirudh Sundar</author><pubDate>Wed, 13 Mar 2024 18:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.12340v3</guid></item><item><title>Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels</title><link>http://arxiv.org/abs/2403.14430v1</link><description>This paper focuses on open-ended video question answering, which aims to findthe correct answers from a large answer set in response to a video-relatedquestion. This is essentially a multi-label classification task, since aquestion may have multiple answers. However, due to annotation costs, thelabels in existing benchmarks are always extremely insufficient, typically oneanswer per question. As a result, existing works tend to directly treat all theunlabeled answers as negative labels, leading to limited ability forgeneralization. In this work, we introduce a simple yet effective rankingdistillation framework (RADI) to mitigate this problem without additionalmanual annotation. RADI employs a teacher model trained with incomplete labelsto generate rankings for potential answers, which contain rich knowledge aboutlabel priority as well as label-associated visual cues, thereby enriching theinsufficient labeling information. To avoid overconfidence in the imperfectteacher model, we further present two robust and parameter-free rankingdistillation approaches: a pairwise approach which introduces adaptive softmargins to dynamically refine the optimization constraints on various pairwiserankings, and a listwise approach which adopts sampling-based partial listwiselearning to resist the bias in teacher ranking. Extensive experiments on fivepopular benchmarks consistently show that both our pairwise and listwise RADIsoutperform state-of-the-art methods. Further analysis demonstrates theeffectiveness of our methods on the insufficient labeling problem.</description><author>Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu</author><pubDate>Thu, 21 Mar 2024 15:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14430v1</guid></item><item><title>PolQA: Polish Question Answering Dataset</title><link>http://arxiv.org/abs/2212.08897v2</link><description>Recently proposed systems for open-domain question answering (OpenQA) requirelarge amounts of training data to achieve state-of-the-art performance.However, data annotation is known to be time-consuming and therefore expensiveto acquire. As a result, the appropriate datasets are available only for ahandful of languages (mainly English and Chinese). In this work, we introduceand publicly release PolQA, the first Polish dataset for OpenQA. It consists of7,000 questions, 87,525 manually labeled evidence passages, and a corpus ofover 7,097,322 candidate passages. Each question is classified according to itsformulation, type, as well as entity type of the answer. This resource allowsus to evaluate the impact of different annotation choices on the performance ofthe QA system and propose an efficient annotation strategy that increases thepassage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation costby 82%.</description><author>Piotr Rybak, Piotr Przybyła, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08897v2</guid></item><item><title>Self-Adaptive Sampling for Efficient Video Question-Answering on Image--Text Models</title><link>http://arxiv.org/abs/2307.04192v4</link><description>Video question-answering is a fundamental task in the field of videounderstanding. Although current vision--language models (VLMs) equipped withVideo Transformers have enabled temporal modeling and yielded superior results,they are at the cost of huge computational power and thus too expensive todeploy in real-time application scenarios. An economical workaround onlysamples a small portion of frames to represent the main content of that videoand tune an image--text model on these sampled frames. Recent videounderstanding models usually randomly sample a set of frames or clips,regardless of internal correlations between their visual contents, nor theirrelevance to the problem. We argue that such kinds of aimless sampling may omitthe key frames from which the correct answer can be deduced, and the situationgets worse when the sampling sparsity increases, which always happens as thevideo lengths increase. To mitigate this issue, we propose two frame samplingstrategies, namely the most domain frames (MDF) and most implied frames (MIF),to maximally preserve those frames that are most likely vital to the givenquestions. MDF passively minimizes the risk of key frame omission in abootstrap manner, while MIS actively searches key frames customized for eachvideo--question pair with the assistance of auxiliary models. The experimentalresults on three public datasets from three advanced VLMs (CLIP, GIT andAll-in-one) demonstrate that our proposed strategies can boost the performancefor image-text pretrained models. The source codes pertaining to the methodproposed in this paper are publicly available athttps://github.com/declare-lab/sas-vqa.</description><author>Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria</author><pubDate>Sun, 31 Mar 2024 13:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04192v4</guid></item><item><title>Multi-hop Question Answering under Temporal Knowledge Editing</title><link>http://arxiv.org/abs/2404.00492v1</link><description>Multi-hop question answering (MQA) under knowledge editing (KE) has garneredsignificant attention in the era of large language models. However, existingmodels for MQA under KE exhibit poor performance when dealing with questionscontaining explicit temporal contexts. To address this limitation, we propose anovel framework, namely TEMPoral knowLEdge augmented Multi-hop QuestionAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs atime-aware graph (TAG) to store edit knowledge in a structured manner. Then,through our proposed inference path, structural retrieval, and joint reasoningstages, TEMPLE-MQA effectively discerns temporal contexts within the questionquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQAsignificantly outperforms baseline models. Additionally, we contribute a newdataset, namely TKEMQA, which serves as the inaugural benchmark tailoredspecifically for MQA with temporal scopes.</description><author>Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, Di Wang</author><pubDate>Sun, 31 Mar 2024 00:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00492v1</guid></item><item><title>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</title><link>http://arxiv.org/abs/2402.11083v1</link><description>Visual Question Answering (VQA) is a fundamental task in computer vision andnatural language process fields. Although the ``pre-training &amp; finetuning''learning paradigm significantly improves the VQA performance, the adversarialrobustness of such a learning paradigm has not been explored. In this paper, wedelve into a new problem: using a pre-trained multimodal source model to createadversarial image-text pairs and then transferring them to attack the targetVQA models. Correspondingly, we propose a novel VQAttack model, which caniteratively generate both image and text perturbations with the designedmodules: the large language model (LLM)-enhanced image attack and thecross-modal joint attack module. At each iteration, the LLM-enhanced imageattack module first optimizes the latent representation-based loss to generatefeature-level image perturbations. Then it incorporates an LLM to furtherenhance the image perturbations by optimizing the designed masked answeranti-recovery loss. The cross-modal joint attack module will be triggered at aspecific iteration, which updates the image and text perturbationssequentially. Notably, the text perturbation updates are based on both thelearned gradients in the word embedding space and word synonym-basedsubstitution. Experimental results on two VQA datasets with five validatedmodels demonstrate the effectiveness of the proposed VQAttack in thetransferable attack setting, compared with state-of-the-art baselines. Thiswork reveals a significant blind spot in the ``pre-training &amp; fine-tuning''paradigm on VQA tasks. Source codes will be released.</description><author>Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</author><pubDate>Fri, 16 Feb 2024 21:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11083v1</guid></item><item><title>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation</title><link>http://arxiv.org/abs/2310.13505v3</link><description>Models for conversational question answering (ConvQA) over knowledge graphs(KGs) are usually trained and tested on benchmarks of gold QA pairs. Thisimplies that training is limited to surface forms seen in the respectivedatasets, and evaluation is on a small set of held-out questions. Through ourproposed framework REIGN, we take several steps to remedy this restrictedlearning setup. First, we systematically generate reformulations of trainingquestions to increase robustness of models to surface form variations. This isa particularly challenging problem, given the incomplete nature of suchquestions. Second, we guide ConvQA models towards higher performance by feedingit only those reformulations that help improve their answering quality, usingdeep reinforcement learning. Third, we demonstrate the viability of trainingmajor model components on one benchmark and applying them zero-shot to another.Finally, for a rigorous evaluation of robustness for trained models, we use andrelease large numbers of diverse reformulations generated by prompting GPT forbenchmark test sets (resulting in 20x increase in sizes). Our findings showthat ConvQA models with robust training via reformulations, significantlyoutperform those with standard training from gold QA pairs only.</description><author>Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum</author><pubDate>Fri, 16 Feb 2024 19:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13505v3</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</title><link>http://arxiv.org/abs/2404.06511v1</link><description>This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).</description><author>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06511v1</guid></item><item><title>FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models</title><link>http://arxiv.org/abs/2402.14116v1</link><description>One type of question that is commonly found in day-to-day scenarios is``fan-out'' questions, complex multi-hop, multi-document reasoning questionsthat require finding information about a large number of entities. However,there exist few resources to evaluate this type of question-answeringcapability among large language models. To evaluate complex reasoning in LLMsmore fully, we present FanOutQA, a high-quality dataset of fan-outquestion-answer pairs and human-annotated decompositions with English Wikipediaas the knowledge base. We formulate three benchmark settings across our datasetand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,finding that contemporary models still have room to improve reasoning overinter-document dependencies in a long context. We provide our dataset andopen-source tools to run models to encourage evaluation at https://fanoutqa.com</description><author>Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch</author><pubDate>Wed, 21 Feb 2024 20:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14116v1</guid></item><item><title>Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision</title><link>http://arxiv.org/abs/2402.16508v1</link><description>Cross-lingual question answering (CLQA) is a complex problem, comprisingcross-lingual retrieval from a multilingual knowledge base, followed by answergeneration either in English or the query language. Both steps are usuallytackled by separate models, requiring substantial annotated datasets, andtypically auxiliary resources, like machine translation systems to bridgebetween languages. In this paper, we show that CLQA can be addressed using asingle encoder-decoder model. To effectively train this model, we propose aself-supervised method based on exploiting the cross-lingual link structurewithin Wikipedia. We demonstrate how linked Wikipedia pages can be used tosynthesise supervisory signals for cross-lingual retrieval, through a form ofcloze query, and generate more natural queries to supervise answer generation.Together, we show our approach, \texttt{CLASS}, outperforms comparable methodson both supervised and zero-shot language adaptation settings, including thoseusing machine translation.</description><author>Fan Jiang, Tom Drummond, Trevor Cohn</author><pubDate>Mon, 26 Feb 2024 11:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16508v1</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models</title><link>http://arxiv.org/abs/2402.13653v1</link><description>We introduce the novel task of zero-shot Protein Question Answering (PQA) forfree-form scientific enquiry. Given a previously unseen protein sequence and anatural language question, the task is to deliver a scientifically accurateanswer. This task not only supports future biological research, but could alsoprovide a test bed for assessing the scientific precision of large languagemodels (LLMs). We contribute the first specialized dataset for PQA modeltraining, containing 257K protein sequences annotated with 1.97M scientificquestion-answer pairs. Additionally, we propose and study several novelbiologically relevant benchmarks for scientific PQA. Employing two robustmulti-modal architectures, we establish an initial state-of-the-art performancefor PQA and reveal key performance factors through ablation studies. Ourcomprehensive PQA framework, named Pika, including dataset, code, modelcheckpoints, and a user-friendly demo, is openly accessible ongithub.com/EMCarrami/Pika, promoting wider research and application in thefield.</description><author>Eli M Carrami, Sahand Sharifzadeh</author><pubDate>Wed, 21 Feb 2024 09:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13653v1</guid></item><item><title>Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA</title><link>http://arxiv.org/abs/2402.15933v1</link><description>In 3D Visual Question Answering (3D VQA), the scarcity of fully annotateddata and limited visual content diversity hampers the generalization to novelscenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA andSQA dataset). Current approaches resort supplement 3D reasoning with 2Dinformation. However, these methods face challenges: either they use top-down2D views that introduce overly complex and sometimes question-irrelevant visualclues, or they rely on globally aggregated scene/image-level representationsfrom 2D VLMs, losing the fine-grained vision-language correlations. To overcomethese limitations, our approach utilizes question-conditional 2D view selectionprocedure, pinpointing semantically relevant 2D inputs for crucial visualclues. We then integrate this 2D knowledge into the 3D-VQA system via atwo-branch Transformer structure. This structure, featuring a Twin-Transformerdesign, compactly combines 2D and 3D modalities and captures fine-grainedcorrelations between modalities, allowing them mutually augmenting each other.Integrating proposed mechanisms above, we present BridgeQA, that offers a freshperspective on multi-modal transformer-based architectures for 3D-VQA.Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasetsand significantly outperforms existing solutions. Code is available at$\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.</description><author>Wentao Mo, Yang Liu</author><pubDate>Sat, 24 Feb 2024 23:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15933v1</guid></item><item><title>CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</title><link>http://arxiv.org/abs/2404.01299v1</link><description>Causal video question answering (QA) has garnered increasing interest, yetexisting datasets often lack depth in causal reasoning analysis. To addressthis gap, we capitalize on the unique properties of cartoons and constructCausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic"Tom and Jerry" cartoon series. With thoughtful questions and multi-levelanswers, our dataset contains much longer causal chains embedded in dynamicinteractions and visuals, at the same time principles of animation allowsanimators to create well-defined, unambiguous causal relationships. Thesefactors allow models to solve more challenging, yet well-defined causalrelationships. We also introduce hard negative mining, includingCausalConfusion version. While models perform well, there is much room forimprovement, especially, on open-ended answers. We identify moreadvanced/explicit causal relationship modeling and joint modeling of vision andlanguage as the immediate areas for future efforts to focus upon. Along withthe other complementary datasets, our new challenging dataset will pave the wayfor these developments in the field. We will release our dataset, codes, andmodels to help future efforts in this domain.</description><author>Ting En Lam, Yuhan Chen, Elston Tan, Eric Peh, Ruirui Chen, Paritosh Parmar, Basura Fernando</author><pubDate>Mon, 01 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01299v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</title><link>http://arxiv.org/abs/2402.07630v2</link><description>Given a graph with textual attributes, we enable users to `chat with theirgraph': that is, to ask questions about the graph using a conversationalinterface. In response to a user's questions, our method provides textualreplies and highlights the relevant parts of the graph. While existing worksintegrate large language models (LLMs) and graph neural networks (GNNs) invarious ways, they mostly focus on either conventional graph tasks (such asnode, edge, and graph classification), or on answering simple graph queries onsmall or synthetic graphs. In contrast, we develop a flexiblequestion-answering framework targeting real-world textual graphs, applicable tomultiple applications including scene graph understanding, common sensereasoning, and knowledge graph reasoning. Toward this goal, we first developour Graph Question Answering (GraphQA) benchmark with data collected fromdifferent tasks. Then, we propose our G-Retriever approach, which integratesthe strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and canbe fine-tuned to enhance graph understanding via soft prompting. To resisthallucination and to allow for textual graphs that greatly exceed the LLM'scontext window size, G-Retriever performs RAG over a graph by formulating thistask as a Prize-Collecting Steiner Tree optimization problem. Empiricalevaluations show that our method outperforms baselines on textual graph tasksfrom multiple domains, scales well with larger graph sizes, and resistshallucination. (Our codes and datasets are available at:https://github.com/XiaoxinHe/G-Retriever.)</description><author>Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</author><pubDate>Thu, 14 Mar 2024 06:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07630v2</guid></item><item><title>Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</title><link>http://arxiv.org/abs/2403.15268v2</link><description>Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have beenproposed to enhance the knowledge required for question answering over LargeLanguage Models (LLMs). However, the former depends on external resources, andboth require incorporating the explicit documents into the context, whichresults in longer contexts that lead to more resource consumption. Recent worksindicate that LLMs have modeled rich knowledge, albeit not effectivelytriggered or activated. Inspired by this, we propose a novelknowledge-augmented framework, Imagination-Augmented-Generation (IAG), whichsimulates the human capacity to compensate for knowledge deficits whileanswering questions solely through imagination, without relying on externalresources. Guided by IAG, we propose an imagine richer context method forquestion answering (IMcQA), which obtains richer context through the followingtwo modules: explicit imagination by generating a short dummy document withlong context compress and implicit imagination with HyperNetwork for generatingadapter weights. Experimental results on three datasets demonstrate that IMcQAexhibits significant advantages in both open-domain and closed-book settings,as well as in both in-distribution performance and out-of-distributiongeneralizations. Our code will be available athttps://github.com/Xnhyacinth/IAG.</description><author>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao</author><pubDate>Thu, 28 Mar 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15268v2</guid></item><item><title>Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</title><link>http://arxiv.org/abs/2403.15268v1</link><description>Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have beenproposed to enhance the knowledge required for question answering over LargeLanguage Models (LLMs). However, the former depends on external resources, andboth require incorporating the explicit documents into the context, whichresults in longer contexts that lead to more resource consumption. Recent worksindicate that LLMs have modeled rich knowledge, albeit not effectivelytriggered or activated. Inspired by this, we propose a novelknowledge-augmented framework, Imagination-Augmented-Generation (IAG), whichsimulates the human capacity to compensate for knowledge deficits whileanswering questions solely through imagination, without relying on externalresources. Guided by IAG, we propose an imagine richer context method forquestion answering (IMcQA), which obtains richer context through the followingtwo modules: explicit imagination by generating a short dummy document withlong context compress and implicit imagination with HyperNetwork for generatingadapter weights. Experimental results on three datasets demonstrate that IMcQAexhibits significant advantages in both open-domain and closed-book settings,as well as in both in-distribution performance and out-of-distributiongeneralizations. Our code will be available athttps://github.com/Xnhyacinth/IAG.</description><author>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao</author><pubDate>Fri, 22 Mar 2024 16:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15268v1</guid></item><item><title>KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations</title><link>http://arxiv.org/abs/2403.01469v2</link><description>We introduce KorMedMCQA, the first Korean multiple-choice question answering(MCQA) benchmark derived from Korean healthcare professional licensingexaminations, covering from the year 2012 to year 2023. This dataset consistsof a selection of questions from the license examinations for doctors, nurses,and pharmacists, featuring a diverse array of subjects. We conduct baselineexperiments on various large language models, includingproprietary/open-source, multilingual/Korean-additional pretrained, andclinical context pretrained models, highlighting the potential for furtherenhancements. We make our data publicly available on HuggingFace(https://huggingface.co/datasets/sean0042/KorMedMCQA) and provide a evaluationscript via LM-Harness, inviting further exploration and advancement in Koreanhealthcare environments.</description><author>Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi</author><pubDate>Tue, 05 Mar 2024 09:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01469v2</guid></item><item><title>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</title><link>http://arxiv.org/abs/2402.12869v1</link><description>Augmenting Large Language Models (LLMs) for Question Answering (QA) withdomain specific data has attracted wide attention. However, domain data oftenexists in a hybrid format, including text and semi-structured tables, posingchallenges for the seamless integration of information. Table-to-TextGeneration is a promising solution by facilitating the transformation of hybriddata into a uniformly text-formatted corpus. Although this technique has beenwidely studied by the NLP community, there is currently no comparative analysison how corpora generated by different table-to-text methods affect theperformance of QA systems. In this paper, we address this research gap in twosteps. First, we innovatively integrate table-to-text generation into theframework of enhancing LLM-based QA systems with domain hybrid data. Then, weutilize this framework in real-world industrial data to conduct extensiveexperiments on two types of QA systems (DSFT and RAG frameworks) with fourrepresentative methods: Markdown format, Template serialization, TPLM-basedmethod, and LLM-based method. Based on the experimental results, we draw someempirical findings and explore the underlying reasons behind the success ofsome methods. We hope the findings of this work will provide a valuablereference for the academic and industrial communities in developing robust QAsystems.</description><author>Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang</author><pubDate>Tue, 20 Feb 2024 10:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12869v1</guid></item><item><title>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</title><link>http://arxiv.org/abs/2402.12869v2</link><description>Augmenting Large Language Models (LLMs) for Question Answering (QA) withdomain specific data has attracted wide attention. However, domain data oftenexists in a hybrid format, including text and semi-structured tables, posingchallenges for the seamless integration of information. Table-to-TextGeneration is a promising solution by facilitating the transformation of hybriddata into a uniformly text-formatted corpus. Although this technique has beenwidely studied by the NLP community, there is currently no comparative analysison how corpora generated by different table-to-text methods affect theperformance of QA systems. In this paper, we address this research gap in twosteps. First, we innovatively integrate table-to-text generation into theframework of enhancing LLM-based QA systems with domain hybrid data. Then, weutilize this framework in real-world industrial data to conduct extensiveexperiments on two types of QA systems (DSFT and RAG frameworks) with fourrepresentative methods: Markdown format, Template serialization, TPLM-basedmethod, and LLM-based method. Based on the experimental results, we draw someempirical findings and explore the underlying reasons behind the success ofsome methods. We hope the findings of this work will provide a valuablereference for the academic and industrial communities in developing robust QAsystems.</description><author>Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang</author><pubDate>Tue, 09 Apr 2024 10:00:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12869v2</guid></item><item><title>Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models</title><link>http://arxiv.org/abs/2402.15131v1</link><description>This study explores the realm of knowledge-base question answering (KBQA).KBQA is considered a challenging task, particularly in parsing intricatequestions into executable logical forms. Traditional semantic parsing(SP)-based methods require extensive data annotations, which result insignificant costs. Recently, the advent of few-shot in-context learning,powered by large language models (LLMs), has showcased promising capabilities.Yet, fully leveraging LLMs to parse questions into logical forms inlow-resource scenarios poses a substantial challenge. To tackle these hurdles,we introduce Interactive-KBQA, a framework designed to generate logical formsthrough direct interaction with knowledge bases (KBs). Within this framework,we have developed three generic APIs for KB interaction. For each category ofcomplex question, we devised exemplars to guide LLMs through the reasoningprocesses. Our method achieves competitive results on the WebQuestionsSP,ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number ofexamples (shots). Importantly, our approach supports manual intervention,allowing for the iterative refinement of LLM outputs. By annotating a datasetwith step-wise reasoning processes, we showcase our model's adaptability andhighlight its potential for contributing significant enhancements to the field.</description><author>Guanming Xiong, Junwei Bao, Wen Zhao</author><pubDate>Fri, 23 Feb 2024 06:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15131v1</guid></item><item><title>Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering</title><link>http://arxiv.org/abs/2305.14882v2</link><description>Recent advances in multimodal large language models (LLMs) have shown extremeeffectiveness in visual question answering (VQA). However, the design nature ofthese end-to-end models prevents them from being interpretable to humans,undermining trust and applicability in critical domains. While post-hocrationales offer certain insight into understanding model behavior, theseexplanations are not guaranteed to be faithful to the model. In this paper, weaddress these shortcomings by introducing an interpretable by design model thatfactors model decisions into intermediate human-legible explanations, andallows people to easily understand why a model fails or succeeds. We proposethe Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towardsan inherently interpretable VQA system. DCLUB provides an explainableintermediate space before the VQA decision and is faithful from the beginning,while maintaining comparable performance to black-box systems. Given aquestion, DCLUB first returns a set of visual clues: natural languagestatements of visually salient evidence from the image, and then generates theoutput based solely on the visual clues. To supervise and evaluate thegeneration of VQA explanations within DCLUB, we collect a dataset of 1.7kreasoning-focused questions with visual clues. Evaluations show that ourinherently interpretable system can improve 4.64% over a comparable black-boxsystem in reasoning-focused questions while preserving 99.43% of performance onVQA-v2.</description><author>Xingyu Fu, Ben Zhou, Sihao Chen, Mark Yatskar, Dan Roth</author><pubDate>Sat, 13 Apr 2024 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14882v2</guid></item><item><title>mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning</title><link>http://arxiv.org/abs/2404.01548v1</link><description>In the fields of computer vision and natural language processing, multimodalchart question-answering, especially involving color, structure, and textlesscharts, poses significant challenges. Traditional methods, which typicallyinvolve either direct multimodal processing or a table-to-text conversionfollowed by language model analysis, have limitations in effectively handlingthese complex scenarios. This paper introduces a novel multimodal chartquestion-answering model, specifically designed to address these intricatetasks. Our model integrates visual and linguistic processing, overcoming theconstraints of existing methods. We adopt a dual-phase training approach: theinitial phase focuses on aligning image and text representations, while thesubsequent phase concentrates on optimizing the model's interpretative andanalytical abilities in chart-related queries. This approach has demonstratedsuperior performance on multiple public datasets, particularly in handlingcolor, structure, and textless chart questions, indicating its effectiveness incomplex multimodal tasks.</description><author>Jingxuan Wei, Nan Xu, Guiyong Chang, Yin Luo, BiHui Yu, Ruifeng Guo</author><pubDate>Tue, 02 Apr 2024 02:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01548v1</guid></item><item><title>A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction</title><link>http://arxiv.org/abs/2402.11177v1</link><description>Electronic health records (EHRs) hold significant value for research andapplications. As a new way of information extraction, question answering (QA)can extract more flexible information than conventional methods and is moreaccessible to clinical researchers, but its progress is impeded by the scarcityof annotated data. In this paper, we propose a novel approach thatautomatically generates training data for transfer learning of QA models. Ourpipeline incorporates a preprocessing module to handle challenges posed byextraction types that are not readily compatible with extractive QA frameworks,including cases with discontinuous answers and many-to-one relationships. Theobtained QA model exhibits excellent performance on subtasks of informationextraction in EHRs, and it can effectively handle few-shot or zero-shotsettings involving yes-no questions. Case studies and ablation studiesdemonstrate the necessity of each component in our design, and the resultingmodel is deemed suitable for practical use.</description><author>Huaiyuan Ying, Sheng Yu</author><pubDate>Sat, 17 Feb 2024 02:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11177v1</guid></item><item><title>BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English</title><link>http://arxiv.org/abs/2403.10900v1</link><description>In this study, we introduce BEnQA, a dataset comprising parallel Bengali andEnglish exam questions for middle and high school levels in Bangladesh. Ourdataset consists of approximately 5K questions covering several subjects inscience with different types of questions, including factual, application, andreasoning-based questions. We benchmark several Large Language Models (LLMs)with our parallel dataset and observe a notable performance disparity betweenthe models in Bengali and English. We also investigate some prompting methods,and find that Chain-of-Thought prompting is beneficial mostly on reasoningquestions, but not so much on factual ones. We also find that appending Englishtranslation helps to answer questions in Bengali. Our findings point topromising future research directions for improving the performance of LLMs inBengali and more generally in low-resource languages.</description><author>Sheikh Shafayat, H M Quamran Hasan, Minhajur Rahman Chowdhury Mahim, Rifki Afina Putri, James Thorne, Alice Oh</author><pubDate>Sat, 16 Mar 2024 12:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10900v1</guid></item><item><title>Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research</title><link>http://arxiv.org/abs/2402.16038v1</link><description>In recent years, advancements in natural language processing (NLP) have beenfueled by deep learning techniques, particularly through the utilization ofpowerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,trained on vast amounts of data, have revolutionized language understanding andgeneration. These pre-trained models serve as robust bases for various tasksincluding semantic understanding, intelligent writing, and reasoning, pavingthe way for a more generalized form of artificial intelligence. NLP, as a vitalapplication of AI, aims to bridge the gap between humans and computers throughnatural language interaction. This paper delves into the current landscape andfuture prospects of large-scale model-based NLP, focusing on thequestion-answering systems within this domain. Practical cases and developmentsin artificial intelligence-driven question-answering systems are analyzed tofoster further exploration and research in the realm of large-scale NLP.</description><author>Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong</author><pubDate>Sun, 25 Feb 2024 09:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16038v1</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction</title><link>http://arxiv.org/abs/2403.18447v1</link><description>Language models have demonstrated impressive ability in context understandingand generative performance. Inspired by the recent success of languagefoundation models, in this paper, we propose LMTraj (Language-based MultimodalTrajectory predictor), which recasts the trajectory prediction task into a sortof question-answering problem. Departing from traditional numerical regressionmodels, which treat the trajectory coordinate sequence as continuous signals,we consider them as discrete signals like text prompts. Specially, we firsttransform an input space for the trajectory coordinate into the naturallanguage space. Here, the entire time-series trajectories of pedestrians areconverted into a text prompt, and scene images are described as textinformation through image captioning. The transformed numerical and image dataare then wrapped into the question-answering template for use in a languagemodel. Next, to guide the language model in understanding and reasoninghigh-level knowledge, such as scene context and social relationships betweenpedestrians, we introduce an auxiliary multi-task question and answering. Wethen train a numerical tokenizer with the prompt data. We encourage thetokenizer to separate the integer and decimal parts well, and leverage it tocapture correlations between the consecutive numbers in the language model.Lastly, we train the language model using the numerical tokenizer and all ofthe question-answer prompts. Here, we propose a beam-search-based most-likelyprediction and a temperature-based multimodal prediction to implement bothdeterministic and stochastic inferences. Applying our LMTraj, we show that thelanguage-based model can be a powerful pedestrian trajectory predictor, andoutperforms existing numerical-based predictor methods. Code is publiclyavailable at https://github.com/inhwanbae/LMTrajectory .</description><author>Inhwan Bae, Junoh Lee, Hae-Gon Jeon</author><pubDate>Wed, 27 Mar 2024 12:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18447v1</guid></item><item><title>Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation</title><link>http://arxiv.org/abs/2403.02707v1</link><description>Leveraging pre-trained visual language models has become a widely adoptedapproach for improving performance in downstream visual question answering(VQA) applications. However, in the specialized field of medical VQA, thescarcity of available data poses a significant barrier to achieving reliablemodel generalization. Numerous methods have been proposed to enhance modelgeneralization, addressing the issue from data-centric and model-centricperspectives. Data augmentation techniques are commonly employed to enrich thedataset, while various regularization approaches aim to prevent modeloverfitting, especially when training on limited data samples. In this paper,we introduce a method that incorporates gradient-guided parameter perturbationsto the visual encoder of the multimodality model during both pre-training andfine-tuning phases, to improve model generalization for downstream medical VQAtasks. The small perturbation is adaptively generated by aligning with thedirection of the moving average gradient in the optimization landscape, whichis opposite to the directions of the optimizer's historical updates. It issubsequently injected into the model's visual encoder. The results show that,even with a significantly smaller pre-training image caption dataset, ourapproach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.</description><author>Gang Liu, Hongyang Li, Zerui He, Shenjun Zhong</author><pubDate>Tue, 05 Mar 2024 06:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02707v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>Language Model Knowledge Distillation for Efficient Question Answering in Spanish</title><link>http://arxiv.org/abs/2312.04193v2</link><description>Recent advances in the development of pre-trained Spanish language models hasled to significant progress in many Natural Language Processing (NLP) tasks,such as question answering. However, the lack of efficient models imposes abarrier for the adoption of such models in resource-constrained environments.Therefore, smaller distilled models for the Spanish language could be proven tobe highly scalable and facilitate their further adoption on a variety of tasksand scenarios. In this work, we take one step in this direction by developingSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficientquestion answering in Spanish. To achieve this, we employ knowledgedistillation from a large model onto a lighter model that allows for a widerimplementation, even in areas with limited computational resources, whilstattaining negligible performance sacrifice. Our experiments show that the densedistilled model can still preserve the performance of its larger counterpart,while significantly increasing inference speedup. This work serves as astarting point for further research and investigation of model compressionefforts for Spanish language models across various NLP tasks.</description><author>Adrián Bazaga, Pietro Liò, Gos Micklem</author><pubDate>Sat, 16 Mar 2024 18:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04193v2</guid></item></channel></rss>