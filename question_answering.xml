<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:19:54 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension</title><link>http://arxiv.org/abs/2407.07321v1</link><description>Large Language Models (LLMs) have been applied to many research problemsacross various domains. One of the applications of LLMs is providingquestion-answering systems that cater to users from different fields. Theeffectiveness of LLM-based question-answering systems has already beenestablished at an acceptable level for users posing questions in popular andpublic domains such as trivia and literature. However, it has not often beenestablished in niche domains that traditionally require specialized expertise.To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performanceof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answeringquestions originating from Environmental Impact Statements prepared by U.S.federal government agencies in accordance with the National EnvironmentalEnvironmental Act (NEPA). We specifically measure the ability of LLMs tounderstand the nuances of legal, technical, and compliance-related informationpresent in NEPA documents in different contextual scenarios. For example, wetest the LLMs' internal prior NEPA knowledge by providing questions without anycontext, as well as assess how LLMs synthesize the contextual informationpresent in long NEPA documents to facilitate the question/answering task. Wecompare the performance of the long context LLMs and RAG powered models inhandling different types of questions (e.g., problem-solving, divergent). Ourresults suggest that RAG powered models significantly outperform the longcontext models in the answer accuracy regardless of the choice of the frontierLLM. Our further analysis reveals that many models perform better answeringclosed questions than divergent and problem-solving questions.</description><author>Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana</author><pubDate>Wed, 10 Jul 2024 02:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07321v1</guid></item><item><title>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation</title><link>http://arxiv.org/abs/2405.09939v2</link><description>We introduce SciQAG, a novel framework for automatically generatinghigh-quality science question-answer pairs from a large corpus of scientificliterature based on large language models (LLMs). SciQAG consists of a QAgenerator and a QA evaluator, which work together to extract diverse andresearch-level questions and answers from scientific papers. Utilizing thisframework, we construct a large-scale, high-quality, open-ended science QAdataset containing 188,042 QA pairs extracted from 22,743 scientific papersacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmarktask designed to evaluate the science question-answering ability of LLMs.Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG datasetsignificantly improves their performance on both open-ended question answeringand scientific tasks. To foster research and collaboration, we make thedatasets, models, and evaluation codes publicly available, contributing to theadvancement of science question answering and developing more interpretable andreasoning-capable AI systems.</description><author>Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster</author><pubDate>Wed, 10 Jul 2024 01:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09939v2</guid></item><item><title>Uncertainty Estimation of Large Language Models in Medical Question Answering</title><link>http://arxiv.org/abs/2407.08662v1</link><description>Large Language Models (LLMs) show promise for natural language generation inhealthcare, but risk hallucinating factually incorrect information. DeployingLLMs for medical question answering necessitates reliable uncertaintyestimation (UE) methods to detect hallucinations. In this work, we benchmarkpopular UE methods with different model sizes on medical question-answeringdatasets. Our results show that current approaches generally perform poorly inthis domain, highlighting the challenge of UE for medical applications. We alsoobserve that larger models tend to yield better results, suggesting acorrelation between model size and the reliability of UE. To address thesechallenges, we propose Two-phase Verification, a probability-free UncertaintyEstimation approach. First, an LLM generates a step-by-step explanationalongside its initial answer, followed by formulating verification questions tocheck the factual claims in the explanation. The model then answers thesequestions twice: first independently, and then referencing the explanation.Inconsistencies between the two sets of answers measure the uncertainty in theoriginal response. We evaluate our approach on three biomedicalquestion-answering datasets using Llama 2 Chat models and compare it againstthe benchmarked baseline methods. The results show that our Two-phaseVerification method achieves the best overall accuracy and stability acrossvarious datasets and model sizes, and its performance scales as the model sizeincreases.</description><author>Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou</author><pubDate>Thu, 11 Jul 2024 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08662v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v2</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Thu, 11 Jul 2024 19:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v2</guid></item><item><title>Meta-Analysis with Untrusted Data</title><link>http://arxiv.org/abs/2407.09387v1</link><description>[See paper for full abstract] Meta-analysis is a crucial tool for answeringscientific questions. It is usually conducted on a relatively small amount of``trusted'' data -- ideally from randomized, controlled trials -- which allowcausal effects to be reliably estimated with minimal assumptions. We show howto answer causal questions much more precisely by making two changes. First, weincorporate untrusted data drawn from large observational databases, relatedscientific literature and practical experience -- without sacrificing rigor orintroducing strong assumptions. Second, we train richer models capable ofhandling heterogeneous trials, addressing a long-standing challenge inmeta-analysis. Our approach is based on conformal prediction, whichfundamentally produces rigorous prediction intervals, but doesn't handleindirect observations: in meta-analysis, we observe only noisy effects due tothe limited number of participants in each trial. To handle noise, we develop asimple, efficient version of fully-conformal kernel ridge regression, based ona novel condition called idiocentricity. We introduce noise-correcting terms inthe residuals and analyze their interaction with a ``variance shaving''technique. In multiple experiments on healthcare datasets, our algorithmsdeliver tighter, sounder intervals than traditional ones. This paper charts anew course for meta-analysis and evidence-based medicine, where heterogeneityand untrusted data are embraced for more nuanced and precise predictions.</description><author>Shiva Kaul, Geoffrey J. Gordon</author><pubDate>Fri, 12 Jul 2024 16:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09387v1</guid></item><item><title>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</title><link>http://arxiv.org/abs/2407.07577v1</link><description>The rapid advancement of Large Vision-Language models (LVLMs) hasdemonstrated a spectrum of emergent capabilities. Nevertheless, current modelsonly focus on the visual content of a single scenario, while their ability toassociate instances across different scenes has not yet been explored, which isessential for understanding complex visual content, such as movies withmultiple characters and intricate plots. Towards movie understanding, acritical initial step for LVLMs is to unleash the potential of characteridentities memory and recognition across multiple visual scenarios. To achievethe goal, we propose visual instruction tuning with ID reference and develop anID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our researchintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory andrecognition across four dimensions: matching, location, question-answering, andcaptioning. Our findings highlight the limitations of existing LVLMs inrecognizing and associating instance identities with ID reference. This paperpaves the way for future artificial intelligence systems to possessmulti-identity visual inputs, thereby facilitating the comprehension of complexvisual narratives like movies.</description><author>Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo</author><pubDate>Wed, 10 Jul 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07577v1</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item><item><title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title><link>http://arxiv.org/abs/2403.01858v3</link><description>We present TMMLU+, a new benchmark designed for Traditional Chinese languageunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66subjects from elementary to professional level. It is six times larger andboasts a more balanced subject distribution than its predecessor, TaiwanMassive Multitask Language Understanding (TMMLU). We also benchmarkclosed-source models and 26 open-weight Chinese large language models (LLMs) ofparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings revealthat (1.) Traditional Chinese models still trail behind their SimplifiedChinese counterparts, highlighting a need for more focused advancements in LLMscatering to Traditional Chinese. (2.) Current LLMs still fall short of humanperformance in average scores, indicating a potential need for future researchto delve deeper into social science and humanities subjects. (3.) Among all thetokenization compression metrics examined, we identify that only the fertilityscore uniquely demonstrates strong correlations with our benchmark results. Weforesee that TMMLU+ will pinpoint areas for future model improvement, therebynarrowing the gap between machine and human linguistic capabilities andsupporting researchers in developing Traditional Chinese LLMs. Our dataset,along with the benchmark source code, is accessible athuggingface.co/datasets/ikala/tmmluplus.</description><author>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</author><pubDate>Thu, 11 Jul 2024 14:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01858v3</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods</title><link>http://arxiv.org/abs/2407.06985v2</link><description>In domain-specific applications, GPT-4, augmented with precise prompts orRetrieval-Augmented Generation (RAG), shows notable potential but faces thecritical tri-lemma of performance, cost, and data privacy. High performancerequires sophisticated processing techniques, yet managing multiple agentswithin a complex workflow often proves costly and challenging. To address this,we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.This systematizes domain-specific tasks by integrating precise questiondecomposition, advanced information retrieval, comprehensive summarization, andrigorous self-assessment. Given the concerns of cost and data privacy,enterprises are shifting from proprietary models like GPT-4 to custom models,striking a balance between cost, security, and performance. We developedindustrial practices leveraging online data and user feedback for efficientmodel tuning. This study provides best practice guidelines for applyingmulti-agent systems in domain-specific problem-solving and implementingeffective agent tuning strategies. Our empirical studies, particularly in thefinancial question-answering domain, demonstrate that our approach achieves95.0% of GPT-4's performance, while effectively managing costs and ensuringdata privacy.</description><author>Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu</author><pubDate>Wed, 10 Jul 2024 03:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06985v2</guid></item><item><title>Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders</title><link>http://arxiv.org/abs/2402.00260v2</link><description>The robotic intervention for individuals with Autism Spectrum Disorder (ASD)has generally used pre-defined scripts to deliver verbal content duringone-to-one therapy sessions. This practice restricts the use of robots tolimited, pre-mediated instructional curricula. In this paper, we increase robotautonomy in one such robotic intervention for children with ASD by implementingperspective-taking teaching. Our approach uses large language models (LLM) togenerate verbal content as texts and then deliver it to the child via roboticspeech. In the proposed pipeline, we teach perspective-taking through which ourrobot takes up three roles: initiator, prompter, and reinforcer. We adopted theGPT-2 + BART pipelines to generate social situations, ask questions (asinitiator), and give options (as prompter) when required. The robot encouragesthe child by giving positive reinforcement for correct answers (as areinforcer). In addition to our technical contribution, we conducted ten-minutesessions with domain experts simulating an actual perspective teaching session,with the researcher acting as a child participant. These sessions validated ourrobotic intervention pipeline through surveys, including those from NASA TLXand GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with anall GPT-2 and found the performance of the former to be better. Based on theresponses by the domain experts, the robot session demonstrated higherperformance with no additional increase in mental or physical demand, temporaldemand, effort, or frustration compared to a no-robot session. We alsoconcluded that the domain experts perceived the robot as ideally safe, likable,and reliable.</description><author>Ruchik Mishra, Karla Conn Welch, Dan O Popa</author><pubDate>Wed, 10 Jul 2024 01:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00260v2</guid></item><item><title>FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding</title><link>http://arxiv.org/abs/2407.05183v2</link><description>Flowcharts are graphical tools for representing complex concepts in concisevisual representations. This paper introduces the FlowLearn dataset, a resourcetailored to enhance the understanding of flowcharts. FlowLearn contains complexscientific flowcharts and simulated flowcharts. The scientific subset contains3,858 flowcharts sourced from scientific literature and the simulated subsetcontains 10,000 flowcharts created using a customizable script. The dataset isenriched with annotations for visual components, OCR, Mermaid coderepresentation, and VQA question-answer pairs. Despite the proven capabilitiesof Large Vision-Language Models (LVLMs) in various visual understanding tasks,their effectiveness in decoding flowcharts - a crucial element of scientificcommunication - has yet to be thoroughly investigated. The FlowLearn test setis crafted to assess the performance of LVLMs in flowchart comprehension. Ourstudy thoroughly evaluates state-of-the-art LVLMs, identifying existinglimitations and establishing a foundation for future enhancements in thisrelatively underexplored domain. For instance, in tasks involving simulatedflowcharts, GPT-4V achieved the highest accuracy (58%) in counting the numberof nodes, while Claude recorded the highest accuracy (83%) in OCR tasks.Notably, no single model excels in all tasks within the FlowLearn framework,highlighting significant opportunities for further development.</description><author>Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, Longin Jan Latecki</author><pubDate>Tue, 09 Jul 2024 21:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05183v2</guid></item><item><title>ConvNLP: Image-based AI Text Detection</title><link>http://arxiv.org/abs/2407.07225v1</link><description>The potentials of Generative-AI technologies like Large Language models(LLMs) to revolutionize education are undermined by ethical considerationsaround their misuse which worsens the problem of academic dishonesty. LLMs likeGPT-4 and Llama 2 are becoming increasingly powerful in generatingsophisticated content and answering questions, from writing academic essays tosolving complex math problems. Students are relying on these LLMs to completetheir assignments and thus compromising academic integrity. Solutions to detectLLM-generated text are compute-intensive and often lack generalization. Thispaper presents a novel approach for detecting LLM-generated AI-text using avisual representation of word embedding. We have formulated a novelConvolutional Neural Network called ZigZag ResNet, as well as a scheduler forimproving generalization, named ZigZag Scheduler. Through extensive evaluationusing datasets of text generated by six different state-of-the-art LLMs, ourmodel demonstrates strong intra-domain and inter-domain generalizationcapabilities. Our best model detects AI-generated text with an impressiveaverage detection rate (over inter- and intra-domain test data) of 88.35%.Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Schedulerprovide a performance improvement of nearly 4% over the vanilla ResNet. Theend-to-end inference latency of our model is below 2.5ms per sentence. Oursolution offers a lightweight, computationally efficient, and fasteralternative to existing tools for AI-generated text detection, with bettergeneralization performance. It can help academic institutions in their fightagainst the misuse of LLMs in academic settings. Through this work, we aim tocontribute to safeguarding the principles of academic integrity and ensuringthe trustworthiness of student work in the era of advanced LLMs.</description><author>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube</author><pubDate>Tue, 09 Jul 2024 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07225v1</guid></item><item><title>Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding</title><link>http://arxiv.org/abs/2306.06094v2</link><description>Large language models (LLMs) have made significant advancements in naturallanguage understanding. However, through that enormous semantic representationthat the LLM has learnt, is it somehow possible for it to understand images aswell? This work investigates this question. To enable the LLM to processimages, we convert them into a representation given by Scalable Vector Graphics(SVG). To study what the LLM can do with this XML-based textual description ofimages, we test the LLM on three broad computer vision tasks: (i) visualreasoning and question answering, (ii) image classification under distributionshift, few-shot learning, and (iii) generating new images using visualprompting. Even though we do not naturally associate LLMs with any visualunderstanding capabilities, our results indicate that the LLM can often do adecent job in many of these tasks, potentially opening new avenues for researchinto LLMs' ability to understand image data. Our code, data, and models can befound here https://github.com/mu-cai/svg-llm.</description><author>Mu Cai, Zeyi Huang, Yuheng Li, Utkarsh Ojha, Haohan Wang, Yong Jae Lee</author><pubDate>Thu, 11 Jul 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06094v2</guid></item><item><title>A Taxonomy for Data Contamination in Large Language Models</title><link>http://arxiv.org/abs/2407.08716v1</link><description>Large language models pretrained on extensive web corpora demonstrateremarkable performance across a wide range of downstream tasks. However, agrowing concern is data contamination, where evaluation datasets may becontained in the pretraining corpus, inflating model performance.Decontamination, the process of detecting and removing such data, is apotential solution; yet these contaminants may originate from altered versionsof the test set, evading detection during decontamination. How different typesof contamination impact the performance of language models on downstream tasksis not fully understood. We present a taxonomy that categorizes the varioustypes of contamination encountered by LLMs during the pretraining phase andidentify which types pose the highest risk. We analyze the impact ofcontamination on two key NLP tasks -- summarization and question answering --revealing how different types of contamination influence task performanceduring evaluation.</description><author>Medha Palavalli, Amanda Bertsch, Matthew R. Gormley</author><pubDate>Thu, 11 Jul 2024 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08716v1</guid></item><item><title>Extracting Training Data from Document-Based VQA Models</title><link>http://arxiv.org/abs/2407.08707v1</link><description>Vision-Language Models (VLMs) have made remarkable progress in document-basedVisual Question Answering (i.e., responding to queries about the contents of aninput document provided as an image). In this work, we show these models canmemorize responses for training samples and regurgitate them even when therelevant visual information has been removed. This includes PersonalIdentifiable Information (PII) repeated once in the training set, indicatingthese models could divulge memorised sensitive information and therefore pose aprivacy risk. We quantitatively measure the extractability of information incontrolled experiments and differentiate between cases where it arises fromgeneralization capabilities or from memorization. We further investigate thefactors that influence memorization across multiple state-of-the-art models andpropose an effective heuristic countermeasure that empirically prevents theextractability of PII.</description><author>Francesco Pinto, Nathalie Rauschmayr, Florian Tramèr, Philip Torr, Federico Tombari</author><pubDate>Thu, 11 Jul 2024 17:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08707v1</guid></item><item><title>Segmentation-guided Attention for Visual Question Answering from Remote Sensing Images</title><link>http://arxiv.org/abs/2407.08669v1</link><description>Visual Question Answering for Remote Sensing (RSVQA) is a task that aims atanswering natural language questions about the content of a remote sensingimage. The visual features extraction is therefore an essential step in a VQApipeline. By incorporating attention mechanisms into this process, models gainthe ability to focus selectively on salient regions of the image, prioritizingthe most relevant visual information for a given question. In this work, wepropose to embed an attention mechanism guided by segmentation into a RSVQApipeline. We argue that segmentation plays a crucial role in guiding attentionby providing a contextual understanding of the visual information, underlyingspecific objects or areas of interest. To evaluate this methodology, we providea new VQA dataset that exploits very high-resolution RGB orthophotos annotatedwith 16 segmentation classes and question/answer pairs. Our study showspromising results of our new methodology, gaining almost 10% of overallaccuracy compared to a classical method on the proposed dataset.</description><author>Lucrezia Tosato, Hichem Boussaid, Flora Weissgerber, Camille Kurtz, Laurent Wendling, Sylvain Lobry</author><pubDate>Thu, 11 Jul 2024 16:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08669v1</guid></item><item><title>Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition</title><link>http://arxiv.org/abs/2403.00499v2</link><description>Recent advances in LLMs have sparked a debate on whether they understandtext. In this position paper, we argue that opponents in this debate holddifferent definitions for understanding, and particularly differ in their viewon the role of consciousness. To substantiate this claim, we propose a thoughtexperiment involving an open-source chatbot $Z$ which excels on every possiblebenchmark, seemingly without subjective experience. We ask whether $Z$ iscapable of understanding, and show that different schools of thought withinseminal AI research seem to answer this question differently, uncovering theirterminological disagreement. Moving forward, we propose two distinct workingdefinitions for understanding which explicitly acknowledge the question ofconsciousness, and draw connections with a rich literature in philosophy,psychology and neuroscience.</description><author>Ariel Goldstein, Gabriel Stanovsky</author><pubDate>Thu, 11 Jul 2024 15:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00499v2</guid></item><item><title>Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin</title><link>http://arxiv.org/abs/2406.06566v2</link><description>Domain specific digital twins, representing a digital replica of varioussegments of the smart grid, are foreseen as able to model, simulate, andcontrol the respective segments. At the same time, knowledge-based digitaltwins, coupled with AI, may also empower humans to understand aspects of thesystem through natural language interaction in view of planning and policymaking. This paper is the first to assess and report on the potential ofRetrieval Augmented Generation (RAG) question answers related to householdelectrical energy measurement aspects leveraging a knowledge-based energydigital twin. Relying on the recently published electricity consumptionknowledge graph that actually represents a knowledge-based digital twin, westudy the capabilities of ChatGPT, Gemini and Llama in answering electricityrelated questions. Furthermore, we compare the answers with the ones generatedthrough a RAG techniques that leverages an existing electricity knowledge-baseddigital twin. Our findings illustrate that the RAG approach not only reducesthe incidence of incorrect information typically generated by LLMs but alsosignificantly improves the quality of the output by grounding responses inverifiable data. This paper details our methodology, presents a comparativeanalysis of responses with and without RAG, and discusses the implications ofour findings for future applications of AI in specialized sectors like energydata analysis.</description><author>Carolina Fortuna, Vid Hanžel, Blaž Bertalanič</author><pubDate>Thu, 11 Jul 2024 13:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06566v2</guid></item><item><title>TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models</title><link>http://arxiv.org/abs/2407.09424v1</link><description>Large Language Models (LLMs) have the potential to revolutionize the SixthGeneration (6G) communication networks. However, current mainstream LLMsgenerally lack the specialized knowledge in telecom domain. In this paper, forthe first time, we propose a pipeline to adapt any general purpose LLMs to atelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,instruction dataset, preference dataset to perform continual pre-training,instruct tuning and alignment tuning respectively. Besides, due to the lack ofwidely accepted evaluation benchmarks in telecom domain, we extend existingevaluation benchmarks and proposed three new benchmarks, namely, Telecom MathModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks providea holistic evaluation of the capabilities of LLMs including math modeling,Open-Ended question answering, code generation, infilling, summarization andanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state ofthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom MathModeling benchmark significantly and achieve comparable performance in variousevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,telecom code summary and generation and infilling.</description><author>Hang Zou, Qiyang Zhao, Yu Tian, Lina Bariah, Faouzi Bader, Thierry Lestable, Merouane Debbah</author><pubDate>Fri, 12 Jul 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09424v1</guid></item><item><title>On scalable oversight with weak LLMs judging strong LLMs</title><link>http://arxiv.org/abs/2407.04622v2</link><description>Scalable oversight protocols aim to enable humans to accurately supervisesuperhuman AI. In this paper we study debate, where two AI's compete toconvince a judge; consultancy, where a single AI tries to convince a judge thatasks questions; and compare to a baseline of direct question-answering, wherethe judge just answers outright without the AI. We use large language models(LLMs) as both AI agents and as stand-ins for human judges, taking the judgemodels to be weaker than agent models. We benchmark on a diverse range ofasymmetries between judges and agents, extending previous work on a singleextractive QA task with information asymmetry, to also include mathematics,coding, logic and multimodal reasoning asymmetries. We find that debateoutperforms consultancy across all tasks when the consultant is randomlyassigned to argue for the correct/incorrect answer. Comparing debate to directquestion answering, the results depend on the type of task: in extractive QAtasks with information asymmetry debate outperforms direct question answering,but in other tasks without information asymmetry the results are mixed.Previous work assigned debaters/consultants an answer to argue for. When weallow them to instead choose which answer to argue for, we find judges are lessfrequently convinced by the wrong answer in debate than in consultancy.Further, we find that stronger debater models increase judge accuracy, thoughmore modestly than in previous studies.</description><author>Zachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah</author><pubDate>Fri, 12 Jul 2024 16:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04622v2</guid></item><item><title>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</title><link>http://arxiv.org/abs/2407.09413v1</link><description>Seeking answers to questions within long scientific research articles is acrucial area of study that aids readers in quickly addressing their inquiries.However, existing question-answering (QA) datasets based on scientific papersare limited in scale and focus solely on textual content. To address thislimitation, we introduce SPIQA (Scientific Paper Image Question Answering), thefirst large-scale QA dataset specifically designed to interpret complex figuresand tables within the context of scientific research articles across variousdomains of computer science. Leveraging the breadth of expertise and ability ofmultimodal large language models (MLLMs) to understand figures, we employautomatic and manual curation to create the dataset. We craft aninformation-seeking task involving multiple images that cover a wide variety ofplots, charts, tables, schematic diagrams, and result visualizations. SPIQAcomprises 270K questions divided into training, validation, and three differentevaluation splits. Through extensive experiments with 12 prominent foundationalmodels, we evaluate the ability of current multimodal systems to comprehend thenuanced aspects of research articles. Additionally, we propose aChain-of-Thought (CoT) evaluation strategy with in-context retrieval thatallows fine-grained, step-by-step assessment and improves model performance. Wefurther explore the upper bounds of performance enhancement with additionaltextual information, highlighting its promising potential for future researchand the dataset's impact on revolutionizing how we interact with scientificliterature.</description><author>Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan</author><pubDate>Fri, 12 Jul 2024 16:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09413v1</guid></item><item><title>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</title><link>http://arxiv.org/abs/2311.09821v2</link><description>Knowledge in the real world is being updated constantly. However, it iscostly to frequently update large language models (LLMs). Therefore, it iscrucial for LLMs to understand the concept of temporal knowledge. However,prior works on temporal question answering (TQA) did not emphasize multi-answerand multi-hop types of temporal reasoning. In this paper, we propose a complextemporal question-answering dataset Complex-TR that focuses on multi-answer andmulti-hop temporal reasoning. Besides, we also propose a novel dataaugmentation strategy to improve the complex temporal reasoning capability androbustness of LLMs. We conducted experiments on multiple temporal QA datasets.Experimental results show that our method is able to improve LLMs' performanceon temporal QA benchmarks by significant margins. Our code and data arereleased at: https://github.com/nusnlp/complex-tr.</description><author>Qingyu Tan, Hwee Tou Ng, Lidong Bing</author><pubDate>Fri, 12 Jul 2024 16:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09821v2</guid></item><item><title>DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection</title><link>http://arxiv.org/abs/2407.09283v1</link><description>Semantic role labeling (SRL) enriches many downstream applications, e.g.,machine translation, question answering, summarization, and stance/beliefdetection. However, building multilingual SRL models is challenging due to thescarcity of semantically annotated corpora for multiple languages. Moreover,state-of-the-art SRL projection (XSRL) based on large language models (LLMs)yields output that is riddled with spurious role labels. Remediation of suchhallucinations is not straightforward due to the lack of explainability ofLLMs. We show that hallucinated role labels are related to naturally occurringdivergence types that interfere with initial alignments. We implementDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraginglinguistically-informed alignment remediation followed by greedy First-ComeFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRLprojection without additional transformer-based machinery, beating XSRL in bothhuman and automatic comparisons, and advancing beyond headwords to accommodatephrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as ourground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt ourapproach to other language pairs (e.g., English-Tagalog).</description><author>Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie Dorr</author><pubDate>Fri, 12 Jul 2024 14:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09283v1</guid></item><item><title>Semi-Supervised Learning for Deep Causal Generative Models</title><link>http://arxiv.org/abs/2403.18717v2</link><description>Developing models that are capable of answering questions of the form "Howwould x change if y had been z?'" is fundamental to advancing medical imageanalysis. Training causal generative models that address such counterfactualquestions, though, currently requires that all relevant variables have beenobserved and that the corresponding labels are available in the training data.However, clinical data may not have complete records for all patients and stateof the art causal generative models are unable to take full advantage of this.We thus develop, for the first time, a semi-supervised deep causal generativemodel that exploits the causal relationships between variables to maximise theuse of all available data. We explore this in the setting where each sample iseither fully labelled or fully unlabelled, as well as the more clinicallyrealistic case of having different labels missing for each sample. We leveragetechniques from causal inference to infer missing values and subsequentlygenerate realistic counterfactuals, even for samples with incomplete labels.</description><author>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</author><pubDate>Fri, 12 Jul 2024 14:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18717v2</guid></item><item><title>Inference Optimization of Foundation Models on AI Accelerators</title><link>http://arxiv.org/abs/2407.09111v1</link><description>Powerful foundation models, including large language models (LLMs), withTransformer architectures have ushered in a new era of Generative AI acrossvarious industries. Industry and research community have witnessed a largenumber of new applications, based on those foundation models. Such applicationsinclude question and answer, customer services, image and video generation, andcode completions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibitive inferencecosts and high latency in real-world scenarios. As a result, the demand forcost-effective and fast inference using AI accelerators is ever more higher. Tothis end, our tutorial offers a comprehensive discussion on complementaryinference optimization techniques using AI accelerators. Beginning with anoverview of basic Transformer architectures and deep learning systemframeworks, we deep dive into system optimization techniques for fast andmemory-efficient attention computations and discuss how they can be implementedefficiently on AI accelerators. Next, we describe architectural elements thatare key for fast transformer inference. Finally, we examine various modelcompression and fast decoding strategies in the same context.</description><author>Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis</author><pubDate>Fri, 12 Jul 2024 09:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09111v1</guid></item><item><title>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</title><link>http://arxiv.org/abs/2406.10421v2</link><description>With the rapid development of Large Language Models (LLMs), it is crucial tohave benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such aswriting algorithms, querying databases or giving mathematical proofs. Inspiredby the way university students are evaluated on such tasks, in this paper, wepropose SciEx - a benchmark consisting of university computer science examquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)multilingual, containing both English and German exams, and (2) multi-modal,containing questions that involve images, and (3) contains various types offreeform questions with different difficulty levels, due to the nature ofuniversity exams. We evaluate the performance of various state-of-the-art LLMson our new benchmark. Since SciEx questions are freeform, it is notstraightforward to evaluate LLM performance. Therefore, we provide human expertgrading of the LLM outputs on SciEx. We show that the free-form exams in SciExremain challenging for the current LLMs, where the best LLM only achieves59.4\% exam grade on average. We also provide detailed comparisons between LLMperformance and student performance on SciEx. To enable future evaluation ofnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solvingthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation withexpert grading.</description><author>Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues</author><pubDate>Fri, 12 Jul 2024 10:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10421v2</guid></item><item><title>CompAct: Compressing Retrieved Documents Actively for Question Answering</title><link>http://arxiv.org/abs/2407.09014v1</link><description>Retrieval-augmented generation supports language models to strengthen theirfactual groundings by providing external contexts. However, language modelsoften face challenges when given extensive information, diminishing theireffectiveness in solving questions. Context compression tackles this issue byfiltering out irrelevant information, but current methods still struggle inrealistic scenarios where crucial information cannot be captured with asingle-step approach. To overcome this limitation, we introduce CompAct, anovel framework that employs an active strategy to condense extensive documentswithout losing key information. Our experiments demonstrate that CompAct bringssignificant improvements in both performance and compression rate on multi-hopquestion-answering (QA) benchmarks. CompAct flexibly operates as acost-efficient plug-in module with various off-the-shelf retrievers or readers,achieving exceptionally high compression rates (47x).</description><author>Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang</author><pubDate>Fri, 12 Jul 2024 06:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09014v1</guid></item><item><title>Static Analysis of Logic Programs via Boolean Networks</title><link>http://arxiv.org/abs/2407.09015v1</link><description>Answer Set Programming (ASP) is a declarative problem solving paradigm thatcan be used to encode a combinatorial problem as a logic program whose stablemodels correspond to the solutions of the considered problem. ASP has beenwidely applied to various domains in AI and beyond. The question "What can besaid about stable models of a logic program from its static information?" hasbeen investigated and proved useful in many circumstances. In this work, wedive into this direction more deeply by making the connection between a logicprogram and a Boolean network, which is a prominent modeling framework withapplications to various areas. The proposed connection can bring the existingresults in the rich history on static analysis of Boolean networks to exploreand prove more theoretical results on ASP, making it become a unified andpowerful tool to further study the static analysis of ASP. In particular, thenewly obtained insights have the potential to benefit many problems in thefield of ASP.</description><author>Van-Giang Trinh, Belaid Benhamou</author><pubDate>Fri, 12 Jul 2024 06:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09015v1</guid></item><item><title>One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning</title><link>http://arxiv.org/abs/2407.09011v1</link><description>This paper presents a novel and comprehensive solution to enhance both therobustness and efficiency of question answering (QA) systems through supervisedcontrastive learning (SCL). Training a high-performance QA system has becomestraightforward with pre-trained language models, requiring only a small amountof data and simple fine-tuning. However, despite recent advances, existing QAsystems still exhibit significant deficiencies in functionality and trainingefficiency. We address the functionality issue by defining four key tasks: userinput intent classification, out-of-domain input detection, new intentdiscovery, and continual learning. We then leverage a unified SCL-basedrepresentation learning method to efficiently build an intra-class compact andinter-class scattered feature space, facilitating both known intentclassification and unknown intent detection and discovery. Consequently, withminimal additional tuning on downstream tasks, our approach significantlyimproves model efficiency and achieves new state-of-the-art performance acrossall tasks.</description><author>Bo Wang, Tsunenori Mine</author><pubDate>Fri, 12 Jul 2024 06:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09011v1</guid></item><item><title>Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?</title><link>http://arxiv.org/abs/2407.08922v1</link><description>With the rapid development of artificial intelligence (AI), large languagemodels (LLMs) such as GPT-4 have garnered significant attention in thescientific community, demonstrating great potential in advancing scientificdiscovery. This progress raises a critical question: are these LLMswell-aligned with real-world physicochemical principles? Current evaluationstrategies largely emphasize fact-based knowledge, such as material propertyprediction or name recognition, but they often lack an understanding offundamental physicochemical mechanisms that require logical reasoning. Tobridge this gap, our study developed a benchmark consisting of 775multiple-choice questions focusing on the mechanisms of gold nanoparticlesynthesis. By reflecting on existing evaluation metrics, we question whether adirect true-or-false assessment merely suggests conjecture. Hence, we propose anovel evaluation metric, the confidence-based score (c-score), which probes theoutput logits to derive the precise probability for the correct answer. Basedon extensive experiments, our results show that in the context of goldnanoparticle synthesis, LLMs understand the underlying physicochemicalmechanisms rather than relying on conjecture. This study underscores thepotential of LLMs to grasp intrinsic scientific mechanisms and sets the stagefor developing more reliable and effective AI tools across various scientificdomains.</description><author>Yingming Pu, Liping Huang, Tao Lin, Hongyu Chen</author><pubDate>Fri, 12 Jul 2024 02:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08922v1</guid></item><item><title>Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison</title><link>http://arxiv.org/abs/2407.07840v2</link><description>Despite tremendous advancements, current state-of-the-art Vision-LanguageModels (VLMs) are still far from perfect. They tend to hallucinate and maygenerate biased responses. In such circumstances, having a way to assess thereliability of a given response generated by a VLM is quite useful. Existingmethods, such as estimating uncertainty using answer likelihoods orprompt-based confidence generation, often suffer from overconfidence. Othermethods use self-consistency comparison but are affected by confirmationbiases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparingthe consistency between the direct answer generated using the VLM's internalreasoning process, and the indirect answers obtained by decomposing thequestion into sub-questions and reasoning over the sub-answers produced by theVLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experimentsacross six vision-language tasks with three VLMs show \texttt{DeCC}'sreliability estimation achieves better correlation with task accuracy comparedto the existing methods.</description><author>Qian Yang, Weixiang Yan, Aishwarya Agrawal</author><pubDate>Thu, 11 Jul 2024 23:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07840v2</guid></item><item><title>RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models</title><link>http://arxiv.org/abs/2404.12065v2</link><description>The escalating challenge of misinformation, particularly in politicaldiscourse, requires advanced fact-checking solutions; this is even clearer inthe more complex scenario of multimodal claims. We tackle this issue using amultimodal large language model in conjunction with retrieval-augmentedgeneration (RAG), and introduce two novel reasoning techniques: Chain of RAG(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims byextracting both textual and image content, retrieving external information, andreasoning subsequent questions to be answered based on prior evidence. Weachieve a weighted F1-score of 0.85, surpassing a baseline reasoning techniqueby 0.14 points. Human evaluation confirms that the vast majority of ourgenerated fact-check explanations contain all information from gold standarddata.</description><author>M. Abdul Khaliq, P. Chang, M. Ma, B. Pflugfelder, F. Miletić</author><pubDate>Thu, 11 Jul 2024 20:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12065v2</guid></item><item><title>Evaluating Nuanced Bias in Large Language Model Free Response Answers</title><link>http://arxiv.org/abs/2407.08842v1</link><description>Pre-trained large language models (LLMs) can now be easily adapted forspecific business purposes using custom prompts or fine tuning. Thesecustomizations are often iteratively re-engineered to improve some aspect ofperformance, but after each change businesses want to ensure that there hasbeen no negative impact on the system's behavior around such critical issues asbias. Prior methods of benchmarking bias use techniques such as word maskingand multiple choice questions to assess bias at scale, but these do not captureall of the nuanced types of bias that can occur in free response answers, thetypes of answers typically generated by LLM systems. In this paper, we identifyseveral kinds of nuanced bias in free text that cannot be similarly identifiedby multiple choice tests. We describe these as: confidence bias, implied bias,inclusion bias and erasure bias. We present a semi-automated pipeline fordetecting these types of bias by first eliminating answers that can beautomatically classified as unbiased and then co-evaluating name reversed pairsusing crowd workers. We believe that the nuanced classifications our methodgenerates can be used to give better feedback to LLMs, especially as LLMreasoning capabilities become more advanced.</description><author>Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar, Moumita Sinha</author><pubDate>Thu, 11 Jul 2024 19:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08842v1</guid></item><item><title>Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison</title><link>http://arxiv.org/abs/2407.07840v1</link><description>Despite tremendous advancements, current state-of-the-art Vision-LanguageModels (VLMs) are still far from perfect. They tend to hallucinate and maygenerate biased responses. In such circumstances, having a way to assess thereliability of a given response generated by a VLM is quite useful. Existingmethods, such as estimating uncertainty using answer likelihoods orprompt-based confidence generation, often suffer from overconfidence. Othermethods use self-consistency comparison but are affected by confirmationbiases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparingthe consistency between the direct answer generated using the VLM's internalreasoning process, and the indirect answers obtained by decomposing thequestion into sub-questions and reasoning over the sub-answers produced by theVLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experimentsacross six vision-language tasks with three VLMs show \texttt{DeCC}'sreliability estimation achieves better correlation with task accuracy comparedto the existing methods.</description><author>Qian Yang, Weixiang Yan, Aishwarya Agrawal</author><pubDate>Wed, 10 Jul 2024 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07840v1</guid></item><item><title>Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing</title><link>http://arxiv.org/abs/2407.08770v1</link><description>Large Language Models (LLMs) have demonstrated great potential as generalistassistants, showcasing powerful task understanding and problem-solvingcapabilities. To deploy LLMs as AI assistants, it is crucial that these modelsexhibit desirable behavioral traits, such as non-toxicity and resilienceagainst jailbreak attempts. Current methods for detoxification or preventingjailbreaking usually involve Supervised Fine-Tuning (SFT) or ReinforcementLearning from Human Feedback (RLHF), which requires finetuning billions ofparameters through gradient descent with substantial computation cost.Furthermore, models modified through SFT and RLHF may deviate from thepretrained models, potentially leading to a degradation in foundational LLMcapabilities. In this paper, we observe that surprisingly, directly editing asmall subset of parameters can effectively modulate specific behaviors of LLMs,such as detoxification and resistance to jailbreaking. Specifically, for abehavior that we aim to avoid, we employ a linear classifier, which we term thebehavior probe, to classify binary behavior labels within the hidden statespace of the LLM. Using this probe, we introduce an algorithm to identify acritical subset of LLM parameters that significantly influence this targetedbehavior. Then we directly edit these selected parameters by shifting themtowards the behavior probe. Such a direct parameter editing method necessitatesonly inference-level computational resources. Experiments demonstrate that inthe representative detoxification task, our approach achieves reductions of upto 90.0\% in toxicity on the RealToxicityPrompts dataset and 49.2\% on ToxiGen,while maintaining the LLM's general capabilities in areas such as common sense,question answering, and mathematics. Our code is available athttps://github.com/lucywang720/model-surgery.</description><author>Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang</author><pubDate>Thu, 11 Jul 2024 17:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08770v1</guid></item><item><title>Ramsey Theorems for Trees and a General 'Private Learning Implies Online Learning' Theorem</title><link>http://arxiv.org/abs/2407.07765v1</link><description>This work continues to investigate the link between differentially private(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed thatfor binary concept classes, DP learnability of a given class implies that ithas a finite Littlestone dimension (equivalently, that it is online learnable).Their proof relies on a model-theoretic result by Hodges (1997), whichdemonstrates that any binary concept class with a large Littlestone dimensioncontains a large subclass of thresholds. In a follow-up work, Jung, Kim, andTewari (2020) extended this proof to multiclass PAC learning with a boundednumber of labels. Unfortunately, Hodges's result does not apply in othernatural settings such as multiclass PAC learning with an unbounded label space,and PAC learning of partial concept classes. This naturally raises the question of whether DP learnability continues toimply online learnability in more general scenarios: indeed, Alon, Hanneke,Holzman, and Moran (2021) explicitly leave it as an open question in thecontext of partial concept classes, and the same question is open in thegeneral multiclass setting. In this work, we give a positive answer to thesequestions showing that for general classification tasks, DP learnabilityimplies online learnability. Our proof reasons directly about Littlestonetrees, without relying on thresholds. We achieve this by establishing severalRamsey-type theorems for trees, which might be of independent interest.</description><author>Simone Fioravanti, Steve Hanneke, Shay Moran, Hilla Schefler, Iska Tsubari</author><pubDate>Wed, 10 Jul 2024 15:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07765v1</guid></item><item><title>MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</title><link>http://arxiv.org/abs/2407.01523v2</link><description>Understanding documents with rich layouts and multi-modal components is along-standing and practical task. Recent Large Vision-Language Models (LVLMs)have made remarkable strides in various tasks, particularly in single-pagedocument understanding (DU). However, their abilities on long-context DU remainan open problem. This work presents MMLongBench-Doc, a long-context,multi-modal benchmark comprising 1,062 expert-annotated questions. Distinctfrom previous datasets, it is constructed upon 130 lengthy PDF-formatteddocuments with an average of 49.4 pages and 20,971 textual tokens. Towardscomprehensive evaluation, answers to these questions rely on pieces of evidencefrom (1) different sources (text, image, chart, table, and layout structure)and (2) various locations (i.e. page number). Moreover, 33.2% of the questionsare cross-page questions requiring evidence across multiple pages. 22.8% of thequestions are designed to be unanswerable for detecting potentialhallucinations. Experiments on 14 LVLMs demonstrate that long-context DUgreatly challenges current models. Notably, the best-performing model, GPT-4o,achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worseperformance than their LLM counterparts which are fed with lossy-parsed OCRdocuments. These results validate the necessity of future research toward morecapable long-context LVLMs. Project Page:https://mayubo2333.github.io/MMLongBench-Doc</description><author>Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun</author><pubDate>Wed, 10 Jul 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01523v2</guid></item><item><title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title><link>http://arxiv.org/abs/2403.01858v2</link><description>We present TMMLU+, a new benchmark designed for Traditional Chinese languageunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66subjects from elementary to professional level. It is six times larger andboasts a more balanced subject distribution than its predecessor, TaiwanMassive Multitask Language Understanding (TMMLU). We also benchmarkclosed-source models and 26 open-weight Chinese large language models (LLMs) ofparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings revealthat (1.) Traditional Chinese models still trail behind their SimplifiedChinese counterparts, highlighting a need for more focused advancements in LLMscatering to Traditional Chinese. (2.) Current LLMs still fall short of humanperformance in average scores, indicating a potential need for future researchto delve deeper into social science and humanities subjects. (3.) Among all thetokenization compression metrics examined, we identify that only the fertilityscore uniquely demonstrates strong correlations with our benchmark results. Weforesee that TMMLU+ will pinpoint areas for future model improvement, therebynarrowing the gap between machine and human linguistic capabilities andsupporting researchers in developing Traditional Chinese LLMs. Our dataset,along with the benchmark source code, is accessible athuggingface.co/datasets/ikala/tmmluplus.</description><author>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</author><pubDate>Wed, 10 Jul 2024 15:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01858v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item></channel></rss>