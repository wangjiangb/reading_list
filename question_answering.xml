<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 12 Mar 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Interactive Question Answering Systems: Literature Review</title><link>http://arxiv.org/abs/2209.01621v2</link><description>Question answering systems are recognized as popular and frequently effectivemeans of information seeking on the web. In such systems, information seekerscan receive a concise response to their query by presenting their questions innatural language. Interactive question answering is a recently proposed andincreasingly popular solution that resides at the intersection of questionanswering and dialogue systems. On the one hand, the user can ask questions innormal language and locate the actual response to her inquiry; on the otherhand, the system can prolong the question-answering session into a dialogue ifthere are multiple probable replies, very few, or ambiguities in the initialrequest. By permitting the user to ask more questions, interactive questionanswering enables users to dynamically interact with the system and receivemore precise results. This survey offers a detailed overview of the interactivequestion-answering methods that are prevalent in current literature. It beginsby explaining the foundational principles of question-answering systems, hencedefining new notations and taxonomies to combine all identified works inside aunified framework. The reviewed published work on interactivequestion-answering systems is then presented and examined in terms of itsproposed methodology, evaluation approaches, and dataset/application domain. Wealso describe trends surrounding specific tasks and issues raised by thecommunity, so shedding light on the future interests of scholars. Our work isfurther supported by a GitHub page with a synthesis of all the major topicscovered in this literature study.https://sisinflab.github.io/interactive-question-answering-systems-survey/</description><author>Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci</author><pubDate>Wed, 06 Mar 2024 21:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01621v2</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v3</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Thu, 07 Mar 2024 06:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v3</guid></item><item><title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge</title><link>http://arxiv.org/abs/2401.10712v2</link><description>With the breakthrough of multi-modal large language models, answering complexvisual questions that demand advanced reasoning abilities and world knowledgehas become a much more important testbed for developing AI models than ever.However, equipping AI models with robust cross-modality reasoning abilityremains challenging since the cognition scheme of humans has not beenunderstood systematically. In this paper, we believe that if we can collectvisual clues in the given image as much as possible, we will recognize theimage more accurately, understand the question better, recall relevantknowledge more easily, and finally reason out the answer. We discover theserich visual clues by mining question-answer pairs in images and sending theminto multi-modal large language models as prompts. We call the proposed methodQ&amp;A Prompts. Specifically, we first use the image-answer pairs and thecorresponding questions in the training set as inputs and outputs to train avisual question generation model. Then, we use an image tagging model toidentify various instances and send packaged image-tag pairs into the visualquestion generation model to generate relevant questions with the extractedimage tags as answers. Finally, we encode these generated question-answer pairsas prompts with a visual-aware prompting module and send them into pre-trainedmulti-modal large language models to reason out the final answers. Experimentalresults show that, compared with state-of-the-art methods, our Q&amp;A Promptsachieves substantial improvements on the challenging visual question answeringdatasets requiring reasoning over diverse world knowledge, such as OK-VQA andA-OKVQA.</description><author>Haibi Wang, Weifeng Ge</author><pubDate>Wed, 06 Mar 2024 12:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10712v2</guid></item><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Unsupervised LLM Adaptation for Question Answering</title><link>http://arxiv.org/abs/2402.12170v1</link><description>Large language models (LLM) learn diverse knowledge present in thelarge-scale training dataset via self-supervised training. Followed byinstruction-tuning, LLM acquires the ability to return correct information fordiverse questions. However, adapting these pre-trained LLMs to new targetdomains, such as different organizations or periods, for the question-answering(QA) task incurs a substantial annotation cost. To tackle this challenge, wepropose a novel task, unsupervised LLM adaptation for question answering. Inthis task, we leverage a pre-trained LLM, a publicly available QA dataset(source data), and unlabeled documents from the target domain. Our goal is tolearn LLM that can answer questions about the target domain. We introduce onesynthetic and two real datasets to evaluate models fine-tuned on the source andtarget data, and reveal intriguing insights; (i) fine-tuned models exhibit theability to provide correct answers for questions about the target domain eventhough they do not see any questions about the information described in theunlabeled documents, but (ii) they have difficulties in accessing informationlocated in the middle or at the end of documents, and (iii) this challenge canbe partially mitigated by replacing input tokens with random ones duringadaptation.</description><author>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</author><pubDate>Fri, 16 Feb 2024 06:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12170v1</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>Quantum Neural Network with Density Matrix for Question Answering and Classical Image Classification</title><link>http://arxiv.org/abs/2203.11155v4</link><description>Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.</description><author>X. Q. Zhao, T. L. Chen</author><pubDate>Wed, 06 Mar 2024 08:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11155v4</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v3</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Sat, 17 Feb 2024 07:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v3</guid></item><item><title>JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability</title><link>http://arxiv.org/abs/2402.17887v2</link><description>With the explosive growth of medical data and the rapid development ofartificial intelligence technology, precision medicine has emerged as a key toenhancing the quality and efficiency of healthcare services. In this context,Large Language Models (LLMs) play an increasingly vital role in medicalknowledge acquisition and question-answering systems. To further improve theperformance of these systems in the medical domain, we introduce an innovativemethod that jointly trains an Information Retrieval (IR) system and an LLMduring the fine-tuning phase. This approach, which we call Joint Medical LLMand Retrieval Training (JMLR), is designed to overcome the challenges faced bytraditional models in handling medical question-answering tasks. By employing asynchronized training mechanism, JMLR reduces the demand for computationalresources and enhances the model's ability to leverage medical knowledge forreasoning and answering questions. Our experimental results demonstrate thatJMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models usingconventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3%on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% onMedQA) significantly outperforms other public models (Meditron-7B: 50.1%,47.9%), proving its superiority in terms of cost (our training time: 37 hours,traditional method: 144 hours), efficiency, and effectiveness in medicalquestion-answering tasks. Through this work, we provide a new and efficientknowledge enhancement tool for healthcare, demonstrating the great potential ofintegrating IR and LLM training in precision medical information retrieval andquestion-answering systems.</description><author>Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu</author><pubDate>Sat, 02 Mar 2024 09:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17887v2</guid></item><item><title>EEE-QA: Exploring Effective and Efficient Question-Answer Representations</title><link>http://arxiv.org/abs/2403.02176v1</link><description>Current approaches to question answering rely on pre-trained language models(PLMs) like RoBERTa. This work challenges the existing question-answer encodingconvention and explores finer representations. We begin with testing variouspooling methods compared to using the begin-of-sentence token as a questionrepresentation for better quality. Next, we explore opportunities tosimultaneously embed all answer candidates with the question. This enablescross-reference between answer choices and improves inference throughput viareduced memory usage. Despite their simplicity and effectiveness, these methodshave yet to be widely studied in current frameworks. We experiment withdifferent PLMs, and with and without the integration of knowledge graphs.Results prove that the memory efficacy of the proposed techniques with littlesacrifice in performance. Practically, our work enhances 38-100% throughputwith 26-65% speedups on consumer-grade GPUs by allowing for considerably largerbatch sizes. Our work sends a message to the community with promisingdirections in both representation quality and efficiency for thequestion-answering task in natural language processing.</description><author>Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen</author><pubDate>Mon, 04 Mar 2024 16:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02176v1</guid></item><item><title>CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</title><link>http://arxiv.org/abs/2403.03203v1</link><description>The integration of learning and reasoning is high on the research agenda inAI. Nevertheless, there is only a little attention to use existing backgroundknowledge for reasoning about partially observed scenes to answer questionsabout the scene. Yet, we as humans use such knowledge frequently to inferplausible answers to visual questions (by eliminating all inconsistent ones).Such knowledge often comes in the form of constraints about objects and ittends to be highly domain or environment-specific. We contribute a novelbenchmark called CLEVR-POC for reasoning-intensive visual question answering(VQA) in partially observable environments under constraints. In CLEVR-POC,knowledge in the form of logical constraints needs to be leveraged to generateplausible answers to questions about a hidden object in a given partial scene.For instance, if one has the knowledge that all cups are colored either red,green or blue and that there is only one green cup, it becomes possible todeduce the color of an occluded cup as either red or blue, provided that allother cups, including the green one, are observed. Through experiments, weobserve that the low performance of pre-trained vision language models likeCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POCascertains the necessity for frameworks that can handle reasoning-intensivetasks where environment-specific background knowledge is available and crucial.Furthermore, our demonstration illustrates that a neuro-symbolic model, whichintegrates an LLM like GPT-4 with a visual perception network and a formallogical reasoner, exhibits exceptional performance on CLEVR-POC.</description><author>Savitha Sam Abraham, Marjan Alirezaie, Luc De Raedt</author><pubDate>Tue, 05 Mar 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03203v1</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v2</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Tue, 20 Feb 2024 19:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v2</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v3</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Fri, 01 Mar 2024 15:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v3</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Question Answering Over Spatio-Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2402.11542v1</link><description>Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledgegraphs (KGs) by incorporating time and location information. While the researchcommunity's focus on Knowledge Graph Question Answering (KGQA), the field ofanswering questions incorporating both spatio-temporal information based onSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasetsalso has hindered progress in this area. To address this issue, we presentSTQAD, a dataset comprising 10,000 natural language questions forspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,various state-of-the-art KGQA approaches fall far short of achievingsatisfactory performance on our dataset. In response, we propose STCQA, a newspatio-temporal KGQA approach that utilizes a novel STKG embedding method namedSTComplEx. By extracting temporal and spatial information from a question, ourQA model can better comprehend the question and retrieve accurate answers fromthe STKG. Through extensive experiments, we demonstrate the quality of ourdataset and the effectiveness of our STKGQA method.</description><author>Xinbang Dai, Huiying Li, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11542v1</guid></item><item><title>EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings</title><link>http://arxiv.org/abs/2402.16040v1</link><description>This study introduces EHRNoteQA, a novel patient-specific question answeringbenchmark tailored for evaluating Large Language Models (LLMs) in clinicalenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of threemedical professionals has curated the dataset comprising 962 unique questions,each linked to a specific patient's EHR clinical notes. What makes EHRNoteQAdistinct from existing EHR-based benchmarks is as follows: Firstly, it is thefirst dataset to adopt a multi-choice question answering format, a designchoice that effectively evaluates LLMs with reliable scores in the context ofautomatic evaluation, compared to other formats. Secondly, it requires ananalysis of multiple clinical notes to answer a single question, reflecting thecomplex nature of real-world clinical decision-making where clinicians reviewextensive records of patient histories. Our comprehensive evaluation on variouslarge language models showed that their scores on EHRNoteQA correlate moreclosely with their performance in addressing real-world medical questionsevaluated by clinicians than their scores from other LLM benchmarks. Thisunderscores the significance of EHRNoteQA in evaluating LLMs for medicalapplications and highlights its crucial role in facilitating the integration ofLLMs into healthcare systems. The dataset will be made available to the publicunder PhysioNet credential access, promoting further research in this vitalfield.</description><author>Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Seunghyun Won, Edward Choi</author><pubDate>Sun, 25 Feb 2024 09:41:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16040v1</guid></item><item><title>PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title><link>http://arxiv.org/abs/2402.11034v1</link><description>Existing work on Temporal Question Answering (TQA) has predominantly focusedon questions anchored to specific timestamps or events (e.g. "Who was the USpresident in 1970?"). Little work has studied questions whose temporal contextis relative to the present time (e.g. "Who was the previous US president?"). Werefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA posesunique challenges: (1) large language models (LLMs) may have outdatedknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') arehard to reason, (3) multi-hop reasoning may be required, and (4) the goldanswers of benchmarks must be continuously updated. To address thesechallenges, we introduce the PAT-Questions benchmark, which includes single andmulti-hop temporal questions. The answers in PAT-Questions can be automaticallyrefreshed by re-running SPARQL queries on a knowledge graph, if available. Weevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model(TEMPREASON-T5) on PAT-Questions through direct prompting andretrieval-augmented generation (RAG). The results highlight the limitations ofexisting solutions in PATQA and motivate the need for new methods to improvePATQA reasoning capabilities.</description><author>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</author><pubDate>Fri, 16 Feb 2024 19:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11034v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</title><link>http://arxiv.org/abs/2402.13188v1</link><description>Many models that leverage knowledge graphs (KGs) have recently demonstratedremarkable success in question answering (QA) tasks. In the real world, manyfacts contained in KGs are time-constrained thus temporal KGQA has receivedincreasing attention. Despite the fruitful efforts of previous models intemporal KGQA, they still have several limitations. (I) They adopt pre-trainedlanguage models (PLMs) to obtain question representations, while PLMs tend tofocus on entity information and ignore entity transfer caused by temporalconstraints, and finally fail to learn specific temporal representations ofentities. (II) They neither emphasize the graph structure between entities norexplicitly model the multi-hop relationship in the graph, which will make itdifficult to solve complex multi-hop question answering. To alleviate thisproblem, we propose a novel Question Calibration and Multi-Hop Modeling(QC-MHM) approach. Specifically, We first calibrate the question representationby fusing the question and the time-constrained concepts in KG. Then, weconstruct the GNN layer to complete multi-hop message passing. Finally, thequestion representation is combined with the embedding output by the GNN togenerate the final prediction. Empirical results verify that the proposed modelachieves better performance than the state-of-the-art models in the benchmarkdataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestionsdataset's complex questions are absolutely improved by 5.1% and 1.2% comparedto the best-performing baseline. Moreover, QC-MHM can generate interpretableand trustworthy predictions.</description><author>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang</author><pubDate>Tue, 20 Feb 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13188v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v2</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, etc. Our proposed framework, named PromptGIP, unifiesthese diverse image processing tasks within a universal framework. Inspired byNLP question answering (QA) techniques, we employ a visual prompting questionanswering paradigm. Specifically, we treat the input-output image pair as astructured question-answer sentence, thereby reprogramming the image processingtask as a prompting QA problem. PromptGIP can undertake diverse cross-domaintasks using provided visual prompts, eliminating the need for task-specificfinetuning. Our methodology offers a universal and adaptive solution to generalimage processing. While PromptGIP has demonstrated a certain degree ofout-of-domain task generalization capability, further research is expected tofully explore its more powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Wed, 21 Feb 2024 03:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v2</guid></item><item><title>Faithful Temporal Question Answering over Heterogeneous Sources</title><link>http://arxiv.org/abs/2402.15400v1</link><description>Temporal question answering (QA) involves time constraints, with phrases suchas "... in 2019" or "... before COVID". In the former, time is an explicitcondition, in the latter it is implicit. State-of-the-art methods havelimitations along three dimensions. First, with neural inference, timeconstraints are merely soft-matched, giving room to invalid or inexplicableanswers. Second, questions with implicit time are poorly supported. Third,answers come from a single source: either a knowledge base (KB) or a textcorpus. We propose a temporal QA system that addresses these shortcomings.First, it enforces temporal constraints for faithful answering with tangibleevidence. Second, it properly handles implicit questions. Third, it operatesover heterogeneous sources, covering KB, text and web tables in a unifiedmanner. The method has three stages: (i) understanding the question and itstemporal conditions, (ii) retrieving evidence from all sources, and (iii)faithfully answering the question. As implicit questions are sparse in priorbenchmarks, we introduce a principled method for generating diverse questions.Experiments show superior performance over a suite of baselines.</description><author>Zhen Jia, Philipp Christmann, Gerhard Weikum</author><pubDate>Fri, 23 Feb 2024 16:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15400v1</guid></item><item><title>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</title><link>http://arxiv.org/abs/2403.04640v1</link><description>This paper focuses on the challenge of answering questions in scenarios thatare composed of rich and complex dynamic audio-visual components. Althoughexisting Multimodal Large Language Models (MLLMs) can respond to audio-visualcontent, these responses are sometimes ambiguous and fail to describe specificaudio-visual events. To overcome this limitation, we introduce the CAT, whichenhances MLLM in three ways: 1) besides straightforwardly bridging audio andvideo, we design a clue aggregator that aggregates question-related clues indynamic audio-visual scenarios to enrich the detailed knowledge required forlarge language models. 2) CAT is trained on a mixed multimodal dataset,allowing direct application in audio-visual scenarios. Notably, we collect anaudio-visual joint instruction dataset named AVinstruct, to further enhance thecapacity of CAT to model cross-semantic correlations. 3) we propose AI-assistedambiguity-aware direct preference optimization, a strategy specialized inretraining the model to favor the non-ambiguity response and improve theability to localize specific audio-visual objects. Extensive experimentalresults demonstrate that CAT outperforms existing methods on multimodal tasks,especially in Audio-Visual Question Answering (AVQA) tasks. The codes and thecollected instructions are released at https://github.com/rikeilong/Bay-CAT.</description><author>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</author><pubDate>Thu, 07 Mar 2024 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04640v1</guid></item><item><title>ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</title><link>http://arxiv.org/abs/2209.08199v2</link><description>We present a new task and dataset, ScreenQA, for screen content understandingvia question answering. The existing screen datasets are focused either onstructure and component-level understanding, or on a much higher-levelcomposite task such as navigation and task completion. We attempt to bridge thegap between these two by annotating 86K question-answer pairs over the RICOdataset in hope to benchmark the screen reading comprehension capacity.</description><author>Yu-Chung Hsiao, Fedir Zubach, Maria Wang, Jindong Chen</author><pubDate>Thu, 22 Feb 2024 08:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08199v2</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v1</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 16 Feb 2024 20:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v1</guid></item><item><title>Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering</title><link>http://arxiv.org/abs/2309.08469v2</link><description>Modern open-domain question answering systems often rely on accurate andefficient retrieval components to find passages containing the facts necessaryto answer the question. Recently, neural retrievers have gained popularity overlexical alternatives due to their superior performance. However, most of thework concerns popular languages such as English or Chinese. For others, such asPolish, few models are available. In this work, we present Silver Retriever, aneural retriever for Polish trained on a diverse collection of manually orweakly labeled datasets. Silver Retriever achieves much better results thanother Polish models and is competitive with larger multilingual models.Together with the model, we open-source five new passage retrieval datasets.</description><author>Piotr Rybak, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08469v2</guid></item><item><title>Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering</title><link>http://arxiv.org/abs/2402.12728v1</link><description>Knowledge-based visual question answering (KVQA) has been extensively studiedto answer visual questions with external knowledge, e.g., knowledge graphs(KGs). While several attempts have been proposed to leverage large languagemodels (LLMs) as an implicit knowledge source, it remains challenging sinceLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,images, KGs and LLMs, cannot be readily aligned for complex scenarios. Totackle these, we present a novel modality-aware integration with LLMs for KVQA(MAIL). It carefully leverages multimodal knowledge for both imageunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stageprompting strategy with LLMs to densely embody the image into a scene graphwith detailed visual features; (ii) We construct a coupled concept graph bylinking the mentioned entities with external facts. (iii) A tailoredpseudo-siamese graph medium fusion is designed for sufficient multimodalfusion. We utilize the shared mentioned entities in two graphs as mediums tobridge a tight inter-modal exchange, while maximally preserving insightfulintra-modal learning by constraining the fusion within mediums. Extensiveexperiments on two benchmark datasets show the superiority of MAIL with 24xless resources.</description><author>Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang</author><pubDate>Tue, 20 Feb 2024 05:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12728v1</guid></item><item><title>CommVQA: Situating Visual Question Answering in Communicative Contexts</title><link>http://arxiv.org/abs/2402.15002v1</link><description>Current visual question answering (VQA) models tend to be trained andevaluated on image-question pairs in isolation. However, the questions peopleask are dependent on their informational needs and prior knowledge about theimage content. To evaluate how situating images within naturalistic contextsshapes visual questions, we introduce CommVQA, a VQA dataset consisting ofimages, image descriptions, real-world communicative scenarios where the imagemight appear (e.g., a travel website), and follow-up questions and answersconditioned on the scenario. We show that CommVQA poses a challenge for currentmodels. Providing contextual information to VQA models improves performancebroadly, highlighting the relevance of situating systems within a communicativescenario.</description><author>Nandita Shankar Naik, Christopher Potts, Elisa Kreiss</author><pubDate>Thu, 22 Feb 2024 22:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15002v1</guid></item><item><title>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering</title><link>http://arxiv.org/abs/2402.16313v1</link><description>Open-ended question answering requires models to find appropriate evidence toform well-reasoned, comprehensive and helpful answers. In practicalapplications, models also need to engage in extended discussions on potentialscenarios closely relevant to the question. With augmentation of retrievalmodule, open-source Large Language Models (LLMs) can produce coherent answersoften with different focuses, but are still sub-optimal in terms of reliableevidence selection and in-depth question analysis. In this paper, we propose anovel Chain-of-Discussion framework to leverage the synergy among multipleopen-source LLMs aiming to provide \textbf{more correct} and \textbf{morecomprehensive} answers for open-ended QA, although they are not strong enoughindividually. Our experiments show that discussions among multiple LLMs play avital role in enhancing the quality of answers. We release our data and code at\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.</description><author>Mingxu Tao, Dongyan Zhao, Yansong Feng</author><pubDate>Mon, 26 Feb 2024 05:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16313v1</guid></item><item><title>Answering Diverse Questions via Text Attached with Key Audio-Visual Clues</title><link>http://arxiv.org/abs/2403.06679v1</link><description>Audio-visual question answering (AVQA) requires reference to video contentand auditory information, followed by correlating the question to predict themost precise answer. Although mining deeper layers of audio-visual informationto interact with questions facilitates the multimodal fusion process, theredundancy of audio-visual parameters tends to reduce the generalization of theinference engine to multiple question-answer pairs in a single video. Indeed,the natural heterogeneous relationship between audiovisuals and text makes theperfect fusion challenging, to prevent high-level audio-visual semantics fromweakening the network's adaptability to diverse question types, we propose aframework for performing mutual correlation distillation (MCD) to aid questioninference. MCD is divided into three main steps: 1) firstly, the residualstructure is utilized to enhance the audio-visual soft associations based onself-attention, then key local audio-visual features relevant to the questioncontext are captured hierarchically by shared aggregators and coupled in theform of clues with specific question vectors. 2) Secondly, knowledgedistillation is enforced to align audio-visual-text pairs in a shared latentspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visualdependencies are decoupled by discarding the decision-level integrations. Weevaluate the proposed method on two publicly available datasets containingmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments showthat our method outperforms other state-of-the-art methods, and one interestingfinding behind is that removing deep audio-visual features during inference caneffectively mitigate overfitting. The source code is released athttp://github.com/rikeilong/MCD-forAVQA.</description><author>Qilang Ye, Zitong Yu, Xin Liu</author><pubDate>Mon, 11 Mar 2024 13:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06679v1</guid></item><item><title>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</title><link>http://arxiv.org/abs/2402.11161v1</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current answer correctness (AC) metrics do not align with human judgments,particularly verbose, free form answers from large language models (LLM). Thereare two challenges: a lack of data and that models are too big. LLM basedscorers correlate better with humans, but this expensive task has only beentested on limited QA datasets. We rectify these issues by providing clearguidelines for evaluating machine QA adopted from human QA contests. We alsointroduce Precise ANswer correctness Determination and Adjudication (PANDA), asmall, efficient, deterministic AC classifier (812 KB) that more accuratelyevaluates answer correctness.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</author><pubDate>Sat, 17 Feb 2024 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11161v1</guid></item><item><title>Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education</title><link>http://arxiv.org/abs/2402.14293v1</link><description>In the domain of Natural Language Processing (NLP), Large Language Models(LLMs) have demonstrated promise in text-generation tasks. However, theireducational applications, particularly for domain-specific queries, remainunderexplored. This study investigates LLMs' capabilities in educationalscenarios, focusing on concept graph recovery and question-answering (QA). Weassess LLMs' zero-shot performance in creating domain-specific concept graphsand introduce TutorQA, a new expert-verified NLP-focused benchmark forscientific graph reasoning and QA. TutorQA consists of five tasks with 500 QApairs. To tackle TutorQA queries, we present CGLLM, a pipeline integratingconcept graphs with LLMs for answering diverse questions. Our results indicatethat LLMs' zero-shot concept graph recovery is competitive with supervisedmethods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMsachieve up to 26% F1 score enhancement. Moreover, human evaluation and analysisshow that CGLLM generates answers with more fine-grained concepts.</description><author>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</author><pubDate>Thu, 22 Feb 2024 05:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14293v1</guid></item><item><title>Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering</title><link>http://arxiv.org/abs/2403.05217v1</link><description>Open-domain question answering (ODQA) has emerged as a pivotal researchspotlight in information systems. Existing methods follow two main paradigms tocollect evidence: (1) The \textit{retrieve-then-read} paradigm retrievespertinent documents from an external corpus; and (2) the\textit{generate-then-read} paradigm employs large language models (LLMs) togenerate relevant documents. However, neither can fully address multifacetedrequirements for evidence. To this end, we propose LLMQA, a generalizedframework that formulates the ODQA process into three basic steps: queryexpansion, document selection, and answer generation, combining the superiorityof both retrieval-based and generation-based evidence. Since LLMs exhibit theirexcellent capabilities to accomplish various tasks, we instruct LLMs to playmultiple roles as generators, rerankers, and evaluators within our framework,integrating them to collaborate in the ODQA process. Furthermore, we introducea novel prompt optimization algorithm to refine role-playing prompts and steerLLMs to produce higher-quality evidence and answers. Extensive experimentalresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate thatLLMQA achieves the best performance in terms of both answer accuracy andevidence quality, showcasing its potential for advancing ODQA research andapplications.</description><author>Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan</author><pubDate>Fri, 08 Mar 2024 11:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05217v1</guid></item><item><title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges</title><link>http://arxiv.org/abs/2307.11471v2</link><description>Visual question answering requires a system to provide an accurate naturallanguage answer given an image and a natural language question. However, it iswidely recognized that previous generic VQA methods often exhibit a tendency tomemorize biases present in the training data rather than learning properbehaviors, such as grounding images before predicting answers. Therefore, thesemethods usually achieve high in-distribution but poor out-of-distributionperformance. In recent years, various datasets and debiasing methods have beenproposed to evaluate and enhance the VQA robustness, respectively. This paperprovides the first comprehensive survey focused on this emerging fashion.Specifically, we first provide an overview of the development process ofdatasets from in-distribution and out-of-distribution perspectives. Then, weexamine the evaluation metrics employed by these datasets. Thirdly, we proposea typology that presents the development process, similarities and differences,robustness comparison, and technical features of existing debiasing methods.Furthermore, we analyze and discuss the robustness of representativevision-and-language pre-training models on VQA. Finally, through a thoroughreview of the available literature and experimental analysis, we discuss thekey areas for future research from various viewpoints.</description><author>Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao</author><pubDate>Sun, 18 Feb 2024 08:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11471v2</guid></item><item><title>PolQA: Polish Question Answering Dataset</title><link>http://arxiv.org/abs/2212.08897v2</link><description>Recently proposed systems for open-domain question answering (OpenQA) requirelarge amounts of training data to achieve state-of-the-art performance.However, data annotation is known to be time-consuming and therefore expensiveto acquire. As a result, the appropriate datasets are available only for ahandful of languages (mainly English and Chinese). In this work, we introduceand publicly release PolQA, the first Polish dataset for OpenQA. It consists of7,000 questions, 87,525 manually labeled evidence passages, and a corpus ofover 7,097,322 candidate passages. Each question is classified according to itsformulation, type, as well as entity type of the answer. This resource allowsus to evaluate the impact of different annotation choices on the performance ofthe QA system and propose an efficient annotation strategy that increases thepassage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation costby 82%.</description><author>Piotr Rybak, Piotr Przybyła, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08897v2</guid></item><item><title>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</title><link>http://arxiv.org/abs/2402.11083v1</link><description>Visual Question Answering (VQA) is a fundamental task in computer vision andnatural language process fields. Although the ``pre-training &amp; finetuning''learning paradigm significantly improves the VQA performance, the adversarialrobustness of such a learning paradigm has not been explored. In this paper, wedelve into a new problem: using a pre-trained multimodal source model to createadversarial image-text pairs and then transferring them to attack the targetVQA models. Correspondingly, we propose a novel VQAttack model, which caniteratively generate both image and text perturbations with the designedmodules: the large language model (LLM)-enhanced image attack and thecross-modal joint attack module. At each iteration, the LLM-enhanced imageattack module first optimizes the latent representation-based loss to generatefeature-level image perturbations. Then it incorporates an LLM to furtherenhance the image perturbations by optimizing the designed masked answeranti-recovery loss. The cross-modal joint attack module will be triggered at aspecific iteration, which updates the image and text perturbationssequentially. Notably, the text perturbation updates are based on both thelearned gradients in the word embedding space and word synonym-basedsubstitution. Experimental results on two VQA datasets with five validatedmodels demonstrate the effectiveness of the proposed VQAttack in thetransferable attack setting, compared with state-of-the-art baselines. Thiswork reveals a significant blind spot in the ``pre-training &amp; fine-tuning''paradigm on VQA tasks. Source codes will be released.</description><author>Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</author><pubDate>Fri, 16 Feb 2024 21:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11083v1</guid></item><item><title>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation</title><link>http://arxiv.org/abs/2310.13505v3</link><description>Models for conversational question answering (ConvQA) over knowledge graphs(KGs) are usually trained and tested on benchmarks of gold QA pairs. Thisimplies that training is limited to surface forms seen in the respectivedatasets, and evaluation is on a small set of held-out questions. Through ourproposed framework REIGN, we take several steps to remedy this restrictedlearning setup. First, we systematically generate reformulations of trainingquestions to increase robustness of models to surface form variations. This isa particularly challenging problem, given the incomplete nature of suchquestions. Second, we guide ConvQA models towards higher performance by feedingit only those reformulations that help improve their answering quality, usingdeep reinforcement learning. Third, we demonstrate the viability of trainingmajor model components on one benchmark and applying them zero-shot to another.Finally, for a rigorous evaluation of robustness for trained models, we use andrelease large numbers of diverse reformulations generated by prompting GPT forbenchmark test sets (resulting in 20x increase in sizes). Our findings showthat ConvQA models with robust training via reformulations, significantlyoutperform those with standard training from gold QA pairs only.</description><author>Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum</author><pubDate>Fri, 16 Feb 2024 19:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13505v3</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision</title><link>http://arxiv.org/abs/2402.16508v1</link><description>Cross-lingual question answering (CLQA) is a complex problem, comprisingcross-lingual retrieval from a multilingual knowledge base, followed by answergeneration either in English or the query language. Both steps are usuallytackled by separate models, requiring substantial annotated datasets, andtypically auxiliary resources, like machine translation systems to bridgebetween languages. In this paper, we show that CLQA can be addressed using asingle encoder-decoder model. To effectively train this model, we propose aself-supervised method based on exploiting the cross-lingual link structurewithin Wikipedia. We demonstrate how linked Wikipedia pages can be used tosynthesise supervisory signals for cross-lingual retrieval, through a form ofcloze query, and generate more natural queries to supervise answer generation.Together, we show our approach, \texttt{CLASS}, outperforms comparable methodson both supervised and zero-shot language adaptation settings, including thoseusing machine translation.</description><author>Fan Jiang, Tom Drummond, Trevor Cohn</author><pubDate>Mon, 26 Feb 2024 11:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16508v1</guid></item><item><title>FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models</title><link>http://arxiv.org/abs/2402.14116v1</link><description>One type of question that is commonly found in day-to-day scenarios is``fan-out'' questions, complex multi-hop, multi-document reasoning questionsthat require finding information about a large number of entities. However,there exist few resources to evaluate this type of question-answeringcapability among large language models. To evaluate complex reasoning in LLMsmore fully, we present FanOutQA, a high-quality dataset of fan-outquestion-answer pairs and human-annotated decompositions with English Wikipediaas the knowledge base. We formulate three benchmark settings across our datasetand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,finding that contemporary models still have room to improve reasoning overinter-document dependencies in a long context. We provide our dataset andopen-source tools to run models to encourage evaluation at https://fanoutqa.com</description><author>Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch</author><pubDate>Wed, 21 Feb 2024 20:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14116v1</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models</title><link>http://arxiv.org/abs/2402.13653v1</link><description>We introduce the novel task of zero-shot Protein Question Answering (PQA) forfree-form scientific enquiry. Given a previously unseen protein sequence and anatural language question, the task is to deliver a scientifically accurateanswer. This task not only supports future biological research, but could alsoprovide a test bed for assessing the scientific precision of large languagemodels (LLMs). We contribute the first specialized dataset for PQA modeltraining, containing 257K protein sequences annotated with 1.97M scientificquestion-answer pairs. Additionally, we propose and study several novelbiologically relevant benchmarks for scientific PQA. Employing two robustmulti-modal architectures, we establish an initial state-of-the-art performancefor PQA and reveal key performance factors through ablation studies. Ourcomprehensive PQA framework, named Pika, including dataset, code, modelcheckpoints, and a user-friendly demo, is openly accessible ongithub.com/EMCarrami/Pika, promoting wider research and application in thefield.</description><author>Eli M Carrami, Sahand Sharifzadeh</author><pubDate>Wed, 21 Feb 2024 09:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13653v1</guid></item><item><title>Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA</title><link>http://arxiv.org/abs/2402.15933v1</link><description>In 3D Visual Question Answering (3D VQA), the scarcity of fully annotateddata and limited visual content diversity hampers the generalization to novelscenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA andSQA dataset). Current approaches resort supplement 3D reasoning with 2Dinformation. However, these methods face challenges: either they use top-down2D views that introduce overly complex and sometimes question-irrelevant visualclues, or they rely on globally aggregated scene/image-level representationsfrom 2D VLMs, losing the fine-grained vision-language correlations. To overcomethese limitations, our approach utilizes question-conditional 2D view selectionprocedure, pinpointing semantically relevant 2D inputs for crucial visualclues. We then integrate this 2D knowledge into the 3D-VQA system via atwo-branch Transformer structure. This structure, featuring a Twin-Transformerdesign, compactly combines 2D and 3D modalities and captures fine-grainedcorrelations between modalities, allowing them mutually augmenting each other.Integrating proposed mechanisms above, we present BridgeQA, that offers a freshperspective on multi-modal transformer-based architectures for 3D-VQA.Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasetsand significantly outperforms existing solutions. Code is available at$\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.</description><author>Wentao Mo, Yang Liu</author><pubDate>Sat, 24 Feb 2024 23:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15933v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations</title><link>http://arxiv.org/abs/2403.01469v2</link><description>We introduce KorMedMCQA, the first Korean multiple-choice question answering(MCQA) benchmark derived from Korean healthcare professional licensingexaminations, covering from the year 2012 to year 2023. This dataset consistsof a selection of questions from the license examinations for doctors, nurses,and pharmacists, featuring a diverse array of subjects. We conduct baselineexperiments on various large language models, includingproprietary/open-source, multilingual/Korean-additional pretrained, andclinical context pretrained models, highlighting the potential for furtherenhancements. We make our data publicly available on HuggingFace(https://huggingface.co/datasets/sean0042/KorMedMCQA) and provide a evaluationscript via LM-Harness, inviting further exploration and advancement in Koreanhealthcare environments.</description><author>Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi</author><pubDate>Tue, 05 Mar 2024 09:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01469v2</guid></item><item><title>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</title><link>http://arxiv.org/abs/2402.12869v1</link><description>Augmenting Large Language Models (LLMs) for Question Answering (QA) withdomain specific data has attracted wide attention. However, domain data oftenexists in a hybrid format, including text and semi-structured tables, posingchallenges for the seamless integration of information. Table-to-TextGeneration is a promising solution by facilitating the transformation of hybriddata into a uniformly text-formatted corpus. Although this technique has beenwidely studied by the NLP community, there is currently no comparative analysison how corpora generated by different table-to-text methods affect theperformance of QA systems. In this paper, we address this research gap in twosteps. First, we innovatively integrate table-to-text generation into theframework of enhancing LLM-based QA systems with domain hybrid data. Then, weutilize this framework in real-world industrial data to conduct extensiveexperiments on two types of QA systems (DSFT and RAG frameworks) with fourrepresentative methods: Markdown format, Template serialization, TPLM-basedmethod, and LLM-based method. Based on the experimental results, we draw someempirical findings and explore the underlying reasons behind the success ofsome methods. We hope the findings of this work will provide a valuablereference for the academic and industrial communities in developing robust QAsystems.</description><author>Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang</author><pubDate>Tue, 20 Feb 2024 10:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12869v1</guid></item><item><title>Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models</title><link>http://arxiv.org/abs/2402.15131v1</link><description>This study explores the realm of knowledge-base question answering (KBQA).KBQA is considered a challenging task, particularly in parsing intricatequestions into executable logical forms. Traditional semantic parsing(SP)-based methods require extensive data annotations, which result insignificant costs. Recently, the advent of few-shot in-context learning,powered by large language models (LLMs), has showcased promising capabilities.Yet, fully leveraging LLMs to parse questions into logical forms inlow-resource scenarios poses a substantial challenge. To tackle these hurdles,we introduce Interactive-KBQA, a framework designed to generate logical formsthrough direct interaction with knowledge bases (KBs). Within this framework,we have developed three generic APIs for KB interaction. For each category ofcomplex question, we devised exemplars to guide LLMs through the reasoningprocesses. Our method achieves competitive results on the WebQuestionsSP,ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number ofexamples (shots). Importantly, our approach supports manual intervention,allowing for the iterative refinement of LLM outputs. By annotating a datasetwith step-wise reasoning processes, we showcase our model's adaptability andhighlight its potential for contributing significant enhancements to the field.</description><author>Guanming Xiong, Junwei Bao, Wen Zhao</author><pubDate>Fri, 23 Feb 2024 06:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15131v1</guid></item><item><title>A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction</title><link>http://arxiv.org/abs/2402.11177v1</link><description>Electronic health records (EHRs) hold significant value for research andapplications. As a new way of information extraction, question answering (QA)can extract more flexible information than conventional methods and is moreaccessible to clinical researchers, but its progress is impeded by the scarcityof annotated data. In this paper, we propose a novel approach thatautomatically generates training data for transfer learning of QA models. Ourpipeline incorporates a preprocessing module to handle challenges posed byextraction types that are not readily compatible with extractive QA frameworks,including cases with discontinuous answers and many-to-one relationships. Theobtained QA model exhibits excellent performance on subtasks of informationextraction in EHRs, and it can effectively handle few-shot or zero-shotsettings involving yes-no questions. Case studies and ablation studiesdemonstrate the necessity of each component in our design, and the resultingmodel is deemed suitable for practical use.</description><author>Huaiyuan Ying, Sheng Yu</author><pubDate>Sat, 17 Feb 2024 02:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11177v1</guid></item><item><title>Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research</title><link>http://arxiv.org/abs/2402.16038v1</link><description>In recent years, advancements in natural language processing (NLP) have beenfueled by deep learning techniques, particularly through the utilization ofpowerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,trained on vast amounts of data, have revolutionized language understanding andgeneration. These pre-trained models serve as robust bases for various tasksincluding semantic understanding, intelligent writing, and reasoning, pavingthe way for a more generalized form of artificial intelligence. NLP, as a vitalapplication of AI, aims to bridge the gap between humans and computers throughnatural language interaction. This paper delves into the current landscape andfuture prospects of large-scale model-based NLP, focusing on thequestion-answering systems within this domain. Practical cases and developmentsin artificial intelligence-driven question-answering systems are analyzed tofoster further exploration and research in the realm of large-scale NLP.</description><author>Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong</author><pubDate>Sun, 25 Feb 2024 09:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16038v1</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation</title><link>http://arxiv.org/abs/2403.02707v1</link><description>Leveraging pre-trained visual language models has become a widely adoptedapproach for improving performance in downstream visual question answering(VQA) applications. However, in the specialized field of medical VQA, thescarcity of available data poses a significant barrier to achieving reliablemodel generalization. Numerous methods have been proposed to enhance modelgeneralization, addressing the issue from data-centric and model-centricperspectives. Data augmentation techniques are commonly employed to enrich thedataset, while various regularization approaches aim to prevent modeloverfitting, especially when training on limited data samples. In this paper,we introduce a method that incorporates gradient-guided parameter perturbationsto the visual encoder of the multimodality model during both pre-training andfine-tuning phases, to improve model generalization for downstream medical VQAtasks. The small perturbation is adaptively generated by aligning with thedirection of the moving average gradient in the optimization landscape, whichis opposite to the directions of the optimizer's historical updates. It issubsequently injected into the model's visual encoder. The results show that,even with a significantly smaller pre-training image caption dataset, ourapproach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.</description><author>Gang Liu, Hongyang Li, Zerui He, Shenjun Zhong</author><pubDate>Tue, 05 Mar 2024 06:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02707v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering</title><link>http://arxiv.org/abs/2402.11194v2</link><description>Large Language Models (LLMs), excel in natural language understanding, buttheir capability for complex mathematical reasoning with an amalgamation ofstructured tables and unstructured text is uncertain. This study explores LLMs'mathematical reasoning on four financial tabular question-answering datasets:TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments withvarious models and prompting techniques, we assess how LLMs adapt to complextables and mathematical tasks. We focus on sensitivity to table complexity andperformance variations with an increasing number of arithmetic reasoning steps.The results provide insights into LLMs' capabilities and limitations inhandling complex mathematical scenarios for semi-structured tables. Ultimately,we introduce a novel prompting technique tailored to semi-structured documents,matching or outperforming other baselines in performance while providing anuanced understanding of LLMs abilities for such a task.</description><author>Pragya Srivastava, Manuj Malik, Vivek Gupta, Tanuja Ganu, Dan Roth</author><pubDate>Thu, 29 Feb 2024 09:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11194v2</guid></item><item><title>Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering</title><link>http://arxiv.org/abs/2402.11194v1</link><description>Large Language Models (LLMs), excel in natural language understanding, buttheir capability for complex mathematical reasoning with an amalgamation ofstructured tables and unstructured text is uncertain. This study explores LLMs'mathematical reasoning on four financial tabular question-answering datasets:TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments withvarious models and prompting techniques, we assess how LLMs adapt to complextables and mathematical tasks. We focus on sensitivity to table complexity andperformance variations with an increasing number of arithmetic reasoning steps.The results provide insights into LLMs' capabilities and limitations inhandling complex mathematical scenarios for semi-structured tables. Ultimately,we introduce a novel prompting technique tailored to semi-structured documents,matching or outperforming other baselines in performance while providing anuanced understanding of LLMs abilities for such a task.</description><author>Pragya Srivastava, Manuj Malik, Tanuja Ganu</author><pubDate>Sat, 17 Feb 2024 05:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11194v1</guid></item><item><title>FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning</title><link>http://arxiv.org/abs/2402.12692v2</link><description>The application of formulas is a fundamental ability of humans whenaddressing numerical reasoning problems. However, existing numerical reasoningdatasets seldom explicitly indicate the formulas employed during the reasoningsteps. To bridge this gap, we propose a question answering dataset forformula-based numerical reasoning called FormulaQA, from junior high schoolphysics examinations. We further conduct evaluations on LLMs with size rangingfrom 7B to over 100B parameters utilizing zero-shot and few-shotchain-of-thoughts methods and we explored the approach of usingretrieval-augmented LLMs when providing an external formula database. We alsofine-tune on smaller models with size not exceeding 2B. Our empirical findingsunderscore the significant potential for improvement in existing models whenapplied to our complex, formula-driven FormulaQA.</description><author>Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei Liu, Gong Cheng</author><pubDate>Wed, 21 Feb 2024 02:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12692v2</guid></item><item><title>FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning</title><link>http://arxiv.org/abs/2402.12692v1</link><description>The application of formulas is a fundamental ability of humans whenaddressing numerical reasoning problems. However, existing numerical reasoningdatasets seldom explicitly indicate the formulas employed during the reasoningsteps. To bridge this gap, we propose a question answering dataset forformula-based numerical reasoning called FormulaQA, from junior high schoolphysics examinations. We further conduct evaluations on LLMs with size rangingfrom 7B to over 100B parameters utilizing zero-shot and few-shotchain-of-thoughts methods and we explored the approach of usingretrieval-augmented LLMs when providing an external formula database. We alsofine-tune on smaller models with size not exceeding 2B. Our empirical findingsunderscore the significant potential for improvement in existing models whenapplied to our complex, formula-driven FormulaQA.</description><author>Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei liu, Gong Cheng</author><pubDate>Tue, 20 Feb 2024 03:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12692v1</guid></item><item><title>Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review</title><link>http://arxiv.org/abs/2403.02469v1</link><description>Medical vision-language models (VLMs) combine computer vision and naturallanguage processing to analyze visual and textual medical data. Our paperreviews recent advancements in developing VLMs specialized for healthcare,focusing on models designed for medical report generation and visual questionanswering. We provide background on natural language processing and computervision, explaining how techniques from both fields are integrated into VLMs toenable learning from multimodal data. Key areas we address include theexploration of medical vision-language datasets, in-depth analyses ofarchitectures and pre-training strategies employed in recent noteworthy medicalVLMs, and comprehensive discussion on evaluation metrics for assessing VLMs'performance in medical report generation and visual question answering. We alsohighlight current challenges and propose future directions, including enhancingclinical validity and addressing patient privacy concerns. Overall, our reviewsummarizes recent progress in developing VLMs to harness multimodal medicaldata for improved healthcare applications.</description><author>Iryna Hartsock, Ghulam Rasool</author><pubDate>Mon, 04 Mar 2024 20:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02469v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v2</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Thu, 15 Feb 2024 10:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v2</guid></item><item><title>Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond</title><link>http://arxiv.org/abs/2402.14259v1</link><description>Uncertainty estimation plays a pivotal role in ensuring the reliability ofsafety-critical human-AI interaction systems, particularly in the medicaldomain. However, a general method for quantifying the uncertainty of free-formanswers has yet to be established in open-ended medical question-answering (QA)tasks, where irrelevant words and sequences with limited semantic informationcan be the primary source of uncertainty due to the presence of generativeinequality. In this paper, we propose the Word-Sequence Entropy (WSE), whichcalibrates the uncertainty proportion at both the word and sequence levelsaccording to the semantic relevance, with greater emphasis placed on keywordsand more relevant sequences when performing uncertainty quantification. Wecompare WSE with 6 baseline methods on 5 free-form medical QA datasets,utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSEexhibits superior performance on accurate uncertainty measurement under twostandard criteria for correctness evaluation (e.g., WSE outperforms existingstate-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, interms of the potential for real-world medical QA applications, we achieve asignificant enhancement in the performance of LLMs when employing sequenceswith lower uncertainty, identified by WSE, as final answers (e.g., +6.36%accuracy improvement on the COVID-QA dataset), without requiring any additionaltask-specific fine-tuning or architectural modifications.</description><author>Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi</author><pubDate>Thu, 22 Feb 2024 03:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14259v1</guid></item><item><title>Zero-shot sampling of adversarial entities in biomedical question answering</title><link>http://arxiv.org/abs/2402.10527v1</link><description>The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.</description><author>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</author><pubDate>Fri, 16 Feb 2024 09:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10527v1</guid></item><item><title>Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?</title><link>http://arxiv.org/abs/2402.18419v1</link><description>Health insurance companies have a defined process called prior authorization(PA) which is a health plan cost-control process that requires doctors andother healthcare professionals to get clearance in advance from a health planbefore performing a particular procedure on a patient in order to be eligiblefor payment coverage. For health insurance companies, approving PA requests forpatients in the medical domain is a time-consuming and challenging task. One ofthose key challenges is validating if a request matches up to certain criteriasuch as age, gender, etc. In this work, we evaluate whether GPT can validatenumerous key factors, in turn helping health plans reach a decision drasticallyfaster. We frame it as a question answering task, prompting GPT to answer aquestion from patient electronic health record. We experiment with differentconventional prompting techniques as well as introduce our own novel promptingtechnique. Moreover, we report qualitative assessment by humans on the naturallanguage generation outputs from our approach. Results show that our methodachieves superior performance with the mean weighted F1 score of 0.61 ascompared to its standard counterparts.</description><author>Shubham Vatsal, Ayush Singh, Shabnam Tafreshi</author><pubDate>Wed, 28 Feb 2024 15:39:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18419v1</guid></item><item><title>Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2402.14320v1</link><description>Recent progress with LLM-based agents has shown promising results acrossvarious tasks. However, their use in answering questions from knowledge basesremains largely unexplored. Implementing a KBQA system using traditionalmethods is challenging due to the shortage of task-specific training data andthe complexity of creating task-focused model structures. In this paper, wepresent Triad, a unified framework that utilizes an LLM-based agent with threeroles for KBQA tasks. The agent is assigned three roles to tackle differentKBQA subtasks: agent as a generalist for mastering various subtasks, as adecision maker for the selection of candidates, and as an advisor for answeringquestions with knowledge. Our KBQA framework is executed in four phases,involving the collaboration of the agent's multiple roles. We evaluated theperformance of our framework using three benchmark datasets, and the resultsshow that our framework outperforms state-of-the-art systems on the LC-QuAD andYAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.</description><author>Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting Zhuang</author><pubDate>Thu, 22 Feb 2024 06:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14320v1</guid></item><item><title>Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark</title><link>http://arxiv.org/abs/2402.19248v1</link><description>How to better evaluate the capabilities of Large Language Models (LLMs) isthe focal point and hot topic in current LLMs research. Previous work has notedthat due to the extremely high cost of iterative updates of LLMs, they areoften unable to answer the latest dynamic questions well. To promote theimprovement of Chinese LLMs' ability to answer dynamic questions, in thispaper, we introduce CDQA, a Chinese Dynamic QA benchmark containingquestion-answer pairs related to the latest news on the Chinese Internet. Weobtain high-quality data through a pipeline that combines humans and models,and carefully classify the samples according to the frequency of answer changesto facilitate a more fine-grained observation of LLMs' capabilities. We havealso evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.Extensive experiments and valuable insights suggest that our proposed CDQA ischallenging and worthy of more further study. We believe that the benchmark weprovide will become the key data resource for improving LLMs' Chinesequestion-answering ability in the future.</description><author>Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Chang Zhou, Fei Huang</author><pubDate>Thu, 29 Feb 2024 15:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19248v1</guid></item><item><title>Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark</title><link>http://arxiv.org/abs/2402.19248v2</link><description>How to better evaluate the capabilities of Large Language Models (LLMs) isthe focal point and hot topic in current LLMs research. Previous work has notedthat due to the extremely high cost of iterative updates of LLMs, they areoften unable to answer the latest dynamic questions well. To promote theimprovement of Chinese LLMs' ability to answer dynamic questions, in thispaper, we introduce CDQA, a Chinese Dynamic QA benchmark containingquestion-answer pairs related to the latest news on the Chinese Internet. Weobtain high-quality data through a pipeline that combines humans and models,and carefully classify the samples according to the frequency of answer changesto facilitate a more fine-grained observation of LLMs' capabilities. We havealso evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.Extensive experiments and valuable insights suggest that our proposed CDQA ischallenging and worthy of more further study. We believe that the benchmark weprovide will become one of the key data resources for improving LLMs' Chinesequestion-answering ability in the future.</description><author>Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Fei Huang</author><pubDate>Sat, 02 Mar 2024 04:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19248v2</guid></item><item><title>RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</title><link>http://arxiv.org/abs/2402.16457v1</link><description>Adaptive retrieval-augmented generation (ARAG) aims to dynamically determinethe necessity of retrieval for queries instead of retrieving indiscriminatelyto enhance the efficiency and relevance of the sourced information. However,previous works largely overlook the evaluation of ARAG approaches, leading totheir effectiveness being understudied. This work presents a benchmark,RetrievalQA, comprising 1,271 short-form questions covering new world andlong-tail knowledge. The knowledge necessary to answer the questions is absentfrom LLMs; therefore, external information must be retrieved to answercorrectly. This makes RetrievalQA a suitable testbed to evaluate existing ARAGmethods. We observe that calibration-based methods heavily rely on thresholdtuning, while vanilla prompting is inadequate for guiding LLMs to make reliableretrieval decisions. Based on our findings, we propose Time-Aware AdaptiveRetrieval (TA-ARE), a simple yet effective method that helps LLMs assess thenecessity of retrieval without calibration or additional training. The datasetand code will be available at \url{https://github.com/hyintell/RetrievalQA}</description><author>Zihan Zhang, Meng Fang, Ling Chen</author><pubDate>Mon, 26 Feb 2024 09:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16457v1</guid></item><item><title>QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners</title><link>http://arxiv.org/abs/2402.07913v2</link><description>In online learning platforms, particularly in rapidly growing computerprogramming courses, addressing the thousands of students' learning queriesrequires considerable human cost. The creation of intelligent assistant largelanguage models (LLMs) tailored for programming education necessitates distinctdata support. However, in real application scenarios, the data resources fortraining such LLMs are relatively scarce. Therefore, to address the datascarcity in intelligent educational systems for programming, this paperproposes a new Chinese question-and-answer dataset for Python learners. Toensure the authenticity and reliability of the sources of the questions, wecollected questions from actual student questions and categorized themaccording to various dimensions such as the type of questions and the type oflearners. This annotation principle is designed to enhance the effectivenessand quality of online programming education, providing a solid data foundationfor developing the programming teaching assists (TA). Furthermore, we conductedcomprehensive evaluations of various LLMs proficient in processing andgenerating Chinese content, highlighting the potential limitations of generalLLMs as intelligent teaching assistants in computer programming courses.</description><author>Rui Xiao, Lu Han, Xiaoying Zhou, Jiong Wang, Na Zong, Pengyu Zhang</author><pubDate>Fri, 23 Feb 2024 02:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07913v2</guid></item><item><title>Automatic Answerability Evaluation for Question Generation</title><link>http://arxiv.org/abs/2309.12546v2</link><description>Conventional automatic evaluation metrics, such as BLEU and ROUGE, developedfor natural language generation (NLG) tasks, are based on measuring the n-gramoverlap between the generated and reference text. These simple metrics may beinsufficient for more complex tasks, such as question generation (QG), whichrequires generating questions that are answerable by the reference answers.Developing a more sophisticated automatic evaluation metric, thus, remains anurgent problem in QG research. This work proposes PMAN (Prompting-based Metricon ANswerability), a novel automatic evaluation metric to assess whether thegenerated questions are answerable by the reference answers for the QG tasks.Extensive experiments demonstrate that its evaluation results are reliable andalign with human evaluations. We further apply our metric to evaluate theperformance of QG models, which shows that our metric complements conventionalmetrics. Our implementation of a GPT-based QG model achieves state-of-the-artperformance in generating answerable questions.</description><author>Zifan Wang, Kotaro Funakoshi, Manabu Okumura</author><pubDate>Mon, 26 Feb 2024 04:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12546v2</guid></item><item><title>SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM</title><link>http://arxiv.org/abs/2403.04735v1</link><description>Vision-extended LLMs have made significant strides in Visual QuestionAnswering (VQA). Despite these advancements, VLLMs still encounter substantialdifficulties in handling queries involving long-tail entities, with a tendencyto produce erroneous or hallucinated responses. In this work, we introduce anovel evaluative benchmark named \textbf{SnapNTell}, specifically tailored forentity-centric VQA. This task aims to test the models' capabilities inidentifying entities and providing detailed, entity-specific knowledge. We havedeveloped the \textbf{SnapNTell Dataset}, distinct from traditional VQAdatasets: (1) It encompasses a wide range of categorized entities, eachrepresented by images and explicitly named in the answers; (2) It features QApairs that require extensive knowledge for accurate responses. The dataset isorganized into 22 major categories, containing 7,568 unique entities in total.For each entity, we curated 10 illustrative images and crafted 10knowledge-intensive QA pairs. To address this novel task, we devised ascalable, efficient, and transparent retrieval-augmented multimodal LLM. Ourapproach markedly outperforms existing methods on the SnapNTell dataset,achieving a 66.5\% improvement in the BELURT score. We will soon make thedataset and the source code publicly accessible.</description><author>Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A. Crook, Yifan Ethan Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, Seungwhan Moon</author><pubDate>Thu, 07 Mar 2024 18:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04735v1</guid></item><item><title>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</title><link>http://arxiv.org/abs/2402.18060v1</link><description>LLMs have demonstrated impressive performance in answering medical questions,such as passing medical licensing examinations. However, most existingbenchmarks rely on board exam questions or general medical questions, fallingshort in capturing the complexity of realistic clinical cases. Moreover, thelack of reference explanations for answers hampers the evaluation of modelexplanations, which are crucial to supporting doctors in making complex medicaldecisions. To address these challenges, we construct two new datasets: JAMAClinical Challenge and Medbullets. JAMA Clinical Challenge consists ofquestions based on challenging clinical cases, while Medbullets comprises USMLEStep 2&amp;3 style clinical questions. Both datasets are structured asmultiple-choice question-answering tasks, where each question is accompanied byan expert-written explanation. We evaluate four LLMs on the two datasets usingvarious prompts. Experiments demonstrate that our datasets are harder thanprevious benchmarks. The inconsistency between automatic and human evaluationsof model-generated explanations highlights the need to develop new metrics tosupport future research on explainable medical QA.</description><author>Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze</author><pubDate>Wed, 28 Feb 2024 05:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18060v1</guid></item><item><title>Biomedical Entity Linking as Multiple Choice Question Answering</title><link>http://arxiv.org/abs/2402.15189v1</link><description>Although biomedical entity linking (BioEL) has made significant progress withpre-trained language models, challenges still exist for fine-grained andlong-tailed entities. To address these challenges, we present BioELQA, a novelmodel that treats Biomedical Entity Linking as Multiple Choice QuestionAnswering. BioELQA first obtains candidate entities with a fast retriever,jointly presents the mention and candidate entities to a generator, and thenoutputs the predicted symbol associated with its chosen entity. Thisformulation enables explicit comparison of different candidate entities, thuscapturing fine-grained interactions between mentions and entities, as well asamong entities themselves. To improve generalization for long-tailed entities,we retrieve similar labeled training instances as clues and concatenate theinput with retrieved instances for the generator. Extensive experimentalresults show that BioELQA outperforms state-of-the-art baselines on severaldatasets.</description><author>Zhenxi Lin, Ziheng Zhang, Xian Wu, Yefeng Zheng</author><pubDate>Fri, 23 Feb 2024 08:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15189v1</guid></item><item><title>PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering</title><link>http://arxiv.org/abs/2402.16288v1</link><description>Long-term memory plays a critical role in personal interaction, consideringlong-term memory can better leverage world knowledge, historical information,and preferences in dialogues. Our research introduces PerLTQA, an innovative QAdataset that combines semantic and episodic memories, including worldknowledge, profiles, social relationships, events, and dialogues. This datasetis collected to investigate the use of personalized memories, focusing onsocial interactions and events in the QA task. PerLTQA features two types ofmemory and a comprehensive benchmark of 8,593 questions for 30 characters,facilitating the exploration and application of personalized memories in LargeLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework formemory integration and generation, consisting of three main components: MemoryClassification, Memory Retrieval, and Memory Synthesis. We evaluate thisframework using five LLMs and three retrievers. Experimental resultsdemonstrate that BERT-based classification models significantly outperform LLMssuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,our study highlights the importance of effective memory integration in the QAtask.</description><author>Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong</author><pubDate>Mon, 26 Feb 2024 04:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16288v1</guid></item><item><title>Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering</title><link>http://arxiv.org/abs/2403.04890v1</link><description>Large Language models (LLMs) have demonstrated significant potential intransforming healthcare by automating tasks such as clinical documentation,information retrieval, and decision support. In this aspect, carefullyengineered prompts have emerged as a powerful tool for using LLMs for medicalscenarios, e.g., patient clinical scenarios. In this paper, we propose amodified version of the MedQA-USMLE dataset, which is subjective, to mimicreal-life clinical scenarios. We explore the Chain of Thought (CoT) reasoningbased on subjective response generation for the modified MedQA-USMLE datasetwith appropriate LM-driven forward reasoning for correct responses to themedical questions. Keeping in mind the importance of response verification inthe medical setting, we utilize a reward training mechanism whereby thelanguage model also provides an appropriate verified response for a particularresponse to a clinical question. In this regard, we also includehuman-in-the-loop for different evaluation aspects. We develop betterin-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt fromarXiv:2207.08143 for the subjective MedQA dataset and developing ourincremental-reasoning prompt. Our evaluations show that the incrementalreasoning prompt performs better than the modified codex prompt in certainscenarios. We also show that greedy decoding with the incremental reasoningmethod performs better than other strategies, such as prompt chaining andeliminative reasoning.</description><author>Ojas Gramopadhye, Saeel Sandeep Nachane, Prateek Chanda, Ganesh Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra Joshi</author><pubDate>Thu, 07 Mar 2024 20:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04890v1</guid></item><item><title>OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models</title><link>http://arxiv.org/abs/2402.19371v1</link><description>LLMs have become increasingly capable at accomplishing a range ofspecialized-tasks and can be utilized to expand equitable access to medicalknowledge. Most medical LLMs have involved extensive fine-tuning, leveragingspecialized medical data and significant, thus costly, amounts of computationalpower. Many of the top performing LLMs are proprietary and their access islimited to very few research groups. However, open-source (OS) models representa key area of growth for medical LLMs due to significant improvements inperformance and an inherent ability to provide the transparency and compliancerequired in healthcare. We present OpenMedLM, a prompting platform whichdelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series ofprompting strategies, including zero-shot, few-shot, chain-of-thought (randomselection and kNN selection), and ensemble/self-consistency voting. We foundthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,surpassing the previous best performing OS models that leveragedcomputationally costly extensive fine-tuning. The model delivers a 72.6%accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, andachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as thefirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlightmedical-specific emergent properties in OS LLMs which have not yet beendocumented to date elsewhere, and showcase the benefits of further leveragingprompt engineering to improve the performance of accessible LLMs for medicalapplications.</description><author>Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das</author><pubDate>Thu, 29 Feb 2024 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19371v1</guid></item><item><title>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</title><link>http://arxiv.org/abs/2402.19350v1</link><description>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) tosimulate human reasoning and inference processes, achieving proficientperformance in multi-hop QA. However, a gap persists between PLMs' reasoningabilities and those of humans when tackling complex problems. Psychologicalstudies suggest a vital connection between explicit information in passages andhuman prior knowledge during reading. Nevertheless, current research has giveninsufficient attention to linking input passages and PLMs' pre-training-basedknowledge from the perspective of human cognition studies. In this study, weintroduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicitknowledge (PEI) framework, which uses prompts to connect explicit and implicitknowledge, aligning with human reading process for multi-hop QA. We considerthe input passages as explicit knowledge, employing them to elicit implicitknowledge through unified prompt reasoning. Furthermore, our model incorporatestype-specific reasoning via prompts, a form of implicit knowledge. Experimentalresults show that PEI performs comparably to the state-of-the-art on HotpotQA.Ablation studies confirm the efficacy of our model in bridging and integratingexplicit and implicit knowledge.</description><author>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun</author><pubDate>Thu, 29 Feb 2024 16:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19350v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>Reasoning over Description Logic-based Contexts with Transformers</title><link>http://arxiv.org/abs/2311.08941v2</link><description>One way that the current state of the art measures the reasoning ability oftransformer-based models is by evaluating accuracy in downstream tasks likelogical question answering or proof generation over synthetic contextsexpressed in natural language. However, most of the contexts used are inpractice very simple; in most cases, they are generated from short first-orderlogic sentences with only a few logical operators and quantifiers. In thiswork, we seek to answer the question how well a transformer-based model willperform reasoning over expressive contexts. For this purpose, we construct asynthetic natural language question-answering dataset, generated by descriptionlogic knowledge bases. For the generation of the knowledge bases, we use theexpressive language $\mathcal{ALCQ}$. The resulting dataset contains 384Kexamples, and increases in two dimensions: i) reasoning depth, and ii) lengthof sentences. We show that the performance of our DeBERTa-based model,DELTA$_M$, is marginally affected when the reasoning depth is increased and itis not affected at all when the length of the sentences is increasing. We alsoevaluate the generalization ability of the model on reasoning depths unseen attraining, both increasing and decreasing, revealing interesting insights intothe model's adaptive generalization abilities.</description><author>Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</author><pubDate>Mon, 26 Feb 2024 08:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08941v2</guid></item><item><title>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</title><link>http://arxiv.org/abs/2402.08277v2</link><description>Advances towards more faithful and traceable answers of Large Language Models(LLMs) are crucial for various research and practical endeavors. One avenue inreaching this goal is basing the answers on reliable sources. However, thisEvidence-Based QA has proven to work insufficiently with LLMs in terms ofciting the correct sources (source quality) and truthfully representing theinformation within sources (answer attributability). In this work, wesystematically investigate how to robustly fine-tune LLMs for better sourcequality and answer attributability. Specifically, we introduce a datageneration pipeline with automated data quality filters, which can synthesizediversified high-quality training and testing data at scale. We furtherintroduce four test sets to benchmark the robustness of fine-tuned specialistmodels. Extensive evaluation shows that fine-tuning on synthetic data improvesperformance on both in- and out-of-distribution. Furthermore, we show that dataquality, which can be drastically improved by proposed quality filters, mattersmore than quantity in improving Evidence-Based QA.</description><author>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</author><pubDate>Fri, 16 Feb 2024 11:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08277v2</guid></item><item><title>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</title><link>http://arxiv.org/abs/2402.08277v3</link><description>Advances towards more faithful and traceable answers of Large Language Models(LLMs) are crucial for various research and practical endeavors. One avenue inreaching this goal is basing the answers on reliable sources. However, thisEvidence-Based QA has proven to work insufficiently with LLMs in terms ofciting the correct sources (source quality) and truthfully representing theinformation within sources (answer attributability). In this work, wesystematically investigate how to robustly fine-tune LLMs for better sourcequality and answer attributability. Specifically, we introduce a datageneration pipeline with automated data quality filters, which can synthesizediversified high-quality training and testing data at scale. We furtherintroduce four test sets to benchmark the robustness of fine-tuned specialistmodels. Extensive evaluation shows that fine-tuning on synthetic data improvesperformance on both in- and out-of-distribution. Furthermore, we show that dataquality, which can be drastically improved by proposed quality filters, mattersmore than quantity in improving Evidence-Based QA.</description><author>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</author><pubDate>Mon, 26 Feb 2024 11:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08277v3</guid></item><item><title>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</title><link>http://arxiv.org/abs/2402.19350v3</link><description>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) tosimulate human reasoning and inference processes, achieving proficientperformance in multi-hop QA. However, a gap persists between PLMs' reasoningabilities and those of humans when tackling complex problems. Psychologicalstudies suggest a vital connection between explicit information in passages andhuman prior knowledge during reading. Nevertheless, current research has giveninsufficient attention to linking input passages and PLMs' pre-training-basedknowledge from the perspective of human cognition studies. In this study, weintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, whichuses prompts to connect explicit and implicit knowledge, aligning with humanreading process for multi-hop QA. We consider the input passages as explicitknowledge, employing them to elicit implicit knowledge through unified promptreasoning. Furthermore, our model incorporates type-specific reasoning viaprompts, a form of implicit knowledge. Experimental results show that PEIperforms comparably to the state-of-the-art on HotpotQA. Ablation studiesconfirm the efficacy of our model in bridging and integrating explicit andimplicit knowledge.</description><author>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun</author><pubDate>Mon, 04 Mar 2024 19:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19350v3</guid></item><item><title>MedLM: Exploring Language Models for Medical Question Answering Systems</title><link>http://arxiv.org/abs/2401.11389v2</link><description>In the face of rapidly expanding online medical literature, automated systemsfor aggregating and summarizing information are becoming increasingly crucialfor healthcare professionals and patients. Large Language Models (LLMs), withtheir advanced generative capabilities, have shown promise in various NLPtasks, and their potential in the healthcare domain, particularly forClosed-Book Generative QnA, is significant. However, the performance of thesemodels in domain-specific tasks such as medical Q&amp;A remains largely unexplored.This study aims to fill this gap by comparing the performance of general andmedical-specific distilled LMs for medical Q&amp;A. We aim to evaluate theeffectiveness of fine-tuning domain-specific LMs and compare the performance ofdifferent families of Language Models. The study will address criticalquestions about these models' reliability, comparative performance, andeffectiveness in the context of medical Q&amp;A. The findings will provide valuableinsights into the suitability of different LMs for specific applications in themedical domain.</description><author>Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila</author><pubDate>Wed, 06 Mar 2024 03:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11389v2</guid></item><item><title>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</title><link>http://arxiv.org/abs/2309.02233v2</link><description>Large-scale language models (LLMs) like ChatGPT have demonstrated impressiveabilities in generating responses based on human instructions. However, theiruse in the medical field can be challenging due to their lack of specific,in-depth knowledge. In this study, we present a system called LLMs Augmentedwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs inspecialized domains. LLM-AMT integrates authoritative medical textbooks intothe LLMs' framework using plug-and-play modules. These modules include a QueryAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,they incorporate authoritative medical knowledge. Additionally, an LLM Readeraids in contextual understanding. Our experimental results on three medical QAtasks demonstrate that LLMAMT significantly improves response quality, withaccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as thebase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained ona massive amount of medical corpus by 2-3%. We found that despite being 100xsmaller in size, medical textbooks as a retrieval corpus is proven to be a moreeffective knowledge database than Wikipedia in the medical domain, boostingperformance by 7.8%-13.7%.</description><author>Yubo Wang, Xueguang Ma, Wenhu Chen</author><pubDate>Thu, 22 Feb 2024 16:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02233v2</guid></item><item><title>Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering</title><link>http://arxiv.org/abs/2403.02966v1</link><description>Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhanceQuesetion Answering (QA) performance of Large Language Models (LLMs), yetstructured KG verbalization remains challengin. Existing methods, such astriple-form or free-form textual conversion of triple-form facts, encounterseveral issues. These include reduced evidence density due to duplicatedentities or relationships, and reduced evidence clarity due to an inability toemphasize crucial evidence. To address these issues, we propose EFSum, anEvidence-focused Fact Summarization framework for enhanced QA withknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizerthrough distillation and preference alignment. Our extensive experiments showthat EFSum improves LLM's zero-shot QA performance, and it is possible toensure both the helpfulness and faithfulness of the summary.</description><author>Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee</author><pubDate>Tue, 05 Mar 2024 13:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02966v1</guid></item><item><title>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</title><link>http://arxiv.org/abs/2402.19350v2</link><description>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) tosimulate human reasoning and inference processes, achieving proficientperformance in multi-hop QA. However, a gap persists between PLMs' reasoningabilities and those of humans when tackling complex problems. Psychologicalstudies suggest a vital connection between explicit information in passages andhuman prior knowledge during reading. Nevertheless, current research has giveninsufficient attention to linking input passages and PLMs' pre-training-basedknowledge from the perspective of human cognition studies. In this study, weintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, whichuses prompts to connect explicit and implicit knowledge, aligning with humanreading process for multi-hop QA. We consider the input passages as explicitknowledge, employing them to elicit implicit knowledge through unified promptreasoning. Furthermore, our model incorporates type-specific reasoning viaprompts, a form of implicit knowledge. Experimental results show that PEIperforms comparably to the state-of-the-art on HotpotQA. Ablation studiesconfirm the efficacy of our model in bridging and integrating explicit andimplicit knowledge.</description><author>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun</author><pubDate>Fri, 01 Mar 2024 11:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19350v2</guid></item><item><title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title><link>http://arxiv.org/abs/2402.09642v1</link><description>This work aims to build a text embedder that can capture characteristics oftexts specified by user instructions. Despite its tremendous potential todeploy user-oriented embeddings, none of previous approaches provides aconcrete solution for it. This paper offers a new viewpoint, which treats theinstruction as a question about the input text and encodes the expected answersto obtain the representation accordingly. Intuitively, texts with the same(implicit) semantics would share similar answers following the instruction,thus leading to more similar embeddings. Specifically, we propose InBedder thatinstantiates this embed-via-answering idea by only fine-tuning language modelson abstractive question answering tasks. InBedder demonstrates significantlyimproved instruction-following capabilities according to our proposedinstruction awareness tests and instruction robustness tests, when applied toboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-basedLMs (e.g., roberta-large). Additionally, our qualitative analysis of clusteringoutcomes, achieved by applying different instructions to the same corpus,demonstrates a high degree of interpretability.</description><author>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</author><pubDate>Thu, 15 Feb 2024 01:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09642v1</guid></item><item><title>RealTime QA: What's the Answer Right Now?</title><link>http://arxiv.org/abs/2207.13332v2</link><description>We introduce REALTIME QA, a dynamic question answering (QA) platform thatannounces questions and evaluates systems on a regular basis (weekly in thisversion). REALTIME QA inquires about the current world, and QA systems need toanswer questions about novel events or information. It therefore challengesstatic, conventional assumptions in open-domain QA datasets and pursuesinstantaneous applications. We build strong baseline models upon largepretrained language models, including GPT-3 and T5. Our benchmark is an ongoingeffort, and this paper presents real-time evaluation results over the pastyear. Our experimental results show that GPT-3 can often properly update itsgeneration results, based on newly-retrieved documents, highlighting theimportance of up-to-date information retrieval. Nonetheless, we find that GPT-3tends to return outdated answers when retrieved documents do not providesufficient information to find an answer. This suggests an important avenue forfuture research: can an open-domain QA system identify such unanswerable casesand communicate with the user or even the retrieval module to modify theretrieval results? We hope that REALTIME QA will spur progress in instantaneousapplications of question answering and beyond.</description><author>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, Kentaro Inui</author><pubDate>Wed, 28 Feb 2024 07:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13332v2</guid></item><item><title>Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models</title><link>http://arxiv.org/abs/2402.13492v1</link><description>While large language models (LMs) demonstrate remarkable performance, theyencounter challenges in providing accurate responses when queried forinformation beyond their pre-trained memorization. Although augmenting themwith relevant external information can mitigate these issues, failure toconsider the necessity of retrieval may adversely affect overall performance.Previous research has primarily focused on examining how entities influenceretrieval models and knowledge recall in LMs, leaving other aspects relativelyunexplored. In this work, our goal is to offer a more detailed, fact-centricanalysis by exploring the effects of combinations of entities and relations. Tofacilitate this, we construct a new question answering (QA) dataset calledWiTQA (Wikipedia Triple Question Answers). This dataset includes questionsabout entities and relations of various popularity levels, each accompanied bya supporting passage. Our extensive experiments with diverse LMs and retrieversreveal when retrieval does not consistently enhance LMs from the viewpoints offact-centric popularity.Confirming earlier findings, we observe that larger LMsexcel in recalling popular facts. However, they notably encounter difficultywith infrequent entity-relation pairs compared to retrievers. Interestingly,they can effectively retain popular relations of less common entities. Wedemonstrate the efficacy of our finer-grained metric and insights through anadaptive retrieval system that selectively employs retrieval and recall basedon the frequencies of entities and relations in the question.</description><author>Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani</author><pubDate>Wed, 21 Feb 2024 03:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13492v1</guid></item><item><title>BiMediX: Bilingual Medical Mixture of Experts LLM</title><link>http://arxiv.org/abs/2402.13253v1</link><description>In this paper, we introduce BiMediX, the first bilingual medical mixture ofexperts LLM designed for seamless interaction in both English and Arabic. Ourmodel facilitates a wide range of medical interactions in English and Arabic,including multi-turn chats to inquire about additional details such as patientsymptoms and medical history, multiple-choice question answering, andopen-ended question answering. We propose a semi-automated English-to-Arabictranslation pipeline with human refinement to ensure high-quality translations.We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingualinstruction set covering 1.3 Million diverse medical interactions, resulting inover 632 million healthcare specialized tokens for instruction tuning. OurBiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats andmaintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-artMed42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,computed across multiple medical evaluation benchmarks in English, whileoperating at 8-times faster inference. Moreover, our BiMediX outperforms thegeneric Arabic-English bilingual LLM, Jais-30B, by average absolute gains of10% on our Arabic medical benchmark and 15% on bilingual evaluations acrossmultiple datasets. Our project page with source code and trained model isavailable at https://github.com/mbzuai-oryx/BiMediX .</description><author>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal</author><pubDate>Tue, 20 Feb 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13253v1</guid></item><item><title>An Evaluation of GPT-4V and Gemini in Online VQA</title><link>http://arxiv.org/abs/2312.10637v2</link><description>While there is much excitement about the potential of large multimodal models(LMM), a comprehensive evaluation is critical to establish their truecapabilities and limitations. In support of this aim, we evaluate twostate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answeringdataset sourced from an authentic online question answering community. Weconduct fine-grained analysis by generating seven types of metadata for nearly2,000 visual questions, such as image type and the required image processingcapabilities. Our zero-shot performance analysis highlights the types ofquestions that are most challenging for both models, including questionsrelated to "puzzling" topic, with "Identification" user intention, with "SheetMusic" image type, or labeled as "hard" by GPT-4.</description><author>Mengchen Liu, Chongyan Chen, Danna Gurari</author><pubDate>Wed, 14 Feb 2024 03:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10637v2</guid></item><item><title>Debating with More Persuasive LLMs Leads to More Truthful Answers</title><link>http://arxiv.org/abs/2402.06782v2</link><description>Common methods for aligning large language models (LLMs) with desiredbehaviour heavily rely on human-labelled data. However, as models growincreasingly sophisticated, they will surpass human expertise, and the role ofhuman evaluation will evolve into non-experts overseeing experts. Inanticipation of this, we ask: can weaker models assess the correctness ofstronger models? We investigate this question in an analogous setting, wherestronger models (experts) possess the necessary information to answer questionsand weaker models (non-experts) lack this information. The method we evaluateis \textit{debate}, where two LLM experts each argue for a different answer,and a non-expert selects the answer. We find that debate consistently helpsboth non-expert models and humans answer questions, achieving 76\% and 88\%accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,optimising expert debaters for persuasiveness in an unsupervised mannerimproves non-expert ability to identify the truth in debates. Our resultsprovide encouraging empirical evidence for the viability of aligning modelswith debate in the absence of ground truth.</description><author>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez</author><pubDate>Thu, 15 Feb 2024 22:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06782v2</guid></item><item><title>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</title><link>http://arxiv.org/abs/2311.06607v3</link><description>Large Multimodal Models (LMMs) have shown promise in vision-language tasksbut struggle with high-resolution input and detailed scene understanding.Addressing these challenges, we introduce Monkey to enhance LMM capabilities.Firstly, Monkey processes input images by dividing them into uniform patches,each matching the size (e.g., 448x448) used in the original training of thewell-trained vision encoder. Equipped with individual adapter for each patch,Monkey can handle higher resolutions up to 1344x896 pixels, enabling thedetailed capture of complex visual information. Secondly, it employs amulti-level description generation method, enriching the context forscene-object associations. This two-part strategy ensures more effectivelearning from generated data: the higher resolution allows for a more detailedcapture of visuals, which in turn enhances the effectiveness of comprehensivedescriptions. Extensive ablative results validate the effectiveness of ourdesigns. Additionally, experiments on 18 datasets further demonstrate thatMonkey surpasses existing LMMs in many tasks like Image Captioning and variousVisual Question Answering formats. Specially, in qualitative tests focused ondense text question answering, Monkey has exhibited encouraging resultscompared with GPT4V. Code is available athttps://github.com/Yuliang-Liu/Monkey.</description><author>Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai</author><pubDate>Thu, 22 Feb 2024 06:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06607v3</guid></item><item><title>SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning</title><link>http://arxiv.org/abs/2401.13246v2</link><description>Elucidating the reasoning process with structured explanations from questionto answer is crucial, as it significantly enhances the interpretability,traceability, and trustworthiness of question-answering (QA) systems. However,structured explanations demand models to perform intricately structuredreasoning, which poses great challenges. Most existing methods focus onsingle-step reasoning through supervised learning, ignoring logicaldependencies between steps. Moreover, existing reinforcement learning (RL)based methods overlook the structured relationships, underutilizing thepotential of RL in structured reasoning. In this paper, we propose SEER, anovel method that maximizes a structure-based return to facilitate structuredreasoning and explanation. Our proposed structure-based return preciselydescribes the hierarchical and branching structure inherent in structuredreasoning, effectively capturing the intricate relationships between differentreasoning steps. In addition, we introduce a fine-grained reward function tometiculously delineate diverse reasoning steps. Extensive experiments show thatSEER significantly outperforms state-of-the-art methods, achieving an absoluteimprovement of 6.9% over RL-based methods on EntailmentBank, a 4.4% averageimprovement on STREET benchmark, and exhibiting outstanding efficiency andcross-dataset generalization performance.</description><author>Guoxin Chen, Kexin Tang, Chao Yang, Fuying Ye, Yu Qiao, Yiming Qian</author><pubDate>Fri, 16 Feb 2024 14:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13246v2</guid></item><item><title>NewsQs: Multi-Source Question Generation for the Inquiring Mind</title><link>http://arxiv.org/abs/2402.18479v1</link><description>We present NewsQs (news-cues), a dataset that provides question-answer pairsfor multiple news documents. To create NewsQs, we augment a traditionalmulti-document summarization dataset with questions automatically generated bya T5-Large model fine-tuned on FAQ-style news articles from the News On the Webcorpus. We show that fine-tuning a model with control codes produces questionsthat are judged acceptable more often than the same model without them asmeasured through human evaluation. We use a QNLI model with high correlationwith human annotations to filter our data. We release our final dataset ofhigh-quality questions, answers, and document clusters as a resource for futurework in query-based multi-document summarization.</description><author>Alyssa Hwang, Kalpit Dixit, Miguel Ballesteros, Yassine Benajiba, Vittorio Castelli, Markus Dreyer, Mohit Bansal, Kathleen McKeown</author><pubDate>Wed, 28 Feb 2024 16:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18479v1</guid></item><item><title>Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning</title><link>http://arxiv.org/abs/2403.02333v1</link><description>Large language models (LLMs) have shown great potential in complex reasoningtasks, yet their performance is often hampered by the scarcity of high-quality,reasoning-focused training datasets. Addressing this challenge, we proposeKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework thatsynthesizes question-answer pairs by leveraging key points and exemplar pairsfrom authentic data sources. KPDDS ensures the generation of novel questionswith rigorous quality control and substantial scalability. As a result, wepresent KPMath, the most extensive synthetic dataset tailored for mathematicalreasoning to date, comprising over one million question-answer pairs. UtilizingKPMath and augmenting it with additional reasoning-intensive corpora, we createthe comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model onKPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, aperformance that not only outpaces other finetuned 7B models but also exceedsthat of certain 34B models. Our ablation studies further confirm thesubstantial enhancement in mathematical reasoning across various subtopics,marking a significant stride in LLMs' reasoning capabilities.</description><author>Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen</author><pubDate>Mon, 04 Mar 2024 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02333v1</guid></item><item><title>Slot-VLM: SlowFast Slots for Video-Language Modeling</title><link>http://arxiv.org/abs/2402.13088v1</link><description>Video-Language Models (VLMs), powered by the advancements in Large LanguageModels (LLMs), are charting new frontiers in video understanding. A pivotalchallenge is the development of an efficient method to encapsulate videocontent into a set of representative tokens to align with LLMs. In this work,we introduce Slot-VLM, a novel framework designed to generate semanticallydecomposed video tokens, in terms of object-wise and event-wise visualrepresentations, to facilitate LLM inference. Particularly, we design aSlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the densevideo tokens from the CLIP vision encoder to a set of representative slots. Inorder to take into account both the spatial object details and the variedtemporal dynamics, SF-Slots is built with a dual-branch structure. TheSlow-Slots branch focuses on extracting object-centric slots from features athigh spatial resolution but low (slow) frame sample rate, emphasizing detailedobject information. Conversely, Fast-Slots branch is engineered to learnevent-centric slots from high temporal sample rate but low spatial resolutionfeatures. These complementary slots are combined to form the vision context,serving as the input to the LLM for efficient question answering. Ourexperimental results demonstrate the effectiveness of our Slot-VLM, whichachieves the state-of-the-art performance on video question-answering.</description><author>Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu</author><pubDate>Tue, 20 Feb 2024 15:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13088v1</guid></item><item><title>MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning</title><link>http://arxiv.org/abs/2402.11260v1</link><description>Adapting large language models (LLMs) to new domains/tasks and enabling themto be efficient lifelong learners is a pivotal challenge. In this paper, wepropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation forLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with thefine-tuning abilities of LoRA for effective life-long learning of LLMs. Incontrast to the conventional approaches that use factual triplets as inputsMoRAL relies on simple question-answer pairs, which is a more practical andeffective strategy for robust and efficient learning. Owing to new datasettings, we introduce a new evaluation benchmark namely: Life Long Learning ofLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,and a set of evaluation metrics for rigorous evaluation of MoRAL in open-bookand closed-book settings. Experimental evaluation shows (i) LLMs learn fast inopen-book settings with up to 30.15% improvement in "RA" for Phi-2-2.7Bcompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL showshigher performance improvement for models with a greater number of parameters;(iii) MoRAL is robust to catastrophic forgetting offering better knowledgeretention compared to baselines.</description><author>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang</author><pubDate>Sat, 17 Feb 2024 12:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11260v1</guid></item><item><title>ScreenAI: A Vision-Language Model for UI and Infographics Understanding</title><link>http://arxiv.org/abs/2402.04615v2</link><description>Screen user interfaces (UIs) and infographics, sharing similar visuallanguage and design principles, play important roles in human communication andhuman-machine interaction. We introduce ScreenAI, a vision-language model thatspecializes in UI and infographics understanding. Our model improves upon thePaLI architecture with the flexible patching strategy of pix2struct and istrained on a unique mixture of datasets. At the heart of this mixture is anovel screen annotation task in which the model has to identify the type andlocation of UI elements. We use these text annotations to describe screens toLarge Language Models and automatically generate question-answering (QA), UInavigation, and summarization training datasets at scale. We run ablationstudies to demonstrate the impact of these design choices. At only 5Bparameters, ScreenAI achieves new state-of-the-artresults on UI- andinfographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and WidgetCaptioning), and new best-in-class performance on others (Chart QA, DocVQA, andInfographicVQA) compared to models of similar size. Finally, we release threenew datasets: one focused on the screen annotation task and two others focusedon question answering.</description><author>Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cărbune, Jason Lin, Jindong Chen, Abhanshu Sharma</author><pubDate>Mon, 19 Feb 2024 17:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04615v2</guid></item><item><title>MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling</title><link>http://arxiv.org/abs/2303.05707v2</link><description>Video-and-language understanding has a variety of applications in theindustry, such as video question answering, text-video retrieval, andmulti-label classification. Existing video-and-language understanding methodsgenerally adopt heavy multi-modal encoders and feature fusion modules, whichconsume high computational costs. Specially, they have difficulty dealing withdense video frames or long text prevalent in industrial applications. Thispaper proposes MuLTI, a highly accurate and efficient video-and-languageunderstanding model that achieves efficient and effective feature fusion andrapid adaptation to downstream tasks. Specifically, we design a Text-GuidedMultiWay-Sampler based on adapt-pooling residual mapping and self-attentionmodules to sample long sequences and fuse multi-modal features, which reducesthe computational costs and addresses performance degradation caused byprevious samplers. Therefore, MuLTI can handle longer sequences with limitedcomputational costs. Then, to further enhance the model's performance and fillin the lack of pretraining tasks in the video question answering, we propose anew pretraining task named Multiple Choice Modeling. This task bridges the gapbetween pretraining and downstream tasks and improves the model's ability toalign video and text features. Benefiting from the efficient feature fusionmodule and the new pretraining task, MuLTI achieves state-of-the-artperformance on multiple datasets. Implementation and pretrained models will bereleased.</description><author>Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi</author><pubDate>Fri, 01 Mar 2024 02:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05707v2</guid></item><item><title>Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning</title><link>http://arxiv.org/abs/2403.03864v1</link><description>This paper introduces the novel task of multimodal puzzle solving, framedwithin the context of visual question-answering. We present a new dataset,AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodallanguage models in solving algorithmic puzzles that necessitate both visualunderstanding, language understanding, and complex algorithmic reasoning. Wecreate the puzzles to encompass a diverse array of mathematical and algorithmictopics such as boolean logic, combinatorics, graph theory, optimization,search, etc., aiming to evaluate the gap between visual data interpretation andalgorithmic problem-solving skills. The dataset is generated automatically fromcode authored by humans. All our puzzles have exact solutions that can be foundfrom the algorithm without tedious human calculations. It ensures that ourdataset can be scaled up arbitrarily in terms of reasoning complexity anddataset size. Our investigation reveals that large language models (LLMs) suchas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. Wefind that their performance is near random in a multi-choice question-answeringsetup for a significant number of puzzles. The findings emphasize thechallenges of integrating visual, language, and algorithmic knowledge forsolving complex reasoning problems.</description><author>Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria</author><pubDate>Wed, 06 Mar 2024 17:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03864v1</guid></item><item><title>Instruction-tuned Language Models are Better Knowledge Learners</title><link>http://arxiv.org/abs/2402.12847v1</link><description>In order for large language model (LLM)-based assistants to effectively adaptto evolving information needs, it must be possible to update their factualknowledge through continued training on new data. The standard recipe for doingso involves continued pre-training on new documents followed byinstruction-tuning on question-answer (QA) pairs. However, we find that LLMstrained with this recipe struggle to answer questions, even though theperplexity of documents is minimized. We found that QA pairs are generallystraightforward, while documents are more complex, weaving many factualstatements together in an intricate manner. Therefore, we hypothesize that itis beneficial to expose LLMs to QA pairs before continued pre-training ondocuments so that the process of encoding knowledge from complex documentstakes into account how this knowledge is accessed through questions. Based onthis, we propose pre-instruction-tuning (PIT), a method that instruction-tuneson questions prior to training on documents. This contrasts with standardinstruction-tuning, which learns how to extract knowledge after training ondocuments. Extensive experiments and ablation studies demonstrate that PITsignificantly enhances the ability of LLMs to absorb knowledge from newdocuments, outperforming standard instruction-tuning by 17.8%.</description><author>Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer</author><pubDate>Tue, 20 Feb 2024 09:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12847v1</guid></item><item><title>Mitigating Temporal Misalignment by Discarding Outdated Facts</title><link>http://arxiv.org/abs/2305.14824v3</link><description>While large language models are able to retain vast amounts of worldknowledge seen during pretraining, such knowledge is prone to going out of dateand is nontrivial to update. Furthermore, these models are often used undertemporal misalignment, tasked with answering questions about the present,despite having only been trained on data collected in the past. To mitigate theeffects of temporal misalignment, we propose fact duration prediction: the taskof predicting how long a given fact will remain true. In our experiments, wedemonstrate that identifying which facts are prone to rapid change can helpmodels avoid reciting outdated information and determine which predictionsrequire seeking out up-to-date knowledge sources. We also show how modelingfact duration improves calibration for knowledge-intensive tasks, such asopen-retrieval question answering, under temporal misalignment, by discardingvolatile facts. Our data and code are released publicly athttps://github.com/mikejqzhang/mitigating_misalignment.</description><author>Michael J. Q. Zhang, Eunsol Choi</author><pubDate>Tue, 05 Mar 2024 16:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14824v3</guid></item></channel></rss>