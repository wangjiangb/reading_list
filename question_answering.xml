<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 14 Aug 2024 13:00:37 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</title><link>http://arxiv.org/abs/2310.10942v5</link><description>Teaching Visual Question Answering (VQA) models to refrain from answeringunanswerable questions is necessary for building a trustworthy AI system.Existing studies, though have explored various aspects of VQA but somewhatignored this particular attribute. This paper aims to bridge the research gapby contributing a comprehensive dataset, called UNK-VQA. The dataset isspecifically designed to address the challenge of questions that models do notknow. To this end, we first augment the existing data via deliberateperturbations on either the image or question. In specific, we carefully ensurethat the question-image semantics remain close to the original unperturbeddistribution. By this means, the identification of unanswerable questionsbecomes challenging, setting our dataset apart from others that involve mereimage replacement. We then extensively evaluate the zero- and few-shotperformance of several emerging multi-modal large models and discover theirsignificant limitations when applied to our dataset. Additionally, we alsopropose a straightforward method to tackle these unanswerable questions. Thisdataset, we believe, will serve as a valuable benchmark for enhancing theabstention capability of VQA models, thereby leading to increasedtrustworthiness of AI systems. We have made the dataset(https://github.com/guoyang9/UNK-VQA) available to facilitate furtherexploration in this area.</description><author>Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</author><pubDate>Sun, 11 Aug 2024 13:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10942v5</guid></item><item><title>RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension</title><link>http://arxiv.org/abs/2407.07321v1</link><description>Large Language Models (LLMs) have been applied to many research problemsacross various domains. One of the applications of LLMs is providingquestion-answering systems that cater to users from different fields. Theeffectiveness of LLM-based question-answering systems has already beenestablished at an acceptable level for users posing questions in popular andpublic domains such as trivia and literature. However, it has not often beenestablished in niche domains that traditionally require specialized expertise.To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performanceof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answeringquestions originating from Environmental Impact Statements prepared by U.S.federal government agencies in accordance with the National EnvironmentalEnvironmental Act (NEPA). We specifically measure the ability of LLMs tounderstand the nuances of legal, technical, and compliance-related informationpresent in NEPA documents in different contextual scenarios. For example, wetest the LLMs' internal prior NEPA knowledge by providing questions without anycontext, as well as assess how LLMs synthesize the contextual informationpresent in long NEPA documents to facilitate the question/answering task. Wecompare the performance of the long context LLMs and RAG powered models inhandling different types of questions (e.g., problem-solving, divergent). Ourresults suggest that RAG powered models significantly outperform the longcontext models in the answer accuracy regardless of the choice of the frontierLLM. Our further analysis reveals that many models perform better answeringclosed questions than divergent and problem-solving questions.</description><author>Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana</author><pubDate>Wed, 10 Jul 2024 02:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07321v1</guid></item><item><title>INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages</title><link>http://arxiv.org/abs/2407.13522v1</link><description>Large Language Models (LLMs) have demonstrated remarkable zero-shot andfew-shot capabilities in unseen tasks, including context-grounded questionanswering (QA) in English. However, the evaluation of LLMs' capabilities innon-English languages for context-based QA is limited by the scarcity ofbenchmarks in non-English languages. To address this gap, we introduceIndic-QA, the largest publicly available context-grounded question-answeringdataset for 11 major Indian languages from two language families. The datasetcomprises both extractive and abstractive question-answering tasks and includesexisting datasets as well as English QA datasets translated into Indianlanguages. Additionally, we generate a synthetic dataset using the Gemini modelto create question-answer pairs given a passage, which is then manuallyverified for quality assurance. We evaluate various multilingual Large LanguageModels and their instruction-fine-tuned variants on the benchmark and observethat their performance is subpar, particularly for low-resource languages. Wehope that the release of this dataset will stimulate further research on thequestion-answering abilities of LLMs for low-resource languages.</description><author>Abhishek Kumar Singh, Rudra Murthy, Vishwajeet kumar, Jaydeep Sen, Ganesh Ramakrishnan</author><pubDate>Thu, 18 Jul 2024 13:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13522v1</guid></item><item><title>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation</title><link>http://arxiv.org/abs/2405.09939v2</link><description>We introduce SciQAG, a novel framework for automatically generatinghigh-quality science question-answer pairs from a large corpus of scientificliterature based on large language models (LLMs). SciQAG consists of a QAgenerator and a QA evaluator, which work together to extract diverse andresearch-level questions and answers from scientific papers. Utilizing thisframework, we construct a large-scale, high-quality, open-ended science QAdataset containing 188,042 QA pairs extracted from 22,743 scientific papersacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmarktask designed to evaluate the science question-answering ability of LLMs.Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG datasetsignificantly improves their performance on both open-ended question answeringand scientific tasks. To foster research and collaboration, we make thedatasets, models, and evaluation codes publicly available, contributing to theadvancement of science question answering and developing more interpretable andreasoning-capable AI systems.</description><author>Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster</author><pubDate>Wed, 10 Jul 2024 01:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09939v2</guid></item><item><title>Decomposed Prompting to Answer Questions on a Course Discussion Board</title><link>http://arxiv.org/abs/2407.21170v1</link><description>We propose and evaluate a question-answering system that uses decomposedprompting to classify and answer student questions on a course discussionboard. Our system uses a large language model (LLM) to classify questions intoone of four types: conceptual, homework, logistics, and not answerable. Thisenables us to employ a different strategy for answering questions that fallunder different types. Using a variant of GPT-3, we achieve $81\%$classification accuracy. We discuss our system's performance on answeringconceptual questions from a machine learning course and various failure modes.</description><author>Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang</author><pubDate>Tue, 30 Jul 2024 20:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21170v1</guid></item><item><title>Extracting Emotion Phrases from Tweets using BART</title><link>http://arxiv.org/abs/2403.14050v3</link><description>Sentiment analysis is a natural language processing task that aims toidentify and extract the emotional aspects of a text. However, many existingsentiment analysis methods primarily classify the overall polarity of a text,overlooking the specific phrases that convey sentiment. In this paper, weapplied an approach to sentiment analysis based on a question-answeringframework. Our approach leverages the power of Bidirectional AutoregressiveTransformer (BART), a pre-trained sequence-to-sequence model, to extract aphrase from a given text that amplifies a given sentiment polarity. We create anatural language question that identifies the specific emotion to extract andthen guide BART to pay attention to the relevant emotional cues in the text. Weuse a classifier within BART to predict the start and end positions of theanswer span within the text, which helps to identify the precise boundaries ofthe extracted emotion phrase. Our approach offers several advantages over mostsentiment analysis studies, including capturing the complete context andmeaning of the text and extracting precise token spans that highlight theintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</description><author>Mahdi Rezapour</author><pubDate>Sat, 27 Jul 2024 17:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14050v3</guid></item><item><title>Q-Bench+: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs</title><link>http://arxiv.org/abs/2402.07116v2</link><description>The rapid development of Multi-modality Large Language Models (MLLMs) hasnavigated a paradigm shift in computer vision, moving towards versatilefoundational models. However, evaluating MLLMs in low-level visual perceptionand understanding remains a yet-to-explore domain. To this end, we designbenchmark settings to emulate human language responses related to low-levelvision: the low-level visual perception (A1) via visual question answeringrelated to low-level attributes (e.g. clarity, lighting); and the low-levelvisual description (A2), on evaluating MLLMs for low-level text descriptions.Furthermore, given that pairwise comparison can better avoid ambiguity ofresponses and has been adopted by many human experiments, we further extend thelow-level perception-related question-answering and description evaluations ofMLLMs from single images to image pairs. Specifically, for perception (A1), wecarry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999image pairs each accompanied by an open-ended question about its low-levelfeatures; for description (A2), we propose the LLDescribe+ dataset, evaluatingMLLMs for low-level descriptions on 499 single images and 450 pairs.Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predictingscore, by employing a softmax-based approach to enable all MLLMs to generatequantifiable quality ratings, tested against human opinions in 7 image qualityassessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate thatseveral MLLMs have decent low-level visual competencies on single images, butonly GPT-4V exhibits higher accuracy on pairwise comparisons than single imageevaluations (like humans). We hope that our benchmark will motivate furtherresearch into uncovering and enhancing these nascent capabilities of MLLMs.Datasets will be available at https://github.com/Q-Future/Q-Bench.</description><author>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin</author><pubDate>Sat, 10 Aug 2024 04:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07116v2</guid></item><item><title>Uncertainty Estimation of Large Language Models in Medical Question Answering</title><link>http://arxiv.org/abs/2407.08662v1</link><description>Large Language Models (LLMs) show promise for natural language generation inhealthcare, but risk hallucinating factually incorrect information. DeployingLLMs for medical question answering necessitates reliable uncertaintyestimation (UE) methods to detect hallucinations. In this work, we benchmarkpopular UE methods with different model sizes on medical question-answeringdatasets. Our results show that current approaches generally perform poorly inthis domain, highlighting the challenge of UE for medical applications. We alsoobserve that larger models tend to yield better results, suggesting acorrelation between model size and the reliability of UE. To address thesechallenges, we propose Two-phase Verification, a probability-free UncertaintyEstimation approach. First, an LLM generates a step-by-step explanationalongside its initial answer, followed by formulating verification questions tocheck the factual claims in the explanation. The model then answers thesequestions twice: first independently, and then referencing the explanation.Inconsistencies between the two sets of answers measure the uncertainty in theoriginal response. We evaluate our approach on three biomedicalquestion-answering datasets using Llama 2 Chat models and compare it againstthe benchmarked baseline methods. The results show that our Two-phaseVerification method achieves the best overall accuracy and stability acrossvarious datasets and model sizes, and its performance scales as the model sizeincreases.</description><author>Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou</author><pubDate>Thu, 11 Jul 2024 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08662v1</guid></item><item><title>Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</title><link>http://arxiv.org/abs/2406.06399v3</link><description>We study the limitations of Large Language Models (LLMs) for the task ofresponse generation in human-machine dialogue. Several techniques have beenproposed in the literature for different dialogue types (e.g., Open-Domain).However, the evaluations of these techniques have been limited in terms of baseLLMs, dialogue types and evaluation metrics. In this work, we extensivelyanalyze different LLM adaptation techniques when applied to different dialoguetypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialoguetypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.We evaluate the performance of in-context learning and fine-tuning techniquesacross datasets selected for each dialogue type. We assess the impact ofincorporating external knowledge to ground the generation in both scenarios ofRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistentevaluation and explainability criteria for automatic metrics and humanevaluation protocols. Our analysis shows that there is no universalbest-technique for adapting large language models as the efficacy of eachtechnique depends on both the base LLM and the specific type of dialogue. Lastbut not least, the assessment of the best adaptation technique should includehuman evaluation to avoid false expectations and outcomes derived fromautomatic metrics.</description><author>Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi</author><pubDate>Sat, 03 Aug 2024 15:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06399v3</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v2</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Thu, 11 Jul 2024 19:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v2</guid></item><item><title>Contextual Object Detection with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2305.18279v2</link><description>Recent Multimodal Large Language Models (MLLMs) are remarkable invision-language tasks, such as image captioning and question answering, butlack the essential perception ability, i.e., object detection. In this work, weaddress this limitation by introducing a novel research problem of contextualobject detection -- understanding visible objects within different human-AIinteractive contexts. Three representative scenarios are investigated,including the language cloze test, visual captioning, and question answering.Moreover, we present ContextDET, a unified multimodal model that is capable ofend-to-end differentiable modeling of visual-language contexts, so as tolocate, identify, and associate visual objects with language inputs forhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visualencoder for extracting visual representations, (ii) a pre-trained LLM formultimodal context decoding, and (iii) a visual decoder for predicting boundingboxes given contextual object words. The new generate-then-detect frameworkenables us to detect object words within human vocabulary. Extensiveexperiments show the advantages of ContextDET on our proposed CODE benchmark,open-vocabulary detection, and referring image segmentation. Github:https://github.com/yuhangzang/ContextDET.</description><author>Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy</author><pubDate>Mon, 12 Aug 2024 07:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18279v2</guid></item><item><title>Meta-Analysis with Untrusted Data</title><link>http://arxiv.org/abs/2407.09387v1</link><description>[See paper for full abstract] Meta-analysis is a crucial tool for answeringscientific questions. It is usually conducted on a relatively small amount of``trusted'' data -- ideally from randomized, controlled trials -- which allowcausal effects to be reliably estimated with minimal assumptions. We show howto answer causal questions much more precisely by making two changes. First, weincorporate untrusted data drawn from large observational databases, relatedscientific literature and practical experience -- without sacrificing rigor orintroducing strong assumptions. Second, we train richer models capable ofhandling heterogeneous trials, addressing a long-standing challenge inmeta-analysis. Our approach is based on conformal prediction, whichfundamentally produces rigorous prediction intervals, but doesn't handleindirect observations: in meta-analysis, we observe only noisy effects due tothe limited number of participants in each trial. To handle noise, we develop asimple, efficient version of fully-conformal kernel ridge regression, based ona novel condition called idiocentricity. We introduce noise-correcting terms inthe residuals and analyze their interaction with a ``variance shaving''technique. In multiple experiments on healthcare datasets, our algorithmsdeliver tighter, sounder intervals than traditional ones. This paper charts anew course for meta-analysis and evidence-based medicine, where heterogeneityand untrusted data are embraced for more nuanced and precise predictions.</description><author>Shiva Kaul, Geoffrey J. Gordon</author><pubDate>Fri, 12 Jul 2024 16:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09387v1</guid></item><item><title>Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost</title><link>http://arxiv.org/abs/2407.19825v1</link><description>Today's large language models (LLMs) can solve challenging question-answeringtasks, and prompt engineering techniques, such as chain-of-thought (CoT), havegained attention for enhancing the explanation and correctness of outputs.Nevertheless, models require significant time to generate answers augmentedwith lengthy reasoning details. To address this issue, this paper analyzes theimpact of output lengths on LLM inference pipelines and proposes novel metricsto evaluate them in terms of \textit{correct conciseness}. It also examines theimpact of controlling output length through a refined prompt engineeringstrategy, Constrained-CoT (CCoT), which encourages the model to limit outputlength. Experiments on pre-trained LLMs demonstrated the benefit of theproposed metrics and the effectiveness of CCoT across different models. Forinstance, constraining the reasoning of LLaMA2-70b to 100 words improves theaccuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, whilereducing the average output length by 28 words.</description><author>Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli</author><pubDate>Mon, 29 Jul 2024 09:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19825v1</guid></item><item><title>UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</title><link>http://arxiv.org/abs/2407.18391v1</link><description>Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on parwith larger models in general-domain visual grounding and question-answeringbenchmarks while offering advantages in computational efficiency and storage.However, their ability to handle rare objects, which fall into the long tail ofdata distributions, is less understood. To rigorously evaluate this aspect, weintroduce the "Uncontextualized Uncommon Objects" (UOUO) benchmark. Thisbenchmark focuses on systematically testing VLMs with both large and smallparameter counts on rare and specialized objects. Our comprehensive analysisreveals that while smaller VLMs maintain competitive performance on commondatasets, they significantly underperform on tasks involving uncommon objects.We also propose an advanced, scalable pipeline for data collection andcleaning, ensuring the UOUO benchmark provides high-quality, challenginginstances. These findings highlight the need to consider long-taildistributions when assessing the true capabilities of VLMs.</description><author>Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</author><pubDate>Thu, 25 Jul 2024 20:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18391v1</guid></item><item><title>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</title><link>http://arxiv.org/abs/2407.12735v1</link><description>Knowledge-based Visual Question Answering (KVQA) tasks require answeringquestions about images using extensive background knowledge. Despitesignificant advancements, generative models often struggle with these tasks dueto the limited integration of external knowledge. In this paper, we introduceEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) frameworkthat enables large language models (LLMs) to answer visual questions requiringfine-grained encyclopedic knowledge. To strive for high-performing retrieval,EchoSight first searches wiki articles by using visual-only information,subsequently, these candidate articles are further reranked according to theirrelevance to the combined text-image query. This approach significantlyimproves the integration of multimodal knowledge, leading to enhanced retrievaloutcomes and more accurate VQA responses. Our experimental results on theEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishesnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of41.8% on Encyclopedic VQA and 31.3% on InfoSeek.</description><author>Yibin Yan, Weidi Xie</author><pubDate>Wed, 17 Jul 2024 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12735v1</guid></item><item><title>ConvNLP: Image-based AI Text Detection</title><link>http://arxiv.org/abs/2407.07225v1</link><description>The potentials of Generative-AI technologies like Large Language models(LLMs) to revolutionize education are undermined by ethical considerationsaround their misuse which worsens the problem of academic dishonesty. LLMs likeGPT-4 and Llama 2 are becoming increasingly powerful in generatingsophisticated content and answering questions, from writing academic essays tosolving complex math problems. Students are relying on these LLMs to completetheir assignments and thus compromising academic integrity. Solutions to detectLLM-generated text are compute-intensive and often lack generalization. Thispaper presents a novel approach for detecting LLM-generated AI-text using avisual representation of word embedding. We have formulated a novelConvolutional Neural Network called ZigZag ResNet, as well as a scheduler forimproving generalization, named ZigZag Scheduler. Through extensive evaluationusing datasets of text generated by six different state-of-the-art LLMs, ourmodel demonstrates strong intra-domain and inter-domain generalizationcapabilities. Our best model detects AI-generated text with an impressiveaverage detection rate (over inter- and intra-domain test data) of 88.35%.Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Schedulerprovide a performance improvement of nearly 4% over the vanilla ResNet. Theend-to-end inference latency of our model is below 2.5ms per sentence. Oursolution offers a lightweight, computationally efficient, and fasteralternative to existing tools for AI-generated text detection, with bettergeneralization performance. It can help academic institutions in their fightagainst the misuse of LLMs in academic settings. Through this work, we aim tocontribute to safeguarding the principles of academic integrity and ensuringthe trustworthiness of student work in the era of advanced LLMs.</description><author>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube</author><pubDate>Tue, 09 Jul 2024 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07225v1</guid></item><item><title>FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding</title><link>http://arxiv.org/abs/2407.05183v2</link><description>Flowcharts are graphical tools for representing complex concepts in concisevisual representations. This paper introduces the FlowLearn dataset, a resourcetailored to enhance the understanding of flowcharts. FlowLearn contains complexscientific flowcharts and simulated flowcharts. The scientific subset contains3,858 flowcharts sourced from scientific literature and the simulated subsetcontains 10,000 flowcharts created using a customizable script. The dataset isenriched with annotations for visual components, OCR, Mermaid coderepresentation, and VQA question-answer pairs. Despite the proven capabilitiesof Large Vision-Language Models (LVLMs) in various visual understanding tasks,their effectiveness in decoding flowcharts - a crucial element of scientificcommunication - has yet to be thoroughly investigated. The FlowLearn test setis crafted to assess the performance of LVLMs in flowchart comprehension. Ourstudy thoroughly evaluates state-of-the-art LVLMs, identifying existinglimitations and establishing a foundation for future enhancements in thisrelatively underexplored domain. For instance, in tasks involving simulatedflowcharts, GPT-4V achieved the highest accuracy (58%) in counting the numberof nodes, while Claude recorded the highest accuracy (83%) in OCR tasks.Notably, no single model excels in all tasks within the FlowLearn framework,highlighting significant opportunities for further development.</description><author>Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, Longin Jan Latecki</author><pubDate>Tue, 09 Jul 2024 21:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05183v2</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item><item><title>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</title><link>http://arxiv.org/abs/2311.09821v2</link><description>Knowledge in the real world is being updated constantly. However, it iscostly to frequently update large language models (LLMs). Therefore, it iscrucial for LLMs to understand the concept of temporal knowledge. However,prior works on temporal question answering (TQA) did not emphasize multi-answerand multi-hop types of temporal reasoning. In this paper, we propose a complextemporal question-answering dataset Complex-TR that focuses on multi-answer andmulti-hop temporal reasoning. Besides, we also propose a novel dataaugmentation strategy to improve the complex temporal reasoning capability androbustness of LLMs. We conducted experiments on multiple temporal QA datasets.Experimental results show that our method is able to improve LLMs' performanceon temporal QA benchmarks by significant margins. Our code and data arereleased at: https://github.com/nusnlp/complex-tr.</description><author>Qingyu Tan, Hwee Tou Ng, Lidong Bing</author><pubDate>Fri, 12 Jul 2024 16:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09821v2</guid></item><item><title>Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders</title><link>http://arxiv.org/abs/2402.00260v2</link><description>The robotic intervention for individuals with Autism Spectrum Disorder (ASD)has generally used pre-defined scripts to deliver verbal content duringone-to-one therapy sessions. This practice restricts the use of robots tolimited, pre-mediated instructional curricula. In this paper, we increase robotautonomy in one such robotic intervention for children with ASD by implementingperspective-taking teaching. Our approach uses large language models (LLM) togenerate verbal content as texts and then deliver it to the child via roboticspeech. In the proposed pipeline, we teach perspective-taking through which ourrobot takes up three roles: initiator, prompter, and reinforcer. We adopted theGPT-2 + BART pipelines to generate social situations, ask questions (asinitiator), and give options (as prompter) when required. The robot encouragesthe child by giving positive reinforcement for correct answers (as areinforcer). In addition to our technical contribution, we conducted ten-minutesessions with domain experts simulating an actual perspective teaching session,with the researcher acting as a child participant. These sessions validated ourrobotic intervention pipeline through surveys, including those from NASA TLXand GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with anall GPT-2 and found the performance of the former to be better. Based on theresponses by the domain experts, the robot session demonstrated higherperformance with no additional increase in mental or physical demand, temporaldemand, effort, or frustration compared to a no-robot session. We alsoconcluded that the domain experts perceived the robot as ideally safe, likable,and reliable.</description><author>Ruchik Mishra, Karla Conn Welch, Dan O Popa</author><pubDate>Wed, 10 Jul 2024 01:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00260v2</guid></item><item><title>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</title><link>http://arxiv.org/abs/2407.07577v1</link><description>The rapid advancement of Large Vision-Language models (LVLMs) hasdemonstrated a spectrum of emergent capabilities. Nevertheless, current modelsonly focus on the visual content of a single scenario, while their ability toassociate instances across different scenes has not yet been explored, which isessential for understanding complex visual content, such as movies withmultiple characters and intricate plots. Towards movie understanding, acritical initial step for LVLMs is to unleash the potential of characteridentities memory and recognition across multiple visual scenarios. To achievethe goal, we propose visual instruction tuning with ID reference and develop anID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our researchintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory andrecognition across four dimensions: matching, location, question-answering, andcaptioning. Our findings highlight the limitations of existing LVLMs inrecognizing and associating instance identities with ID reference. This paperpaves the way for future artificial intelligence systems to possessmulti-identity visual inputs, thereby facilitating the comprehension of complexvisual narratives like movies.</description><author>Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo</author><pubDate>Wed, 10 Jul 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07577v1</guid></item><item><title>MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</title><link>http://arxiv.org/abs/2407.01523v2</link><description>Understanding documents with rich layouts and multi-modal components is along-standing and practical task. Recent Large Vision-Language Models (LVLMs)have made remarkable strides in various tasks, particularly in single-pagedocument understanding (DU). However, their abilities on long-context DU remainan open problem. This work presents MMLongBench-Doc, a long-context,multi-modal benchmark comprising 1,062 expert-annotated questions. Distinctfrom previous datasets, it is constructed upon 130 lengthy PDF-formatteddocuments with an average of 49.4 pages and 20,971 textual tokens. Towardscomprehensive evaluation, answers to these questions rely on pieces of evidencefrom (1) different sources (text, image, chart, table, and layout structure)and (2) various locations (i.e. page number). Moreover, 33.2% of the questionsare cross-page questions requiring evidence across multiple pages. 22.8% of thequestions are designed to be unanswerable for detecting potentialhallucinations. Experiments on 14 LVLMs demonstrate that long-context DUgreatly challenges current models. Notably, the best-performing model, GPT-4o,achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worseperformance than their LLM counterparts which are fed with lossy-parsed OCRdocuments. These results validate the necessity of future research toward morecapable long-context LVLMs. Project Page:https://mayubo2333.github.io/MMLongBench-Doc</description><author>Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun</author><pubDate>Wed, 10 Jul 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01523v2</guid></item><item><title>On scalable oversight with weak LLMs judging strong LLMs</title><link>http://arxiv.org/abs/2407.04622v2</link><description>Scalable oversight protocols aim to enable humans to accurately supervisesuperhuman AI. In this paper we study debate, where two AI's compete toconvince a judge; consultancy, where a single AI tries to convince a judge thatasks questions; and compare to a baseline of direct question-answering, wherethe judge just answers outright without the AI. We use large language models(LLMs) as both AI agents and as stand-ins for human judges, taking the judgemodels to be weaker than agent models. We benchmark on a diverse range ofasymmetries between judges and agents, extending previous work on a singleextractive QA task with information asymmetry, to also include mathematics,coding, logic and multimodal reasoning asymmetries. We find that debateoutperforms consultancy across all tasks when the consultant is randomlyassigned to argue for the correct/incorrect answer. Comparing debate to directquestion answering, the results depend on the type of task: in extractive QAtasks with information asymmetry debate outperforms direct question answering,but in other tasks without information asymmetry the results are mixed.Previous work assigned debaters/consultants an answer to argue for. When weallow them to instead choose which answer to argue for, we find judges are lessfrequently convinced by the wrong answer in debate than in consultancy.Further, we find that stronger debater models increase judge accuracy, thoughmore modestly than in previous studies.</description><author>Zachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah</author><pubDate>Fri, 12 Jul 2024 16:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04622v2</guid></item><item><title>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</title><link>http://arxiv.org/abs/2407.09413v1</link><description>Seeking answers to questions within long scientific research articles is acrucial area of study that aids readers in quickly addressing their inquiries.However, existing question-answering (QA) datasets based on scientific papersare limited in scale and focus solely on textual content. To address thislimitation, we introduce SPIQA (Scientific Paper Image Question Answering), thefirst large-scale QA dataset specifically designed to interpret complex figuresand tables within the context of scientific research articles across variousdomains of computer science. Leveraging the breadth of expertise and ability ofmultimodal large language models (MLLMs) to understand figures, we employautomatic and manual curation to create the dataset. We craft aninformation-seeking task involving multiple images that cover a wide variety ofplots, charts, tables, schematic diagrams, and result visualizations. SPIQAcomprises 270K questions divided into training, validation, and three differentevaluation splits. Through extensive experiments with 12 prominent foundationalmodels, we evaluate the ability of current multimodal systems to comprehend thenuanced aspects of research articles. Additionally, we propose aChain-of-Thought (CoT) evaluation strategy with in-context retrieval thatallows fine-grained, step-by-step assessment and improves model performance. Wefurther explore the upper bounds of performance enhancement with additionaltextual information, highlighting its promising potential for future researchand the dataset's impact on revolutionizing how we interact with scientificliterature.</description><author>Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan</author><pubDate>Fri, 12 Jul 2024 16:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09413v1</guid></item><item><title>BatchPrompt: Accomplish more with less</title><link>http://arxiv.org/abs/2309.00384v3</link><description>As the ever-increasing token limits of large language models (LLMs) haveenabled long context as input, prompting with single data samples might nolonger an efficient way. A straightforward strategy improving efficiency is tobatch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4),which we call BatchPrompt. We have two initial observations for prompting withbatched data. First, we find that prompting with batched data in longercontexts will inevitably lead to worse performance, compared to single-dataprompting. Second, the performance of the language model is significantlycorrelated with the positions and order of the batched data, due to thecorresponding change in decoder context. To retain efficiency and overcomeperformance loss, we propose Batch Permutation and Ensembling (BPE), and anovel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensiveexperimental evaluation demonstrates that BPE can boost the performance ofBatchPrompt with a striking margin on a range of popular NLP tasks, includingquestion answering (Boolq), textual entailment (RTE), and duplicate questionsidentification (QQP). These performances are even competitive with/higher thansingle-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLMcalls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32,using just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5%to 91.1% with 30.8% tokens). To the best of our knowledge, this is the firstwork to technically improve prompting efficiency of large language models. Wehope our simple yet effective approach will shed light on the future researchof large language models. The code will be released.</description><author>Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham</author><pubDate>Mon, 15 Jul 2024 05:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00384v3</guid></item><item><title>NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?</title><link>http://arxiv.org/abs/2407.11963v1</link><description>In evaluating the long-context capabilities of large language models (LLMs),identifying content relevant to a user's query from original long documents isa crucial prerequisite for any LLM to answer questions based on long text. Wepresent NeedleBench, a framework consisting of a series of progressively morechallenging tasks for assessing bilingual long-context capabilities, spanningmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) anddifferent depth ranges, allowing the strategic insertion of critical datapoints in different text depth zones to rigorously test the retrieval andreasoning capabilities of models in diverse contexts. We use the NeedleBenchframework to assess how well the leading open-source models can identify keyinformation relevant to the question and apply that information to reasoning inbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge(ATC) to mimic the complexity of logical reasoning challenges that are likelyto be present in real-world long-context tasks, providing a simple method forevaluating LLMs in dealing with complex long-context situations. Our resultssuggest that current LLMs have significant room for improvement in practicallong-context applications, as they struggle with the complexity of logicalreasoning challenges that are likely to be present in real-world long-contexttasks. All codes and resources are available at OpenCompass:https://github.com/open-compass/opencompass.</description><author>Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen</author><pubDate>Tue, 16 Jul 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11963v1</guid></item><item><title>DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection</title><link>http://arxiv.org/abs/2407.09283v1</link><description>Semantic role labeling (SRL) enriches many downstream applications, e.g.,machine translation, question answering, summarization, and stance/beliefdetection. However, building multilingual SRL models is challenging due to thescarcity of semantically annotated corpora for multiple languages. Moreover,state-of-the-art SRL projection (XSRL) based on large language models (LLMs)yields output that is riddled with spurious role labels. Remediation of suchhallucinations is not straightforward due to the lack of explainability ofLLMs. We show that hallucinated role labels are related to naturally occurringdivergence types that interfere with initial alignments. We implementDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraginglinguistically-informed alignment remediation followed by greedy First-ComeFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRLprojection without additional transformer-based machinery, beating XSRL in bothhuman and automatic comparisons, and advancing beyond headwords to accommodatephrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as ourground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt ourapproach to other language pairs (e.g., English-Tagalog).</description><author>Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie Dorr</author><pubDate>Fri, 12 Jul 2024 14:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09283v1</guid></item><item><title>Semi-Supervised Learning for Deep Causal Generative Models</title><link>http://arxiv.org/abs/2403.18717v2</link><description>Developing models that are capable of answering questions of the form "Howwould x change if y had been z?'" is fundamental to advancing medical imageanalysis. Training causal generative models that address such counterfactualquestions, though, currently requires that all relevant variables have beenobserved and that the corresponding labels are available in the training data.However, clinical data may not have complete records for all patients and stateof the art causal generative models are unable to take full advantage of this.We thus develop, for the first time, a semi-supervised deep causal generativemodel that exploits the causal relationships between variables to maximise theuse of all available data. We explore this in the setting where each sample iseither fully labelled or fully unlabelled, as well as the more clinicallyrealistic case of having different labels missing for each sample. We leveragetechniques from causal inference to infer missing values and subsequentlygenerate realistic counterfactuals, even for samples with incomplete labels.</description><author>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</author><pubDate>Fri, 12 Jul 2024 14:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18717v2</guid></item><item><title>Inference Optimization of Foundation Models on AI Accelerators</title><link>http://arxiv.org/abs/2407.09111v1</link><description>Powerful foundation models, including large language models (LLMs), withTransformer architectures have ushered in a new era of Generative AI acrossvarious industries. Industry and research community have witnessed a largenumber of new applications, based on those foundation models. Such applicationsinclude question and answer, customer services, image and video generation, andcode completions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibitive inferencecosts and high latency in real-world scenarios. As a result, the demand forcost-effective and fast inference using AI accelerators is ever more higher. Tothis end, our tutorial offers a comprehensive discussion on complementaryinference optimization techniques using AI accelerators. Beginning with anoverview of basic Transformer architectures and deep learning systemframeworks, we deep dive into system optimization techniques for fast andmemory-efficient attention computations and discuss how they can be implementedefficiently on AI accelerators. Next, we describe architectural elements thatare key for fast transformer inference. Finally, we examine various modelcompression and fast decoding strategies in the same context.</description><author>Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis</author><pubDate>Fri, 12 Jul 2024 09:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09111v1</guid></item><item><title>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</title><link>http://arxiv.org/abs/2406.10421v2</link><description>With the rapid development of Large Language Models (LLMs), it is crucial tohave benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such aswriting algorithms, querying databases or giving mathematical proofs. Inspiredby the way university students are evaluated on such tasks, in this paper, wepropose SciEx - a benchmark consisting of university computer science examquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)multilingual, containing both English and German exams, and (2) multi-modal,containing questions that involve images, and (3) contains various types offreeform questions with different difficulty levels, due to the nature ofuniversity exams. We evaluate the performance of various state-of-the-art LLMson our new benchmark. Since SciEx questions are freeform, it is notstraightforward to evaluate LLM performance. Therefore, we provide human expertgrading of the LLM outputs on SciEx. We show that the free-form exams in SciExremain challenging for the current LLMs, where the best LLM only achieves59.4\% exam grade on average. We also provide detailed comparisons between LLMperformance and student performance on SciEx. To enable future evaluation ofnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solvingthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation withexpert grading.</description><author>Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues</author><pubDate>Fri, 12 Jul 2024 10:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10421v2</guid></item><item><title>TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models</title><link>http://arxiv.org/abs/2407.09424v1</link><description>Large Language Models (LLMs) have the potential to revolutionize the SixthGeneration (6G) communication networks. However, current mainstream LLMsgenerally lack the specialized knowledge in telecom domain. In this paper, forthe first time, we propose a pipeline to adapt any general purpose LLMs to atelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,instruction dataset, preference dataset to perform continual pre-training,instruct tuning and alignment tuning respectively. Besides, due to the lack ofwidely accepted evaluation benchmarks in telecom domain, we extend existingevaluation benchmarks and proposed three new benchmarks, namely, Telecom MathModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks providea holistic evaluation of the capabilities of LLMs including math modeling,Open-Ended question answering, code generation, infilling, summarization andanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state ofthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom MathModeling benchmark significantly and achieve comparable performance in variousevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,telecom code summary and generation and infilling.</description><author>Hang Zou, Qiyang Zhao, Yu Tian, Lina Bariah, Faouzi Bader, Thierry Lestable, Merouane Debbah</author><pubDate>Fri, 12 Jul 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09424v1</guid></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>http://arxiv.org/abs/2407.10887v1</link><description>Amid growing concerns over the ease of theft and misuse of Large LanguageModels (LLMs), the need for fingerprinting models has increased.Fingerprinting, in this context, means that the model owner can link a givenmodel to their original version, thereby identifying if their model is beingmisused or has been completely stolen. In this paper, we first define a setfive properties a successful fingerprint should satisfy; namely, thefingerprint should be Transparent, Efficient, Persistent, Robust, andUnforgeable. Next, we propose Chain &amp; Hash, a new, simple fingerprintingapproach that implements a fingerprint with a cryptographic flavor, achievingall these properties. Chain &amp; Hash involves generating a set of questions (thefingerprints) along with a set of potential answers. These elements are hashedtogether using a secure hashing technique to select the value for eachquestion, hence providing an unforgeability property-preventing adversariesfrom claiming false ownership. We evaluate the Chain &amp; Hash technique onmultiple models and demonstrate its robustness against benign transformations,such as fine-tuning on different datasets, and adversarial attempts to erasethe fingerprint. Finally, our experiments demonstrate the efficiency ofimplementing Chain &amp; Hash and its utility, where fingerprinted models achievealmost the same performance as non-fingerprinted ones across differentbenchmarks.</description><author>Mark Russinovich, Ahmed Salem</author><pubDate>Mon, 15 Jul 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10887v1</guid></item><item><title>Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin</title><link>http://arxiv.org/abs/2406.06566v2</link><description>Domain specific digital twins, representing a digital replica of varioussegments of the smart grid, are foreseen as able to model, simulate, andcontrol the respective segments. At the same time, knowledge-based digitaltwins, coupled with AI, may also empower humans to understand aspects of thesystem through natural language interaction in view of planning and policymaking. This paper is the first to assess and report on the potential ofRetrieval Augmented Generation (RAG) question answers related to householdelectrical energy measurement aspects leveraging a knowledge-based energydigital twin. Relying on the recently published electricity consumptionknowledge graph that actually represents a knowledge-based digital twin, westudy the capabilities of ChatGPT, Gemini and Llama in answering electricityrelated questions. Furthermore, we compare the answers with the ones generatedthrough a RAG techniques that leverages an existing electricity knowledge-baseddigital twin. Our findings illustrate that the RAG approach not only reducesthe incidence of incorrect information typically generated by LLMs but alsosignificantly improves the quality of the output by grounding responses inverifiable data. This paper details our methodology, presents a comparativeanalysis of responses with and without RAG, and discusses the implications ofour findings for future applications of AI in specialized sectors like energydata analysis.</description><author>Carolina Fortuna, Vid Hanžel, Blaž Bertalanič</author><pubDate>Thu, 11 Jul 2024 13:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06566v2</guid></item><item><title>Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition</title><link>http://arxiv.org/abs/2403.00499v2</link><description>Recent advances in LLMs have sparked a debate on whether they understandtext. In this position paper, we argue that opponents in this debate holddifferent definitions for understanding, and particularly differ in their viewon the role of consciousness. To substantiate this claim, we propose a thoughtexperiment involving an open-source chatbot $Z$ which excels on every possiblebenchmark, seemingly without subjective experience. We ask whether $Z$ iscapable of understanding, and show that different schools of thought withinseminal AI research seem to answer this question differently, uncovering theirterminological disagreement. Moving forward, we propose two distinct workingdefinitions for understanding which explicitly acknowledge the question ofconsciousness, and draw connections with a rich literature in philosophy,psychology and neuroscience.</description><author>Ariel Goldstein, Gabriel Stanovsky</author><pubDate>Thu, 11 Jul 2024 15:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00499v2</guid></item><item><title>Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing</title><link>http://arxiv.org/abs/2407.08770v1</link><description>Large Language Models (LLMs) have demonstrated great potential as generalistassistants, showcasing powerful task understanding and problem-solvingcapabilities. To deploy LLMs as AI assistants, it is crucial that these modelsexhibit desirable behavioral traits, such as non-toxicity and resilienceagainst jailbreak attempts. Current methods for detoxification or preventingjailbreaking usually involve Supervised Fine-Tuning (SFT) or ReinforcementLearning from Human Feedback (RLHF), which requires finetuning billions ofparameters through gradient descent with substantial computation cost.Furthermore, models modified through SFT and RLHF may deviate from thepretrained models, potentially leading to a degradation in foundational LLMcapabilities. In this paper, we observe that surprisingly, directly editing asmall subset of parameters can effectively modulate specific behaviors of LLMs,such as detoxification and resistance to jailbreaking. Specifically, for abehavior that we aim to avoid, we employ a linear classifier, which we term thebehavior probe, to classify binary behavior labels within the hidden statespace of the LLM. Using this probe, we introduce an algorithm to identify acritical subset of LLM parameters that significantly influence this targetedbehavior. Then we directly edit these selected parameters by shifting themtowards the behavior probe. Such a direct parameter editing method necessitatesonly inference-level computational resources. Experiments demonstrate that inthe representative detoxification task, our approach achieves reductions of upto 90.0\% in toxicity on the RealToxicityPrompts dataset and 49.2\% on ToxiGen,while maintaining the LLM's general capabilities in areas such as common sense,question answering, and mathematics. Our code is available athttps://github.com/lucywang720/model-surgery.</description><author>Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang</author><pubDate>Thu, 11 Jul 2024 17:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08770v1</guid></item><item><title>PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods</title><link>http://arxiv.org/abs/2407.06985v2</link><description>In domain-specific applications, GPT-4, augmented with precise prompts orRetrieval-Augmented Generation (RAG), shows notable potential but faces thecritical tri-lemma of performance, cost, and data privacy. High performancerequires sophisticated processing techniques, yet managing multiple agentswithin a complex workflow often proves costly and challenging. To address this,we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.This systematizes domain-specific tasks by integrating precise questiondecomposition, advanced information retrieval, comprehensive summarization, andrigorous self-assessment. Given the concerns of cost and data privacy,enterprises are shifting from proprietary models like GPT-4 to custom models,striking a balance between cost, security, and performance. We developedindustrial practices leveraging online data and user feedback for efficientmodel tuning. This study provides best practice guidelines for applyingmulti-agent systems in domain-specific problem-solving and implementingeffective agent tuning strategies. Our empirical studies, particularly in thefinancial question-answering domain, demonstrate that our approach achieves95.0% of GPT-4's performance, while effectively managing costs and ensuringdata privacy.</description><author>Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu</author><pubDate>Wed, 10 Jul 2024 03:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06985v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>Towards Multimodal In-Context Learning for Vision &amp; Language Models</title><link>http://arxiv.org/abs/2403.12736v2</link><description>State-of-the-art Vision-Language Models (VLMs) ground the vision and thelanguage modality primarily via projecting the vision tokens from the encoderto language-like tokens, which are directly fed to the Large Language Model(LLM) decoder. While these models have shown unprecedented performance in manydownstream zero-shot tasks (eg image captioning, question answers, etc), stilllittle emphasis has been put on transferring one of the core LLM capability ofIn-Context Learning (ICL). ICL is the ability of a model to reason about adownstream task with a few examples demonstrations embedded in the prompt. Inthis work, through extensive evaluations, we find that the state-of-the-artVLMs somewhat lack the ability to follow ICL instructions. In particular, wediscover that even models that underwent large-scale mixed modalitypre-training and were implicitly guided to make use of interleaved image andtext information (intended to consume helpful context from multiple images)under-perform when prompted with few-shot demonstrations (in an ICL way),likely due to their lack of direct ICL instruction tuning. To enhance the ICLabilities of the present VLM, we propose a simple yet surprisingly effectivemulti-turn curriculum-based learning methodology with effective data mixes,leading up to a significant 21.03% (and 11.3% on average) ICL performance boostover the strongest VLM baselines and a variety of ICL benchmarks. Furthermore,we also contribute new benchmarks for ICL evaluation in VLMs and discuss theiradvantages over the prior art.</description><author>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky</author><pubDate>Wed, 17 Jul 2024 08:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12736v2</guid></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>http://arxiv.org/abs/2407.10887v2</link><description>Amid growing concerns over the ease of theft and misuse of Large LanguageModels (LLMs), the need for fingerprinting models has increased.Fingerprinting, in this context, means that the model owner can link a givenmodel to their original version, thereby identifying if their model is beingmisused or has been completely stolen. In this paper, we first define a setfive properties a successful fingerprint should satisfy; namely, thefingerprint should be Transparent, Efficient, Persistent, Robust, andUnforgeable. Next, we propose Chain &amp; Hash, a new, simple fingerprintingapproach that implements a fingerprint with a cryptographic flavor, achievingall these properties. Chain &amp; Hash involves generating a set of questions (thefingerprints) along with a set of potential answers. These elements are hashedtogether using a secure hashing technique to select the value for eachquestion, hence providing an unforgeability property-preventing adversariesfrom claiming false ownership. We evaluate the Chain &amp; Hash technique onmultiple models and demonstrate its robustness against benign transformations,such as fine-tuning on different datasets, and adversarial attempts to erasethe fingerprint. Finally, our experiments demonstrate the efficiency ofimplementing Chain &amp; Hash and its utility, where fingerprinted models achievealmost the same performance as non-fingerprinted ones across differentbenchmarks.</description><author>Mark Russinovich, Ahmed Salem</author><pubDate>Wed, 17 Jul 2024 07:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10887v2</guid></item><item><title>SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant</title><link>http://arxiv.org/abs/2403.11299v2</link><description>Recent advances in vision-language models have shown notable generalizationin broad tasks through visual instruction tuning. However, bridging the gapbetween the pre-trained vision encoder and the large language models (LLMs)becomes the whole network's bottleneck. To improve cross-modality alignment,existing works usually consider more visual instruction data covering a broaderrange of vision tasks to fine-tune the model for question-answering, which,however, is costly to obtain and has not thoroughly explored the richcontextual information contained in images. This paper first attempts toharness the overlooked context within visual instruction data, training themodel to self-supervised "learning" how to ask high-quality questions. In thisway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for LargeVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexibleand meaningful image-related questions while analyzing the visual clue andprior language knowledge, signifying an advanced level of generalized visualunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instructiondata shows a performance improvement compared with traditionalvisual-instruction tuning methods. This improvement highlights the efficacy ofself-questioning techniques in achieving a deeper and more nuancedcomprehension of visual content across various contexts.</description><author>Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao</author><pubDate>Mon, 15 Jul 2024 17:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11299v2</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v1</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Mon, 15 Jul 2024 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v1</guid></item><item><title>AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering</title><link>http://arxiv.org/abs/2311.14906v2</link><description>We propose a novel and challenging benchmark, AutoEval-Video, tocomprehensively evaluate large vision-language models in open-ended videoquestion answering. The comprehensiveness of AutoEval-Video is demonstrated intwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9skill dimensions, addressing capabilities of perception, comprehension, andgeneration. 2) AutoEval-Video contains newly collected videos that cover over40 distinct themes. To efficiently evaluate responses to the open-endedquestions, we employ an LLM-based evaluation approach, but instead of merelyproviding a reference answer, we annotate unique evaluation rules for everysingle instance (video-question pair). To maximize the robustness of theserules, we develop a novel adversarial annotation mechanism. By usinginstance-specific rules as prompt, GPT-4, as an automatic evaluator, canachieve a stable evaluation accuracy of around 97.0%, comparable to the 94.9% -97.5% accuracy of a human evaluator. Furthermore, we assess the performance ofeight large vision-language models on AutoEval-Video. Among them, GPT-4V(ision)significantly outperforms other models, achieving an accuracy of 32.2%.However, there is still substantial room for improvement compared to humanaccuracy of 72.8%. By conducting an extensive case study, we uncover severaldrawbacks of GPT-4V, such as limited temporal and dynamic comprehension, andoverly general responses. Code is available athttps://github.com/Xiuyuan-Chen/AutoEval-Video.</description><author>Xiuyuan Chen, Yuan Lin, Yuchen Zhang, Weiran Huang</author><pubDate>Mon, 15 Jul 2024 16:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14906v2</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v2</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Thu, 18 Jul 2024 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v2</guid></item><item><title>FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries</title><link>http://arxiv.org/abs/2407.10810v1</link><description>Intelligence is key to advancing integrated circuit (IC) fabrication. Recentbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleledabilities in understanding images and text, fostering intelligent fabrication.Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabricationlarge multimodal model for wafer defect knowledge query. FabGPT manifestsexpertise in conducting defect detection in Scanning Electron Microscope (SEM)images, performing root cause analysis, and providing expert question-answering(Q&amp;A) on fabrication processes. FabGPT matches enhanced multimodal features toautomatically detect minute defects under complex wafer backgrounds and reducethe subjectivity of manual threshold settings. Besides, the proposed modulationmodule and interactive corpus training strategy embed wafer defect knowledgeinto the pre-trained model, effectively balancing Q&amp;A queries related to defectknowledge and original knowledge and mitigating the modality bias issues.Experiments on in-house fab data (SEM-WaD) show that our FabGPT achievessignificant performance improvement in wafer defect detection and knowledgequerying.</description><author>Yuqi Jiang, Xudong Lu, Qian Jin, Qi Sun, Hanming Wu, Cheng Zhuo</author><pubDate>Mon, 15 Jul 2024 15:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10810v1</guid></item><item><title>Advancing Question Answering on Handwritten Documents: A State-of-the-Art Recognition-Based Model for HW-SQuAD</title><link>http://arxiv.org/abs/2406.17437v2</link><description>Question-answering handwritten documents is a challenging task with numerousreal-world applications. This paper proposes a novel recognition-based approachthat improves upon the previous state-of-the-art on the HW-SQuAD and BenthamQAdatasets. Our model incorporates transformer-based document retrieval andensemble methods at the model level, achieving an Exact Match score of 82.02%and 69% in HW-SQuAD and BenthamQA datasets, respectively, surpassing theprevious best recognition-based approach by 10.89% and 3%. We also enhance thedocument retrieval component, boosting the top-5 retrieval accuracy from 90% to95.30%. Our results demonstrate the significance of our proposed approach inadvancing question answering on handwritten documents. The code and trainedmodels will be publicly available to facilitate future research in thiscritical area of natural language.</description><author>Aniket Pal, Ajoy Mondal, C. V. Jawahar</author><pubDate>Mon, 15 Jul 2024 14:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17437v2</guid></item><item><title>Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation</title><link>http://arxiv.org/abs/2404.15687v2</link><description>Vulnerability detection is crucial for ensuring the security and reliabilityof software systems. Recently, Graph Neural Networks (GNNs) have emerged as aprominent code embedding approach for vulnerability detection, owing to theirability to capture the underlying semantic structure of source code. However,GNNs face significant challenges in explainability due to their inherentlyblack-box nature. To this end, several factual reasoning-based explainers havebeen proposed. These explainers provide explanations for the predictions madeby GNNs by analyzing the key features that contribute to the outcomes. We arguethat these factual reasoning-based explanations cannot answer critical what-ifquestions: What would happen to the GNN's decision if we were to alter the codegraph into alternative structures? Inspired by advancements of counterfactualreasoning in artificial intelligence, we propose CFExplainer, a novelcounterfactual explainer for GNN-based vulnerability detection. Unlike factualreasoning-based explainers, CFExplainer seeks the minimal perturbation to theinput code graph that leads to a change in the prediction, thereby addressingthe what-if questions for vulnerability detection. We term this perturbation acounterfactual explanation, which can pinpoint the root causes of the detectedvulnerability and furnish valuable insights for developers to undertakeappropriate actions for fixing the vulnerability. Extensive experiments on fourGNN-based vulnerability detection models demonstrate the effectiveness ofCFExplainer over existing state-of-the-art factual reasoning-based explainers.</description><author>Zhaoyang Chu, Yao Wan, Qian Li, Yang Wu, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin</author><pubDate>Mon, 15 Jul 2024 14:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15687v2</guid></item><item><title>$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity</title><link>http://arxiv.org/abs/2407.10691v1</link><description>Recent studies show the growing significance of document retrieval in thegeneration of LLMs, i.e., RAG, within the scientific domain by bridging theirknowledge gap. However, dense retrievers often struggle with domain-specificretrieval and complex query-document relationships, particularly when querysegments correspond to various parts of a document. To alleviate such prevalentchallenges, this paper introduces $\texttt{MixGR}$, which improves denseretrievers' awareness of query-document matching across various levels ofgranularity in queries and documents using a zero-shot approach.$\texttt{MixGR}$ fuses various metrics based on these granularities to a unitedscore that reflects a comprehensive query-document similarity. Our experimentsdemonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,respectively, averaged on queries containing multiple subqueries from fivescientific retrieval datasets. Moreover, the efficacy of two downstreamscientific question-answering tasks highlights the advantage of$\texttt{MixGR}$to boost the application of LLMs in the scientific domain.</description><author>Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl</author><pubDate>Mon, 15 Jul 2024 13:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10691v1</guid></item><item><title>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2401.17043v3</link><description>Retrieval-Augmented Generation (RAG) is a technique that enhances thecapabilities of large language models (LLMs) by incorporating externalknowledge sources. This method addresses common LLM limitations, includingoutdated information and the tendency to produce inaccurate "hallucinated"content. However, the evaluation of RAG systems is challenging, as existingbenchmarks are limited in scope and diversity. Most of the current benchmarkspredominantly assess question-answering applications, overlooking the broaderspectrum of situations where RAG could prove advantageous. Moreover, they onlyevaluate the performance of the LLM component of the RAG pipeline in theexperiments, and neglect the influence of the retrieval component and theexternal knowledge database. To address these issues, this paper constructs alarge-scale and more comprehensive benchmark, and evaluates all the componentsof RAG systems in various RAG application scenarios. Specifically, we havecategorized the range of RAG applications into four distinct types-Create,Read, Update, and Delete (CRUD), each representing a unique use case. "Create"refers to scenarios requiring the generation of original, varied content."Read" involves responding to intricate questions in knowledge-intensivesituations. "Update" focuses on revising and rectifying inaccuracies orinconsistencies in pre-existing texts. "Delete" pertains to the task ofsummarizing extensive texts into more concise forms. For each of these CRUDcategories, we have developed comprehensive datasets to evaluate theperformance of RAG systems. We also analyze the effects of various componentsof the RAG system, such as the retriever, the context length, the knowledgebase construction, and the LLM. Finally, we provide useful insights foroptimizing the RAG technology for different scenarios.</description><author>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen</author><pubDate>Mon, 15 Jul 2024 11:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17043v3</guid></item><item><title>CompAct: Compressing Retrieved Documents Actively for Question Answering</title><link>http://arxiv.org/abs/2407.09014v2</link><description>Retrieval-augmented generation supports language models to strengthen theirfactual groundings by providing external contexts. However, language modelsoften face challenges when given extensive information, diminishing theireffectiveness in solving questions. Context compression tackles this issue byfiltering out irrelevant information, but current methods still struggle inrealistic scenarios where crucial information cannot be captured with asingle-step approach. To overcome this limitation, we introduce CompAct, anovel framework that employs an active strategy to condense extensive documentswithout losing key information. Our experiments demonstrate that CompAct bringssignificant improvements in both performance and compression rate on multi-hopquestion-answering (QA) benchmarks. CompAct flexibly operates as acost-efficient plug-in module with various off-the-shelf retrievers or readers,achieving exceptionally high compression rates (47x).</description><author>Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang</author><pubDate>Mon, 15 Jul 2024 11:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09014v2</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>Evaluating Nuanced Bias in Large Language Model Free Response Answers</title><link>http://arxiv.org/abs/2407.08842v1</link><description>Pre-trained large language models (LLMs) can now be easily adapted forspecific business purposes using custom prompts or fine tuning. Thesecustomizations are often iteratively re-engineered to improve some aspect ofperformance, but after each change businesses want to ensure that there hasbeen no negative impact on the system's behavior around such critical issues asbias. Prior methods of benchmarking bias use techniques such as word maskingand multiple choice questions to assess bias at scale, but these do not captureall of the nuanced types of bias that can occur in free response answers, thetypes of answers typically generated by LLM systems. In this paper, we identifyseveral kinds of nuanced bias in free text that cannot be similarly identifiedby multiple choice tests. We describe these as: confidence bias, implied bias,inclusion bias and erasure bias. We present a semi-automated pipeline fordetecting these types of bias by first eliminating answers that can beautomatically classified as unbiased and then co-evaluating name reversed pairsusing crowd workers. We believe that the nuanced classifications our methodgenerates can be used to give better feedback to LLMs, especially as LLMreasoning capabilities become more advanced.</description><author>Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar, Moumita Sinha</author><pubDate>Thu, 11 Jul 2024 19:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08842v1</guid></item><item><title>Teaching CORnet Human fMRI Representations for Enhanced Model-Brain Alignment</title><link>http://arxiv.org/abs/2407.10414v1</link><description>Deep convolutional neural networks (DCNNs) have demonstrated excellentperformance in object recognition and have been found to share somesimilarities with brain visual processing. However, the substantial gap betweenDCNNs and human visual perception still exists. Functional magnetic resonanceimaging (fMRI) as a widely used technique in cognitive neuroscience can recordneural activation in the human visual cortex during the process of visualperception. Can we teach DCNNs human fMRI signals to achieve a more brain-likemodel? To answer this question, this study proposed ReAlnet-fMRI, a model basedon the SOTA vision model CORnet but optimized using human fMRI data through amulti-layer encoding-based alignment framework. This framework has been shownto effectively enable the model to learn human brain representations. ThefMRI-optimized ReAlnet-fMRI exhibited higher similarity to the human brain thanboth CORnet and the control model in within-and across-subject as well aswithin- and across-modality model-brain (fMRI and EEG) alignment evaluations.Additionally, we conducted an in-depth analyses to investigate how the internalrepresentations of ReAlnet-fMRI differ from CORnet in encoding various objectdimensions. These findings provide the possibility of enhancing thebrain-likeness of visual models by integrating human neural data, helping tobridge the gap between computer vision and visual neuroscience.</description><author>Zitong Lu, Yile Wang</author><pubDate>Mon, 15 Jul 2024 03:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10414v1</guid></item><item><title>Segmentation-guided Attention for Visual Question Answering from Remote Sensing Images</title><link>http://arxiv.org/abs/2407.08669v1</link><description>Visual Question Answering for Remote Sensing (RSVQA) is a task that aims atanswering natural language questions about the content of a remote sensingimage. The visual features extraction is therefore an essential step in a VQApipeline. By incorporating attention mechanisms into this process, models gainthe ability to focus selectively on salient regions of the image, prioritizingthe most relevant visual information for a given question. In this work, wepropose to embed an attention mechanism guided by segmentation into a RSVQApipeline. We argue that segmentation plays a crucial role in guiding attentionby providing a contextual understanding of the visual information, underlyingspecific objects or areas of interest. To evaluate this methodology, we providea new VQA dataset that exploits very high-resolution RGB orthophotos annotatedwith 16 segmentation classes and question/answer pairs. Our study showspromising results of our new methodology, gaining almost 10% of overallaccuracy compared to a classical method on the proposed dataset.</description><author>Lucrezia Tosato, Hichem Boussaid, Flora Weissgerber, Camille Kurtz, Laurent Wendling, Sylvain Lobry</author><pubDate>Thu, 11 Jul 2024 16:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08669v1</guid></item><item><title>Language models are better than humans at next-token prediction</title><link>http://arxiv.org/abs/2212.11281v2</link><description>Current language models are considered to have sub-human capabilities atnatural language tasks like question-answering or writing code. However,language models are not trained to perform well at these tasks, they aretrained to accurately predict the next token given previous tokes in tokenizedtext. It is not clear whether language models are better or worse than humansat next token prediction. To try to answer this question, we performed twodistinct experiments to directly compare humans and language models on thisfront: one measuring top-1 accuracy and the other measuring perplexity. In bothexperiments, we find humans to be consistently \emph{worse} than evenrelatively small language models like GPT3-Ada at next-token prediction.</description><author>Buck Shlegeris, Fabien Roger, Lawrence Chan, Euan McLean</author><pubDate>Mon, 15 Jul 2024 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11281v2</guid></item><item><title>Backpropagation through space, time, and the brain</title><link>http://arxiv.org/abs/2403.16933v2</link><description>How physical networks of neurons, bound by spatio-temporal localityconstraints, can perform efficient credit assignment, remains, to a largeextent, an open question. In machine learning, the answer is almost universallygiven by the error backpropagation algorithm, through both space and time.However, this algorithm is well-known to rely on biologically implausibleassumptions, in particular with respect to spatio-temporal (non-)locality.Alternative forward-propagation models such as real-time recurrent learningonly partially solve the locality problem, but only at the cost of scaling, dueto prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational frameworkfor fully local spatio-temporal credit assignment in physical, dynamicalnetworks of neurons. We start by defining an energy based on neuron-localmismatches, from which we derive both neuronal dynamics via stationarity andparameter dynamics via gradient descent. The resulting dynamics can beinterpreted as a real-time, biologically plausible approximation ofbackpropagation through space and time in deep cortical networks withcontinuous-time neuronal dynamics and continuously active, local synapticplasticity. In particular, GLE exploits the morphology of dendritic trees toenable more complex information storage and processing in single neurons, aswell as the ability of biological neurons to phase-shift their output rate withrespect to their membrane potential, which is essential in both directions ofinformation propagation. For the forward computation, it enables the mapping oftime-continuous inputs to neuronal space, effectively performing aspatio-temporal convolution. For the backward computation, it permits thetemporal inversion of feedback signals, which consequently approximate theadjoint variables necessary for useful parameter updates.</description><author>Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici</author><pubDate>Tue, 16 Jul 2024 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16933v2</guid></item><item><title>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title><link>http://arxiv.org/abs/2407.09879v2</link><description>Despite the remarkable success of LLMs in English, there is a significant gapin performance in non-English languages. In order to address this, we introducea novel recipe for creating a multilingual synthetic instruction tuningdataset, sPhinX, which is created by selectively translating instructionresponse pairs from English into 50 languages. We test the effectiveness ofsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small andMistral-7B and then evaluating them across a comprehensive suite ofmultilingual benchmarks that test reasoning, question answering, and readingcomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned withsPhinX perform better on an average by 4.2%pt and 5%pt respectively as comparedto the baselines. We also devise a strategy to incorporate N-shot examples ineach fine-tuning sample which further boosts the performance of these models by3%pt and 10%pt respectively. Additionally, sPhinX also outperforms othermultilingual instruction tuning datasets on the same benchmarks along withbeing sample efficient and diverse, thereby reducing dataset creation costs.Additionally, instruction tuning with sPhinX does not lead to regression onmost standard LLM benchmarks.</description><author>Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram</author><pubDate>Tue, 16 Jul 2024 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09879v2</guid></item><item><title>Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering</title><link>http://arxiv.org/abs/2407.11930v1</link><description>Long-form question answering (LFQA) aims to provide thorough and in-depthanswers to complex questions, enhancing comprehension. However, such detailedresponses are prone to hallucinations and factual inconsistencies, challengingtheir faithful evaluation. This work introduces HaluQuestQA, the firsthallucination dataset with localized error annotations for human-written andmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7kspan-level error annotations for five different error types by expertannotators, along with preference judgments. Using our collected data, wethoroughly analyze the shortcomings of long-form answers and find that theylack comprehensiveness and provide unhelpful references. We train an automaticfeedback model on this dataset that predicts error spans with incompleteinformation and provides associated explanations. Finally, we propose aprompt-based approach, Error-informed refinement, that uses signals from thelearned feedback model to refine generated answers, which we show reduceshallucination and improves answer quality. Furthermore, humans find answersgenerated by our approach comprehensive and highly prefer them (84%) over thebaseline answers.</description><author>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</author><pubDate>Tue, 16 Jul 2024 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11930v1</guid></item><item><title>Towards a Benchmark for Causal Business Process Reasoning with LLMs</title><link>http://arxiv.org/abs/2406.05506v2</link><description>Large Language Models (LLMs) are increasingly used for boostingorganizational efficiency and automating tasks. While not originally designedfor complex cognitive processes, recent efforts have further extended to employLLMs in activities such as reasoning, planning, and decision-making. Inbusiness processes, such abilities could be invaluable for leveraging on themassive corpora LLMs have been trained on for gaining deep understanding ofsuch processes. In this work, we plant the seeds for the development of abenchmark to assess the ability of LLMs to reason about causal and processperspectives of business operations. We refer to this view asCausally-augmented Business Processes (BP^C). The core of the benchmarkcomprises a set of BP^C related situations, a set of questions about thesesituations, and a set of deductive rules employed to systematically resolve theground truth answers to these questions. Also with the power of LLMs, the seedis then instantiated into a larger-scale set of domain-specific situations andquestions. Reasoning on BP^C is of crucial importance for process interventionsand process improvement. Our benchmark, accessible athttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possiblemodalities: testing the performance of any target LLM and training an LLM toadvance its capability to reason about BP^C.</description><author>Fabiana Fournier, Lior Limonad, Inna Skarbovsky</author><pubDate>Tue, 16 Jul 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05506v2</guid></item><item><title>Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</title><link>http://arxiv.org/abs/2407.10151v2</link><description>Multi-object tracking (MOT) endeavors to precisely estimate the positions andidentities of multiple objects over time. The prevailing approach,tracking-by-detection (TbD), first detects objects and then links detections,resulting in a simple yet effective method. However, contemporary detectors mayoccasionally miss some objects in certain frames, causing trackers to ceasetracking prematurely. To tackle this issue, we propose BUSCA, meaning `tosearch', a versatile framework compatible with any online TbD system, enhancingits ability to persistently track those objects missed by the detector,primarily due to occlusions. Remarkably, this is accomplished without modifyingpast tracking results or accessing future frames, i.e., in a fully onlinemanner. BUSCA generates proposals based on neighboring tracks, motion, andlearned tokens. Utilizing a decision Transformer that integrates multimodalvisual and spatiotemporal information, it addresses the object-proposalassociation as a multi-choice question-answering task. BUSCA is trainedindependently of the underlying tracker, solely on synthetic data, withoutrequiring fine-tuning. Through BUSCA, we showcase consistent performanceenhancements across five different trackers and establish a newstate-of-the-art baseline across three different benchmarks. Code available at:https://github.com/lorenzovaquero/BUSCA.</description><author>Lorenzo Vaquero, Yihong Xu, Xavier Alameda-Pineda, Victor M. Brea, Manuel Mucientes</author><pubDate>Tue, 16 Jul 2024 14:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10151v2</guid></item><item><title>Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer</title><link>http://arxiv.org/abs/2407.11677v1</link><description>Video-language alignment is a crucial multi-modal task that benefits variousdownstream applications, e.g., video-text retrieval and video questionanswering. Existing methods either utilize multi-modal information invideo-text pairs or apply global and local alignment techniques to promotealignment precision. However, these methods often fail to fully explore thespatio-temporal relationships among vision tokens within video and acrossdifferent video-text pairs. In this paper, we propose a novel Spatio-TemporalGraph Transformer module to uniformly learn spatial and temporal contexts forvideo-language alignment pre-training (dubbed STGT). Specifically, our STGTcombines spatio-temporal graph structure information with attention intransformer block, effectively utilizing the spatio-temporal contexts. In thisway, we can model the relationships between vision tokens, promoting video-textalignment precision for benefiting downstream tasks. In addition, we propose aself-similarity alignment loss to explore the inherent self-similarity in thevideo and text. With the initial optimization achieved by contrastive learning,it can further promote the alignment accuracy between video and text.Experimental results on challenging downstream tasks, including video-textretrieval and video question answering, verify the superior performance of ourmethod.</description><author>Shi-Xue Zhang, Hongfa Wang, Xiaobin Zhu, Weibo Gu, Tianjin Zhang, Chun Yang, Wei Liu, Xu-Cheng Yin</author><pubDate>Tue, 16 Jul 2024 12:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11677v1</guid></item><item><title>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</title><link>http://arxiv.org/abs/2402.03094v3</link><description>This paper studies the challenging cross-domain few-shot object detection(CD-FSOD), aiming to develop an accurate object detector for novel domains withminimal labeled examples. While transformer-based open-set detectors, such asDE-ViT, show promise in traditional few-shot object detection, theirgeneralization to CD-FSOD remains unclear: 1) can such open-set detectionmethods easily generalize to CD-FSOD? 2) If not, how can models be enhancedwhen facing huge domain gaps? To answer the first question, we employ measuresincluding style, inter-class variance (ICV), and indefinable boundaries (IB) tounderstand the domain gap. Based on these measures, we establish a newbenchmark named CD-FSOD to evaluate object detection methods, revealing thatmost of the current approaches fail to generalize across domains. Technically,we observe that the performance decline is associated with our proposedmeasures: style, ICV, and IB. Consequently, we propose several novel modules toaddress these issues. First, the learnable instance features align initialfixed instances with target categories, enhancing feature distinctiveness.Second, the instance reweighting module assigns higher importance tohigh-quality instances with slight IB. Third, the domain prompter encouragesfeatures resilient to different styles by synthesizing imaginary domainswithout altering semantic contents. These techniques collectively contribute tothe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),significantly improving upon the base DE-ViT. Experimental results validate theefficacy of our model.</description><author>Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang</author><pubDate>Tue, 16 Jul 2024 12:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03094v3</guid></item><item><title>A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting</title><link>http://arxiv.org/abs/2407.11638v1</link><description>Recently, Large Language Models (LLMs) have demonstrated great potential invarious data mining tasks, such as knowledge question answering, mathematicalreasoning, and commonsense reasoning. However, the reasoning capability of LLMson temporal event forecasting has been under-explored. To systematicallyinvestigate their abilities in temporal event forecasting, we conduct acomprehensive evaluation of LLM-based methods for temporal event forecasting.Due to the lack of a high-quality dataset that involves both graph and textualdata, we first construct a benchmark dataset, named MidEast-TE-mini. Based onthis dataset, we design a series of baseline methods, characterized by variousinput formats and retrieval augmented generation(RAG) modules. From extensiveexperiments, we find that directly integrating raw texts into the input of LLMsdoes not enhance zero-shot extrapolation performance. In contrast,incorporating raw texts in specific complex events and fine-tuning LLMssignificantly improves performance. Moreover, enhanced with retrieval modules,LLM can effectively capture temporal relational patterns hidden in historicalevents. Meanwhile, issues such as popularity bias and the long-tail problemstill persist in LLMs, particularly in the RAG-based method. These findings notonly deepen our understanding of LLM-based event forecasting methods but alsohighlight several promising research directions.We consider that thiscomprehensive evaluation, along with the identified research opportunities,will significantly contribute to future research on temporal event forecastingthrough LLMs.</description><author>He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua</author><pubDate>Tue, 16 Jul 2024 11:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11638v1</guid></item><item><title>RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models</title><link>http://arxiv.org/abs/2404.12065v2</link><description>The escalating challenge of misinformation, particularly in politicaldiscourse, requires advanced fact-checking solutions; this is even clearer inthe more complex scenario of multimodal claims. We tackle this issue using amultimodal large language model in conjunction with retrieval-augmentedgeneration (RAG), and introduce two novel reasoning techniques: Chain of RAG(CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims byextracting both textual and image content, retrieving external information, andreasoning subsequent questions to be answered based on prior evidence. Weachieve a weighted F1-score of 0.85, surpassing a baseline reasoning techniqueby 0.14 points. Human evaluation confirms that the vast majority of ourgenerated fact-check explanations contain all information from gold standarddata.</description><author>M. Abdul Khaliq, P. Chang, M. Ma, B. Pflugfelder, F. Miletić</author><pubDate>Thu, 11 Jul 2024 20:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12065v2</guid></item><item><title>Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison</title><link>http://arxiv.org/abs/2407.07840v2</link><description>Despite tremendous advancements, current state-of-the-art Vision-LanguageModels (VLMs) are still far from perfect. They tend to hallucinate and maygenerate biased responses. In such circumstances, having a way to assess thereliability of a given response generated by a VLM is quite useful. Existingmethods, such as estimating uncertainty using answer likelihoods orprompt-based confidence generation, often suffer from overconfidence. Othermethods use self-consistency comparison but are affected by confirmationbiases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparingthe consistency between the direct answer generated using the VLM's internalreasoning process, and the indirect answers obtained by decomposing thequestion into sub-questions and reasoning over the sub-answers produced by theVLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experimentsacross six vision-language tasks with three VLMs show \texttt{DeCC}'sreliability estimation achieves better correlation with task accuracy comparedto the existing methods.</description><author>Qian Yang, Weixiang Yan, Aishwarya Agrawal</author><pubDate>Thu, 11 Jul 2024 23:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07840v2</guid></item><item><title>Extracting Training Data from Document-Based VQA Models</title><link>http://arxiv.org/abs/2407.08707v1</link><description>Vision-Language Models (VLMs) have made remarkable progress in document-basedVisual Question Answering (i.e., responding to queries about the contents of aninput document provided as an image). In this work, we show these models canmemorize responses for training samples and regurgitate them even when therelevant visual information has been removed. This includes PersonalIdentifiable Information (PII) repeated once in the training set, indicatingthese models could divulge memorised sensitive information and therefore pose aprivacy risk. We quantitatively measure the extractability of information incontrolled experiments and differentiate between cases where it arises fromgeneralization capabilities or from memorization. We further investigate thefactors that influence memorization across multiple state-of-the-art models andpropose an effective heuristic countermeasure that empirically prevents theextractability of PII.</description><author>Francesco Pinto, Nathalie Rauschmayr, Florian Tramèr, Philip Torr, Federico Tombari</author><pubDate>Thu, 11 Jul 2024 17:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08707v1</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>$\textit{GeoHard}$: Towards Measuring Class-wise Hardness through Modelling Class Semantics</title><link>http://arxiv.org/abs/2407.12512v1</link><description>Recent advances in measuring hardness-wise properties of data guide languagemodels in sample selection within low-resource scenarios. However,class-specific properties are overlooked for task setup and learning. How willthese properties influence model learning and is it generalizable acrossdatasets? To answer this question, this work formally initiates the concept of$\textit{class-wise hardness}$. Experiments across eight natural languageunderstanding (NLU) datasets demonstrate a consistent hardness distributionacross learning paradigms, models, and human judgment. Subsequent experimentsunveil a notable challenge in measuring such class-wise hardness withinstance-level metrics in previous works. To address this, we propose$\textit{GeoHard}$ for class-wise hardness measurement by modeling classgeometry in the semantic embedding space. $\textit{GeoHard}$ surpassesinstance-level metrics by over 59 percent on $\textit{Pearson}$'s correlationon measuring class-wise hardness. Our analysis theoretically and empiricallyunderscores the generality of $\textit{GeoHard}$ as a fresh perspective on datadiagnosis. Additionally, we showcase how understanding class-wise hardness canpractically aid in improving task learning.</description><author>Fengyu Cai, Xinran Zhao, Hongming Zhang, Iryna Gurevych, Heinz Koeppl</author><pubDate>Wed, 17 Jul 2024 11:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12512v1</guid></item><item><title>Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?</title><link>http://arxiv.org/abs/2407.08922v1</link><description>With the rapid development of artificial intelligence (AI), large languagemodels (LLMs) such as GPT-4 have garnered significant attention in thescientific community, demonstrating great potential in advancing scientificdiscovery. This progress raises a critical question: are these LLMswell-aligned with real-world physicochemical principles? Current evaluationstrategies largely emphasize fact-based knowledge, such as material propertyprediction or name recognition, but they often lack an understanding offundamental physicochemical mechanisms that require logical reasoning. Tobridge this gap, our study developed a benchmark consisting of 775multiple-choice questions focusing on the mechanisms of gold nanoparticlesynthesis. By reflecting on existing evaluation metrics, we question whether adirect true-or-false assessment merely suggests conjecture. Hence, we propose anovel evaluation metric, the confidence-based score (c-score), which probes theoutput logits to derive the precise probability for the correct answer. Basedon extensive experiments, our results show that in the context of goldnanoparticle synthesis, LLMs understand the underlying physicochemicalmechanisms rather than relying on conjecture. This study underscores thepotential of LLMs to grasp intrinsic scientific mechanisms and sets the stagefor developing more reliable and effective AI tools across various scientificdomains.</description><author>Yingming Pu, Liping Huang, Tao Lin, Hongyu Chen</author><pubDate>Fri, 12 Jul 2024 02:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08922v1</guid></item><item><title>SHMamba: Structured Hyperbolic State Space Model for Audio-Visual Question Answering</title><link>http://arxiv.org/abs/2406.09833v3</link><description>The Audio-Visual Question Answering (AVQA) task holds significant potentialfor applications. Compared to traditional unimodal approaches, the multi-modalinput of AVQA makes feature extraction and fusion processes more challenging.Euclidean space is difficult to effectively represent multi-dimensionalrelationships of data. Especially when extracting and processing data with atree structure or hierarchical structure, Euclidean space is not suitable as anembedding space. Additionally, the self-attention mechanism in Transformers iseffective in capturing the dynamic relationships between elements in asequence. However, the self-attention mechanism's limitations in windowmodeling and quadratic computational complexity reduce its effectiveness inmodeling long sequences. To address these limitations, we propose SHMamba:Structured Hyperbolic State Space Model to integrate the advantages ofhyperbolic geometry and state space models. Specifically, SHMamba leverages theintrinsic properties of hyperbolic space to represent hierarchical structuresand complex relationships in audio-visual data. Meanwhile, the state spacemodel captures dynamic changes over time by globally modeling the entiresequence. Furthermore, we introduce an adaptive curvature hyperbolic alignmentmodule and a cross fusion block to enhance the understanding of hierarchicalstructures and the dynamic exchange of cross-modal information, respectively.Extensive experiments demonstrate that SHMamba outperforms previous methodswith fewer parameters and computational costs. Our learnable parameters arereduced by 78.12\%, while the average performance improves by 2.53\%.Experiments show that our method demonstrates superiority among all currentmajor methods and is more suitable for practical application scenarios.</description><author>Zhe Yang, Wenrui Li, Guanghui Cheng</author><pubDate>Tue, 16 Jul 2024 08:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09833v3</guid></item><item><title>Cross-lingual QA: A Key to Unlocking In-context Cross-lingual Performance</title><link>http://arxiv.org/abs/2305.15233v3</link><description>Multilingual large language models (MLLMs) have demonstrated significantcross-lingual capabilities through in-context learning. Existing approachestypically construct monolingual in-context examples, either in the source ortarget language. However, translating entire in-context examples into thetarget language might compromise contextual integrity and be costly in the caseof long-context passages. To address this, we introduce Cross-lingual QA, across-lingual prompting method that translates only the question and answerparts, thus reducing translation costs. Experiments on four typologicallydiverse multilingual benchmarks show that Cross-lingual QA promptingeffectively stimulates models to elicit their cross-lingual knowledge,outperforming prior monolingual prompting approaches. Furthermore, we show thatprompting open-source MLLMs with cross-lingual in-context examples enhancesperformance as the model scale increases.</description><author>Sunkyoung Kim, Dayeon Ki, Yireun Kim, Jinsik Lee</author><pubDate>Tue, 16 Jul 2024 08:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15233v3</guid></item><item><title>MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline</title><link>http://arxiv.org/abs/2407.12508v1</link><description>The rapid expansion of multimedia content has made accurately retrievingrelevant videos from large collections increasingly challenging. Recentadvancements in text-video retrieval have focused on cross-modal interactions,large-scale foundation model training, and probabilistic modeling, yet oftenneglect the crucial user perspective, leading to discrepancies between userqueries and the content retrieved. To address this, we introduce MERLIN(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,training-free pipeline that leverages Large Language Models (LLMs) foriterative feedback learning. MERLIN refines query embeddings from a userperspective, enhancing alignment between queries and video content through adynamic question answering process. Experimental results on datasets likeMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improvesRecall@1, outperforming existing systems and confirming the benefits ofintegrating LLMs into multimodal retrieval systems for more responsive andcontext-aware multimedia retrieval.</description><author>Donghoon Han, Eunhwan Park, Gisang Lee, Adam Lee, Nojun Kwak</author><pubDate>Wed, 17 Jul 2024 11:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12508v1</guid></item><item><title>Continual Learning for Temporal-Sensitive Question Answering</title><link>http://arxiv.org/abs/2407.12470v1</link><description>In this study, we explore an emerging research area of Continual Learning forTemporal Sensitive Question Answering (CLTSQA). Previous research has primarilyfocused on Temporal Sensitive Question Answering (TSQA), often overlooking theunpredictable nature of future events. In real-world applications, it's crucialfor models to continually acquire knowledge over time, rather than relying on astatic, complete dataset. Our paper investigates strategies that enable modelsto adapt to the ever-evolving information landscape, thereby addressing thechallenges inherent in CLTSQA. To support our research, we first create a noveldataset, divided into five subsets, designed specifically for various stages ofcontinual learning. We then propose a training framework for CLTSQA thatintegrates temporal memory replay and temporal contrastive learning. Ourexperimental results highlight two significant insights: First, the CLTSQA taskintroduces unique challenges for existing models. Second, our proposedframework effectively navigates these challenges, resulting in improvedperformance.</description><author>Wanqi Yang, Yunqiu Xu, Yanda Li, Kunze Wang, Binbin Huang, Ling Chen</author><pubDate>Wed, 17 Jul 2024 10:47:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12470v1</guid></item><item><title>Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions</title><link>http://arxiv.org/abs/2407.12468v2</link><description>Search engines have traditionally served as primary tools for informationseeking. However, the new Large Language Models (LLMs) have recentlydemonstrated remarkable capabilities in multiple tasks and, specifically, theiradoption as question answering systems is becoming increasingly prevalent. Itis expected that LLM-based conversational systems and traditional web engineswill continue to coexist in the future, supporting end users in various ways.But there is a need for more scientific research on the effectiveness of bothtypes of systems in facilitating accurate information seeking. In this study,we focus on their merits in answering health questions. We conducted anextensive study comparing different web search engines, LLMs andretrieval-augmented (RAG) approaches. Our research reveals intriguingconclusions. For example, we observed that the quality of webpages potentiallyresponding to a health question does not decline as we navigate further downthe ranked lists. However, according to our evaluation, web engines are lessaccurate than LLMs in finding correct answers to health questions. On the otherhand, LLMs are quite sensitive to the input prompts, and we also found out thatRAG leads to highly effective information seeking methods.</description><author>Marcos Fernández-Pichel, Juan C. Pichel, David E. Losada</author><pubDate>Thu, 18 Jul 2024 10:11:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12468v2</guid></item><item><title>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</title><link>http://arxiv.org/abs/2402.10373v3</link><description>Large Language Models (LLMs) have demonstrated remarkable versatility inrecent years, offering potential applications across specialized domains suchas healthcare and medicine. Despite the availability of various open-sourceLLMs tailored for health contexts, adapting general-purpose LLMs to the medicaldomain presents significant challenges. In this paper, we introduce BioMistral,an open-source LLM tailored for the biomedical domain, utilizing Mistral as itsfoundation model and further pre-trained on PubMed Central. We conduct acomprehensive evaluation of BioMistral on a benchmark comprising 10 establishedmedical question-answering (QA) tasks in English. We also explore lightweightmodels obtained through quantization and model merging approaches. Our resultsdemonstrate BioMistral's superior performance compared to existing open-sourcemedical models and its competitive edge against proprietary counterparts.Finally, to address the limited availability of data beyond English and toassess the multilingual generalization of medical LLMs, we automaticallytranslated and evaluated this benchmark into 7 other languages. This marks thefirst large-scale multilingual evaluation of LLMs in the medical domain.Datasets, multilingual evaluation benchmarks, scripts, and all the modelsobtained during our experiments are freely released.</description><author>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour</author><pubDate>Wed, 17 Jul 2024 09:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10373v3</guid></item><item><title>MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs</title><link>http://arxiv.org/abs/2406.07243v3</link><description>Generative large language models (LLMs) have been shown to exhibit harmfulbiases and stereotypes. While safety fine-tuning typically takes place inEnglish, if at all, these models are being used by speakers of many differentlanguages. There is existing evidence that the performance of these models isinconsistent across languages and that they discriminate based on demographicfactors of the user. Motivated by this, we investigate whether the socialstereotypes exhibited by LLMs differ as a function of the language used toprompt them, while controlling for cultural differences and task accuracy. Tothis end, we present MBBQ (Multilingual Bias Benchmark for Question-answering),a carefully curated version of the English BBQ dataset extended to Dutch,Spanish, and Turkish, which measures stereotypes commonly held across theselanguages. We further complement MBBQ with a parallel control dataset tomeasure task performance on the question-answering task independently of bias.Our results based on several open-source and proprietary LLMs confirm that somenon-English languages suffer from bias more than English, even when controllingfor cultural shifts. Moreover, we observe significant cross-lingual differencesin bias behaviour for all except the most accurate models. With the release ofMBBQ, we hope to encourage further research on bias in multilingual settings.The dataset and code are available at https://github.com/Veranep/MBBQ.</description><author>Vera Neplenbroek, Arianna Bisazza, Raquel Fernández</author><pubDate>Wed, 17 Jul 2024 08:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07243v3</guid></item><item><title>TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish</title><link>http://arxiv.org/abs/2407.12402v1</link><description>Multiple choice question answering tasks evaluate the reasoning,comprehension, and mathematical abilities of Large Language Models (LLMs).While existing benchmarks employ automatic translation for multilingualevaluation, this approach is error-prone and potentially introduces culturallybiased questions, especially in social sciences. We introduce the firstmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'understanding of the Turkish language. TurkishMMLU includes over 10,000questions, covering 9 different subjects from Turkish high-school educationcurricula. These questions are written by curriculum experts, suitable for thehigh-school curricula in Turkey, covering subjects ranging from naturalsciences and math questions to more culturally representative topics such asTurkish Literature and the history of the Turkish Republic. We evaluate over 20LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)models. We provide an extensive evaluation, including zero-shot and few-shotevaluation of LLMs, chain-of-thought reasoning, and question difficultyanalysis along with model performance. We provide an in-depth analysis of theTurkish capabilities and limitations of current LLMs to provide insights forfuture LLMs for the Turkish language. We publicly release our code for thedataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.</description><author>Arda Yüksel, Abdullatif Köksal, Lütfi Kerem Şenel, Anna Korhonen, Hinrich Schütze</author><pubDate>Wed, 17 Jul 2024 08:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12402v1</guid></item><item><title>One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning</title><link>http://arxiv.org/abs/2407.09011v1</link><description>This paper presents a novel and comprehensive solution to enhance both therobustness and efficiency of question answering (QA) systems through supervisedcontrastive learning (SCL). Training a high-performance QA system has becomestraightforward with pre-trained language models, requiring only a small amountof data and simple fine-tuning. However, despite recent advances, existing QAsystems still exhibit significant deficiencies in functionality and trainingefficiency. We address the functionality issue by defining four key tasks: userinput intent classification, out-of-domain input detection, new intentdiscovery, and continual learning. We then leverage a unified SCL-basedrepresentation learning method to efficiently build an intra-class compact andinter-class scattered feature space, facilitating both known intentclassification and unknown intent detection and discovery. Consequently, withminimal additional tuning on downstream tasks, our approach significantlyimproves model efficiency and achieves new state-of-the-art performance acrossall tasks.</description><author>Bo Wang, Tsunenori Mine</author><pubDate>Fri, 12 Jul 2024 06:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09011v1</guid></item><item><title>DriveLM: Driving with Graph Visual Question Answering</title><link>http://arxiv.org/abs/2312.14150v2</link><description>We study how vision-language models (VLMs) trained on web-scale data can beintegrated into end-to-end driving systems to boost generalization and enableinteractivity with human users. While recent approaches adapt VLMs to drivingvia single-round visual question answering (VQA), human drivers reason aboutdecisions in multiple steps. Starting from the localization of key objects,humans estimate object interactions before taking actions. The key insight isthat with our proposed task, Graph VQA, where we model graph-structuredreasoning through perception, prediction and planning question-answer pairs, weobtain a suitable proxy task to mimic the human reasoning process. Weinstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and proposea VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQAand end-to-end driving. The experiments demonstrate that Graph VQA provides asimple, principled framework for reasoning about a driving scene, andDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agentbaseline performs end-to-end autonomous driving competitively in comparison tostate-of-the-art driving-specific architectures. Notably, its benefits arepronounced when it is evaluated zero-shot on unseen objects or sensorconfigurations. We hope this work can be the starting point to shed new lighton how to apply VLMs for autonomous driving. To facilitate future research, allcode, data, and models are available to the public.</description><author>Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li</author><pubDate>Wed, 17 Jul 2024 07:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14150v2</guid></item><item><title>A Taxonomy for Data Contamination in Large Language Models</title><link>http://arxiv.org/abs/2407.08716v1</link><description>Large language models pretrained on extensive web corpora demonstrateremarkable performance across a wide range of downstream tasks. However, agrowing concern is data contamination, where evaluation datasets may becontained in the pretraining corpus, inflating model performance.Decontamination, the process of detecting and removing such data, is apotential solution; yet these contaminants may originate from altered versionsof the test set, evading detection during decontamination. How different typesof contamination impact the performance of language models on downstream tasksis not fully understood. We present a taxonomy that categorizes the varioustypes of contamination encountered by LLMs during the pretraining phase andidentify which types pose the highest risk. We analyze the impact ofcontamination on two key NLP tasks -- summarization and question answering --revealing how different types of contamination influence task performanceduring evaluation.</description><author>Medha Palavalli, Amanda Bertsch, Matthew R. Gormley</author><pubDate>Thu, 11 Jul 2024 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08716v1</guid></item><item><title>Conversational Query Reformulation with the Guidance of Retrieved Documents</title><link>http://arxiv.org/abs/2407.12363v1</link><description>Conversational search seeks to retrieve relevant passages for the givenquestions in Conversational QA (ConvQA). Questions in ConvQA face challengessuch as omissions and coreferences, making it difficult to obtain desiredsearch results. Conversational Query Reformulation (CQR) transforms thesecurrent queries into de-contextualized forms to resolve these issues. However,existing CQR methods focus on rewriting human-friendly queries, which may notalways yield optimal search results for the retriever. To overcome thischallenge, we introduce GuideCQR, a framework that utilizes guided documents torefine queries, ensuring that they are optimal for retrievers. Specifically, weaugment keywords, generate expected answers from the re-ranked documents, andunify them with the filtering process. Experimental results show that queriesenhanced by guided documents outperform previous CQR methods. Especially,GuideCQR surpasses the performance of Large Language Model (LLM) prompt-poweredapproaches and demonstrates the importance of the guided documents informulating retriever-friendly queries across diverse setups.</description><author>Jeonghyun Park, Hwanhee Lee</author><pubDate>Wed, 17 Jul 2024 07:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12363v1</guid></item><item><title>Fully Authentic Visual Question Answering Dataset from Online Communities</title><link>http://arxiv.org/abs/2311.15562v4</link><description>Visual Question Answering (VQA) entails answering questions about images. Weintroduce the first VQA dataset in which all contents originate from anauthentic use case. Sourced from online question answering community forums, wecall it VQAonline. We characterize this dataset and how it relates to eightmainstream VQA datasets. Observing that answers in our dataset tend to be muchlonger (i.e., a mean of 173 words) and so incompatible with standard VQAevaluation metrics, we instead utilize popular metrics for longer textevaluation for evaluating six state-of-the-art VQA models on VQAonline andreport where they struggle most. Finally, we analyze which evaluation metricsalign best with human judgments. To facilitate future extensions, wepublicly-share the dataset at: https://vqaonline.github.io/.</description><author>Chongyan Chen, Mengchen Liu, Noel Codella, Yunsheng Li, Lu Yuan, Danna Gurari</author><pubDate>Wed, 17 Jul 2024 07:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15562v4</guid></item><item><title>ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities</title><link>http://arxiv.org/abs/2407.01525v3</link><description>Although great progress has been made in 3D visual grounding, current modelsstill rely on explicit textual descriptions for grounding and lack the abilityto reason human intentions from implicit instructions. We propose a new taskcalled 3D reasoning grounding and introduce a new benchmark ScanReason whichprovides over 10K question-answer-location pairs from five reasoning types thatrequire the synerization of reasoning and grounding. We further design ourapproach, ReGround3D, composed of the visual-centric reasoning module empoweredby Multi-modal Large Language Model (MLLM) and the 3D grounding module toobtain accurate object locations by looking back to the enhanced geometry andfine-grained details from the 3D scenes. A chain-of-grounding mechanism isproposed to further boost the performance with interleaved reasoning andgrounding steps during inference. Extensive experiments on the proposedbenchmark validate the effectiveness of our proposed approach.</description><author>Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu</author><pubDate>Wed, 17 Jul 2024 07:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01525v3</guid></item><item><title>Multimodal Reranking for Knowledge-Intensive Visual Question Answering</title><link>http://arxiv.org/abs/2407.12277v1</link><description>Knowledge-intensive visual question answering requires models to effectivelyuse external knowledge to help answer visual questions. A typical pipelineincludes a knowledge retriever and an answer generator. However, a retrieverthat utilizes local information, such as an image patch, may not providereliable question-candidate relevance scores. Besides, the two-towerarchitecture also limits the relevance score modeling of a retriever to selecttop candidates for answer generator reasoning. In this paper, we introduce anadditional module, a multi-modal reranker, to improve the ranking quality ofknowledge candidates for answer generation. Our reranking module takesmulti-modal information from both candidates and questions and performscross-item interaction for better relevance score modeling. Experiments onOK-VQA and A-OKVQA show that multi-modal reranker from distant supervisionprovides consistent improvements. We also find a training-testing discrepancywith reranking in answer generation, where performance improves if trainingknowledge candidates are similar to or noisier than those used in testing.</description><author>Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann, Michael Bendersky</author><pubDate>Wed, 17 Jul 2024 02:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12277v1</guid></item><item><title>Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</title><link>http://arxiv.org/abs/2407.11282v2</link><description>Large Language Models (LLMs) are employed across various high-stakes domains,where the reliability of their outputs is crucial. One commonly used method toassess the reliability of LLMs' responses is uncertainty estimation, whichgauges the likelihood of their answers being correct. While many studies focuson improving the accuracy of uncertainty estimations for LLMs, our researchinvestigates the fragility of uncertainty estimation and explores potentialattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,when activated by a specific trigger in the input, manipulates the model'suncertainty without affecting the final output. Specifically, the proposedbackdoor attack method can alter an LLM's output probability distribution,causing the probability distribution to converge towards an attacker-predefineddistribution while ensuring that the top-1 prediction remains unchanged. Ourexperimental results demonstrate that this attack effectively undermines themodel's self-evaluation reliability in multiple-choice questions. For instance,we achieved a 100 attack success rate (ASR) across three different triggeringstrategies in four models. Further, we investigate whether this manipulationgeneralizes across different prompts and domains. This work highlights asignificant threat to the reliability of LLMs and underscores the need forfuture defenses against such attacks. The code is available athttps://github.com/qcznlp/uncertainty_attack.</description><author>Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang</author><pubDate>Wed, 17 Jul 2024 02:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11282v2</guid></item><item><title>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</title><link>http://arxiv.org/abs/2403.09635v2</link><description>In spite of their huge success, transformer models remain difficult to scalein depth. In this work, we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients, rank collapse, and instabilityassociated with high attention scores. We also propose DeepScaleLM, aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model, enabling the training of very deep models with 1000layers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling, SpeechTranslation, and Image Classification, across encoder-only, decoder-only andencoder-decoder variants, for both Pre-LN and Post-LN transformers, formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for Image Classification.</description><author>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee</author><pubDate>Thu, 18 Jul 2024 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09635v2</guid></item><item><title>Visual Haystacks: Answering Harder Questions About Sets of Images</title><link>http://arxiv.org/abs/2407.13766v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have made significantprogress in the field of single-image visual question answering. However, thesemodels face substantial challenges when tasked with queries that span extensivecollections of images, similar to real-world scenarios like searching throughlarge photo albums, finding specific information across the internet, ormonitoring environmental changes through satellite imagery. This paper exploresthe task of Multi-Image Visual Question Answering (MIQA): given a large set ofimages and a natural language query, the task is to generate a relevant andgrounded response. We propose a new public benchmark, dubbed "Visual Haystacks(VHs)," specifically designed to evaluate LMMs' capabilities in visualretrieval and reasoning over sets of unrelated images, where we performcomprehensive evaluations demonstrating that even robust closed-source modelsstruggle significantly. Towards addressing these shortcomings, we introduceMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QAframework tailored for LMMs that confronts the challenges of MIQA with markedefficiency and accuracy improvements over baseline methods. Our evaluationshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHsbenchmark and offers up to 3.4x improvements in efficiency over text-focusedmulti-stage approaches.</description><author>Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, David M. Chan</author><pubDate>Thu, 18 Jul 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13766v1</guid></item><item><title>Scientific QA System with Verifiable Answers</title><link>http://arxiv.org/abs/2407.11485v1</link><description>In this paper, we introduce the VerifAI project, a pioneering open-sourcescientific question-answering system, designed to provide answers that are notonly referenced but also automatically vetted and verifiable. The components ofthe system are (1) an Information Retrieval system combining semantic andlexical search techniques over scientific papers (PubMed), (2) aRetrieval-Augmented Generation (RAG) module using fine-tuned generative model(Mistral 7B) and retrieved articles to generate claims with references to thearticles from which it was derived, and (3) a Verification engine, based on afine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference taskusing SciFACT dataset. The verification engine cross-checks the generated claimand the article from which the claim was derived, verifying whether there mayhave been any hallucinations in generating the claim. By leveraging theInformation Retrieval and RAG modules, Verif.ai excels in generating factualinformation from a vast array of scientific sources. At the same time, theVerification engine rigorously double-checks this output, ensuring its accuracyand reliability. This dual-stage process plays a crucial role in acquiring andconfirming factual information, significantly enhancing the informationlandscape. Our methodology could significantly enhance scientists'productivity, concurrently fostering trust in applying generative languagemodels within scientific domains, where hallucinations and misinformation areunacceptable.</description><author>Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano, Nikola Milošević</author><pubDate>Tue, 16 Jul 2024 08:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11485v1</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v2</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Thu, 18 Jul 2024 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v2</guid></item><item><title>Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following</title><link>http://arxiv.org/abs/2406.02774v2</link><description>Training gaze following models requires a large number of images with gazetarget coordinates annotated by human annotators, which is a laborious andinherently ambiguous process. We propose the first semi-supervised method forgaze following by introducing two novel priors to the task. We obtain the firstprior using a large pretrained Visual Question Answering (VQA) model, where wecompute Grad-CAM heatmaps by `prompting' the VQA model with a gaze followingquestion. These heatmaps can be noisy and not suited for use in training. Theneed to refine these noisy annotations leads us to incorporate a second prior.We utilize a diffusion model trained on limited human annotations and modifythe reverse sampling process to refine the Grad-CAM heatmaps. By tuning thediffusion process we achieve a trade-off between the human annotation prior andthe VQA heatmap prior, which retains the useful VQA prior information whileexhibiting similar properties to the training data distribution. Our methodoutperforms simple pseudo-annotation generation baselines on the GazeFollowimage dataset. More importantly, our pseudo-annotation strategy, applied to awidely used supervised gaze following model (VAT), reduces the annotation needby 50%. Our method also performs the best on the VideoAttentionTarget dataset.</description><author>Qiaomu Miao, Alexandros Graikos, Jingwei Zhang, Sounak Mondal, Minh Hoai, Dimitris Samaras</author><pubDate>Thu, 18 Jul 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02774v2</guid></item><item><title>Reasoning with Large Language Models, a Survey</title><link>http://arxiv.org/abs/2407.11511v1</link><description>Scaling up language models to billions of parameters has opened uppossibilities for in-context learning, allowing instruction tuning and few-shotlearning on tasks that the model was not specifically trained for. This hasachieved breakthrough performance on language tasks such as translation,summarization, and question-answering. Furthermore, in addition to theseassociative "System 1" tasks, recent advances in Chain-of-thought promptlearning have demonstrated strong "System 2" reasoning abilities, answering aquestion in the field of artificial general intelligence whether LLMs canreason. The field started with the question whether LLMs can solve grade schoolmath word problems. This paper reviews the rapidly expanding field ofprompt-based reasoning with LLMs. Our taxonomy identifies different ways togenerate, evaluate, and control multi-step reasoning. We provide an in-depthcoverage of core approaches and open problems, and we propose a research agendafor the near future. Finally, we highlight the relation between reasoning andprompt-based learning, and we discuss the relation between reasoning,sequential decision processes, and reinforcement learning. We find thatself-improvement, self-reflection, and some metacognitive abilities of thereasoning processes are possible through the judicious use of prompts. Trueself-improvement and self-reasoning, to go from reasoning with LLMs toreasoning by LLMs, remains future work.</description><author>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</author><pubDate>Tue, 16 Jul 2024 08:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11511v1</guid></item><item><title>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2402.13950v3</link><description>Large language models (LLMs) have been shown to perform better when asked toreason step-by-step before answering a question. However, it is unclear to whatdegree the model's final answer is faithful to the stated reasoning steps. Inthis paper, we perform a causal mediation analysis on twelve LLMs to examinehow intermediate reasoning steps generated by the LLM influence the finaloutcome and find that LLMs do not reliably use their intermediate reasoningsteps when generating an answer. To address this issue, we introduce FRODO, aframework to tailor small-sized LMs to generate correct reasoning steps androbustly reason over these steps. FRODO consists of an inference module thatlearns to generate correct reasoning steps using an implicit causal rewardfunction and a reasoning module that learns to faithfully reason over theseintermediate inferences using a counterfactual and causal preference objective.Our experiments show that FRODO significantly outperforms four competitivebaselines. Furthermore, FRODO improves the robustness and generalizationability of the reasoning LM, yielding higher performance on out-of-distributiontest sets. Finally, we find that FRODO's rationales are more faithful to itsfinal answer predictions than standard supervised fine-tuning.</description><author>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</author><pubDate>Thu, 18 Jul 2024 13:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13950v3</guid></item><item><title>BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models</title><link>http://arxiv.org/abs/2407.13442v1</link><description>Vision language models (VLMs) perceive the world through a combination of avisual encoder and a large language model (LLM). The visual encoder,pre-trained on large-scale vision-text datasets, provides zero-shotgeneralization to visual data, and the LLM endows its high reasoning ability toVLMs. It leads VLMs to achieve high performance on wide benchmarks withoutfine-tuning, exhibiting zero or few-shot capability. However, recent studiesshow that VLMs are vulnerable to hallucination. This undesirable behaviordegrades reliability and credibility, thereby making users unable to fullytrust the output from VLMs. To enhance trustworthiness and better tackle thehallucination of VLMs, we curate a new evaluation dataset, called theBEfore-AFter hallucination dataset (BEAF), and introduce new metrics: TrueUnderstanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID).Unlike prior works that focus only on constructing questions and answers, thekey idea of our benchmark is to manipulate visual scene information by imageediting models and to design the metrics based on scene changes. This allows usto clearly assess whether VLMs correctly understand a given scene by observingthe ability to perceive changes. We also visualize image-wise objectrelationship by virtue of our two-axis view: vision and text. Upon evaluatingVLMs with our dataset, we observed that our metrics reveal different aspects ofVLM hallucination that have not been reported before. Project page:\url{https://beafbench.github.io/}</description><author>Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Tae-Hyun Oh</author><pubDate>Thu, 18 Jul 2024 12:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13442v1</guid></item><item><title>Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ</title><link>http://arxiv.org/abs/2403.03814v2</link><description>Large language models (LLMs) need to serve everyone, including a globalmajority of non-English speakers. However, most LLMs today, and open LLMs inparticular, are often intended for use in just English (e.g. Llama2, Mistral)or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recentresearch shows that, despite limits in their intended use, people prompt LLMsin many different languages. Therefore, in this paper, we investigate the basicmultilingual capabilities of state-of-the-art open LLMs beyond their intendeduse. For this purpose, we introduce MultiQ, a new silver standard benchmark forbasic open-ended question answering with 27.4k test questions across atypologically diverse set of 137 languages. With MultiQ, we evaluate languagefidelity, i.e. whether models respond in the prompted language, and questionanswering accuracy. All LLMs we test respond faithfully and/or accurately forat least some languages beyond their intended use. Most models are moreaccurate when they respond faithfully. However, differences across models arelarge, and there is a long tail of languages where models are neither accuratenor faithful. We explore differences in tokenization as a potential explanationfor our findings, identifying possible correlations that warrant furtherinvestigation.</description><author>Carolin Holtermann, Paul Röttger, Timm Dill, Anne Lauscher</author><pubDate>Thu, 18 Jul 2024 07:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03814v2</guid></item><item><title>Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach</title><link>http://arxiv.org/abs/2407.13101v1</link><description>Multi-hop question answering is a challenging task with distinct industrialrelevance, and Retrieval-Augmented Generation (RAG) methods based on largelanguage models (LLMs) have become a popular approach to tackle this task.Owing to the potential inability to retrieve all necessary information in asingle iteration, a series of iterative RAG methods has been recentlydeveloped, showing significant performance improvements. However, existingmethods still face two critical challenges: context overload resulting frommultiple rounds of retrieval, and over-planning and repetitive planning due tothe lack of a recorded retrieval trajectory. In this paper, we propose a noveliterative RAG method called ReSP, equipped with a dual-function summarizer.This summarizer compresses information from retrieved documents, targeting boththe overarching question and the current sub-question concurrently.Experimental results on the multi-hop question-answering datasets HotpotQA and2WikiMultihopQA demonstrate that our method significantly outperforms thestate-of-the-art, and exhibits excellent robustness concerning context length.</description><author>Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang</author><pubDate>Thu, 18 Jul 2024 02:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13101v1</guid></item><item><title>Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses</title><link>http://arxiv.org/abs/2407.09690v2</link><description>We revisit the problem of federated learning (FL) with private data frompeople who do not trust the server or other silos/clients. In this context,every silo (e.g. hospital) has data from several people (e.g. patients) andneeds to protect the privacy of each person's data (e.g. health records), evenif the server and/or other silos try to uncover this data. Inter-SiloRecord-Level Differential Privacy (ISRL-DP) prevents each silo's data frombeing leaked, by requiring that silo i's communications satisfy item-leveldifferential privacy. Prior work arXiv:2106.09779 characterized the optimalexcess risk bounds for ISRL-DP algorithms with homogeneous (i.i.d.) silo dataand convex loss functions. However, two important questions were left open: (1)Can the same excess risk bounds be achieved with heterogeneous (non-i.i.d.)silo data? (2) Can the optimal risk bounds be achieved with fewer communicationrounds? In this paper, we give positive answers to both questions. We providenovel ISRL-DP FL algorithms that achieve the optimal excess risk bounds in thepresence of heterogeneous silo data. Moreover, our algorithms are morecommunication-efficient than the prior state-of-the-art. For smooth lossfunctions, our algorithm achieves the optimal excess risk bound and hascommunication complexity that matches the non-private lower bound.Additionally, our algorithms are more computationally efficient than theprevious state-of-the-art.</description><author>Changyu Gao, Andrew Lowy, Xingyu Zhou, Stephen J. Wright</author><pubDate>Wed, 17 Jul 2024 23:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09690v2</guid></item><item><title>The Linear Representation Hypothesis and the Geometry of Large Language Models</title><link>http://arxiv.org/abs/2311.03658v2</link><description>Informally, the 'linear representation hypothesis' is the idea thathigh-level concepts are represented linearly as directions in somerepresentation space. In this paper, we address two closely related questions:What does "linear representation" actually mean? And, how do we make sense ofgeometric notions (e.g., cosine similarity or projection) in the representationspace? To answer these, we use the language of counterfactuals to give twoformalizations of "linear representation", one in the output (word)representation space, and one in the input (sentence) space. We then provethese connect to linear probing and model steering, respectively. To make senseof geometric notions, we use the formalization to identify a particular(non-Euclidean) inner product that respects language structure in a sense wemake precise. Using this causal inner product, we show how to unify all notionsof linear representation. In particular, this allows the construction of probesand steering vectors using counterfactual pairs. Experiments with LLaMA-2demonstrate the existence of linear representations of concepts, the connectionto interpretation and control, and the fundamental role of the choice of innerproduct.</description><author>Kiho Park, Yo Joong Choe, Victor Veitch</author><pubDate>Wed, 17 Jul 2024 22:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03658v2</guid></item><item><title>Establishing Knowledge Preference in Language Models</title><link>http://arxiv.org/abs/2407.13048v1</link><description>Language models are known to encode a great amount of factual knowledgethrough pretraining. However, such knowledge might be insufficient to cater touser requests, requiring the model to integrate external knowledge sources andadhere to user-provided specifications. When answering questions about ongoingevents, the model should use recent news articles to update its response; whenasked to provide recommendations, the model should prioritize userspecifications over retrieved product reviews; when some facts are edited inthe model, the updated facts should override all prior knowledge learned by themodel even if they are conflicting. In all of the cases above, the model facesa decision between its own parametric knowledge, (retrieved) contextualknowledge, and user instruction knowledge. In this paper, we (1) unify suchsettings into the problem of knowledge preference and define a three-levelpreference hierarchy over these knowledge sources; (2) compile a collection ofexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings(with/without user specifications, with/without context documents) tosystematically evaluate how well models obey the intended knowledge preference;and (3) propose a dataset synthesis method that composes diversequestion-answer pairs with user assumptions and related context to directlyfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a7B model, fine-tuned on only a few thousand examples automatically generated byour proposed method, effectively achieves superior performance (more than 18%improvement across all evaluation benchmarks) in adhering to the desiredknowledge preference hierarchy.</description><author>Sizhe Zhou, Sha Li, Yu Meng, Yizhu Jiao, Heng Ji, Jiawei Han</author><pubDate>Wed, 17 Jul 2024 23:16:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13048v1</guid></item><item><title>Language models show human-like content effects on reasoning tasks</title><link>http://arxiv.org/abs/2207.07051v4</link><description>Reasoning is a key ability for an intelligent system. Large language models(LMs) achieve above-chance performance on abstract reasoning tasks, but exhibitmany imperfections. However, human abstract reasoning is also imperfect. Forexample, human reasoning is affected by our real-world knowledge and beliefs,and shows notable "content effects"; humans reason more reliably when thesemantic content of a problem supports the correct logical inferences. Thesecontent-entangled reasoning patterns play a central role in debates about thefundamental nature of human intelligence. Here, we investigate whether languagemodels $\unicode{x2014}$ whose prior expectations capture some aspects of humanknowledge $\unicode{x2014}$ similarly mix content into their answers to logicalproblems. We explored this question across three logical reasoning tasks:natural language inference, judging the logical validity of syllogisms, and theWason selection task. We evaluate state of the art large language models, aswell as humans, and find that the language models reflect many of the samepatterns observed in humans across these tasks $\unicode{x2014}$ like humans,models answer more accurately when the semantic content of a task supports thelogical inferences. These parallels are reflected both in answer patterns, andin lower-level features like the relationship between model answerdistributions and human response times. Our findings have implications forunderstanding both these cognitive effects in humans, and the factors thatcontribute to language model performance.</description><author>Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill</author><pubDate>Wed, 17 Jul 2024 22:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07051v4</guid></item><item><title>On Pre-training of Multimodal Language Models Customized for Chart Understanding</title><link>http://arxiv.org/abs/2407.14506v1</link><description>Recent studies customizing Multimodal Large Language Models (MLLMs) fordomain-specific tasks have yielded promising results, especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answer(QA) accuracy within the chart domain. However, they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data, particularly in the models' capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs' comprehension ofcharts. We present three key findings: (1) Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. (2)Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. (3) Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of charts,including unannotated ones, while maintaining robust reasoning abilities.Furthermore, we establish a new benchmark to evaluate MLLMs' understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</description><author>Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Lu Yuan, Leonid Sigal</author><pubDate>Fri, 19 Jul 2024 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14506v1</guid></item><item><title>Static Analysis of Logic Programs via Boolean Networks</title><link>http://arxiv.org/abs/2407.09015v1</link><description>Answer Set Programming (ASP) is a declarative problem solving paradigm thatcan be used to encode a combinatorial problem as a logic program whose stablemodels correspond to the solutions of the considered problem. ASP has beenwidely applied to various domains in AI and beyond. The question "What can besaid about stable models of a logic program from its static information?" hasbeen investigated and proved useful in many circumstances. In this work, wedive into this direction more deeply by making the connection between a logicprogram and a Boolean network, which is a prominent modeling framework withapplications to various areas. The proposed connection can bring the existingresults in the rich history on static analysis of Boolean networks to exploreand prove more theoretical results on ASP, making it become a unified andpowerful tool to further study the static analysis of ASP. In particular, thenewly obtained insights have the potential to benefit many problems in thefield of ASP.</description><author>Van-Giang Trinh, Belaid Benhamou</author><pubDate>Fri, 12 Jul 2024 06:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09015v1</guid></item></channel></rss>