<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 19 Feb 2024 06:00:45 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v2</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Thu, 15 Feb 2024 10:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v2</guid></item><item><title>Zero-shot sampling of adversarial entities in biomedical question answering</title><link>http://arxiv.org/abs/2402.10527v1</link><description>The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.</description><author>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</author><pubDate>Fri, 16 Feb 2024 09:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10527v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</title><link>http://arxiv.org/abs/2402.08277v2</link><description>Advances towards more faithful and traceable answers of Large Language Models(LLMs) are crucial for various research and practical endeavors. One avenue inreaching this goal is basing the answers on reliable sources. However, thisEvidence-Based QA has proven to work insufficiently with LLMs in terms ofciting the correct sources (source quality) and truthfully representing theinformation within sources (answer attributability). In this work, wesystematically investigate how to robustly fine-tune LLMs for better sourcequality and answer attributability. Specifically, we introduce a datageneration pipeline with automated data quality filters, which can synthesizediversified high-quality training and testing data at scale. We furtherintroduce four test sets to benchmark the robustness of fine-tuned specialistmodels. Extensive evaluation shows that fine-tuning on synthetic data improvesperformance on both in- and out-of-distribution. Furthermore, we show that dataquality, which can be drastically improved by proposed quality filters, mattersmore than quantity in improving Evidence-Based QA.</description><author>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</author><pubDate>Fri, 16 Feb 2024 11:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08277v2</guid></item><item><title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title><link>http://arxiv.org/abs/2402.09642v1</link><description>This work aims to build a text embedder that can capture characteristics oftexts specified by user instructions. Despite its tremendous potential todeploy user-oriented embeddings, none of previous approaches provides aconcrete solution for it. This paper offers a new viewpoint, which treats theinstruction as a question about the input text and encodes the expected answersto obtain the representation accordingly. Intuitively, texts with the same(implicit) semantics would share similar answers following the instruction,thus leading to more similar embeddings. Specifically, we propose InBedder thatinstantiates this embed-via-answering idea by only fine-tuning language modelson abstractive question answering tasks. InBedder demonstrates significantlyimproved instruction-following capabilities according to our proposedinstruction awareness tests and instruction robustness tests, when applied toboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-basedLMs (e.g., roberta-large). Additionally, our qualitative analysis of clusteringoutcomes, achieved by applying different instructions to the same corpus,demonstrates a high degree of interpretability.</description><author>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</author><pubDate>Thu, 15 Feb 2024 01:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09642v1</guid></item><item><title>An Evaluation of GPT-4V and Gemini in Online VQA</title><link>http://arxiv.org/abs/2312.10637v2</link><description>While there is much excitement about the potential of large multimodal models(LMM), a comprehensive evaluation is critical to establish their truecapabilities and limitations. In support of this aim, we evaluate twostate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answeringdataset sourced from an authentic online question answering community. Weconduct fine-grained analysis by generating seven types of metadata for nearly2,000 visual questions, such as image type and the required image processingcapabilities. Our zero-shot performance analysis highlights the types ofquestions that are most challenging for both models, including questionsrelated to "puzzling" topic, with "Identification" user intention, with "SheetMusic" image type, or labeled as "hard" by GPT-4.</description><author>Mengchen Liu, Chongyan Chen, Danna Gurari</author><pubDate>Wed, 14 Feb 2024 03:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10637v2</guid></item><item><title>Debating with More Persuasive LLMs Leads to More Truthful Answers</title><link>http://arxiv.org/abs/2402.06782v2</link><description>Common methods for aligning large language models (LLMs) with desiredbehaviour heavily rely on human-labelled data. However, as models growincreasingly sophisticated, they will surpass human expertise, and the role ofhuman evaluation will evolve into non-experts overseeing experts. Inanticipation of this, we ask: can weaker models assess the correctness ofstronger models? We investigate this question in an analogous setting, wherestronger models (experts) possess the necessary information to answer questionsand weaker models (non-experts) lack this information. The method we evaluateis \textit{debate}, where two LLM experts each argue for a different answer,and a non-expert selects the answer. We find that debate consistently helpsboth non-expert models and humans answer questions, achieving 76\% and 88\%accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,optimising expert debaters for persuasiveness in an unsupervised mannerimproves non-expert ability to identify the truth in debates. Our resultsprovide encouraging empirical evidence for the viability of aligning modelswith debate in the absence of ground truth.</description><author>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez</author><pubDate>Thu, 15 Feb 2024 22:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06782v2</guid></item><item><title>SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning</title><link>http://arxiv.org/abs/2401.13246v2</link><description>Elucidating the reasoning process with structured explanations from questionto answer is crucial, as it significantly enhances the interpretability,traceability, and trustworthiness of question-answering (QA) systems. However,structured explanations demand models to perform intricately structuredreasoning, which poses great challenges. Most existing methods focus onsingle-step reasoning through supervised learning, ignoring logicaldependencies between steps. Moreover, existing reinforcement learning (RL)based methods overlook the structured relationships, underutilizing thepotential of RL in structured reasoning. In this paper, we propose SEER, anovel method that maximizes a structure-based return to facilitate structuredreasoning and explanation. Our proposed structure-based return preciselydescribes the hierarchical and branching structure inherent in structuredreasoning, effectively capturing the intricate relationships between differentreasoning steps. In addition, we introduce a fine-grained reward function tometiculously delineate diverse reasoning steps. Extensive experiments show thatSEER significantly outperforms state-of-the-art methods, achieving an absoluteimprovement of 6.9% over RL-based methods on EntailmentBank, a 4.4% averageimprovement on STREET benchmark, and exhibiting outstanding efficiency andcross-dataset generalization performance.</description><author>Guoxin Chen, Kexin Tang, Chao Yang, Fuying Ye, Yu Qiao, Yiming Qian</author><pubDate>Fri, 16 Feb 2024 14:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13246v2</guid></item><item><title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title><link>http://arxiv.org/abs/2402.08479v3</link><description>The increasing use of complex and opaque black box models requires theadoption of interpretable measures, one such option is extractive rationalizingmodels, which serve as a more interpretable alternative. These models, alsoknown as Explain-Then-Predict models, employ an explainer model to extractrationales and subsequently condition the predictor with the extractedinformation. Their primary objective is to provide precise and faithfulexplanations, represented by the extracted rationales. In this paper, we take asemi-supervised approach to optimize for the plausibility of extractedrationales. We adopt a pre-trained natural language inference (NLI) model andfurther fine-tune it on a small set of supervised rationales ($10\%$). The NLIpredictor is leveraged as a source of supervisory signals to the explainer viaentailment alignment. We show that, by enforcing the alignment agreementbetween the explanation and answer in a question-answering task, theperformance can be improved without access to ground truth labels. We evaluateour approach on the ERASER dataset and show that our approach achievescomparable results with supervised extractive models and outperformsunsupervised approaches by $&gt; 100\%$.</description><author>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria</author><pubDate>Fri, 16 Feb 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08479v3</guid></item><item><title>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</title><link>http://arxiv.org/abs/2402.10150v1</link><description>In self-supervised contrastive learning, a widely-adopted objective functionis InfoNCE, which uses the heuristic cosine similarity for the representationcomparison, and is closely related to maximizing the Kullback-Leibler(KL)-based mutual information. In this paper, we aim at answering twointriguing questions: (1) Can we go beyond the KL-based objective? (2) Besidesthe popular cosine similarity, can we design a better similarity function? Weprovide answers to both questions by generalizing the KL-based mutualinformation to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)using the $f$-divergences. To answer the first question, we provide a widerange of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,alignment and uniformity), and meanwhile result in similar or even superiorperformance. For the second question, assuming that the joint featuredistribution is proportional to the Gaussian kernel, we derive an $f$-Gaussiansimilarity with better interpretability and empirical performance. Finally, weidentify close relationships between the $f$-MICL objective and several popularInfoNCE-based objectives. Using benchmark tasks from both vision and naturallanguage, we empirically evaluate $f$-MICL with different $f$-divergences onvarious architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that$f$-MICL generally outperforms the benchmarks and the best-performing$f$-divergence is task and dataset dependent.</description><author>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 17:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10150v1</guid></item><item><title>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</title><link>http://arxiv.org/abs/2402.06764v2</link><description>Integrating large language models (LLMs) with knowledge graphs derived fromdomain-specific data represents an important advancement towards more powerfuland factual reasoning. As these models grow more capable, it is crucial toenable them to perform multi-step inferences over real-world knowledge graphswhile minimizing hallucination. While large language models excel atconversation and text generation, their ability to reason overdomain-specialized graphs of interconnected entities remains limited. Forexample, can we query a LLM to identify the optimal contact in a professionalnetwork for a specific goal, based on relationships and attributes in a privatedatabase? The answer is no--such capabilities lie beyond current methods.However, this question underscores a critical technical gap that must beaddressed. Many high-value applications in areas such as science, security, ande-commerce rely on proprietary knowledge graphs encoding unique structures,relationships, and logical constraints. We introduce a fine-tuning frameworkfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledgegraph into an alternate text representation with labeled question-answer pairs.We demonstrate that grounding the models in specific graph-based knowledgeexpands the models' capacity for structure-based reasoning. Our methodologyleverages the large-language model's generative capabilities to create thedataset and proposes an efficient alternate to retrieval-augmented generationstyled methods.</description><author>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</author><pubDate>Fri, 16 Feb 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06764v2</guid></item><item><title>Tokenization Preference for Human and Machine Learning Model: An Annotation Study</title><link>http://arxiv.org/abs/2304.10813v3</link><description>Is preferred tokenization for humans also preferred for machine-learning (ML)models? This study examines the relations between preferred tokenization forhumans (appropriateness and readability) and one for ML models (performance onan NLP task). The question texts of the Japanese commonsense question-answeringdataset are tokenized with six different tokenizers, and the performances ofhuman annotators and ML models were compared. Furthermore, we analyze relationsamong performance of answers by human and ML model, the appropriateness oftokenization for human, and response time to questions by human. This studyprovides a quantitative investigation result that shows that preferredtokenizations for humans and ML models are not necessarily always the same. Theresult also implies that existing methods using language models fortokenization could be a good compromise both for human and ML models.</description><author>Tatsuya Hiraoka, Tomoya Iwakura</author><pubDate>Fri, 16 Feb 2024 07:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10813v3</guid></item><item><title>OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models</title><link>http://arxiv.org/abs/2310.07637v3</link><description>Information Technology (IT) Operations (Ops), particularly ArtificialIntelligence for IT Operations (AIOps), is the guarantee for maintaining theorderly and stable operation of existing information systems. According toGartner's prediction, the use of AI technology for automated IT operations hasbecome a new trend. Large language models (LLMs) that have exhibited remarkablecapabilities in NLP-related tasks, are showing great potential in the field ofAIOps, such as in aspects of root cause analysis of failures, generation ofoperations and maintenance scripts, and summarizing of alert information.Nevertheless, the performance of current LLMs in Ops tasks is yet to bedetermined. In this paper, we present OpsEval, a comprehensive task-orientedOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'proficiency in various crucial scenarios at different ability levels. Thebenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)formats in English and Chinese. By conducting a comprehensive performanceevaluation of the current leading large language models, we show how variousLLM techniques can affect the performance of Ops, and discussed findingsrelated to various topics, including model quantification, QA evaluation, andhallucination issues. To ensure the credibility of our evaluation, we invitedozens of domain experts to manually review our questions. At the same time, wehave open-sourced 20% of the test QA to assist current researchers inpreliminary evaluations of their OpsLLM models. The remaining 80% of the data,which is not disclosed, is used to eliminate the issue of the test set leakage.Additionally, we have constructed an online leaderboard that is updated inreal-time and will continue to be updated, ensuring that any newly emergingLLMs will be evaluated promptly. Both our dataset and leaderboard have beenmade public.</description><author>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei</author><pubDate>Fri, 16 Feb 2024 08:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07637v3</guid></item><item><title>Probabilistic Reasoning in Generative Large Language Models</title><link>http://arxiv.org/abs/2402.09614v1</link><description>This paper considers the challenges that Large Language Models (LLMs) facewhen reasoning over text that includes information involving uncertaintyexplicitly quantified via probability values. This type of reasoning isrelevant to a variety of contexts ranging from everyday conversations tomedical decision-making. Despite improvements in the mathematical reasoningcapabilities of LLMs, they still exhibit significant difficulties when it comesto probabilistic reasoning. To deal with this problem, we first introduce theBayesian Linguistic Inference Dataset (BLInD), a new dataset specificallydesigned to test the probabilistic reasoning capabilities of LLMs. We thenleverage this new dataset to thoroughly illustrate the specific limitations ofLLMs for tasks involving probabilistic reasoning and present several strategiesthat map the problem to different formal representations, including Pythoncode, probabilistic inference algorithms, and probabilistic logicalprogramming. We conclude by providing an evaluation of our methods on BLInD andon an adaptation of a causal reasoning question-answering dataset, whichfurther shows their practical effectiveness.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Wed, 14 Feb 2024 23:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09614v1</guid></item><item><title>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</title><link>http://arxiv.org/abs/2305.11738v3</link><description>Recent developments in large language models (LLMs) have been impressive.However, these models sometimes show inconsistencies and problematic behavior,such as hallucinating facts, generating flawed code, or creating offensive andtoxic content. Unlike these models, humans typically utilize external tools tocross-check and refine their initial content, like using a search engine forfact-checking, or a code interpreter for debugging. Inspired by thisobservation, we introduce a framework called CRITIC that allows LLMs, which areessentially "black boxes" to validate and progressively amend their own outputsin a manner similar to human interaction with tools. More specifically,starting with an initial output, CRITIC interacts with appropriate tools toevaluate certain aspects of the text, and then revises the output based on thefeedback obtained during this validation process. Comprehensive evaluationsinvolving free-form question answering, mathematical program synthesis, andtoxicity reduction demonstrate that CRITIC consistently enhances theperformance of LLMs. Meanwhile, our research highlights the crucial importanceof external feedback in promoting the ongoing self-improvement of LLMs.</description><author>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</author><pubDate>Fri, 16 Feb 2024 08:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11738v3</guid></item><item><title>Uncovering the Full Potential of Visual Grounding Methods in VQA</title><link>http://arxiv.org/abs/2401.07803v2</link><description>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt toimprove VQA performance by strengthening a model's reliance onquestion-relevant visual information. The presence of such relevant informationin the visual input is typically assumed in training and testing. Thisassumption, however, is inherently flawed when dealing with imperfect imagerepresentations common in large-scale VQA, where the information carried byvisual features frequently deviates from expected ground-truth contents. As aresult, training and testing of VG-methods is performed with largely inaccuratedata, which obstructs proper assessment of their potential benefits. In thisstudy, we demonstrate that current evaluation schemes for VG-methods areproblematic due to the flawed assumption of availability of relevant visualinformation. Our experiments show that these methods can be much more effectivewhen evaluation conditions are corrected. Code is provided on GitHub.</description><author>Daniel Reich, Tanja Schultz</author><pubDate>Thu, 15 Feb 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07803v2</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>Multi-modal preference alignment remedies regression of visual instruction tuning on language model</title><link>http://arxiv.org/abs/2402.10884v1</link><description>In production, multi-modal large language models (MLLMs) are expected tosupport multi-turn queries of interchanging image and text modalities. However,the current MLLMs trained with visual-question-answering (VQA) datasets couldsuffer from degradation, as VQA datasets lack the diversity and complexity ofthe original text instruction datasets which the underlying language model hadbeen trained with. To address this challenging degradation, we first collect alightweight (6k entries) VQA preference dataset where answers were annotated byGemini for 5 quality metrics in a granular fashion, and investigate standardSupervised Fine-tuning, rejection sampling, Direct Preference Optimization(DPO), and SteerLM. Our findings indicate that the with DPO we are able tosurpass instruction-following capabilities of the language model, achieving a6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despitesmall data scale. This enhancement in textual instruction proficiencycorrelates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarkscompared to previous RLHF approach. In conclusion, we propose adistillation-based multi-modal alignment model with fine-grained annotations ona small dataset that reconciles the textual and visual performance of MLLMs,restoring and boosting language capability after visual instruction tuning.</description><author>Shengzhi Li, Rongyu Lin, Shichao Pei</author><pubDate>Fri, 16 Feb 2024 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10884v1</guid></item><item><title>Inference to the Best Explanation in Large Language Models</title><link>http://arxiv.org/abs/2402.10767v1</link><description>While Large Language Models (LLMs) have found success in real-worldapplications, their underlying explanatory process is still poorly understood.This paper proposes IBE-Eval, a framework inspired by philosophical accounts onInference to the Best Explanation (IBE) to advance the interpretation andevaluation of LLMs' explanations. IBE-Eval estimates the plausibility ofnatural language explanations through a combination of explicit logical andlinguistic features including: consistency, parsimony, coherence, anduncertainty. Extensive experiments are conducted on Causal Question Answering(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causalexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama2). The experiments reveal that IBE-Eval can successfully identify the bestexplanation with up to 77\% accuracy ($\approx 27\%$ above random), improvingupon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsicallymore efficient and interpretable. Additional analyses suggest that, despitemodel-specific variances, LLM-generated explanations tend to conform to IBEcriteria and that IBE-Eval is significantly correlated with human judgment,opening up opportunities for future development of automated explanationverification tools.</description><author>Dhairya Dalal, Marco Valentino, André Freitas, Paul Buitelaar</author><pubDate>Fri, 16 Feb 2024 15:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10767v1</guid></item><item><title>Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks</title><link>http://arxiv.org/abs/2402.09177v1</link><description>Large Language Models (LLMs) are susceptible to Jailbreaking attacks, whichaim to extract harmful information by subtly modifying the attack query. Asdefense mechanisms evolve, directly obtaining harmful information becomesincreasingly challenging for Jailbreaking attacks. In this work, inspired byhuman practices of indirect context to elicit harmful information, we focus ona new attack form called Contextual Interaction Attack. The idea relies on theautoregressive nature of the generation process in LLMs. We contend that theprior context--the information preceding the attack query--plays a pivotal rolein enabling potent Jailbreaking attacks. Specifically, we propose an approachthat leverages preliminary question-answer pairs to interact with the LLM. Bydoing so, we guide the responses of the model toward revealing the 'desired'harmful information. We conduct experiments on four different LLMs anddemonstrate the efficacy of this attack, which is black-box and can alsotransfer across LLMs. We believe this can lead to further developments andunderstanding of the context vector in LLMs.</description><author>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos</author><pubDate>Wed, 14 Feb 2024 13:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09177v1</guid></item><item><title>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</title><link>http://arxiv.org/abs/2402.10373v1</link><description>Large Language Models (LLMs) have demonstrated remarkable versatility inrecent years, offering potential applications across specialized domains suchas healthcare and medicine. Despite the availability of various open-sourceLLMs tailored for health contexts, adapting general-purpose LLMs to the medicaldomain presents significant challenges. In this paper, we introduce BioMistral,an open-source LLM tailored for the biomedical domain, utilizing Mistral as itsfoundation model and further pre-trained on PubMed Central. We conduct acomprehensive evaluation of BioMistral on a benchmark comprising 10 establishedmedical question-answering (QA) tasks in English. We also explore lightweightmodels obtained through quantization and model merging approaches. Our resultsdemonstrate BioMistral's superior performance compared to existing open-sourcemedical models and its competitive edge against proprietary counterparts.Finally, to address the limited availability of data beyond English and toassess the multilingual generalization of medical LLMs, we automaticallytranslated and evaluated this benchmark into 7 other languages. This marks thefirst large-scale multilingual evaluation of LLMs in the medical domain.Datasets, multilingual evaluation benchmarks, scripts, and all the modelsobtained during our experiments are freely released.</description><author>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour</author><pubDate>Thu, 15 Feb 2024 23:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10373v1</guid></item><item><title>AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis</title><link>http://arxiv.org/abs/2402.09742v1</link><description>The incorporation of Large Language Models (LLMs) in healthcare marks asignificant advancement. However, the application has predominantly beenlimited to discriminative and question-answering tasks, which does not fullyleverage their interactive potential. To address this limitation, our paperpresents AI Hospital, a framework designed to build a real-time interactivediagnosis environment. To simulate the procedure, we collect high-qualitymedical records to create patient, examiner, and medical director agents. AIHospital is then utilized for the interactive evaluation and collaboration ofLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmarkwhere various LLMs serve as intern doctors for interactive diagnosis.Subsequently, to improve diagnostic accuracy, we introduce a collaborativemechanism that involves iterative discussions and a dispute resolution processunder the supervision of the medical director. In our experiments, we validatethe reliability of AI Hospital. The results not only explore the feasibility ofapply LLMs in clinical consultation but also confirm the effectiveness of thedispute resolution focused collaboration method.</description><author>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</author><pubDate>Thu, 15 Feb 2024 06:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09742v1</guid></item><item><title>OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM</title><link>http://arxiv.org/abs/2402.09181v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in various multimodal tasks. However, their potential in themedical domain remains largely unexplored. A significant challenge arises fromthe scarcity of diverse medical images spanning various modalities andanatomical regions, which is essential in real-world medical applications. Tosolve this problem, in this paper, we introduce OmniMedVQA, a novelcomprehensive medical Visual Question Answering (VQA) benchmark. This benchmarkis collected from 75 different medical datasets, including 12 differentmodalities and covering more than 20 distinct anatomical regions. Importantly,all images in this benchmark are sourced from authentic medical scenarios,ensuring alignment with the requirements of the medical field and suitabilityfor evaluating LVLMs. Through our extensive experiments, we have found thatexisting LVLMs struggle to address these medical VQA problems effectively.Moreover, what surprises us is that medical-specialized LVLMs even exhibitinferior performance to those general-domain models, calling for a moreversatile and robust LVLM in the biomedical field. The evaluation results notonly reveal the current limitations of LVLM in understanding real medicalimages but also highlight our dataset's significance. Our dataset will be madepublicly available.</description><author>Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo</author><pubDate>Wed, 14 Feb 2024 13:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09181v1</guid></item><item><title>Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research</title><link>http://arxiv.org/abs/2402.10743v1</link><description>Entity and relationship extraction is a crucial component in natural languageprocessing tasks such as knowledge graph construction, question answeringsystem design, and semantic analysis. Most of the information of the Yishuischool of traditional Chinese Medicine (TCM) is stored in the form ofunstructured classical Chinese text. The key information extraction of TCMtexts plays an important role in mining and studying the academic schools ofTCM. In order to solve these problems efficiently using artificial intelligencemethods, this study constructs a word segmentation and entity relationshipextraction model based on conditional random fields under the framework ofnatural language processing technology to identify and extract the entityrelationship of traditional Chinese medicine texts, and uses the commonweighting technology of TF-IDF information retrieval and data mining to extractimportant key entity information in different ancient books. The dependencysyntactic parser based on neural network is used to analyze the grammaticalrelationship between entities in each ancient book article, and it isrepresented as a tree structure visualization, which lays the foundation forthe next construction of the knowledge graph of Yishui school and the use ofartificial intelligence methods to carry out the research of TCM academicschools.</description><author>Hanqing Zhao, Yuehan Li</author><pubDate>Fri, 16 Feb 2024 14:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10743v1</guid></item><item><title>Anchor-based Large Language Models</title><link>http://arxiv.org/abs/2402.07616v2</link><description>Large language models (LLMs) predominantly employ decoder-only transformerarchitectures, necessitating the retention of keys/values information forhistorical tokens to provide contextual information and avoid redundantcomputation. However, the substantial size and parameter volume of these LLMsrequire massive GPU memory. This memory demand increases with the length of theinput text, leading to an urgent need for more efficient methods of informationstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), whichutilize an innovative anchor-based self-attention network (AnSAN) and also ananchor-based inference strategy. This approach enables LLMs to compresssequence information into an anchor token, reducing the keys/values cache andenhancing inference efficiency. Experiments on question-answering benchmarksreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%keys/values cache reduction and up to 3.5 times faster inference. Despite aminor compromise in accuracy, the substantial enhancements of AnLLMs employingthe AnSAN technique in resource utilization and computational efficiencyunderscore their potential for practical LLM applications.</description><author>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</author><pubDate>Fri, 16 Feb 2024 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07616v2</guid></item><item><title>Pruning vs Quantization: Which is Better?</title><link>http://arxiv.org/abs/2307.02973v2</link><description>Neural network pruning and quantization techniques are almost as old asneural networks themselves. However, to date only ad-hoc comparisons betweenthe two have been published. In this paper, we set out to answer the questionon which is better: neural network quantization or pruning? By answering thisquestion, we hope to inform design decisions made on neural network hardwaregoing forward. We provide an extensive comparison between the two techniquesfor compressing deep neural networks. First, we give an analytical comparisonof expected quantization and pruning error for general data distributions.Then, we provide lower bounds for the per-layer pruning and quantization errorin trained networks, and compare these to empirical error after optimization.Finally, we provide an extensive experimental comparison for training 8large-scale models on 3 tasks. Our results show that in most cases quantizationoutperforms pruning. Only in some scenarios with very high compression ratio,pruning might be beneficial from an accuracy standpoint.</description><author>Andrey Kuzmin, Markus Nagel, Mart van Baalen, Arash Behboodi, Tijmen Blankevoort</author><pubDate>Fri, 16 Feb 2024 09:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02973v2</guid></item><item><title>Optimal Differentially Private Model Training with Public Data</title><link>http://arxiv.org/abs/2306.15056v2</link><description>Differential privacy (DP) ensures that training a machine learning model doesnot leak private data. In practice, we may have access to auxiliary public datathat is free of privacy concerns. In this work, we assume access to a givenamount of public data and settle the following fundamental open questions: 1.What is the optimal (worst-case) error of a DP model trained over a privatedata set while having access to side public data? 2. How can we harness publicdata to improve DP model training in practice? We consider these questions inboth the local and central models of pure and approximate DP. To answer thefirst question, we prove tight (up to log factors) lower and upper bounds thatcharacterize the optimal error rates of three fundamental problems: meanestimation, empirical risk minimization, and stochastic convex optimization. Weshow that the optimal error rates can be attained (up to log factors) by eitherdiscarding private data and training a public model, or treating public datalike it is private and using an optimal DP algorithm. To address the secondquestion, we develop novel algorithms that are "even more optimal" (i.e. betterconstants) than the asymptotically optimal approaches described above. Forlocal DP mean estimation, our algorithm is \ul{optimal including constants}.Empirically, our algorithms show benefits over the state-of-the-art.</description><author>Andrew Lowy, Zeman Li, Tianjian Huang, Meisam Razaviyayn</author><pubDate>Wed, 14 Feb 2024 04:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15056v2</guid></item><item><title>Gender Bias in News Summarization: Measures, Pitfalls and Corpora</title><link>http://arxiv.org/abs/2309.08047v2</link><description>Summarization is an important application of large language models (LLMs).Most previous evaluation of summarization models has focused on theirperformance in content selection, faithfulness, grammaticality and coherence.However, it is well known that LLMs reproduce and reinforce harmful socialbiases. This raises the question: Do these biases affect model outputs in arelatively constrained setting like summarization? To help answer this question, we first motivate and introduce a number ofdefinitions for biased behaviours in summarization models, along with practicaloperationalizations. Since we find that biases inherent to input documents canconfound bias analysis in summaries, we propose a method to generate inputdocuments with carefully controlled demographic attributes. This allows us tostudy summarizer behavior in a controlled setting, while still working withrealistic input documents. Finally, we measure gender bias in English summaries generated by bothpurpose-built summarization models and general purpose chat models as a casestudy. We find content selection in single document summarization to be largelyunaffected by gender bias, while hallucinations exhibit evidence of downstreambiases in summarization.</description><author>Julius Steen, Katja Markert</author><pubDate>Fri, 16 Feb 2024 12:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08047v2</guid></item><item><title>Pushing The Limit of LLM Capacity for Text Classification</title><link>http://arxiv.org/abs/2402.07470v2</link><description>The value of text classification's future research has encountered challengesand uncertainties, due to the extraordinary efficacy demonstrated by largelanguage models (LLMs) across numerous downstream NLP tasks. In this era ofopen-ended language modeling, where task boundaries are gradually fading, anurgent question emerges: have we made significant advances in textclassification under the full benefit of LLMs? To answer this question, wepropose RGPT, an adaptive boosting framework tailored to produce a specializedtext classification LLM by recurrently ensembling a pool of strong baselearners. The base learners are constructed by adaptively adjusting thedistribution of training samples and iteratively fine-tuning LLMs with them.Such base learners are then ensembled to be a specialized text classificationLLM, by recurrently incorporating the historical predictions from the previouslearners. Through a comprehensive empirical comparison, we show that RGPTsignificantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by1.36% on average. Further evaluation experiments show a clear surpassing ofRGPT over human classification.</description><author>Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin</author><pubDate>Fri, 16 Feb 2024 15:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07470v2</guid></item><item><title>(Ir)rationality and Cognitive Biases in Large Language Models</title><link>http://arxiv.org/abs/2402.09193v2</link><description>Do large language models (LLMs) display rational reasoning? LLMs have beenshown to contain human biases due to the data they have been trained on;whether this is reflected in rational reasoning remains less clear. In thispaper, we answer this question by evaluating seven language models using tasksfrom the cognitive psychology literature. We find that, like humans, LLMsdisplay irrationality in these tasks. However, the way this irrationality isdisplayed does not reflect that shown by humans. When incorrect answers aregiven by LLMs to these tasks, they are often incorrect in ways that differ fromhuman-like biases. On top of this, the LLMs reveal an additional layer ofirrationality in the significant inconsistency of the responses. Aside from theexperimental results, this paper seeks to make a methodological contribution byshowing how we can assess and compare different capabilities of these types ofmodels, in this case with respect to rational reasoning.</description><author>Olivia Macmillan-Scott, Mirco Musolesi</author><pubDate>Thu, 15 Feb 2024 11:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09193v2</guid></item><item><title>Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States</title><link>http://arxiv.org/abs/2402.09733v1</link><description>Large Language Models (LLMs) can make up answers that are not real, and thisis known as hallucination. This research aims to see if, how, and to whatextent LLMs are aware of hallucination. More specifically, we check whether andhow an LLM reacts differently in its hidden states when it answers a questionright versus when it hallucinates. To do this, we introduce an experimentalframework which allows examining LLM's hidden states in different hallucinationsituations. Building upon this framework, we conduct a series of experimentswith language models in the LLaMA family (Touvron et al., 2023). Our empiricalfindings suggest that LLMs react differently when processing a genuine responseversus a fabricated one. We then apply various model interpretation techniquesto help understand and explain the findings better. Moreover, informed by theempirical observations, we show great potential of using the guidance derivedfrom LLM's hidden representation space to mitigate hallucination. We believethis work provides insights into how LLMs produce hallucinated answers and howto make them occur less often.</description><author>Hanyu Duan, Yi Yang, Kar Yan Tam</author><pubDate>Thu, 15 Feb 2024 06:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09733v1</guid></item><item><title>Can Transformers Predict Vibrations?</title><link>http://arxiv.org/abs/2402.10511v1</link><description>Highly accurate time-series vibration prediction is an important researchissue for electric vehicles (EVs). EVs often experience vibrations when drivingon rough terrains, known as torsional resonance. This resonance, caused by theinteraction between motor and tire vibrations, puts excessive loads on thevehicle's drive shaft. However, current damping technologies only detectresonance after the vibration amplitude of the drive shaft torque reaches acertain threshold, leading to significant loads on the shaft at the time ofdetection. In this study, we propose a novel approach to address this issue byintroducing Resoformer, a transformer-based model for predicting torsionalresonance. Resoformer utilizes time-series of the motor rotation speed as inputand predicts the amplitude of torsional vibration at a specified quantileoccurring in the shaft after the input series. By calculating the attentionbetween recursive and convolutional features extracted from the measured datapoints, Resoformer improves the accuracy of vibration forecasting. To evaluatethe model, we use a vibration dataset called VIBES (Dataset for ForecastingVibration Transition in EVs), consisting of 2,600 simulator-generated vibrationsequences. Our experiments, conducted on strong baselines built on the VIBESdataset, demonstrate that Resoformer achieves state-of-the-art results. Inconclusion, our study answers the question "Can Transformers ForecastVibrations?" While traditional transformer architectures show low performancein forecasting torsional resonance waves, our findings indicate that combiningrecurrent neural network and temporal convolutional network using thetransformer architecture improves the accuracy of long-term vibrationforecasting.</description><author>Fusataka Kuniyoshi, Yoshihide Sawada</author><pubDate>Fri, 16 Feb 2024 08:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10511v1</guid></item><item><title>Robust agents learn causal world models</title><link>http://arxiv.org/abs/2402.10877v1</link><description>It has long been hypothesised that causal reasoning plays a fundamental rolein robust and general intelligence. However, it is not known if agents mustlearn causal models in order to generalise to new domains, or if otherinductive biases are sufficient. We answer this question, showing that anyagent capable of satisfying a regret bound under a large set of distributionalshifts must have learned an approximate causal model of the data generatingprocess, which converges to the true causal model for optimal agents. Wediscuss the implications of this result for several research areas includingtransfer learning and causal inference.</description><author>Jonathan Richens, Tom Everitt</author><pubDate>Fri, 16 Feb 2024 18:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10877v1</guid></item><item><title>Model approximation in MDPs with unbounded per-step cost</title><link>http://arxiv.org/abs/2402.08813v1</link><description>We consider the problem of designing a control policy for an infinite-horizondiscounted cost Markov decision process $\mathcal{M}$ when we only have accessto an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy$\hat{\pi}^{\star}$ of the approximate model perform when used in the originalmodel $\mathcal{M}$? We answer this question by bounding a weighted norm of thedifference between the value function of $\hat{\pi}^\star $ when used in$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extendour results and obtain potentially tighter upper bounds by considering affinetransformations of the per-step cost. We further provide upper bounds thatexplicitly depend on the weighted distance between cost functions and weighteddistance between transition kernels of the original and approximate models. Wepresent examples to illustrate our results.</description><author>Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang</author><pubDate>Tue, 13 Feb 2024 21:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08813v1</guid></item><item><title>Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty</title><link>http://arxiv.org/abs/2402.10423v1</link><description>Our work focuses on understanding the underpinning mechanism of datasetcondensation by drawing connections with ($\epsilon$, $\delta$)-differentialprivacy where the optimal noise, $\epsilon$, is chosen by adversarialuncertainty \cite{Grining2017}. We can answer the question about the innerworkings of the dataset condensation procedure. Previous work \cite{dong2022}proved the link between dataset condensation (DC) and ($\epsilon$,$\delta$)-differential privacy. However, it is unclear from existing works onablating DC to obtain a lower-bound estimate of $\epsilon$ that will sufficefor creating high-fidelity synthetic data. We suggest that adversarialuncertainty is the most appropriate method to achieve an optimal noise level,$\epsilon$. As part of the internal dynamics of dataset condensation, we adopta satisfactory scheme for noise estimation that guarantees high-fidelity datawhile providing privacy.</description><author>Kenneth Odoh</author><pubDate>Fri, 16 Feb 2024 03:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10423v1</guid></item><item><title>ProtChatGPT: Towards Understanding Proteins with Large Language Models</title><link>http://arxiv.org/abs/2402.09649v1</link><description>Protein research is crucial in various fundamental disciplines, butunderstanding their intricate structure-function relationships remainschallenging. Recent Large Language Models (LLMs) have made significant stridesin comprehending task-specific knowledge, suggesting the potential forChatGPT-like systems specialized in protein to facilitate basic research. Inthis work, we introduce ProtChatGPT, which aims at learning and understandingprotein structures via natural languages. ProtChatGPT enables users to uploadproteins, ask questions, and engage in interactive conversations to producecomprehensive answers. The system comprises protein encoders, aProtein-Language Pertaining Transformer (PLP-former), a projection adapter, andan LLM. The protein first undergoes protein encoders and PLP-former to produceprotein embeddings, which are then projected by the adapter to conform with theLLM. The LLM finally combines user questions with projected embeddings togenerate informative answers. Experiments show that ProtChatGPT can producepromising responses to proteins and their corresponding questions. We hope thatProtChatGPT could form the basis for further exploration and application inprotein research. Code and our pre-trained model will be publicly available.</description><author>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang</author><pubDate>Thu, 15 Feb 2024 01:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09649v1</guid></item><item><title>Explaining generative diffusion models via visual analysis for interpretable decision-making process</title><link>http://arxiv.org/abs/2402.10404v1</link><description>Diffusion models have demonstrated remarkable performance in generationtasks. Nevertheless, explaining the diffusion process remains challenging dueto it being a sequence of denoising noisy images that are difficult for expertsto interpret. To address this issue, we propose the three research questions tointerpret the diffusion process from the perspective of the visual conceptsgenerated by the model and the region where the model attends in each timestep. We devise tools for visualizing the diffusion process and answering theaforementioned research questions to render the diffusion processhuman-understandable. We show how the output is progressively generated in thediffusion process by explaining the level of denoising and highlightingrelationships to foundational visual concepts at each time step through theresults of experiments with various visual analyses using the tools. Throughoutthe training of the diffusion model, the model learns diverse visual conceptscorresponding to each time-step, enabling the model to predict varying levelsof visual concepts at different stages. We substantiate our tools using AreaUnder Cover (AUC) score, correlation quantification, and cross-attentionmapping. Our findings provide insights into the diffusion process and pave theway for further research into explainable diffusion mechanisms.</description><author>Ji-Hoon Park, Yeong-Joon Ju, Seong-Whan Lee</author><pubDate>Fri, 16 Feb 2024 02:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10404v1</guid></item><item><title>Can We Verify Step by Step for Incorrect Answer Detection?</title><link>http://arxiv.org/abs/2402.10528v1</link><description>Chain-of-Thought (CoT) prompting has marked a significant advancement inenhancing the reasoning capabilities of large language models (LLMs). Previousstudies have developed various extensions of CoT, which focus primarily onenhancing end-task performance. In addition, there has been research onassessing the quality of reasoning chains in CoT. This raises an intriguingquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizingthe reasoning chains they generate? To answer this research question, weintroduce a benchmark, R2PE, designed specifically to explore the relationshipbetween reasoning chains and performance in various reasoning tasks spanningfive different domains. This benchmark aims to measure the falsehood of thefinal output of LLMs based on the reasoning steps. To make full use ofinformation in multiple reasoning chains, we propose the process discernibilityscore (PDS) framework that beats the answer-checking baseline by a largemargin. Concretely, this resulted in an average of 5.1% increase in the F1score across all 45 subsets within R2PE. We further demonstrate our PDS'sefficacy in advancing open-domain QA accuracy. Data and code are available athttps://github.com/XinXU-USTC/R2PE.</description><author>Xin Xu, Shizhe Diao, Can Yang, Yang Wang</author><pubDate>Fri, 16 Feb 2024 09:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10528v1</guid></item><item><title>Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation</title><link>http://arxiv.org/abs/2302.03038v2</link><description>Spatially resolved transcriptomics brings exciting breakthroughs tosingle-cell analysis by providing physical locations along with geneexpression. However, as a cost of the extremely high spatial resolution, thecellular level spatial transcriptomic data suffer significantly from missingvalues. While a standard solution is to perform imputation on the missingvalues, most existing methods either overlook spatial information or onlyincorporate localized spatial context without the ability to capture long-rangespatial information. Using multi-head self-attention mechanisms and positionalencoding, transformer models can readily grasp the relationship between tokensand encode location information. In this paper, by treating single cells asspatial tokens, we study how to leverage transformers to facilitate spatialtanscriptomics imputation. In particular, investigate the following two keyquestions: (1) $\textit{how to encode spatial information of cells intransformers}$, and (2) $\textit{ how to train a transformer for transcriptomicimputation}$. By answering these two questions, we present a transformer-basedimputation framework, SpaFormer, for cellular-level spatial transcriptomicdata. Extensive experiments demonstrate that SpaFormer outperforms existingstate-of-the-art imputation algorithms on three large-scale datasets whilemaintaining superior computational efficiency.</description><author>Hongzhi Wen, Wenzhuo Tang, Wei Jin, Jiayuan Ding, Renming Liu, Xinnan Dai, Feng Shi, Lulu Shang, Hui Liu, Yuying Xie</author><pubDate>Fri, 16 Feb 2024 17:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03038v2</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v1</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Wed, 14 Feb 2024 17:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v1</guid></item><item><title>ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions</title><link>http://arxiv.org/abs/2402.08801v1</link><description>Since its release in November 2022, ChatGPT has shaken up Stack Overflow, thepremier platform for developers' queries on programming and softwaredevelopment. Demonstrating an ability to generate instant, human-like responsesto technical questions, ChatGPT has ignited debates within the developercommunity about the evolving role of human-driven platforms in the age ofgenerative AI. Two months after ChatGPT's release, Meta released its answerwith its own Large Language Model (LLM) called LLaMA: the race was on. Weconducted an empirical study analyzing questions from Stack Overflow and usingthese LLMs to address them. This way, we aim to (ii) measure user engagementevolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'answers and their potential to replace Stack Overflow in the long term; (iii)identify and understand why LLMs fails; and (iv) compare LLMs together. Ourempirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,yet do not outperform it for some domains, while a significant decline in userposting activity has been observed. Furthermore, we also discuss the impact ofour findings regarding the usage and development of new LLMs.</description><author>Leuson Da Silva, Jordan Samhi, Foutse Khomh</author><pubDate>Tue, 13 Feb 2024 21:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08801v1</guid></item><item><title>Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4</title><link>http://arxiv.org/abs/2402.10083v1</link><description>Purpose: To assess the alignment of GPT-4-based evaluation to human clinicianexperts, for the evaluation of responses to ophthalmology-related patientqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmologyquestions and paired answers were created by ophthalmologists to representcommonly asked patient questions, divided into fine-tuning (368; 92%), andtesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,additional 8 glaucoma QnA pairs were included. 200 responses to the testingdataset were generated by 5 fine-tuned LLMs for evaluation. A customizedclinical evaluation rubric was used to guide GPT-4 evaluation, grounded onclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4evaluation was then compared against ranking by 5 clinicians for clinicalalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4evaluation demonstrated significant agreement with human clinician rankings,with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80respectively; while correlation based on Cohen Kappa was more modest at 0.50.Notably, qualitative analysis and the glaucoma sub-analysis revealed clinicalinaccuracies in the LLM-generated responses, which were appropriatelyidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignmentof GPT-4 evaluation highlighted its potential to streamline the clinicalevaluation of LLM chatbot responses to healthcare-related queries. Bycomplementing the existing clinician-dependent manual grading, this efficientand automated evaluation could assist the validation of future developments inLLM applications for healthcare.</description><author>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</author><pubDate>Thu, 15 Feb 2024 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10083v1</guid></item><item><title>Scaling the Authoring of AutoTutors with Large Language Models</title><link>http://arxiv.org/abs/2402.09216v1</link><description>Large Language Models (LLMs) have found several use cases in education,ranging from automatic question generation to essay evaluation. In this paper,we explore the potential of using Large Language Models (LLMs) to authorIntelligent Tutoring Systems. A common pitfall of LLMs is their straying fromdesired pedagogical strategies such as leaking the answer to the student, andin general, providing no guarantees. We posit that while LLMs with certainguardrails can take the place of subject experts, the overall pedagogicaldesign still needs to be handcrafted for the best learning results. Based onthis principle, we create a sample end-to-end tutoring system named MWPTutor,which uses LLMs to fill in the state space of a pre-defined finite statetransducer. This approach retains the structure and the pedagogy of traditionaltutoring systems that has been developed over the years by learning scientistsbut brings in additional flexibility of LLM-based approaches. Through a humanevaluation study on two datasets based on math word problems, we show that ourhybrid approach achieves a better overall tutoring score than an instructed,but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up thescope for the community to improve its performance by improving individualmodules or using different teaching strategies that it can follow</description><author>Sankalan Pal Chowdhury, Vilém Zouhar, Mrinmaya Sachan</author><pubDate>Wed, 14 Feb 2024 14:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09216v1</guid></item></channel></rss>