<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 18 Feb 2024 18:05:34 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v2</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Thu, 15 Feb 2024 10:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v2</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title><link>http://arxiv.org/abs/2402.09642v1</link><description>This work aims to build a text embedder that can capture characteristics oftexts specified by user instructions. Despite its tremendous potential todeploy user-oriented embeddings, none of previous approaches provides aconcrete solution for it. This paper offers a new viewpoint, which treats theinstruction as a question about the input text and encodes the expected answersto obtain the representation accordingly. Intuitively, texts with the same(implicit) semantics would share similar answers following the instruction,thus leading to more similar embeddings. Specifically, we propose InBedder thatinstantiates this embed-via-answering idea by only fine-tuning language modelson abstractive question answering tasks. InBedder demonstrates significantlyimproved instruction-following capabilities according to our proposedinstruction awareness tests and instruction robustness tests, when applied toboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-basedLMs (e.g., roberta-large). Additionally, our qualitative analysis of clusteringoutcomes, achieved by applying different instructions to the same corpus,demonstrates a high degree of interpretability.</description><author>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</author><pubDate>Thu, 15 Feb 2024 01:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09642v1</guid></item><item><title>An Evaluation of GPT-4V and Gemini in Online VQA</title><link>http://arxiv.org/abs/2312.10637v2</link><description>While there is much excitement about the potential of large multimodal models(LMM), a comprehensive evaluation is critical to establish their truecapabilities and limitations. In support of this aim, we evaluate twostate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answeringdataset sourced from an authentic online question answering community. Weconduct fine-grained analysis by generating seven types of metadata for nearly2,000 visual questions, such as image type and the required image processingcapabilities. Our zero-shot performance analysis highlights the types ofquestions that are most challenging for both models, including questionsrelated to "puzzling" topic, with "Identification" user intention, with "SheetMusic" image type, or labeled as "hard" by GPT-4.</description><author>Mengchen Liu, Chongyan Chen, Danna Gurari</author><pubDate>Wed, 14 Feb 2024 03:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10637v2</guid></item><item><title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title><link>http://arxiv.org/abs/2402.08479v3</link><description>The increasing use of complex and opaque black box models requires theadoption of interpretable measures, one such option is extractive rationalizingmodels, which serve as a more interpretable alternative. These models, alsoknown as Explain-Then-Predict models, employ an explainer model to extractrationales and subsequently condition the predictor with the extractedinformation. Their primary objective is to provide precise and faithfulexplanations, represented by the extracted rationales. In this paper, we take asemi-supervised approach to optimize for the plausibility of extractedrationales. We adopt a pre-trained natural language inference (NLI) model andfurther fine-tune it on a small set of supervised rationales ($10\%$). The NLIpredictor is leveraged as a source of supervisory signals to the explainer viaentailment alignment. We show that, by enforcing the alignment agreementbetween the explanation and answer in a question-answering task, theperformance can be improved without access to ground truth labels. We evaluateour approach on the ERASER dataset and show that our approach achievescomparable results with supervised extractive models and outperformsunsupervised approaches by $&gt; 100\%$.</description><author>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria</author><pubDate>Fri, 16 Feb 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08479v3</guid></item><item><title>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</title><link>http://arxiv.org/abs/2402.10150v1</link><description>In self-supervised contrastive learning, a widely-adopted objective functionis InfoNCE, which uses the heuristic cosine similarity for the representationcomparison, and is closely related to maximizing the Kullback-Leibler(KL)-based mutual information. In this paper, we aim at answering twointriguing questions: (1) Can we go beyond the KL-based objective? (2) Besidesthe popular cosine similarity, can we design a better similarity function? Weprovide answers to both questions by generalizing the KL-based mutualinformation to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)using the $f$-divergences. To answer the first question, we provide a widerange of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,alignment and uniformity), and meanwhile result in similar or even superiorperformance. For the second question, assuming that the joint featuredistribution is proportional to the Gaussian kernel, we derive an $f$-Gaussiansimilarity with better interpretability and empirical performance. Finally, weidentify close relationships between the $f$-MICL objective and several popularInfoNCE-based objectives. Using benchmark tasks from both vision and naturallanguage, we empirically evaluate $f$-MICL with different $f$-divergences onvarious architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that$f$-MICL generally outperforms the benchmarks and the best-performing$f$-divergence is task and dataset dependent.</description><author>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 17:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10150v1</guid></item><item><title>Tokenization Preference for Human and Machine Learning Model: An Annotation Study</title><link>http://arxiv.org/abs/2304.10813v3</link><description>Is preferred tokenization for humans also preferred for machine-learning (ML)models? This study examines the relations between preferred tokenization forhumans (appropriateness and readability) and one for ML models (performance onan NLP task). The question texts of the Japanese commonsense question-answeringdataset are tokenized with six different tokenizers, and the performances ofhuman annotators and ML models were compared. Furthermore, we analyze relationsamong performance of answers by human and ML model, the appropriateness oftokenization for human, and response time to questions by human. This studyprovides a quantitative investigation result that shows that preferredtokenizations for humans and ML models are not necessarily always the same. Theresult also implies that existing methods using language models fortokenization could be a good compromise both for human and ML models.</description><author>Tatsuya Hiraoka, Tomoya Iwakura</author><pubDate>Fri, 16 Feb 2024 07:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10813v3</guid></item><item><title>OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM</title><link>http://arxiv.org/abs/2402.09181v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in various multimodal tasks. However, their potential in themedical domain remains largely unexplored. A significant challenge arises fromthe scarcity of diverse medical images spanning various modalities andanatomical regions, which is essential in real-world medical applications. Tosolve this problem, in this paper, we introduce OmniMedVQA, a novelcomprehensive medical Visual Question Answering (VQA) benchmark. This benchmarkis collected from 75 different medical datasets, including 12 differentmodalities and covering more than 20 distinct anatomical regions. Importantly,all images in this benchmark are sourced from authentic medical scenarios,ensuring alignment with the requirements of the medical field and suitabilityfor evaluating LVLMs. Through our extensive experiments, we have found thatexisting LVLMs struggle to address these medical VQA problems effectively.Moreover, what surprises us is that medical-specialized LVLMs even exhibitinferior performance to those general-domain models, calling for a moreversatile and robust LVLM in the biomedical field. The evaluation results notonly reveal the current limitations of LVLM in understanding real medicalimages but also highlight our dataset's significance. Our dataset will be madepublicly available.</description><author>Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo</author><pubDate>Wed, 14 Feb 2024 13:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09181v1</guid></item><item><title>AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis</title><link>http://arxiv.org/abs/2402.09742v1</link><description>The incorporation of Large Language Models (LLMs) in healthcare marks asignificant advancement. However, the application has predominantly beenlimited to discriminative and question-answering tasks, which does not fullyleverage their interactive potential. To address this limitation, our paperpresents AI Hospital, a framework designed to build a real-time interactivediagnosis environment. To simulate the procedure, we collect high-qualitymedical records to create patient, examiner, and medical director agents. AIHospital is then utilized for the interactive evaluation and collaboration ofLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmarkwhere various LLMs serve as intern doctors for interactive diagnosis.Subsequently, to improve diagnostic accuracy, we introduce a collaborativemechanism that involves iterative discussions and a dispute resolution processunder the supervision of the medical director. In our experiments, we validatethe reliability of AI Hospital. The results not only explore the feasibility ofapply LLMs in clinical consultation but also confirm the effectiveness of thedispute resolution focused collaboration method.</description><author>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</author><pubDate>Thu, 15 Feb 2024 06:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09742v1</guid></item><item><title>Probabilistic Reasoning in Generative Large Language Models</title><link>http://arxiv.org/abs/2402.09614v1</link><description>This paper considers the challenges that Large Language Models (LLMs) facewhen reasoning over text that includes information involving uncertaintyexplicitly quantified via probability values. This type of reasoning isrelevant to a variety of contexts ranging from everyday conversations tomedical decision-making. Despite improvements in the mathematical reasoningcapabilities of LLMs, they still exhibit significant difficulties when it comesto probabilistic reasoning. To deal with this problem, we first introduce theBayesian Linguistic Inference Dataset (BLInD), a new dataset specificallydesigned to test the probabilistic reasoning capabilities of LLMs. We thenleverage this new dataset to thoroughly illustrate the specific limitations ofLLMs for tasks involving probabilistic reasoning and present several strategiesthat map the problem to different formal representations, including Pythoncode, probabilistic inference algorithms, and probabilistic logicalprogramming. We conclude by providing an evaluation of our methods on BLInD andon an adaptation of a causal reasoning question-answering dataset, whichfurther shows their practical effectiveness.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Wed, 14 Feb 2024 23:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09614v1</guid></item><item><title>Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks</title><link>http://arxiv.org/abs/2402.09177v1</link><description>Large Language Models (LLMs) are susceptible to Jailbreaking attacks, whichaim to extract harmful information by subtly modifying the attack query. Asdefense mechanisms evolve, directly obtaining harmful information becomesincreasingly challenging for Jailbreaking attacks. In this work, inspired byhuman practices of indirect context to elicit harmful information, we focus ona new attack form called Contextual Interaction Attack. The idea relies on theautoregressive nature of the generation process in LLMs. We contend that theprior context--the information preceding the attack query--plays a pivotal rolein enabling potent Jailbreaking attacks. Specifically, we propose an approachthat leverages preliminary question-answer pairs to interact with the LLM. Bydoing so, we guide the responses of the model toward revealing the 'desired'harmful information. We conduct experiments on four different LLMs anddemonstrate the efficacy of this attack, which is black-box and can alsotransfer across LLMs. We believe this can lead to further developments andunderstanding of the context vector in LLMs.</description><author>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos</author><pubDate>Wed, 14 Feb 2024 13:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09177v1</guid></item><item><title>Uncovering the Full Potential of Visual Grounding Methods in VQA</title><link>http://arxiv.org/abs/2401.07803v2</link><description>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt toimprove VQA performance by strengthening a model's reliance onquestion-relevant visual information. The presence of such relevant informationin the visual input is typically assumed in training and testing. Thisassumption, however, is inherently flawed when dealing with imperfect imagerepresentations common in large-scale VQA, where the information carried byvisual features frequently deviates from expected ground-truth contents. As aresult, training and testing of VG-methods is performed with largely inaccuratedata, which obstructs proper assessment of their potential benefits. In thisstudy, we demonstrate that current evaluation schemes for VG-methods areproblematic due to the flawed assumption of availability of relevant visualinformation. Our experiments show that these methods can be much more effectivewhen evaluation conditions are corrected. Code is provided on GitHub.</description><author>Daniel Reich, Tanja Schultz</author><pubDate>Thu, 15 Feb 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07803v2</guid></item><item><title>Optimal Differentially Private Model Training with Public Data</title><link>http://arxiv.org/abs/2306.15056v2</link><description>Differential privacy (DP) ensures that training a machine learning model doesnot leak private data. In practice, we may have access to auxiliary public datathat is free of privacy concerns. In this work, we assume access to a givenamount of public data and settle the following fundamental open questions: 1.What is the optimal (worst-case) error of a DP model trained over a privatedata set while having access to side public data? 2. How can we harness publicdata to improve DP model training in practice? We consider these questions inboth the local and central models of pure and approximate DP. To answer thefirst question, we prove tight (up to log factors) lower and upper bounds thatcharacterize the optimal error rates of three fundamental problems: meanestimation, empirical risk minimization, and stochastic convex optimization. Weshow that the optimal error rates can be attained (up to log factors) by eitherdiscarding private data and training a public model, or treating public datalike it is private and using an optimal DP algorithm. To address the secondquestion, we develop novel algorithms that are "even more optimal" (i.e. betterconstants) than the asymptotically optimal approaches described above. Forlocal DP mean estimation, our algorithm is \ul{optimal including constants}.Empirically, our algorithms show benefits over the state-of-the-art.</description><author>Andrew Lowy, Zeman Li, Tianjian Huang, Meisam Razaviyayn</author><pubDate>Wed, 14 Feb 2024 04:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15056v2</guid></item><item><title>(Ir)rationality and Cognitive Biases in Large Language Models</title><link>http://arxiv.org/abs/2402.09193v2</link><description>Do large language models (LLMs) display rational reasoning? LLMs have beenshown to contain human biases due to the data they have been trained on;whether this is reflected in rational reasoning remains less clear. In thispaper, we answer this question by evaluating seven language models using tasksfrom the cognitive psychology literature. We find that, like humans, LLMsdisplay irrationality in these tasks. However, the way this irrationality isdisplayed does not reflect that shown by humans. When incorrect answers aregiven by LLMs to these tasks, they are often incorrect in ways that differ fromhuman-like biases. On top of this, the LLMs reveal an additional layer ofirrationality in the significant inconsistency of the responses. Aside from theexperimental results, this paper seeks to make a methodological contribution byshowing how we can assess and compare different capabilities of these types ofmodels, in this case with respect to rational reasoning.</description><author>Olivia Macmillan-Scott, Mirco Musolesi</author><pubDate>Thu, 15 Feb 2024 11:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09193v2</guid></item><item><title>Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States</title><link>http://arxiv.org/abs/2402.09733v1</link><description>Large Language Models (LLMs) can make up answers that are not real, and thisis known as hallucination. This research aims to see if, how, and to whatextent LLMs are aware of hallucination. More specifically, we check whether andhow an LLM reacts differently in its hidden states when it answers a questionright versus when it hallucinates. To do this, we introduce an experimentalframework which allows examining LLM's hidden states in different hallucinationsituations. Building upon this framework, we conduct a series of experimentswith language models in the LLaMA family (Touvron et al., 2023). Our empiricalfindings suggest that LLMs react differently when processing a genuine responseversus a fabricated one. We then apply various model interpretation techniquesto help understand and explain the findings better. Moreover, informed by theempirical observations, we show great potential of using the guidance derivedfrom LLM's hidden representation space to mitigate hallucination. We believethis work provides insights into how LLMs produce hallucinated answers and howto make them occur less often.</description><author>Hanyu Duan, Yi Yang, Kar Yan Tam</author><pubDate>Thu, 15 Feb 2024 06:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09733v1</guid></item><item><title>Model approximation in MDPs with unbounded per-step cost</title><link>http://arxiv.org/abs/2402.08813v1</link><description>We consider the problem of designing a control policy for an infinite-horizondiscounted cost Markov decision process $\mathcal{M}$ when we only have accessto an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy$\hat{\pi}^{\star}$ of the approximate model perform when used in the originalmodel $\mathcal{M}$? We answer this question by bounding a weighted norm of thedifference between the value function of $\hat{\pi}^\star $ when used in$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extendour results and obtain potentially tighter upper bounds by considering affinetransformations of the per-step cost. We further provide upper bounds thatexplicitly depend on the weighted distance between cost functions and weighteddistance between transition kernels of the original and approximate models. Wepresent examples to illustrate our results.</description><author>Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang</author><pubDate>Tue, 13 Feb 2024 21:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08813v1</guid></item><item><title>ProtChatGPT: Towards Understanding Proteins with Large Language Models</title><link>http://arxiv.org/abs/2402.09649v1</link><description>Protein research is crucial in various fundamental disciplines, butunderstanding their intricate structure-function relationships remainschallenging. Recent Large Language Models (LLMs) have made significant stridesin comprehending task-specific knowledge, suggesting the potential forChatGPT-like systems specialized in protein to facilitate basic research. Inthis work, we introduce ProtChatGPT, which aims at learning and understandingprotein structures via natural languages. ProtChatGPT enables users to uploadproteins, ask questions, and engage in interactive conversations to producecomprehensive answers. The system comprises protein encoders, aProtein-Language Pertaining Transformer (PLP-former), a projection adapter, andan LLM. The protein first undergoes protein encoders and PLP-former to produceprotein embeddings, which are then projected by the adapter to conform with theLLM. The LLM finally combines user questions with projected embeddings togenerate informative answers. Experiments show that ProtChatGPT can producepromising responses to proteins and their corresponding questions. We hope thatProtChatGPT could form the basis for further exploration and application inprotein research. Code and our pre-trained model will be publicly available.</description><author>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang</author><pubDate>Thu, 15 Feb 2024 01:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09649v1</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v1</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Wed, 14 Feb 2024 17:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v1</guid></item><item><title>ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions</title><link>http://arxiv.org/abs/2402.08801v1</link><description>Since its release in November 2022, ChatGPT has shaken up Stack Overflow, thepremier platform for developers' queries on programming and softwaredevelopment. Demonstrating an ability to generate instant, human-like responsesto technical questions, ChatGPT has ignited debates within the developercommunity about the evolving role of human-driven platforms in the age ofgenerative AI. Two months after ChatGPT's release, Meta released its answerwith its own Large Language Model (LLM) called LLaMA: the race was on. Weconducted an empirical study analyzing questions from Stack Overflow and usingthese LLMs to address them. This way, we aim to (ii) measure user engagementevolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'answers and their potential to replace Stack Overflow in the long term; (iii)identify and understand why LLMs fails; and (iv) compare LLMs together. Ourempirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,yet do not outperform it for some domains, while a significant decline in userposting activity has been observed. Furthermore, we also discuss the impact ofour findings regarding the usage and development of new LLMs.</description><author>Leuson Da Silva, Jordan Samhi, Foutse Khomh</author><pubDate>Tue, 13 Feb 2024 21:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08801v1</guid></item><item><title>Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4</title><link>http://arxiv.org/abs/2402.10083v1</link><description>Purpose: To assess the alignment of GPT-4-based evaluation to human clinicianexperts, for the evaluation of responses to ophthalmology-related patientqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmologyquestions and paired answers were created by ophthalmologists to representcommonly asked patient questions, divided into fine-tuning (368; 92%), andtesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,additional 8 glaucoma QnA pairs were included. 200 responses to the testingdataset were generated by 5 fine-tuned LLMs for evaluation. A customizedclinical evaluation rubric was used to guide GPT-4 evaluation, grounded onclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4evaluation was then compared against ranking by 5 clinicians for clinicalalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4evaluation demonstrated significant agreement with human clinician rankings,with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80respectively; while correlation based on Cohen Kappa was more modest at 0.50.Notably, qualitative analysis and the glaucoma sub-analysis revealed clinicalinaccuracies in the LLM-generated responses, which were appropriatelyidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignmentof GPT-4 evaluation highlighted its potential to streamline the clinicalevaluation of LLM chatbot responses to healthcare-related queries. Bycomplementing the existing clinician-dependent manual grading, this efficientand automated evaluation could assist the validation of future developments inLLM applications for healthcare.</description><author>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</author><pubDate>Thu, 15 Feb 2024 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10083v1</guid></item><item><title>Scaling the Authoring of AutoTutors with Large Language Models</title><link>http://arxiv.org/abs/2402.09216v1</link><description>Large Language Models (LLMs) have found several use cases in education,ranging from automatic question generation to essay evaluation. In this paper,we explore the potential of using Large Language Models (LLMs) to authorIntelligent Tutoring Systems. A common pitfall of LLMs is their straying fromdesired pedagogical strategies such as leaking the answer to the student, andin general, providing no guarantees. We posit that while LLMs with certainguardrails can take the place of subject experts, the overall pedagogicaldesign still needs to be handcrafted for the best learning results. Based onthis principle, we create a sample end-to-end tutoring system named MWPTutor,which uses LLMs to fill in the state space of a pre-defined finite statetransducer. This approach retains the structure and the pedagogy of traditionaltutoring systems that has been developed over the years by learning scientistsbut brings in additional flexibility of LLM-based approaches. Through a humanevaluation study on two datasets based on math word problems, we show that ourhybrid approach achieves a better overall tutoring score than an instructed,but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up thescope for the community to improve its performance by improving individualmodules or using different teaching strategies that it can follow</description><author>Sankalan Pal Chowdhury, Vilém Zouhar, Mrinmaya Sachan</author><pubDate>Wed, 14 Feb 2024 14:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09216v1</guid></item></channel></rss>