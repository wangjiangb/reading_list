<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 12 Jul 2024 01:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension</title><link>http://arxiv.org/abs/2407.07321v1</link><description>Large Language Models (LLMs) have been applied to many research problemsacross various domains. One of the applications of LLMs is providingquestion-answering systems that cater to users from different fields. Theeffectiveness of LLM-based question-answering systems has already beenestablished at an acceptable level for users posing questions in popular andpublic domains such as trivia and literature. However, it has not often beenestablished in niche domains that traditionally require specialized expertise.To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performanceof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answeringquestions originating from Environmental Impact Statements prepared by U.S.federal government agencies in accordance with the National EnvironmentalEnvironmental Act (NEPA). We specifically measure the ability of LLMs tounderstand the nuances of legal, technical, and compliance-related informationpresent in NEPA documents in different contextual scenarios. For example, wetest the LLMs' internal prior NEPA knowledge by providing questions without anycontext, as well as assess how LLMs synthesize the contextual informationpresent in long NEPA documents to facilitate the question/answering task. Wecompare the performance of the long context LLMs and RAG powered models inhandling different types of questions (e.g., problem-solving, divergent). Ourresults suggest that RAG powered models significantly outperform the longcontext models in the answer accuracy regardless of the choice of the frontierLLM. Our further analysis reveals that many models perform better answeringclosed questions than divergent and problem-solving questions.</description><author>Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana</author><pubDate>Wed, 10 Jul 2024 02:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07321v1</guid></item><item><title>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation</title><link>http://arxiv.org/abs/2405.09939v2</link><description>We introduce SciQAG, a novel framework for automatically generatinghigh-quality science question-answer pairs from a large corpus of scientificliterature based on large language models (LLMs). SciQAG consists of a QAgenerator and a QA evaluator, which work together to extract diverse andresearch-level questions and answers from scientific papers. Utilizing thisframework, we construct a large-scale, high-quality, open-ended science QAdataset containing 188,042 QA pairs extracted from 22,743 scientific papersacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmarktask designed to evaluate the science question-answering ability of LLMs.Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG datasetsignificantly improves their performance on both open-ended question answeringand scientific tasks. To foster research and collaboration, we make thedatasets, models, and evaluation codes publicly available, contributing to theadvancement of science question answering and developing more interpretable andreasoning-capable AI systems.</description><author>Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster</author><pubDate>Wed, 10 Jul 2024 01:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09939v2</guid></item><item><title>MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</title><link>http://arxiv.org/abs/2407.01523v2</link><description>Understanding documents with rich layouts and multi-modal components is along-standing and practical task. Recent Large Vision-Language Models (LVLMs)have made remarkable strides in various tasks, particularly in single-pagedocument understanding (DU). However, their abilities on long-context DU remainan open problem. This work presents MMLongBench-Doc, a long-context,multi-modal benchmark comprising 1,062 expert-annotated questions. Distinctfrom previous datasets, it is constructed upon 130 lengthy PDF-formatteddocuments with an average of 49.4 pages and 20,971 textual tokens. Towardscomprehensive evaluation, answers to these questions rely on pieces of evidencefrom (1) different sources (text, image, chart, table, and layout structure)and (2) various locations (i.e. page number). Moreover, 33.2% of the questionsare cross-page questions requiring evidence across multiple pages. 22.8% of thequestions are designed to be unanswerable for detecting potentialhallucinations. Experiments on 14 LVLMs demonstrate that long-context DUgreatly challenges current models. Notably, the best-performing model, GPT-4o,achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worseperformance than their LLM counterparts which are fed with lossy-parsed OCRdocuments. These results validate the necessity of future research toward morecapable long-context LVLMs. Project Page:https://mayubo2333.github.io/MMLongBench-Doc</description><author>Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun</author><pubDate>Wed, 10 Jul 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01523v2</guid></item><item><title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title><link>http://arxiv.org/abs/2403.01858v2</link><description>We present TMMLU+, a new benchmark designed for Traditional Chinese languageunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66subjects from elementary to professional level. It is six times larger andboasts a more balanced subject distribution than its predecessor, TaiwanMassive Multitask Language Understanding (TMMLU). We also benchmarkclosed-source models and 26 open-weight Chinese large language models (LLMs) ofparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings revealthat (1.) Traditional Chinese models still trail behind their SimplifiedChinese counterparts, highlighting a need for more focused advancements in LLMscatering to Traditional Chinese. (2.) Current LLMs still fall short of humanperformance in average scores, indicating a potential need for future researchto delve deeper into social science and humanities subjects. (3.) Among all thetokenization compression metrics examined, we identify that only the fertilityscore uniquely demonstrates strong correlations with our benchmark results. Weforesee that TMMLU+ will pinpoint areas for future model improvement, therebynarrowing the gap between machine and human linguistic capabilities andsupporting researchers in developing Traditional Chinese LLMs. Our dataset,along with the benchmark source code, is accessible athuggingface.co/datasets/ikala/tmmluplus.</description><author>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</author><pubDate>Wed, 10 Jul 2024 15:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01858v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</title><link>http://arxiv.org/abs/2407.07577v1</link><description>The rapid advancement of Large Vision-Language models (LVLMs) hasdemonstrated a spectrum of emergent capabilities. Nevertheless, current modelsonly focus on the visual content of a single scenario, while their ability toassociate instances across different scenes has not yet been explored, which isessential for understanding complex visual content, such as movies withmultiple characters and intricate plots. Towards movie understanding, acritical initial step for LVLMs is to unleash the potential of characteridentities memory and recognition across multiple visual scenarios. To achievethe goal, we propose visual instruction tuning with ID reference and develop anID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our researchintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory andrecognition across four dimensions: matching, location, question-answering, andcaptioning. Our findings highlight the limitations of existing LVLMs inrecognizing and associating instance identities with ID reference. This paperpaves the way for future artificial intelligence systems to possessmulti-identity visual inputs, thereby facilitating the comprehension of complexvisual narratives like movies.</description><author>Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo</author><pubDate>Wed, 10 Jul 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07577v1</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item><item><title>An Improved Traditional Chinese Evaluation Suite for Foundation Model</title><link>http://arxiv.org/abs/2403.01858v3</link><description>We present TMMLU+, a new benchmark designed for Traditional Chinese languageunderstanding. TMMLU+ is a multi-choice question-answering dataset with 66subjects from elementary to professional level. It is six times larger andboasts a more balanced subject distribution than its predecessor, TaiwanMassive Multitask Language Understanding (TMMLU). We also benchmarkclosed-source models and 26 open-weight Chinese large language models (LLMs) ofparameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings revealthat (1.) Traditional Chinese models still trail behind their SimplifiedChinese counterparts, highlighting a need for more focused advancements in LLMscatering to Traditional Chinese. (2.) Current LLMs still fall short of humanperformance in average scores, indicating a potential need for future researchto delve deeper into social science and humanities subjects. (3.) Among all thetokenization compression metrics examined, we identify that only the fertilityscore uniquely demonstrates strong correlations with our benchmark results. Weforesee that TMMLU+ will pinpoint areas for future model improvement, therebynarrowing the gap between machine and human linguistic capabilities andsupporting researchers in developing Traditional Chinese LLMs. Our dataset,along with the benchmark source code, is accessible athuggingface.co/datasets/ikala/tmmluplus.</description><author>Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Jun-Da Chen, Wei-Min Chu, Sega Cheng, Hong-Han Shuai</author><pubDate>Thu, 11 Jul 2024 14:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01858v3</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods</title><link>http://arxiv.org/abs/2407.06985v2</link><description>In domain-specific applications, GPT-4, augmented with precise prompts orRetrieval-Augmented Generation (RAG), shows notable potential but faces thecritical tri-lemma of performance, cost, and data privacy. High performancerequires sophisticated processing techniques, yet managing multiple agentswithin a complex workflow often proves costly and challenging. To address this,we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.This systematizes domain-specific tasks by integrating precise questiondecomposition, advanced information retrieval, comprehensive summarization, andrigorous self-assessment. Given the concerns of cost and data privacy,enterprises are shifting from proprietary models like GPT-4 to custom models,striking a balance between cost, security, and performance. We developedindustrial practices leveraging online data and user feedback for efficientmodel tuning. This study provides best practice guidelines for applyingmulti-agent systems in domain-specific problem-solving and implementingeffective agent tuning strategies. Our empirical studies, particularly in thefinancial question-answering domain, demonstrate that our approach achieves95.0% of GPT-4's performance, while effectively managing costs and ensuringdata privacy.</description><author>Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu</author><pubDate>Wed, 10 Jul 2024 03:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06985v2</guid></item><item><title>Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders</title><link>http://arxiv.org/abs/2402.00260v2</link><description>The robotic intervention for individuals with Autism Spectrum Disorder (ASD)has generally used pre-defined scripts to deliver verbal content duringone-to-one therapy sessions. This practice restricts the use of robots tolimited, pre-mediated instructional curricula. In this paper, we increase robotautonomy in one such robotic intervention for children with ASD by implementingperspective-taking teaching. Our approach uses large language models (LLM) togenerate verbal content as texts and then deliver it to the child via roboticspeech. In the proposed pipeline, we teach perspective-taking through which ourrobot takes up three roles: initiator, prompter, and reinforcer. We adopted theGPT-2 + BART pipelines to generate social situations, ask questions (asinitiator), and give options (as prompter) when required. The robot encouragesthe child by giving positive reinforcement for correct answers (as areinforcer). In addition to our technical contribution, we conducted ten-minutesessions with domain experts simulating an actual perspective teaching session,with the researcher acting as a child participant. These sessions validated ourrobotic intervention pipeline through surveys, including those from NASA TLXand GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with anall GPT-2 and found the performance of the former to be better. Based on theresponses by the domain experts, the robot session demonstrated higherperformance with no additional increase in mental or physical demand, temporaldemand, effort, or frustration compared to a no-robot session. We alsoconcluded that the domain experts perceived the robot as ideally safe, likable,and reliable.</description><author>Ruchik Mishra, Karla Conn Welch, Dan O Popa</author><pubDate>Wed, 10 Jul 2024 01:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00260v2</guid></item><item><title>FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding</title><link>http://arxiv.org/abs/2407.05183v2</link><description>Flowcharts are graphical tools for representing complex concepts in concisevisual representations. This paper introduces the FlowLearn dataset, a resourcetailored to enhance the understanding of flowcharts. FlowLearn contains complexscientific flowcharts and simulated flowcharts. The scientific subset contains3,858 flowcharts sourced from scientific literature and the simulated subsetcontains 10,000 flowcharts created using a customizable script. The dataset isenriched with annotations for visual components, OCR, Mermaid coderepresentation, and VQA question-answer pairs. Despite the proven capabilitiesof Large Vision-Language Models (LVLMs) in various visual understanding tasks,their effectiveness in decoding flowcharts - a crucial element of scientificcommunication - has yet to be thoroughly investigated. The FlowLearn test setis crafted to assess the performance of LVLMs in flowchart comprehension. Ourstudy thoroughly evaluates state-of-the-art LVLMs, identifying existinglimitations and establishing a foundation for future enhancements in thisrelatively underexplored domain. For instance, in tasks involving simulatedflowcharts, GPT-4V achieved the highest accuracy (58%) in counting the numberof nodes, while Claude recorded the highest accuracy (83%) in OCR tasks.Notably, no single model excels in all tasks within the FlowLearn framework,highlighting significant opportunities for further development.</description><author>Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, Longin Jan Latecki</author><pubDate>Tue, 09 Jul 2024 21:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05183v2</guid></item><item><title>Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison</title><link>http://arxiv.org/abs/2407.07840v1</link><description>Despite tremendous advancements, current state-of-the-art Vision-LanguageModels (VLMs) are still far from perfect. They tend to hallucinate and maygenerate biased responses. In such circumstances, having a way to assess thereliability of a given response generated by a VLM is quite useful. Existingmethods, such as estimating uncertainty using answer likelihoods orprompt-based confidence generation, often suffer from overconfidence. Othermethods use self-consistency comparison but are affected by confirmationbiases. To alleviate these, we propose \textbf{De}compose and \textbf{C}ompare\textbf{C}onsistency (\texttt{DeCC}) for reliability measurement. By comparingthe consistency between the direct answer generated using the VLM's internalreasoning process, and the indirect answers obtained by decomposing thequestion into sub-questions and reasoning over the sub-answers produced by theVLM, \texttt{DeCC} measures the reliability of VLM's direct answer. Experimentsacross six vision-language tasks with three VLMs show \texttt{DeCC}'sreliability estimation achieves better correlation with task accuracy comparedto the existing methods.</description><author>Qian Yang, Weixiang Yan, Aishwarya Agrawal</author><pubDate>Wed, 10 Jul 2024 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07840v1</guid></item><item><title>ConvNLP: Image-based AI Text Detection</title><link>http://arxiv.org/abs/2407.07225v1</link><description>The potentials of Generative-AI technologies like Large Language models(LLMs) to revolutionize education are undermined by ethical considerationsaround their misuse which worsens the problem of academic dishonesty. LLMs likeGPT-4 and Llama 2 are becoming increasingly powerful in generatingsophisticated content and answering questions, from writing academic essays tosolving complex math problems. Students are relying on these LLMs to completetheir assignments and thus compromising academic integrity. Solutions to detectLLM-generated text are compute-intensive and often lack generalization. Thispaper presents a novel approach for detecting LLM-generated AI-text using avisual representation of word embedding. We have formulated a novelConvolutional Neural Network called ZigZag ResNet, as well as a scheduler forimproving generalization, named ZigZag Scheduler. Through extensive evaluationusing datasets of text generated by six different state-of-the-art LLMs, ourmodel demonstrates strong intra-domain and inter-domain generalizationcapabilities. Our best model detects AI-generated text with an impressiveaverage detection rate (over inter- and intra-domain test data) of 88.35%.Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Schedulerprovide a performance improvement of nearly 4% over the vanilla ResNet. Theend-to-end inference latency of our model is below 2.5ms per sentence. Oursolution offers a lightweight, computationally efficient, and fasteralternative to existing tools for AI-generated text detection, with bettergeneralization performance. It can help academic institutions in their fightagainst the misuse of LLMs in academic settings. Through this work, we aim tocontribute to safeguarding the principles of academic integrity and ensuringthe trustworthiness of student work in the era of advanced LLMs.</description><author>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube</author><pubDate>Tue, 09 Jul 2024 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07225v1</guid></item><item><title>Ramsey Theorems for Trees and a General 'Private Learning Implies Online Learning' Theorem</title><link>http://arxiv.org/abs/2407.07765v1</link><description>This work continues to investigate the link between differentially private(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed thatfor binary concept classes, DP learnability of a given class implies that ithas a finite Littlestone dimension (equivalently, that it is online learnable).Their proof relies on a model-theoretic result by Hodges (1997), whichdemonstrates that any binary concept class with a large Littlestone dimensioncontains a large subclass of thresholds. In a follow-up work, Jung, Kim, andTewari (2020) extended this proof to multiclass PAC learning with a boundednumber of labels. Unfortunately, Hodges's result does not apply in othernatural settings such as multiclass PAC learning with an unbounded label space,and PAC learning of partial concept classes. This naturally raises the question of whether DP learnability continues toimply online learnability in more general scenarios: indeed, Alon, Hanneke,Holzman, and Moran (2021) explicitly leave it as an open question in thecontext of partial concept classes, and the same question is open in thegeneral multiclass setting. In this work, we give a positive answer to thesequestions showing that for general classification tasks, DP learnabilityimplies online learnability. Our proof reasons directly about Littlestonetrees, without relying on thresholds. We achieve this by establishing severalRamsey-type theorems for trees, which might be of independent interest.</description><author>Simone Fioravanti, Steve Hanneke, Shay Moran, Hilla Schefler, Iska Tsubari</author><pubDate>Wed, 10 Jul 2024 15:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07765v1</guid></item></channel></rss>