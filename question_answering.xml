<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 27 Oct 2024 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</title><link>http://arxiv.org/abs/2310.10942v6</link><description>Teaching Visual Question Answering (VQA) models to refrain from answeringunanswerable questions is necessary for building a trustworthy AI system.Existing studies, though have explored various aspects of VQA but somewhatignored this particular attribute. This paper aims to bridge the research gapby contributing a comprehensive dataset, called UNK-VQA. The dataset isspecifically designed to address the challenge of questions that models do notknow. To this end, we first augment the existing data via deliberateperturbations on either the image or question. In specific, we carefully ensurethat the question-image semantics remain close to the original unperturbeddistribution. By this means, the identification of unanswerable questionsbecomes challenging, setting our dataset apart from others that involve mereimage replacement. We then extensively evaluate the zero- and few-shotperformance of several emerging multi-modal large models and discover theirsignificant limitations when applied to our dataset. Additionally, we alsopropose a straightforward method to tackle these unanswerable questions. Thisdataset, we believe, will serve as a valuable benchmark for enhancing theabstention capability of VQA models, thereby leading to increasedtrustworthiness of AI systems. We have made the dataset(https://github.com/guoyang9/UNK-VQA) available to facilitate furtherexploration in this area.</description><author>Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</author><pubDate>Wed, 21 Aug 2024 06:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10942v6</guid></item><item><title>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</title><link>http://arxiv.org/abs/2310.10942v5</link><description>Teaching Visual Question Answering (VQA) models to refrain from answeringunanswerable questions is necessary for building a trustworthy AI system.Existing studies, though have explored various aspects of VQA but somewhatignored this particular attribute. This paper aims to bridge the research gapby contributing a comprehensive dataset, called UNK-VQA. The dataset isspecifically designed to address the challenge of questions that models do notknow. To this end, we first augment the existing data via deliberateperturbations on either the image or question. In specific, we carefully ensurethat the question-image semantics remain close to the original unperturbeddistribution. By this means, the identification of unanswerable questionsbecomes challenging, setting our dataset apart from others that involve mereimage replacement. We then extensively evaluate the zero- and few-shotperformance of several emerging multi-modal large models and discover theirsignificant limitations when applied to our dataset. Additionally, we alsopropose a straightforward method to tackle these unanswerable questions. Thisdataset, we believe, will serve as a valuable benchmark for enhancing theabstention capability of VQA models, thereby leading to increasedtrustworthiness of AI systems. We have made the dataset(https://github.com/guoyang9/UNK-VQA) available to facilitate furtherexploration in this area.</description><author>Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</author><pubDate>Sun, 11 Aug 2024 13:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10942v5</guid></item><item><title>Mamba Fusion: Learning Actions Through Questioning</title><link>http://arxiv.org/abs/2409.11513v1</link><description>Video Language Models (VLMs) are crucial for generalizing across diversetasks and using language cues to enhance learning. While transformer-basedarchitectures have been the de facto in vision-language training, they facechallenges like quadratic computational complexity, high GPU memory usage, anddifficulty with long-term dependencies. To address these limitations, weintroduce MambaVL, a novel model that leverages recent advancements inselective state space modality fusion to efficiently capture long-rangedependencies and learn joint representations for vision and language data.MambaVL utilizes a shared state transition matrix across both modalities,allowing the model to capture information about actions from multipleperspectives within the scene. Furthermore, we propose a question-answeringtask that helps guide the model toward relevant cues. These questions providecritical information about actions, objects, and environmental context, leadingto enhanced performance. As a result, MambaVL achieves state-of-the-artperformance in action recognition on the Epic-Kitchens-100 dataset andoutperforms baseline methods in action anticipation.</description><author>Zhikang Dong, Apoorva Beedu, Jason Sheinkopf, Irfan Essa</author><pubDate>Tue, 17 Sep 2024 19:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11513v1</guid></item><item><title>Precise Model Benchmarking with Only a Few Observations</title><link>http://arxiv.org/abs/2410.05222v1</link><description>How can we precisely estimate a large language model's (LLM) accuracy onquestions belonging to a specific topic within a larger question-answeringdataset? The standard direct estimator, which averages the model's accuracy onthe questions in each subgroup, may exhibit high variance for subgroups(topics) with small sample sizes. Synthetic regression modeling, whichleverages the model's accuracy on questions about other topics, may yieldbiased estimates that are too unreliable for large subgroups. We prescribe asimple yet effective solution: an empirical Bayes (EB) estimator that balancesdirect and regression estimates for each subgroup separately, improving theprecision of subgroup-level estimates of model performance. Our experiments onmultiple datasets show that this approach consistently provides more preciseestimates of the LLM performance compared to the direct and regressionapproaches, achieving substantial reductions in the mean squared error.Confidence intervals for EB estimates also have near-nominal coverage and arenarrower compared to those for the direct estimator. Additional experiments ontabular and vision data validate the benefits of this EB approach.</description><author>Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort</author><pubDate>Mon, 07 Oct 2024 17:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05222v1</guid></item><item><title>Which questions should I answer? Salience Prediction of Inquisitive Questions</title><link>http://arxiv.org/abs/2404.10917v2</link><description>Inquisitive questions -- open-ended, curiosity-driven questions people ask asthey read -- are an integral part of discourse processing (Kehler and Rohde,2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP hastaken advantage of question generation capabilities of LLMs to enhance a widerange of applications. But the space of inquisitive questions is vast: manyquestions can be evoked from a given context. So which of those should beprioritized to find answers? Linguistic theories, unfortunately, have not yetprovided an answer to this question. This paper presents QSALIENCE, a saliencepredictor of inquisitive questions. QSALIENCE is instruction-tuned over ourdataset of linguist-annotated salience scores of 1,766 (context, question)pairs. A question scores high on salience if answering it would greatly enhancethe understanding of the text (Van Rooy, 2003). We show that highly salientquestions are empirically more likely to be answered in the same article,bridging potential questions (Onea, 2016) with Questions Under Discussion(Roberts, 2012). We further validate our findings by showing that answeringsalient questions is an indicator of summarization quality in news.</description><author>Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li</author><pubDate>Thu, 03 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10917v2</guid></item><item><title>RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension</title><link>http://arxiv.org/abs/2407.07321v1</link><description>Large Language Models (LLMs) have been applied to many research problemsacross various domains. One of the applications of LLMs is providingquestion-answering systems that cater to users from different fields. Theeffectiveness of LLM-based question-answering systems has already beenestablished at an acceptable level for users posing questions in popular andpublic domains such as trivia and literature. However, it has not often beenestablished in niche domains that traditionally require specialized expertise.To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performanceof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answeringquestions originating from Environmental Impact Statements prepared by U.S.federal government agencies in accordance with the National EnvironmentalEnvironmental Act (NEPA). We specifically measure the ability of LLMs tounderstand the nuances of legal, technical, and compliance-related informationpresent in NEPA documents in different contextual scenarios. For example, wetest the LLMs' internal prior NEPA knowledge by providing questions without anycontext, as well as assess how LLMs synthesize the contextual informationpresent in long NEPA documents to facilitate the question/answering task. Wecompare the performance of the long context LLMs and RAG powered models inhandling different types of questions (e.g., problem-solving, divergent). Ourresults suggest that RAG powered models significantly outperform the longcontext models in the answer accuracy regardless of the choice of the frontierLLM. Our further analysis reveals that many models perform better answeringclosed questions than divergent and problem-solving questions.</description><author>Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana</author><pubDate>Wed, 10 Jul 2024 02:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07321v1</guid></item><item><title>INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages</title><link>http://arxiv.org/abs/2407.13522v1</link><description>Large Language Models (LLMs) have demonstrated remarkable zero-shot andfew-shot capabilities in unseen tasks, including context-grounded questionanswering (QA) in English. However, the evaluation of LLMs' capabilities innon-English languages for context-based QA is limited by the scarcity ofbenchmarks in non-English languages. To address this gap, we introduceIndic-QA, the largest publicly available context-grounded question-answeringdataset for 11 major Indian languages from two language families. The datasetcomprises both extractive and abstractive question-answering tasks and includesexisting datasets as well as English QA datasets translated into Indianlanguages. Additionally, we generate a synthetic dataset using the Gemini modelto create question-answer pairs given a passage, which is then manuallyverified for quality assurance. We evaluate various multilingual Large LanguageModels and their instruction-fine-tuned variants on the benchmark and observethat their performance is subpar, particularly for low-resource languages. Wehope that the release of this dataset will stimulate further research on thequestion-answering abilities of LLMs for low-resource languages.</description><author>Abhishek Kumar Singh, Rudra Murthy, Vishwajeet kumar, Jaydeep Sen, Ganesh Ramakrishnan</author><pubDate>Thu, 18 Jul 2024 13:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13522v1</guid></item><item><title>Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs</title><link>http://arxiv.org/abs/2410.11437v1</link><description>Multimodal Large Language Models (MLLMs) demonstrate a strong understandingof the real world and can even handle complex tasks. However, they still failon some straightforward visual question-answering (VQA) problems. This paperdives deeper into this issue, revealing that models tend to err when answeringeasy questions (e.g. Yes/No questions) about an image, even though they cancorrectly describe it. We refer to this model behavior discrepancy betweendifficult and simple questions as model laziness. To systematically investigatemodel laziness, we manually construct LazyBench, a benchmark that includesYes/No, multiple choice, short answer questions, and image description tasksthat are related to the same subjects in the images. Based on LazyBench, weobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,Gemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced onstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and findthat about half of its failure cases are caused by model laziness, whichfurther highlights the importance of ensuring that the model fully utilizes itscapability. To this end, we conduct preliminary exploration on how to mitigatelaziness and find that chain of thought (CoT) can effectively address thisissue.</description><author>Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He</author><pubDate>Tue, 15 Oct 2024 09:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11437v1</guid></item><item><title>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation</title><link>http://arxiv.org/abs/2405.09939v2</link><description>We introduce SciQAG, a novel framework for automatically generatinghigh-quality science question-answer pairs from a large corpus of scientificliterature based on large language models (LLMs). SciQAG consists of a QAgenerator and a QA evaluator, which work together to extract diverse andresearch-level questions and answers from scientific papers. Utilizing thisframework, we construct a large-scale, high-quality, open-ended science QAdataset containing 188,042 QA pairs extracted from 22,743 scientific papersacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmarktask designed to evaluate the science question-answering ability of LLMs.Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG datasetsignificantly improves their performance on both open-ended question answeringand scientific tasks. To foster research and collaboration, we make thedatasets, models, and evaluation codes publicly available, contributing to theadvancement of science question answering and developing more interpretable andreasoning-capable AI systems.</description><author>Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster</author><pubDate>Wed, 10 Jul 2024 01:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09939v2</guid></item><item><title>Decomposed Prompting to Answer Questions on a Course Discussion Board</title><link>http://arxiv.org/abs/2407.21170v1</link><description>We propose and evaluate a question-answering system that uses decomposedprompting to classify and answer student questions on a course discussionboard. Our system uses a large language model (LLM) to classify questions intoone of four types: conceptual, homework, logistics, and not answerable. Thisenables us to employ a different strategy for answering questions that fallunder different types. Using a variant of GPT-3, we achieve $81\%$classification accuracy. We discuss our system's performance on answeringconceptual questions from a machine learning course and various failure modes.</description><author>Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang</author><pubDate>Tue, 30 Jul 2024 20:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21170v1</guid></item><item><title>From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing</title><link>http://arxiv.org/abs/2409.16089v1</link><description>Face Recognition (FR) has advanced significantly with the development of deeplearning, achieving high accuracy in several applications. However, the lack ofinterpretability of these systems raises concerns about their accountability,fairness, and reliability. In the present study, we propose an interactiveframework to enhance the explainability of FR models by combiningmodel-agnostic Explainable Artificial Intelligence (XAI) and Natural LanguageProcessing (NLP) techniques. The proposed framework is able to accuratelyanswer various questions of the user through an interactive chatbot. Inparticular, the explanations generated by our proposed method are in the formof natural language text and visual representations, which for example candescribe how different facial regions contribute to the similarity measurebetween two faces. This is achieved through the automatic analysis of theoutput's saliency heatmaps of the face images and a BERT question-answeringmodel, providing users with an interface that facilitates a comprehensiveunderstanding of the FR decisions. The proposed approach is interactive,allowing the users to ask questions to get more precise information based onthe user's background knowledge. More importantly, in contrast to previousstudies, our solution does not decrease the face recognition performance. Wedemonstrate the effectiveness of the method through different experiments,highlighting its potential to make FR systems more interpretable anduser-friendly, especially in sensitive applications where decision-makingtransparency is crucial.</description><author>Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp Terhörst</author><pubDate>Tue, 24 Sep 2024 13:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16089v1</guid></item><item><title>Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering</title><link>http://arxiv.org/abs/2403.19631v2</link><description>Large Language Models (LLMs) have shown proficiency in question-answeringtasks but often struggle to integrate real-time knowledge, leading topotentially outdated or inaccurate responses. This problem becomes even morechallenging when dealing with multi-hop questions, since they require LLMs toupdate and integrate multiple knowledge pieces relevant to the questions. Totackle the problem, we propose the Retrieval-Augmented model Editing (RAE)framework for multi-hop question answering. RAE first retrieves edited factsand then refines the language model through in-context learning. Specifically,our retrieval approach, based on mutual information maximization, leverages thereasoning abilities of LLMs to identify chain facts that traditionalsimilarity-based searches might miss. In addition, our framework includes apruning strategy to eliminate redundant information from the retrieved facts,which enhances the editing accuracy and mitigates the hallucination problem.Our framework is supported by theoretical justification for its fact retrievalefficacy. Finally, comprehensive evaluation across various LLMs validates RAE'sability in providing accurate answers with updated knowledge. Our code isavailable at: https://github.com/sycny/RAE.</description><author>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</author><pubDate>Tue, 13 Aug 2024 19:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19631v2</guid></item><item><title>Extracting Emotion Phrases from Tweets using BART</title><link>http://arxiv.org/abs/2403.14050v3</link><description>Sentiment analysis is a natural language processing task that aims toidentify and extract the emotional aspects of a text. However, many existingsentiment analysis methods primarily classify the overall polarity of a text,overlooking the specific phrases that convey sentiment. In this paper, weapplied an approach to sentiment analysis based on a question-answeringframework. Our approach leverages the power of Bidirectional AutoregressiveTransformer (BART), a pre-trained sequence-to-sequence model, to extract aphrase from a given text that amplifies a given sentiment polarity. We create anatural language question that identifies the specific emotion to extract andthen guide BART to pay attention to the relevant emotional cues in the text. Weuse a classifier within BART to predict the start and end positions of theanswer span within the text, which helps to identify the precise boundaries ofthe extracted emotion phrase. Our approach offers several advantages over mostsentiment analysis studies, including capturing the complete context andmeaning of the text and extracting precise token spans that highlight theintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</description><author>Mahdi Rezapour</author><pubDate>Sat, 27 Jul 2024 17:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14050v3</guid></item><item><title>Q-Bench+: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs</title><link>http://arxiv.org/abs/2402.07116v2</link><description>The rapid development of Multi-modality Large Language Models (MLLMs) hasnavigated a paradigm shift in computer vision, moving towards versatilefoundational models. However, evaluating MLLMs in low-level visual perceptionand understanding remains a yet-to-explore domain. To this end, we designbenchmark settings to emulate human language responses related to low-levelvision: the low-level visual perception (A1) via visual question answeringrelated to low-level attributes (e.g. clarity, lighting); and the low-levelvisual description (A2), on evaluating MLLMs for low-level text descriptions.Furthermore, given that pairwise comparison can better avoid ambiguity ofresponses and has been adopted by many human experiments, we further extend thelow-level perception-related question-answering and description evaluations ofMLLMs from single images to image pairs. Specifically, for perception (A1), wecarry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999image pairs each accompanied by an open-ended question about its low-levelfeatures; for description (A2), we propose the LLDescribe+ dataset, evaluatingMLLMs for low-level descriptions on 499 single images and 450 pairs.Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predictingscore, by employing a softmax-based approach to enable all MLLMs to generatequantifiable quality ratings, tested against human opinions in 7 image qualityassessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate thatseveral MLLMs have decent low-level visual competencies on single images, butonly GPT-4V exhibits higher accuracy on pairwise comparisons than single imageevaluations (like humans). We hope that our benchmark will motivate furtherresearch into uncovering and enhancing these nascent capabilities of MLLMs.Datasets will be available at https://github.com/Q-Future/Q-Bench.</description><author>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin</author><pubDate>Sat, 10 Aug 2024 04:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07116v2</guid></item><item><title>Uncertainty Estimation of Large Language Models in Medical Question Answering</title><link>http://arxiv.org/abs/2407.08662v1</link><description>Large Language Models (LLMs) show promise for natural language generation inhealthcare, but risk hallucinating factually incorrect information. DeployingLLMs for medical question answering necessitates reliable uncertaintyestimation (UE) methods to detect hallucinations. In this work, we benchmarkpopular UE methods with different model sizes on medical question-answeringdatasets. Our results show that current approaches generally perform poorly inthis domain, highlighting the challenge of UE for medical applications. We alsoobserve that larger models tend to yield better results, suggesting acorrelation between model size and the reliability of UE. To address thesechallenges, we propose Two-phase Verification, a probability-free UncertaintyEstimation approach. First, an LLM generates a step-by-step explanationalongside its initial answer, followed by formulating verification questions tocheck the factual claims in the explanation. The model then answers thesequestions twice: first independently, and then referencing the explanation.Inconsistencies between the two sets of answers measure the uncertainty in theoriginal response. We evaluate our approach on three biomedicalquestion-answering datasets using Llama 2 Chat models and compare it againstthe benchmarked baseline methods. The results show that our Two-phaseVerification method achieves the best overall accuracy and stability acrossvarious datasets and model sizes, and its performance scales as the model sizeincreases.</description><author>Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou</author><pubDate>Thu, 11 Jul 2024 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08662v1</guid></item><item><title>Meta-Analysis with Untrusted Data</title><link>http://arxiv.org/abs/2407.09387v1</link><description>[See paper for full abstract] Meta-analysis is a crucial tool for answeringscientific questions. It is usually conducted on a relatively small amount of``trusted'' data -- ideally from randomized, controlled trials -- which allowcausal effects to be reliably estimated with minimal assumptions. We show howto answer causal questions much more precisely by making two changes. First, weincorporate untrusted data drawn from large observational databases, relatedscientific literature and practical experience -- without sacrificing rigor orintroducing strong assumptions. Second, we train richer models capable ofhandling heterogeneous trials, addressing a long-standing challenge inmeta-analysis. Our approach is based on conformal prediction, whichfundamentally produces rigorous prediction intervals, but doesn't handleindirect observations: in meta-analysis, we observe only noisy effects due tothe limited number of participants in each trial. To handle noise, we develop asimple, efficient version of fully-conformal kernel ridge regression, based ona novel condition called idiocentricity. We introduce noise-correcting terms inthe residuals and analyze their interaction with a ``variance shaving''technique. In multiple experiments on healthcare datasets, our algorithmsdeliver tighter, sounder intervals than traditional ones. This paper charts anew course for meta-analysis and evidence-based medicine, where heterogeneityand untrusted data are embraced for more nuanced and precise predictions.</description><author>Shiva Kaul, Geoffrey J. Gordon</author><pubDate>Fri, 12 Jul 2024 16:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09387v1</guid></item><item><title>Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</title><link>http://arxiv.org/abs/2406.06399v3</link><description>We study the limitations of Large Language Models (LLMs) for the task ofresponse generation in human-machine dialogue. Several techniques have beenproposed in the literature for different dialogue types (e.g., Open-Domain).However, the evaluations of these techniques have been limited in terms of baseLLMs, dialogue types and evaluation metrics. In this work, we extensivelyanalyze different LLM adaptation techniques when applied to different dialoguetypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialoguetypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.We evaluate the performance of in-context learning and fine-tuning techniquesacross datasets selected for each dialogue type. We assess the impact ofincorporating external knowledge to ground the generation in both scenarios ofRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistentevaluation and explainability criteria for automatic metrics and humanevaluation protocols. Our analysis shows that there is no universalbest-technique for adapting large language models as the efficacy of eachtechnique depends on both the base LLM and the specific type of dialogue. Lastbut not least, the assessment of the best adaptation technique should includehuman evaluation to avoid false expectations and outcomes derived fromautomatic metrics.</description><author>Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi</author><pubDate>Sat, 03 Aug 2024 15:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06399v3</guid></item><item><title>V-RoAst: A New Dataset for Visual Road Assessment</title><link>http://arxiv.org/abs/2408.10872v1</link><description>Road traffic crashes cause millions of deaths annually and have a significanteconomic impact, particularly in low- and middle-income countries (LMICs). Thispaper presents an approach using Vision Language Models (VLMs) for road safetyassessment, overcoming the limitations of traditional Convolutional NeuralNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answeringfor Road Assessment), with a real-world dataset. Our approach optimizes promptengineering and evaluates advanced VLMs, including Gemini-1.5-flash andGPT-4o-mini. The models effectively examine attributes for road assessment.Using crowdsourced imagery from Mapillary, our scalable solution influentiallyestimates road safety levels. In addition, this approach is designed for localstakeholders who lack resources, as it does not require training data. Itoffers a cost-effective and automated methods for global road safetyassessments, potentially saving lives and reducing economic burdens.</description><author>Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth</author><pubDate>Tue, 20 Aug 2024 14:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10872v1</guid></item><item><title>The representation landscape of few-shot learning and fine-tuning in large language models</title><link>http://arxiv.org/abs/2409.03662v1</link><description>In-context learning (ICL) and supervised fine-tuning (SFT) are two commonstrategies for improving the performance of modern large language models (LLMs)on specific tasks. Despite their different natures, these strategies often leadto comparable performance gains. However, little is known about whether theyinduce similar representations inside LLMs. We approach this problem byanalyzing the probability landscape of their hidden representations in the twocases. More specifically, we compare how LLMs solve the same question-answeringtask, finding that ICL and SFT create very different internal structures, inboth cases undergoing a sharp transition in the middle of the network. In thefirst half of the network, ICL shapes interpretable representationshierarchically organized according to their semantic content. In contrast, theprobability landscape obtained with SFT is fuzzier and semantically mixed. Inthe second half of the model, the fine-tuned representations developprobability modes that better encode the identity of answers, while thelandscape of ICL representations is characterized by less defined peaks. Ourapproach reveals the diverse computational strategies developed inside LLMs tosolve the same task across different conditions, allowing us to make a steptowards designing optimal methods to extract information from language models.</description><author>Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga</author><pubDate>Thu, 05 Sep 2024 16:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03662v1</guid></item><item><title>Object-Centric Temporal Consistency via Conditional Autoregressive Inductive Biases</title><link>http://arxiv.org/abs/2410.15728v1</link><description>Unsupervised object-centric learning from videos is a promising approachtowards learning compositional representations that can be applied to variousdownstream tasks, such as prediction and reasoning. Recently, it was shown thatpretrained Vision Transformers (ViTs) can be useful to learn object-centricrepresentations on real-world video datasets. However, while these approachessucceed at extracting objects from the scenes, the slot-based representationsfail to maintain temporal consistency across consecutive frames in a video,i.e. the mapping of objects to slots changes across the video. To address this,we introduce Conditional Autoregressive Slot Attention (CA-SA), a frameworkthat enhances the temporal consistency of extracted object-centricrepresentations in video-centric vision tasks. Leveraging an autoregressiveprior network to condition representations on previous timesteps and a novelconsistency loss function, CA-SA predicts future slot representations andimposes consistency across frames. We present qualitative and quantitativeresults showing that our proposed method outperforms the considered baselineson downstream tasks, such as video prediction and visual question-answeringtasks.</description><author>Cristian Meo, Akihiro Nakano, Mircea Lică, Aniket Didolkar, Masahiro Suzuki, Anirudh Goyal, Mengmi Zhang, Justin Dauwels, Yutaka Matsuo, Yoshua Bengio</author><pubDate>Mon, 21 Oct 2024 07:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15728v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v3</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Tue, 15 Oct 2024 20:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v3</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v1</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Nafis Irtiza Tripto, Reshma Lal Jagadheesh, Nian Yan</author><pubDate>Thu, 05 Sep 2024 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v1</guid></item><item><title>Enhanced Fine-Tuning of Lightweight Domain-Specific Q&amp;A Model Based on Large Language Models</title><link>http://arxiv.org/abs/2408.12247v2</link><description>Large language models (LLMs) excel at general question-answering (Q&amp;A) butoften fall short in specialized domains due to a lack of domain-specificknowledge. Commercial companies face the dual challenges of privacy protectionand resource constraints when involving LLMs for fine-tuning. This paperpropose a novel framework, Self-Evolution, designed to address these issues byleveraging lightweight open-source LLMs through multiple iterative fine-tuningrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolutionemploy a strategy that filters and reinforces the knowledge with higher valueduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chatusing 4,000 documents containing rich domain knowledge from China Mobile,achieving a performance score 174% higher on domain-specific question-answeringevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.Self-Evolution has been deployed in China Mobile's daily operation andmaintenance for 117 days, and it improves the efficiency of locating alarms,fixing problems, and finding related reports, with an average efficiencyimprovement of over 18.6%. In addition, we release Self-Evolution frameworkcode in https://github.com/Zero-Pointer/Self-Evolution.</description><author>Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jiagang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hua, Lin Zhu, Dan Pei</author><pubDate>Fri, 23 Aug 2024 01:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12247v2</guid></item><item><title>Internal and External Knowledge Interactive Refinement Framework for Knowledge-Intensive Question Answering</title><link>http://arxiv.org/abs/2408.12979v1</link><description>Recent works have attempted to integrate external knowledge into LLMs toaddress the limitations and potential factual errors in LLM-generated content.However, how to retrieve the correct knowledge from the large amount ofexternal knowledge imposes a challenge. To this end, we empirically observethat LLMs have already encoded rich knowledge in their pretrained parametersand utilizing these internal knowledge improves the retrieval of externalknowledge when applying them to knowledge-intensive tasks. In this paper, wepropose a new internal and external knowledge interactive refinement paradigmdubbed IEKR to utilize internal knowledge in LLM to help retrieve relevantknowledge from the external knowledge base, as well as exploit the externalknowledge to refine the hallucination of generated internal knowledge. Bysimply adding a prompt like 'Tell me something about' to the LLMs, we try toreview related explicit knowledge and insert them with the query into theretriever for external retrieval. The external knowledge is utilized tocomplement the internal knowledge into input of LLM for answers. We conductexperiments on 3 benchmark datasets in knowledge-intensive question answeringtask with different LLMs and domains, achieving the new state-of-the-art.Further analysis shows the effectiveness of different modules in our approach.</description><author>Haowei Du, Dongyan Zhao</author><pubDate>Fri, 23 Aug 2024 10:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12979v1</guid></item><item><title>On Bits and Bandits: Quantifying the Regret-Information Trade-off</title><link>http://arxiv.org/abs/2405.16581v3</link><description>In many sequential decision problems, an agent performs a repeated task. Hethen suffers regret and obtains information that he may use in the followingrounds. However, sometimes the agent may also obtain information and avoidsuffering regret by querying external sources. We study the trade-off betweenthe information an agent accumulates and the regret it suffers. We invokeinformation-theoretic methods for obtaining regret lower bounds, that alsoallow us to easily re-derive several known lower bounds. We introduce the firstBayesian regret lower bounds that depend on the information an agentaccumulates. We also prove regret upper bounds using the amount of informationthe agent accumulates. These bounds show that information measured in bits, canbe traded off for regret, measured in reward. Finally, we demonstrate theutility of these bounds in improving the performance of a question-answeringtask with large language models, allowing us to obtain valuable insights.</description><author>Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor</author><pubDate>Mon, 07 Oct 2024 13:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16581v3</guid></item><item><title>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</title><link>http://arxiv.org/abs/2410.08209v1</link><description>Current large multimodal models (LMMs) face challenges in grounding, whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision, we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding, we introduce an "attend-and-segment" method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMMutilizing a diffusion-based visual encoder, as opposed to the standard CLIPvisual encoder, and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata, our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.Notably, we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision, outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</description><author>Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Thu, 10 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08209v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v2</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Thu, 11 Jul 2024 19:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v2</guid></item><item><title>Efficient Multivariate Time Series Anomaly Detection Through Transfer Learning for Large-Scale Web services</title><link>http://arxiv.org/abs/2408.12247v1</link><description>Large language models (LLMs) excel at general question-answering (Q&amp;A) butoften fall short in specialized domains due to a lack of domain-specificknowledge. Commercial companies face the dual challenges of privacy protectionand resource constraints when involving LLMs for fine-tuning. This paperpropose a novel framework, Self-Evolution, designed to address these issues byleveraging lightweight open-source LLMs through multiple iterative fine-tuningrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolutionemploy a strategy that filters and reinforces the knowledge with higher valueduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chatusing 4,000 documents containing rich domain knowledge from China Mobile,achieving a performance score 174% higher on domain-specific question-answeringevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.Self-Evolution has been deployed in China Mobile's daily operation andmaintenance for 117 days, and it improves the efficiency of locating alarms,fixing problems, and finding related reports, with an average efficiencyimprovement of over 18.6%. In addition, we release Self-Evolution frameworkcode in https://github.com/Zero-Pointer/Self-Evolution.</description><author>Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jiagang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hua, Lin Zhu, Dan Pei</author><pubDate>Thu, 22 Aug 2024 09:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12247v1</guid></item><item><title>Contextual Object Detection with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2305.18279v2</link><description>Recent Multimodal Large Language Models (MLLMs) are remarkable invision-language tasks, such as image captioning and question answering, butlack the essential perception ability, i.e., object detection. In this work, weaddress this limitation by introducing a novel research problem of contextualobject detection -- understanding visible objects within different human-AIinteractive contexts. Three representative scenarios are investigated,including the language cloze test, visual captioning, and question answering.Moreover, we present ContextDET, a unified multimodal model that is capable ofend-to-end differentiable modeling of visual-language contexts, so as tolocate, identify, and associate visual objects with language inputs forhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visualencoder for extracting visual representations, (ii) a pre-trained LLM formultimodal context decoding, and (iii) a visual decoder for predicting boundingboxes given contextual object words. The new generate-then-detect frameworkenables us to detect object words within human vocabulary. Extensiveexperiments show the advantages of ContextDET on our proposed CODE benchmark,open-vocabulary detection, and referring image segmentation. Github:https://github.com/yuhangzang/ContextDET.</description><author>Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy</author><pubDate>Mon, 12 Aug 2024 07:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18279v2</guid></item><item><title>Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost</title><link>http://arxiv.org/abs/2407.19825v1</link><description>Today's large language models (LLMs) can solve challenging question-answeringtasks, and prompt engineering techniques, such as chain-of-thought (CoT), havegained attention for enhancing the explanation and correctness of outputs.Nevertheless, models require significant time to generate answers augmentedwith lengthy reasoning details. To address this issue, this paper analyzes theimpact of output lengths on LLM inference pipelines and proposes novel metricsto evaluate them in terms of \textit{correct conciseness}. It also examines theimpact of controlling output length through a refined prompt engineeringstrategy, Constrained-CoT (CCoT), which encourages the model to limit outputlength. Experiments on pre-trained LLMs demonstrated the benefit of theproposed metrics and the effectiveness of CCoT across different models. Forinstance, constraining the reasoning of LLaMA2-70b to 100 words improves theaccuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, whilereducing the average output length by 28 words.</description><author>Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli</author><pubDate>Mon, 29 Jul 2024 09:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19825v1</guid></item><item><title>UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</title><link>http://arxiv.org/abs/2407.18391v1</link><description>Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on parwith larger models in general-domain visual grounding and question-answeringbenchmarks while offering advantages in computational efficiency and storage.However, their ability to handle rare objects, which fall into the long tail ofdata distributions, is less understood. To rigorously evaluate this aspect, weintroduce the "Uncontextualized Uncommon Objects" (UOUO) benchmark. Thisbenchmark focuses on systematically testing VLMs with both large and smallparameter counts on rare and specialized objects. Our comprehensive analysisreveals that while smaller VLMs maintain competitive performance on commondatasets, they significantly underperform on tasks involving uncommon objects.We also propose an advanced, scalable pipeline for data collection andcleaning, ensuring the UOUO benchmark provides high-quality, challenginginstances. These findings highlight the need to consider long-taildistributions when assessing the true capabilities of VLMs.</description><author>Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</author><pubDate>Thu, 25 Jul 2024 20:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18391v1</guid></item><item><title>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</title><link>http://arxiv.org/abs/2410.14655v1</link><description>Language models are often trained to maximize the likelihood of the nexttoken given past tokens in the training dataset. However, during inferencetime, they are utilized differently, generating text sequentially andauto-regressively by using previously generated tokens as input to predict thenext one. Marginal differences in predictions at each step can cascade oversuccessive steps, resulting in different distributions from what the modelswere trained for and potentially leading to unpredictable behavior. This paperproposes two simple approaches based on model own generation to address thisdiscrepancy between the training and inference time. Our first approach isBatch-Scheduled Sampling, where, during training, we stochastically choosebetween the ground-truth token from the dataset and the model's own generatedtoken as input to predict the next token. This is done in an offline manner,modifying the context window by interleaving ground-truth tokens with thosegenerated by the model. Our second approach is Reference-Answer-basedCorrection, where we explicitly incorporate a self-correction capability intothe model during training. This enables the model to effectively self-correctthe gaps between the generated sequences and the ground truth data withoutrelying on an external oracle model. By incorporating our proposed strategiesduring training, we have observed an overall improvement in performancecompared to baseline methods, as demonstrated by our extensive experimentsusing summarization, general question-answering, and math question-answeringtasks.</description><author>Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhar, Huzefa Rangwala, George Karypis, Rasool Fakoor</author><pubDate>Fri, 18 Oct 2024 17:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14655v1</guid></item><item><title>Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks</title><link>http://arxiv.org/abs/2409.07353v1</link><description>Large Vision-Language Models (LVLMs), trained on multimodal big datasets,have significantly advanced AI by excelling in vision-language tasks. However,these models remain vulnerable to adversarial attacks, particularly jailbreakattacks, which bypass safety protocols and cause the model to generatemisleading or harmful responses. This vulnerability stems from both theinherent susceptibilities of LLMs and the expanded attack surface introduced bythe visual modality. We propose Sim-CLIP+, a novel defense mechanism thatadversarially fine-tunes the CLIP vision encoder by leveraging a Siamesearchitecture. This approach maximizes cosine similarity between perturbed andclean samples, facilitating resilience against adversarial manipulations.Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration intoexisting LVLM architectures as a robust vision encoder. Unlike previousdefenses, our method requires no structural modifications to the LVLM andincurs minimal computational overhead. Sim-CLIP+ demonstrates effectivenessagainst both gradient-based adversarial attacks and various jailbreaktechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attackstrategies and perform clean evaluations using standard downstream datasets,including COCO for image captioning and OKVQA for visual question answering.Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracywhile substantially improving robustness against both gradient-basedadversarial attacks and jailbreak techniques. Our code and robust visionencoders are available athttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.</description><author>Md Zarif Hossain, Ahmed Imteaj</author><pubDate>Wed, 11 Sep 2024 15:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07353v1</guid></item><item><title>V-RoAst: A New Dataset for Visual Road Assessment</title><link>http://arxiv.org/abs/2408.10872v2</link><description>Road traffic crashes cause millions of deaths annually and have a significanteconomic impact, particularly in low- and middle-income countries (LMICs). Thispaper presents an approach using Vision Language Models (VLMs) for road safetyassessment, overcoming the limitations of traditional Convolutional NeuralNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answeringfor Road Assessment), with a real-world dataset. Our approach optimizes promptengineering and evaluates advanced VLMs, including Gemini-1.5-flash andGPT-4o-mini. The models effectively examine attributes for road assessment.Using crowdsourced imagery from Mapillary, our scalable solution influentiallyestimates road safety levels. In addition, this approach is designed for localstakeholders who lack resources, as it does not require training data. Itoffers a cost-effective and automated methods for global road safetyassessments, potentially saving lives and reducing economic burdens.</description><author>Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth</author><pubDate>Wed, 21 Aug 2024 11:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10872v2</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v2</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan</author><pubDate>Fri, 06 Sep 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v2</guid></item><item><title>VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images</title><link>http://arxiv.org/abs/2408.16176v1</link><description>Images are increasingly becoming the currency for documenting biodiversity onthe planet, providing novel opportunities for accelerating scientificdiscoveries in the field of organismal biology, especially with the advent oflarge vision-language models (VLMs). We ask if pre-trained VLMs can aidscientists in answering a range of biologically relevant questions without anyadditional fine-tuning. In this paper, we evaluate the effectiveness of 12state-of-the-art (SOTA) VLMs in the field of organismal biology using a noveldataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K imagesfrom three groups of organisms: fishes, birds, and butterflies, covering fivebiologically relevant tasks. We also explore the effects of applying promptingtechniques and tests for reasoning hallucination on the performance of VLMs,shedding new light on the capabilities of current SOTA VLMs in answeringbiologically relevant questions using images. The code and datasets for runningall the analyses reported in this paper can be found athttps://github.com/sammarfy/VLM4Bio.</description><author>M. Maruf, Arka Daw, Kazi Sajeed Mehrab, Harish Babu Manogaran, Abhilash Neog, Medha Sawhney, Mridul Khurana, James P. Balhoff, Yasin Bakis, Bahadir Altintas, Matthew J. Thompson, Elizabeth G. Campolongo, Josef C. Uyeda, Hilmar Lapp, Henry L. Bart, Paula M. Mabee, Yu Su, Wei-Lun Chao, Charles Stewart, Tanya Berger-Wolf, Wasila Dahdul, Anuj Karpatne</author><pubDate>Wed, 28 Aug 2024 23:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16176v1</guid></item><item><title>Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant</title><link>http://arxiv.org/abs/2409.02864v1</link><description>We present a prototype for a Bioinformatics Retrieval Augmentation Data(BRAD) digital assistant. BRAD integrates a suite of tools to handle a widerange of bioinformatics tasks, from code execution to online search. Wedemonstrate BRAD's capabilities through (1) improved question-and-answeringwith retrieval augmented generation (RAG), (2) BRAD's ability to run and writecomplex software pipelines, and (3) BRAD's ability to organize and distributetasks across individual and teams of agents. We use BRAD for automation ofbioinformatics workflows, performing tasks ranging from gene enrichment andsearching the archive to automatic code generation and running biomarkeridentification pipelines. BRAD is a step toward the ultimate goal to develop adigital twin of laboratories driven by self-contained loops for hypothesisgeneration and testing of digital biology experiments.</description><author>Joshua Pickard, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Nicholas Galioto, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse</author><pubDate>Wed, 04 Sep 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02864v1</guid></item><item><title>DataVisT5: A Pre-trained Language Model for Jointly Understanding Text and Data Visualization</title><link>http://arxiv.org/abs/2408.07401v1</link><description>Data visualization (DV) is the fundamental and premise tool to improve theefficiency in conveying the insights behind the big data, which has been widelyaccepted in existing data-driven world. Task automation in DV, such asconverting natural language queries to visualizations (i.e., text-to-vis),generating explanations from visualizations (i.e., vis-to-text), answeringDV-related questions in free form (i.e. FeVisQA), and explicating tabular data(i.e., table-to-text), is vital for advancing the field. Despite theirpotential, the application of pre-trained language models (PLMs) like T5 andBERT in DV has been limited by high costs and challenges in handlingcross-modal information, leading to few studies on PLMs for DV. We introduce\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5architecture through a hybrid objective pre-training and multi-task fine-tuningstrategy, integrating text and DV datasets to effectively interpret cross-modalsemantics. Extensive evaluations on public datasets show that DataVisT5consistently outperforms current state-of-the-art models on various DV-relatedtasks. We anticipate that DataVisT5 will not only inspire further research onvertical PLMs but also expand the range of applications for PLMs.</description><author>Zhuoyue Wan, Yuanfeng Song, Shuaimin Li, Chen Jason Zhang, Raymond Chi-Wing Wong</author><pubDate>Wed, 14 Aug 2024 09:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07401v1</guid></item><item><title>Thinking LLMs: General Instruction Following with Thought Generation</title><link>http://arxiv.org/abs/2410.10630v1</link><description>LLMs are typically trained to answer user questions or follow instructionssimilarly to how human experts respond. However, in the standard alignmentframework they lack the basic ability of explicit thinking before answering.Thinking is important for complex questions that require reasoning and planning-- but can be applied to any task. We propose a training method for equippingexisting LLMs with such thinking abilities for general instruction followingwithout use of additional human data. We achieve this by an iterative searchand optimization procedure that explores the space of possible thoughtgenerations, allowing the model to learn how to think without directsupervision. For each instruction, the thought candidates are scored using ajudge model to evaluate their responses only, and then optimized via preferenceoptimization. We show that this procedure leads to superior performance onAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoningcategories such as marketing, health and general knowledge, in addition to moretraditional reasoning &amp; problem-solving tasks.</description><author>Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Mon, 14 Oct 2024 15:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10630v1</guid></item><item><title>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</title><link>http://arxiv.org/abs/2407.12735v1</link><description>Knowledge-based Visual Question Answering (KVQA) tasks require answeringquestions about images using extensive background knowledge. Despitesignificant advancements, generative models often struggle with these tasks dueto the limited integration of external knowledge. In this paper, we introduceEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) frameworkthat enables large language models (LLMs) to answer visual questions requiringfine-grained encyclopedic knowledge. To strive for high-performing retrieval,EchoSight first searches wiki articles by using visual-only information,subsequently, these candidate articles are further reranked according to theirrelevance to the combined text-image query. This approach significantlyimproves the integration of multimodal knowledge, leading to enhanced retrievaloutcomes and more accurate VQA responses. Our experimental results on theEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishesnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of41.8% on Encyclopedic VQA and 31.3% on InfoSeek.</description><author>Yibin Yan, Weidi Xie</author><pubDate>Wed, 17 Jul 2024 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12735v1</guid></item><item><title>Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method</title><link>http://arxiv.org/abs/2410.16788v1</link><description>Multi-Span Question Answering (MSQA) requires models to extract one ormultiple answer spans from a given context to answer a question. Prior workmainly focuses on designing specific methods or applying heuristic strategiesto encourage models to predict more correct predictions. However, these modelsare trained on gold answers and fail to consider the incorrect predictions.Through a statistical analysis, we observe that models with stronger abilitiesdo not predict less incorrect predictions compared with other models. In thiswork, we propose Answering-Classifying-Correcting (ACC) framework, whichemploys a post-processing strategy to handle incorrect predictions.Specifically, the ACC framework first introduces a classifier to classify thepredictions into three types and exclude "wrong predictions", then introduces acorrector to modify "partially correct predictions". Experiments on severalMSQA datasets show that ACC framework significantly improves the Exact Match(EM) scores, and further analysis demostrates that ACC framework efficientlyreduces the number of incorrect predictions, improving the quality ofpredictions.</description><author>Jiayi Lin, Chenyang Zhang, Haibo Tong, Dongyu Zhang, Qingqing Hong, Bingxuan Hou, Junli Wang</author><pubDate>Tue, 22 Oct 2024 08:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16788v1</guid></item><item><title>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</title><link>http://arxiv.org/abs/2406.10421v2</link><description>With the rapid development of Large Language Models (LLMs), it is crucial tohave benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such aswriting algorithms, querying databases or giving mathematical proofs. Inspiredby the way university students are evaluated on such tasks, in this paper, wepropose SciEx - a benchmark consisting of university computer science examquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)multilingual, containing both English and German exams, and (2) multi-modal,containing questions that involve images, and (3) contains various types offreeform questions with different difficulty levels, due to the nature ofuniversity exams. We evaluate the performance of various state-of-the-art LLMson our new benchmark. Since SciEx questions are freeform, it is notstraightforward to evaluate LLM performance. Therefore, we provide human expertgrading of the LLM outputs on SciEx. We show that the free-form exams in SciExremain challenging for the current LLMs, where the best LLM only achieves59.4\% exam grade on average. We also provide detailed comparisons between LLMperformance and student performance on SciEx. To enable future evaluation ofnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solvingthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation withexpert grading.</description><author>Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues</author><pubDate>Fri, 12 Jul 2024 10:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10421v2</guid></item><item><title>Inference Optimization of Foundation Models on AI Accelerators</title><link>http://arxiv.org/abs/2407.09111v1</link><description>Powerful foundation models, including large language models (LLMs), withTransformer architectures have ushered in a new era of Generative AI acrossvarious industries. Industry and research community have witnessed a largenumber of new applications, based on those foundation models. Such applicationsinclude question and answer, customer services, image and video generation, andcode completions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibitive inferencecosts and high latency in real-world scenarios. As a result, the demand forcost-effective and fast inference using AI accelerators is ever more higher. Tothis end, our tutorial offers a comprehensive discussion on complementaryinference optimization techniques using AI accelerators. Beginning with anoverview of basic Transformer architectures and deep learning systemframeworks, we deep dive into system optimization techniques for fast andmemory-efficient attention computations and discuss how they can be implementedefficiently on AI accelerators. Next, we describe architectural elements thatare key for fast transformer inference. Finally, we examine various modelcompression and fast decoding strategies in the same context.</description><author>Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis</author><pubDate>Fri, 12 Jul 2024 09:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09111v1</guid></item><item><title>ConvNLP: Image-based AI Text Detection</title><link>http://arxiv.org/abs/2407.07225v1</link><description>The potentials of Generative-AI technologies like Large Language models(LLMs) to revolutionize education are undermined by ethical considerationsaround their misuse which worsens the problem of academic dishonesty. LLMs likeGPT-4 and Llama 2 are becoming increasingly powerful in generatingsophisticated content and answering questions, from writing academic essays tosolving complex math problems. Students are relying on these LLMs to completetheir assignments and thus compromising academic integrity. Solutions to detectLLM-generated text are compute-intensive and often lack generalization. Thispaper presents a novel approach for detecting LLM-generated AI-text using avisual representation of word embedding. We have formulated a novelConvolutional Neural Network called ZigZag ResNet, as well as a scheduler forimproving generalization, named ZigZag Scheduler. Through extensive evaluationusing datasets of text generated by six different state-of-the-art LLMs, ourmodel demonstrates strong intra-domain and inter-domain generalizationcapabilities. Our best model detects AI-generated text with an impressiveaverage detection rate (over inter- and intra-domain test data) of 88.35%.Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Schedulerprovide a performance improvement of nearly 4% over the vanilla ResNet. Theend-to-end inference latency of our model is below 2.5ms per sentence. Oursolution offers a lightweight, computationally efficient, and fasteralternative to existing tools for AI-generated text detection, with bettergeneralization performance. It can help academic institutions in their fightagainst the misuse of LLMs in academic settings. Through this work, we aim tocontribute to safeguarding the principles of academic integrity and ensuringthe trustworthiness of student work in the era of advanced LLMs.</description><author>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube</author><pubDate>Tue, 09 Jul 2024 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07225v1</guid></item><item><title>Semi-Supervised Learning for Deep Causal Generative Models</title><link>http://arxiv.org/abs/2403.18717v2</link><description>Developing models that are capable of answering questions of the form "Howwould x change if y had been z?'" is fundamental to advancing medical imageanalysis. Training causal generative models that address such counterfactualquestions, though, currently requires that all relevant variables have beenobserved and that the corresponding labels are available in the training data.However, clinical data may not have complete records for all patients and stateof the art causal generative models are unable to take full advantage of this.We thus develop, for the first time, a semi-supervised deep causal generativemodel that exploits the causal relationships between variables to maximise theuse of all available data. We explore this in the setting where each sample iseither fully labelled or fully unlabelled, as well as the more clinicallyrealistic case of having different labels missing for each sample. We leveragetechniques from causal inference to infer missing values and subsequentlygenerate realistic counterfactuals, even for samples with incomplete labels.</description><author>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</author><pubDate>Fri, 12 Jul 2024 14:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18717v2</guid></item><item><title>DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection</title><link>http://arxiv.org/abs/2407.09283v1</link><description>Semantic role labeling (SRL) enriches many downstream applications, e.g.,machine translation, question answering, summarization, and stance/beliefdetection. However, building multilingual SRL models is challenging due to thescarcity of semantically annotated corpora for multiple languages. Moreover,state-of-the-art SRL projection (XSRL) based on large language models (LLMs)yields output that is riddled with spurious role labels. Remediation of suchhallucinations is not straightforward due to the lack of explainability ofLLMs. We show that hallucinated role labels are related to naturally occurringdivergence types that interfere with initial alignments. We implementDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraginglinguistically-informed alignment remediation followed by greedy First-ComeFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRLprojection without additional transformer-based machinery, beating XSRL in bothhuman and automatic comparisons, and advancing beyond headwords to accommodatephrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as ourground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt ourapproach to other language pairs (e.g., English-Tagalog).</description><author>Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie Dorr</author><pubDate>Fri, 12 Jul 2024 14:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09283v1</guid></item><item><title>FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding</title><link>http://arxiv.org/abs/2407.05183v2</link><description>Flowcharts are graphical tools for representing complex concepts in concisevisual representations. This paper introduces the FlowLearn dataset, a resourcetailored to enhance the understanding of flowcharts. FlowLearn contains complexscientific flowcharts and simulated flowcharts. The scientific subset contains3,858 flowcharts sourced from scientific literature and the simulated subsetcontains 10,000 flowcharts created using a customizable script. The dataset isenriched with annotations for visual components, OCR, Mermaid coderepresentation, and VQA question-answer pairs. Despite the proven capabilitiesof Large Vision-Language Models (LVLMs) in various visual understanding tasks,their effectiveness in decoding flowcharts - a crucial element of scientificcommunication - has yet to be thoroughly investigated. The FlowLearn test setis crafted to assess the performance of LVLMs in flowchart comprehension. Ourstudy thoroughly evaluates state-of-the-art LVLMs, identifying existinglimitations and establishing a foundation for future enhancements in thisrelatively underexplored domain. For instance, in tasks involving simulatedflowcharts, GPT-4V achieved the highest accuracy (58%) in counting the numberof nodes, while Claude recorded the highest accuracy (83%) in OCR tasks.Notably, no single model excels in all tasks within the FlowLearn framework,highlighting significant opportunities for further development.</description><author>Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, Longin Jan Latecki</author><pubDate>Tue, 09 Jul 2024 21:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05183v2</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item><item><title>NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?</title><link>http://arxiv.org/abs/2407.11963v1</link><description>In evaluating the long-context capabilities of large language models (LLMs),identifying content relevant to a user's query from original long documents isa crucial prerequisite for any LLM to answer questions based on long text. Wepresent NeedleBench, a framework consisting of a series of progressively morechallenging tasks for assessing bilingual long-context capabilities, spanningmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) anddifferent depth ranges, allowing the strategic insertion of critical datapoints in different text depth zones to rigorously test the retrieval andreasoning capabilities of models in diverse contexts. We use the NeedleBenchframework to assess how well the leading open-source models can identify keyinformation relevant to the question and apply that information to reasoning inbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge(ATC) to mimic the complexity of logical reasoning challenges that are likelyto be present in real-world long-context tasks, providing a simple method forevaluating LLMs in dealing with complex long-context situations. Our resultssuggest that current LLMs have significant room for improvement in practicallong-context applications, as they struggle with the complexity of logicalreasoning challenges that are likely to be present in real-world long-contexttasks. All codes and resources are available at OpenCompass:https://github.com/open-compass/opencompass.</description><author>Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen</author><pubDate>Tue, 16 Jul 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11963v1</guid></item><item><title>Teaching CORnet Human fMRI Representations for Enhanced Model-Brain Alignment</title><link>http://arxiv.org/abs/2407.10414v1</link><description>Deep convolutional neural networks (DCNNs) have demonstrated excellentperformance in object recognition and have been found to share somesimilarities with brain visual processing. However, the substantial gap betweenDCNNs and human visual perception still exists. Functional magnetic resonanceimaging (fMRI) as a widely used technique in cognitive neuroscience can recordneural activation in the human visual cortex during the process of visualperception. Can we teach DCNNs human fMRI signals to achieve a more brain-likemodel? To answer this question, this study proposed ReAlnet-fMRI, a model basedon the SOTA vision model CORnet but optimized using human fMRI data through amulti-layer encoding-based alignment framework. This framework has been shownto effectively enable the model to learn human brain representations. ThefMRI-optimized ReAlnet-fMRI exhibited higher similarity to the human brain thanboth CORnet and the control model in within-and across-subject as well aswithin- and across-modality model-brain (fMRI and EEG) alignment evaluations.Additionally, we conducted an in-depth analyses to investigate how the internalrepresentations of ReAlnet-fMRI differ from CORnet in encoding various objectdimensions. These findings provide the possibility of enhancing thebrain-likeness of visual models by integrating human neural data, helping tobridge the gap between computer vision and visual neuroscience.</description><author>Zitong Lu, Yile Wang</author><pubDate>Mon, 15 Jul 2024 03:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10414v1</guid></item><item><title>BatchPrompt: Accomplish more with less</title><link>http://arxiv.org/abs/2309.00384v3</link><description>As the ever-increasing token limits of large language models (LLMs) haveenabled long context as input, prompting with single data samples might nolonger an efficient way. A straightforward strategy improving efficiency is tobatch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4),which we call BatchPrompt. We have two initial observations for prompting withbatched data. First, we find that prompting with batched data in longercontexts will inevitably lead to worse performance, compared to single-dataprompting. Second, the performance of the language model is significantlycorrelated with the positions and order of the batched data, due to thecorresponding change in decoder context. To retain efficiency and overcomeperformance loss, we propose Batch Permutation and Ensembling (BPE), and anovel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensiveexperimental evaluation demonstrates that BPE can boost the performance ofBatchPrompt with a striking margin on a range of popular NLP tasks, includingquestion answering (Boolq), textual entailment (RTE), and duplicate questionsidentification (QQP). These performances are even competitive with/higher thansingle-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLMcalls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32,using just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5%to 91.1% with 30.8% tokens). To the best of our knowledge, this is the firstwork to technically improve prompting efficiency of large language models. Wehope our simple yet effective approach will shed light on the future researchof large language models. The code will be released.</description><author>Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham</author><pubDate>Mon, 15 Jul 2024 05:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00384v3</guid></item><item><title>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</title><link>http://arxiv.org/abs/2311.09821v2</link><description>Knowledge in the real world is being updated constantly. However, it iscostly to frequently update large language models (LLMs). Therefore, it iscrucial for LLMs to understand the concept of temporal knowledge. However,prior works on temporal question answering (TQA) did not emphasize multi-answerand multi-hop types of temporal reasoning. In this paper, we propose a complextemporal question-answering dataset Complex-TR that focuses on multi-answer andmulti-hop temporal reasoning. Besides, we also propose a novel dataaugmentation strategy to improve the complex temporal reasoning capability androbustness of LLMs. We conducted experiments on multiple temporal QA datasets.Experimental results show that our method is able to improve LLMs' performanceon temporal QA benchmarks by significant margins. Our code and data arereleased at: https://github.com/nusnlp/complex-tr.</description><author>Qingyu Tan, Hwee Tou Ng, Lidong Bing</author><pubDate>Fri, 12 Jul 2024 16:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09821v2</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>Language models are better than humans at next-token prediction</title><link>http://arxiv.org/abs/2212.11281v2</link><description>Current language models are considered to have sub-human capabilities atnatural language tasks like question-answering or writing code. However,language models are not trained to perform well at these tasks, they aretrained to accurately predict the next token given previous tokes in tokenizedtext. It is not clear whether language models are better or worse than humansat next token prediction. To try to answer this question, we performed twodistinct experiments to directly compare humans and language models on thisfront: one measuring top-1 accuracy and the other measuring perplexity. In bothexperiments, we find humans to be consistently \emph{worse} than evenrelatively small language models like GPT3-Ada at next-token prediction.</description><author>Buck Shlegeris, Fabien Roger, Lawrence Chan, Euan McLean</author><pubDate>Mon, 15 Jul 2024 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11281v2</guid></item><item><title>Backpropagation through space, time, and the brain</title><link>http://arxiv.org/abs/2403.16933v2</link><description>How physical networks of neurons, bound by spatio-temporal localityconstraints, can perform efficient credit assignment, remains, to a largeextent, an open question. In machine learning, the answer is almost universallygiven by the error backpropagation algorithm, through both space and time.However, this algorithm is well-known to rely on biologically implausibleassumptions, in particular with respect to spatio-temporal (non-)locality.Alternative forward-propagation models such as real-time recurrent learningonly partially solve the locality problem, but only at the cost of scaling, dueto prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational frameworkfor fully local spatio-temporal credit assignment in physical, dynamicalnetworks of neurons. We start by defining an energy based on neuron-localmismatches, from which we derive both neuronal dynamics via stationarity andparameter dynamics via gradient descent. The resulting dynamics can beinterpreted as a real-time, biologically plausible approximation ofbackpropagation through space and time in deep cortical networks withcontinuous-time neuronal dynamics and continuously active, local synapticplasticity. In particular, GLE exploits the morphology of dendritic trees toenable more complex information storage and processing in single neurons, aswell as the ability of biological neurons to phase-shift their output rate withrespect to their membrane potential, which is essential in both directions ofinformation propagation. For the forward computation, it enables the mapping oftime-continuous inputs to neuronal space, effectively performing aspatio-temporal convolution. For the backward computation, it permits thetemporal inversion of feedback signals, which consequently approximate theadjoint variables necessary for useful parameter updates.</description><author>Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici</author><pubDate>Tue, 16 Jul 2024 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16933v2</guid></item><item><title>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title><link>http://arxiv.org/abs/2407.09879v2</link><description>Despite the remarkable success of LLMs in English, there is a significant gapin performance in non-English languages. In order to address this, we introducea novel recipe for creating a multilingual synthetic instruction tuningdataset, sPhinX, which is created by selectively translating instructionresponse pairs from English into 50 languages. We test the effectiveness ofsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small andMistral-7B and then evaluating them across a comprehensive suite ofmultilingual benchmarks that test reasoning, question answering, and readingcomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned withsPhinX perform better on an average by 4.2%pt and 5%pt respectively as comparedto the baselines. We also devise a strategy to incorporate N-shot examples ineach fine-tuning sample which further boosts the performance of these models by3%pt and 10%pt respectively. Additionally, sPhinX also outperforms othermultilingual instruction tuning datasets on the same benchmarks along withbeing sample efficient and diverse, thereby reducing dataset creation costs.Additionally, instruction tuning with sPhinX does not lead to regression onmost standard LLM benchmarks.</description><author>Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram</author><pubDate>Tue, 16 Jul 2024 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09879v2</guid></item><item><title>Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering</title><link>http://arxiv.org/abs/2407.11930v1</link><description>Long-form question answering (LFQA) aims to provide thorough and in-depthanswers to complex questions, enhancing comprehension. However, such detailedresponses are prone to hallucinations and factual inconsistencies, challengingtheir faithful evaluation. This work introduces HaluQuestQA, the firsthallucination dataset with localized error annotations for human-written andmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7kspan-level error annotations for five different error types by expertannotators, along with preference judgments. Using our collected data, wethoroughly analyze the shortcomings of long-form answers and find that theylack comprehensiveness and provide unhelpful references. We train an automaticfeedback model on this dataset that predicts error spans with incompleteinformation and provides associated explanations. Finally, we propose aprompt-based approach, Error-informed refinement, that uses signals from thelearned feedback model to refine generated answers, which we show reduceshallucination and improves answer quality. Furthermore, humans find answersgenerated by our approach comprehensive and highly prefer them (84%) over thebaseline answers.</description><author>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</author><pubDate>Tue, 16 Jul 2024 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11930v1</guid></item><item><title>Towards a Benchmark for Causal Business Process Reasoning with LLMs</title><link>http://arxiv.org/abs/2406.05506v2</link><description>Large Language Models (LLMs) are increasingly used for boostingorganizational efficiency and automating tasks. While not originally designedfor complex cognitive processes, recent efforts have further extended to employLLMs in activities such as reasoning, planning, and decision-making. Inbusiness processes, such abilities could be invaluable for leveraging on themassive corpora LLMs have been trained on for gaining deep understanding ofsuch processes. In this work, we plant the seeds for the development of abenchmark to assess the ability of LLMs to reason about causal and processperspectives of business operations. We refer to this view asCausally-augmented Business Processes (BP^C). The core of the benchmarkcomprises a set of BP^C related situations, a set of questions about thesesituations, and a set of deductive rules employed to systematically resolve theground truth answers to these questions. Also with the power of LLMs, the seedis then instantiated into a larger-scale set of domain-specific situations andquestions. Reasoning on BP^C is of crucial importance for process interventionsand process improvement. Our benchmark, accessible athttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possiblemodalities: testing the performance of any target LLM and training an LLM toadvance its capability to reason about BP^C.</description><author>Fabiana Fournier, Lior Limonad, Inna Skarbovsky</author><pubDate>Tue, 16 Jul 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05506v2</guid></item><item><title>Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</title><link>http://arxiv.org/abs/2407.10151v2</link><description>Multi-object tracking (MOT) endeavors to precisely estimate the positions andidentities of multiple objects over time. The prevailing approach,tracking-by-detection (TbD), first detects objects and then links detections,resulting in a simple yet effective method. However, contemporary detectors mayoccasionally miss some objects in certain frames, causing trackers to ceasetracking prematurely. To tackle this issue, we propose BUSCA, meaning `tosearch', a versatile framework compatible with any online TbD system, enhancingits ability to persistently track those objects missed by the detector,primarily due to occlusions. Remarkably, this is accomplished without modifyingpast tracking results or accessing future frames, i.e., in a fully onlinemanner. BUSCA generates proposals based on neighboring tracks, motion, andlearned tokens. Utilizing a decision Transformer that integrates multimodalvisual and spatiotemporal information, it addresses the object-proposalassociation as a multi-choice question-answering task. BUSCA is trainedindependently of the underlying tracker, solely on synthetic data, withoutrequiring fine-tuning. Through BUSCA, we showcase consistent performanceenhancements across five different trackers and establish a newstate-of-the-art baseline across three different benchmarks. Code available at:https://github.com/lorenzovaquero/BUSCA.</description><author>Lorenzo Vaquero, Yihong Xu, Xavier Alameda-Pineda, Victor M. Brea, Manuel Mucientes</author><pubDate>Tue, 16 Jul 2024 14:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10151v2</guid></item><item><title>Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer</title><link>http://arxiv.org/abs/2407.11677v1</link><description>Video-language alignment is a crucial multi-modal task that benefits variousdownstream applications, e.g., video-text retrieval and video questionanswering. Existing methods either utilize multi-modal information invideo-text pairs or apply global and local alignment techniques to promotealignment precision. However, these methods often fail to fully explore thespatio-temporal relationships among vision tokens within video and acrossdifferent video-text pairs. In this paper, we propose a novel Spatio-TemporalGraph Transformer module to uniformly learn spatial and temporal contexts forvideo-language alignment pre-training (dubbed STGT). Specifically, our STGTcombines spatio-temporal graph structure information with attention intransformer block, effectively utilizing the spatio-temporal contexts. In thisway, we can model the relationships between vision tokens, promoting video-textalignment precision for benefiting downstream tasks. In addition, we propose aself-similarity alignment loss to explore the inherent self-similarity in thevideo and text. With the initial optimization achieved by contrastive learning,it can further promote the alignment accuracy between video and text.Experimental results on challenging downstream tasks, including video-textretrieval and video question answering, verify the superior performance of ourmethod.</description><author>Shi-Xue Zhang, Hongfa Wang, Xiaobin Zhu, Weibo Gu, Tianjin Zhang, Chun Yang, Wei Liu, Xu-Cheng Yin</author><pubDate>Tue, 16 Jul 2024 12:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11677v1</guid></item><item><title>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</title><link>http://arxiv.org/abs/2402.03094v3</link><description>This paper studies the challenging cross-domain few-shot object detection(CD-FSOD), aiming to develop an accurate object detector for novel domains withminimal labeled examples. While transformer-based open-set detectors, such asDE-ViT, show promise in traditional few-shot object detection, theirgeneralization to CD-FSOD remains unclear: 1) can such open-set detectionmethods easily generalize to CD-FSOD? 2) If not, how can models be enhancedwhen facing huge domain gaps? To answer the first question, we employ measuresincluding style, inter-class variance (ICV), and indefinable boundaries (IB) tounderstand the domain gap. Based on these measures, we establish a newbenchmark named CD-FSOD to evaluate object detection methods, revealing thatmost of the current approaches fail to generalize across domains. Technically,we observe that the performance decline is associated with our proposedmeasures: style, ICV, and IB. Consequently, we propose several novel modules toaddress these issues. First, the learnable instance features align initialfixed instances with target categories, enhancing feature distinctiveness.Second, the instance reweighting module assigns higher importance tohigh-quality instances with slight IB. Third, the domain prompter encouragesfeatures resilient to different styles by synthesizing imaginary domainswithout altering semantic contents. These techniques collectively contribute tothe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),significantly improving upon the base DE-ViT. Experimental results validate theefficacy of our model.</description><author>Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang</author><pubDate>Tue, 16 Jul 2024 12:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03094v3</guid></item><item><title>A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting</title><link>http://arxiv.org/abs/2407.11638v1</link><description>Recently, Large Language Models (LLMs) have demonstrated great potential invarious data mining tasks, such as knowledge question answering, mathematicalreasoning, and commonsense reasoning. However, the reasoning capability of LLMson temporal event forecasting has been under-explored. To systematicallyinvestigate their abilities in temporal event forecasting, we conduct acomprehensive evaluation of LLM-based methods for temporal event forecasting.Due to the lack of a high-quality dataset that involves both graph and textualdata, we first construct a benchmark dataset, named MidEast-TE-mini. Based onthis dataset, we design a series of baseline methods, characterized by variousinput formats and retrieval augmented generation(RAG) modules. From extensiveexperiments, we find that directly integrating raw texts into the input of LLMsdoes not enhance zero-shot extrapolation performance. In contrast,incorporating raw texts in specific complex events and fine-tuning LLMssignificantly improves performance. Moreover, enhanced with retrieval modules,LLM can effectively capture temporal relational patterns hidden in historicalevents. Meanwhile, issues such as popularity bias and the long-tail problemstill persist in LLMs, particularly in the RAG-based method. These findings notonly deepen our understanding of LLM-based event forecasting methods but alsohighlight several promising research directions.We consider that thiscomprehensive evaluation, along with the identified research opportunities,will significantly contribute to future research on temporal event forecastingthrough LLMs.</description><author>He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua</author><pubDate>Tue, 16 Jul 2024 11:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11638v1</guid></item><item><title>CompAct: Compressing Retrieved Documents Actively for Question Answering</title><link>http://arxiv.org/abs/2407.09014v2</link><description>Retrieval-augmented generation supports language models to strengthen theirfactual groundings by providing external contexts. However, language modelsoften face challenges when given extensive information, diminishing theireffectiveness in solving questions. Context compression tackles this issue byfiltering out irrelevant information, but current methods still struggle inrealistic scenarios where crucial information cannot be captured with asingle-step approach. To overcome this limitation, we introduce CompAct, anovel framework that employs an active strategy to condense extensive documentswithout losing key information. Our experiments demonstrate that CompAct bringssignificant improvements in both performance and compression rate on multi-hopquestion-answering (QA) benchmarks. CompAct flexibly operates as acost-efficient plug-in module with various off-the-shelf retrievers or readers,achieving exceptionally high compression rates (47x).</description><author>Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang</author><pubDate>Mon, 15 Jul 2024 11:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09014v2</guid></item><item><title>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</title><link>http://arxiv.org/abs/2407.09413v1</link><description>Seeking answers to questions within long scientific research articles is acrucial area of study that aids readers in quickly addressing their inquiries.However, existing question-answering (QA) datasets based on scientific papersare limited in scale and focus solely on textual content. To address thislimitation, we introduce SPIQA (Scientific Paper Image Question Answering), thefirst large-scale QA dataset specifically designed to interpret complex figuresand tables within the context of scientific research articles across variousdomains of computer science. Leveraging the breadth of expertise and ability ofmultimodal large language models (MLLMs) to understand figures, we employautomatic and manual curation to create the dataset. We craft aninformation-seeking task involving multiple images that cover a wide variety ofplots, charts, tables, schematic diagrams, and result visualizations. SPIQAcomprises 270K questions divided into training, validation, and three differentevaluation splits. Through extensive experiments with 12 prominent foundationalmodels, we evaluate the ability of current multimodal systems to comprehend thenuanced aspects of research articles. Additionally, we propose aChain-of-Thought (CoT) evaluation strategy with in-context retrieval thatallows fine-grained, step-by-step assessment and improves model performance. Wefurther explore the upper bounds of performance enhancement with additionaltextual information, highlighting its promising potential for future researchand the dataset's impact on revolutionizing how we interact with scientificliterature.</description><author>Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan</author><pubDate>Fri, 12 Jul 2024 16:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09413v1</guid></item><item><title>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2401.17043v3</link><description>Retrieval-Augmented Generation (RAG) is a technique that enhances thecapabilities of large language models (LLMs) by incorporating externalknowledge sources. This method addresses common LLM limitations, includingoutdated information and the tendency to produce inaccurate "hallucinated"content. However, the evaluation of RAG systems is challenging, as existingbenchmarks are limited in scope and diversity. Most of the current benchmarkspredominantly assess question-answering applications, overlooking the broaderspectrum of situations where RAG could prove advantageous. Moreover, they onlyevaluate the performance of the LLM component of the RAG pipeline in theexperiments, and neglect the influence of the retrieval component and theexternal knowledge database. To address these issues, this paper constructs alarge-scale and more comprehensive benchmark, and evaluates all the componentsof RAG systems in various RAG application scenarios. Specifically, we havecategorized the range of RAG applications into four distinct types-Create,Read, Update, and Delete (CRUD), each representing a unique use case. "Create"refers to scenarios requiring the generation of original, varied content."Read" involves responding to intricate questions in knowledge-intensivesituations. "Update" focuses on revising and rectifying inaccuracies orinconsistencies in pre-existing texts. "Delete" pertains to the task ofsummarizing extensive texts into more concise forms. For each of these CRUDcategories, we have developed comprehensive datasets to evaluate theperformance of RAG systems. We also analyze the effects of various componentsof the RAG system, such as the retriever, the context length, the knowledgebase construction, and the LLM. Finally, we provide useful insights foroptimizing the RAG technology for different scenarios.</description><author>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen</author><pubDate>Mon, 15 Jul 2024 11:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17043v3</guid></item><item><title>$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity</title><link>http://arxiv.org/abs/2407.10691v1</link><description>Recent studies show the growing significance of document retrieval in thegeneration of LLMs, i.e., RAG, within the scientific domain by bridging theirknowledge gap. However, dense retrievers often struggle with domain-specificretrieval and complex query-document relationships, particularly when querysegments correspond to various parts of a document. To alleviate such prevalentchallenges, this paper introduces $\texttt{MixGR}$, which improves denseretrievers' awareness of query-document matching across various levels ofgranularity in queries and documents using a zero-shot approach.$\texttt{MixGR}$ fuses various metrics based on these granularities to a unitedscore that reflects a comprehensive query-document similarity. Our experimentsdemonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,respectively, averaged on queries containing multiple subqueries from fivescientific retrieval datasets. Moreover, the efficacy of two downstreamscientific question-answering tasks highlights the advantage of$\texttt{MixGR}$to boost the application of LLMs in the scientific domain.</description><author>Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl</author><pubDate>Mon, 15 Jul 2024 13:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10691v1</guid></item><item><title>On scalable oversight with weak LLMs judging strong LLMs</title><link>http://arxiv.org/abs/2407.04622v2</link><description>Scalable oversight protocols aim to enable humans to accurately supervisesuperhuman AI. In this paper we study debate, where two AI's compete toconvince a judge; consultancy, where a single AI tries to convince a judge thatasks questions; and compare to a baseline of direct question-answering, wherethe judge just answers outright without the AI. We use large language models(LLMs) as both AI agents and as stand-ins for human judges, taking the judgemodels to be weaker than agent models. We benchmark on a diverse range ofasymmetries between judges and agents, extending previous work on a singleextractive QA task with information asymmetry, to also include mathematics,coding, logic and multimodal reasoning asymmetries. We find that debateoutperforms consultancy across all tasks when the consultant is randomlyassigned to argue for the correct/incorrect answer. Comparing debate to directquestion answering, the results depend on the type of task: in extractive QAtasks with information asymmetry debate outperforms direct question answering,but in other tasks without information asymmetry the results are mixed.Previous work assigned debaters/consultants an answer to argue for. When weallow them to instead choose which answer to argue for, we find judges are lessfrequently convinced by the wrong answer in debate than in consultancy.Further, we find that stronger debater models increase judge accuracy, thoughmore modestly than in previous studies.</description><author>Zachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah</author><pubDate>Fri, 12 Jul 2024 16:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04622v2</guid></item><item><title>Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders</title><link>http://arxiv.org/abs/2402.00260v2</link><description>The robotic intervention for individuals with Autism Spectrum Disorder (ASD)has generally used pre-defined scripts to deliver verbal content duringone-to-one therapy sessions. This practice restricts the use of robots tolimited, pre-mediated instructional curricula. In this paper, we increase robotautonomy in one such robotic intervention for children with ASD by implementingperspective-taking teaching. Our approach uses large language models (LLM) togenerate verbal content as texts and then deliver it to the child via roboticspeech. In the proposed pipeline, we teach perspective-taking through which ourrobot takes up three roles: initiator, prompter, and reinforcer. We adopted theGPT-2 + BART pipelines to generate social situations, ask questions (asinitiator), and give options (as prompter) when required. The robot encouragesthe child by giving positive reinforcement for correct answers (as areinforcer). In addition to our technical contribution, we conducted ten-minutesessions with domain experts simulating an actual perspective teaching session,with the researcher acting as a child participant. These sessions validated ourrobotic intervention pipeline through surveys, including those from NASA TLXand GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with anall GPT-2 and found the performance of the former to be better. Based on theresponses by the domain experts, the robot session demonstrated higherperformance with no additional increase in mental or physical demand, temporaldemand, effort, or frustration compared to a no-robot session. We alsoconcluded that the domain experts perceived the robot as ideally safe, likable,and reliable.</description><author>Ruchik Mishra, Karla Conn Welch, Dan O Popa</author><pubDate>Wed, 10 Jul 2024 01:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00260v2</guid></item><item><title>IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model</title><link>http://arxiv.org/abs/2407.07577v1</link><description>The rapid advancement of Large Vision-Language models (LVLMs) hasdemonstrated a spectrum of emergent capabilities. Nevertheless, current modelsonly focus on the visual content of a single scenario, while their ability toassociate instances across different scenes has not yet been explored, which isessential for understanding complex visual content, such as movies withmultiple characters and intricate plots. Towards movie understanding, acritical initial step for LVLMs is to unleash the potential of characteridentities memory and recognition across multiple visual scenarios. To achievethe goal, we propose visual instruction tuning with ID reference and develop anID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our researchintroduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory andrecognition across four dimensions: matching, location, question-answering, andcaptioning. Our findings highlight the limitations of existing LVLMs inrecognizing and associating instance identities with ID reference. This paperpaves the way for future artificial intelligence systems to possessmulti-identity visual inputs, thereby facilitating the comprehension of complexvisual narratives like movies.</description><author>Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo</author><pubDate>Wed, 10 Jul 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07577v1</guid></item><item><title>MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</title><link>http://arxiv.org/abs/2407.01523v2</link><description>Understanding documents with rich layouts and multi-modal components is along-standing and practical task. Recent Large Vision-Language Models (LVLMs)have made remarkable strides in various tasks, particularly in single-pagedocument understanding (DU). However, their abilities on long-context DU remainan open problem. This work presents MMLongBench-Doc, a long-context,multi-modal benchmark comprising 1,062 expert-annotated questions. Distinctfrom previous datasets, it is constructed upon 130 lengthy PDF-formatteddocuments with an average of 49.4 pages and 20,971 textual tokens. Towardscomprehensive evaluation, answers to these questions rely on pieces of evidencefrom (1) different sources (text, image, chart, table, and layout structure)and (2) various locations (i.e. page number). Moreover, 33.2% of the questionsare cross-page questions requiring evidence across multiple pages. 22.8% of thequestions are designed to be unanswerable for detecting potentialhallucinations. Experiments on 14 LVLMs demonstrate that long-context DUgreatly challenges current models. Notably, the best-performing model, GPT-4o,achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worseperformance than their LLM counterparts which are fed with lossy-parsed OCRdocuments. These results validate the necessity of future research toward morecapable long-context LVLMs. Project Page:https://mayubo2333.github.io/MMLongBench-Doc</description><author>Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun</author><pubDate>Wed, 10 Jul 2024 15:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01523v2</guid></item><item><title>Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation</title><link>http://arxiv.org/abs/2404.15687v2</link><description>Vulnerability detection is crucial for ensuring the security and reliabilityof software systems. Recently, Graph Neural Networks (GNNs) have emerged as aprominent code embedding approach for vulnerability detection, owing to theirability to capture the underlying semantic structure of source code. However,GNNs face significant challenges in explainability due to their inherentlyblack-box nature. To this end, several factual reasoning-based explainers havebeen proposed. These explainers provide explanations for the predictions madeby GNNs by analyzing the key features that contribute to the outcomes. We arguethat these factual reasoning-based explanations cannot answer critical what-ifquestions: What would happen to the GNN's decision if we were to alter the codegraph into alternative structures? Inspired by advancements of counterfactualreasoning in artificial intelligence, we propose CFExplainer, a novelcounterfactual explainer for GNN-based vulnerability detection. Unlike factualreasoning-based explainers, CFExplainer seeks the minimal perturbation to theinput code graph that leads to a change in the prediction, thereby addressingthe what-if questions for vulnerability detection. We term this perturbation acounterfactual explanation, which can pinpoint the root causes of the detectedvulnerability and furnish valuable insights for developers to undertakeappropriate actions for fixing the vulnerability. Extensive experiments on fourGNN-based vulnerability detection models demonstrate the effectiveness ofCFExplainer over existing state-of-the-art factual reasoning-based explainers.</description><author>Zhaoyang Chu, Yao Wan, Qian Li, Yang Wu, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin</author><pubDate>Mon, 15 Jul 2024 14:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15687v2</guid></item><item><title>Advancing Question Answering on Handwritten Documents: A State-of-the-Art Recognition-Based Model for HW-SQuAD</title><link>http://arxiv.org/abs/2406.17437v2</link><description>Question-answering handwritten documents is a challenging task with numerousreal-world applications. This paper proposes a novel recognition-based approachthat improves upon the previous state-of-the-art on the HW-SQuAD and BenthamQAdatasets. Our model incorporates transformer-based document retrieval andensemble methods at the model level, achieving an Exact Match score of 82.02%and 69% in HW-SQuAD and BenthamQA datasets, respectively, surpassing theprevious best recognition-based approach by 10.89% and 3%. We also enhance thedocument retrieval component, boosting the top-5 retrieval accuracy from 90% to95.30%. Our results demonstrate the significance of our proposed approach inadvancing question answering on handwritten documents. The code and trainedmodels will be publicly available to facilitate future research in thiscritical area of natural language.</description><author>Aniket Pal, Ajoy Mondal, C. V. Jawahar</author><pubDate>Mon, 15 Jul 2024 14:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17437v2</guid></item><item><title>TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models</title><link>http://arxiv.org/abs/2407.09424v1</link><description>Large Language Models (LLMs) have the potential to revolutionize the SixthGeneration (6G) communication networks. However, current mainstream LLMsgenerally lack the specialized knowledge in telecom domain. In this paper, forthe first time, we propose a pipeline to adapt any general purpose LLMs to atelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,instruction dataset, preference dataset to perform continual pre-training,instruct tuning and alignment tuning respectively. Besides, due to the lack ofwidely accepted evaluation benchmarks in telecom domain, we extend existingevaluation benchmarks and proposed three new benchmarks, namely, Telecom MathModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks providea holistic evaluation of the capabilities of LLMs including math modeling,Open-Ended question answering, code generation, infilling, summarization andanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state ofthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom MathModeling benchmark significantly and achieve comparable performance in variousevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,telecom code summary and generation and infilling.</description><author>Hang Zou, Qiyang Zhao, Yu Tian, Lina Bariah, Faouzi Bader, Thierry Lestable, Merouane Debbah</author><pubDate>Fri, 12 Jul 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09424v1</guid></item><item><title>FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries</title><link>http://arxiv.org/abs/2407.10810v1</link><description>Intelligence is key to advancing integrated circuit (IC) fabrication. Recentbreakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleledabilities in understanding images and text, fostering intelligent fabrication.Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabricationlarge multimodal model for wafer defect knowledge query. FabGPT manifestsexpertise in conducting defect detection in Scanning Electron Microscope (SEM)images, performing root cause analysis, and providing expert question-answering(Q&amp;A) on fabrication processes. FabGPT matches enhanced multimodal features toautomatically detect minute defects under complex wafer backgrounds and reducethe subjectivity of manual threshold settings. Besides, the proposed modulationmodule and interactive corpus training strategy embed wafer defect knowledgeinto the pre-trained model, effectively balancing Q&amp;A queries related to defectknowledge and original knowledge and mitigating the modality bias issues.Experiments on in-house fab data (SEM-WaD) show that our FabGPT achievessignificant performance improvement in wafer defect detection and knowledgequerying.</description><author>Yuqi Jiang, Xudong Lu, Qian Jin, Qi Sun, Hanming Wu, Cheng Zhuo</author><pubDate>Mon, 15 Jul 2024 15:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10810v1</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v2</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Thu, 18 Jul 2024 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v2</guid></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>http://arxiv.org/abs/2407.10887v1</link><description>Amid growing concerns over the ease of theft and misuse of Large LanguageModels (LLMs), the need for fingerprinting models has increased.Fingerprinting, in this context, means that the model owner can link a givenmodel to their original version, thereby identifying if their model is beingmisused or has been completely stolen. In this paper, we first define a setfive properties a successful fingerprint should satisfy; namely, thefingerprint should be Transparent, Efficient, Persistent, Robust, andUnforgeable. Next, we propose Chain &amp; Hash, a new, simple fingerprintingapproach that implements a fingerprint with a cryptographic flavor, achievingall these properties. Chain &amp; Hash involves generating a set of questions (thefingerprints) along with a set of potential answers. These elements are hashedtogether using a secure hashing technique to select the value for eachquestion, hence providing an unforgeability property-preventing adversariesfrom claiming false ownership. We evaluate the Chain &amp; Hash technique onmultiple models and demonstrate its robustness against benign transformations,such as fine-tuning on different datasets, and adversarial attempts to erasethe fingerprint. Finally, our experiments demonstrate the efficiency ofimplementing Chain &amp; Hash and its utility, where fingerprinted models achievealmost the same performance as non-fingerprinted ones across differentbenchmarks.</description><author>Mark Russinovich, Ahmed Salem</author><pubDate>Mon, 15 Jul 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10887v1</guid></item><item><title>Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin</title><link>http://arxiv.org/abs/2406.06566v2</link><description>Domain specific digital twins, representing a digital replica of varioussegments of the smart grid, are foreseen as able to model, simulate, andcontrol the respective segments. At the same time, knowledge-based digitaltwins, coupled with AI, may also empower humans to understand aspects of thesystem through natural language interaction in view of planning and policymaking. This paper is the first to assess and report on the potential ofRetrieval Augmented Generation (RAG) question answers related to householdelectrical energy measurement aspects leveraging a knowledge-based energydigital twin. Relying on the recently published electricity consumptionknowledge graph that actually represents a knowledge-based digital twin, westudy the capabilities of ChatGPT, Gemini and Llama in answering electricityrelated questions. Furthermore, we compare the answers with the ones generatedthrough a RAG techniques that leverages an existing electricity knowledge-baseddigital twin. Our findings illustrate that the RAG approach not only reducesthe incidence of incorrect information typically generated by LLMs but alsosignificantly improves the quality of the output by grounding responses inverifiable data. This paper details our methodology, presents a comparativeanalysis of responses with and without RAG, and discusses the implications ofour findings for future applications of AI in specialized sectors like energydata analysis.</description><author>Carolina Fortuna, Vid Hanžel, Blaž Bertalanič</author><pubDate>Thu, 11 Jul 2024 13:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06566v2</guid></item><item><title>AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering</title><link>http://arxiv.org/abs/2311.14906v2</link><description>We propose a novel and challenging benchmark, AutoEval-Video, tocomprehensively evaluate large vision-language models in open-ended videoquestion answering. The comprehensiveness of AutoEval-Video is demonstrated intwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9skill dimensions, addressing capabilities of perception, comprehension, andgeneration. 2) AutoEval-Video contains newly collected videos that cover over40 distinct themes. To efficiently evaluate responses to the open-endedquestions, we employ an LLM-based evaluation approach, but instead of merelyproviding a reference answer, we annotate unique evaluation rules for everysingle instance (video-question pair). To maximize the robustness of theserules, we develop a novel adversarial annotation mechanism. By usinginstance-specific rules as prompt, GPT-4, as an automatic evaluator, canachieve a stable evaluation accuracy of around 97.0%, comparable to the 94.9% -97.5% accuracy of a human evaluator. Furthermore, we assess the performance ofeight large vision-language models on AutoEval-Video. Among them, GPT-4V(ision)significantly outperforms other models, achieving an accuracy of 32.2%.However, there is still substantial room for improvement compared to humanaccuracy of 72.8%. By conducting an extensive case study, we uncover severaldrawbacks of GPT-4V, such as limited temporal and dynamic comprehension, andoverly general responses. Code is available athttps://github.com/Xiuyuan-Chen/AutoEval-Video.</description><author>Xiuyuan Chen, Yuan Lin, Yuchen Zhang, Weiran Huang</author><pubDate>Mon, 15 Jul 2024 16:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14906v2</guid></item><item><title>Multimodal Reranking for Knowledge-Intensive Visual Question Answering</title><link>http://arxiv.org/abs/2407.12277v1</link><description>Knowledge-intensive visual question answering requires models to effectivelyuse external knowledge to help answer visual questions. A typical pipelineincludes a knowledge retriever and an answer generator. However, a retrieverthat utilizes local information, such as an image patch, may not providereliable question-candidate relevance scores. Besides, the two-towerarchitecture also limits the relevance score modeling of a retriever to selecttop candidates for answer generator reasoning. In this paper, we introduce anadditional module, a multi-modal reranker, to improve the ranking quality ofknowledge candidates for answer generation. Our reranking module takesmulti-modal information from both candidates and questions and performscross-item interaction for better relevance score modeling. Experiments onOK-VQA and A-OKVQA show that multi-modal reranker from distant supervisionprovides consistent improvements. We also find a training-testing discrepancywith reranking in answer generation, where performance improves if trainingknowledge candidates are similar to or noisier than those used in testing.</description><author>Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander Hauptmann, Michael Bendersky</author><pubDate>Wed, 17 Jul 2024 02:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12277v1</guid></item><item><title>Benchmarking Vision Language Models for Cultural Understanding</title><link>http://arxiv.org/abs/2407.10920v1</link><description>Foundation models and vision-language pre-training have notably advancedVision Language Models (VLMs), enabling multimodal processing of visual andlinguistic data. However, their performance has been typically assessed ongeneral scene understanding - recognizing objects, attributes, and actions -rather than cultural comprehension. This study introduces CulturalVQA, a visualquestion-answering benchmark aimed at assessing VLM's geo-diverse culturalunderstanding. We curate a collection of 2,378 image-question pairs with 1-5answers per question representing cultures from 11 countries across 5continents. The questions probe understanding of various facets of culture suchas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs onCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level ofcultural understanding across regions, with strong cultural understandingcapabilities for North America while significantly lower performance forAfrica. We observe disparity in their performance across cultural facets too,with clothing, rituals, and traditions seeing higher performances than food anddrink. These disparities help us identify areas where VLMs lack culturalunderstanding and demonstrate the potential of CulturalVQA as a comprehensiveevaluation set for gauging VLM progress in understanding diverse cultures.</description><author>Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal</author><pubDate>Mon, 15 Jul 2024 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10920v1</guid></item><item><title>Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition</title><link>http://arxiv.org/abs/2403.00499v2</link><description>Recent advances in LLMs have sparked a debate on whether they understandtext. In this position paper, we argue that opponents in this debate holddifferent definitions for understanding, and particularly differ in their viewon the role of consciousness. To substantiate this claim, we propose a thoughtexperiment involving an open-source chatbot $Z$ which excels on every possiblebenchmark, seemingly without subjective experience. We ask whether $Z$ iscapable of understanding, and show that different schools of thought withinseminal AI research seem to answer this question differently, uncovering theirterminological disagreement. Moving forward, we propose two distinct workingdefinitions for understanding which explicitly acknowledge the question ofconsciousness, and draw connections with a rich literature in philosophy,psychology and neuroscience.</description><author>Ariel Goldstein, Gabriel Stanovsky</author><pubDate>Thu, 11 Jul 2024 15:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00499v2</guid></item><item><title>ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities</title><link>http://arxiv.org/abs/2407.01525v3</link><description>Although great progress has been made in 3D visual grounding, current modelsstill rely on explicit textual descriptions for grounding and lack the abilityto reason human intentions from implicit instructions. We propose a new taskcalled 3D reasoning grounding and introduce a new benchmark ScanReason whichprovides over 10K question-answer-location pairs from five reasoning types thatrequire the synerization of reasoning and grounding. We further design ourapproach, ReGround3D, composed of the visual-centric reasoning module empoweredby Multi-modal Large Language Model (MLLM) and the 3D grounding module toobtain accurate object locations by looking back to the enhanced geometry andfine-grained details from the 3D scenes. A chain-of-grounding mechanism isproposed to further boost the performance with interleaved reasoning andgrounding steps during inference. Extensive experiments on the proposedbenchmark validate the effectiveness of our proposed approach.</description><author>Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu</author><pubDate>Wed, 17 Jul 2024 07:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01525v3</guid></item><item><title>Fully Authentic Visual Question Answering Dataset from Online Communities</title><link>http://arxiv.org/abs/2311.15562v4</link><description>Visual Question Answering (VQA) entails answering questions about images. Weintroduce the first VQA dataset in which all contents originate from anauthentic use case. Sourced from online question answering community forums, wecall it VQAonline. We characterize this dataset and how it relates to eightmainstream VQA datasets. Observing that answers in our dataset tend to be muchlonger (i.e., a mean of 173 words) and so incompatible with standard VQAevaluation metrics, we instead utilize popular metrics for longer textevaluation for evaluating six state-of-the-art VQA models on VQAonline andreport where they struggle most. Finally, we analyze which evaluation metricsalign best with human judgments. To facilitate future extensions, wepublicly-share the dataset at: https://vqaonline.github.io/.</description><author>Chongyan Chen, Mengchen Liu, Noel Codella, Yunsheng Li, Lu Yuan, Danna Gurari</author><pubDate>Wed, 17 Jul 2024 07:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15562v4</guid></item><item><title>Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</title><link>http://arxiv.org/abs/2407.11282v2</link><description>Large Language Models (LLMs) are employed across various high-stakes domains,where the reliability of their outputs is crucial. One commonly used method toassess the reliability of LLMs' responses is uncertainty estimation, whichgauges the likelihood of their answers being correct. While many studies focuson improving the accuracy of uncertainty estimations for LLMs, our researchinvestigates the fragility of uncertainty estimation and explores potentialattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,when activated by a specific trigger in the input, manipulates the model'suncertainty without affecting the final output. Specifically, the proposedbackdoor attack method can alter an LLM's output probability distribution,causing the probability distribution to converge towards an attacker-predefineddistribution while ensuring that the top-1 prediction remains unchanged. Ourexperimental results demonstrate that this attack effectively undermines themodel's self-evaluation reliability in multiple-choice questions. For instance,we achieved a 100 attack success rate (ASR) across three different triggeringstrategies in four models. Further, we investigate whether this manipulationgeneralizes across different prompts and domains. This work highlights asignificant threat to the reliability of LLMs and underscores the need forfuture defenses against such attacks. The code is available athttps://github.com/qcznlp/uncertainty_attack.</description><author>Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang</author><pubDate>Wed, 17 Jul 2024 02:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11282v2</guid></item><item><title>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</title><link>http://arxiv.org/abs/2403.09635v2</link><description>In spite of their huge success, transformer models remain difficult to scalein depth. In this work, we develop a unified signal propagation theory andprovide formulae that govern the moments of the forward and backward signalthrough the transformer model. Our framework can be used to understand andmitigate vanishing/exploding gradients, rank collapse, and instabilityassociated with high attention scores. We also propose DeepScaleLM, aninitialization and scaling scheme that conserves unit output/gradient momentsthroughout the model, enabling the training of very deep models with 1000layers. We find that transformer models could be much deeper - our deep modelswith fewer parameters outperform shallow models in Language Modeling, SpeechTranslation, and Image Classification, across encoder-only, decoder-only andencoder-decoder variants, for both Pre-LN and Post-LN transformers, formultiple datasets and model sizes. These improvements also translate intoimproved performance on downstream Question Answering tasks and improvedrobustness for Image Classification.</description><author>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee</author><pubDate>Thu, 18 Jul 2024 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09635v2</guid></item><item><title>Visual Haystacks: Answering Harder Questions About Sets of Images</title><link>http://arxiv.org/abs/2407.13766v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have made significantprogress in the field of single-image visual question answering. However, thesemodels face substantial challenges when tasked with queries that span extensivecollections of images, similar to real-world scenarios like searching throughlarge photo albums, finding specific information across the internet, ormonitoring environmental changes through satellite imagery. This paper exploresthe task of Multi-Image Visual Question Answering (MIQA): given a large set ofimages and a natural language query, the task is to generate a relevant andgrounded response. We propose a new public benchmark, dubbed "Visual Haystacks(VHs)," specifically designed to evaluate LMMs' capabilities in visualretrieval and reasoning over sets of unrelated images, where we performcomprehensive evaluations demonstrating that even robust closed-source modelsstruggle significantly. Towards addressing these shortcomings, we introduceMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QAframework tailored for LMMs that confronts the challenges of MIQA with markedefficiency and accuracy improvements over baseline methods. Our evaluationshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHsbenchmark and offers up to 3.4x improvements in efficiency over text-focusedmulti-stage approaches.</description><author>Tsung-Han Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, David M. Chan</author><pubDate>Thu, 18 Jul 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13766v1</guid></item><item><title>SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant</title><link>http://arxiv.org/abs/2403.11299v2</link><description>Recent advances in vision-language models have shown notable generalizationin broad tasks through visual instruction tuning. However, bridging the gapbetween the pre-trained vision encoder and the large language models (LLMs)becomes the whole network's bottleneck. To improve cross-modality alignment,existing works usually consider more visual instruction data covering a broaderrange of vision tasks to fine-tune the model for question-answering, which,however, is costly to obtain and has not thoroughly explored the richcontextual information contained in images. This paper first attempts toharness the overlooked context within visual instruction data, training themodel to self-supervised "learning" how to ask high-quality questions. In thisway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for LargeVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexibleand meaningful image-related questions while analyzing the visual clue andprior language knowledge, signifying an advanced level of generalized visualunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instructiondata shows a performance improvement compared with traditionalvisual-instruction tuning methods. This improvement highlights the efficacy ofself-questioning techniques in achieving a deeper and more nuancedcomprehension of visual content across various contexts.</description><author>Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao</author><pubDate>Mon, 15 Jul 2024 17:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11299v2</guid></item><item><title>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</title><link>http://arxiv.org/abs/2402.09327v2</link><description>In this work, we investigate the interplay between memorization and learningin the context of \emph{stochastic convex optimization} (SCO). We definememorization via the information a learning algorithm reveals about itstraining data points. We then quantify this information using the framework ofconditional mutual information (CMI) proposed by Steinke and Zakynthinou(2020). Our main result is a precise characterization of the tradeoff betweenthe accuracy of a learning algorithm and its CMI, answering an open questionposed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded settingand under strong convexity, every learner with an excess error $\varepsilon$has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,respectively. We further demonstrate the essential role of memorization inlearning problems in SCO by designing an adversary capable of accuratelyidentifying a significant fraction of the training samples in specific SCOproblems. Finally, we enumerate several implications of our results, such as alimitation of generalization bounds based on CMI and the incompressibility ofsamples in SCO problems.</description><author>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</author><pubDate>Thu, 18 Jul 2024 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09327v2</guid></item><item><title>Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following</title><link>http://arxiv.org/abs/2406.02774v2</link><description>Training gaze following models requires a large number of images with gazetarget coordinates annotated by human annotators, which is a laborious andinherently ambiguous process. We propose the first semi-supervised method forgaze following by introducing two novel priors to the task. We obtain the firstprior using a large pretrained Visual Question Answering (VQA) model, where wecompute Grad-CAM heatmaps by `prompting' the VQA model with a gaze followingquestion. These heatmaps can be noisy and not suited for use in training. Theneed to refine these noisy annotations leads us to incorporate a second prior.We utilize a diffusion model trained on limited human annotations and modifythe reverse sampling process to refine the Grad-CAM heatmaps. By tuning thediffusion process we achieve a trade-off between the human annotation prior andthe VQA heatmap prior, which retains the useful VQA prior information whileexhibiting similar properties to the training data distribution. Our methodoutperforms simple pseudo-annotation generation baselines on the GazeFollowimage dataset. More importantly, our pseudo-annotation strategy, applied to awidely used supervised gaze following model (VAT), reduces the annotation needby 50%. Our method also performs the best on the VideoAttentionTarget dataset.</description><author>Qiaomu Miao, Alexandros Graikos, Jingwei Zhang, Sounak Mondal, Minh Hoai, Dimitris Samaras</author><pubDate>Thu, 18 Jul 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02774v2</guid></item><item><title>Conversational Query Reformulation with the Guidance of Retrieved Documents</title><link>http://arxiv.org/abs/2407.12363v1</link><description>Conversational search seeks to retrieve relevant passages for the givenquestions in Conversational QA (ConvQA). Questions in ConvQA face challengessuch as omissions and coreferences, making it difficult to obtain desiredsearch results. Conversational Query Reformulation (CQR) transforms thesecurrent queries into de-contextualized forms to resolve these issues. However,existing CQR methods focus on rewriting human-friendly queries, which may notalways yield optimal search results for the retriever. To overcome thischallenge, we introduce GuideCQR, a framework that utilizes guided documents torefine queries, ensuring that they are optimal for retrievers. Specifically, weaugment keywords, generate expected answers from the re-ranked documents, andunify them with the filtering process. Experimental results show that queriesenhanced by guided documents outperform previous CQR methods. Especially,GuideCQR surpasses the performance of Large Language Model (LLM) prompt-poweredapproaches and demonstrates the importance of the guided documents informulating retriever-friendly queries across diverse setups.</description><author>Jeonghyun Park, Hwanhee Lee</author><pubDate>Wed, 17 Jul 2024 07:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12363v1</guid></item><item><title>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2402.13950v3</link><description>Large language models (LLMs) have been shown to perform better when asked toreason step-by-step before answering a question. However, it is unclear to whatdegree the model's final answer is faithful to the stated reasoning steps. Inthis paper, we perform a causal mediation analysis on twelve LLMs to examinehow intermediate reasoning steps generated by the LLM influence the finaloutcome and find that LLMs do not reliably use their intermediate reasoningsteps when generating an answer. To address this issue, we introduce FRODO, aframework to tailor small-sized LMs to generate correct reasoning steps androbustly reason over these steps. FRODO consists of an inference module thatlearns to generate correct reasoning steps using an implicit causal rewardfunction and a reasoning module that learns to faithfully reason over theseintermediate inferences using a counterfactual and causal preference objective.Our experiments show that FRODO significantly outperforms four competitivebaselines. Furthermore, FRODO improves the robustness and generalizationability of the reasoning LM, yielding higher performance on out-of-distributiontest sets. Finally, we find that FRODO's rationales are more faithful to itsfinal answer predictions than standard supervised fine-tuning.</description><author>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</author><pubDate>Thu, 18 Jul 2024 13:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13950v3</guid></item><item><title>BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models</title><link>http://arxiv.org/abs/2407.13442v1</link><description>Vision language models (VLMs) perceive the world through a combination of avisual encoder and a large language model (LLM). The visual encoder,pre-trained on large-scale vision-text datasets, provides zero-shotgeneralization to visual data, and the LLM endows its high reasoning ability toVLMs. It leads VLMs to achieve high performance on wide benchmarks withoutfine-tuning, exhibiting zero or few-shot capability. However, recent studiesshow that VLMs are vulnerable to hallucination. This undesirable behaviordegrades reliability and credibility, thereby making users unable to fullytrust the output from VLMs. To enhance trustworthiness and better tackle thehallucination of VLMs, we curate a new evaluation dataset, called theBEfore-AFter hallucination dataset (BEAF), and introduce new metrics: TrueUnderstanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID).Unlike prior works that focus only on constructing questions and answers, thekey idea of our benchmark is to manipulate visual scene information by imageediting models and to design the metrics based on scene changes. This allows usto clearly assess whether VLMs correctly understand a given scene by observingthe ability to perceive changes. We also visualize image-wise objectrelationship by virtue of our two-axis view: vision and text. Upon evaluatingVLMs with our dataset, we observed that our metrics reveal different aspects ofVLM hallucination that have not been reported before. Project page:\url{https://beafbench.github.io/}</description><author>Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Tae-Hyun Oh</author><pubDate>Thu, 18 Jul 2024 12:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13442v1</guid></item><item><title>Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ</title><link>http://arxiv.org/abs/2403.03814v2</link><description>Large language models (LLMs) need to serve everyone, including a globalmajority of non-English speakers. However, most LLMs today, and open LLMs inparticular, are often intended for use in just English (e.g. Llama2, Mistral)or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recentresearch shows that, despite limits in their intended use, people prompt LLMsin many different languages. Therefore, in this paper, we investigate the basicmultilingual capabilities of state-of-the-art open LLMs beyond their intendeduse. For this purpose, we introduce MultiQ, a new silver standard benchmark forbasic open-ended question answering with 27.4k test questions across atypologically diverse set of 137 languages. With MultiQ, we evaluate languagefidelity, i.e. whether models respond in the prompted language, and questionanswering accuracy. All LLMs we test respond faithfully and/or accurately forat least some languages beyond their intended use. Most models are moreaccurate when they respond faithfully. However, differences across models arelarge, and there is a long tail of languages where models are neither accuratenor faithful. We explore differences in tokenization as a potential explanationfor our findings, identifying possible correlations that warrant furtherinvestigation.</description><author>Carolin Holtermann, Paul Röttger, Timm Dill, Anne Lauscher</author><pubDate>Thu, 18 Jul 2024 07:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03814v2</guid></item><item><title>Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach</title><link>http://arxiv.org/abs/2407.13101v1</link><description>Multi-hop question answering is a challenging task with distinct industrialrelevance, and Retrieval-Augmented Generation (RAG) methods based on largelanguage models (LLMs) have become a popular approach to tackle this task.Owing to the potential inability to retrieve all necessary information in asingle iteration, a series of iterative RAG methods has been recentlydeveloped, showing significant performance improvements. However, existingmethods still face two critical challenges: context overload resulting frommultiple rounds of retrieval, and over-planning and repetitive planning due tothe lack of a recorded retrieval trajectory. In this paper, we propose a noveliterative RAG method called ReSP, equipped with a dual-function summarizer.This summarizer compresses information from retrieved documents, targeting boththe overarching question and the current sub-question concurrently.Experimental results on the multi-hop question-answering datasets HotpotQA and2WikiMultihopQA demonstrate that our method significantly outperforms thestate-of-the-art, and exhibits excellent robustness concerning context length.</description><author>Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang</author><pubDate>Thu, 18 Jul 2024 02:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13101v1</guid></item><item><title>Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses</title><link>http://arxiv.org/abs/2407.09690v2</link><description>We revisit the problem of federated learning (FL) with private data frompeople who do not trust the server or other silos/clients. In this context,every silo (e.g. hospital) has data from several people (e.g. patients) andneeds to protect the privacy of each person's data (e.g. health records), evenif the server and/or other silos try to uncover this data. Inter-SiloRecord-Level Differential Privacy (ISRL-DP) prevents each silo's data frombeing leaked, by requiring that silo i's communications satisfy item-leveldifferential privacy. Prior work arXiv:2106.09779 characterized the optimalexcess risk bounds for ISRL-DP algorithms with homogeneous (i.i.d.) silo dataand convex loss functions. However, two important questions were left open: (1)Can the same excess risk bounds be achieved with heterogeneous (non-i.i.d.)silo data? (2) Can the optimal risk bounds be achieved with fewer communicationrounds? In this paper, we give positive answers to both questions. We providenovel ISRL-DP FL algorithms that achieve the optimal excess risk bounds in thepresence of heterogeneous silo data. Moreover, our algorithms are morecommunication-efficient than the prior state-of-the-art. For smooth lossfunctions, our algorithm achieves the optimal excess risk bound and hascommunication complexity that matches the non-private lower bound.Additionally, our algorithms are more computationally efficient than theprevious state-of-the-art.</description><author>Changyu Gao, Andrew Lowy, Xingyu Zhou, Stephen J. Wright</author><pubDate>Wed, 17 Jul 2024 23:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09690v2</guid></item><item><title>The Linear Representation Hypothesis and the Geometry of Large Language Models</title><link>http://arxiv.org/abs/2311.03658v2</link><description>Informally, the 'linear representation hypothesis' is the idea thathigh-level concepts are represented linearly as directions in somerepresentation space. In this paper, we address two closely related questions:What does "linear representation" actually mean? And, how do we make sense ofgeometric notions (e.g., cosine similarity or projection) in the representationspace? To answer these, we use the language of counterfactuals to give twoformalizations of "linear representation", one in the output (word)representation space, and one in the input (sentence) space. We then provethese connect to linear probing and model steering, respectively. To make senseof geometric notions, we use the formalization to identify a particular(non-Euclidean) inner product that respects language structure in a sense wemake precise. Using this causal inner product, we show how to unify all notionsof linear representation. In particular, this allows the construction of probesand steering vectors using counterfactual pairs. Experiments with LLaMA-2demonstrate the existence of linear representations of concepts, the connectionto interpretation and control, and the fundamental role of the choice of innerproduct.</description><author>Kiho Park, Yo Joong Choe, Victor Veitch</author><pubDate>Wed, 17 Jul 2024 22:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03658v2</guid></item><item><title>Establishing Knowledge Preference in Language Models</title><link>http://arxiv.org/abs/2407.13048v1</link><description>Language models are known to encode a great amount of factual knowledgethrough pretraining. However, such knowledge might be insufficient to cater touser requests, requiring the model to integrate external knowledge sources andadhere to user-provided specifications. When answering questions about ongoingevents, the model should use recent news articles to update its response; whenasked to provide recommendations, the model should prioritize userspecifications over retrieved product reviews; when some facts are edited inthe model, the updated facts should override all prior knowledge learned by themodel even if they are conflicting. In all of the cases above, the model facesa decision between its own parametric knowledge, (retrieved) contextualknowledge, and user instruction knowledge. In this paper, we (1) unify suchsettings into the problem of knowledge preference and define a three-levelpreference hierarchy over these knowledge sources; (2) compile a collection ofexisting datasets IfQA, MQuAKE, and MRQA covering a combination of settings(with/without user specifications, with/without context documents) tosystematically evaluate how well models obey the intended knowledge preference;and (3) propose a dataset synthesis method that composes diversequestion-answer pairs with user assumptions and related context to directlyfine-tune LMs for instilling the hierarchy of knowledge. We demonstrate that a7B model, fine-tuned on only a few thousand examples automatically generated byour proposed method, effectively achieves superior performance (more than 18%improvement across all evaluation benchmarks) in adhering to the desiredknowledge preference hierarchy.</description><author>Sizhe Zhou, Sha Li, Yu Meng, Yizhu Jiao, Heng Ji, Jiawei Han</author><pubDate>Wed, 17 Jul 2024 23:16:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13048v1</guid></item><item><title>Language models show human-like content effects on reasoning tasks</title><link>http://arxiv.org/abs/2207.07051v4</link><description>Reasoning is a key ability for an intelligent system. Large language models(LMs) achieve above-chance performance on abstract reasoning tasks, but exhibitmany imperfections. However, human abstract reasoning is also imperfect. Forexample, human reasoning is affected by our real-world knowledge and beliefs,and shows notable "content effects"; humans reason more reliably when thesemantic content of a problem supports the correct logical inferences. Thesecontent-entangled reasoning patterns play a central role in debates about thefundamental nature of human intelligence. Here, we investigate whether languagemodels $\unicode{x2014}$ whose prior expectations capture some aspects of humanknowledge $\unicode{x2014}$ similarly mix content into their answers to logicalproblems. We explored this question across three logical reasoning tasks:natural language inference, judging the logical validity of syllogisms, and theWason selection task. We evaluate state of the art large language models, aswell as humans, and find that the language models reflect many of the samepatterns observed in humans across these tasks $\unicode{x2014}$ like humans,models answer more accurately when the semantic content of a task supports thelogical inferences. These parallels are reflected both in answer patterns, andin lower-level features like the relationship between model answerdistributions and human response times. Our findings have implications forunderstanding both these cognitive effects in humans, and the factors thatcontribute to language model performance.</description><author>Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, Felix Hill</author><pubDate>Wed, 17 Jul 2024 22:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07051v4</guid></item><item><title>On Pre-training of Multimodal Language Models Customized for Chart Understanding</title><link>http://arxiv.org/abs/2407.14506v1</link><description>Recent studies customizing Multimodal Large Language Models (MLLMs) fordomain-specific tasks have yielded promising results, especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answer(QA) accuracy within the chart domain. However, they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data, particularly in the models' capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs' comprehension ofcharts. We present three key findings: (1) Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. (2)Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. (3) Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of charts,including unannotated ones, while maintaining robust reasoning abilities.Furthermore, we establish a new benchmark to evaluate MLLMs' understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</description><author>Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Lu Yuan, Leonid Sigal</author><pubDate>Fri, 19 Jul 2024 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14506v1</guid></item><item><title>Hey, That's My Model! Introducing Chain &amp; Hash, An LLM Fingerprinting Technique</title><link>http://arxiv.org/abs/2407.10887v2</link><description>Amid growing concerns over the ease of theft and misuse of Large LanguageModels (LLMs), the need for fingerprinting models has increased.Fingerprinting, in this context, means that the model owner can link a givenmodel to their original version, thereby identifying if their model is beingmisused or has been completely stolen. In this paper, we first define a setfive properties a successful fingerprint should satisfy; namely, thefingerprint should be Transparent, Efficient, Persistent, Robust, andUnforgeable. Next, we propose Chain &amp; Hash, a new, simple fingerprintingapproach that implements a fingerprint with a cryptographic flavor, achievingall these properties. Chain &amp; Hash involves generating a set of questions (thefingerprints) along with a set of potential answers. These elements are hashedtogether using a secure hashing technique to select the value for eachquestion, hence providing an unforgeability property-preventing adversariesfrom claiming false ownership. We evaluate the Chain &amp; Hash technique onmultiple models and demonstrate its robustness against benign transformations,such as fine-tuning on different datasets, and adversarial attempts to erasethe fingerprint. Finally, our experiments demonstrate the efficiency ofimplementing Chain &amp; Hash and its utility, where fingerprinted models achievealmost the same performance as non-fingerprinted ones across differentbenchmarks.</description><author>Mark Russinovich, Ahmed Salem</author><pubDate>Wed, 17 Jul 2024 07:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10887v2</guid></item></channel></rss>