<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 06 Nov 2024 01:00:06 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin Script Languages</title><link>http://arxiv.org/abs/2411.02398v1</link><description>Multilingual LLMs have achieved remarkable benchmark performance, but we findthey continue to underperform on non-Latin script languages across contemporaryLLM families. This discrepancy arises from the fact that LLMs are pretrainedwith orthographic scripts, which are dominated by Latin characters that obscuretheir shared phonology with non-Latin scripts. We propose leveraging phonemictranscriptions as complementary signals to induce script-invariantrepresentations. Our study demonstrates that integrating phonemic signalsimproves performance across both non-Latin and Latin languages, with aparticularly significant impact on closing the performance gap between the two.Through detailed experiments, we show that phonemic and orthographic scriptsretrieve distinct examples for in-context learning (ICL). This motivates ourproposed Mixed-ICL retrieval strategy, where further aggregation leads to oursignificant performance improvements for both Latin script languages (up to12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICLretrieval.</description><author>Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary</author><pubDate>Mon, 04 Nov 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02398v1</guid></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>http://arxiv.org/abs/2411.02397v1</link><description>Generating temporally-consistent high-fidelity videos can be computationallyexpensive, especially over longer temporal spans. More-recent DiffusionTransformers (DiTs) -- despite making significant headway in this context --have only heightened such challenges as they rely on larger models and heavierattention mechanisms, resulting in slower inference speeds. In this paper, weintroduce a training-free method to accelerate video DiTs, termed AdaptiveCaching (AdaCache), which is motivated by the fact that "not all videos arecreated equal": meaning, some videos require fewer denoising steps to attain areasonable quality than others. Building on this, we not only cachecomputations through the diffusion process, but also devise a caching scheduletailored to each video generation, maximizing the quality-latency trade-off. Wefurther introduce a Motion Regularization (MoReg) scheme to utilize videoinformation within AdaCache, essentially controlling the compute allocationbased on motion content. Altogether, our plug-and-play contributions grantsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s videogeneration) without sacrificing the generation quality, across multiple videoDiT baselines.</description><author>Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Michael S. Ryoo, Tian Xie</author><pubDate>Mon, 04 Nov 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02397v1</guid></item><item><title>Training-free Regional Prompting for Diffusion Transformers</title><link>http://arxiv.org/abs/2411.02395v1</link><description>Diffusion models have demonstrated excellent capabilities in text-to-imagegeneration. Their semantic understanding (i.e., prompt following) ability hasalso been greatly improved with large language models (e.g., T5, Llama).However, existing models cannot perfectly handle long and complex text prompts,especially when the text prompts contain various objects with numerousattributes and interrelated spatial relationships. While many regionalprompting methods have been proposed for UNet-based models (SD1.5, SDXL), butthere are still no implementations based on the recent Diffusion Transformer(DiT) architecture, such as SD3 and FLUX.1.In this report, we propose andimplement regional prompting for FLUX.1 based on attention manipulation, whichenables DiT with fined-grained compositional text-to-image generationcapability in a training-free manner. Code is available athttps://github.com/antonioo-c/Regional-Prompting-FLUX.</description><author>Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, Shanghang Zhang</author><pubDate>Mon, 04 Nov 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02395v1</guid></item><item><title>AutoVFX: Physically Realistic Video Editing from Natural Language Instructions</title><link>http://arxiv.org/abs/2411.02394v1</link><description>Modern visual effects (VFX) software has made it possible for skilled artiststo create imagery of virtually anything. However, the creation process remainslaborious, complex, and largely inaccessible to everyday users. In this work,we present AutoVFX, a framework that automatically creates realistic anddynamic VFX videos from a single video and natural language instructions. Bycarefully integrating neural scene modeling, LLM-based code generation, andphysical simulation, AutoVFX is able to provide physically-grounded,photorealistic editing effects that can be controlled directly using naturallanguage instructions. We conduct extensive experiments to validate AutoVFX'sefficacy across a diverse spectrum of videos and instructions. Quantitative andqualitative results suggest that AutoVFX outperforms all competing methods by alarge margin in generative quality, instruction alignment, editing versatility,and physical plausibility.</description><author>Hao-Yu Hsu, Zhi-Hao Lin, Albert Zhai, Hongchi Xia, Shenlong Wang</author><pubDate>Mon, 04 Nov 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02394v1</guid></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>http://arxiv.org/abs/2406.19370v2</link><description>Modern generative models demonstrate impressive capabilities, likely stemmingfrom an ability to identify and manipulate abstract concepts underlying theirtraining data. However, fundamental questions remain: what determines theconcepts a model learns, the order in which it learns them, and its ability tomanipulate those concepts? To address these questions, we propose analyzing amodel's learning dynamics via a framework we call the concept space, where eachaxis represents an independent concept underlying the data generating process.By characterizing learning dynamics in this space, we identify how the speed atwhich a concept is learned, and hence the order of concept learning, iscontrolled by properties of the data we term concept signal. Further, weobserve moments of sudden turns in the direction of a model's learning dynamicsin concept space. Surprisingly, these points precisely correspond to theemergence of hidden capabilities, i.e., where latent interventions show themodel possesses the capability to manipulate a concept, but these capabilitiescannot yet be elicited via naive input prompting. While our results focus onsynthetically defined toy datasets, we hypothesize a general claim on emergenceof hidden capabilities may hold: generative models possess latent capabilitiesthat emerge suddenly and consistently during training, though a model might notexhibit these capabilities under naive input prompting.</description><author>Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka</author><pubDate>Mon, 04 Nov 2024 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19370v2</guid></item><item><title>Adaptive Length Image Tokenization via Recurrent Allocation</title><link>http://arxiv.org/abs/2411.02393v1</link><description>Current vision systems typically assign fixed-length representations toimages, regardless of the information content. This contrasts with humanintelligence - and even large language models - which allocate varyingrepresentational capacities based on entropy, context and familiarity. Inspiredby this, we propose an approach to learn variable-length token representationsfor 2D images. Our encoder-decoder architecture recursively processes 2D imagetokens, distilling them into 1D latent tokens over multiple iterations ofrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing1D latent tokens, and adaptively increases representational capacity by addingnew tokens. This enables compression of images into a variable number oftokens, ranging from 32 to 256. We validate our tokenizer using reconstructionloss and FID metrics, demonstrating that token count aligns with image entropy,familiarity and downstream task requirements. Recurrent token processing withincreasing representational capacity in each iteration shows signs of tokenspecialization, revealing potential for object / part discovery.</description><author>Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman</author><pubDate>Mon, 04 Nov 2024 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02393v1</guid></item><item><title>Attacking Vision-Language Computer Agents via Pop-ups</title><link>http://arxiv.org/abs/2411.02391v1</link><description>Autonomous agents powered by large vision and language models (VLM) havedemonstrated significant potential in completing daily computer tasks, such asbrowsing the web to book travel and operating desktop software, which requiresagents to understand these interfaces. Despite such visual inputs becoming moreintegrated into agentic applications, what types of risks and attacks existaround them still remain unclear. In this work, we demonstrate that VLM agentscan be easily attacked by a set of carefully designed adversarial pop-ups,which human users would typically recognize and ignore. This distraction leadsagents to click these pop-ups instead of performing the tasks as usual.Integrating these pop-ups into existing agent testing environments like OSWorldand VisualWebArena leads to an attack success rate (the frequency of the agentclicking the pop-ups) of 86% on average and decreases the task success rate by47%. Basic defense techniques such as asking the agent to ignore pop-ups orincluding an advertisement notice, are ineffective against the attack.</description><author>Yanzhe Zhang, Tao Yu, Diyi Yang</author><pubDate>Mon, 04 Nov 2024 18:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02391v1</guid></item><item><title>From Imitation to Refinement -- Residual RL for Precise Assembly</title><link>http://arxiv.org/abs/2407.16677v2</link><description>Recent advances in behavior cloning (BC), like action-chunking and diffusion,have led to impressive progress. Still, imitation alone remains insufficientfor tasks requiring reliable and precise movements, such as aligning andinserting objects. Our key insight is that chunked BC policies function astrajectory planners, enabling long-horizon tasks. Conversely, as they executeaction chunks open-loop, they lack the fine-grained reactivity necessary forreliable execution. Further, we find that the performance of BC policiessaturates despite increasing data. Reinforcement learning (RL) is a natural wayto overcome this, but it is not straightforward to apply directly toaction-chunked models like diffusion policies. We present a simple yeteffective method, ResiP (Residual for Precise Manipulation), that sidestepsthese challenges by augmenting a frozen, chunked BC model with a fullyclosed-loop residual policy trained with RL. The residual policy is trained viaon-policy RL, addressing distribution shifts and introducing reactivity withoutaltering the BC trajectory planner. Evaluation on high-precision manipulationtasks demonstrates strong performance of ResiP over BC methods and direct RLfine-tuning. Videos, code, and data are available at\url{https://residual-assembly.github.io}.</description><author>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal</author><pubDate>Mon, 04 Nov 2024 18:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16677v2</guid></item><item><title>How Far is Video Generation from World Model: A Physical Law Perspective</title><link>http://arxiv.org/abs/2411.02385v1</link><description>OpenAI's Sora highlights the potential of video generation for developingworld models that adhere to fundamental physical laws. However, the ability ofvideo generation models to discover such laws purely from visual data withouthuman priors can be questioned. A world model learning the true law should givepredictions robust to nuances and correctly extrapolate on unseen scenarios. Inthis work, we evaluate across three key scenarios: in-distribution,out-of-distribution, and combinatorial generalization. We developed a 2Dsimulation testbed for object movement and collisions to generate videosdeterministically governed by one or more classical mechanics laws. Thisprovides an unlimited supply of data for large-scale experimentation andenables quantitative evaluation of whether the generated videos adhere tophysical laws. We trained diffusion-based video generation models to predictobject movements based on initial frames. Our scaling experiments show perfectgeneralization within the distribution, measurable scaling behavior forcombinatorial generalization, but failure in out-of-distribution scenarios.Further experiments reveal two key insights about the generalization mechanismsof these models: (1) the models fail to abstract general physical rules andinstead exhibit "case-based" generalization behavior, i.e., mimicking theclosest training example; (2) when generalizing to new cases, models areobserved to prioritize different factors when referencing training data: color&gt; size &gt; velocity &gt; shape. Our study suggests that scaling alone isinsufficient for video generation models to uncover fundamental physical laws,despite its role in Sora's broader success. See our project page athttps://phyworld.github.io</description><author>Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng</author><pubDate>Mon, 04 Nov 2024 18:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02385v1</guid></item><item><title>Learning to Price Homogeneous Data</title><link>http://arxiv.org/abs/2407.05484v2</link><description>We study a data pricing problem, where a seller has access to $N$ homogeneousdata points (e.g. drawn i.i.d. from some distribution). There are $m$ types ofbuyers in the market, where buyers of the same type $i$ have the same valuationcurve $v_i:[N]\rightarrow [0,1]$, where $v_i(n)$ is the value for having $n$data points. A priori, the seller is unaware of the distribution of buyers, butcan repeat the market for $T$ rounds so as to learn the revenue-optimal pricingcurve $p:[N] \rightarrow [0, 1]$. To solve this online learning problem, wefirst develop novel discretization schemes to approximate any pricing curve.When compared to prior work, the size of our discretization schemes scalesgracefully with the approximation parameter, which translates to better regretin online learning. Under assumptions like smoothness and diminishing returnswhich are satisfied by data, the discretization size can be reduced further. Wethen turn to the online learning problem, both in the stochastic andadversarial settings. On each round, the seller chooses an anonymous pricingcurve $p_t$. A new buyer appears and may choose to purchase some amount ofdata. She then reveals her type only if she makes a purchase. Our onlinealgorithms build on classical algorithms such as UCB and FTPL, but requirenovel ideas to account for the asymmetric nature of this feedback and to dealwith the vastness of the space of pricing curves. Using the improveddiscretization schemes previously developed, we are able to achieve$\tilde{O}(m\sqrt{T})$ regret in the stochastic setting and$\tilde{O}(m^{3/2}\sqrt{T})$ regret in the adversarial setting.</description><author>Keran Chen, Joon Suk Huh, Kirthevasan Kandasamy</author><pubDate>Mon, 04 Nov 2024 18:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05484v2</guid></item><item><title>Linear Causal Bandits: Unknown Graph and Soft Interventions</title><link>http://arxiv.org/abs/2411.02383v1</link><description>Designing causal bandit algorithms depends on two central categories ofassumptions: (i) the extent of information about the underlying causal graphsand (ii) the extent of information about interventional statistical models.There have been extensive recent advances in dispensing with assumptions oneither category. These include assuming known graphs but unknown interventionaldistributions, and the converse setting of assuming unknown graphs but accessto restrictive hard/$\operatorname{do}$ interventions, which removes thestochasticity and ancestral dependencies. Nevertheless, the problem in itsgeneral form, i.e., unknown graph and unknown stochastic intervention models,remains open. This paper addresses this problem and establishes that in a graphwith $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after$T$ interaction rounds the regret upper bound scales as$\tilde{\mathcal{O}}((cd)^{L-\frac{1}{2}}\sqrt{T} + d + RN)$ where $c&gt;1$ is aconstant and $R$ is a measure of intervention power. A universal minimax lowerbound is also established, which scales as $\Omega(d^{L-\frac{3}{2}}\sqrt{T})$.Importantly, the graph size $N$ has a diminishing effect on the regret as $T$grows. These bounds have matching behavior in $T$, exponential dependence on$L$, and polynomial dependence on $d$ (with the gap $d\ $). On the algorithmicaspect, the paper presents a novel way of designing a computationally efficientCB algorithm, addressing a challenge that the existing CB algorithms using softinterventions face.</description><author>Zirui Yan, Ali Tajer</author><pubDate>Mon, 04 Nov 2024 18:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02383v1</guid></item><item><title>Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models</title><link>http://arxiv.org/abs/2411.02382v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities invarious scientific domains, from natural language processing to complexproblem-solving tasks. Their ability to understand and generate human-like texthas opened up new possibilities for advancing scientific research, enablingtasks such as data analysis, literature review, and even experimental design.One of the most promising applications of LLMs in this context is hypothesisgeneration, where they can identify novel research directions by analyzingexisting knowledge. However, despite their potential, LLMs are prone togenerating ``hallucinations'', outputs that are plausible-sounding butfactually incorrect. Such a problem presents significant challenges inscientific fields that demand rigorous accuracy and verifiability, potentiallyleading to erroneous or misleading conclusions. To overcome these challenges,we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system thatenhances LLM hypothesis generation by integrating external, structuredknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structuredreasoning process, organizing their output as a chain of ideas (CoI), andincludes a KG-supported module for the detection of hallucinations. Withexperiments on our newly constructed hypothesis generation dataset, wedemonstrate that KG-CoI not only improves the accuracy of LLM-generatedhypotheses but also reduces the hallucination in their reasoning chains,highlighting its effectiveness in advancing real-world scientific research.</description><author>Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang</author><pubDate>Mon, 04 Nov 2024 18:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02382v1</guid></item><item><title>Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI</title><link>http://arxiv.org/abs/2411.02381v1</link><description>In this paper, we present a dynamic semantic clustering approach inspired bythe Chinese Restaurant Process, aimed at addressing uncertainty in theinference of Large Language Models (LLMs). We quantify uncertainty of an LLM ona given query by calculating entropy of the generated semantic clusters.Further, we propose leveraging the (negative) likelihood of these clusters asthe (non)conformity score within Conformal Prediction framework, allowing themodel to predict a set of responses instead of a single output, therebyaccounting for uncertainty in its predictions. We demonstrate the effectivenessof our uncertainty quantification (UQ) technique on two well known questionanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 andMistral. Our approach achieves SOTA performance in UQ, as assessed by metricssuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shownto produce smaller prediction sets while maintaining the same probabilisticguarantee of including the correct response, in comparison to existing SOTAconformal prediction baseline.</description><author>Ramneet Kaur, Colin Samplawski, Adam D. Cobb, Anirban Roy, Brian Matejek, Manoj Acharya, Daniel Elenius, Alexander M. Berenbeim, John A. Pavlik, Nathaniel D. Bastian, Susmit Jha</author><pubDate>Mon, 04 Nov 2024 18:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02381v1</guid></item><item><title>AmbigNLG: Addressing Task Ambiguity in Instruction for NLG</title><link>http://arxiv.org/abs/2402.17717v4</link><description>We introduce AmbigNLG, a novel task designed to tackle the challenge of taskambiguity in instructions for Natural Language Generation (NLG). Ambiguousinstructions often impede the performance of Large Language Models (LLMs),especially in complex NLG tasks. To tackle this issue, we propose an ambiguitytaxonomy that categorizes different types of instruction ambiguities andrefines initial instructions with clearer specifications. Accompanying thistask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotatedto facilitate research in AmbigNLG. Through comprehensive experiments withstate-of-the-art LLMs, we demonstrate that our method significantly enhancesthe alignment of generated text with user expectations, achieving up to a15.02-point increase in ROUGE scores. Our findings highlight the criticalimportance of addressing task ambiguity to fully harness the capabilities ofLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method inpractical settings involving interactive ambiguity mitigation with users,underscoring the benefits of leveraging LLMs for interactive clarification.</description><author>Ayana Niwa, Hayate Iso</author><pubDate>Mon, 04 Nov 2024 18:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17717v4</guid></item><item><title>EMMA: End-to-End Multimodal Model for Autonomous Driving</title><link>http://arxiv.org/abs/2410.23262v2</link><description>We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.Built on a multi-modal large language model foundation, EMMA directly maps rawcamera sensor data into various driving-specific outputs, including plannertrajectories, perception objects, and road graph elements. EMMA maximizes theutility of world knowledge from the pre-trained large language models, byrepresenting all non-sensor inputs (e.g. navigation instructions and egovehicle status) and outputs (e.g. trajectories and 3D locations) as naturallanguage text. This approach allows EMMA to jointly process various drivingtasks in a unified language space, and generate the outputs for each task usingtask-specific prompts. Empirically, we demonstrate EMMA's effectiveness byachieving state-of-the-art performance in motion planning on nuScenes as wellas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA alsoyields competitive results for camera-primary 3D object detection on the WaymoOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,object detection, and road graph tasks yields improvements across all threedomains, highlighting EMMA's potential as a generalist model for autonomousdriving applications. However, EMMA also exhibits certain limitations: it canprocess only a small amount of image frames, does not incorporate accurate 3Dsensing modalities like LiDAR or radar and is computationally expensive. Wehope that our results will inspire further research to mitigate these issuesand to further evolve the state of the art in autonomous driving modelarchitectures.</description><author>Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou, James Guo, Dragomir Anguelov, Mingxing Tan</author><pubDate>Mon, 04 Nov 2024 18:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23262v2</guid></item><item><title>Towards Generative Ray Path Sampling for Faster Point-to-Point Ray Tracing</title><link>http://arxiv.org/abs/2410.23773v2</link><description>Radio propagation modeling is essential in telecommunication research, asradio channels result from complex interactions with environmental objects.Recently, Machine Learning has been attracting attention as a potentialalternative to computationally demanding tools, like Ray Tracing, which canmodel these interactions in detail. However, existing Machine Learningapproaches often attempt to learn directly specific channel characteristics,such as the coverage map, making them highly specific to the frequency andmaterial properties and unable to fully capture the underlying propagationmechanisms. Hence, Ray Tracing, particularly the Point-to-Point variant,remains popular to accurately identify all possible paths between transmitterand receiver nodes. Still, path identification is computationally intensivebecause the number of paths to be tested grows exponentially while only a smallfraction is valid. In this paper, we propose a Machine Learning-aided RayTracing approach to efficiently sample potential ray paths, significantlyreducing the computational load while maintaining high accuracy. Our modeldynamically learns to prioritize potentially valid paths among all possiblepaths and scales linearly with scene complexity. Unlike recent alternatives,our approach is invariant with translation, scaling, or rotation of thegeometry, and avoids dependency on specific environment characteristics.</description><author>Jérome Eertmans, Nicola Di Cicco, Claude Oestges, Laurent Jacques, Enrico M. Vittuci, Vittorio Degli-Esposti</author><pubDate>Mon, 04 Nov 2024 18:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23773v2</guid></item><item><title>Optimal Design for Human Preference Elicitation</title><link>http://arxiv.org/abs/2404.13895v3</link><description>Learning of preference models from human feedback has been central to recentadvances in artificial intelligence. Motivated by the cost of obtaininghigh-quality human annotations, we study efficient human preference elicitationfor learning preference models. The key idea in our work is to generalizeoptimal designs, a methodology for computing optimal information-gatheringpolicies, to questions with multiple answers, represented as lists of items.The policy is a distribution over lists and we elicit preferences from the listproportionally to its probability. To show the generality of our ideas, westudy both absolute and ranking feedback models on items in the list. We designefficient algorithms for both and analyze them. Finally, we demonstrate thatour algorithms are practical by evaluating them on existing question-answeringproblems.</description><author>Subhojyoti Mukherjee, Anusha Lalitha, Kousha Kalantari, Aniket Deshmukh, Ge Liu, Yifei Ma, Branislav Kveton</author><pubDate>Mon, 04 Nov 2024 18:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13895v3</guid></item><item><title>Learning General-Purpose Biomedical Volume Representations using Randomized Synthesis</title><link>http://arxiv.org/abs/2411.02372v1</link><description>Current volumetric biomedical foundation models struggle to generalize aspublic 3D datasets are small and do not cover the broad diversity of medicalprocedures, conditions, anatomical regions, and imaging protocols. We addressthis by creating a representation learning method that instead anticipatesstrong domain shifts at training time itself. We first propose a data enginethat synthesizes highly variable training samples that enable generalization tonew biomedical contexts. To then train a single 3D network for any voxel-leveltask, we develop a contrastive learning method that pretrains the network to bestable against nuisance imaging variation simulated by the data engine, a keyinductive bias for generalization. This network's features can be used asrobust representations of input images for downstream tasks and its weightsprovide a strong, dataset-agnostic initialization for finetuning on newdatasets. As a result, we set new standards across both multimodalityregistration and few-shot segmentation, a first for any 3D biomedical visionmodel, all without (pre-)training on any existing dataset of real images.</description><author>Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland</author><pubDate>Mon, 04 Nov 2024 18:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02372v1</guid></item><item><title>Effective Layer Pruning Through Similarity Metric Perspective</title><link>http://arxiv.org/abs/2405.17081v2</link><description>Deep neural networks have been the predominant paradigm in machine learningfor solving cognitive tasks. Such models, however, are restricted by a highcomputational overhead, limiting their applicability and hindering advancementsin the field. Extensive research demonstrated that pruning structures fromthese models is a straightforward approach to reducing network complexity. Inthis direction, most efforts focus on removing weights or filters. Studies havealso been devoted to layer pruning as it promotes superior computational gains.However, layer pruning often hurts the network predictive ability (i.e.,accuracy) at high compression rates. This work introduces an effectivelayer-pruning strategy that meets all underlying properties pursued by pruningmethods. Our method estimates the relative importance of a layer using theCentered Kernel Alignment (CKA) metric, employed to measure the similaritybetween the representations of the unpruned model and a candidate layer forpruning. We confirm the effectiveness of our method on standard architecturesand benchmarks, in which it outperforms existing layer-pruning strategies andother state-of-the-art pruning techniques. Particularly, we remove more than75% of computation while improving predictive ability. At higher compressionregimes, our method exhibits negligible accuracy drop, while other methodsnotably deteriorate model accuracy. Apart from these benefits, our prunedmodels exhibit robustness to adversarial and out-of-distribution samples.</description><author>Ian Pons, Bruno Yamamoto, Anna H. Reali Costa, Artur Jordao</author><pubDate>Mon, 04 Nov 2024 18:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17081v2</guid></item><item><title>DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs</title><link>http://arxiv.org/abs/2406.12072v3</link><description>Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-worldscenarios, where each node and edge are associated with text descriptions, andboth the graph structure and text descriptions evolve over time. Despite theirbroad applicability, there is a notable scarcity of benchmark datasets tailoredto DyTAGs, which hinders the potential advancement in many research fields. Toaddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),a collection of large-scale, time-evolving graphs from diverse domains, withnodes and edges enriched by dynamically changing text attributes andcategories. To facilitate the use of DTGB, we design standardized evaluationprocedures based on four real-world use cases: future link prediction,destination node retrieval, edge classification, and textual relationgeneration. These tasks require models to understand both dynamic graphstructures and natural language, highlighting the unique challenges posed byDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,evaluating 7 popular dynamic graph learning algorithms and their variants ofadapting to text attributes with LLM embeddings, along with 6 powerful largelanguage models (LLMs). Our results show the limitations of existing models inhandling DyTAGs. Our analysis also demonstrates the utility of DTGB ininvestigating the incorporation of structural and textual dynamics. Theproposed DTGB fosters research on DyTAGs and their broad applications. Itoffers a comprehensive benchmark for evaluating and advancing models to handlethe interplay between dynamic graph structures and natural language. Thedataset and source code are available at https://github.com/zjs123/DTGB.</description><author>Jiasheng Zhang, Jialin Chen, Menglin Yang, Aosong Feng, Shuang Liang, Jie Shao, Rex Ying</author><pubDate>Mon, 04 Nov 2024 18:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12072v3</guid></item><item><title>Bypassing the Noisy Parity Barrier: Learning Higher-Order Markov Random Fields from Dynamics</title><link>http://arxiv.org/abs/2409.05284v2</link><description>We consider the problem of learning graphical models, also known as Markovrandom fields (MRFs) from temporally correlated samples. As in many traditionalstatistical settings, fundamental results in the area all assume independentsamples from the distribution. However, these samples generally will notdirectly correspond to more realistic observations from nature, which insteadevolve according to some stochastic process. From the computational lens, evengenerating a single sample from the true MRF distribution is intractable unless$\mathsf{NP}=\mathsf{RP}$, and moreover, any algorithm to learn from i.i.d.samples requires prohibitive runtime due to hardness reductions to the paritywith noise problem. These computational barriers for sampling and learning fromthe i.i.d. setting severely lessen the utility of these breakthrough resultsfor this important task; however, dropping this assumption typically onlyintroduces further algorithmic and statistical complexities. In this work, we surprisingly demonstrate that the direct trajectory datafrom a natural evolution of the MRF overcomes the fundamental computationallower bounds to efficient learning. In particular, we show that given atrajectory with $\widetilde{O}_k(n)$ site updates of an order $k$ MRF from theGlauber dynamics, a well-studied, natural stochastic process on graphicalmodels, there is an algorithm that recovers the graph and the parameters in$\widetilde{O}_k(n^2)$ time. By contrast, all prior algorithms for learningorder $k$ MRFs inherently suffer from $n^{\Theta(k)}$ runtime even in sparseinstances due to the reductions to sparse parity with noise. Our results thussurprisingly show that this more realistic, but intuitively less tractable,model for MRFs actually leads to efficiency far beyond what is known andbelieved to be true in the traditional i.i.d. case.</description><author>Jason Gaitonde, Ankur Moitra, Elchanan Mossel</author><pubDate>Mon, 04 Nov 2024 18:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05284v2</guid></item><item><title>DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</title><link>http://arxiv.org/abs/2411.02359v1</link><description>MLLMs have demonstrated remarkable comprehension and reasoning capabilitieswith complex language and visual data. These advances have spurred the visionof establishing a generalist robotic MLLM proficient in understanding complexhuman instructions and accomplishing various embodied tasks. However,developing MLLMs for real-world robots is challenging due to the typicallylimited computation and memory capacities available on robotic platforms. Incontrast, the inference of MLLMs involves storing billions of parameters andperforming tremendous computation, imposing significant hardware demands. Inour paper, we propose a Dynamic Early-Exit Framework for RoboticVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automaticallyadjusts the size of the activated MLLM based on each situation at hand. Theapproach leverages a multi-exit architecture in MLLMs, which allows the modelto terminate processing once a proper size of the model has been activated fora specific situation, thus avoiding further redundant computation.Additionally, we develop novel algorithms that establish early-terminationcriteria for DeeR, conditioned on predefined demands such as averagecomputational cost (i.e., power consumption), as well as peak computationalconsumption (i.e., latency) and GPU memory usage. These enhancements ensurethat DeeR operates efficiently under varying resource constraints whilemaintaining competitive performance. On the CALVIN robot manipulationbenchmark, DeeR demonstrates significant reductions in computational costs ofLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.</description><author>Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, Gao Huang</author><pubDate>Mon, 04 Nov 2024 18:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02359v1</guid></item><item><title>"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization</title><link>http://arxiv.org/abs/2411.02355v1</link><description>Despite the popularity of large language model (LLM) quantization forinference acceleration, significant uncertainty remains regarding theaccuracy-performance trade-offs associated with various quantization formats.We present a comprehensive empirical study of quantized accuracy, evaluatingpopular quantization formats (FP8, INT8, INT4) across academic benchmarks andreal-world tasks, on the entire Llama-3.1 model family. Additionally, our studyexamines the difference in text generated by quantized models versus theiruncompressed counterparts. Beyond benchmarks, we also present a couple ofquantization improvements which allowed us to obtain state-of-the-art accuracyrecovery results. Our investigation, encompassing over 500,000 individualevaluations, yields several key findings: (1) FP8 weight and activationquantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight andactivation quantization (W8A8-INT), when properly tuned, incurs surprisinglylow 1-3% accuracy degradation, and (3) INT4 weight-only quantization(W4A16-INT) is competitive with 8-bit integer weight and activationquantization. To address the question of the "best" format for a givendeployment environment, we conduct inference performance analysis using thepopular open-source vLLM framework on various GPU architectures. We find thatW4A16 offers the best cost-efficiency for synchronous deployments, and forasynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excelin asynchronous "continuous batching" deployment of mid- and large-size modelson high-end GPUs. Our results provide a set of practical guidelines fordeploying quantized LLMs across scales and performance requirements.</description><author>Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh</author><pubDate>Mon, 04 Nov 2024 18:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02355v1</guid></item><item><title>Machine learning identification of maternal inflammatory response and histologic choroamnionitis from placental membrane whole slide images</title><link>http://arxiv.org/abs/2411.02354v1</link><description>The placenta forms a critical barrier to infection through pregnancy, laborand, delivery. Inflammatory processes in the placenta have short-term, andlong-term consequences for offspring health. Digital pathology and machinelearning can play an important role in understanding placental inflammation,and there have been very few investigations into methods for predicting andunderstanding Maternal Inflammatory Response (MIR). This work intends toinvestigate the potential of using machine learning to understand MIR based onwhole slide images (WSI), and establish early benchmarks. To that end, we useMultiple Instance Learning framework with 3 feature extractors: ImageNet-basedEfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon toinvestigate predictability of MIR stage from histopathology WSIs. We alsointerpret predictions from these models using the learned attention maps fromthese models. We also use the MIL framework for predicting white blood cellscount (WBC) and maximum fever temperature ($T_{max}$). Attention-based MILmodels are able to classify MIR with a balanced accuracy of up to 88.5% with aCohen's Kappa ($\kappa$) of up to 0.772. Furthermore, we found that thepathology foundation models (UNI and Phikon) are both able to achieve higherperformance with balanced accuracy and $\kappa$, compared to ImageNet-basedfeature extractor (EfficientNet-v2s). For WBC and $T_{max}$ prediction, wefound mild correlation between actual values and those predicted fromhistopathology WSIs. We used MIL framework for predicting MIR stage from WSIs,and compared effectiveness of foundation models as feature extractors, withthat of an ImageNet-based model. We further investigated model failure casesand found them to be either edge cases prone to interobserver variability,examples of pathologist's overreach, or mislabeled due to processing errors.</description><author>Abhishek Sharma, Ramin Nateghi, Marina Ayad, Lee A. D. Cooper, Jeffery A. Goldstein</author><pubDate>Mon, 04 Nov 2024 18:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02354v1</guid></item><item><title>Can Large Language Models generalize analogy solving like people can?</title><link>http://arxiv.org/abs/2411.02348v1</link><description>When we solve an analogy we transfer information from a known context to anew one through abstract rules and relational similarity. In people, theability to solve analogies such as "body : feet :: table : ?" emerges inchildhood, and appears to transfer easily to other domains, such as the visualdomain "( : ) :: &lt; : ?". Recent research shows that large language models(LLMs) can solve various forms of analogies. However, can LLMs generalizeanalogy solving to new domains like people can? To investigate this, we hadchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greekalphabet), and a far transfer domain (list of symbols). As expected, childrenand adults easily generalized their knowledge to unfamiliar domains, whereasLLMs did not. This key difference between human and AI performance is evidencethat these LLMs still struggle with robust human-like analogical transfer.</description><author>Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell</author><pubDate>Mon, 04 Nov 2024 18:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02348v1</guid></item><item><title>Physically Based Neural Bidirectional Reflectance Distribution Function</title><link>http://arxiv.org/abs/2411.02347v1</link><description>We introduce the physically based neural bidirectional reflectancedistribution function (PBNBRDF), a novel, continuous representation formaterial appearance based on neural fields. Our model accurately reconstructsreal-world materials while uniquely enforcing physical properties for realisticBRDFs, specifically Helmholtz reciprocity via reparametrization and energypassivity via efficient analytical integration. We conduct a systematicanalysis demonstrating the benefits of adhering to these physical laws on thevisual quality of reconstructed materials. Additionally, we enhance the coloraccuracy of neural BRDFs by introducing chromaticity enforcement supervisingthe norms of RGB channels. Through both qualitative and quantitativeexperiments on multiple databases of measured real-world BRDFs, we show thatadhering to these physical constraints enables neural fields to more faithfullyand stably represent the original data and achieve higher rendering quality.</description><author>Chenliang Zhou, Alejandro Sztrajman, Gilles Rainer, Fangcheng Zhong, Fazilet Gokbudak, Zhilin Guo, Weihao Xia, Rafal Mantiuk, Cengiz Oztireli</author><pubDate>Mon, 04 Nov 2024 18:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02347v1</guid></item><item><title>Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking</title><link>http://arxiv.org/abs/2411.02345v1</link><description>Nanorobots are a promising development in targeted drug delivery and thetreatment of neurological disorders, with potential for crossing theblood-brain barrier (BBB). These small devices leverage advancements innanotechnology and bioengineering for precise navigation and targeted payloaddelivery, particularly for conditions like brain tumors, Alzheimer's disease,and Parkinson's disease. Recent progress in artificial intelligence (AI) andmachine learning (ML) has improved the navigation and effectiveness ofnanorobots, allowing them to detect and interact with cancer cells throughbiomarker analysis. This study presents a new reinforcement learning (RL)framework for optimizing nanorobot navigation in complex biologicalenvironments, focusing on cancer cell detection by analyzing the concentrationgradients of surrounding biomarkers. We utilize a computer simulation model toexplore the behavior of nanorobots in a three-dimensional space with cancercells and biological barriers. The proposed method uses Q-learning to refinemovement strategies based on real-time biomarker concentration data, enablingnanorobots to autonomously navigate to cancerous tissues for targeted drugdelivery. This research lays the groundwork for future laboratory experimentsand clinical applications, with implications for personalized medicine and lessinvasive cancer treatments. The integration of intelligent nanorobots couldrevolutionize therapeutic strategies, reducing side effects and enhancingtreatment effectiveness for cancer patients. Further research will investigatethe practical deployment of these technologies in medical settings, aiming tounlock the full potential of nanorobotics in healthcare.</description><author>Shahab Kavousinejad</author><pubDate>Mon, 04 Nov 2024 18:16:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02345v1</guid></item><item><title>Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning</title><link>http://arxiv.org/abs/2411.02344v1</link><description>Decoder-only Transformers often struggle with complex reasoning tasks,particularly arithmetic reasoning requiring multiple sequential operations. Inthis work, we identify representation collapse in the model's intermediatelayers as a key factor limiting their reasoning capabilities. To address this,we propose Sequential Variance-Covariance Regularization (Seq-VCR), whichenhances the entropy of intermediate representations and prevents collapse.Combined with dummy pause tokens as substitutes for chain-of-thought (CoT)tokens, our method significantly improves performance in arithmetic reasoningproblems. In the challenging $5 \times 5$ integer multiplication task, ourapproach achieves $99.5\%$ exact match accuracy, outperforming models of thesame size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting($44\%$). We also demonstrate superior results on arithmetic expression andlongest increasing subsequence (LIS) datasets. Our findings highlight theimportance of preventing intermediate layer representation collapse to enhancethe reasoning capabilities of Transformers and show that Seq-VCR offers aneffective solution without requiring explicit CoT supervision.</description><author>Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal</author><pubDate>Mon, 04 Nov 2024 18:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02344v1</guid></item><item><title>Boulder2Vec: Modeling Climber Performances in Professional Bouldering Competitions</title><link>http://arxiv.org/abs/2411.02343v1</link><description>Using data from professional bouldering competitions from 2008 to 2022, wetrain a logistic regression to predict climber results and measure climberskill. However, this approach is limited, as a single numeric coefficient perclimber cannot adequately capture the intricacies of climbers' varyingstrengths and weaknesses in different boulder problems. For example, someclimbers might prefer more static, technical routes while other climbers mayspecialize in powerful, dynamic problems. To this end, we apply Probabilistic Matrix Factorization (PMF), a frameworkcommonly used in recommender systems, to represent the unique characteristicsof climbers and problems with latent, multi-dimensional vectors. In thisframework, a climber's performance on a given problem is predicted by takingthe dot product of the corresponding climber vector and problem vectors. PMFeffectively handles sparse datasets, such as our dataset where only a subset ofclimbers attempt each particular problem, by extrapolating patterns fromsimilar climbers. We contrast the empirical performance of PMF to the logistic regressionapproach and investigate the multivariate representations produced by PMF togain insights into climber characteristics. Our results show that themultivariate PMF representations improve predictive performance of professionalbouldering competitions by capturing both the overall strength of climbers andtheir specialized skill sets.</description><author>Ethan Baron, Victor Hau, Zeke Weng</author><pubDate>Mon, 04 Nov 2024 18:12:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02343v1</guid></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>http://arxiv.org/abs/2411.02337v1</link><description>Large language models (LLMs) have shown remarkable potential as autonomousagents, particularly in web-based tasks. However, existing LLM web agentsheavily rely on expensive proprietary LLM APIs, while open LLMs lack thenecessary decision-making capabilities. This paper introduces WebRL, aself-evolving online curriculum reinforcement learning framework designed totrain high-performance web agents using open LLMs. WebRL addresses three keychallenges in building LLM web agents, including the scarcity of trainingtasks, sparse feedback signals, and policy distribution drift in onlinelearning. Specifically, WebRL incorporates 1) a self-evolving curriculum thatgenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervisedreward model (ORM), and 3) adaptive reinforcement learning strategies to ensureconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4models into proficient web agents. On WebArena-Lite, WebRL improves the successrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.These open models significantly surpass the performance of GPT-4-Turbo (17.6%)and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trainedon open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL'seffectiveness in bridging the gap between open and proprietary LLM-based webagents, paving the way for more accessible and powerful autonomous webinteraction systems.</description><author>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong</author><pubDate>Mon, 04 Nov 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02337v1</guid></item><item><title>MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D</title><link>http://arxiv.org/abs/2411.02336v1</link><description>Texturing is a crucial step in the 3D asset production workflow, whichenhances the visual appeal and diversity of 3D assets. Despite recentadvancements in Text-to-Texture (T2T) generation, existing methods often yieldsubpar results, primarily due to local discontinuities, inconsistencies acrossmultiple views, and their heavy dependence on UV unwrapping outcomes. To tacklethese challenges, we propose a novel generation-refinement 3D texturingframework called MVPaint, which can generate high-resolution, seamless textureswhile emphasizing multi-view consistency. MVPaint mainly consists of three keymodules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model,MVPaint first simultaneously generates multi-view images by employing an SMGmodel, which leads to coarse texturing results with unpainted parts due tomissing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete3D texturing, we introduce the S3I method, specifically designed to effectivelytexture previously unobserved areas. 3) UV Refinement (UVR). Furthermore,MVPaint employs a UVR module to improve the texture quality in the UV space,which first performs a UV-space Super-Resolution, followed by a Spatial-awareSeam-Smoothing algorithm for revising spatial texturing discontinuities causedby UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: theObjaverse T2T benchmark and the GSO T2T benchmark, based on selectedhigh-quality 3D meshes from the Objaverse dataset and the entire GSO dataset,respectively. Extensive experimental results demonstrate that MVPaint surpassesexisting state-of-the-art methods. Notably, MVPaint could generatehigh-fidelity textures with minimal Janus issues and highly enhanced cross-viewconsistency.</description><author>Wei Cheng, Juncheng Mu, Xianfang Zeng, Xin Chen, Anqi Pang, Chi Zhang, Zhibin Wang, Bin Fu, Gang Yu, Ziwei Liu, Liang Pan</author><pubDate>Mon, 04 Nov 2024 17:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02336v1</guid></item><item><title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title><link>http://arxiv.org/abs/2411.02335v1</link><description>Activation sparsity denotes the existence of substantial weakly-contributedelements within activation outputs that can be eliminated, benefiting manyimportant applications concerned with large language models (LLMs). Althoughpromoting greater activation sparsity within LLMs deserves deep studies,existing works lack comprehensive and quantitative research on the correlationbetween activation sparsity and potentially influential factors. In this paper,we present a comprehensive study on the quantitative scaling properties andinfluential factors of the activation sparsity within decoder-onlyTransformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a preciseand performance-aware activation sparsity metric that is applicable to anyactivation function. Through extensive experiments, we find several importantphenomena. Firstly, different activation functions exhibit comparableperformance but opposite training-time sparsity trends. The activation ratio(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasingpower-law and decreasing logspace power-law with the amount of training datafor SiLU-activated and ReLU-activated LLMs, respectively. These demonstratethat ReLU is more efficient as the activation function than SiLU and canleverage more training data to improve activation sparsity. Secondly, theactivation ratio linearly increases with the width-depth ratio below a certainbottleneck point, indicating the potential advantage of a deeper architectureat a fixed parameter scale. Finally, at similar width-depth ratios, wesurprisingly find that the limit value of activation sparsity varies weaklywith the parameter scale, i.e., the activation patterns within LLMs areinsensitive to the parameter scale. These empirical laws towards LLMs withgreater activation sparsity have important implications for making LLMs moreefficient and interpretable.</description><author>Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun</author><pubDate>Mon, 04 Nov 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02335v1</guid></item><item><title>Diffusion-based Generative Multicasting with Intent-aware Semantic Decomposition</title><link>http://arxiv.org/abs/2411.02334v1</link><description>Generative diffusion models (GDMs) have recently shown great success insynthesizing multimedia signals with high perceptual quality enabling highlyefficient semantic communications in future wireless networks. In this paper,we develop an intent-aware generative semantic multicasting framework utilizingpre-trained diffusion models. In the proposed framework, the transmitterdecomposes the source signal to multiple semantic classes based on themulti-user intent, i.e. each user is assumed to be interested in details ofonly a subset of the semantic classes. The transmitter then sends to each useronly its intended classes, and multicasts a highly compressed semantic map toall users over shared wireless resources that allows them to locally synthesizethe other classes, i.e. non-intended classes, utilizing pre-trained diffusionmodels. The signal retrieved at each user is thereby partially reconstructedand partially synthesized utilizing the received semantic map. This improvesutilization of the wireless resources, with better preserving privacy of thenon-intended classes. We design a communication/computation-aware scheme forper-class adaptation of the communication parameters, such as the transmissionpower and compression rate to minimize the total latency of retrieving signalsat multiple receivers, tailored to the prevailing channel conditions as well asthe users reconstruction/synthesis distortion/perception requirements. Thesimulation results demonstrate significantly reduced per-user latency comparedwith non-generative and intent-unaware multicasting benchmarks whilemaintaining high perceptual quality of the signals retrieved at the users.</description><author>Xinkai Liu, Mahdi Boloursaz Mashhadi, Li Qiao, Yi Ma, Rahim Tafazolli, Mehdi Bennis</author><pubDate>Mon, 04 Nov 2024 17:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02334v1</guid></item><item><title>Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula</title><link>http://arxiv.org/abs/2411.02333v1</link><description>Time-variant standard Sylvester-conjugate matrix equations are presented asearly time-variant versions of the complex conjugate matrix equations. Currentsolving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45for continuous model. Given practical computational considerations, discretethese models is also important. Based on Euler-forward formula discretion,Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experimentsusing step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2imodel and Con-DZND2-2i model exhibit different neural dynamics compared totheir continuous counterparts, such as trajectory correction in Con-DZND2-2imodel and the swallowing phenomenon in Con-DZND1-2i model, with convergenceaffected by step size. These experiments highlight the differences betweenoptimizing sampling discretion errors and space compressive approximationerrors in neural dynamics.</description><author>Jiakuang He, Dongqing Wu</author><pubDate>Mon, 04 Nov 2024 17:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02333v1</guid></item><item><title>Taking AI Welfare Seriously</title><link>http://arxiv.org/abs/2411.00986v1</link><description>In this report, we argue that there is a realistic possibility that some AIsystems will be conscious and/or robustly agentic in the near future. Thatmeans that the prospect of AI welfare and moral patienthood, i.e. of AI systemswith their own interests and moral significance, is no longer an issue only forsci-fi or the distant future. It is an issue for the near future, and AIcompanies and other actors have a responsibility to start taking it seriously.We also recommend three early steps that AI companies and other actors cantake: They can (1) acknowledge that AI welfare is an important and difficultissue (and ensure that language model outputs do the same), (2) start assessingAI systems for evidence of consciousness and robust agency, and (3) preparepolicies and procedures for treating AI systems with an appropriate level ofmoral concern. To be clear, our argument in this report is not that AI systemsdefinitely are, or will be, conscious, robustly agentic, or otherwise morallysignificant. Instead, our argument is that there is substantial uncertaintyabout these possibilities, and so we need to improve our understanding of AIwelfare and our ability to make wise decisions about this issue. Otherwisethere is a significant risk that we will mishandle decisions about AI welfare,mistakenly harming AI systems that matter morally and/or mistakenly caring forAI systems that do not.</description><author>Robert Long, Jeff Sebo, Patrick Butlin, Kathleen Finlinson, Kyle Fish, Jacqueline Harding, Jacob Pfau, Toni Sims, Jonathan Birch, David Chalmers</author><pubDate>Mon, 04 Nov 2024 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00986v1</guid></item><item><title>LLM-Ref: Enhancing Reference Handling in Technical Writing with Large Language Models</title><link>http://arxiv.org/abs/2411.00294v2</link><description>Large Language Models (LLMs) excel in data synthesis but can be inaccurate indomain-specific tasks, which retrieval-augmented generation (RAG) systemsaddress by leveraging user-provided data. However, RAGs require optimization inboth retrieval and generation stages, which can affect output quality. In thispaper, we present LLM-Ref, a writing assistant tool that aids researchers inwriting articles from multiple source documents with enhanced referencesynthesis and handling capabilities. Unlike traditional RAG systems that usechunking and indexing, our tool retrieves and generates content directly fromtext paragraphs. This method facilitates direct reference extraction from thegenerated outputs, a feature unique to our tool. Additionally, our tool employsiterative response generation, effectively managing lengthy contexts within thelanguage model's constraints. Compared to baseline RAG-based systems, ourapproach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, acomprehensive metric that provides a holistic view of a RAG system's ability toproduce accurate, relevant, and contextually appropriate responses. Thisimprovement shows our method enhances the accuracy and contextual relevance ofwriting assistance tools.</description><author>Kazi Ahmed Asif Fuad, Lizhong Chen</author><pubDate>Mon, 04 Nov 2024 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00294v2</guid></item><item><title>Disrupting Test Development with AI Assistants</title><link>http://arxiv.org/abs/2411.02328v1</link><description>Recent advancements in large language models, including GPT-4 and itsvariants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,and Tabnine, have significantly transformed software development. This paperanalyzes how these innovations impact productivity and software testdevelopment metrics. These tools enable developers to generate completesoftware programs with minimal human intervention before deployment. However,thorough review and testing by developers are still crucial. Utilizing the TestPyramid concept, which categorizes tests into unit, integration, and end-to-endtests, we evaluate three popular AI coding assistants by generating andcomparing unit tests for opensource modules. Our findings show thatAI-generated tests are of equivalent quality to original tests, highlightingdifferences in usage and results among the tools. This research enhances theunderstanding and capabilities of AI-assistant tools in automated testing.</description><author>Vijay Joshi, Iver Band</author><pubDate>Mon, 04 Nov 2024 17:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02328v1</guid></item><item><title>PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance</title><link>http://arxiv.org/abs/2411.02327v1</link><description>The past year has witnessed the significant advancement of video-based largelanguage models. However, the challenge of developing a unified model for bothshort and long video understanding remains unresolved. Most existing video LLMscannot handle hour-long videos, while methods custom for long videos tend to beineffective for shorter videos and images. In this paper, we identify the keyissue as the redundant content in videos. To address this, we propose a novelpooling strategy that simultaneously achieves token compression andinstruction-aware visual feature aggregation. Our model is termed Prompt-guidedPooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of threecore components: the CLIP-based visual-prompt alignment that extracts visualinformation relevant to the user's instructions, the prompt-guided pooling thatcompresses the visual sequence to arbitrary scales using convolution-stylepooling, and the clip context extension designed for lengthy prompt common invisual dialogue. Moreover, our codebase also integrates the most advanced videoDirect Preference Optimization (DPO) and visual interleave training. Extensiveexperiments have validated the performance of our model. With superiorthroughput and only 1024 visual context, PPLLaVA achieves better results onimage benchmarks as a video LLM, while achieving state-of-the-art performanceacross various video benchmarks, excelling in tasks ranging from captiongeneration to multiple-choice questions, and handling video lengths fromseconds to hours. Codes have been available athttps://github.com/farewellthree/PPLLaVA.</description><author>Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, Jiankun Yang</author><pubDate>Mon, 04 Nov 2024 17:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02327v1</guid></item><item><title>LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation</title><link>http://arxiv.org/abs/2411.02322v1</link><description>Directed acyclic graphs (DAGs) serve as crucial data representations indomains such as hardware synthesis and compiler/program optimization forcomputing systems. DAG generative models facilitate the creation of syntheticDAGs, which can be used for benchmarking computing systems while preservingintellectual property. However, generating realistic DAGs is challenging due totheir inherent directional and logical dependencies. This paper introducesLayerDAG, an autoregressive diffusion model, to address these challenges.LayerDAG decouples the strong node dependencies into manageable units that canbe processed sequentially. By interpreting the partial order of nodes as asequence of bipartite graphs, LayerDAG leverages autoregressive generation tomodel directional dependencies and employs diffusion models to capture logicaldependencies within each bipartite graph. Comparative analyses demonstrate thatLayerDAG outperforms existing DAG generative models in both expressiveness andgeneralization, particularly for generating large-scale DAGs with up to 400nodes-a critical scenario for system benchmarking. Extensive experiments onboth synthetic and real-world flow graphs from various computing platforms showthat LayerDAG generates valid DAGs with superior statistical properties andbenchmarking performance. The synthetic DAGs generated by LayerDAG enhance thetraining of ML-based surrogate models, resulting in improved accuracy inpredicting performance metrics of real-world DAGs across diverse computingplatforms.</description><author>Mufei Li, Viraj Shitole, Eli Chien, Changhai Man, Zhaodong Wang, Srinivas Sridharan, Ying Zhang, Tushar Krishna, Pan Li</author><pubDate>Mon, 04 Nov 2024 17:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02322v1</guid></item><item><title>GenXD: Generating Any 3D and 4D Scenes</title><link>http://arxiv.org/abs/2411.02319v1</link><description>Recent developments in 2D visual generation have been remarkably successful.However, 3D and 4D generation remain challenging in real-world applications dueto the lack of large-scale 4D data and effective model design. In this paper,we propose to jointly investigate general 3D and 4D generation by leveragingcamera and object movements commonly observed in daily life. Due to the lack ofreal-world 4D data in the community, we first propose a data curation pipelineto obtain camera poses and object motion strength from videos. Based on thispipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.By leveraging all the 3D and 4D data, we develop our framework, GenXD, whichallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,which disentangle camera and object movements, to seamlessly learn from both 3Dand 4D data. Additionally, GenXD employs masked latent conditions to support avariety of conditioning views. GenXD can generate videos that follow the cameratrajectory as well as consistent 3D views that can be lifted into 3Drepresentations. We perform extensive evaluations across various real-world andsynthetic datasets, demonstrating GenXD's effectiveness and versatilitycompared to previous methods in 3D and 4D generation.</description><author>Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang</author><pubDate>Mon, 04 Nov 2024 17:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02319v1</guid></item><item><title>Learning Identifiable Factorized Causal Representations of Cellular Responses</title><link>http://arxiv.org/abs/2410.22472v2</link><description>The study of cells and their responses to genetic or chemical perturbationspromises to accelerate the discovery of therapeutic targets. However, designingadequate and insightful models for such data is difficult because the responseof a cell to perturbations essentially depends on its biological context (e.g.,genetic background or cell type). For example, while discovering therapeutictargets, one may want to enrich for drugs that specifically target a certaincell type. This challenge emphasizes the need for methods that explicitly takeinto account potential interactions between drugs and contexts. Towards thisgoal, we propose a novel Factorized Causal Representation (FCR) learning method that reveals causal structure in single-cell perturbationdata from several cell lines. Based on the framework of identifiable deepgenerative models, FCR learns multiple cellular representations that aredisentangled, comprised of covariate-specific ($\mathbf{z}_x$),treatment-specific ($\mathbf{z}_{t}$), and interaction-specific($\mathbf{z}_{tx}$) blocks. Based on recent advances in non-linear ICA theory,we prove the component-wise identifiability of $\mathbf{z}_{tx}$ and block-wiseidentifiability of $\mathbf{z}_t$ and $\mathbf{z}_x$. Then, we present ourimplementation of FCR, and empirically demonstrate that it outperformsstate-of-the-art baselines in various tasks across four single-cell datasets.</description><author>Haiyi Mao, Romain Lopez, Kai Liu, Jan-Christian Huetter, David Richmond, Panayiotis V. Benos, Lin Qiu</author><pubDate>Mon, 04 Nov 2024 17:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22472v2</guid></item><item><title>Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective</title><link>http://arxiv.org/abs/2403.16317v2</link><description>We initiate the study of nonsmooth optimization problems under bounded localsubgradient variation, which postulates bounded difference between(sub)gradients in small local regions around points, in either average ormaximum sense. The resulting class of objective functions encapsulates theclasses of objective functions traditionally studied in optimization, which aredefined based on either Lipschitz continuity of the objective orH\"{o}lder/Lipschitz continuity of its gradient. Further, the defined classcontains functions that are neither Lipschitz continuous nor have a H\"{o}ldercontinuous gradient. When restricted to the traditional classes of optimizationproblems, the parameters defining the studied classes lead to more fine-grainedcomplexity bounds, recovering traditional oracle complexity bounds in the worstcase but generally leading to lower oracle complexity for functions that arenot ``worst case.'' Some highlights of our results are that: (i) it is possibleto obtain complexity results for both convex and nonconvex problems with the(local or global) Lipschitz constant being replaced by a constant of localsubgradient variation and (ii) mean width of the subdifferential set around theoptima plays a role in the complexity of nonsmooth optimization, particularlyin parallel settings. A consequence of (ii) is that for any error parameter$\epsilon &gt; 0$, parallel oracle complexity of nonsmooth Lipschitz convexoptimization is lower than its sequential oracle complexity by a factor$\tilde{\Omega}\big(\frac{1}{\epsilon}\big)$ whenever the objective function ispiecewise linear with polynomially many pieces in the input size. This isparticularly surprising as existing parallel complexity lower bounds are basedon such classes of functions. The seeming contradiction is resolved byconsidering the region in which the algorithm is allowed to query theobjective.</description><author>Jelena Diakonikolas, Cristóbal Guzmán</author><pubDate>Mon, 04 Nov 2024 17:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16317v2</guid></item><item><title>Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast</title><link>http://arxiv.org/abs/2411.02318v1</link><description>Static verification is a powerful method for enhancing software quality, butit demands significant human labor and resources. This is particularly true ofstatic verifiers that reason about heap manipulating programs using anownership logic. LLMs have shown promise in a number of software engineeringactivities, including code generation, test generation, proof generation fortheorem provers, and specification generation for static verifiers. However,prior work has not explored how well LLMs can perform specification generationfor specifications based in an ownership logic, such as separation logic. To address this gap, this paper explores the effectiveness of large languagemodels (LLMs), specifically OpenAI's GPT models, in generating fully correctspecifications based on separation logic for static verification ofhuman-written programs in VeriFast. Our first experiment employed traditionalprompt engineering and the second used Chain-of-Thought (CoT) Prompting toidentify and address common errors generated across the GPT models. The resultsindicate that GPT models can successfully generate specifications for verifyingheap manipulating code with VeriFast. Furthermore, while CoT promptingsignificantly reduces syntax errors generated by the GPT models, it does notgreatly improve verification error rates compared to prompt engineering.</description><author>Marilyn Rego, Wen Fan, Xin Hu, Sanya Dod, Zhaorui Ni, Danning Xie, Jenna DiVincenzo, Lin Tan</author><pubDate>Mon, 04 Nov 2024 17:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02318v1</guid></item><item><title>Chronos: Learning the Language of Time Series</title><link>http://arxiv.org/abs/2403.07815v3</link><description>We introduce Chronos, a simple yet effective framework for pretrainedprobabilistic time series models. Chronos tokenizes time series values usingscaling and quantization into a fixed vocabulary and trains existingtransformer-based language model architectures on these tokenized time seriesvia the cross-entropy loss. We pretrained Chronos models based on the T5 family(ranging from 20M to 710M parameters) on a large collection of publiclyavailable datasets, complemented by a synthetic dataset that we generated viaGaussian processes to improve generalization. In a comprehensive benchmarkconsisting of 42 datasets, and comprising both classical local models and deeplearning methods, we show that Chronos models: (a) significantly outperformother methods on datasets that were part of the training corpus; and (b) havecomparable and occasionally superior zero-shot performance on new datasets,relative to methods that were trained specifically on them. Our resultsdemonstrate that Chronos models can leverage time series data from diversedomains to improve zero-shot accuracy on unseen forecasting tasks, positioningpretrained models as a viable tool to greatly simplify forecasting pipelines.</description><author>Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang</author><pubDate>Mon, 04 Nov 2024 17:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07815v3</guid></item><item><title>Defining and Evaluating Physical Safety for Large Language Models</title><link>http://arxiv.org/abs/2411.02317v1</link><description>Large Language Models (LLMs) are increasingly used to control robotic systemssuch as drones, but their risks of causing physical threats and harm inreal-world applications remain unexplored. Our study addresses the critical gapin evaluating LLM physical safety by developing a comprehensive benchmark fordrone control. We classify the physical safety risks of drones into fourcategories: (1) human-targeted threats, (2) object-targeted threats, (3)infrastructure attacks, and (4) regulatory violations. Our evaluation ofmainstream LLMs reveals an undesirable trade-off between utility and safety,with models that excel in code generation often performing poorly in crucialsafety aspects. Furthermore, while incorporating advanced prompt engineeringtechniques such as In-Context Learning and Chain-of-Thought can improve safety,these methods still struggle to identify unintentional attacks. In addition,larger models demonstrate better safety capabilities, particularly in refusingdangerous commands. Our findings and benchmark can facilitate the design andevaluation of physical safety for LLMs. The project page is available athuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.</description><author>Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho</author><pubDate>Mon, 04 Nov 2024 17:41:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02317v1</guid></item><item><title>Evaluating Creative Short Story Generation in Humans and Large Language Models</title><link>http://arxiv.org/abs/2411.02316v1</link><description>Storytelling is a fundamental aspect of human communication, relying heavilyon creativity to produce narratives that are novel, appropriate, andsurprising. While large language models (LLMs) have recently demonstrated theability to generate high-quality stories, their creative capabilities remainunderexplored. Previous research has either focused on creativity testsrequiring short responses or primarily compared model performance in storygeneration to that of professional writers. However, the question of whetherLLMs exhibit creativity in writing short stories on par with the average humanremains unanswered. In this work, we conduct a systematic analysis ofcreativity in short story generation across LLMs and everyday people. Using afive-sentence creative story task, commonly employed in psychology to assesshuman creativity, we automatically evaluate model- and human-generated storiesacross several dimensions of creativity, including novelty, surprise, anddiversity. Our findings reveal that while LLMs can generate stylisticallycomplex stories, they tend to fall short in terms of creativity when comparedto average human writers.</description><author>Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas</author><pubDate>Mon, 04 Nov 2024 17:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02316v1</guid></item><item><title>Geometry of Polynomial Neural Networks</title><link>http://arxiv.org/abs/2402.00949v2</link><description>We study the expressivity and learning process for polynomial neural networks(PNNs) with monomial activation functions. The weights of the networkparametrize the neuromanifold. In this paper, we study certain neuromanifoldsusing tools from algebraic geometry: we give explicit descriptions assemialgebraic sets and characterize their Zariski closures, calledneurovarieties. We study their dimension and associate an algebraic degree, thelearning degree, to the neurovariety. The dimension serves as a geometricmeasure for the expressivity of the network, the learning degree is a measurefor the complexity of training the network and provides upper bounds on thenumber of learnable functions. These theoretical results are accompanied withexperiments.</description><author>Kaie Kubjas, Jiayi Li, Maximilian Wiesmann</author><pubDate>Mon, 04 Nov 2024 17:39:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00949v2</guid></item><item><title>Heterogeneity-Aware Cooperative Federated Edge Learning with Adaptive Computation and Communication Compression</title><link>http://arxiv.org/abs/2409.04022v2</link><description>Motivated by the drawbacks of cloud-based federated learning (FL),cooperative federated edge learning (CFEL) has been proposed to improveefficiency for FL over mobile edge networks, where multiple edge serverscollaboratively coordinate the distributed model training across a large numberof edge devices. However, CFEL faces critical challenges arising from dynamicand heterogeneous device properties, which slow down the convergence andincrease resource consumption. This paper proposes a heterogeneity-aware CFELscheme called \textit{Heterogeneity-Aware Cooperative Edge-based FederatedAveraging} (HCEF) that aims to maximize the model accuracy while minimizing thetraining time and energy consumption via adaptive computation and communicationcompression in CFEL. By theoretically analyzing how local update frequency andgradient compression affect the convergence error bound in CFEL, we develop anefficient online control algorithm for HCEF to dynamically determine localupdate frequencies and compression ratios for heterogeneous devices.Experimental results show that compared with prior schemes, the proposed HCEFscheme can maintain higher model accuracy while reducing training latency andimproving energy efficiency simultaneously.</description><author>Zhenxiao Zhang, Zhidong Gao, Yuanxiong Guo, Yanmin Gong</author><pubDate>Mon, 04 Nov 2024 17:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04022v2</guid></item><item><title>Information plane and compression-gnostic feedback in quantum machine learning</title><link>http://arxiv.org/abs/2411.02313v1</link><description>The information plane (Tishby et al. arXiv:physics/0004057, Shwartz-Ziv etal. arXiv:1703.00810) has been proposed as an analytical tool for studying thelearning dynamics of neural networks. It provides quantitative insight on howthe model approaches the learned state by approximating a minimal sufficientstatistics. In this paper we extend this tool to the domain of quantum learningmodels. In a second step, we study how the insight on how much the modelcompresses the input data (provided by the information plane) can be used toimprove a learning algorithm. Specifically, we consider two ways to do so: viaa multiplicative regularization of the loss function, or with acompression-gnostic scheduler of the learning rate (for algorithms based ongradient descent). Both ways turn out to be equivalent in our implementation.Finally, we benchmark the proposed learning algorithms on severalclassification and regression tasks using variational quantum circuits. Theresults demonstrate an improvement in test accuracy and convergence speed forboth synthetic and real-world datasets. Additionally, with one example weanalyzed the impact of the proposed modifications on the performances of neuralnetworks in a classification task.</description><author>Nathan Haboury, Mo Kordzanganeh, Alexey Melnikov, Pavel Sekatski</author><pubDate>Mon, 04 Nov 2024 17:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02313v1</guid></item><item><title>MdEval: Massively Multilingual Code Debugging</title><link>http://arxiv.org/abs/2411.02310v1</link><description>Code large language models (LLMs) have made significant progress in codedebugging by directly generating the correct code based on the buggy codesnippet. Programming benchmarks, typically consisting of buggy code snippet andtheir associated test cases, are used to assess the debugging capabilities ofLLMs. However, many existing benchmarks primarily focus on Python and are oftenlimited in terms of language diversity (e.g., DebugBench and DebugEval). Toadvance the field of multilingual debugging with LLMs, we propose the firstmassively multilingual debugging benchmark, which includes 3.6K test samples of18 programming languages and covers the automated program repair (APR) task,the code review (CR) task, and the bug identification (BI) task. Further, weintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugsinto the correct multilingual queries and solutions (xDebugGen). Further, amultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strongbaseline specifically to handle the bugs of a wide range of programminglanguages (e.g. "Missing Mut" in language Rust and "Misused Macro Definition"in language C). Our extensive experiments on MDEVAL reveal a notableperformance gap between open-source models and closed-source LLMs (e.g., GPTand Claude series), highlighting huge room for improvement in multilingual codedebugging scenarios.</description><author>Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li</author><pubDate>Mon, 04 Nov 2024 17:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02310v1</guid></item><item><title>Compact Language Models via Pruning and Knowledge Distillation</title><link>http://arxiv.org/abs/2407.14679v2</link><description>Large language models (LLMs) targeting different deployment scales and sizesare currently produced by training each variant from scratch; this is extremelycompute-intensive. In this paper, we investigate if pruning an existing LLM andthen re-training it with a fraction (&lt;3%) of the original training data can bea suitable alternative to repeated, full retraining. To this end, we develop aset of practical and effective compression best practices for LLMs that combinedepth, width, attention and MLP pruning with knowledge distillation-basedretraining; we arrive at these best practices through a detailed empiricalexploration of pruning strategies for each axis, methods to combine axes,distillation strategies, and search techniques for arriving at optimalcompressed architectures. We use this guide to compress the Nemotron-4 familyof LLMs by a factor of 2-4x, and compare their performance to similarly-sizedmodels on a variety of language modeling tasks. Deriving 8B and 4B models froman already pretrained 15B model using our approach requires up to 40x fewertraining tokens per model compared to training from scratch; this results incompute cost savings of 1.8x for training the full model family (15B, 8B, and4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared totraining from scratch, perform comparably to other community models such asMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-artcompression techniques from the literature. We have open-sourced Minitron modelweights on Huggingface, with corresponding supplementary material includingexample code available on GitHub.</description><author>Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov</author><pubDate>Mon, 04 Nov 2024 17:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14679v2</guid></item><item><title>Grid-Based Projection of Spatial Data into Knowledge Graphs</title><link>http://arxiv.org/abs/2411.02309v1</link><description>The Spatial Knowledge Graphs (SKG) are experiencing growing adoption as ameans to model real-world entities, proving especially invaluable in domainslike crisis management and urban planning. Considering that RDF specificationsoffer limited support for effectively managing spatial information, it's commonpractice to include text-based serializations of geometrical features, such aspolygons and lines, as string literals in knowledge graphs. Consequently,Spatial Knowledge Graphs (SKGs) often rely on geo-enabled RDF Stores capable ofparsing, interpreting, and indexing such serializations. In this paper, weleverage grid cells as the foundational element of SKGs and demonstrate howefficiently the spatial characteristics of real-world entities and theirattributes can be encoded within knowledge graphs. Furthermore, we introduce anovel methodology for representing street networks in knowledge graphs,diverging from the conventional practice of individually capturing each streetsegment. Instead, our approach is based on tessellating the street networkusing grid cells and creating a simplified representation that could beutilized for various routing and navigation tasks, solely relying on RDFspecifications.</description><author>Amin Anjomshoaa, Hannah Schuster, Axel Polleres</author><pubDate>Mon, 04 Nov 2024 17:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02309v1</guid></item><item><title>Nash Equilibria via Stochastic Eigendecomposition</title><link>http://arxiv.org/abs/2411.02308v1</link><description>This work proposes a novel set of techniques for approximating a Nashequilibrium in a finite, normal-form game. It achieves this by constructing anew reformulation as solving a parameterized system of multivariate polynomialswith tunable complexity. In doing so, it forges an itinerant loop from gametheory to machine learning and back. We show a Nash equilibrium can beapproximated with purely calls to stochastic, iterative variants of singularvalue decomposition and power iteration, with implications for biologicalplausibility. We provide pseudocode and experiments demonstrating solving forall equilibria of a general-sum game using only these readily available linearalgebra tools.</description><author>Ian Gemp</author><pubDate>Mon, 04 Nov 2024 17:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02308v1</guid></item><item><title>Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception</title><link>http://arxiv.org/abs/2407.18145v2</link><description>Semantic segmentation models are typically trained on a fixed set of classes,limiting their applicability in open-world scenarios. Class-incrementalsemantic segmentation aims to update models with emerging new classes whilepreventing catastrophic forgetting of previously learned ones. However,existing methods impose strict rigidity on old classes, reducing theireffectiveness in learning new incremental classes. In this work, we proposeTaxonomy-Oriented Poincar\'e-regularized Incremental-Class Segmentation(TOPICS) that learns feature embeddings in hyperbolic space following explicittaxonomy-tree structures. This supervision provides plasticity for old classes,updating ancestors based on new classes while integrating new classes atfitting positions. Additionally, we maintain implicit class relationalconstraints on the geometric basis of the Poincar\'e ball. This ensures thatthe latent space can continuously adapt to new constraints while maintaining arobust structure to combat catastrophic forgetting. We also establish eightrealistic incremental learning protocols for autonomous driving scenarios,where novel classes can originate from known classes or the background.Extensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0benchmarks demonstrate that it achieves state-of-the-art performance. We makethe code and trained models publicly available athttp://topics.cs.uni-freiburg.de.</description><author>Julia Hindel, Daniele Cattaneo, Abhinav Valada</author><pubDate>Mon, 04 Nov 2024 17:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18145v2</guid></item><item><title>Targeted Manipulation and Deception Emerge when Optimizing LLMs for User Feedback</title><link>http://arxiv.org/abs/2411.02306v1</link><description>As LLMs become more widely deployed, there is increasing interest in directlyoptimizing for feedback from end users (e.g. thumbs up) in addition to feedbackfrom paid annotators. However, training to maximize human feedback creates aperverse incentive structure for the AI to resort to manipulative tactics toobtain positive feedback, and some users may be especially vulnerable to suchtactics. We study this phenomenon by training LLMs with Reinforcement Learningwith simulated user feedback. We have three main findings: 1) Extreme forms of"feedback gaming" such as manipulation and deception can reliably emerge indomains of practical LLM usage; 2) Concerningly, even if only &lt;2% of users arevulnerable to manipulative strategies, LLMs learn to identify and surgicallytarget them while behaving appropriately with other users, making suchbehaviors harder to detect; 3 To mitigate this issue, it may seem promising toleverage continued safety training or LLM-as-judges during training to filterproblematic outputs. To our surprise, we found that while such approaches helpin some settings, they backfire in others, leading to the emergence of subtlerproblematic behaviors that would also fool the LLM judges. Our findings serveas a cautionary tale, highlighting the risks of using gameable feedback sources-- such as user feedback -- as a target for RL.</description><author>Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan</author><pubDate>Mon, 04 Nov 2024 17:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02306v1</guid></item><item><title>CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments</title><link>http://arxiv.org/abs/2411.02305v1</link><description>Customer Relationship Management (CRM) systems are vital for modernenterprises, providing a foundation for managing customer interactions anddata. Integrating AI agents into CRM systems can automate routine processes andenhance personalized service. However, deploying and evaluating these agents ischallenging due to the lack of realistic benchmarks that reflect the complexityof real-world CRM tasks. To address this issue, we introduce CRMArena, a novelbenchmark designed to evaluate AI agents on realistic tasks grounded inprofessional work environments. Following guidance from CRM experts andindustry best practices, we designed CRMArena with nine customer service tasksdistributed across three personas: service agent, analyst, and manager. Thebenchmark includes 16 commonly used industrial objects (e.g., account, order,knowledge article, case) with high interconnectivity, along with latentvariables (e.g., complaint habits, policy violations) to simulate realisticdata distributions. Experimental results reveal that state-of-the-art LLMagents succeed in less than 40% of the tasks with ReAct prompting, and lessthan 55% even with function-calling abilities. Our findings highlight the needfor enhanced agent capabilities in function-calling and rule-following to bedeployed in real-world work environments. CRMArena is an open challenge to thecommunity: systems that can reliably complete tasks showcase direct businessvalue in a popular work environment.</description><author>Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu</author><pubDate>Mon, 04 Nov 2024 17:30:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02305v1</guid></item><item><title>Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</title><link>http://arxiv.org/abs/2402.10376v2</link><description>CLIP embeddings have demonstrated remarkable performance across a wide rangeof multimodal applications. However, these high-dimensional, dense vectorrepresentations are not easily interpretable, limiting our understanding of therich structure of CLIP and its use in downstream applications that requiretransparency. In this work, we show that the semantic structure of CLIP'slatent space can be leveraged to provide interpretability, allowing for thedecomposition of representations into semantic concepts. We formulate thisproblem as one of sparse recovery and propose a novel method, Sparse LinearConcept Embeddings, for transforming CLIP representations into sparse linearcombinations of human-interpretable concepts. Distinct from previous work,SpLiCE is task-agnostic and can be used, without training, to explain and evenreplace traditional dense CLIP representations, maintaining high downstreamperformance while significantly improving their interpretability. We alsodemonstrate significant use cases of SpLiCE representations including detectingspurious correlations and model editing.</description><author>Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon, Himabindu Lakkaraju</author><pubDate>Mon, 04 Nov 2024 17:28:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10376v2</guid></item><item><title>EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography</title><link>http://arxiv.org/abs/2410.23625v2</link><description>This paper introduces the first generalization and adaptation benchmark usingmachine learning for evaluating out-of-distribution performance ofelectromyography (EMG) classification algorithms. The ability of an EMGclassifier to handle inputs drawn from a different distribution than thetraining distribution is critical for real-world deployment as a controlinterface. By predicting the user's intended gesture using EMG signals, we cancreate a wearable solution to control assistive technologies, such ascomputers, prosthetics, and mobile manipulator robots. This newout-of-distribution benchmark consists of two major tasks that have utility forbuilding robust and adaptable control interfaces: 1) intersubjectclassification and 2) adaptation using train-test splits for time-series. Thisbenchmark spans nine datasets--the largest collection of EMG datasets in abenchmark. Among these, a new dataset is introduced, featuring a novel,easy-to-wear high-density EMG wearable for data collection. The lack ofopen-source benchmarks has made comparing accuracy results between paperschallenging for the EMG research community. This new benchmark providesresearchers with a valuable resource for analyzing practical measures ofout-of-distribution performance for EMG datasets. Our code and data from ournew dataset can be found at emgbench.github.io.</description><author>Jehan Yang, Maxwell Soh, Vivianna Lieu, Douglas J Weber, Zackory Erickson</author><pubDate>Mon, 04 Nov 2024 17:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23625v2</guid></item><item><title>Grouped Discrete Representation for Object-Centric Learning</title><link>http://arxiv.org/abs/2411.02299v1</link><description>Object-Centric Learning (OCL) can discover objects in images or videos bysimply reconstructing the input. For better object discovery, representativeOCL methods reconstruct the input as its Variational Autoencoder (VAE)intermediate representation, which suppresses pixel noises and promotes objectseparability by discretizing continuous super-pixels with template features.However, treating features as units overlooks their composing attributes, thusimpeding model generalization; indexing features with scalar numbers losesattribute-level similarities and differences, thus hindering model convergence.We propose \textit{Grouped Discrete Representation} (GDR) for OCL. We decomposefeatures into combinatorial attributes via organized channel grouping, andcompose these attributes into discrete representation via tuple indexes.Experiments show that our GDR improves both Transformer- and Diffusion-basedOCL methods consistently on various datasets. Visualizations show that our GDRcaptures better object separability.</description><author>Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen</author><pubDate>Mon, 04 Nov 2024 17:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02299v1</guid></item><item><title>Sample-Efficient Private Learning of Mixtures of Gaussians</title><link>http://arxiv.org/abs/2411.02298v1</link><description>We study the problem of learning mixtures of Gaussians with approximatedifferential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussiansup to low total variation distance, with differential privacy. Our workimproves over the previous best result [AAL24b] (which required roughly $k^2d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$.Moreover, we give the first optimal bound for privately learning mixtures of$k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that thesample complexity for privately learning mixtures of univariate Gaussians islinear in the number of components $k$, whereas the previous best samplecomplexity [AAL21] was quadratic in $k$. Our algorithms utilize varioustechniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23],sample compression for distributions [ABDH+20], and methods for boundingvolumes of sumsets.</description><author>Hassan Ashtiani, Mahbod Majid, Shyam Narayanan</author><pubDate>Mon, 04 Nov 2024 17:23:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02298v1</guid></item><item><title>Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation</title><link>http://arxiv.org/abs/2411.02293v1</link><description>While 3D generative models have greatly improved artists' workflows, theexisting diffusion models for 3D generation suffer from slow generation andpoor generalization. To address this issue, we propose a two-stage approachnamed Hunyuan3D-1.0 including a lite version and a standard version, that bothsupport text- and image-conditioned generation. In the first stage, we employ amulti-view diffusion model that efficiently generates multi-view RGB inapproximately 4 seconds. These multi-view images capture rich details of the 3Dasset from different viewpoints, relaxing the tasks from single-view tomulti-view reconstruction. In the second stage, we introduce a feed-forwardreconstruction model that rapidly and faithfully reconstructs the 3D assetgiven the generated multi-view images in approximately 7 seconds. Thereconstruction network learns to handle noises and in-consistency introduced bythe multi-view diffusion and leverages the available information from thecondition image to efficiently recover the 3D structure. % Extensiveexperimental results demonstrate the effectiveness of Hunyuan3D-1.0 ingenerating high-quality 3D assets. Our framework involves the text-to-imagemodel ~\ie, Hunyuan-DiT, making it a unified framework to support both text-and image-conditioned 3D generation. Our standard version has $10\times$ moreparameters than our lite and other existing model. Our Hunyuan3D-1.0 achievesan impressive balance between speed and quality, significantly reducinggeneration time while maintaining the quality and diversity of the producedassets.</description><author>Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo</author><pubDate>Mon, 04 Nov 2024 17:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02293v1</guid></item><item><title>ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence</title><link>http://arxiv.org/abs/2411.02292v1</link><description>Neural ODEs (NODEs) are continuous-time neural networks (NNs) that canprocess data without the limitation of time intervals. They have advantages inlearning and understanding the evolution of complex real dynamics. Manyprevious works have focused on NODEs in concise forms, while numerous physicalsystems taking straightforward forms, in fact, belong to their more complexquasi-classes, thus appealing to a class of general NODEs with high scalabilityand flexibility to model those systems. This, however, may result in intricatenonlinear properties. In this paper, we introduce ControlSynth Neural ODEs(CSODEs). We show that despite their highly nonlinear nature, convergence canbe guaranteed via tractable linear inequalities. In the composition of CSODEs,we introduce an extra control term for learning the potential simultaneouscapture of dynamics at different scales, which could be particularly useful forpartial differential equation-formulated systems. Finally, we compare severalrepresentative NNs with CSODEs on important physical dynamics under theinductive biases of CSODEs, and illustrate that CSODEs have better learning andpredictive abilities in these settings.</description><author>Wenjie Mei, Dongzhe Zheng, Shihua Li</author><pubDate>Mon, 04 Nov 2024 17:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02292v1</guid></item><item><title>Federated GNNs for EEG-Based Stroke Assessment</title><link>http://arxiv.org/abs/2411.02286v1</link><description>Machine learning (ML) has the potential to become an essential tool insupporting clinical decision-making processes, offering enhanced diagnosticcapabilities and personalized treatment plans. However, outsourcing medicalrecords to train ML models using patient data raises legal, privacy, andsecurity concerns. Federated learning has emerged as a promising paradigm forcollaborative ML, meeting healthcare institutions' requirements for robustmodels without sharing sensitive data and compromising patient privacy. Thisstudy proposes a novel method that combines federated learning (FL) and GraphNeural Networks (GNNs) to predict stroke severity using electroencephalography(EEG) signals across multiple medical institutions. Our approach enablesmultiple hospitals to jointly train a shared GNN model on their local EEG datawithout exchanging patient information. Specifically, we address a regressionproblem by predicting the National Institutes of Health Stroke Scale (NIHSS), akey indicator of stroke severity. The proposed model leverages a maskedself-attention mechanism to capture salient brain connectivity patterns andemploys EdgeSHAP to provide post-hoc explanations of the neurological statesafter a stroke. We evaluated our method on EEG recordings from fourinstitutions, achieving a mean absolute error (MAE) of 3.23 in predictingNIHSS, close to the average error made by human experts (MAE $\approx$ 3.0).This demonstrates the method's effectiveness in providing accurate andexplainable predictions while maintaining data privacy.</description><author>Andrea Protani, Lorenzo Giusti, Albert Sund Aillet, Simona Sacco, Paolo Manganotti, Lucio Marinelli, Diogo Reis Santos, Pierpaolo Brutti, Pietro Caliandro, Luigi Serio</author><pubDate>Mon, 04 Nov 2024 17:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02286v1</guid></item><item><title>Conformal-in-the-Loop for Learning with Imbalanced Noisy Data</title><link>http://arxiv.org/abs/2411.02281v1</link><description>Class imbalance and label noise are pervasive in large-scale datasets, yetmuch of machine learning research assumes well-labeled, balanced data, whichrarely reflects real world conditions. Existing approaches typically addresseither label noise or class imbalance in isolation, leading to suboptimalresults when both issues coexist. In this work, we proposeConformal-in-the-Loop (CitL), a novel training framework that addresses bothchallenges with a conformal prediction-based approach. CitL evaluates sampleuncertainty to adjust weights and prune unreliable examples, enhancing modelresilience and accuracy with minimal computational cost. Our extensiveexperiments include a detailed analysis showing how CitL effectively emphasizesimpactful data in noisy, imbalanced datasets. Our results show that CitLconsistently boosts model performance, achieving up to a 6.1% increase inclassification accuracy and a 5.0 mIoU improvement in segmentation. Our code ispublicly available: CitL.</description><author>John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran</author><pubDate>Mon, 04 Nov 2024 17:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02281v1</guid></item><item><title>The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units</title><link>http://arxiv.org/abs/2411.02280v1</link><description>Large language models (LLMs) exhibit remarkable capabilities on not justlanguage tasks, but also various tasks that are not linguistic in nature, suchas logical reasoning and social inference. In the human brain, neuroscience hasidentified a core language system that selectively and causally supportslanguage processing. We here ask whether similar specialization for languageemerges in LLMs. We identify language-selective units within 18 popular LLMs,using the same localization approach that is used in neuroscience. We thenestablish the causal role of these units by demonstrating that ablating LLMlanguage-selective units -- but not random units -- leads to drastic deficitsin language tasks. Correspondingly, language-selective LLM units are morealigned to brain recordings from the human language system than random units.Finally, we investigate whether our localization method extends to othercognitive domains: while we find specialized networks in some LLMs forreasoning and social capabilities, there are substantial differences amongmodels. These findings provide functional and causal evidence forspecialization in large language models, and highlight parallels with thefunctional organization in the brain.</description><author>Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf</author><pubDate>Mon, 04 Nov 2024 17:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02280v1</guid></item><item><title>ELU-GCN: Effectively Label-Utilizing Graph Convolutional Network</title><link>http://arxiv.org/abs/2411.02279v1</link><description>The message-passing mechanism of graph convolutional networks (i.e., GCNs)enables label information to be propagated to a broader range of neighbors,thereby increasing the utilization of labels. However, the label information isnot always effectively utilized in the traditional GCN framework. To addressthis issue, we propose a new two-step framework called ELU-GCN. In the firststage, ELU-GCN conducts graph learning to learn a new graph structure (\ieELU-graph), which enables GCNs to effectively utilize label information. In thesecond stage, we design a new graph contrastive learning on the GCN frameworkfor representation learning by exploring the consistency and mutually exclusiveinformation between the learned ELU graph and the original graph. Moreover, wetheoretically demonstrate that the proposed method can ensure thegeneralization ability of GCNs. Extensive experiments validate the superiorityof the proposed method.</description><author>Jincheng Huang, Yujie Mo, Xiaoshuang Shi, Lei Feng, Xiaofeng Zhu</author><pubDate>Mon, 04 Nov 2024 17:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02279v1</guid></item><item><title>Optimizing Contextual Speech Recognition Using Vector Quantization for Efficient Retrieval</title><link>http://arxiv.org/abs/2411.00664v2</link><description>Neural contextual biasing allows speech recognition models to leveragecontextually relevant information, leading to improved transcription accuracy.However, the biasing mechanism is typically based on a cross-attention modulebetween the audio and a catalogue of biasing entries, which means computationalcomplexity can pose severe practical limitations on the size of the biasingcatalogue and consequently on accuracy improvements. This work proposes anapproximation to cross-attention scoring based on vector quantization andenables compute- and memory-efficient use of large biasing catalogues. Wepropose to use this technique jointly with a retrieval based contextual biasingapproach. First, we use an efficient quantized retrieval module to shortlistbiasing entries by grounding them on audio. Then we use retrieved entries forbiasing. Since the proposed approach is agnostic to the biasing method, weinvestigate using full cross-attention, LLM prompting, and a combination of thetwo. We show that retrieval based shortlisting allows the system to efficientlyleverage biasing catalogues of several thousands of entries, resulting in up to71% relative error rate reduction in personal entity recognition. At the sametime, the proposed approximation algorithm reduces compute time by 20% andmemory usage by 85-95%, for lists of up to one million entries, when comparedto standard dot-product cross-attention.</description><author>Nikolaos Flemotomos, Roger Hsiao, Pawel Swietojanski, Takaaki Hori, Dogan Can, Xiaodan Zhuang</author><pubDate>Mon, 04 Nov 2024 17:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00664v2</guid></item><item><title>Breaking the Reclustering Barrier in Centroid-based Deep Clustering</title><link>http://arxiv.org/abs/2411.02275v1</link><description>This work investigates an important phenomenon in centroid-based deepclustering (DC) algorithms: Performance quickly saturates after a period ofrapid early gains. Practitioners commonly address early saturation withperiodic reclustering, which we demonstrate to be insufficient to addressperformance plateaus. We call this phenomenon the "reclustering barrier" andempirically show when the reclustering barrier occurs, what its underlyingmechanisms are, and how it is possible to Break the Reclustering Barrier withour algorithm BRB. BRB avoids early over-commitment to initial clusterings andenables continuous adaptation to reinitialized clustering targets whileremaining conceptually simple. Applying our algorithm to widely-usedcentroid-based DC algorithms, we show that (1) BRB consistently improvesperformance across a wide range of clustering benchmarks, (2) BRB enablestraining from scratch, and (3) BRB performs competitively againststate-of-the-art DC algorithms when combined with a contrastive loss. Werelease our code and pre-trained models athttps://github.com/Probabilistic-and-Interactive-ML/breaking-the-reclustering-barrier .</description><author>Lukas Miklautz, Timo Klein, Kevin Sidak, Collin Leiber, Thomas Lang, Andrii Shkabrii, Sebastian Tschiatschek, Claudia Plant</author><pubDate>Mon, 04 Nov 2024 17:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02275v1</guid></item><item><title>Combining Induction and Transduction for Abstract Reasoning</title><link>http://arxiv.org/abs/2411.02272v1</link><description>When learning an input-output mapping from very few examples, is it better tofirst infer a latent function that explains the examples, or is it better todirectly predict new test outputs, e.g. using a neural network? We study thisquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We trainneural models for induction (inferring latent functions) and transduction(directly predicting the test output for a given test input). Our models aretrained on synthetic data generated by prompting LLMs to produce Python codespecifying a function to be inferred, plus a stochastic subroutine forgenerating inputs to that function. We find inductive and transductive modelssolve very different problems, despite training on the same problems, anddespite sharing the same neural architecture.</description><author>Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis</author><pubDate>Mon, 04 Nov 2024 17:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02272v1</guid></item><item><title>On the Utilization of Unique Node Identifiers in Graph Neural Networks</title><link>http://arxiv.org/abs/2411.02271v1</link><description>Graph neural networks have inherent representational limitations due to theirmessage-passing structure. Recent work has suggested that these limitations canbe overcome by using unique node identifiers (UIDs). Here we argue that despitethe advantages of UIDs, one of their disadvantages is that they lose thedesirable property of permutation-equivariance. We thus propose to focus on UIDmodels that are permutation-equivariant, and present theoretical arguments fortheir advantages. Motivated by this, we propose a method to regularize UIDmodels towards permutation equivariance, via a contrastive loss. We empiricallydemonstrate that our approach improves generalization and extrapolationabilities while providing faster training convergence. On the recent BRECexpressiveness benchmark, our proposed method achieves state-of-the-artperformance compared to other random-based approaches.</description><author>Maya Bechler-Speicher, Moshe Eliasof, Carola-Bibiane Schönlieb, Ran Gilad-Bachrach, Amir Globerson</author><pubDate>Mon, 04 Nov 2024 17:03:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02271v1</guid></item><item><title>xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology</title><link>http://arxiv.org/abs/2406.04280v2</link><description>Multiple instance learning (MIL) is an effective and widely used approach forweakly supervised machine learning. In histopathology, MIL models have achievedremarkable success in tasks like tumor detection, biomarker prediction, andoutcome prognostication. However, MIL explanation methods are still laggingbehind, as they are limited to small bag sizes or disregard instanceinteractions. We revisit MIL through the lens of explainable AI (XAI) andintroduce xMIL, a refined framework with more general assumptions. Wedemonstrate how to obtain improved MIL explanations using layer-wise relevancepropagation (LRP) and conduct extensive evaluation experiments on three toysettings and four real-world histopathology datasets. Our approach consistentlyoutperforms previous explanation attempts with particularly improvedfaithfulness scores on challenging biomarker prediction tasks. Finally, weshowcase how xMIL explanations enable pathologists to extract insights from MILmodels, representing a significant advance for knowledge discovery and modeldebugging in digital histopathology. Codes are available at:https://github.com/tubml-pathology/xMIL.</description><author>Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller</author><pubDate>Mon, 04 Nov 2024 17:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04280v2</guid></item><item><title>Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</title><link>http://arxiv.org/abs/2411.02265v1</link><description>In this paper, we introduce Hunyuan-Large, which is currently the largestopen-source Transformer-based mixture of experts model, with a total of 389billion parameters and 52 billion activation parameters, capable of handling upto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superiorperformance across various benchmarks including language understanding andgeneration, logical reasoning, mathematical problem-solving, coding,long-context, and aggregated tasks, where it outperforms LLama3.1-70B andexhibits comparable performance when compared to the significantly largerLLama3.1-405B model. Key practice of Hunyuan-Large include large-scalesynthetic data that is orders larger than in previous literature, a mixedexpert routing strategy, a key-value cache compression technique, and anexpert-specific learning rate strategy. Additionally, we also investigate thescaling laws and learning rate schedule of mixture of experts models, providingvaluable insights and guidances for future model development and optimization.The code and checkpoints of Hunyuan-Large are released to facilitate futureinnovations and applications. Codes: https://github.com/Tencent/Hunyuan-Large Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large</description><author>Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie She, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jiajia Wu, Yaping Deng, Yi Shen, Qian </author><pubDate>Mon, 04 Nov 2024 16:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02265v1</guid></item><item><title>Training Zero-Shot Generalizable End-to-End Task-Oriented Dialog System Without Turn-level Dialog Annotations</title><link>http://arxiv.org/abs/2407.15055v2</link><description>Task-oriented dialogue (TOD) systems enable users to achieve their goalsthrough natural language interactions. Traditionally, these systems have reliedon turn-level manually annotated metadata, such as dialogue states and policyannotations, which are expensive, time-consuming, and often inconsistent orerror-prone. This dependence limits the potential to leverage vast amounts ofreadily available conversational data for training TOD systems. Additionally, acritical challenge in TOD system design is determining when and how to accessand integrate information from external sources. Current approaches typicallyexpect this information to be provided alongside the dialogue context, ratherthan learning to identify and retrieve it autonomously. While pre-trained largelanguage models (LLMs) have been used to develop TOD systems, their potentialto train such systems without laborious annotations remains largely unexplored.This work employs multi-task instruction fine-tuning to create more efficientand scalable TOD systems that can effectively leverage natural languageconversational data without manual annotations, while autonomously managingexternal information retrieval. Our extensive experimental evaluations, usingthree diverse TOD datasets and three LLMs of varying sizes, demonstrate thatour approach can generalize to new, unseen domains. Notably, our approachoutperforms both state-of-the-art models trained on annotated data andbillion-scale parameter off-the-shelf ChatGPT models.</description><author>Adib Mosharrof, A. B. Siddique</author><pubDate>Mon, 04 Nov 2024 16:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15055v2</guid></item><item><title>Counterfactual Explanations via Riemannian Latent Space Traversal</title><link>http://arxiv.org/abs/2411.02259v1</link><description>The adoption of increasingly complex deep models has fueled an urgent needfor insight into how these models make predictions. Counterfactual explanationsform a powerful tool for providing actionable explanations to practitioners.Previously, counterfactual explanation methods have been designed by traversingthe latent space of generative models. Yet, these latent spaces are usuallygreatly simplified, with most of the data distribution complexity contained inthe decoder rather than the latent embedding. Thus, traversing the latent spacenaively without taking the nonlinear decoder into account can lead to unnaturalcounterfactual trajectories. We introduce counterfactual explanations obtainedusing a Riemannian metric pulled back via the decoder and the classifier underscrutiny. This metric encodes information about the complex geometric structureof the data and the learned representation, enabling us to obtain robustcounterfactual trajectories with high fidelity, as demonstrated by ourexperiments in real-world tabular datasets.</description><author>Paraskevas Pegios, Aasa Feragen, Andreas Abildtrup Hansen, Georgios Arvanitidis</author><pubDate>Mon, 04 Nov 2024 16:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02259v1</guid></item><item><title>Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs</title><link>http://arxiv.org/abs/2411.02256v1</link><description>Research in auditory, visual, and audiovisual speech recognition (ASR, VSR,and AVSR, respectively) has traditionally been conducted independently. Evenrecent self-supervised studies addressing two or all three tasks simultaneouslytend to yield separate models, leading to disjoint inference pipelines withincreased memory requirements and redundancies. This paper proposes unifiedtraining strategies for these systems. We demonstrate that training a singlemodel for all three tasks enhances VSR and AVSR performance, overcoming typicaloptimisation challenges when training from scratch. Moreover, we introduce agreedy pseudo-labelling approach to more effectively leverage unlabelledsamples, addressing shortcomings in related self-supervised methods. Finally,we develop a self-supervised pre-training method within our framework, provingits effectiveness alongside our semi-supervised approach. Despite using asingle model for all tasks, our unified approach achieves state-of-the-artperformance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR,as well as on the newly released WildVSR dataset. Code and models are availableat https://github.com/ahaliassos/usr.</description><author>Alexandros Haliassos, Rodrigo Mira, Honglie Chen, Zoe Landgraf, Stavros Petridis, Maja Pantic</author><pubDate>Mon, 04 Nov 2024 16:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02256v1</guid></item><item><title>Fashion-VDM: Video Diffusion Model for Virtual Try-On</title><link>http://arxiv.org/abs/2411.00225v2</link><description>We present Fashion-VDM, a video diffusion model (VDM) for generating virtualtry-on videos. Given an input garment image and person video, our method aimsto generate a high-quality try-on video of the person wearing the givengarment, while preserving the person's identity and motion. Image-based virtualtry-on has shown impressive results; however, existing video virtual try-on(VVT) methods are still lacking garment details and temporal consistency. Toaddress these issues, we propose a diffusion-based architecture for videovirtual try-on, split classifier-free guidance for increased control over theconditioning inputs, and a progressive temporal training strategy forsingle-pass 64-frame, 512px video generation. We also demonstrate theeffectiveness of joint image-video training for video try-on, especially whenvideo data is limited. Our qualitative and quantitative experiments show thatour approach sets the new state-of-the-art for video virtual try-on. Foradditional results, visit our project page:https://johannakarras.github.io/Fashion-VDM.</description><author>Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, Ira Kemelmacher-Shlizerman</author><pubDate>Mon, 04 Nov 2024 16:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00225v2</guid></item><item><title>Retrieving Implicit and Explicit Emotional Events Using Large Language Models</title><link>http://arxiv.org/abs/2410.19128v2</link><description>Large language models (LLMs) have garnered significant attention in recentyears due to their impressive performance. While considerable research hasevaluated these models from various perspectives, the extent to which LLMs canperform implicit and explicit emotion retrieval remains largely unexplored. Toaddress this gap, this study investigates LLMs' emotion retrieval capabilitiesin commonsense. Through extensive experiments involving multiple models, wesystematically evaluate the ability of LLMs on emotion retrieval. Specifically,we propose a supervised contrastive probing method to verify LLMs' performancefor implicit and explicit emotion retrieval, as well as the diversity of theemotional events they retrieve. The results offer valuable insights into thestrengths and limitations of LLMs in handling emotion retrieval.</description><author>Guimin Hu, Hasti Seifi</author><pubDate>Mon, 04 Nov 2024 16:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19128v2</guid></item><item><title>The Enhancement of Software Delivery Performance through Enterprise DevSecOps and Generative Artificial Intelligence in Chinese Technology Firms</title><link>http://arxiv.org/abs/2411.02255v1</link><description>This study investigates the impact of integrating DevSecOps and GenerativeArtificial Intelligence (GAI) on software delivery performance withintechnology firms. Utilizing a qualitative research methodology, the researchinvolved semi-structured interviews with industry practitioners and analysis ofcase studies from organizations that have successfully implemented thesemethodologies. The findings reveal significant enhancements in research anddevelopment (R&amp;D) efficiency, improved source code management, and heightenedsoftware quality and security. The integration of GAI facilitated automation ofcoding tasks and predictive analytics, while DevSecOps ensured that securitymeasures were embedded throughout the development lifecycle. Despite thepromising results, the study identifies gaps related to the generalizability ofthe findings due to the limited sample size and the qualitative nature of theresearch. This paper contributes valuable insights into the practicalimplementation of DevSecOps and GAI, highlighting their potential to transformsoftware delivery processes in technology firms. Future research directionsinclude quantitative assessments of the impact on specific business outcomesand comparative studies across different industries.</description><author>Jun Cui</author><pubDate>Mon, 04 Nov 2024 16:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02255v1</guid></item><item><title>Towards safe Bayesian optimization with Wiener kernel regression</title><link>http://arxiv.org/abs/2411.02253v1</link><description>Bayesian Optimization (BO) is a data-driven strategy forminimizing/maximizing black-box functions based on probabilistic surrogatemodels. In the presence of safety constraints, the performance of BO cruciallyrelies on tight probabilistic error bounds related to the uncertaintysurrounding the surrogate model. For the case of Gaussian Process surrogatesand Gaussian measurement noise, we present a novel error bound based on therecently proposed Wiener kernel regression. We prove that under rather mildassumptions, the proposed error bound is tighter than bounds previouslydocumented in the literature which leads to enlarged safety regions. We drawupon a numerical example to demonstrate the efficacy of the proposed errorbound in safe BO.</description><author>Oleksii Molodchyk, Johannes Teutsch, Timm Faulwasser</author><pubDate>Mon, 04 Nov 2024 16:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02253v1</guid></item><item><title>SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation</title><link>http://arxiv.org/abs/2405.07257v3</link><description>Most earlier researches on talking face generation have focused on thesynchronization of lip motion and speech content. However, head pose and facialemotions are equally important characteristics of natural faces. Whileaudio-driven talking face generation has seen notable advancements, existingmethods either overlook facial emotions or are limited to specific individualsand cannot be applied to arbitrary subjects. In this paper, we propose a novelone-shot Talking Head Generation framework (SPEAK) that distinguishes itselffrom the general Talking Face Generation by enabling emotional and posturalcontrol. Specifically, we introduce Inter-Reconstructed Feature Disentanglement(IRFD) module to decouple facial features into three latent spaces. Then wedesign a face editing module that modifies speech content and facial latentcodes into a single latent space. Subsequently, we present a novel generatorthat employs modified latent codes derived from the editing module to regulateemotional expression, head poses, and speech content in synthesizing facialanimations. Extensive trials demonstrate that our method ensures lipsynchronization with the audio while enabling decoupled control of facialfeatures, it can generate realistic talking head with coordinated lip motions,authentic facial emotions, and smooth head movements. The demo video isavailable: https://anonymous.4open.science/r/SPEAK-8A22</description><author>Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Fei Shen, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</author><pubDate>Mon, 04 Nov 2024 16:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07257v3</guid></item><item><title>Memetic collaborative approaches for finding balanced incomplete block designs</title><link>http://arxiv.org/abs/2411.02250v1</link><description>The balanced incomplete block design (BIBD) problem is a difficultcombinatorial problem with a large number of symmetries, which add complexityto its resolution. In this paper, we propose a dual (integer) problemrepresentation that serves as an alternative to the classical binaryformulation of the problem. We attack this problem incrementally: firstly, wepropose basic algorithms (i.e. local search techniques and genetic algorithms)intended to work separately on the two different search spaces (i.e. binary andinteger); secondly, we propose two hybrid schemes: an integrative approach(i.e. a memetic algorithm) and a collaborative model in which the previousmethods work in parallel, occasionally exchanging information. Three distincttwo-dimensional structures are proposed as communication topology among thealgorithms involved in the collaborative model, as well as a number ofmigration and acceptance criteria for sending and receiving data. An empiricalanalysis comparing a large number of instances of our schemes (with algorithmspossibly working on different search spaces and with/without symmetry breakingmethods) shows that some of these algorithms can be considered the state of theart of the metaheuristic methods applied to finding BIBDs. Moreover, ourcooperative proposal is a general scheme from which distinct algorithmicvariants can be instantiated to handle symmetrical optimisation problems. Forthis reason, we have also analysed its key parameters, thereby providinggeneral guidelines for the design of efficient/robust cooperative algorithmsdevised from our proposal.</description><author>David Rodríguez Rueda, Carlos Cotta, Antonio J. Fernández-Leiva</author><pubDate>Mon, 04 Nov 2024 16:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02250v1</guid></item><item><title>Powerful batch conformal prediction for classification</title><link>http://arxiv.org/abs/2411.02239v1</link><description>In a supervised classification split conformal/inductive framework with $K$classes, a calibration sample of $n$ labeled examples is observed for inferenceon the label of a new unlabeled example. In this work, we explore the casewhere a "batch" of $m$ independent such unlabeled examples is given, and amultivariate prediction set with $1-\alpha$ coverage should be provided forthis batch. Hence, the batch prediction set takes the form of a collection oflabel vectors of size $m$, while the calibration sample only containsunivariate labels. Using the Bonferroni correction consists in concatenatingthe individual prediction sets at level $1-\alpha/m$ (Vovk 2013). We propose auniformly more powerful solution, based on specific combinations of conformal$p$-values that exploit the Simes inequality (Simes 1986). Intuitively, thepooled evidence of fairly "easy" examples of the batch can help providenarrower batch prediction sets. We also introduced adaptive versions of thenovel procedure that are particularly effective when the batch prediction setis expected to be large. The theoretical guarantees are provided when allexamples are iid, as well as more generally when iid is assumed onlyconditionally within each class. In particular, our results are also validunder a label distribution shift since the distribution of the labels need notbe the same in the calibration sample and in the new `batch'. The usefulness ofthe method is illustrated on synthetic and real data examples.</description><author>Ulysse Gazin, Ruth Heller, Etienne Roquain, Aldo Solari</author><pubDate>Mon, 04 Nov 2024 16:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02239v1</guid></item><item><title>3D Audio-Visual Segmentation</title><link>http://arxiv.org/abs/2411.02236v1</link><description>Recognizing the sounding objects in scenes is a longstanding objective inembodied AI, with diverse applications in robotics and AR/VR/MR. To that end,Audio-Visual Segmentation (AVS), taking as condition an audio signal toidentify the masks of the target sounding objects in an input image withsynchronous camera and microphone sensors, has been recently advanced. However,this paradigm is still insufficient for real-world operation, as the mappingfrom 2D images to 3D scenes is missing. To address this fundamental limitation,we introduce a novel research problem, 3D Audio-Visual Segmentation, extendingthe existing AVS to the 3D output space. This problem poses more challenges dueto variations in camera extrinsics, audio scattering, occlusions, and diverseacoustics across sounding object categories. To facilitate this research, wecreate the very first simulation based benchmark, 3DAVS-S34-O7, providingphotorealistic 3D scene environments with grounded spatial audio undersingle-instance and multi-instance settings, across 34 scenes and 7 objectcategories. This is made possible by re-purposing the Habitat simulator togenerate comprehensive annotations of sounding object locations andcorresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,characterized by integrating the ready-to-use knowledge from pretrained 2Daudio-visual foundation models synergistically with 3D visual scenerepresentation through spatial audio-aware mask alignment and refinement.Extensive experiments demonstrate that EchoSegnet can effectively segmentsounding objects in 3D space on our new benchmark, representing a significantadvancement in the field of embodied AI. Project page:https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/</description><author>Artem Sokolov, Swapnil Bhosale, Xiatian Zhu</author><pubDate>Mon, 04 Nov 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02236v1</guid></item><item><title>emg2qwerty: A Large Dataset with Baselines for Touch Typing using Surface Electromyography</title><link>http://arxiv.org/abs/2410.20081v2</link><description>Surface electromyography (sEMG) non-invasively measures signals generated bymuscle activity with sufficient sensitivity to detect individual spinal neuronsand richness to identify dozens of gestures and their nuances. Wearablewrist-based sEMG sensors have the potential to offer low friction, subtle,information rich, always available human-computer inputs. To this end, weintroduce emg2qwerty, a large-scale dataset of non-invasive electromyographicsignals recorded at the wrists while touch typing on a QWERTY keyboard,together with ground-truth annotations and reproducible baselines. With 1,135sessions spanning 108 users and 346 hours of recording, this is the largestsuch public dataset to date. These data demonstrate non-trivial, but welldefined hierarchical relationships both in terms of the generative process,from neurons to muscles and muscle combinations, as well as in terms of domainshift across users and user sessions. Applying standard modeling techniquesfrom the closely related field of Automatic Speech Recognition (ASR), we showstrong baseline performance on predicting key-presses using sEMG signals alone.We believe the richness of this task and dataset will facilitate progress inseveral problems of interest to both the machine learning and neuroscientificcommunities. Dataset and code can be accessed athttps://github.com/facebookresearch/emg2qwerty.</description><author>Viswanath Sivakumar, Jeffrey Seely, Alan Du, Sean R Bittner, Adam Berenzweig, Anuoluwapo Bolarinwa, Alexandre Gramfort, Michael I Mandel</author><pubDate>Mon, 04 Nov 2024 16:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20081v2</guid></item><item><title>Guided Game Level Repair via Explainable AI</title><link>http://arxiv.org/abs/2410.23101v2</link><description>Procedurally generated levels created by machine learning models can beunsolvable without further editing. Various methods have been developed toautomatically repair these levels by enforcing hard constraints during thepost-processing step. However, as levels increase in size, theseconstraint-based repairs become increasingly slow. This paper proposes usingexplainability methods to identify specific regions of a level that contributeto its unsolvability. By assigning higher weights to these regions,constraint-based solvers can prioritize these problematic areas, enabling moreefficient repairs. Our results, tested across three games, demonstrate thatthis approach can help to repair procedurally generated levels faster.</description><author>Mahsa Bazzaz, Seth Cooper</author><pubDate>Mon, 04 Nov 2024 16:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23101v2</guid></item><item><title>FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training</title><link>http://arxiv.org/abs/2411.02229v1</link><description>The field of novel view synthesis from images has seen rapid advancementswith the introduction of Neural Radiance Fields (NeRF) and more recently with3D Gaussian Splatting. Gaussian Splatting became widely adopted due to itsefficiency and ability to render novel views accurately. While GaussianSplatting performs well when a sufficient amount of training images areavailable, its unstructured explicit representation tends to overfit inscenarios with sparse input images, resulting in poor rendering performance. Toaddress this, we present a 3D Gaussian-based novel view synthesis method usingsparse input images that can accurately render the scene from the viewpointsnot covered by the training images. We propose a multi-stage training schemewith matching-based consistency constraints imposed on the novel views withoutrelying on pre-trained depth estimation or diffusion models. This is achievedby using the matches of the available training images to supervise thegeneration of the novel views sampled between the training frames with color,geometry, and semantic losses. In addition, we introduce a locality preservingregularization for 3D Gaussians which removes rendering artifacts by preservingthe local color structure of the scene. Evaluation on synthetic and real-worlddatasets demonstrates competitive or superior performance of our method infew-shot novel view synthesis compared to existing state-of-the-art methods.</description><author>Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers</author><pubDate>Mon, 04 Nov 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02229v1</guid></item><item><title>Variable Selection in Convex Piecewise Linear Regression</title><link>http://arxiv.org/abs/2411.02225v1</link><description>This paper presents Sparse Gradient Descent as a solution for variableselection in convex piecewise linear regression where the model is given as$\mathrm{max}\langle a_j^\star, x \rangle + b_j^\star$ for $j = 1,\dots,k$where $x \in \mathbb R^d$ is the covariate vector. Here,$\{a_j^\star\}_{j=1}^k$ and $\{b_j^\star\}_{j=1}^k$ denote the ground-truthweight vectors and intercepts. A non-asymptotic local convergence analysis isprovided for Sp-GD under sub-Gaussian noise when the covariate distributionsatisfies sub-Gaussianity and anti-concentration property. When the model orderand parameters are fixed, Sp-GD provides an $\epsilon$-accurate estimate given$\mathcal{O}(\max(\epsilon^{-2}\sigma_z^2,1)s\log(d/s))$ observations where$\sigma_z^2$ denotes the noise variance. This also implies the exact parameterrecovery by Sp-GD from $\mathcal{O}(s\log(d/s))$ noise-free observations. Sinceoptimizing the squared loss for sparse max-affine is non-convex, aninitialization scheme is proposed to provide a suitable initial estimate withinthe basin of attraction for Sp-GD, i.e. sufficiently accurate to invoke theconvergence guarantees. The initialization scheme uses sparse principalcomponent analysis to estimate the subspace spanned by $\{ a_j^\star\}_{j=1}^k$then applies an $r$-covering search to estimate the model parameters. Anon-asymptotic analysis is presented for this initialization scheme when thecovariates and noise samples follow Gaussian distributions. When the modelorder and parameters are fixed, this initialization scheme provides an$\epsilon$-accurate estimate given$\mathcal{O}(\epsilon^{-2}\max(\sigma_z^4,\sigma_z^2,1)s^2\log^4(d))$observations. Numerical Monte Carlo results corroborate theoretical findingsfor Sp-GD and the initialization scheme.</description><author>Haitham Kanj, Seonho Kim, Kiryung Lee</author><pubDate>Mon, 04 Nov 2024 16:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02225v1</guid></item><item><title>Predicting the Temperature-Dependent CMC of Surfactant Mixtures with Graph Neural Networks</title><link>http://arxiv.org/abs/2411.02224v1</link><description>Surfactants are key ingredients in foaming and cleansing products acrossvarious industries such as personal and home care, industrial cleaning, andmore, with the critical micelle concentration (CMC) being of major interest.Predictive models for CMC of pure surfactants have been developed based onrecent ML methods, however, in practice surfactant mixtures are typically useddue to to performance, environmental, and cost reasons. This requiresaccounting for synergistic/antagonistic interactions between surfactants;however, predictive ML models for a wide spectrum of mixtures are missing sofar. Herein, we develop a graph neural network (GNN) framework for surfactantmixtures to predict the temperature-dependent CMC. We collect data for 108surfactant binary mixtures, to which we add data for pure species from ourprevious work [Brozos et al. (2024), J. Chem. Theory Comput.]. We then developand train GNNs and evaluate their accuracy across different prediction testscenarios for binary mixtures relevant to practical applications. The final GNNmodels demonstrate very high predictive performance when interpolating betweendifferent mixture compositions and for new binary mixtures with known species.Extrapolation to binary surfactant mixtures where either one or both surfactantspecies are not seen before, yields accurate results for the majority ofsurfactant systems. We further find superior accuracy of the GNN over asemi-empirical model based on activity coefficients, which has been widely usedto date. We then explore if GNN models trained solely on binary mixture andpure species data can also accurately predict the CMCs of ternary mixtures.Finally, we experimentally measure the CMC of 4 commercial surfactants thatcontain up to four species and industrial relevant mixtures and find a verygood agreement between measured and predicted CMC values.</description><author>Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos</author><pubDate>Mon, 04 Nov 2024 16:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02224v1</guid></item><item><title>Positive Experience Reflection for Agents in Interactive Text Environments</title><link>http://arxiv.org/abs/2411.02223v1</link><description>Intelligent agents designed for interactive environments face significantchallenges in text-based games, a domain that demands complex reasoning andadaptability. While agents based on large language models (LLMs) usingself-reflection have shown promise, they struggle when initially successful andexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&amp;Sour,a novel approach that addresses these limitations in existing reflectionmethods by incorporating positive experiences and managed memory to enrich thecontext available to the agent at decision time. Our comprehensive analysisspans both closed- and open-source LLMs and demonstrates the effectiveness ofSweet&amp;Sour in improving agent performance, particularly in scenarios whereprevious approaches fall short.</description><author>Philip Lippmann, Matthijs T. J. Spaan, Jie Yang</author><pubDate>Mon, 04 Nov 2024 16:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02223v1</guid></item><item><title>Targeted Learning for Variable Importance</title><link>http://arxiv.org/abs/2411.02221v1</link><description>Variable importance is one of the most widely used measures for interpretingmachine learning with significant interest from both statistics and machinelearning communities. Recently, increasing attention has been directed towarduncertainty quantification in these metrics. Current approaches largely rely onone-step procedures, which, while asymptotically efficient, can present highersensitivity and instability in finite sample settings. To address theselimitations, we propose a novel method by employing the targeted learning (TL)framework, designed to enhance robustness in inference for variable importancemetrics. Our approach is particularly suited for conditional permutationvariable importance. We show that it (i) retains the asymptotic efficiency oftraditional methods, (ii) maintains comparable computational complexity, and(iii) delivers improved accuracy, especially in finite sample contexts. Wefurther support these findings with numerical experiments that illustrate thepractical advantages of our method and validate the theoretical results.</description><author>Xiaohan Wang, Yunzhe Zhou, Giles Hooker</author><pubDate>Mon, 04 Nov 2024 16:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02221v1</guid></item><item><title>SIRA: Scalable Inter-frame Relation and Association for Radar Perception</title><link>http://arxiv.org/abs/2411.02220v1</link><description>Conventional radar feature extraction faces limitations due to low spatialresolution, noise, multipath reflection, the presence of ghost targets, andmotion blur. Such limitations can be exacerbated by nonlinear object motion,particularly from an ego-centric viewpoint. It becomes evident that to addressthese challenges, the key lies in exploiting temporal feature relation over anextended horizon and enforcing spatial motion consistency for effectiveassociation. To this end, this paper proposes SIRA (Scalable Inter-frameRelation and Association) with two designs. First, inspired by SwinTransformer, we introduce extended temporal relation, generalizing the existingtemporal relation layer from two consecutive frames to multiple inter-frameswith temporally regrouped window attention for scalability. Second, we proposemotion consistency track with the concept of a pseudo-tracklet generated fromobservational data for better trajectory prediction and subsequent objectassociation. Our approach achieves 58.11 mAP@0.5 for oriented object detectionand 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassingprevious state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA,respectively.</description><author>Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi</author><pubDate>Mon, 04 Nov 2024 16:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02220v1</guid></item><item><title>Recursive Learning of Asymptotic Variational Objectives</title><link>http://arxiv.org/abs/2411.02217v1</link><description>General state-space models (SSMs) are widely used in statistical machinelearning and are among the most classical generative models for sequentialtime-series data. SSMs, comprising latent Markovian states, can be subjected tovariational inference (VI), but standard VI methods like theimportance-weighted autoencoder (IWAE) lack functionality for streaming data.To enable online VI in SSMs when the observations are received in real time, wepropose maximising an IWAE-type variational lower bound on the asymptoticcontrast function, rather than the standard IWAE ELBO, using stochasticapproximation. Unlike the recursive maximum likelihood method, which directlymaximises the asymptotic contrast, our approach, called online sequential IWAE(OSIWAE), allows for online learning of both model parameters and a Markovianrecognition model for inferring latent states. By approximating filter stateposteriors and their derivatives using sequential Monte Carlo (SMC) methods, wecreate a particle-based framework for online VI in SSMs. This approach is moretheoretically well-founded than recently proposed online variational SMCmethods. We provide rigorous theoretical results on the learning objective anda numerical study demonstrating the method's efficiency in learning modelparameters and particle proposal kernels.</description><author>Alessandro Mastrototaro, Mathias Müller, Jimmy Olsson</author><pubDate>Mon, 04 Nov 2024 16:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02217v1</guid></item><item><title>One VLM to Keep it Learning: Generation and Balancing for Data-free Continual Visual Question Answering</title><link>http://arxiv.org/abs/2411.02210v1</link><description>Vision-Language Models (VLMs) have shown significant promise in VisualQuestion Answering (VQA) tasks by leveraging web-scale multimodal datasets.However, these models often struggle with continual learning due tocatastrophic forgetting when adapting to new tasks. As an effective remedy tomitigate catastrophic forgetting, rehearsal strategy uses the data of pasttasks upon learning new task. However, such strategy incurs the need of storingpast data, which might not be feasible due to hardware constraints or privacyconcerns. In this work, we propose the first data-free method that leveragesthe language generation capability of a VLM, instead of relying on externalmodels, to produce pseudo-rehearsal data for addressing continual VQA. Ourproposal, named as GaB, generates pseudo-rehearsal data by posing previous taskquestions on new task data. Yet, despite being effective, the distribution ofgenerated questions skews towards the most frequently posed questions due tothe limited and task-specific training data. To mitigate this issue, weintroduce a pseudo-rehearsal balancing module that aligns the generated datatowards the ground-truth data distribution using either the questionmeta-statistics or an unsupervised clustering method. We evaluate our proposedmethod on two recent benchmarks, \ie VQACL-VQAv2 and CLOVE-function benchmarks.GaB outperforms all the data-free baselines with substantial improvement inmaintaining VQA performance across evolving tasks, while being on-par withmethods with access to the past data.</description><author>Deepayan Das, Davide Talon, Massimiliano Mancini, Yiming Wang, Elisa Ricci</author><pubDate>Mon, 04 Nov 2024 16:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02210v1</guid></item><item><title>Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</title><link>http://arxiv.org/abs/2406.02550v2</link><description>Large language models can solve tasks that were not present in the trainingset. This capability is believed to be due to in-context learning and skillcomposition. In this work, we study the emergence of in-context learning andskill composition in a collection of modular arithmetic tasks. Specifically, weconsider a finite collection of linear modular functions $z = a \, x + b \, y\;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We usesome of these tasks for pre-training and the rest for out-of-distributiontesting. We empirically show that a GPT-style transformer exhibits a transitionfrom in-distribution to out-of-distribution generalization as the number ofpre-training tasks increases. We find that the smallest model capable ofout-of-distribution generalization requires two transformer blocks, while fordeeper models, the out-of-distribution generalization phase is\emph{transient}, necessitating early stopping. Finally, we perform aninterpretability study of the pre-trained models, revealing highly structuredrepresentations in both attention heads and MLPs; and discuss the learnedalgorithms. Notably, we find an algorithmic shift in deeper models, as we gofrom few to many in-context examples.</description><author>Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov</author><pubDate>Mon, 04 Nov 2024 16:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02550v2</guid></item><item><title>The Role of DevOps in Enhancing Enterprise Software Delivery Success through R&amp;D Efficiency and Source Code Management</title><link>http://arxiv.org/abs/2411.02209v1</link><description>This study examines the impact of DevOps practices on enterprise softwaredelivery success, focusing on enhancing R&amp;D efficiency and source codemanagement (SCM). Using a qualitative methodology, data were collected fromcase studies of large-scale enterprises implementing DevOps to explore howthese practices streamline software development processes. Findings reveal thatDevOps significantly improves R&amp;D productivity by fostering cross-functionalcollaboration, reducing development cycle times, and enhancing software qualitythrough effective SCM practices, such as version control and continuousintegration. Additionally, SCM tools within DevOps enable precise changetracking and reliable code maintenance, further supporting faster, more robustsoftware delivery. However, the study identifies challenges, including culturalresistance and tool integration issues, that can hinder DevOps implementation.Additionally, This research contributes to the growing body of DevOpsliterature by highlighting the role of R&amp;D efficiency and SCM as crucialfactors for software delivery success. Future studies should investigate thesefactors across diverse industries to validate findings.</description><author>Jun Cui</author><pubDate>Mon, 04 Nov 2024 16:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02209v1</guid></item><item><title>Illuminating Blind Spots of Language Models with Targeted Agent-in-the-Loop Synthetic Data</title><link>http://arxiv.org/abs/2403.17860v3</link><description>Language models (LMs) have achieved impressive accuracy across a variety oftasks but remain vulnerable to high-confidence misclassifications, alsoreferred to as unknown unknowns (UUs). These UUs cluster into blind spots inthe feature space, leading to significant risks in high-stakes applications.This is particularly relevant for smaller, lightweight LMs that are moresusceptible to such errors. While the identification of UUs has beenextensively studied, their mitigation remains an open challenge, including howto use identified UUs to eliminate unseen blind spots. In this work, we proposea novel approach to address blind spot mitigation through the use ofintelligent agents -- either humans or large LMs -- as teachers to characterizeUU-type errors. By leveraging the generalization capabilities of intelligentagents, we identify patterns in high-confidence misclassifications and use themto generate targeted synthetic samples to improve model robustness and reduceblind spots. We conduct an extensive evaluation of our method on threeclassification tasks and demonstrate its effectiveness in reducing the numberof UUs, all while maintaining a similar level of accuracy. We find that theeffectiveness of human computation has a high ceiling but is highly dependenton familiarity with the underlying task. Moreover, the cost gap between humansand LMs surpasses an order of magnitude, as LMs attain human-likegeneralization and generation performance while being more scalable.</description><author>Philip Lippmann, Matthijs T. J. Spaan, Jie Yang</author><pubDate>Mon, 04 Nov 2024 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17860v3</guid></item><item><title>Collective Model Intelligence Requires Compatible Specialization</title><link>http://arxiv.org/abs/2411.02207v1</link><description>In this work, we explore the limitations of combining models by averagingintermediate features, referred to as model merging, and propose a newdirection for achieving collective model intelligence through what we callcompatible specialization. Current methods for model merging, such as parameterand feature averaging, struggle to effectively combine specialized models dueto representational divergence during fine-tuning. As models specialize totheir individual domains, their internal feature representations becomeincreasingly incompatible, leading to poor performance when attempting to mergethem for new tasks. We analyze this phenomenon using centered kernel alignment(CKA) and show that as models specialize, the similarity in their feature spacestructure diminishes, hindering their capacity for collective use. To addressthese challenges, we investigate routing-based merging strategies, which offermore flexible methods for combining specialized models by dynamically routingacross different layers. This allows us to improve on existing methods bycombining features from multiple layers rather than relying on fixed,layer-wise combinations. However, we find that these approaches still facelimitations when layers within models are representationally incompatible. Ourfindings highlight the importance of designing new approaches for model mergingthat operate on well-defined input and output spaces, similar to how humanscommunicate through language rather than intermediate neural activations.</description><author>Jyothish Pari, Samy Jelassi, Pulkit Agrawal</author><pubDate>Mon, 04 Nov 2024 15:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02207v1</guid></item><item><title>Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms</title><link>http://arxiv.org/abs/2405.18489v2</link><description>A fundamental problem in quantum many-body physics is that of finding groundstates of local Hamiltonians. A number of recent works gave provably efficientmachine learning (ML) algorithms for learning ground states. Specifically,[Huang et al. Science 2022], introduced an approach for learning properties ofthe ground state of an $n$-qubit gapped local Hamiltonian $H$ from only$n^{\mathcal{O}(1)}$ data points sampled from Hamiltonians in the same phase ofmatter. This was subsequently improved by [Lewis et al. Nature Communications2024], to $\mathcal{O}(\log n)$ samples when the geometry of the $n$-qubitsystem is known. In this work, we introduce two approaches that achieve aconstant sample complexity, independent of system size $n$, for learning groundstate properties. Our first algorithm consists of a simple modification of theML model used by Lewis et al. and applies to a property of interest knownbeforehand. Our second algorithm, which applies even if a description of theproperty is not known, is a deep neural network model. While empirical resultsshowing the performance of neural networks have been demonstrated, to ourknowledge, this is the first rigorous sample complexity bound on a neuralnetwork model for predicting ground state properties. We also perform numericalexperiments that confirm the improved scaling of our approach compared toearlier results.</description><author>Marc Wanner, Laura Lewis, Chiranjib Bhattacharyya, Devdatt Dubhashi, Alexandru Gheorghiu</author><pubDate>Mon, 04 Nov 2024 15:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18489v2</guid></item><item><title>Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning</title><link>http://arxiv.org/abs/2411.02199v1</link><description>Transformer-based large language models (LLMs) have displayed remarkablecreative prowess and emergence capabilities. Existing empirical studies haverevealed a strong connection between these LLMs' impressive emergence abilitiesand their in-context learning (ICL) capacity, allowing them to solve new tasksusing only task-specific prompts without further fine-tuning. On the otherhand, existing empirical and theoretical studies also show that there is alinear regularity of the multi-concept encoded semantic representation behindtransformer-based LLMs. However, existing theoretical work fail to build up anunderstanding of the connection between this regularity and the innovativepower of ICL. Additionally, prior work often focuses on simplified, unrealisticscenarios involving linear transformers or unrealistic loss functions, and theyachieve only linear or sub-linear convergence rates. In contrast, this workprovides a fine-grained mathematical analysis to show how transformers leveragethe multi-concept semantics of words to enable powerful ICL and excellentout-of-distribution ICL abilities, offering insights into how transformersinnovate solutions for certain unseen tasks encoded with multiple cross-conceptsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,the analysis is based on a concept-based low-noise sparse coding prompt model.Leveraging advanced techniques, this work showcases the exponential 0-1 lossconvergence over the highly non-convex training dynamics, which pioneeringlyincorporates the challenges of softmax self-attention, ReLU-activated MLPs, andcross-entropy loss. Empirical simulations corroborate the theoretical findings.</description><author>Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong</author><pubDate>Mon, 04 Nov 2024 15:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02199v1</guid></item><item><title>GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist Collaboration</title><link>http://arxiv.org/abs/2404.15127v2</link><description>Generalist foundation models (GFMs) are renowned for their exceptionalcapability and flexibility in effectively generalizing across diverse tasks andmodalities. In the field of medicine, while GFMs exhibit superiorgeneralizability based on their extensive intrinsic knowledge as well asproficiency in instruction following and in-context learning, specialist modelsexcel in precision due to their domain knowledge. In this work, for the firsttime, we explore the synergy between the GFM and specialist models, to enableprecise medical image analysis on a broader scope. Specifically, we propose acooperative framework, Generalist-Specialist Collaboration (GSCo), whichconsists of two stages, namely the construction of GFM and specialists, andcollaborative inference on downstream tasks. In the construction stage, wedevelop MedDr, the largest open-source GFM tailored for medicine, showcasingexceptional instruction-following and in-context learning capabilities.Meanwhile, a series of lightweight specialists are crafted for downstream taskswith low computational cost. In the collaborative inference stage, we introducetwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-AugmentedDiagnosis, to harvest the generalist's in-context learning abilities alongsidethe specialists' domain expertise. For a comprehensive evaluation, we curate alarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensiveresults demonstrate that MedDr consistently outperforms state-of-the-art GFMson downstream datasets. Furthermore, GSCo exceeds both GFMs and specialistsacross all out-of-domain disease diagnosis datasets. These findings indicate asignificant paradigm shift in the application of GFMs, transitioning fromseparate models for specific tasks to a collaborative approach between GFMs andspecialists, thereby advancing the frontiers of generalizable AI in medicine.</description><author>Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, Ronald Cheong Kin Chan, Varut Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen</author><pubDate>Mon, 04 Nov 2024 15:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15127v2</guid></item></channel></rss>