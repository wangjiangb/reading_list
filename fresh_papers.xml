<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 03 Dec 2024 13:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Compute-Constrained Data Selection</title><link>http://arxiv.org/abs/2410.16208v3</link><description>Data selection can reduce the amount of training data needed to finetuneLLMs; however, the efficacy of data selection scales directly with its compute.Motivated by the practical challenge of compute-constrained finetuning, weconsider the setting in which both the cost of selecting data and training arebudgeted for. We first formalize the problem of data selection with acost-aware utility function, and model the data selection problem as tradingoff initial-selection cost for training gain. We run a comprehensive sweep ofexperiments across multiple tasks, varying compute budget by scaling finetuningtokens, model sizes, and data selection compute. Interestingly we find thatmany powerful data selection methods are almost never compute-optimal, and thatcheaper data selection alternatives dominate both from a theoretical andempirical perspective. For compute-optimal training, we find that perplexityand gradient data selection require training-to-selection model size ratios of5x and 10x, respectively.</description><author>Junjie Oscar Yin, Alexander M. Rush</author><pubDate>Mon, 02 Dec 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16208v3</guid></item><item><title>A Note on Doubly Robust Estimator in Regression Continuity Designs</title><link>http://arxiv.org/abs/2411.07978v3</link><description>This note introduces a doubly robust (DR) estimator for regressiondiscontinuity (RD) designs. RD designs provide a quasi-experimental frameworkfor estimating treatment effects, where treatment assignment depends on whethera running variable surpasses a predefined cutoff. A common approach in RDestimation is the use of nonparametric regression methods, such as local linearregression. However, the validity of these methods still relies on theconsistency of the nonparametric estimators. In this study, we propose theDR-RD estimator, which combines two distinct estimators for the conditionalexpected outcomes. The primary advantage of the DR-RD estimator lies in itsability to ensure the consistency of the treatment effect estimation as long asat least one of the two estimators is consistent. Consequently, our DR-RDestimator enhances robustness of treatment effect estimators in RD designs.</description><author>Masahiro Kato</author><pubDate>Mon, 02 Dec 2024 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07978v3</guid></item><item><title>Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers</title><link>http://arxiv.org/abs/2411.17501v2</link><description>Recent research has generated hope that inference scaling could allow weakerlanguage models to match or exceed the accuracy of stronger models, such as byrepeatedly sampling solutions to a coding problem until it passes unit tests.The central thesis of this paper is that there is no free lunch for inferencescaling: indefinite accuracy improvement through resampling can only berealized if the "verifier" (in this case, a set of unit tests) is perfect. Whenthe verifier is imperfect, as it almost always is in domains such as reasoningor coding (for example, unit tests have imperfect coverage), there is a nonzeroprobability of false positives: incorrect solutions that pass the verifier.Resampling cannot decrease this probability, so it imposes an upper bound tothe accuracy of resampling-based inference scaling even with an infinitecompute budget. We find that there is a very strong correlation between themodel's single-sample accuracy (i.e. accuracy without unit tests) and its falsepositive rate on coding benchmarks HumanEval and MBPP, whose unit tests havelimited coverage. Therefore, no amount of inference scaling of weaker modelscan enable them to match the single-sample accuracy of a sufficiently strongmodel (Fig. 1a). When we consider that false positives have a negative utilitycompared to abstaining from producing a solution, it bends the inferencescaling curve further downward. Empirically, we find that the optimal number ofsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, weshow that beyond accuracy, false positives may have other undesirablequalities, such as poor adherence to coding style conventions.</description><author>Benedikt Stroebl, Sayash Kapoor, Arvind Narayanan</author><pubDate>Mon, 02 Dec 2024 18:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17501v2</guid></item><item><title>Topology-Based Reconstruction Prevention for Decentralised Learning</title><link>http://arxiv.org/abs/2312.05248v3</link><description>Decentralised learning has recently gained traction as an alternative tofederated learning in which both data and coordination are distributed. Topreserve the confidentiality of users' data, decentralised learning relies ondifferential privacy, multi-party computation, or both. However, runningmultiple privacy-preserving summations in sequence may allow adversaries toperform reconstruction attacks. Current reconstruction countermeasures eithercannot trivially be adapted to the distributed setting, or add excessiveamounts of noise. In this work, we first show that passive honest-but-curious adversaries caninfer other users' private data after several privacy-preserving summations.For example, in subgraphs with 18 users, we show that only three passivehonest-but-curious adversaries succeed at reconstructing private data 11.0% ofthe time, requiring an average of 8.8 summations per adversary. The successrate depends only on the adversaries' direct neighbourhood, and is independentof the size of the full network. We consider weak adversaries that do notcontrol the graph topology, cannot exploit the summation's inner workings, anddo not have auxiliary knowledge; and show that these adversaries can stillinfer private data. We analyse how reconstruction relates to topology and propose the firsttopology-based decentralised defence against reconstruction attacks. We showthat reconstruction requires a number of adversaries linear in the length ofthe network's shortest cycle. Consequently, exact attacks overprivacy-preserving summations are impossible in acyclic networks. Our work is a stepping stone for a formal theory of topology-baseddecentralised reconstruction defences. Such a theory would generalise ourcountermeasure beyond summation, define confidentiality in terms of entropy,and describe the interactions with (topology-aware) differential privacy.</description><author>Florine W. Dekker, Zekeriya Erkin, Mauro Conti</author><pubDate>Mon, 02 Dec 2024 18:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05248v3</guid></item><item><title>Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive Model</title><link>http://arxiv.org/abs/2410.09943v2</link><description>We introduce a new class of adaptive non-linear autoregressive (Nlar) modelsincorporating the concept of momentum, which dynamically estimate both thelearning rates and momentum as the number of iterations increases. In ourmethod, the growth of the gradients is controlled using a scaling (clipping)function, leading to stable convergence. Within this framework, we proposethree distinct estimators for learning rates and provide theoretical proof oftheir convergence. We further demonstrate how these estimators underpin thedevelopment of effective Nlar optimizers. The performance of the proposedestimators and optimizers is rigorously evaluated through extensive experimentsacross several datasets and a reinforcement learning environment. The resultshighlight two key features of the Nlar optimizers: robust convergence despitevariations in underlying parameters, including large initial learning rates,and strong adaptability with rapid convergence during the initial epochs.</description><author>Ramin Okhrati</author><pubDate>Mon, 02 Dec 2024 18:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.09943v2</guid></item><item><title>CREW: Facilitating Human-AI Teaming Research</title><link>http://arxiv.org/abs/2408.00170v2</link><description>With the increasing deployment of artificial intelligence (AI) technologies,the potential of humans working with AI agents has been growing at a greatspeed. Human-AI teaming is an important paradigm for studying various aspectswhen humans and AI agents work together. The unique aspect of Human-AI teamingresearch is the need to jointly study humans and AI agents, demandingmultidisciplinary research efforts from machine learning to human-computerinteraction, robotics, cognitive science, neuroscience, psychology, socialscience, and complex systems. However, existing platforms for Human-AI teamingresearch are limited, often supporting oversimplified scenarios and a singletask, or specifically focusing on either human-teaming research or multi-agentAI algorithms. We introduce CREW, a platform to facilitate Human-AI teamingresearch in real-time decision-making scenarios and engage collaborations frommultiple scientific disciplines, with a strong emphasis on human involvement.It includes pre-built tasks for cognitive studies and Human-AI teaming withexpandable potentials from our modular design. Following conventional cognitiveneuroscience research, CREW also supports multimodal human physiological signalrecording for behavior analysis. Moreover, CREW benchmarks real-timehuman-guided reinforcement learning agents using state-of-the-art algorithmsand well-tuned baselines. With CREW, we were able to conduct 50 human subjectstudies within a week to verify the effectiveness of our benchmark.</description><author>Lingyu Zhang, Zhengran Ji, Boyuan Chen</author><pubDate>Mon, 02 Dec 2024 18:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00170v2</guid></item><item><title>Two Tales of Single-Phase Contrastive Hebbian Learning</title><link>http://arxiv.org/abs/2402.08573v3</link><description>The search for ``biologically plausible'' learning algorithms has convergedon the idea of representing gradients as activity differences. However, mostapproaches require a high degree of synchronization (distinct phases duringlearning) and introduce substantial computational overhead, which raises doubtsregarding their biological plausibility as well as their potential utility forneuromorphic computing. Furthermore, they commonly rely on applyinginfinitesimal perturbations (nudges) to output units, which is impractical innoisy environments. Recently it has been shown that by modelling artificialneurons as dyads with two oppositely nudged compartments, it is possible for afully local learning algorithm named ``dual propagation'' to bridge theperformance gap to backpropagation, without requiring separate learning phasesor infinitesimal nudging. However, the algorithm has the drawback that itsnumerical stability relies on symmetric nudging, which may be restrictive inbiological and analog implementations. In this work we first provide a solidfoundation for the objective underlying the dual propagation method, which alsoreveals a surprising connection with adversarial robustness. Second, wedemonstrate how dual propagation is related to a particular adjoint statemethod, which is stable regardless of asymmetric nudging.</description><author>Rasmus Kjær Høier, Christopher Zach</author><pubDate>Mon, 02 Dec 2024 18:33:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08573v3</guid></item><item><title>Inducing Group Fairness in Prompt-Based Language Model Decisions</title><link>http://arxiv.org/abs/2406.16738v2</link><description>Classifiers are used throughout industry to enforce policies, ranging fromthe detection of toxic content to age-appropriate content filtering. Whilethese classifiers serve important functions, it is also essential that they arebuilt in ways that minimize unfair biases for users. One such fairness consideration is called group fairness, which desires thatdifferent sub-population of users receive equal treatment. This is awell-studied problem in the context of 'classical' classifiers. However, theemergence of prompt-based language model (LM) decision making has created newopportunities to solve text-based classification tasks, and the fairnessproperties of these new classifiers are not yet well understood. Further, the`remediation toolkit' is incomplete for LM-based decision makers and little isunderstood about how to improve decision maker group fairness while maintainingclassifier performance. This work sets out to add more tools to that toolbox. We introduceadaptations of existing effective approaches from the classical classifierfairness to the prompt-based classifier space. We also devise simple methodsthat take advantage of the new structure of prompt-based decision makers andoperate at the prompt level. We compare these approaches empirically on realdata. Our results suggest that adaptations of approaches that are effective forclassical classifiers remain effective in the LM-based classifier environment.However, there is room for further exploration of prompt-based remediationmethods (and other remediation methods that take advantage of LM structure).</description><author>James Atwood, Nino Scherrer, Preethi Lahoti, Ananth Balashankar, Flavien Prost, Ahmad Beirami</author><pubDate>Mon, 02 Dec 2024 18:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16738v2</guid></item><item><title>RIRAG: Regulatory Information Retrieval and Answer Generation</title><link>http://arxiv.org/abs/2409.05677v2</link><description>Regulatory documents, issued by governmental regulatory bodies, establishrules, guidelines, and standards that organizations must adhere to for legalcompliance. These documents, characterized by their length, complexity andfrequent updates, are challenging to interpret, requiring significantallocation of time and expertise on the part of organizations to ensure ongoingcompliance. Regulatory Natural Language Processing (RegNLP) is amultidisciplinary field aimed at simplifying access to and interpretation ofregulatory rules and obligations. We introduce a task of generatingquestion-passages pairs, where questions are automatically created and pairedwith relevant regulatory passages, facilitating the development of regulatoryquestion-answering systems. We create the ObliQA dataset, containing 27,869questions derived from the collection of Abu Dhabi Global Markets (ADGM)financial regulation documents, design a baseline Regulatory InformationRetrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, anovel evaluation metric that tests whether generated answers accurately captureall relevant obligations while avoiding contradictions.</description><author>Tuba Gokhan, Kexin Wang, Iryna Gurevych, Ted Briscoe</author><pubDate>Mon, 02 Dec 2024 18:13:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05677v2</guid></item><item><title>Regression Trees Know Calculus</title><link>http://arxiv.org/abs/2405.13846v2</link><description>Regression trees have emerged as a preeminent tool for solving real-worldregression problems due to their ability to deal with nonlinearities,interaction effects and sharp discontinuities. In this article, we rather studyregression trees applied to well-behaved, differentiable functions, anddetermine the relationship between node parameters and the local gradient ofthe function being approximated. We find a simple estimate of the gradientwhich can be efficiently computed using quantities exposed by popular treelearning libraries. This allows the tools developed in the context ofdifferentiable algorithms, like neural nets and Gaussian processes, to bedeployed to tree-based models. To demonstrate this, we study measures of modelsensitivity defined in terms of integrals of gradients and demonstrate how tocompute them for regression trees using the proposed gradient estimates.Quantitative and qualitative numerical experiments reveal the capability ofgradients estimated by regression trees to improve predictive analysis, solvetasks in uncertainty quantification, and provide interpretation of modelbehavior.</description><author>Nathan Wycoff</author><pubDate>Mon, 02 Dec 2024 18:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13846v2</guid></item><item><title>Asynchronous Message-Passing and Zeroth-Order Optimization Based Distributed Learning with a Use-Case in Resource Allocation in Communication Networks</title><link>http://arxiv.org/abs/2311.04604v3</link><description>Distributed learning and adaptation have received significant interest andfound wide-ranging applications in machine learning and signal processing.While various approaches, such as shared-memory optimization, multi-tasklearning, and consensus-based learning (e.g., federated learning and learningover graphs), focus on optimizing either local costs or a global cost, thereremains a need for further exploration of their interconnections. This paperspecifically focuses on a scenario where agents collaborate towards a commontask (i.e., optimizing a global cost equal to aggregated local costs) whileeffectively having distinct individual tasks (i.e., optimizing individual localparameters in a local cost). Each agent's actions can potentially impact otheragents' performance through interactions. Notably, each agent has access toonly its local zeroth-order oracle (i.e., cost function value) and sharesscalar values, rather than gradient vectors, with other agents, leading tocommunication bandwidth efficiency and agent privacy. Agents employzeroth-order optimization to update their parameters, and the asynchronousmessage-passing between them is subject to bounded but possibly randomcommunication delays. This paper presents theoretical convergence analyses andestablishes a convergence rate for nonconvex problems. Furthermore, itaddresses the relevant use-case of deep learning-based resource allocation incommunication networks and conducts numerical experiments in which agents,acting as transmitters, collaboratively train their individual policies tomaximize a global reward, e.g., a sum of data rates.</description><author>Pourya Behmandpoor, Marc Moonen, Panagiotis Patrinos</author><pubDate>Mon, 02 Dec 2024 18:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04604v3</guid></item><item><title>Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation</title><link>http://arxiv.org/abs/2409.16921v2</link><description>Motion correction (MoCo) in radial MRI is a challenging problem due to theunpredictability of subject's motion. Current state-of-the-art (SOTA) MoCoalgorithms often use extensive high-quality MR images to pre-train neuralnetworks, obtaining excellent reconstructions. However, the need forlarge-scale datasets significantly increases costs and limits modelgeneralization. In this work, we propose Moner, an unsupervised MoCo methodthat jointly solves artifact-free MR images and accurate motion fromundersampled, rigid motion-corrupted k-space data, without requiring trainingdata. Our core idea is to leverage the continuous prior of implicit neuralrepresentation (INR) to constrain this ill-posed inverse problem, enablingideal solutions. Specifically, we incorporate a quasi-static motion model intothe INR, granting its ability to correct subject's motion. To stabilize modeloptimization, we reformulate radial MRI as a back-projection problem using theFourier-slice theorem. Additionally, we propose a novel coarse-to-fine hashencoding strategy, significantly enhancing MoCo accuracy. Experiments onmultiple MRI datasets show our Moner achieves performance comparable to SOTAMoCo techniques on in-domain data, while demonstrating significant improvementson out-of-domain data.</description><author>Qing Wu, Chenhe Du, XuanYu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei</author><pubDate>Mon, 02 Dec 2024 18:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16921v2</guid></item><item><title>Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure</title><link>http://arxiv.org/abs/2410.24060v5</link><description>In this work, we study the generalizability of diffusion models by lookinginto the hidden properties of the learned score functions, which areessentially a series of deep denoisers trained on various noise levels. Weobserve that as diffusion models transition from memorization togeneralization, their corresponding nonlinear diffusion denoisers exhibitincreasing linearity. This discovery leads us to investigate the linearcounterparts of the nonlinear diffusion models, which are a series of linearmodels trained to match the function mappings of the nonlinear diffusiondenoisers. Surprisingly, these linear denoisers are approximately the optimaldenoisers for a multivariate Gaussian distribution characterized by theempirical mean and covariance of the training dataset. This finding impliesthat diffusion models have the inductive bias towards capturing and utilizingthe Gaussian structure (covariance information) of the training dataset fordata generation. We empirically demonstrate that this inductive bias is aunique property of diffusion models in the generalization regime, which becomesincreasingly evident when the model's capacity is relatively small compared tothe training dataset size. In the case that the model is highlyoverparameterized, this inductive bias emerges during the initial trainingphases before the model fully memorizes its training data. Our study providescrucial insights into understanding the notable strong generalizationphenomenon recently observed in real-world diffusion models.</description><author>Xiang Li, Yixiang Dai, Qing Qu</author><pubDate>Mon, 02 Dec 2024 18:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24060v5</guid></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>http://arxiv.org/abs/2411.15098v3</link><description>In this paper, we introduce OminiControl, a highly versatile andparameter-efficient framework that integrates image conditions into pre-trainedDiffusion Transformer (DiT) models. At its core, OminiControl leverages aparameter reuse mechanism, enabling the DiT to encode image conditions usingitself as a powerful backbone and process them with its flexible multi-modalattention processors. Unlike existing methods, which rely heavily on additionalencoder modules with complex architectures, OminiControl (1) effectively andefficiently incorporates injected image conditions with only ~0.1% additionalparameters, and (2) addresses a wide range of image conditioning tasks in aunified manner, including subject-driven generation and spatially-alignedconditions such as edges, depth, and more. Remarkably, these capabilities areachieved by training on images generated by the DiT itself, which isparticularly beneficial for subject-driven generation. Extensive evaluationsdemonstrate that OminiControl outperforms existing UNet-based and DiT-adaptedmodels in both subject-driven and spatially-aligned conditional generation.Additionally, we release our training dataset, Subjects200K, a diversecollection of over 200,000 identity-consistent images, along with an efficientdata synthesis pipeline to advance research in subject-consistent generation.</description><author>Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang</author><pubDate>Mon, 02 Dec 2024 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15098v3</guid></item><item><title>GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2411.19895v2</link><description>3D Gaussian Splatting (3DGS) has recently created impressive assets forvarious applications. However, the copyright of these assets is not wellprotected as existing watermarking methods are not suited for 3DGS consideringsecurity, capacity, and invisibility. Besides, these methods often requirehours or even days for optimization, limiting the application scenarios. Inthis paper, we propose GuardSplat, an innovative and efficient framework thateffectively protects the copyright of 3DGS assets. Specifically, 1) We firstpropose a CLIP-guided Message Decoupling Optimization module for training themessage decoder, leveraging CLIP's aligning capability and rich representationsto achieve a high extraction accuracy with minimal optimization costs,presenting exceptional capability and efficiency. 2) Then, we propose aSpherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,which employs a set of SH offsets to seamlessly embed the message into the SHfeatures of each 3D Gaussian while maintaining the original 3D structure. Itenables the 3DGS assets to be watermarked with minimal fidelity trade-offs andprevents malicious users from removing the messages from the model files,meeting the demands for invisibility and security. 3) We further propose anAnti-distortion Message Extraction module to improve robustness against variousvisual distortions. Extensive experiments demonstrate that GuardSplatoutperforms the state-of-the-art methods and achieves fast optimization speed.</description><author>Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie</author><pubDate>Mon, 02 Dec 2024 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19895v2</guid></item><item><title>What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics</title><link>http://arxiv.org/abs/2411.17593v3</link><description>The integration of new literature into the English curriculum remains achallenge since educators often lack scalable tools to rapidly evaluatereadability and adapt texts for diverse classroom needs. This study proposes toaddress this gap through a multimodal approach that combines transformer-basedtext classification with linguistic feature analysis to align texts with UK KeyStages. Eight state-of-the-art Transformers were fine-tuned on segmented textdata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,500 deep neural network topologies were searched for the classification oflinguistic characteristics, achieving an F1 score of 0.392. The fusion of thesemodalities shows a significant improvement, with every multimodal approachoutperforming all unimodal models. In particular, the ELECTRA Transformer fusedwith the neural network achieved an F1 score of 0.996. Unimodal and multimodalapproaches are shown to have statistically significant differences in allvalidation metrics (accuracy, precision, recall, F1 score) except for inferencetime. The proposed approach is finally encapsulated in a stakeholder-facing webapplication, providing non-technical stakeholder access to real-time insightson text complexity, reading difficulty, curriculum alignment, andrecommendations for learning age range. The application empowers data-drivendecision making and reduces manual workload by integrating AI-basedrecommendations into lesson planning for English literature.</description><author>Jordan J. Bird</author><pubDate>Mon, 02 Dec 2024 17:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17593v3</guid></item><item><title>End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization</title><link>http://arxiv.org/abs/2401.12850v2</link><description>Speaker diarization, the task of segmenting an audio recording based onspeaker identity, constitutes an important speech pre-processing step forseveral downstream applications.The conventional approach to diarizationinvolves multiple steps of embedding extraction and clustering, which are oftenoptimized in an isolated fashion. While end-to-end diarization systems attemptto learn a single model for the task, they are often cumbersome to train andrequire large supervised datasets. In this paper, we propose an end-to-endsupervised hierarchical clustering algorithm based on graph neural networks(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). Theembedding extractor is initialized using a pre-trained x-vector model while theGNN model is trained initially using the x-vector embeddings from thepre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbankfeatures as input and jointly optimizes the embedding extractor and the GNNclustering module, performing representation learning, metric learning, andclustering with end-to-end optimization. Further, with additional inputs froman external overlap detector, the E-SHARC approach is capable of predicting thespeakers in the overlapping speech regions. The experimental evaluation onbenchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that theproposed E-SHARC framework provides competitive diarization results using graphbased clustering methods.</description><author>Prachi Singh, Sriram Ganapathy</author><pubDate>Mon, 02 Dec 2024 17:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12850v2</guid></item><item><title>Discovering group dynamics in coordinated time series via hierarchical recurrent switching-state models</title><link>http://arxiv.org/abs/2401.14973v2</link><description>We seek a computationally efficient model for a collection of time seriesarising from multiple interacting entities (a.k.a. "agents"). Recent models ofspatiotemporal patterns across individuals fail to incorporate explicitsystem-level collective behavior that can influence the trajectories ofindividual entities. To address this gap in the literature, we present a newhierarchical switching-state model that can be trained in an unsupervisedfashion to simultaneously learn both system-level and individual-leveldynamics. We employ a latent system-level discrete state Markov chain thatprovides top-down influence on latent entity-level chains which in turn governthe emission of each observed time series. Recurrent feedback from theobservations to the latent chains at both entity and system levels allowsrecent situational context to inform how dynamics unfold at all levels inbottom-up fashion. We hypothesize that including both top-down and bottom-upinfluences on group dynamics will improve interpretability of the learneddynamics and reduce error when forecasting. Our hierarchical switchingrecurrent dynamical model can be learned via closed-form variational coordinateascent updates to all latent chains that scale linearly in the number ofentities. This is asymptotically no more costly than fitting a separate modelfor each entity. Analysis of both synthetic data and real basketball teammovements suggests our lean parametric model can achieve competitive forecastscompared to larger neural network models that require far more computationalresources. Further experiments on soldier data as well as a synthetic task with64 cooperating entities show how our approach can yield interpretable insightsabout team dynamics over time.</description><author>Michael T. Wojnowicz, Kaitlin Gili, Preetish Rath, Eric Miller, Jeffrey Miller, Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Tad T. Brunyé, Michael C. Hughes</author><pubDate>Mon, 02 Dec 2024 17:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14973v2</guid></item><item><title>A Conditional Independence Test in the Presence of Discretization</title><link>http://arxiv.org/abs/2404.17644v4</link><description>Testing conditional independence has many applications, such as in Bayesiannetwork learning and causal discovery. Different test methods have beenproposed. However, existing methods generally can not work when onlydiscretized observations are available. Specifically, consider $X_1$,$\tilde{X}_2$ and $X_3$ are observed variables, where $\tilde{X}_2$ is adiscretization of latent variables $X_2$. Applying existing test methods to theobservations of $X_1$, $\tilde{X}_2$ and $X_3$ can lead to a false conclusionabout the underlying conditional independence of variables $X_1$, $X_2$ and$X_3$. Motivated by this, we propose a conditional independence testspecifically designed to accommodate the presence of such discretization. Toachieve this, we design the bridge equations to recover the parameterreflecting the statistical information of the underlying latent continuousvariables. An appropriate test statistic and its asymptotic distribution underthe null hypothesis of conditional independence have also been derived. Boththeoretical results and empirical validation have been provided, demonstratingthe effectiveness of our test methods.</description><author>Boyang Sun, Yu Yao, Huangyuan Hao, Yumou Qiu, Kun Zhang</author><pubDate>Mon, 02 Dec 2024 17:12:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17644v4</guid></item><item><title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title><link>http://arxiv.org/abs/2411.07118v3</link><description>Transformer models have demonstrated remarkable success in many domains suchas natural language processing (NLP) and computer vision. With the growinginterest in transformer-based architectures, they are now utilized for gesturerecognition. So, we also explore and devise a novel ConvMixFormer architecturefor dynamic hand gestures. The transformers use quadratic scaling of theattention features with the sequential data, due to which these models arecomputationally complex and heavy. We have considered this drawback of thetransformer and designed a resource-efficient model that replaces theself-attention in the transformer with the simple convolutional layer-basedtoken mixer. The computational cost and the parameters used for theconvolution-based mixer are comparatively less than the quadraticself-attention. Convolution-mixer helps the model capture the local spatialfeatures that self-attention struggles to capture due to their sequentialprocessing nature. Further, an efficient gate mechanism is employed instead ofa conventional feed-forward network in the transformer to help the modelcontrol the flow of features within different stages of the proposed model.This design uses fewer learnable parameters which is nearly half the vanillatransformer that helps in fast and efficient training. The proposed method isevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model hasachieved state-of-the-art results on single and multimodal inputs. We have alsoshown the parameter efficiency of the proposed ConvMixFormer model compared toother methods. The source code is available athttps://github.com/mallikagarg/ConvMixFormer.</description><author>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</author><pubDate>Mon, 02 Dec 2024 17:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07118v3</guid></item><item><title>Learning Temporally Consistent Video Depth from Video Diffusion Priors</title><link>http://arxiv.org/abs/2406.01493v3</link><description>This work addresses the challenge of streamed video depth estimation, whichexpects not only per-frame accuracy but, more importantly, cross-frameconsistency. We argue that sharing contextual information between frames orclips is pivotal in fostering temporal consistency. Thus, instead of directlydeveloping a depth estimator from scratch, we reformulate this predictive taskinto a conditional generation problem to provide contextual information withina clip and across clips. Specifically, we propose a consistent context-awaretraining and inference strategy for arbitrarily long videos to providecross-clip context. We sample independent noise levels for each frame within aclip during training while using a sliding window strategy and initializingoverlapping frames with previously predicted frames without adding noise.Moreover, we design an effective training strategy to provide context within aclip. Extensive experimental results validate our design choices anddemonstrate the superiority of our approach, dubbed ChronoDepth. Project page:https://xdimlab.github.io/ChronoDepth/.</description><author>Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, Yiyi Liao</author><pubDate>Mon, 02 Dec 2024 17:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01493v3</guid></item><item><title>PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics</title><link>http://arxiv.org/abs/2408.10789v2</link><description>Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3DGaussians, are commonly used to represent 3D objects or scenes. However, humanperception typically understands 3D objects at a higher level as a compositionof parts or structures rather than points or voxels. Representing 3D objects orscenes as semantic parts can benefit further understanding and applications. Inthis paper, we introduce $\textbf{PartGS}$, $\textbf{part}$-aware 3Dreconstruction by a hybrid representation of 2D $\textbf{G}$aussians and$\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts,digging 3D structural clues from multi-view image inputs. Accurate structuredgeometry reconstruction and high-quality rendering are achieved at the sametime. Our method simultaneously optimizes superquadric meshes and Gaussians bycoupling their parameters within our hybrid representation. On one hand, thishybrid representation inherits the advantage of superquadrics to representdifferent shape primitives, supporting flexible part decomposition of scenes.On the other hand, 2D Gaussians capture complex texture and geometry details,ensuring high-quality appearance and geometry reconstruction. Our method isfully unsupervised and outperforms existing state-of-the-art approaches inextensive experiments on DTU, ShapeNet, and real-life datasets.</description><author>Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</author><pubDate>Mon, 02 Dec 2024 17:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10789v2</guid></item><item><title>Artificial intelligence contribution to translation industry: looking back and forward</title><link>http://arxiv.org/abs/2411.19855v2</link><description>This study provides a comprehensive analysis of artificial intelligence (AI)contribution to translation industry (ACTI) research, synthesizing it overforty-one years from 1980-2024. 13220 articles were retrieved from threesources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,scientometric and thematic, focusing on cluster, subject categories, keywords,burstness, centrality and research centers as for the former. For the latter,we thematically review 18 articles, selected purposefully from the articlesinvolved, centering on purpose, approach, findings, and contribution to ACTIfuture directions. The findings reveal that in the past AI contribution totranslation industry was not rigorous, resulting in rule-based machinetranslation and statistical machine translation whose output was notsatisfactory. However, the more AI develops, the more machine translationdevelops, incorporating Neural Networking Algorithms and (Deep) LanguageLearning Models like ChatGPT whose translation output has developedconsiderably. However, much rigorous research is still needed to overcomeseveral problems encountering translation industry, specifically concerninglow-source languages, multi-dialectical and free word order languages, andcultural and religious registers.</description><author>Mohammed Q. Shormani</author><pubDate>Mon, 02 Dec 2024 16:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19855v2</guid></item><item><title>DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object Detection and Tracking in Traffic Surveillance</title><link>http://arxiv.org/abs/2411.17251v2</link><description>Accurate detection and tracking of small objects such as pedestrians,cyclists, and motorbikes are critical for traffic surveillance systems, whichare crucial in improving road safety and decision-making in intelligenttransportation systems. However, traditional methods struggle with challengessuch as occlusion, low resolution, and dynamic traffic conditions,necessitating innovative approaches to address these limitations. This paperintroduces DGNN-YOLO, a novel framework integrating dynamic graph neuralnetworks (DGNN) with YOLO11 to enhance small object detection and tracking intraffic surveillance systems. The framework leverages YOLO11's advanced spatialfeature extraction capabilities for precise object detection and incorporatesDGNN to model spatial-temporal relationships for robust real-time trackingdynamically. By constructing and updating graph structures, DGNN-YOLOeffectively represents objects as nodes and their interactions as edges,ensuring adaptive and accurate tracking in complex and dynamic environments.Extensive experiments demonstrate that DGNN-YOLO consistently outperformsstate-of-the-art methods in detecting and tracking small objects under diversetraffic conditions, achieving the highest precision (0.8382), recall (0.6875),and mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,particularly in challenging scenarios involving small and occluded objects.This work provides a scalable, real-time traffic surveillance and analysissolution, significantly contributing to intelligent transportation systems.</description><author>Shahriar Soudeep, M. F. Mridha, Md Abrar Jahin, Nilanjan Dey</author><pubDate>Mon, 02 Dec 2024 16:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17251v2</guid></item><item><title>Probabilistic Graph Rewiring via Virtual Nodes</title><link>http://arxiv.org/abs/2405.17311v3</link><description>Message-passing graph neural networks (MPNNs) have emerged as a powerfulparadigm for graph-based machine learning. Despite their effectiveness, MPNNsface challenges such as under-reaching and over-squashing, where limitedreceptive fields and structural bottlenecks hinder information flow in thegraph. While graph transformers hold promise in addressing these issues, theirscalability is limited due to quadratic complexity regarding the number ofnodes, rendering them impractical for larger graphs. Here, we proposeimplicitly rewired message-passing neural networks (IPR-MPNNs), a novelapproach that integrates implicit probabilistic graph rewiring into MPNNs. Byintroducing a small number of virtual nodes, i.e., adding additional nodes to agiven graph and connecting them to existing nodes, in a differentiable,end-to-end manner, IPR-MPNNs enable long-distance message propagation,circumventing quadratic complexity. Theoretically, we demonstrate thatIPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, wevalidate our approach by showcasing its ability to mitigate under-reaching andover-squashing effects, achieving state-of-the-art performance across multiplegraph datasets. Notably, IPR-MPNNs outperform graph transformers whilemaintaining significantly faster computational efficiency.</description><author>Chendi Qian, Andrei Manolache, Christopher Morris, Mathias Niepert</author><pubDate>Mon, 02 Dec 2024 16:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17311v3</guid></item><item><title>Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network</title><link>http://arxiv.org/abs/2404.13983v2</link><description>Given a source portrait, the automatic human body reshaping task aims atediting it to an aesthetic body shape. As the technology has been widely usedin media, several methods have been proposed mainly focusing on generatingoptical flow to warp the body shape. However, those previous works onlyconsider the local transformation of different body parts (arms, torso, andlegs), ignoring the global affinity, and limiting the capacity to ensureconsistency and quality across the entire body. In this paper, we propose anovel Adaptive Affinity-Graph Network (AAGN), which extracts the globalaffinity between different body parts to enhance the quality of the generatedoptical flow. Specifically, our AAGN primarily introduces the followingdesigns: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leveragesthe characteristic of a fully connected graph. AAG represents different bodyparts as nodes in an adaptive fully connected graph and captures all theaffinities between nodes to obtain a global affinity map. The design couldbetter improve the consistency between body parts. (2) Besides, forhigh-frequency details are crucial for photo aesthetics, a Body ShapeDiscriminator (BSD) is designed to extract information from both high-frequencyand spatial domain. Particularly, an SRM filter is utilized to extracthigh-frequency details, which are combined with spatial features as input tothe BSD. With this design, BSD guides the Flow Generator (FG) to pay attentionto various fine details rather than rigid pixel-level fitting. Extensiveexperiments conducted on the BR-5K dataset demonstrate that our frameworksignificantly enhances the aesthetic appeal of reshaped photos, surpassing allprevious work to achieve state-of-the-art in all evaluation metrics.</description><author>Qiwen Deng, Yangcen Liu, Wen Li, Guoqing Wang</author><pubDate>Mon, 02 Dec 2024 16:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13983v2</guid></item><item><title>ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities</title><link>http://arxiv.org/abs/2409.19839v3</link><description>Forecasts of future events are essential inputs into informeddecision-making. Machine learning (ML) systems have the potential to deliverforecasts at scale, but there is no framework for evaluating the accuracy of MLsystems on a standardized set of forecasting questions. To address this gap, weintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of MLsystems on an automatically generated and regularly updated set of 1,000forecasting questions. To avoid any possibility of data leakage, ForecastBenchis comprised solely of questions about future events that have no known answerat the time of submission. We quantify the capabilities of current ML systemsby collecting forecasts from expert (human) forecasters, the general public,and LLMs on a random subset of questions from the benchmark ($N=200$). WhileLLMs have achieved super-human performance on many benchmarks, they performless well here: expert forecasters outperform the top-performing LLM (p-value$&lt;0.01$). We display system and human scores in a public leaderboard atwww.forecastbench.org.</description><author>Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock</author><pubDate>Mon, 02 Dec 2024 16:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19839v3</guid></item><item><title>Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps</title><link>http://arxiv.org/abs/2406.14539v3</link><description>Diffusion distillation represents a highly promising direction for achievingfaithful text-to-image generation in a few sampling steps. However, despiterecent successes, existing distilled models still do not provide the fullspectrum of diffusion abilities, such as real image inversion, which enablesmany precise image manipulation methods. This work aims to enrich distilledtext-to-image diffusion models with the ability to effectively encode realimages into their latent space. To this end, we introduce invertibleConsistency Distillation (iCD), a generalized consistency distillationframework that facilitates both high-quality image synthesis and accurate imageencoding in only 3-4 inference steps. Though the inversion problem fortext-to-image diffusion models gets exacerbated by high classifier-freeguidance scales, we notice that dynamic guidance significantly reducesreconstruction errors without noticeable degradation in generation performance.As a result, we demonstrate that iCD equipped with dynamic guidance may serveas a highly effective tool for zero-shot text-guided image editing, competingwith more expensive state-of-the-art alternatives.</description><author>Nikita Starodubcev, Mikhail Khoroshikh, Artem Babenko, Dmitry Baranchuk</author><pubDate>Mon, 02 Dec 2024 16:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14539v3</guid></item><item><title>Scaling Speech-Text Pre-training with Synthetic Interleaved Data</title><link>http://arxiv.org/abs/2411.17607v2</link><description>Speech language models (SpeechLMs) accept speech input and produce speechoutput, allowing for more natural human-computer interaction compared totext-based large language models (LLMs). Traditional approaches for developingSpeechLMs are constrained by the limited availability of unsupervised speechdata and parallel speech-text data, which are significantly less abundant thantext pre-training data, thereby limiting their scalability as LLMs. We proposea novel approach to scaling speech-text pre-training by leveraging large-scalesynthetic interleaved data derived from text corpora, eliminating the need forparallel speech-text datasets. Our method efficiently constructs speech-textinterleaved data by sampling text spans from existing text corpora andsynthesizing corresponding speech spans using a text-to-token model, bypassingthe need to generate actual speech. We also employ a supervised speechtokenizer derived from an automatic speech recognition (ASR) model byincorporating a vector-quantized bottleneck into the encoder. This supervisedtraining approach results in discrete speech tokens with strong semanticpreservation even at lower frame rates (e.g. 12.5Hz), while still maintainingspeech reconstruction quality. Starting from a pre-trained language model andscaling our pre-training to 1 trillion tokens (with 600B synthetic interleavedspeech-text data), we achieve state-of-the-art performance in speech languagemodeling and spoken question answering, improving performance on spokenquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We furtherdemonstrate that by fine-tuning the pre-trained model with speech dialoguedata, we can develop an end-to-end spoken chatbot that achieves competitiveperformance comparable to existing baselines in both conversational abilitiesand speech quality, even operating exclusively in the speech domain.</description><author>Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang</author><pubDate>Mon, 02 Dec 2024 16:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17607v2</guid></item><item><title>Physics-Informed Real NVP for Satellite Power System Fault Detection</title><link>http://arxiv.org/abs/2405.17339v2</link><description>The unique challenges posed by the space environment, characterized byextreme conditions and limited accessibility, raise the need for robust andreliable techniques to identify and prevent satellite faults. Fault detectionmethods in the space sector are required to ensure mission success and toprotect valuable assets. In this context, this paper proposes an ArtificialIntelligence (AI) based fault detection methodology and evaluates itsperformance on ADAPT (Advanced Diagnostics and Prognostics Testbed), anElectrical Power System (EPS) dataset, crafted in laboratory by NASA. Our studyfocuses on the application of a physics-informed (PI) real-valued non-volumepreserving (Real NVP) model for fault detection in space systems. The efficacyof this method is systematically compared against other AI approaches such asGated Recurrent Unit (GRU) and Autoencoder-based techniques. Results show thatour physics-informed approach outperforms existing methods of fault detection,demonstrating its suitability for addressing the unique challenges of satelliteEPS sub-system faults. Furthermore, we unveil the competitive advantage ofphysics-informed loss in AI models to address specific space needs, namelyrobustness, reliability, and power constraints, crucial for space explorationand satellite missions.</description><author>Carlo Cena, Umberto Albertin, Mauro Martini, Silvia Bucci, Marcello Chiaberge</author><pubDate>Mon, 02 Dec 2024 16:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17339v2</guid></item><item><title>A Self-Supervised Task for Fault Detection in Satellite Multivariate Time Series</title><link>http://arxiv.org/abs/2407.02861v2</link><description>In the space sector, due to environmental conditions and restrictedaccessibility, robust fault detection methods are imperative for ensuringmission success and safeguarding valuable assets. This work proposes a novelapproach leveraging Physics-Informed Real NVP neural networks, renowned fortheir ability to model complex and high-dimensional distributions, augmentedwith a self-supervised task based on sensors' data permutation. It focuses onenhancing fault detection within the satellite multivariate time series. Theexperiments involve various configurations, including pre-training withself-supervision, multi-task learning, and standalone self-supervised training.Results indicate significant performance improvements across all settings. Inparticular, employing only the self-supervised loss yields the best overallresults, suggesting its efficacy in guiding the network to extract relevantfeatures for fault detection. This study presents a promising direction forimproving fault detection in space systems and warrants further exploration inother datasets and applications.</description><author>Carlo Cena, Silvia Bucci, Alessandro Balossino, Marcello Chiaberge</author><pubDate>Mon, 02 Dec 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02861v2</guid></item><item><title>Brain Tumour Removing and Missing Modality Generation using 3D WDM</title><link>http://arxiv.org/abs/2411.04630v2</link><description>This paper presents the second-placed solution for task 8 and theparticipation solution for task 7 of BraTS 2024. The adoption of automatedbrain analysis algorithms to support clinical practice is increasing. However,many of these algorithms struggle with the presence of brain lesions or theabsence of certain MRI modalities. The alterations in the brain's morphologyleads to high variability and thus poor performance of predictive models thatwere trained only on healthy brains. The lack of information that is usuallyprovided by some of the missing MRI modalities also reduces the reliability ofthe prediction models trained with all modalities. In order to improve theperformance of these models, we propose the use of conditional 3D waveletdiffusion models. The wavelet transform enabled full-resolution image trainingand prediction on a GPU with 48 GB VRAM, without patching or downsampling,preserving all information for prediction. The code for these tasks isavailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.</description><author>André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger</author><pubDate>Mon, 02 Dec 2024 15:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04630v2</guid></item><item><title>Limits to Predicting Online Speech Using Large Language Models</title><link>http://arxiv.org/abs/2407.12850v2</link><description>We study the predictability of online speech on social media, and whetherpredictability improves with information outside a user's own posts. Recenttheoretical results suggest that posts from a user's social circle are aspredictive of the user's future posts as that of the user's past posts.Motivated by the success of large language models, we empirically test thishypothesis. We define predictability as a measure of the model's uncertainty,i.e., its negative log-likelihood on future tokens given context. As the basisof our study, we collect 10M tweets for ``tweet-tuning'' base models and afurther 6.25M posts from more than five thousand X (previously Twitter) usersand their peers. Across four large language models ranging in size from 1.5billion to 70 billion parameters, we find that predicting a user's posts fromtheir peers' posts performs poorly. Moreover, the value of the user's own postsfor prediction is consistently higher than that of their peers'. We extend ourinvestigation with a detailed analysis on what's learned in-context and therobustness of our findings. From context, base models learn to correctlypredict @-mentions and hashtags. Moreover, our results replicate if instead ofprompting the model with additional context, we finetune on it. Across theboard, we find that predicting the posts of individual users remains hard.</description><author>Mina Remeli, Moritz Hardt, Robert C. Williamson</author><pubDate>Mon, 02 Dec 2024 15:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12850v2</guid></item><item><title>From Text to Insight: Large Language Models for Materials Science Data Extraction</title><link>http://arxiv.org/abs/2407.16867v2</link><description>The vast majority of materials science knowledge exists in unstructurednatural language, yet structured data is crucial for innovative and systematicmaterials design. Traditionally, the field has relied on manual curation andpartial automation for data extraction for specific use cases. The advent oflarge language models (LLMs) represents a significant shift, potentiallyenabling efficient extraction of structured, actionable data from unstructuredtext by non-experts. While applying LLMs to materials science data extractionpresents unique challenges, domain knowledge offers opportunities to guide andvalidate LLM outputs. This review provides a comprehensive overview ofLLM-based structured data extraction in materials science, synthesizing currentknowledge and outlining future directions. We address the lack of standardizedguidelines and present frameworks for leveraging the synergy between LLMs andmaterials science expertise. This work serves as a foundational resource forresearchers aiming to harness LLMs for data-driven materials research. Theinsights presented here could significantly enhance how researchers acrossdisciplines access and utilize scientific information, potentially acceleratingthe development of novel materials for critical societal needs.</description><author>Mara Schilling-Wilhelmi, Martiño Ríos-García, Sherjeel Shabih, María Victoria Gil, Santiago Miret, Christoph T. Koch, José A. Márquez, Kevin Maik Jablonka</author><pubDate>Mon, 02 Dec 2024 15:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16867v2</guid></item><item><title>Reviewer2: Optimizing Review Generation Through Prompt Generation</title><link>http://arxiv.org/abs/2402.10886v2</link><description>Recent developments in LLMs offer new opportunities for assisting authors inimproving their work. In this paper, we envision a use case where authors canreceive LLM-generated reviews that uncover weak points in the current draft.While initial methods for automated review generation already exist, thesemethods tend to produce reviews that lack detail, and they do not cover therange of opinions that human reviewers produce. To address this shortcoming, wepropose an efficient two-stage review generation framework called Reviewer2.Unlike prior work, this approach explicitly models the distribution of possibleaspects that the review may address. We show that this leads to more detailedreviews that better cover the range of aspects that human reviewers identify inthe draft. As part of the research, we generate a large-scale review dataset of27k papers and 99k reviews that we annotate with aspect prompts, which we makeavailable as a resource for future research.</description><author>Zhaolin Gao, Kianté Brantley, Thorsten Joachims</author><pubDate>Mon, 02 Dec 2024 15:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10886v2</guid></item><item><title>On Meta-Prompting</title><link>http://arxiv.org/abs/2312.06562v2</link><description>Modern generative language models are capable of interpreting input stringsas instructions, or prompts, and carry out tasks based on them. Many approachesto prompting and pre-training these models involve the automated generation ofthese prompts: meta-prompting, or prompting to obtain prompts. We propose atheoretical framework based on category theory to generalize and describe them.This framework is flexible enough to account for stochasticity, and allows usto obtain formal results around task agnosticity and equivalence of variousmeta-prompting approaches. Experimentally, we test our framework in two activeareas of model research: creativity and ideation. We find that user preferencestrongly favors (p &lt; 0.01) the prompts generated under meta-prompting, as wellas their corresponding outputs, over a series of hardcoded baseline promptsthat include the original task definition. Using our framework, we argue thatmeta-prompting is more effective than basic prompting at generating desirableoutputs.</description><author>Adrian de Wynter, Xun Wang, Qilong Gu, Si-Qing Chen</author><pubDate>Mon, 02 Dec 2024 15:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06562v2</guid></item><item><title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title><link>http://arxiv.org/abs/2407.01036v2</link><description>A/B testers that conduct large-scale tests often prioritize lifts as the mainoutcome metric and want to be able to control costs resulting from falserejections of the null. This work develops a decision-theoretic framework formaximizing profits subject to false discovery rate (FDR) control. We build anempirical Bayes solution for the problem via a greedy knapsack approach. Wederive an oracle rule based on ranking the ratio of expected lifts and the costof wrong rejections using the local false discovery rate (lfdr) statistic. Ouroracle decision rule is valid and optimal for large-scale tests. Further, weestablish asymptotic validity for the data-driven procedure and demonstratefinite-sample validity in experimental studies. We also demonstrate the meritof the proposed method over other FDR control methods. Finally, we discuss anapplication to data collected by experiments on the Optimizely platform.</description><author>Pallavi Basu, Ron Berman</author><pubDate>Mon, 02 Dec 2024 15:31:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01036v2</guid></item><item><title>Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation, Embrace Orthogonality</title><link>http://arxiv.org/abs/2411.08085v2</link><description>We introduce a yat-product-powered neural network, the Neural Matter Network(NMN), a breakthrough in deep learning that achieves non-linear patternrecognition without activation functions. Our key innovation relies on theyat-product and yat-product, which naturally induces non-linearity byprojecting inputs into a pseudo-metric space, eliminating the need fortraditional activation functions while maintaining only a softmax layer forfinal class probability distribution. This approach simplifies networkarchitecture and provides unprecedented transparency into the network'sdecision-making process. Our comprehensive empirical evaluation acrossdifferent datasets demonstrates that NMN consistently outperforms traditionalMLPs. The results challenge the assumption that separate activation functionsare necessary for effective deep-learning models. The implications of this workextend beyond immediate architectural benefits, by eliminating intermediateactivation functions while preserving non-linear capabilities, yat-MLPestablishes a new paradigm for neural network design that combines simplicitywith effectiveness. Most importantly, our approach provides unprecedentedinsights into the traditionally opaque "black-box" nature of neural networks,offering a clearer understanding of how these models process and classifyinformation.</description><author>Taha Bouhsine</author><pubDate>Mon, 02 Dec 2024 15:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08085v2</guid></item><item><title>Latent Diffusion for Neural Spiking Data</title><link>http://arxiv.org/abs/2407.08751v2</link><description>Modern datasets in neuroscience enable unprecedented inquiries into therelationship between complex behaviors and the activity of many simultaneouslyrecorded neurons. While latent variable models can successfully extractlow-dimensional embeddings from such recordings, using them to generaterealistic spiking data, especially in a behavior-dependent manner, still posesa challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),a diffusion-based generative model with a low-dimensional latent space: LDNSemploys an autoencoder with structured state-space (S4) layers to projectdiscrete high-dimensional spiking data into continuous time-aligned latents. Onthese inferred latents, we train expressive (conditional) diffusion models,enabling us to sample neural activity with realistic single-neuron andpopulation spiking statistics. We validate LDNS on synthetic data, accuratelyrecovering latent structure, firing rates, and spiking statistics. Next, wedemonstrate its flexibility by generating variable-length data that mimicshuman cortical activity during attempted speech. We show how to equip LDNS withan expressive observation model that accounts for single-neuron dynamics notmediated by the latent state, further increasing the realism of generatedsamples. Finally, conditional LDNS trained on motor cortical activity duringdiverse reaching behaviors can generate realistic spiking data given reachdirection or unseen reach trajectories. In summary, LDNS simultaneously enablesinference of low-dimensional latents and realistic conditional generation ofneural spiking datasets, opening up further possibilities for simulatingexperimentally testable hypotheses.</description><author>Jaivardhan Kapoor, Auguste Schulz, Julius Vetter, Felix Pei, Richard Gao, Jakob H. Macke</author><pubDate>Mon, 02 Dec 2024 15:16:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08751v2</guid></item><item><title>VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning</title><link>http://arxiv.org/abs/2409.13730v2</link><description>Multi-modal large language models (MLLMs) have demonstrated promisingcapabilities across various tasks by integrating textual and visual informationto achieve visual understanding in complex scenarios. Despite the availabilityof several benchmarks aims to evaluating MLLMs in tasks from visual questionanswering to complex problem-solving, most focus predominantly on mathematicsor general visual understanding tasks. This reveals a critical gap in currentbenchmarks, which often overlook the inclusion of other key scientificdisciplines such as physics and chemistry. To address this gap, we meticulouslyconstruct a comprehensive benchmark, named VisScience, which is utilized toassess the multi-modal scientific reasoning across the three disciplines ofmathematics, physics, and chemistry. This benchmark comprises 3,000 questionsdrawn from K12 education - spanning elementary school through high school -equally distributed across three disciplines, with 1,000 questions perdiscipline. The questions within VisScience span 21 distinct subjects and arecategorized into five difficulty levels, offering a broad spectrum of topicswithin each discipline. With VisScience, we present a detailed evaluation ofthe performance of 25 representative MLLMs in scientific reasoning.Experimental results demonstrate that closed-source MLLMs generally outperformopen-source models. The best performance observed include a 53.4\% accuracy inmathematics by Claude3.5-Sonnet, 38.2\% in physics by GPT-4o, and 47.0\% inchemistry by Gemini-1.5-Pro. These results underscore the strengths andlimitations of MLLMs, suggesting areas for future improvement and highlightingthe importance of developing models that can effectively handle the diversedemands of multi-modal scientific reasoning.</description><author>Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Jie Tang</author><pubDate>Mon, 02 Dec 2024 15:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13730v2</guid></item><item><title>Deepfake for the Good: Generating Avatars through Face-Swapping with Implicit Deepfake Generation</title><link>http://arxiv.org/abs/2402.06390v2</link><description>Numerous emerging deep-learning techniques have had a substantial impact oncomputer graphics. Among the most promising breakthroughs are the rise ofNeural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode theobject's shape and color in neural network weights using a handful of imageswith known camera positions to generate novel views. In contrast, GS providesaccelerated training and inference without a decrease in rendering quality byencoding the object's characteristics in a collection of Gaussiandistributions. These two techniques have found many use cases in spatialcomputing and other domains. On the other hand, the emergence of deepfakemethods has sparked considerable controversy. Deepfakes refers to artificialintelligence-generated videos that closely mimic authentic footage. Usinggenerative models, they can modify facial features, enabling the creation ofaltered identities or expressions that exhibit a remarkably realisticappearance to a real person. Despite these controversies, deepfake can offer anext-generation solution for avatar creation and gaming when of desirablequality. To that end, we show how to combine all these emerging technologies toobtain a more plausible outcome. Our ImplicitDeepfake uses the classicaldeepfake algorithm to modify all training images separately and then train NeRFand GS on modified faces. Such simple strategies can produce plausible 3Ddeepfake-based avatars.</description><author>Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</author><pubDate>Mon, 02 Dec 2024 15:04:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06390v2</guid></item><item><title>MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model</title><link>http://arxiv.org/abs/2409.13729v2</link><description>Large language models (LLMs) have demonstrated significant capabilities inmathematical reasoning, particularly with text-based mathematical problems.However, current multi-modal large language models (MLLMs), especially thosespecialized in mathematics, tend to focus predominantly on solving geometricproblems but ignore the diversity of visual information available in otherareas of mathematics. Moreover, the geometric information for these specializedmathematical MLLMs is derived from several public datasets, which are typicallylimited in diversity and complexity. To address these limitations, we aim toconstruct a fine-tuning dataset named MathVL, and develop a series ofspecialized mathematical MLLMs termed MathGLM-Vision by conducting SupervisedFine-Tuning (SFT) on MathVL with various parameter-scale backbones. Toextensively evaluate the effectiveness of MathGLM-Vision, we conductexperiments on several public benchmarks and our curated MathVL-test consistingof 2,000 problems. Experimental results demonstrate that MathGLM-Visionachieves significant improvements compared with some existing models, includingbackbone models and open-source mathematical MLLMs. These findings indicate theimportance of diversity dataset in enhancing the mathematical reasoningabilities of MLLMs.</description><author>Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Jie Tang</author><pubDate>Mon, 02 Dec 2024 14:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13729v2</guid></item><item><title>Efficient Multi-modal Large Language Models via Visual Token Grouping</title><link>http://arxiv.org/abs/2411.17773v2</link><description>The development of Multi-modal Large Language Models (MLLMs) enhances LargeLanguage Models (LLMs) with the ability to perceive data formats beyond text,significantly advancing a range of downstream applications, such as visualquestion answering and image captioning. However, the substantial computationalcosts associated with processing high-resolution images and videos pose abarrier to their broader adoption. To address this challenge, compressingvision tokens in MLLMs has emerged as a promising approach to reduce inferencecosts. While existing methods conduct token reduction in the feature alignmentphase. In this paper, we introduce VisToG, a novel grouping mechanism thatleverages the capabilities of pre-trained vision encoders to group similarimage segments without the need for segmentation masks. Specifically, weconcatenate semantic tokens to represent image semantic segments after thelinear projection layer before feeding into the vision encoder. Besides, withthe isolated attention we adopt, VisToG can identify and eliminate redundantvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,which effectively reduces computational demands. Extensive experimentsdemonstrate the effectiveness of VisToG, maintaining 98.1% of the originalperformance while achieving a reduction of over 27\% inference time.</description><author>Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li, Hong Cheng</author><pubDate>Mon, 02 Dec 2024 14:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17773v2</guid></item><item><title>Continual Learning in the Presence of Repetition</title><link>http://arxiv.org/abs/2405.04101v2</link><description>Continual learning (CL) provides a framework for training models inever-evolving environments. Although re-occurrence of previously seen objectsor tasks is common in real-world problems, the concept of repetition in thedata stream is not often considered in standard benchmarks for CL. Unlike withthe rehearsal mechanism in buffer-based strategies, where sample repetition iscontrolled by the strategy, repetition in the data stream naturally stems fromthe environment. This report provides a summary of the CLVision challenge atCVPR 2023, which focused on the topic of repetition in class-incrementallearning. The report initially outlines the challenge objective and thendescribes three solutions proposed by finalist teams that aim to effectivelyexploit the repetition in the stream to learn continually. The experimentalresults from the challenge highlight the effectiveness of ensemble-basedsolutions that employ multiple versions of similar modules, each trained ondifferent but overlapping subsets of classes. This report underscores thetransformative potential of taking a different perspective in CL by employingrepetition in the data stream to foster innovative strategy design.</description><author>Hamed Hemati, Lorenzo Pellegrini, Xiaotian Duan, Zixuan Zhao, Fangfang Xia, Marc Masana, Benedikt Tscheschner, Eduardo Veas, Yuxiang Zheng, Shiji Zhao, Shao-Yuan Li, Sheng-Jun Huang, Vincenzo Lomonaco, Gido M. van de Ven</author><pubDate>Mon, 02 Dec 2024 14:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04101v2</guid></item><item><title>GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models</title><link>http://arxiv.org/abs/2410.06154v2</link><description>In this work, we propose a novel method (GLOV) enabling Large Language Models(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) toenhance downstream vision tasks. Our GLOV meta-prompts an LLM with thedownstream task description, querying it for suitable VLM prompts (e.g., forzero-shot classification with CLIP). These prompts are ranked according to apurity measure obtained through a fitness function. In each respectiveoptimization step, the ranked prompts are fed as in-context examples (withtheir accuracies) to equip the LLM with the knowledge of the type of textprompts preferred by the downstream VLM. Furthermore, we also explicitly steerthe LLM generation process in each optimization step by specifically adding anoffset difference vector of the embeddings from the positive and negativesolutions found by the LLM, in previous optimization steps, to the intermediatelayer of the network for the next generation step. This offset vector steersthe LLM generation toward the type of language preferred by the downstream VLM,resulting in enhanced performance on the downstream vision tasks. Wecomprehensively evaluate our GLOV on 16 diverse datasets using two families ofVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models-- showing that the discovered solutions can enhance the recognitionperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for thesemodels.</description><author>M. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogerio Feris, Leonid Karlinsky, James Glass</author><pubDate>Mon, 02 Dec 2024 14:49:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06154v2</guid></item><item><title>Fair Generalized Linear Mixed Models</title><link>http://arxiv.org/abs/2405.09273v7</link><description>When using machine learning for automated prediction, it is important toaccount for fairness in the prediction. Fairness in machine learning aims toensure that biases in the data and model inaccuracies do not lead todiscriminatory decisions. E.g., predictions from fair machine learning modelsshould not discriminate against sensitive variables such as sexual orientationand ethnicity. The training data often in obtained from social surveys. Insocial surveys, oftentimes the data collection process is a strata sampling,e.g. due to cost restrictions. In strata samples, the assumption ofindependence between the observation is not fulfilled. Hence, if the machinelearning models do not account for the strata correlations, the results may bebiased. Especially high is the bias in cases where the strata assignment iscorrelated to the variable of interest. We present in this paper an algorithmthat can handle both problems simultaneously, and we demonstrate the impact ofstratified sampling on the quality of fair machine learning predictions in areproducible simulation study.</description><author>Jan Pablo Burgard, João Vitor Pamplona</author><pubDate>Mon, 02 Dec 2024 14:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09273v7</guid></item><item><title>Fair Mixed Effects Support Vector Machine</title><link>http://arxiv.org/abs/2405.06433v6</link><description>To ensure unbiased and ethical automated predictions, fairness must be a coreprinciple in machine learning applications. Fairness in machine learning aimsto mitigate biases present in the training data and model imperfections thatcould lead to discriminatory outcomes. This is achieved by preventing the modelfrom making decisions based on sensitive characteristics like ethnicity orsexual orientation. A fundamental assumption in machine learning is theindependence of observations. However, this assumption often does not hold truefor data describing social phenomena, where data points are often clusteredbased. Hence, if the machine learning models do not account for the clustercorrelations, the results may be biased. Especially high is the bias in caseswhere the cluster assignment is correlated to the variable of interest. Wepresent a fair mixed effects support vector machine algorithm that can handleboth problems simultaneously. With a reproducible simulation study wedemonstrate the impact of clustered data on the quality of fair machinelearning predictions.</description><author>Jan Pablo Burgard, João Vitor Pamplona</author><pubDate>Mon, 02 Dec 2024 14:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06433v6</guid></item><item><title>ProtFAD: Introducing function-aware domains as implicit modality towards protein function prediction</title><link>http://arxiv.org/abs/2405.15158v2</link><description>Protein function prediction is currently achieved by encoding its sequence orstructure, where the sequence-to-function transcendence and high-qualitystructural data scarcity lead to obvious performance bottlenecks. Proteindomains are "building blocks" of proteins that are functionally independent,and their combinations determine the diverse biological functions. However,most existing studies have yet to thoroughly explore the intricate functionalinformation contained in the protein domains. To fill this gap, we propose asynergistic integration approach for a function-aware domain representation,and a domain-joint contrastive learning strategy to distinguish differentprotein functions while aligning the modalities. Specifically, we align thedomain semantics with GO terms and text description to pre-train domainembeddings. Furthermore, we partition proteins into multiple sub-views based oncontinuous joint domains for contrastive training under the supervision of anovel triplet InfoNCE loss. Our approach significantly and comprehensivelyoutperforms the state-of-the-art methods on various benchmarks, and clearlydifferentiates proteins carrying distinct functions compared to the competitor.Our implementation is available athttps://github.com/AI-HPC-Research-Team/ProtFAD.</description><author>Mingqing Wang, Zhiwei Nie, Yonghong He, Athanasios V. Vasilakos, Zhixiang Ren</author><pubDate>Mon, 02 Dec 2024 14:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15158v2</guid></item><item><title>Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing to Improve Segmentation Ability</title><link>http://arxiv.org/abs/2411.01819v2</link><description>Current semantic segmentation models typically require a substantial amountof manually annotated data, a process that is both time-consuming andresource-intensive. Alternatively, leveraging advanced text-to-image modelssuch as Midjourney and Stable Diffusion has emerged as an efficient strategy,enabling the automatic generation of synthetic data in place of manualannotations. However, previous methods have been limited to generatingsingle-instance images, as the generation of multiple instances with StableDiffusion has proven unstable. To address this limitation and expand the scopeand diversity of synthetic datasets, we propose a framework \textbf{Free-Mask}that combines a Diffusion Model for segmentation with advanced image editingcapabilities, allowing for the integration of multiple objects into images viatext-to-image models. Our method facilitates the creation of highly realisticdatasets that closely emulate open-world environments while generating accuratesegmentation masks. It reduces the labor associated with manual annotation andalso ensures precise mask generation. Experimental results demonstrate thatsynthetic data generated by \textbf{Free-Mask} enables segmentation models tooutperform those trained on real data, especially in zero-shot settings.Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previouslyunseen classes in the VOC 2012 benchmark.</description><author>Bo Gao, Fangxu Xing, Daniel Tang</author><pubDate>Mon, 02 Dec 2024 14:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01819v2</guid></item><item><title>InvDesFlow: An AI search engine to explore possible high-temperature superconductors</title><link>http://arxiv.org/abs/2409.08065v2</link><description>The discovery of new superconducting materials, particularly those exhibitinghigh critical temperature ($T_c$), has been a vibrant area of study within thefield of condensed matter physics. Conventional approaches primarily rely onphysical intuition to search for potential superconductors within the existingdatabases. However, the known materials only scratch the surface of theextensive array of possibilities within the realm of materials. Here, wedevelop InvDesFlow, an AI search engine that integrates deep model pre-trainingand fine-tuning techniques, diffusion models, and physics-based approaches(e.g., first-principles electronic structure calculation) for the discovery ofhigh-$T_c$ superconductors. Utilizing InvDesFlow, we have obtained 74dynamically stable materials with critical temperatures predicted by the AImodel to be $T_c \geq$ 15 K based on a very small set of samples. Notably,these materials are not contained in any existing dataset. Furthermore, weanalyze trends in our dataset and individual materials including B$_4$CN$_3$(at 5 GPa) and B$_5$CN$_2$ (at ambient pressure) whose $T_c$s are 24.08 K and15.93 K, respectively. We demonstrate that AI technique can discover a set ofnew high-$T_c$ superconductors, outline its potential for acceleratingdiscovery of the materials with targeted properties.</description><author>Xiao-Qi Han, Zhenfeng Ouyang, Peng-Jie Guo, Hao Sun, Ze-Feng Gao, Zhong-Yi Lu</author><pubDate>Mon, 02 Dec 2024 14:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08065v2</guid></item><item><title>Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis</title><link>http://arxiv.org/abs/2411.19655v2</link><description>After the introduction of Large Language Models (LLMs), there have beensubstantial improvements in the performance of Natural Language Generation(NLG) tasks, including Text Summarization and Machine Translation. However,LLMs still produce outputs containing hallucinations, that is, content notgrounded in factual information. Therefore, developing methods to assess thefactuality of LLMs has become urgent. Indeed, resources for factuality evaluation have recently emerged. Althoughchallenging, these resources face one or more of the following limitations: (i)they are tailored to a specific task or domain; (ii) they are limited in size,thereby preventing the training of new factuality evaluators; (iii) they aredesigned for simpler verification tasks, such as claim verification. To address these issues, we introduce LLM-Oasis, to the best of our knowledgethe largest resource for training end-to-end factuality evaluators. LLM-Oasisis constructed by extracting claims from Wikipedia, falsifying a subset ofthese claims, and generating pairs of factual and unfactual texts. We then relyon human annotators to both validate the quality of our dataset and to create agold standard test set for benchmarking factuality evaluation systems. Our experiments demonstrate that LLM-Oasis presents a significant challengefor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in ourproposed end-to-end factuality evaluation task, highlighting its potential todrive future research in the field.</description><author>Alessandro Scirè, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli</author><pubDate>Mon, 02 Dec 2024 14:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19655v2</guid></item><item><title>Anomaly Detection in Medical Imaging -- A Mini Review</title><link>http://arxiv.org/abs/2108.11986v2</link><description>The increasing digitization of medical imaging enables machine learning basedimprovements in detecting, visualizing and segmenting lesions, easing theworkload for medical experts. However, supervised machine learning requiresreliable labelled data, which is is often difficult or impossible to collect orat least time consuming and thereby costly. Therefore methods requiring onlypartly labeled data (semi-supervised) or no labeling at all (unsupervisedmethods) have been applied more regularly. Anomaly detection is one possiblemethodology that is able to leverage semi-supervised and unsupervised methodsto handle medical imaging tasks like classification and segmentation. Thispaper uses a semi-exhaustive literature review of relevant anomaly detectionpapers in medical imaging to cluster into applications, highlight importantresults, establish lessons learned and give further advice on how to approachanomaly detection in medical imaging. The qualitative analysis is based ongoogle scholar and 4 different search terms, resulting in 120 differentanalysed papers. The main results showed that the current research is mostlymotivated by reducing the need for labelled data. Also, the successful andsubstantial amount of research in the brain MRI domain shows the potential forapplications in further domains like OCT and chest X-ray.</description><author>Maximilian E. Tschuchnig, Michael Gadermayr</author><pubDate>Mon, 02 Dec 2024 14:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11986v2</guid></item><item><title>Moral Alignment for LLM Agents</title><link>http://arxiv.org/abs/2410.01639v2</link><description>Decision-making agents based on pre-trained Large Language Models (LLMs) areincreasingly being deployed across various domains of human activity. Whiletheir applications are currently rather specialized, several research effortsare under way to develop more generalist agents. As LLM-based systems becomemore agentic, their influence on human activity will grow and the transparencyof this will decrease. Consequently, developing effective methods for aligningthem to human values is vital. The prevailing practice in alignment often relies on human preference data(e.g., in RLHF or DPO), in which values are implicit and are essentiallydeduced from relative preferences over different model outputs. In this work,instead of relying on human feedback, we introduce the design of rewardfunctions that explicitly encode core human values for ReinforcementLearning-based fine-tuning of foundation agent models. Specifically, we useintrinsic rewards for the moral alignment of LLM agents. We evaluate our approach using the traditional philosophical frameworks ofDeontological Ethics and Utilitarianism, quantifying moral rewards for agentsin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)environment. We also show how moral fine-tuning can be deployed to enable anagent to unlearn a previously developed selfish strategy. Finally, we find thatcertain moral strategies learned on the IPD game generalize to several othermatrix game environments. In summary, we demonstrate that fine-tuning withintrinsic rewards is a promising general solution for aligning LLM agents tohuman values, and it might represent a more transparent and cost-effectivealternative to currently predominant alignment techniques.</description><author>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</author><pubDate>Mon, 02 Dec 2024 14:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01639v2</guid></item><item><title>Constraining Generative Models for Engineering Design with Negative Data</title><link>http://arxiv.org/abs/2306.15166v2</link><description>Generative models have recently achieved remarkable success and widespreadadoption in society, yet they often struggle to generate realistic and accurateoutputs. This challenge extends beyond language and vision into fields likeengineering design, where safety-critical engineering standards andnon-negotiable physical laws tightly constrain what outputs are consideredacceptable. In this work, we introduce a novel training method to guide agenerative model toward constraint-satisfying outputs using `negative data' --examples of what to avoid. Our negative-data generative model (NDGM)formulation easily outperforms classic models, generating 1/6 as manyconstraint-violating samples using 1/8 as much data in certain problems. Italso consistently outperforms other baselines, achieving a balance betweenconstraint satisfaction and distributional similarity that is unsurpassed byany other model in 12 of the 14 problems tested. This widespread superiority isrigorously demonstrated across numerous synthetic tests and real engineeringproblems, such as ship hull synthesis with hydrodynamic constraints and vehicledesign with impact safety constraints. Our benchmarks showcase both thebest-in-class performance of our new NDGM formulation and the overall dominanceof NDGMs versus classic generative models. We publicly release the code andbenchmarks at https://github.com/Lyleregenwetter/NDGMs.</description><author>Lyle Regenwetter, Giorgio Giannone, Akash Srivastava, Dan Gutfreund, Faez Ahmed</author><pubDate>Mon, 02 Dec 2024 14:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15166v2</guid></item><item><title>Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics</title><link>http://arxiv.org/abs/2301.13306v6</link><description>We study a game between autobidding algorithms that compete in an onlineadvertising platform. Each autobidder is tasked with maximizing itsadvertiser's total value over multiple rounds of a repeated auction, subject tobudget and return-on-investment constraints. We propose a gradient-basedlearning algorithm that is guaranteed to satisfy all constraints and achievesvanishing individual regret. Our algorithm uses only bandit feedback and can beused with the first- or second-price auction, as well as with any"intermediate" auction format. Our main result is that when these autobiddersplay against each other, the resulting expected liquid welfare over all roundsis at least half of the expected optimal liquid welfare achieved by anyallocation. This holds whether or not the bidding dynamics converges to anequilibrium.</description><author>Brendan Lucier, Sarath Pattathil, Aleksandrs Slivkins, Mengxiao Zhang</author><pubDate>Mon, 02 Dec 2024 14:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13306v6</guid></item><item><title>Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid Cancer Classification</title><link>http://arxiv.org/abs/2204.10942v2</link><description>Thyroid cancer is currently the fifth most common malignancy diagnosed inwomen. Since differentiation of cancer sub-types is important for treatment andcurrent, manual methods are time consuming and subjective, automaticcomputer-aided differentiation of cancer types is crucial. Manualdifferentiation of thyroid cancer is based on tissue sections, analysed bypathologists using histological features. Due to the enormous size of gigapixelwhole slide images, holistic classification using deep learning methods is notfeasible. Patch based multiple instance learning approaches, combined withaggregations such as bag-of-words, is a common approach. This work'scontribution is to extend a patch based state-of-the-art method by generatingand combining feature vectors of three different patch resolutions andanalysing three distinct ways of combining them. The results showedimprovements in one of the three multi-scale approaches, while the others ledto decreased scores. This provides motivation for analysis and discussion ofthe individual approaches.</description><author>Maximilian E. Tschuchnig, Philipp Grubmüller, Lea M. Stangassinger, Christina Kreutzer, Sébastien Couillard-Després, Gertie J. Oostingh, Anton Hittmair, Michael Gadermayr</author><pubDate>Mon, 02 Dec 2024 14:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.10942v2</guid></item><item><title>Sample Complexity Bounds for Linear System Identification from a Finite Set</title><link>http://arxiv.org/abs/2409.11141v2</link><description>This paper considers a finite sample perspective on the problem ofidentifying an LTI system from a finite set of possible systems usingtrajectory data. To this end, we use the maximum likelihood estimator toidentify the true system and provide an upper bound for its sample complexity.Crucially, the derived bound does not rely on a potentially restrictivestability assumption. Additionally, we leverage tools from information theoryto provide a lower bound to the sample complexity that holds independently ofthe used estimator. The derived sample complexity bounds are analyzedanalytically and numerically.</description><author>Nicolas Chatzikiriakos, Andrea Iannelli</author><pubDate>Mon, 02 Dec 2024 14:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11141v2</guid></item><item><title>Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels</title><link>http://arxiv.org/abs/2411.12440v3</link><description>Recent advancements in 3D Gaussian Splatting (3DGS) have substantiallyimproved novel view synthesis, enabling high-quality reconstruction andreal-time rendering. However, blurring artifacts, such as floating primitivesand over-reconstruction, remain challenging. Current methods address theseissues by refining scene structure, enhancing geometric representations,addressing blur in training images, improving rendering consistency, andoptimizing density control, yet the role of kernel design remainsunderexplored. We identify the soft boundaries of Gaussian ellipsoids as one ofthe causes of these artifacts, limiting detail capture in high-frequencyregions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), whichreplaces Gaussian kernels with linear kernels to achieve sharper and moreprecise results, particularly in high-frequency regions. Through evaluations onthree datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, alongwith a 30% FPS improvement over baseline 3DGS. The implementation will be madepublicly available upon acceptance.</description><author>Haodong Chen, Runnan Chen, Qiang Qu, Zhaoqing Wang, Tongliang Liu, Xiaoming Chen, Yuk Ying Chung</author><pubDate>Mon, 02 Dec 2024 13:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12440v3</guid></item><item><title>Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using Image Reconstruction</title><link>http://arxiv.org/abs/2312.12990v2</link><description>Semantic segmentation is a crucial task in medical image processing,essential for segmenting organs or lesions such as tumors. In this study we aimto improve automated segmentation in CBCTs through multi-task learning. Toevaluate effects on different volume qualities, a CBCT dataset is synthesisedfrom the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improvesegmentation, two approaches are investigated. First, we perform multi-tasklearning to add morphology based regularization through a volume reconstructiontask. Second, we use this reconstruction task to reconstruct the best qualityCBCT (most similar to the original CT), facilitating denoising effects. Weexplore both holistic and patch-based approaches. Our findings reveal that,especially using a patch-based approach, multi-task learning improvessegmentation in most cases and that these results can further be improved byour denoising approach.</description><author>Maximilian Ernst Tschuchnig, Julia Coste-Marin, Philipp Steininger, Michael Gadermayr</author><pubDate>Mon, 02 Dec 2024 13:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12990v2</guid></item><item><title>GRDD: A Dataset for Greek Dialectal NLP</title><link>http://arxiv.org/abs/2308.00802v4</link><description>In this paper, we present a dataset for the computational study of a numberof Modern Greek dialects. It consists of raw text data from four dialects ofModern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset isof considerable size, albeit imbalanced, and presents the first attempt tocreate large scale dialectal resources of this type for Modern Greek dialects.We then use the dataset to perform dialect idefntification. We experiment withtraditional ML algorithms, as well as simple DL architectures. The results showvery good performance on the task, potentially revealing that the dialects inquestion have distinct enough characteristics allowing even simple ML models toperform well on the task. Error analysis is performed for the top performingalgorithms showing that in a number of cases the errors are due to insufficientdataset cleaning.</description><author>Stergios Chatzikyriakidis, Chatrine Qwaider, Ilias Kolokousis, Christina Koula, Dimitris Papadakis, Efthymia Sakellariou</author><pubDate>Mon, 02 Dec 2024 13:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00802v4</guid></item><item><title>Enriching Ontologies with Disjointness Axioms using Large Language Models</title><link>http://arxiv.org/abs/2410.03235v2</link><description>Ontologies often lack explicit disjointness declarations between classes,despite their usefulness for sophisticated reasoning and consistency checkingin Knowledge Graphs. In this study, we explore the potential of Large LanguageModels (LLMs) to enrich ontologies by identifying and asserting classdisjointness axioms. Our approach aims at leveraging the implicit knowledgeembedded in LLMs, using prompt engineering to elicit this knowledge forclassifying ontological disjointness. We validate our methodology on theDBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,when guided by effective prompt strategies, can reliably identify disjointclass relationships, thus streamlining the process of ontology completionwithout extensive manual input. For comprehensive disjointness enrichment, wepropose a process that takes logical relationships between disjointness andsubclass statements into account in order to maintain satisfiability and reducethe number of calls to the LLM. This work provides a foundation for futureapplications of LLMs in automated ontology enhancement and offers insights intooptimizing LLM performance through strategic prompt design. Our code ispublicly available on GitHub at https://github.com/n28div/llm-disjointness.</description><author>Elias Crum, Antonio De Santis, Manon Ovide, Jiaxin Pan, Alessia Pisu, Nicolas Lazzari, Sebastian Rudolph</author><pubDate>Mon, 02 Dec 2024 13:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03235v2</guid></item><item><title>RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for AIGC Applications</title><link>http://arxiv.org/abs/2406.11536v2</link><description>The dramatic surge in the utilisation of generative artificial intelligence(GenAI) underscores the need for a secure and efficient mechanism toresponsibly manage, use and disseminate multi-dimensional data generated byartificial intelligence (AI). In this paper, we propose a blockchain-basedcopyright traceability framework called ring oscillator-singular valuedecomposition (RO-SVD), which introduces decomposition computing to approximatelow-rank matrices generated from hardware entropy sources and establishes anAI-generated content (AIGC) copyright traceability mechanism at the devicelevel. By leveraging the parallelism and reconfigurability offield-programmable gate arrays (FPGAs), our framework can be easily constructedon existing AI-accelerated devices and provide a low-cost solution to emergingcopyright issues of AIGC. We developed a hardware-software (HW/SW) co-designprototype based on comprehensive analysis and on-board experiments withmultiple AI-applicable FPGAs. Using AI-generated images as a case study, ourframework demonstrated effectiveness and emphasised customisation,unpredictability, efficiency, management and reconfigurability. To the best ofour knowledge, this is the first practical hardware study discussing andimplementing copyright traceability specifically for AI-generated content.</description><author>Zhuoheng Ran, Muhammad A. A. Abdelgawad, Zekai Zhang, Ray C. C. Cheung, Hong Yan</author><pubDate>Mon, 02 Dec 2024 13:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11536v2</guid></item><item><title>Multimodal Perception System for Real Open Environment</title><link>http://arxiv.org/abs/2410.07926v2</link><description>This paper presents a novel multimodal perception system for a real openenvironment. The proposed system includes an embedded computation platform,cameras, ultrasonic sensors, GPS, and IMU devices. Unlike the traditionalframeworks, our system integrates multiple sensors with advanced computervision algorithms to help users walk outside reliably. The system canefficiently complete various tasks, including navigating to specific locations,passing through obstacle regions, and crossing intersections. Specifically, wealso use ultrasonic sensors and depth cameras to enhance obstacle avoidanceperformance. The path planning module is designed to find the locally optimalroute based on various feedback and the user's current state. To evaluate theperformance of the proposed system, we design several experiments underdifferent scenarios. The results show that the system can help users walkefficiently and independently in complex situations.</description><author>Yuyang Sha</author><pubDate>Mon, 02 Dec 2024 13:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07926v2</guid></item><item><title>Advances in 3D Neural Stylization: A Survey</title><link>http://arxiv.org/abs/2311.18328v3</link><description>Modern artificial intelligence offers a novel and transformative approach tocreating digital art across diverse styles and modalities like images, videosand 3D data, unleashing the power of creativity and revolutionizing the waythat we perceive and interact with visual content. This paper reports on recentadvances in stylized 3D asset creation and manipulation with the expressivepower of neural networks. We establish a taxonomy for neural stylization,considering crucial design choices such as scene representation, guidance data,optimization strategies, and output styles. Building on such taxonomy, oursurvey first revisits the background of neural stylization on 2D images, andthen presents in-depth discussions on recent neural stylization methods for 3Ddata, accompanied by a benchmark evaluating selected mesh and neural fieldstylization methods. Based on the insights gained from the survey, we highlightthe practical significance, open challenges, future research, and potentialimpacts of neural stylization, which facilitates researchers and practitionersto navigate the rapidly evolving landscape of 3D content creation using modernartificial intelligence.</description><author>Yingshu Chen, Guocheng Shao, Ka Chun Shum, Binh-Son Hua, Sai-Kit Yeung</author><pubDate>Mon, 02 Dec 2024 13:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18328v3</guid></item><item><title>Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem</title><link>http://arxiv.org/abs/2409.07123v2</link><description>Natural language explanations (NLEs) are vital for elucidating the reasoningbehind large language model (LLM) decisions. Many techniques have beendeveloped to generate NLEs using LLMs. However, like humans, LLMs might notalways produce optimal NLEs on first attempt. Inspired by human learningprocesses, we introduce Cross-Refine, which employs role modeling by deployingtwo LLMs as generator and critic, respectively. The generator outputs a firstNLE and then refines this initial explanation using feedback and suggestionsprovided by the critic. Cross-Refine does not require any supervised trainingdata or additional training. We validate Cross-Refine across three NLP tasksusing three state-of-the-art open-source LLMs through automatic and humanevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, whichonly utilizes self-feedback to refine the explanations. Our findings fromautomatic evaluation and a user study indicate that Cross-Refine outperformsSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerfulLLMs, whereas Self-Refine only yields strong results with ChatGPT.Additionally, we conduct an ablation study to assess the importance of feedbackand suggestions. Both of them play an important role in refining explanations.We further evaluate Cross-Refine on a bilingual dataset in English and German.</description><author>Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt</author><pubDate>Mon, 02 Dec 2024 13:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07123v2</guid></item><item><title>DUSt3R: Geometric 3D Vision Made Easy</title><link>http://arxiv.org/abs/2312.14132v3</link><description>Multi-view stereo reconstruction (MVS) in the wild requires to first estimatethe camera parameters e.g. intrinsic and extrinsic parameters. These areusually tedious and cumbersome to obtain, yet they are mandatory to triangulatecorresponding pixels in 3D space, which is the core of all best performing MVSalgorithms. In this work, we take an opposite stance and introduce DUSt3R, aradically novel paradigm for Dense and Unconstrained Stereo 3D Reconstructionof arbitrary image collections, i.e. operating without prior information aboutcamera calibration nor viewpoint poses. We cast the pairwise reconstructionproblem as a regression of pointmaps, relaxing the hard constraints of usualprojective camera models. We show that this formulation smoothly unifies themonocular and binocular reconstruction cases. In the case where more than twoimages are provided, we further propose a simple yet effective global alignmentstrategy that expresses all pairwise pointmaps in a common reference frame. Webase our network architecture on standard Transformer encoders and decoders,allowing us to leverage powerful pretrained models. Our formulation directlyprovides a 3D model of the scene as well as depth information, butinterestingly, we can seamlessly recover from it, pixel matches, relative andabsolute camera. Exhaustive experiments on all these tasks showcase that theproposed DUSt3R can unify various 3D vision tasks and set new SoTAs onmonocular/multi-view depth estimation as well as relative pose estimation. Insummary, DUSt3R makes many geometric 3D vision tasks easy.</description><author>Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</author><pubDate>Mon, 02 Dec 2024 13:00:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14132v3</guid></item><item><title>BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion</title><link>http://arxiv.org/abs/2305.15798v4</link><description>Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involveshigh computing demands due to billion-scale parameters. To enhance efficiency,recent studies have reduced sampling steps and applied network quantizationwhile retaining the original architectures. The lack of architectural reductionattempts may stem from worries over expensive retraining for such massivemodels. In this work, we uncover the surprising potential of block pruning andfeature distillation for low-cost general-purpose T2I. By removing severalresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%reduction in model size, MACs, and latency. We show that distillationretraining is effective even under limited resources: using only 13 A100 daysand a tiny dataset, our compact models can imitate the original SDMs (v1.4 andv2.1-base with over 6,000 A100 days). Benefiting from the transferredknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO againstlarger multi-billion parameter models. We further demonstrate the applicabilityof our lightweight backbones in personalized generation and image-to-imagetranslation. Deployment of our models on edge devices attains 4-secondinference. Code and models can be found at:https://github.com/Nota-NetsPresso/BK-SDM</description><author>Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, Shinkook Choi</author><pubDate>Mon, 02 Dec 2024 12:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15798v4</guid></item><item><title>MASP: Scalable GNN-based Planning for Multi-Agent Navigation</title><link>http://arxiv.org/abs/2312.02522v2</link><description>We investigate multi-agent navigation tasks, where multiple agents need toreach initially unassigned goals in a limited time. Classical planning-basedmethods suffer from expensive computation overhead at each step and offerlimited expressiveness for complex cooperation strategies. In contrast,reinforcement learning (RL) has recently become a popular approach foraddressing this issue. However, RL struggles with low data efficiency andcooperation when directly exploring (nearly) optimal policies in a largeexploration space, especially with an increased number of agents(e.g., 10+agents) or in complex environments (e.g., 3-D simulators). In this paper, wepropose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditionedhierarchical planner for navigation tasks with a substantial number of agentsin the decentralized setting. MASP employs a hierarchical framework to reducespace complexity by decomposing a large exploration space into multiplegoal-conditioned subspaces, where a high-level policy assigns agents goals, anda low-level policy navigates agents toward designated goals. For agentcooperation and the adaptation to varying team sizes, we model agents and goalsas graphs to better capture their relationship. The high-level policy, the GoalMatcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimizegoal assignment by updating the agent and the goal graphs. The low-levelpolicy, the Coordinated Action Executor, introduces the Group InformationFusion to facilitate group division and extract agent relationships acrossgroups, enhancing training efficiency for agent cooperation. The resultsdemonstrate that MASP outperforms RL and planning-based baselines in taskefficiency.</description><author>Xinyi Yang, Xinting Yang, Chao Yu, Jiayu Chen, Wenbo Ding, Huazhong Yang, Yu Wang</author><pubDate>Mon, 02 Dec 2024 12:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02522v2</guid></item><item><title>Masked Generative Priors Improve World Models Sequence Modelling Capabilities</title><link>http://arxiv.org/abs/2410.07836v4</link><description>Deep Reinforcement Learning (RL) has become the leading approach for creatingartificial agents in complex environments. Model-based approaches, which are RLmethods with world models that predict environment dynamics, are among the mostpromising directions for improving data efficiency, forming a critical steptoward bridging the gap between research and real-world deployment. Inparticular, world models enhance sample efficiency by learning in imagination,which involves training a generative sequence model of the environment in aself-supervised manner. Recently, Masked Generative Modelling has emerged as amore efficient and superior inductive bias for modelling and generating tokensequences. Building on the Efficient Stochastic Transformer-based World Models(STORM) architecture, we replace the traditional MLP prior with a MaskedGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate ourmodel on two downstream tasks: reinforcement learning and video prediction.GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari100k benchmark. Moreover, we apply Transformer-based World Models to continuousaction environments for the first time, addressing a significant gap in priorresearch. To achieve this, we employ a state mixer function that integrateslatent state representations with actions, enabling our model to handlecontinuous control tasks. We validate this approach through qualitative andquantitative analyses on the DeepMind Control Suite, showcasing theeffectiveness of Transformer-based World Models in this new domain. Our resultshighlight the versatility and efficacy of the MaskGIT dynamics prior, pavingthe way for more accurate world models and effective RL policies.</description><author>Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels</author><pubDate>Mon, 02 Dec 2024 12:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07836v4</guid></item><item><title>Topology Only Pre-Training: Towards Generalised Multi-Domain Graph Models</title><link>http://arxiv.org/abs/2311.03976v4</link><description>The principal benefit of unsupervised representation learning is that apre-trained model can be fine-tuned where data or labels are scarce. Existingapproaches for graph representation learning are domain specific, maintainingconsistent node and edge features across the pre-training and target datasets.This has precluded transfer to multiple domains. We present Topology OnlyPre-Training (ToP), a graph pre-training method based on node and edge featureexclusion. We show positive transfer on evaluation datasets from multipledomains, including domains not present in pre-training data, running directlycontrary to assumptions made in contemporary works. On 75% of experiments, ToPmodels perform significantly $p \leq 0.01$ better than a supervised baseline.Performance is significantly positive on 85.7% of tasks when node and edgefeatures are used in fine-tuning. We further show that out-of-domain topologiescan produce more useful pre-training than in-domain. Under ToP we show bettertransfer from non-molecule pre-training, compared to molecule pre-training, on79% of molecular benchmarks. Against the limited set of other generalist graphmodels ToP performs strongly, including against models with many orders ofmagnitude larger. These findings show that ToP opens broad areas of research inboth transfer learning on scarcely populated graph domains and in graphfoundation models.</description><author>Alex O. Davies, Riku W. Green, Nirav S. Ajmeri, Telmo M. Silva Filho</author><pubDate>Mon, 02 Dec 2024 12:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03976v4</guid></item><item><title>Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision</title><link>http://arxiv.org/abs/2411.18025v2</link><description>Integrating RGB and NIR stereo imaging provides complementary spectralinformation, potentially enhancing robotic 3D vision in challenging lightingconditions. However, existing datasets and imaging systems lack pixel-levelalignment between RGB and NIR images, posing challenges for downstream visiontasks. In this paper, we introduce a robotic vision system equipped withpixel-aligned RGB-NIR stereo cameras and a LiDAR sensor mounted on a mobilerobot. The system simultaneously captures pixel-aligned pairs of RGB stereoimages, NIR stereo images, and temporally synchronized LiDAR points. Utilizingthe mobility of the robot, we present a dataset containing continuous videoframes under diverse lighting conditions. We then introduce two methods thatutilize the pixel-aligned RGB-NIR images: an RGB-NIR image fusion method and afeature fusion method. The first approach enables existing RGB-pretrainedvision models to directly utilize RGB-NIR information without fine-tuning. Thesecond approach fine-tunes existing vision models to more effectively utilizeRGB-NIR information. Experimental results demonstrate the effectiveness ofusing pixel-aligned RGB-NIR images across diverse lighting conditions.</description><author>Jinnyeong Kim, Seung-Hwan Baek</author><pubDate>Mon, 02 Dec 2024 12:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18025v2</guid></item><item><title>LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation</title><link>http://arxiv.org/abs/2408.16886v3</link><description>While large models have achieved significant progress in computer vision,challenges such as optimization complexity, the intricacy of transformerarchitectures, computational constraints, and practical application demandshighlight the importance of simpler model designs in medical imagesegmentation. This need is particularly pronounced in mobile medical devices,which require lightweight, deployable models with real-time performance.However, existing lightweight models often suffer from poor robustness acrossdatasets, limiting their widespread adoption. To address these challenges, thispaper introduces LV-UNet, a lightweight and vanilla model that leveragespre-trained MobileNetv3-Large backbones and incorporates fusible modules.LV-UNet employs an enhanced deep training strategy and switches to a deploymentmode during inference by re-parametrization, significantly reducing parametercount and computational overhead. Experimental results on ISIC 2016, BUSI,CVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a bettertrade-off between performance and the computational load. The code will bereleased at \url{https://github.com/juntaoJianggavin/LV-UNet}.</description><author>Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu</author><pubDate>Mon, 02 Dec 2024 12:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16886v3</guid></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>http://arxiv.org/abs/2411.14951v2</link><description>Human motion generation plays a vital role in applications such as digitalhumans and humanoid robot control. However, most existing approaches disregardphysics constraints, leading to the frequent production of physicallyimplausible motions with pronounced artifacts such as floating and footsliding. In this paper, we propose \textbf{Morph}, a\textbf{Mo}tion-f\textbf{r}ee \textbf{ph}ysics optimization framework,comprising a Motion Generator and a Motion Physics Refinement module, forenhancing physical plausibility without relying on costly real-world motiondata. Specifically, the Motion Generator is responsible for providinglarge-scale synthetic motion data, while the Motion Physics Refinement Moduleutilizes these synthetic data to train a motion imitator within a physicssimulator, enforcing physical constraints to project the noisy motions into aphysically-plausible space. These physically refined motions, in turn, are usedto fine-tune the Motion Generator, further enhancing its capability.Experiments on both text-to-motion and music-to-dance generation tasksdemonstrate that our framework achieves state-of-the-art motion generationquality while improving physical plausibility drastically.</description><author>Zhuo Li, Mingshuang Luo, Ruibing Hou, Xin Zhao, Hao Liu, Hong Chang, Zimo Liu, Chen Li</author><pubDate>Mon, 02 Dec 2024 12:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14951v2</guid></item><item><title>Multi-turn Reinforcement Learning from Preference Human Feedback</title><link>http://arxiv.org/abs/2405.14655v2</link><description>Reinforcement Learning from Human Feedback (RLHF) has become the standardapproach for aligning Large Language Models (LLMs) with human preferences,allowing LLMs to demonstrate remarkable abilities in various tasks. Existingmethods work by emulating the preferences at the single decision (turn) level,limiting their capabilities in settings that require planning or multi-turninteractions to achieve a long-term goal. In this paper, we address this issueby developing novel methods for Reinforcement Learning (RL) from preferencefeedback between two full multi-turn conversations. In the tabular setting, wepresent a novel mirror-descent-based policy optimization algorithm for thegeneral multi-turn preference-based RL problem, and prove its convergence toNash equilibrium. To evaluate performance, we create a new environment,Education Dialogue, where a teacher agent guides a student in learning a randomtopic, and show that a deep RL variant of our algorithm outperforms RLHFbaselines. Finally, we show that in an environment with explicit rewards, ouralgorithm recovers the same performance as a reward-based RL baseline, despiterelying solely on a weaker preference signal.</description><author>Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan Hassidim, Yossi Matias, Rémi Munos</author><pubDate>Mon, 02 Dec 2024 12:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14655v2</guid></item><item><title>Methods for generating and evaluating synthetic longitudinal patient data: a systematic review</title><link>http://arxiv.org/abs/2309.12380v3</link><description>The rapid growth in data availability has facilitated research anddevelopment, yet not all industries have benefited equally due to legal andprivacy constraints. The healthcare sector faces significant challenges inutilizing patient data because of concerns about data security andconfidentiality. To address this, various privacy-preserving methods, includingsynthetic data generation, have been proposed. Synthetic data replicateexisting data as closely as possible, acting as a proxy for sensitiveinformation. While patient data are often longitudinal, this aspect remainsunderrepresented in existing reviews of synthetic data generation inhealthcare. This paper maps and describes methods for generating and evaluatingsynthetic longitudinal patient data in real-life settings through a systematicliterature review, conducted following the PRISMA guidelines and incorporatingdata from five databases up to May 2024. Thirty-nine methods were identified,with four addressing all challenges of longitudinal data generation, thoughnone included privacy-preserving mechanisms. Resemblance was evaluated in moststudies, utility in the majority, and privacy in just over half. Only a smallfraction of studies assessed all three aspects. Our findings highlight the needfor further research in this area.</description><author>Katariina Perkonoja, Kari Auranen, Joni Virta</author><pubDate>Mon, 02 Dec 2024 12:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12380v3</guid></item><item><title>Combining Induction and Transduction for Abstract Reasoning</title><link>http://arxiv.org/abs/2411.02272v4</link><description>When learning an input-output mapping from very few examples, is it better tofirst infer a latent function that explains the examples, or is it better todirectly predict new test outputs, e.g. using a neural network? We study thisquestion on ARC by training neural models for induction (inferring latentfunctions) and transduction (directly predicting the test output for a giventest input). We train on synthetically generated variations of Python programsthat solve ARC training tasks. We find inductive and transductive models solvedifferent kinds of test problems, despite having the same training problems andsharing the same neural architecture: Inductive program synthesis excels atprecise computations, and at composing multiple concepts, while transductionsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-levelperformance on ARC.</description><author>Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis</author><pubDate>Mon, 02 Dec 2024 12:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02272v4</guid></item><item><title>Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning</title><link>http://arxiv.org/abs/2402.07818v5</link><description>Fine-tuning on task-specific datasets is a widely-embraced paradigm ofharnessing the powerful capability of pretrained LLMs for various downstreamtasks. Due to the popularity of LLMs fine-tuning and its accompanying privacyconcerns, differentially private (DP) fine-tuning of pretrained LLMs has beenwidely used to safeguarding the privacy of task-specific datasets. Lying at thedesign core of DP LLM fine-tuning methods is the satisfactory tradeoff amongprivacy, utility, and scalability. Most existing methods build upon the seminalwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,DP-SGD-based fine-tuning methods are unfortunately limited by the inherentinefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods forLLM pretraining, which avoids the scalability bottleneck of SGD byapproximating the gradient with the more efficient zeroth-order gradient.Rather than treating the zeroth-order method as a drop-in replacement for SGD,this paper presents a comprehensive study both theoretically and empirically.First, we propose the stagewise DP zeroth-order method (DP-ZOSO) thatdynamically schedules key hyperparameters. This design is grounded on thesynergy between DP random perturbation and the gradient approximation error ofthe zeroth-order method, and its effect on fine-tuning trajectory. We provide theoretical analysis for both proposed methods. We conductextensive empirical analysis on both encoder-only masked language model anddecoder-only autoregressive language model, achieving impressive results interms of scalability and utility regardless of the class of tasks (comparedwith DPZero, DP-ZOPO improves $4.5\%$ on SST-5, $5.5\%$ on MNLI withRoBERTa-Large and 9.2\% on CB, 3.9\% on BoolQ with OPT-2.7b when $\epsilon=4$,demonstrates more significant enhancement in performance on more complicatedtasks).</description><author>Z Liu, J Lou, W Bao, Y Hu, B Li, Z Qin, K Ren</author><pubDate>Mon, 02 Dec 2024 12:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07818v5</guid></item><item><title>Multi-View Large Reconstruction Model via Geometry-Aware Positional Encoding and Attention</title><link>http://arxiv.org/abs/2406.07648v2</link><description>Despite recent advancements in the Large Reconstruction Model (LRM)demonstrating impressive results, when extending its input from single image tomultiple images, it exhibits inefficiencies, subpar geometric and texturequality, as well as slower convergence speed than expected. It is attributed tothat, LRM formulates 3D reconstruction as a naive images-to-3D translationproblem, ignoring the strong 3D coherence among the input images. In thispaper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed toreconstruct high-quality 3D shapes from multi-views in a 3D-aware manner.Specifically, we introduce a multi-view consistent cross-attention scheme toenable M-LRM to accurately query information from the input images. Moreover,we employ the 3D priors of the input multi-view images to initialize thetriplane tokens. Compared to previous methods, the proposed M-LRM can generate3D shapes of high fidelity. Experimental studies demonstrate that our modelachieves a significant performance gain and faster training convergence.Project page: \url{https://murphylmf.github.io/M-LRM/}.</description><author>Mengfei Li, Xiaoxiao Long, Yixun Liang, Weiyu Li, Yuan Liu, Peng Li, Wenhan Luo, Wenping Wang, Yike Guo</author><pubDate>Mon, 02 Dec 2024 12:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07648v2</guid></item><item><title>AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned Diffusion Models</title><link>http://arxiv.org/abs/2406.13272v2</link><description>Animating stylized avatars with dynamic poses and expressions has attractedincreasing attention for its broad range of applications. Previous research hasmade significant progress by training controllable generative models tosynthesize animations based on reference characteristics, pose, and expressionconditions. However, the mechanisms used in these methods to control pose andexpression often inadvertently introduce unintended features from the targetmotion, while also causing a loss of expression-related details, particularlywhen applied to stylized animation. This paper proposes a new method based onStable Diffusion, called AniFaceDiff, incorporating a new conditioning modulefor animating stylized avatars. First, we propose a refined spatialconditioning approach by Facial Alignment to prevent the inclusion of identitycharacteristics from the target motion. Then, we introduce an ExpressionAdapter that incorporates additional cross-attention layers to address thepotential loss of expression-related information. Our approach effectivelypreserves pose and expression from the target video while maintaining inputimage consistency. Extensive experiments demonstrate that our method achievesstate-of-the-art results, showcasing superior image quality, preservation ofreference features, and expression accuracy, particularly for out-of-domainanimation across diverse styles, highlighting its versatility and stronggeneralization capabilities. This work aims to enhance the quality of virtualstylized animation for positive applications. To promote responsible use invirtual environments, we contribute to the advancement of detection forgenerative content by evaluating state-of-the-art detectors, highlightingpotential areas for improvement, and suggesting solutions.</description><author>Ken Chen, Sachith Seneviratne, Wei Wang, Dongting Hu, Sanjay Saha, Md. Tarek Hasan, Sanka Rasnayaka, Tamasha Malepathirana, Mingming Gong, Saman Halgamuge</author><pubDate>Mon, 02 Dec 2024 12:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13272v2</guid></item><item><title>Learning General Representation of 12-Lead Electrocardiogram with a Joint-Embedding Predictive Architecture</title><link>http://arxiv.org/abs/2410.08559v3</link><description>Electrocardiogram (ECG) captures the heart's electrical signals, offeringvaluable information for diagnosing cardiac conditions. However, the scarcityof labeled data makes it challenging to fully leverage supervised learning inmedical domain. Self-supervised learning (SSL) offers a promising solution,enabling models to learn from unlabeled data and uncover meaningful patterns.In this paper, we show that masked modeling in the latent space can be apowerful alternative to existing self-supervised methods in the ECG domain. Weintroduce ECG-JEPA, a SSL model for 12-lead ECG analysis that learns semanticrepresentations of ECG data by predicting in the hidden latent space, bypassingthe need to reconstruct raw signals. This approach offers several advantages inthe ECG domain: (1) it avoids producing unnecessary details, such as noise,which is common in ECG; and (2) it addresses the limitations of na\"ive L2 lossbetween raw signals. Another key contribution is the introduction ofCross-Pattern Attention (CroPA), a specialized masked attention mechanismtailored for 12-lead ECG data. ECG-JEPA is trained on the union of several openECG datasets, totaling approximately 180,000 samples, and achievesstate-of-the-art performance in various downstream tasks including ECGclassification and feature prediction. Our code is openly available athttps://github.com/sehunfromdaegu/ECG_JEPA.</description><author>Sehun Kim</author><pubDate>Mon, 02 Dec 2024 12:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08559v3</guid></item><item><title>A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds</title><link>http://arxiv.org/abs/2301.13656v4</link><description>We present a comprehensive survey and benchmark of both traditional andlearning-based methods for surface reconstruction from point clouds. This taskis particularly challenging for real-world acquisitions due to factors such asnoise, outliers, non-uniform sampling, and missing data. Traditional approachesoften simplify the problem by imposing handcrafted priors on either the inputpoint clouds or the resulting surface, a process that can require tedioushyperparameter tuning. In contrast, deep learning models have the capability todirectly learn the properties of input point clouds and desired surfaces fromdata. We study the influence of handcrafted and learned priors on the precisionand robustness of surface reconstruction techniques. We evaluate varioustime-tested and contemporary methods in a standardized manner. When bothtrained and evaluated on point clouds with identical characteristics, thelearning-based models consistently produce higher-quality surfaces compared totheir traditional counterparts -- even in scenarios involving novel shapecategories. However, traditional methods demonstrate greater resilience to thediverse anomalies commonly found in real-world 3D acquisitions. For the benefitof the research community, we make our code and datasets available, invitingfurther enhancements to learning-based surface reconstruction. This can beaccessed at https://github.com/raphaelsulzer/dsr-benchmark .</description><author>Raphael Sulzer, Renaud Marlet, Bruno Vallet, Loic Landrieu</author><pubDate>Mon, 02 Dec 2024 12:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13656v4</guid></item><item><title>VQA$^2$: Visual Question Answering for Video Quality Assessment</title><link>http://arxiv.org/abs/2411.03795v4</link><description>The advent and proliferation of large multi-modal models (LMMs) haveintroduced new paradigms to computer vision, transforming various tasks into aunified visual question answering framework. Video Quality Assessment (VQA), aclassic field in low-level visual perception, focused initially on quantitativevideo quality scoring. However, driven by advances in LMMs, it is nowprogressing toward more holistic visual quality understanding tasks. Recentstudies in the image domain have demonstrated that Visual Question Answering(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,related work has not been explored in the video domain, leaving substantialroom for improvement. To address this gap, we introduce the VQA2 InstructionDataset - the first visual question answering instruction dataset that focuseson video quality assessment. This dataset consists of 3 subsets and coversvarious video types, containing 157,755 instruction question-answer pairs.Then, leveraging this foundation, we present the VQA2 series models. The VQA2series models interleave visual and motion tokens to enhance the perception ofspatial-temporal quality details in videos. We conduct extensive experiments onvideo quality scoring and understanding tasks, and results demonstrate that theVQA2series models achieve excellent performance in both tasks. Notably, ourfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual qualityunderstanding tasks while maintaining strong competitiveness in quality scoringtasks. Our work provides a foundation and feasible approach for integratinglow-level video quality assessment and understanding with LMMs.</description><author>Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min</author><pubDate>Mon, 02 Dec 2024 12:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03795v4</guid></item><item><title>Simulation-based inference with scattering representations: scattering is all you need</title><link>http://arxiv.org/abs/2410.11883v3</link><description>We demonstrate the successful use of scattering representations withoutfurther compression for simulation-based inference (SBI) with images (i.e.field-level), illustrated with a cosmological case study. Scatteringrepresentations provide a highly effective representational space forsubsequent learning tasks, although the higher dimensional compressed spaceintroduces challenges. We overcome these through spatial averaging, coupledwith more expressive density estimators. Compared to alternative methods, suchan approach does not require additional simulations for either training orcomputing derivatives, is interpretable, and resilient to covariate shift. Asexpected, we show that a scattering only approach extracts more informationthan traditional second order summary statistics.</description><author>Kiyam Lin, Benjamin Joachimi, Jason D. McEwen</author><pubDate>Mon, 02 Dec 2024 12:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11883v3</guid></item><item><title>Revisiting MAE pre-training for 3D medical image segmentation</title><link>http://arxiv.org/abs/2410.23132v2</link><description>Self-Supervised Learning (SSL) presents an exciting opportunity to unlock thepotential of vast, untapped clinical datasets, for various downstreamapplications that suffer from the scarcity of labeled data. While SSL hasrevolutionized fields like natural language processing and computer vision, itsadoption in 3D medical image computing has been limited by three key pitfalls:Small pre-training dataset sizes, architectures inadequate for 3D medical imageanalysis, and insufficient evaluation practices. In this paper, we addressthese issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumesand ii) using a Residual Encoder U-Net architecture within the state-of-the-artnnU-Net framework. iii) A robust development framework, incorporating 5development and 8 testing brain MRI segmentation datasets, allowedperformance-driven design decisions to optimize the simple concept of MaskedAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpassesprevious SSL methods but also outperforms the strong nnU-Net baseline by anaverage of approximately 3 Dice points setting a new state-of-the-art. Our codeand models are made available here.</description><author>Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Leander Maerkisch, Paul F. Jäger, Klaus Maier-Hein</author><pubDate>Mon, 02 Dec 2024 12:05:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23132v2</guid></item><item><title>Protecting Federated Learning from Extreme Model Poisoning Attacks via Multidimensional Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2303.16668v3</link><description>Current defense mechanisms against model poisoning attacks in federatedlearning (FL) systems have proven effective up to a certain threshold ofmalicious clients. In this work, we introduce FLANDERS, a novel pre-aggregationfilter for FL resilient to large-scale model poisoning attacks, i.e., whenmalicious clients far exceed legitimate participants. FLANDERS treats thesequence of local models sent by clients in each FL round as a matrix-valuedtime series. Then, it identifies malicious client updates as outliers in thistime series by comparing actual observations with estimates generated by amatrix autoregressive forecasting model maintained by the server. Experimentsconducted in several non-iid FL setups show that FLANDERS significantlyimproves robustness across a wide spectrum of attacks when paired with standardand robust existing aggregation methods.</description><author>Edoardo Gabrielli, Dimitri Belli, Zoe Matrullo, Vittorio Miori, Gabriele Tolomei</author><pubDate>Mon, 02 Dec 2024 12:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16668v3</guid></item><item><title>An Architectural Approach to Enhance Deep Long-Tailed Learning</title><link>http://arxiv.org/abs/2411.06098v3</link><description>Deep long-tailed recognition has been widely studied to address the issue ofimbalanced data distributions in real-world scenarios. However, there has beeninsufficient focus on the design of neural architectures, despite empiricalevidence suggesting that architecture can significantly impact performance. Inthis paper, we attempt to mitigate long-tailed issues through architecturalimprovements. To simplify the design process, we utilize DifferentialArchitecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTSmethods struggle to perform well in long-tailed scenarios. To tackle thischallenge, we introduce Long-Tailed Differential Architecture Search (LTDAS).Specifically, we conduct extensive experiments to explore architecturalcomponents that demonstrate better performance on long-tailed data and proposea new search space based on our observations. This ensures that thearchitecture obtained through our search process incorporates superiorcomponents. Additionally, we propose replacing the learnable linear classifierwith an Equiangular Tight Frame (ETF) classifier to further enhance our method.This classifier effectively alleviates the biased search process and preventsperformance collapse. Extensive experimental evaluations demonstrate that ourapproach consistently improves upon existing methods from an orthogonalperspective and achieves state-of-the-art results with simple enhancements.</description><author>Yuhan Pan, Yanan Sun, Wei Gong</author><pubDate>Mon, 02 Dec 2024 11:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06098v3</guid></item><item><title>Reliable Generation of Privacy-preserving Synthetic Electronic Health Record Time Series via Diffusion Models</title><link>http://arxiv.org/abs/2310.15290v6</link><description>Electronic Health Records (EHRs) are rich sources of patient-level data,offering valuable resources for medical data analysis. However, privacyconcerns often restrict access to EHRs, hindering downstream analysis. CurrentEHR de-identification methods are flawed and can lead to potential privacyleakage. Additionally, existing publicly available EHR databases are limited,preventing the advancement of medical research using EHR. This study aims toovercome these challenges by generating realistic and privacy-preservingsynthetic electronic health records (EHRs) time series efficiently. Weintroduce a new method for generating diverse and realistic synthetic EHR timeseries data using Denoising Diffusion Probabilistic Models (DDPM). We conductedexperiments on six databases: Medical Information Mart for Intensive Care IIIand IV (MIMIC-III/IV), the eICU Collaborative Research Database (eICU), andnon-EHR datasets on Stocks and Energy. We compared our proposed method witheight existing methods. Our results demonstrate that our approach significantlyoutperforms all existing methods in terms of data fidelity while requiring lesstraining effort. Additionally, data generated by our method yields a lowerdiscriminative accuracy compared to other baseline methods, indicating theproposed method can generate data with less privacy risk. The proposeddiffusion-model-based method can reliably and efficiently generate syntheticEHR time series, which facilitates the downstream medical data analysis. Ournumerical results show the superiority of the proposed method over all otherexisting methods.</description><author>Muhang Tian, Bernie Chen, Allan Guo, Shiyi Jiang, Anru R. Zhang</author><pubDate>Mon, 02 Dec 2024 11:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15290v6</guid></item><item><title>Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head</title><link>http://arxiv.org/abs/2403.06892v2</link><description>End-to-end transformer-based detectors (DETRs) have shown exceptionalperformance in both closed-set and open-vocabulary object detection (OVD) tasksthrough the integration of language modalities. However, their demandingcomputational requirements have hindered their practical application inreal-time object detection (OD) scenarios. In this paper, we scrutinize thelimitations of two leading models in the OVDEval benchmark, OmDet andGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-basedreal-time OVD model features an innovative Efficient Fusion Head (EFH) moduledesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) withTensorRT and language cache techniques applied. Notably, in zero-shot scenarioson COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly onpar with current state-of-the-art supervised models. Furthermore, itestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting anAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality ofOmDet-Turbo in industrial applications is underscored by its exceptionalperformance on benchmark datasets and superior inference speed, positioning itas a compelling choice for real-time object detection tasks. Code:\url{https://github.com/om-ai-lab/OmDet}</description><author>Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee</author><pubDate>Mon, 02 Dec 2024 11:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06892v2</guid></item><item><title>Anticipating Object State Changes in Long Procedural Videos</title><link>http://arxiv.org/abs/2405.12789v3</link><description>In this work, we introduce (a) the new problem of anticipating object statechanges in images and videos during procedural activities, (b) new curatedannotation data for object state change classification based on the Ego4Ddataset, and (c) the first method for addressing this challenging problem.Solutions to this new task have important implications in vision-based sceneunderstanding, automated monitoring systems, and action planning. The proposednovel framework predicts object state changes that will occur in the nearfuture due to yet unseen human actions by integrating learned visual featuresthat represent recent visual information with natural language (NLP) featuresthat represent past object state changes and actions. Leveraging the extensiveand challenging Ego4D dataset which provides a large-scale collection offirst-person perspective videos across numerous interaction scenarios, weintroduce an extension noted Ego4D-OSCA that provides new curated annotationdata for the object state change anticipation task (OSCA). An extensiveexperimental evaluation is presented demonstrating the proposed method'sefficacy in predicting object state changes in dynamic scenarios. Theperformance of the proposed approach also underscores the potential ofintegrating video and linguistic cues to enhance the predictive performance ofvideo understanding systems and lays the groundwork for future research on thenew task of object state change anticipation. The source code and the newannotation data (Ego4D-OSCA) will be made publicly available.</description><author>Victoria Manousaki, Konstantinos Bacharidis, Filippos Gouidis, Konstantinos Papoutsakis, Dimitris Plexousakis, Antonis Argyros</author><pubDate>Mon, 02 Dec 2024 11:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12789v3</guid></item><item><title>Nonequilbrium physics of generative diffusion models</title><link>http://arxiv.org/abs/2405.11932v3</link><description>Generative diffusion models apply the concept of Langevin dynamics in physicsto machine leaning, attracting a lot of interests from engineering, statisticsand physics, but a complete picture about inherent mechanisms is still lacking.In this paper, we provide a transparent physics analysis of diffusion models,formulating the fluctuation theorem, entropy production, equilibrium measure,and Franz-Parisi potential to understand the dynamic process and intrinsicphase transitions. Our analysis is rooted in a path integral representation ofboth forward and backward dynamics, and in treating the reverse diffusiongenerative process as a statistical inference, where the time-dependent statevariables serve as quenched disorder akin to that in spin glass theory. Ourstudy thus links stochastic thermodynamics, statistical inference and geometrybased analysis together to yield a coherent picture about how the generativediffusion models work.</description><author>Zhendong Yu, Haiping Huang</author><pubDate>Mon, 02 Dec 2024 10:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11932v3</guid></item><item><title>Understanding LLM Embeddings for Regression</title><link>http://arxiv.org/abs/2411.14708v2</link><description>With the rise of large language models (LLMs) for flexibly processinginformation as strings, a natural application is regression, specifically bypreprocessing string representations into LLM embeddings as downstream featuresfor metric prediction. In this paper, we provide one of the first comprehensiveinvestigations into embedding-based regression and demonstrate that LLMembeddings as features can be better for high-dimensional regression tasks thanusing traditional feature engineering. This regression performance can beexplained in part due to LLM embeddings over numeric data inherently preservingLipschitz continuity over the feature space. Furthermore, we quantify thecontribution of different model effects, most notably model size and languageunderstanding, which we find surprisingly do not always improve regressionperformance.</description><author>Eric Tang, Bangding Yang, Xingyou Song</author><pubDate>Mon, 02 Dec 2024 10:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14708v2</guid></item><item><title>Uncertainty quantification for fast reconstruction methods using augmented equivariant bootstrap: Application to radio interferometry</title><link>http://arxiv.org/abs/2410.23178v2</link><description>The advent of next-generation radio interferometers like the Square KilometerArray promises to revolutionise our radio astronomy observational capabilities.The unprecedented volume of data these devices generate requires fast andaccurate image reconstruction algorithms to solve the ill-posed radiointerferometric imaging problem. Most state-of-the-art reconstruction methodslack trustworthy and scalable uncertainty quantification, which is critical forthe rigorous scientific interpretation of radio observations. We propose anunsupervised technique based on a conformalized version of a radio-augmentedequivariant bootstrapping method, which allows us to quantify uncertainties forfast reconstruction methods. Noticeably, we rely on reconstructions fromultra-fast unrolled algorithms. The proposed method brings more reliableuncertainty estimations to our problem than existing alternatives.</description><author>Mostafa Cherif, Tobías I. Liaudat, Jonathan Kern, Christophe Kervazo, Jérôme Bobin</author><pubDate>Mon, 02 Dec 2024 10:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23178v2</guid></item><item><title>Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</title><link>http://arxiv.org/abs/2403.01432v5</link><description>Language Models (LMs) memorize a vast amount of factual knowledge, exhibitingstrong performance across diverse tasks and domains. However, it has beenobserved that the performance diminishes when dealing with less-popular orlow-frequency concepts and entities, for example in domain specificapplications. The two prominent approaches to enhance the performance of LMs onlow-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning(FT) over synthetic data. This paper explores and evaluates the impact of RAGand FT on customizing LMs in handling low-frequency entities on questionanswering tasks. We conduct extensive experiments on twelve LMs of varying sizeand type and different fine tuning, data augmentation, and retrieval models.Our findings indicate that while FT boosts the performance across entities ofvarying popularity, RAG surpasses FT by a large margin particularly for leastpopular factual knowledge. Additionally, the success of both RAG and FTapproaches is amplified by improving retrieval and data augmentationtechniques. Fine tuning, while beneficial for small LMs, requires extensiveresources. To address this issue, we propose the new Stimulus RAG approach thatsurpasses the effectiveness of fine tuning based approaches, therebyeliminating the need for the costly data augmentation and fine tuning step forenriching LMs with less popular factual knowledge. The code is available at\url{https://github.com/informagi/RAGvsFT}.</description><author>Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi</author><pubDate>Mon, 02 Dec 2024 10:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01432v5</guid></item><item><title>Improved Multi-Task Brain Tumour Segmentation with Synthetic Data Augmentation</title><link>http://arxiv.org/abs/2411.04632v2</link><description>This paper presents the winning solution of task 1 and the third-placedsolution of task 3 of the BraTS challenge. The use of automated tools inclinical practice has increased due to the development of more and moresophisticated and reliable algorithms. However, achieving clinical standardsand developing tools for real-life scenarios is a major challenge. To this end,BraTS has organised tasks to find the most advanced solutions for specificpurposes. In this paper, we propose the use of synthetic data to trainstate-of-the-art frameworks in order to improve the segmentation of adultgliomas in a post-treatment scenario, and the segmentation of meningioma forradiotherapy planning. Our results suggest that the use of synthetic data leadsto more robust algorithms, although the synthetic data generation pipeline isnot directly suited to the meningioma task. In task 1, we achieved a DSC of0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,in task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.The code for these tasks is available athttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.</description><author>André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger</author><pubDate>Mon, 02 Dec 2024 10:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04632v2</guid></item><item><title>Dual-Personalizing Adapter for Federated Foundation Models</title><link>http://arxiv.org/abs/2403.19211v2</link><description>Recently, foundation models, particularly large language models (LLMs), havedemonstrated an impressive ability to adapt to various tasks by fine-tuningdiverse instruction data. Notably, federated foundation models (FedFM) emergeas a privacy preservation method to fine-tune models collaboratively underfederated learning (FL) settings by leveraging many distributed datasets withnon-IID data. To alleviate communication and computation overhead,parameter-efficient methods are introduced for efficiency, and some researchadapted personalization methods to FedFM for better user preferences alignment.However, a critical gap in existing research is the neglect of test-timedistribution shifts in real-world applications, and conventional methods fortest-time distribution shifts in personalized FL are less effective for FedFMdue to their failure to adapt to complex distribution shift scenarios and therequirement to train all parameters. To bridge this gap, we refine the settingin FedFM, termed test-time personalization, which aims to learn personalizedfederated foundation models on clients while effectively handling test-timedistribution shifts simultaneously. To address challenges in this setting, weexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter(FedDPA) architecture. By co-working with a foundation model, a global adapterand a local adapter jointly tackle the test-time distribution shifts andclient-specific personalization. Additionally, we introduce an instance-wisedynamic weighting mechanism that dynamically integrates the global and localadapters for each test instance during inference, facilitating effectivetest-time personalization. The effectiveness of the proposed method has beenevaluated on benchmark datasets across different NLP tasks.</description><author>Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein</author><pubDate>Mon, 02 Dec 2024 10:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19211v2</guid></item><item><title>QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection</title><link>http://arxiv.org/abs/2404.02595v3</link><description>This study introduces the Quantum Federated Neural Network for FinancialFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum MachineLearning (QML) and quantum computing with Federated Learning (FL) for financialfraud detection. Using quantum technologies' computational power and the robustdata privacy protections offered by FL, QFNN-FFD emerges as a secure andefficient method for identifying fraudulent transactions within the financialsector. Implementing a dual-phase training model across distributed clientsenhances data integrity and enables superior performance metrics, achievingprecision rates consistently above 95%. Additionally, QFNN-FFD demonstratesexceptional resilience by maintaining an impressive 80% accuracy, highlightingits robustness and readiness for real-world applications. This combination ofhigh performance, security, and robustness against noise positions QFNN-FFD asa transformative advancement in financial technology solutions and establishesit as a new benchmark for privacy-focused fraud detection systems. Thisframework facilitates the broader adoption of secure, quantum-enhancedfinancial services and inspires future innovations that could use QML to tacklecomplex challenges in other areas requiring high confidentiality and accuracy.</description><author>Nouhaila Innan, Alberto Marchisio, Mohamed Bennai, Muhammad Shafique</author><pubDate>Mon, 02 Dec 2024 10:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02595v3</guid></item><item><title>Image Statistics Predict the Sensitivity of Perceptual Quality Metrics</title><link>http://arxiv.org/abs/2303.09874v4</link><description>Previously, Barlow and Attneave hypothesised a link between biological visionand information maximisation. Following Shannon, information was defined usingthe probability of natural images. Several physiological and psychophysicalphenomena have been derived from principles like info-max, efficient coding, oroptimal denoising. However, it remains unclear how this link is expressed inmathematical terms from image probability. Classical derivations were subjectedto strong assumptions on the probability models and on the behaviour of thesensors. Moreover, the direct evaluation of the hypothesis was limited by theinability of classical image models to deliver accurate estimates of theprobability. Here, we directly evaluate image probabilities using a generativemodel for natural images, and analyse how probability-related factors can becombined to predict the sensitivity of state-of-the-art subjective imagequality metrics, a proxy for human perception. We use information theory andregression analysis to find a simple model that when combining just twoprobability-related factors achieves 0.77 correlation with subjective metrics.This probability-based model is validated in two ways: through directcomparison with the opinion of real observers in a subjective qualityexperiment, and by reproducing basic trends of classical psychophysical factssuch as the Contrast Sensitivity Function, the Weber-law, and contrast masking.</description><author>Alexander Hepburn, Valero Laparra, Raúl Santos-Rodriguez, Jesús Malo</author><pubDate>Mon, 02 Dec 2024 10:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09874v4</guid></item><item><title>Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations</title><link>http://arxiv.org/abs/2401.14212v3</link><description>Recognizing visual entities in a natural language sentence and arranging themin a 2D spatial layout require a compositional understanding of language andspace. This task of layout prediction is valuable in text-to-image synthesis asit allows localized and controlled in-painting of the image. In thiscomparative study it is shown that we can predict layouts from languagerepresentations that implicitly or explicitly encode sentence syntax, if thesentences mention similar entity-relationships to the ones seen duringtraining. To test compositional understanding, we collect a test set ofgrammatically correct sentences and layouts describing compositions of entitiesand relations that unlikely have been seen during training. Performance on thistest set substantially drops, showing that current models rely on correlationsin the training data and have difficulties in understanding the structure ofthe input sentences. We propose a novel structural loss function that betterenforces the syntactic structure of the input sentence and show largeperformance gains in the task of 2D spatial layout prediction conditioned ontext. The loss has the potential to be used in other generation tasks where atree-like structure underlies the conditioning modality. Code, trained modelsand the USCOCO evaluation set are available via github.</description><author>Wolf Nuyts, Ruben Cartuyvels, Marie-Francine Moens</author><pubDate>Mon, 02 Dec 2024 10:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14212v3</guid></item><item><title>Object-Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data</title><link>http://arxiv.org/abs/2309.01574v4</link><description>As infrastructure ages, the need for efficient monitoring methods becomesincreasingly critical. Bridge Weigh-In-Motion (BWIM) systems are crucial forcost-effective determination of loads and, consequently, the residual servicelife of road and railway infrastructure. However, conventional BWIM systemsrequire additional sensors for axle detection, which must be installed inpotentially inaccessible locations or places that interfere with bridgeoperation. This study presents a novel approach for real-time detection of train axlesusing sensors arbitrarily placed on bridges, providing an alternative todedicated axle detectors. The developed Virtual Axle Detector with EnhancedReceptive Field (VADER) has been validated on a single-track railway bridgeusing only acceleration measurements, detecting 99.9% of axles with a spatialerror of 3.69cm. Using raw data as input outperformed the state-of-the-artspectrogram-based method in both speed and memory usage by 99%, thereby makingreal-time application feasible for the first time. Additionally, we introduce the Maximum Receptive Field (MRF) rule, a novelapproach to optimise hyperparameters of Convolutional Neural Networks (CNNs)based on the size of objects. In this context, the object size relates to thefundamental frequency of a bridge. The MRF rule effectively narrows thehyperparameter search space, overcoming the need for extensive hyperparametertuning. Since the MRF rule can theoretically be applied to all unstructureddata, it could have implications for a wide range of deep learning problems,from earthquake prediction to object recognition.</description><author>Henik Riedel, Robert Steven Lorenzen, Clemens Hübler</author><pubDate>Mon, 02 Dec 2024 10:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01574v4</guid></item><item><title>Self-Adaptive Quantum Kernel Principal Components Analysis for Compact Readout of Chemiresistive Sensor Arrays</title><link>http://arxiv.org/abs/2409.00115v2</link><description>The rapid growth of Internet of Things (IoT) devices necessitates efficientdata compression techniques to handle the vast amounts of data generated bythese devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate butcrucial component in IoT systems, generate large volumes of data due to theirsimultaneous multi-sensor operations. Classical principal component analysis(cPCA) methods, a common solution to the data compression challenge, facelimitations in preserving critical information during dimensionality reduction.In this study, we present self-adaptive quantum kernel (SAQK) PCA as a superioralternative to enhance information retention. Our findings demonstrate thatSAQK PCA outperforms cPCA in various back-end machine-learning tasks,especially in low-dimensional scenarios where access to quantum bits islimited. These results highlight the potential of noisy intermediate-scalequantum (NISQ) computers to revolutionize data processing in real-world IoTapplications by improving the efficiency and reliability of CSA datacompression and readout, despite the current constraints on qubit availability.</description><author>Zeheng Wang, Timothy van der Laan, Muhammad Usman</author><pubDate>Mon, 02 Dec 2024 10:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00115v2</guid></item></channel></rss>