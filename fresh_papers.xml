<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 14 Jan 2025 13:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dataset Distillation via Committee Voting</title><link>http://arxiv.org/abs/2501.07575v1</link><description>Dataset distillation aims to synthesize a smaller, representative datasetthat preserves the essential properties of the original data, enablingefficient model training with reduced computational resources. Prior work hasprimarily focused on improving the alignment or matching process betweenoriginal and synthetic data, or on enhancing the efficiency of distilling largedatasets. In this work, we introduce ${\bf C}$ommittee ${\bf V}$oting for ${\bfD}$ataset ${\bf D}$istillation (CV-DD), a novel and orthogonal approach thatleverages the collective wisdom of multiple models or experts to createhigh-quality distilled datasets. We start by showing how to establish a strongbaseline that already achieves state-of-the-art accuracy through leveragingrecent advancements and thoughtful adjustments in model design and optimizationprocesses. By integrating distributions and predictions from a committee ofmodels while generating high-quality soft labels, our method captures a widerspectrum of data features, reduces model-specific biases and the adverseeffects of distribution shifts, leading to significant improvements ingeneralization. This voting-based strategy not only promotes diversity androbustness within the distilled dataset but also significantly reducesoverfitting, resulting in improved performance on post-eval tasks. Extensiveexperiments across various datasets and IPCs (images per class) demonstratethat Committee Voting leads to more reliable and adaptable distilled datacompared to single/multi-model distillation methods, demonstrating itspotential for efficient and accurate dataset distillation. Code is availableat: https://github.com/Jiacheng8/CV-DD.</description><author>Jiacheng Cui, Zhaoyi Li, Xiaochen Ma, Xinyue Bi, Yaxin Luo, Zhiqiang Shen</author><pubDate>Mon, 13 Jan 2025 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07575v1</guid></item><item><title>UnCommon Objects in 3D</title><link>http://arxiv.org/abs/2501.07574v1</link><description>We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for3D deep learning and 3D generative AI. uCO3D is the largest publicly-availablecollection of high-resolution videos of objects with 3D annotations thatensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse thanMVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also ofhigher quality, due to extensive quality checks of both the collected videosand the 3D annotations. Similar to analogous datasets, uCO3D containsannotations for 3D camera poses, depth maps and sparse point clouds. Inaddition, each object is equipped with a caption and a 3D Gaussian Splatreconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3Dand obtain superior results using the latter, showing that uCO3D is better forlearning applications.</description><author>Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny</author><pubDate>Mon, 13 Jan 2025 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07574v1</guid></item><item><title>WebWalker: Benchmarking LLMs in Web Traversal</title><link>http://arxiv.org/abs/2501.07572v1</link><description>Retrieval-augmented generation (RAG) demonstrates remarkable performanceacross tasks in open-domain question-answering. However, traditional searchengines may retrieve shallow content, limiting the ability of LLMs to handlecomplex, multi-layered information. To address it, we introduce WebWalkerQA, abenchmark designed to assess the ability of LLMs to perform web traversal. Itevaluates the capacity of LLMs to traverse a website's subpages to extracthigh-quality data systematically. We propose WebWalker, which is a multi-agentframework that mimics human-like web navigation through an explore-criticparadigm. Extensive experimental results show that WebWalkerQA is challengingand demonstrates the effectiveness of RAG combined with WebWalker, through thehorizontal and vertical integration in real-world scenarios.</description><author>Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, Fei Huang</author><pubDate>Mon, 13 Jan 2025 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07572v1</guid></item><item><title>E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack Prediction</title><link>http://arxiv.org/abs/2501.07564v1</link><description>Pre-routing slack prediction remains a critical area of research inElectronic Design Automation (EDA). Despite numerous machine learning-basedapproaches targeting this task, there is still a lack of a truly end-to-endframework that engineers can use to obtain TNS/WNS metrics from raw circuitdata at the placement stage. Existing works have demonstrated effectiveness inArrival Time (AT) prediction but lack a mechanism for Required Arrival Time(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNSmetrics. In this work, we propose E2ESlack, an end-to-end graph-based frameworkfor pre-routing slack prediction. The framework includes a TimingParser thatsupports DEF, SDF and LIB files for feature extraction and graph construction,an arrival time prediction model and a fast RAT estimation module. To the bestof our knowledge, this is the first work capable of predicting path-levelslacks at the pre-routing stage. We perform extensive experiments anddemonstrate that our proposed RAT estimation method outperforms the SOTAML-based prediction method and also pre-routing STA tool. Additionally, theproposed E2ESlack framework achieves TNS/WNS values comparable to post-routingSTA results while saving up to 23x runtime.</description><author>Saurabh Bodhe, Zhanguang Zhang, Atia Hamidizadeh, Shixiong Kai, Yingxue Zhang, Mingxuan Yuan</author><pubDate>Mon, 13 Jan 2025 18:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07564v1</guid></item><item><title>Training-Free Motion-Guided Video Generation with Enhanced Temporal Consistency Using Motion Consistency Loss</title><link>http://arxiv.org/abs/2501.07563v1</link><description>In this paper, we address the challenge of generating temporally consistentvideos with motion guidance. While many existing methods depend on additionalcontrol modules or inference-time fine-tuning, recent studies suggest thateffective motion guidance is achievable without altering the model architectureor requiring extra training. Such approaches offer promising compatibility withvarious video generation foundation models. However, existing training-freemethods often struggle to maintain consistent temporal coherence across framesor to follow guided motion accurately. In this work, we propose a simple yeteffective solution that combines an initial-noise-based approach with a novelmotion consistency loss, the latter being our key innovation. Specifically, wecapture the inter-frame feature correlation patterns of intermediate featuresfrom a video diffusion model to represent the motion pattern of the referencevideo. We then design a motion consistency loss to maintain similar featurecorrelation patterns in the generated video, using the gradient of this loss inthe latent space to guide the generation process for precise motion control.This approach improves temporal consistency across various motion control taskswhile preserving the benefits of a training-free setup. Extensive experimentsshow that our method sets a new standard for efficient, temporally coherentvideo generation.</description><author>Xinyu Zhang, Zicheng Duan, Dong Gong, Lingqiao Liu</author><pubDate>Mon, 13 Jan 2025 18:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07563v1</guid></item><item><title>SecAlign: Defending Against Prompt Injection with Preference Optimization</title><link>http://arxiv.org/abs/2410.05451v2</link><description>Large language models (LLMs) are becoming increasingly prevalent in modernsoftware systems, interfacing between the user and the Internet to assist withtasks that require advanced language understanding. To accomplish these tasks,the LLM often uses external data sources such as user documents, web retrieval,results from API calls, etc. This opens up new avenues for attackers tomanipulate the LLM via prompt injection. Adversarial prompts can be injectedinto external data sources to override the system's intended instruction andinstead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlignbased on the technique of preference optimization. Our defense first constructsa preference dataset with prompt-injected inputs, secure outputs (ones thatrespond to the legitimate instruction), and insecure outputs (ones that respondto the injection). We then perform preference optimization on this dataset toteach the LLM to prefer the secure output over the insecure one. This providesthe first known method that reduces the success rates of various promptinjections to around 0%, even against attacks much more sophisticated than onesseen during training. This indicates our defense generalizes well againstunknown and yet-to-come attacks. Also, our defended models are still practicalwith similar utility to the one before our defensive training. Our code is athttps://github.com/facebookresearch/SecAlign</description><author>Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, David Wagner, Chuan Guo</author><pubDate>Mon, 13 Jan 2025 18:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05451v2</guid></item><item><title>Geometric Scattering on Measure Spaces</title><link>http://arxiv.org/abs/2208.08561v3</link><description>The scattering transform is a multilayered, wavelet-based transform initiallyintroduced as a model of convolutional neural networks (CNNs) that has played afoundational role in our understanding of these networks' stability andinvariance properties. Subsequently, there has been widespread interest inextending the success of CNNs to data sets with non-Euclidean structure, suchas graphs and manifolds, leading to the emerging field of geometric deeplearning. In order to improve our understanding of the architectures used inthis new field, several papers have proposed generalizations of the scatteringtransform for non-Euclidean data structures such as undirected graphs andcompact Riemannian manifolds without boundary. In this paper, we introduce a general, unified model for geometric scatteringon measure spaces. Our proposed framework includes previous work on geometricscattering as special cases but also applies to more general settings such asdirected graphs, signed graphs, and manifolds with boundary. We propose a newcriterion that identifies to which groups a useful representation should beinvariant and show that this criterion is sufficient to guarantee that thescattering transform has desirable stability and invariance properties.Additionally, we consider finite measure spaces that are obtained from randomlysampling an unknown manifold. We propose two methods for constructing adata-driven graph on which the associated graph scattering transformapproximates the scattering transform on the underlying manifold. Moreover, weuse a diffusion-maps based approach to prove quantitative estimates on the rateof convergence of one of these approximations as the number of sample pointstends to infinity. Lastly, we showcase the utility of our method on sphericalimages, directed graphs, and on high-dimensional single-cell data.</description><author>Joyce Chew, Matthew Hirn, Smita Krishnaswamy, Deanna Needell, Michael Perlmutter, Holly Steach, Siddharth Viswanath, Hau-Tieng Wu</author><pubDate>Mon, 13 Jan 2025 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08561v3</guid></item><item><title>MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training</title><link>http://arxiv.org/abs/2501.07556v1</link><description>Image matching, which aims to identify corresponding pixel locations betweenimages, is crucial in a wide range of scientific disciplines, aiding in imageregistration, fusion, and analysis. In recent years, deep learning-based imagematching algorithms have dramatically outperformed humans in rapidly andaccurately finding large amounts of correspondences. However, when dealing withimages captured under different imaging modalities that result in significantappearance changes, the performance of these algorithms often deteriorates dueto the scarcity of annotated cross-modal training data. This limitation hindersapplications in various fields that rely on multiple image modalities to obtaincomplementary information. To address this challenge, we propose a large-scalepre-training framework that utilizes synthetic cross-modal training signals,incorporating diverse data from various sources, to train models to recognizeand match fundamental structures across images. This capability is transferableto real-world, unseen cross-modality image matching tasks. Our key finding isthat the matching model trained with our framework achieves remarkablegeneralizability across more than eight unseen cross-modality registrationtasks using the same network weight, substantially outperforming existingmethods, whether designed for generalization or tailored for specific tasks.This advancement significantly enhances the applicability of image matchingtechnologies across various scientific disciplines and paves the way for newapplications in multi-modality human and artificial intelligence analysis andbeyond.</description><author>Xingyi He, Hao Yu, Sida Peng, Dongli Tan, Zehong Shen, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 13 Jan 2025 18:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07556v1</guid></item><item><title>Dynamic Prototype Rehearsal for Continual Learning in ECG Arrhythmia Detection</title><link>http://arxiv.org/abs/2501.07555v1</link><description>Continual Learning (CL) methods aim to learn from a sequence of tasks whileavoiding the challenge of forgetting previous knowledge. We present DREAM-CL, anovel CL method for ECG arrhythmia detection that introduces dynamic prototyperehearsal memory. DREAM-CL selects representative prototypes by clustering databased on learning behavior during each training session. Within each cluster,we apply a smooth sorting operation that ranks samples by training difficulty,compressing extreme values and removing outliers. The more challenging samplesare then chosen as prototypes for the rehearsal memory, ensuring effectiveknowledge retention across sessions. We evaluate our method ontime-incremental, class-incremental, and lead-incremental scenarios using twowidely used ECG arrhythmia datasets, Chapman and PTB-XL. The resultsdemonstrate that DREAM-CL outperforms the state-of-the-art in CL for ECGarrhythmia detection. Detailed ablation and sensitivity studies are performedto validate the different design choices of our method.</description><author>Sana Rahmani, Reetam Chatterjee, Ali Etemad, Javad Hashemi</author><pubDate>Mon, 13 Jan 2025 18:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07555v1</guid></item><item><title>SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing</title><link>http://arxiv.org/abs/2501.07554v1</link><description>Video editing models have advanced significantly, but evaluating theirperformance remains challenging. Traditional metrics, such as CLIP text andimage scores, often fall short: text scores are limited by inadequate trainingdata and hierarchical dependencies, while image scores fail to assess temporalconsistency. We present SST-EM (Semantic, Spatial, and Temporal EvaluationMetric), a novel evaluation framework that leverages modern Vision-LanguageModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EMcomprises four components: (1) semantic extraction from frames using a VLM, (2)primary object tracking with Object Detection, (3) focused object refinementvia an LLM agent, and (4) temporal consistency assessment using a VisionTransformer (ViT). These components are integrated into a unified metric withweights derived from human evaluations and regression analysis. The name SST-EMreflects its focus on Semantic, Spatial, and Temporal aspects of videoevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity andtemporal smoothness in video editing. The source code is available in the\textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHubRepository}}.</description><author>Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang</author><pubDate>Mon, 13 Jan 2025 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07554v1</guid></item><item><title>disco: Distributional Synthetic Controls</title><link>http://arxiv.org/abs/2501.07550v1</link><description>The method of synthetic controls is widely used for evaluating causal effectsof policy changes in settings with observational data. Often, researchers aimto estimate the causal impact of policy interventions on a treated unit at anaggregate level while also possessing data at a finer granularity. In thisarticle, we introduce the new disco command, which implements theDistributional Synthetic Controls method introduced in Gunsilius (2023). Thiscommand allows researchers to construct entire synthetic distributions for thetreated unit based on an optimally weighted average of the distributions of thecontrol units. Several aggregation schemes are provided to facilitate clearreporting of the distributional effects of the treatment. The package offersboth quantile-based and CDF-based approaches, comprehensive inferenceprocedures via bootstrap and permutation methods, and visualizationcapabilities. We empirically illustrate the use of the package by replicatingthe results in Van Dijcke et al. (2024).</description><author>Florian Gunsilius, David Van Dijcke</author><pubDate>Mon, 13 Jan 2025 18:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07550v1</guid></item><item><title>Few-Shot Task Learning through Inverse Generative Modeling</title><link>http://arxiv.org/abs/2411.04987v2</link><description>Learning the intents of an agent, defined by its goals or motion style, isoften extremely challenging from just a few examples. We refer to this problemas task concept learning and present our approach, Few-Shot Task Learningthrough Inverse Generative Modeling (FTL-IGM), which learns new task conceptsby leveraging invertible neural generative models. The core idea is to pretraina generative model on a set of basic concepts and their demonstrations. Then,given a few demonstrations of a new concept (such as a new goal or a newaction), our method learns the underlying concepts through backpropagationwithout updating the model weights, thanks to the invertibility of thegenerative model. We evaluate our method in five domains -- objectrearrangement, goal-oriented navigation, motion caption of human actions,autonomous driving, and real-world table-top manipulation. Our experimentalresults demonstrate that via the pretrained generative model, we successfullylearn novel concepts and generate agent plans or motion corresponding to theseconcepts in (1) unseen environments and (2) in composition with trainingconcepts.</description><author>Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal</author><pubDate>Mon, 13 Jan 2025 18:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04987v2</guid></item><item><title>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</title><link>http://arxiv.org/abs/2501.07542v1</link><description>Chain-of-Thought (CoT) prompting has proven highly effective for enhancingcomplex reasoning in Large Language Models (LLMs) and Multimodal Large LanguageModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.Nonetheless, human cognition extends beyond language alone, enabling theremarkable capability to think in both words and images. Inspired by thismechanism, we propose a new reasoning paradigm, MultimodalVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs bygenerating image visualizations of their reasoning traces. To ensurehigh-quality visualization, we introduce token discrepancy loss intoautoregressive MLLMs. This innovation significantly improves both visualcoherence and fidelity. We validate this approach through several dynamicspatial reasoning tasks. Experimental results reveal that MVoT demonstratescompetitive performance across tasks. Moreover, it exhibits robust and reliableimprovements in the most challenging scenarios where CoT fails. Ultimately,MVoT establishes new possibilities for complex reasoning tasks where visualthinking can effectively complement verbal reasoning.</description><author>Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei</author><pubDate>Mon, 13 Jan 2025 18:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07542v1</guid></item><item><title>Improving the Performance of Echo State Networks Through State Feedback</title><link>http://arxiv.org/abs/2312.15141v2</link><description>Reservoir computing, using nonlinear dynamical systems, offers acost-effective alternative to neural networks for complex tasks involvingprocessing of sequential data, time series modeling, and system identification.Echo state networks (ESNs), a type of reservoir computer, mirror neuralnetworks but simplify training. They apply fixed, random linear transformationsto the internal state, followed by nonlinear changes. This process, guided byinput signals and linear regression, adapts the system to match targetcharacteristics, reducing computational demands. A potential drawback of ESNsis that the fixed reservoir may not offer the complexity needed for specificproblems. While directly altering (training) the internal ESN would reintroducethe computational burden, an indirect modification can be achieved byredirecting some output as input. This feedback can influence the internalreservoir state, yielding ESNs with enhanced complexity suitable for broaderchallenges. In this paper, we demonstrate that by feeding some component of thereservoir state back into the network through the input, we can drasticallyimprove upon the performance of a given ESN. We rigorously prove that, for anygiven ESN, feedback will almost always improve the accuracy of the output. Fora set of three tasks, each representing different problem classes, we find thatwith feedback the average error measures are reduced by $30\%-60\%$.Remarkably, feedback provides at least an equivalent performance boost todoubling the initial number of computational nodes, a computationally expensiveand technologically challenging alternative. These results demonstrate thebroad applicability and substantial usefulness of this feedback scheme.</description><author>Peter J. Ehlers, Hendra I. Nurdin, Daniel Soh</author><pubDate>Mon, 13 Jan 2025 18:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15141v2</guid></item><item><title>The Sound of Water: Inferring Physical Properties from Pouring Liquids</title><link>http://arxiv.org/abs/2411.11222v2</link><description>We study the connection between audio-visual observations and the underlyingphysics of a mundane yet intriguing everyday activity: pouring liquids. Givenonly the sound of liquid pouring into a container, our objective is toautomatically infer physical properties such as the liquid level, the shape andsize of the container, the pouring rate and the time to fill. To this end, we:(i) show in theory that these properties can be determined from the fundamentalfrequency (pitch); (ii) train a pitch detection model with supervision fromsimulated data and visual data with a physics-inspired objective; (iii)introduce a new large dataset of real pouring videos for a systematic study;(iv) show that the trained model can indeed infer these physical properties forreal data; and finally, (v) we demonstrate strong generalization to variouscontainer shapes, other datasets, and in-the-wild YouTube videos. Our workpresents a keen understanding of a narrow yet rich problem at the intersectionof acoustics, physics, and learning. It opens up applications to enhancemultisensory perception in robotic pouring.</description><author>Piyush Bagad, Makarand Tapaswi, Cees G. M. Snoek, Andrew Zisserman</author><pubDate>Mon, 13 Jan 2025 18:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11222v2</guid></item><item><title>Robot Synesthesia: A Sound and Emotion Guided AI Painter</title><link>http://arxiv.org/abs/2302.04850v3</link><description>If a picture paints a thousand words, sound may voice a million. While recentrobotic painting and image synthesis methods have achieved progress ingenerating visuals from text inputs, the translation of sound into images isvastly unexplored. Generally, sound-based interfaces and sonic interactionshave the potential to expand accessibility and control for the user and providea means to convey complex emotions and the dynamic aspects of the real world.In this paper, we propose an approach for using sound and speech to guide arobotic painting process, known here as robot synesthesia. For general sound,we encode the simulated paintings and input sounds into the same latent space.For speech, we decouple speech into its transcribed text and the tone of thespeech. Whereas we use the text to control the content, we estimate theemotions from the tone to guide the mood of the painting. Our approach has beenfully integrated with FRIDA, a robotic painting framework, adding sound andspeech to FRIDA's existing input modalities, such as text and style. In twosurveys, participants were able to correctly guess the emotion or natural soundused to generate a given painting more than twice as likely as random chance.On our sound-guided image manipulation and music-guided paintings, we discussthe results qualitatively.</description><author>Vihaan Misra, Peter Schaldenbrand, Jean Oh</author><pubDate>Mon, 13 Jan 2025 18:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04850v3</guid></item><item><title>Quilt-1M: One Million Image-Text Pairs for Histopathology</title><link>http://arxiv.org/abs/2306.11207v4</link><description>Recent accelerations in multi-modal applications have been made possible withthe plethora of image and text data available online. However, the scarcity ofanalogous data in the medical field, specifically in histopathology, has slowedcomparable progress. To enable similar representation learning forhistopathology, we turn to YouTube, an untapped resource of videos, offering$1,087$ hours of valuable educational histopathology videos from expertclinicians. From YouTube, we curate QUILT: a large-scale vision-languagedataset consisting of $802, 144$ image and text pairs. QUILT was automaticallycurated using a mixture of models, including large language models, handcraftedalgorithms, human knowledge databases, and automatic speech recognition. Incomparison, the most comprehensive datasets curated for histopathology amassonly around $200$K samples. We combine QUILT with datasets from other sources,including Twitter, research papers, and the internet in general, to create aneven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking itas the largest vision-language histopathology dataset to date. We demonstratethe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our modeloutperforms state-of-the-art models on both zero-shot and linear probing tasksfor classifying new histopathology images across $13$ diverse patch-leveldatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.</description><author>Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro</author><pubDate>Mon, 13 Jan 2025 18:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11207v4</guid></item><item><title>ML Mule: Mobile-Driven Context-Aware Collaborative Learning</title><link>http://arxiv.org/abs/2501.07536v1</link><description>Artificial intelligence has been integrated into nearly every aspect of dailylife, powering applications from object detection with computer vision to largelanguage models for writing emails and compact models in smart homes. Thesemachine learning models cater to individual users but are often detached fromthem, as they are typically stored and processed in centralized data centers.This centralized approach raises privacy concerns, incurs high infrastructurecosts, and struggles with personalization. Federated and fully decentralizedlearning methods have been proposed to address these issues, but they stilldepend on centralized servers or face slow convergence due to communicationconstraints. To overcome these challenges, we propose ML Mule, a approach thatutilizes individual mobile devices as 'Mules' to train and transport modelsnapshots as they move through physical spaces, sharing these models with thephysical 'Spaces' they inhabit. This method implicitly forms affinity groupsamong devices associated with users who share particular spaces, enablingcollaborative model evolution, and protecting users' privacy. Our approachaddresses several major shortcomings of traditional, federated, and fullydecentralized learning systems. The proposed framework represents a new classof machine learning methods that are more robust, distributed, andpersonalized, bringing the field closer to realizing the original vision ofintelligent, adaptive, and genuinely context-aware smart environments. Theresults show that ML Mule converges faster and achieves higher model accuracycompared to other existing methods.</description><author>Haoxiang Yu, Javier Berrocal, Christine Julien</author><pubDate>Mon, 13 Jan 2025 18:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07536v1</guid></item><item><title>Investigating Map-Based Path Loss Models: A Study of Feature Representations in Convolutional Neural Networks</title><link>http://arxiv.org/abs/2501.07534v1</link><description>Path loss prediction is a beneficial tool for efficient use of the radiofrequency spectrum. Building on prior research on high-resolution map-basedpath loss models, this paper studies convolutional neural network inputrepresentations in more detail. We investigate different methods ofrepresenting scalar features in convolutional neural networks. Specifically, wecompare using frequency and distance as input channels to convolutional layersor as scalar inputs to regression layers. We assess model performance usingthree different feature configurations and find that representing scalarfeatures as image channels results in the strongest generalization.</description><author>Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu</author><pubDate>Mon, 13 Jan 2025 18:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07534v1</guid></item><item><title>Higher-Order Topological Directionality and Directed Simplicial Neural Networks</title><link>http://arxiv.org/abs/2409.08389v2</link><description>Topological Deep Learning (TDL) has emerged as a paradigm to process andlearn from signals defined on higher-order combinatorial topological spaces,such as simplicial or cell complexes. Although many complex systems have anasymmetric relational structure, most TDL models forcibly symmetrize theserelationships. In this paper, we first introduce a novel notion of higher-orderdirectionality and we then design Directed Simplicial Neural Networks(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating ondirected simplicial complexes able to leverage directed and possibly asymmetricinteractions among the simplices. To our knowledge, this is the first TDL modelusing a notion of higher-order directionality. We theoretically and empiricallyprove that Dir-SNNs are more expressive than their directed graph counterpartin distinguishing isomorphic directed graphs. Experiments on a synthetic sourcelocalization task demonstrate that Dir-SNNs outperform undirected SNNs when theunderlying complex is directed, and perform comparably when the underlyingcomplex is undirected.</description><author>Manuel Lecha, Andrea Cavallo, Francesca Dominici, Elvin Isufi, Claudio Battiloro</author><pubDate>Mon, 13 Jan 2025 18:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08389v2</guid></item><item><title>Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly Detection</title><link>http://arxiv.org/abs/2501.07533v1</link><description>Canine cardiomegaly, marked by an enlarged heart, poses serious health risksif undetected, requiring accurate diagnostic methods. Current detection modelsoften rely on small, poorly annotated datasets and struggle to generalizeacross diverse imaging conditions, limiting their real-world applicability. Toaddress these issues, we propose a Confident Pseudo-labeled DiffusionAugmentation (CDA) model for identifying canine cardiomegaly. Our approachaddresses the challenge of limited high-quality training data by employingdiffusion models to generate synthetic X-ray images and annotate VertebralHeart Score key points, thereby expanding the dataset. We also employ apseudo-labeling strategy with Monte Carlo Dropout to select high-confidencelabels, refine the synthetic dataset, and improve accuracy. Iterativelyincorporating these labels enhances the model's performance, overcoming thelimitations of existing approaches. Experimental results show that the CDAmodel outperforms traditional methods, achieving state-of-the-art accuracy incanine cardiomegaly detection. The code implementation is available athttps://github.com/Shira7z/CDA.</description><author>Shiman Zhang, Lakshmikar Reddy Polamreddy, Youshan Zhang</author><pubDate>Mon, 13 Jan 2025 18:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07533v1</guid></item><item><title>Investigating Large Language Models in Inferring Personality Traits from User Conversations</title><link>http://arxiv.org/abs/2501.07532v1</link><description>Large Language Models (LLMs) are demonstrating remarkable human likecapabilities across diverse domains, including psychological assessment. Thisstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can inferBig Five personality traits and generate Big Five Inventory-10 (BFI-10) itemscores from user conversations under zero-shot prompting conditions. Ourfindings reveal that incorporating an intermediate step--prompting for BFI-10item scores before calculating traits--enhances accuracy and aligns moreclosely with the gold standard than direct trait inference. This structuredapproach underscores the importance of leveraging psychological frameworks inimproving predictive precision. Additionally, a group comparison based ondepressive symptom presence revealed differential model performance.Participants were categorized into two groups: those experiencing at least onedepressive symptom and those without symptoms. GPT-4o mini demonstratedheightened sensitivity to depression-related shifts in traits such asNeuroticism and Conscientiousness within the symptom-present group, whereasGPT-4o exhibited strengths in nuanced interpretation across groups. Thesefindings underscore the potential of LLMs to analyze real-world psychologicaldata effectively, offering a valuable foundation for interdisciplinary researchat the intersection of artificial intelligence and psychology.</description><author>Jianfeng Zhu, Ruoming Jin, Karin G. Coifman</author><pubDate>Mon, 13 Jan 2025 18:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07532v1</guid></item><item><title>Evaluating Agent-based Program Repair at Google</title><link>http://arxiv.org/abs/2501.07531v1</link><description>Agent-based program repair offers to automatically resolve complex bugsend-to-end by combining the planning, tool use, and code generation abilitiesof modern LLMs. Recent work has explored the use of agent-based repairapproaches on the popular open-source SWE-Bench, a collection of bugs fromhighly-rated GitHub Python projects. In addition, various agentic approachessuch as SWE-Agent have been proposed to solve bugs in this benchmark. Thispaper explores the viability of using an agentic approach to address bugs in anenterprise context. To investigate this, we curate an evaluation set of 178bugs drawn from Google's issue tracking system. This dataset spans bothhuman-reported (78) and machine-reported bugs (100). To establish a repair performance baseline on this benchmark, we implementPasserine, an agent similar in spirit to SWE-Agent that can work withinGoogle's development environment. We show that with 20 trajectory samples andGemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,plausible) for 73% of machine-reported and 25.6% of human-reported bugs in ourevaluation set. After manual examination, we found that 43% of machine-reportedbugs and 17.9% of human-reported bugs have at least one patch that issemantically equivalent to the ground-truth patch. These results establish a baseline on an industrially relevant benchmark,which as we show, contains bugs drawn from a different distribution -- in termsof language diversity, size, and spread of changes, etc. -- compared to thosein the popular SWE-Bench dataset.</description><author>Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant Sanyam, Michele Tufano, Satish Chandra</author><pubDate>Mon, 13 Jan 2025 18:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07531v1</guid></item><item><title>IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion</title><link>http://arxiv.org/abs/2501.07530v1</link><description>Facial video editing has become increasingly important for content creators,enabling the manipulation of facial expressions and attributes. However,existing models encounter challenges such as poor editing quality, highcomputational costs and difficulties in preserving facial identity acrossdiverse edits. Additionally, these models are often constrained to editingpredefined facial attributes, limiting their flexibility to diverse editingprompts. To address these challenges, we propose a novel facial video editingframework that leverages the rich latent space of pre-trained text-to-image(T2I) diffusion models and fine-tune them specifically for facial video editingtasks. Our approach introduces a targeted fine-tuning scheme that enables highquality, localized, text-driven edits while ensuring identity preservationacross video frames. Additionally, by using pre-trained T2I models duringinference, our approach significantly reduces editing time by 80%, whilemaintaining temporal consistency throughout the video sequence. We evaluate theeffectiveness of our approach through extensive testing across a wide range ofchallenging scenarios, including varying head poses, complex action sequences,and diverse facial expressions. Our method consistently outperforms existingtechniques, demonstrating superior performance across a broad set of metricsand benchmarks.</description><author>Tharun Anand, Aryan Garg, Kaushik Mitra</author><pubDate>Mon, 13 Jan 2025 18:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07530v1</guid></item><item><title>Enhance Eye Disease Detection using Learnable Probabilistic Discrete Latents in Machine Learning Architectures</title><link>http://arxiv.org/abs/2402.16865v3</link><description>Ocular diseases, including diabetic retinopathy and glaucoma, present asignificant public health challenge due to their high prevalence and potentialfor causing vision impairment. Early and accurate diagnosis is crucial foreffective treatment and management. In recent years, deep learning models haveemerged as powerful tools for analysing medical images, such as retina imaging.However, challenges persist in model relibability and uncertainty estimation,which are critical for clinical decision-making. This study leverages theprobabilistic framework of Generative Flow Networks (GFlowNets) to learn theposterior distribution over latent discrete dropout masks for theclassification and analysis of ocular diseases using fundus images. We developa robust and generalizable method that utilizes GFlowOut integrated withResNet18 and ViT models as the backbone in identifying various ocularconditions. This study employs a unique set of dropout masks - none, random,bottomup, and topdown - to enhance model performance in analyzing these fundusimages. Our results demonstrate that our learnable probablistic latentssignificantly improves accuracy, outperforming the traditional dropoutapproach. We utilize a gradient map calculation method, Grad-CAM, to assessmodel explainability, observing that the model accurately focuses on criticalimage regions for predictions. The integration of GFlowOut in neural networkspresents a promising advancement in the automated diagnosis of ocular diseases,with implications for improving clinical workflows and patient outcomes.</description><author>Anirudh Prabhakaran, YeKun Xiao, Ching-Yu Cheng, Dianbo Liu</author><pubDate>Mon, 13 Jan 2025 18:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16865v3</guid></item><item><title>Path Loss Prediction Using Deep Learning</title><link>http://arxiv.org/abs/2411.17752v2</link><description>Radio deployments and spectrum planning benefit from path loss predictions.Obstructions along a communications link are often considered implicitly orthrough derived metrics such as representative clutter height or totalobstruction depth. In this paper, we propose a path-specific path lossprediction method that uses convolutional neural networks to automaticallyperform feature extraction from high-resolution obstruction height maps. Ourmethods result in low prediction error in a variety of environments withoutrequiring derived metrics.</description><author>Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu</author><pubDate>Mon, 13 Jan 2025 18:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17752v2</guid></item><item><title>Communication-Efficient, 2D Parallel Stochastic Gradient Descent for Distributed-Memory Optimization</title><link>http://arxiv.org/abs/2501.07526v1</link><description>Distributed-memory implementations of numerical optimization algorithm, suchas stochastic gradient descent (SGD), require interprocessor communication atevery iteration of the algorithm. On modern distributed-memory clusters wherecommunication is more expensive than computation, the scalability andperformance of these algorithms are limited by communication cost. This workgeneralizes prior work on 1D $s$-step SGD and 1D Federated SGD with Averaging(FedAvg) to yield a 2D parallel SGD method (HybridSGD) which attains acontinuous performance trade off between the two baseline algorithms. Wepresent theoretical analysis which show the convergence, computation,communication, and memory trade offs between $s$-step SGD, FedAvg, 2D parallelSGD, and other parallel SGD variants. We implement all algorithms in C++ andMPI and evaluate their performance on a Cray EX supercomputing system. Ourempirical results show that HybridSGD achieves better convergence than FedAvgat similar processor scales while attaining speedups of $5.3\times$ over$s$-step SGD and speedups up to $121\times$ over FedAvg when used to solvebinary classification tasks using the convex, logistic regression model ondatasets obtained from the LIBSVM repository.</description><author>Aditya Devarakonda, Ramakrishnan Kannan</author><pubDate>Mon, 13 Jan 2025 17:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07526v1</guid></item><item><title>RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment</title><link>http://arxiv.org/abs/2501.07525v1</link><description>Automated chest radiographs interpretation requires both accurate diseaseclassification and detailed radiology report generation, presenting asignificant challenge in the clinical workflow. Current approaches either focuson classification accuracy at the expense of interpretability or generatedetailed but potentially unreliable reports through image captioningtechniques. In this study, we present RadAlign, a novel framework that combinesthe predictive accuracy of vision-language models (VLMs) with the reasoningcapabilities of large language models (LLMs). Inspired by the radiologist'sworkflow, RadAlign first employs a specialized VLM to align visual featureswith key medical concepts, achieving superior disease classification with anaverage AUC of 0.885 across multiple diseases. These recognized medicalconditions, represented as text-based concepts in the aligned visual-languagespace, are then used to prompt LLM-based report generation. Enhanced by aretrieval-augmented generation mechanism that grounds outputs in similarhistorical cases, RadAlign delivers superior report quality with a GREEN scoreof 0.678, outperforming state-of-the-art methods' 0.634. Our frameworkmaintains strong clinical interpretability while reducing hallucinations,advancing automated medical imaging and report analysis through integratedpredictive and generative AI. Code is available athttps://github.com/difeigu/RadAlign.</description><author>Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas</author><pubDate>Mon, 13 Jan 2025 17:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07525v1</guid></item><item><title>Parallel Key-Value Cache Fusion for Position Invariant RAG</title><link>http://arxiv.org/abs/2501.07523v1</link><description>Recent advancements in Large Language Models (LLMs) underscore the necessityof Retrieval Augmented Generation (RAG) to leverage external information.However, LLMs are sensitive to the position of relevant information withincontexts and tend to generate incorrect responses when such information isplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,we introduce a framework that generates consistent outputs for decoder-onlymodels, irrespective of the input context order. Experimental results for threeopen domain question answering tasks demonstrate position invariance, where themodel is not sensitive to input context order, and superior robustness toirrelevent passages compared to prevailing approaches for RAG pipelines.</description><author>Philhoon Oh, Jinwoo Shin, James Thorne</author><pubDate>Mon, 13 Jan 2025 17:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07523v1</guid></item><item><title>Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation</title><link>http://arxiv.org/abs/2408.09698v5</link><description>Recent advances in Large Language Models (LLMs) have demonstrated significantpotential in the field of Recommendation Systems (RSs). Most existing studieshave focused on converting user behavior logs into textual prompts andleveraging techniques such as prompt tuning to enable LLMs for recommendationtasks. Meanwhile, research interest has recently grown in multimodalrecommendation systems that integrate data from images, text, and other sourcesusing modality fusion techniques. This introduces new challenges to theexisting LLM-based recommendation paradigm which relies solely on text modalityinformation. Moreover, although Multimodal Large Language Models (MLLMs)capable of processing multi-modal inputs have emerged, how to equip MLLMs withmulti-modal recommendation capabilities remains largely unexplored. To thisend, in this paper, we propose the Multimodal Large Language Model-enhancedMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamicuser preference, we design a two-stage user preference summarization method.Specifically, we first utilize an MLLM-based item-summarizer to extract imagefeature given an item and convert the image into text. Then, we employ arecurrent user preference summarization generation paradigm to capture thedynamic changes in user preferences based on an LLM-based user-summarizer.Finally, to enable the MLLM for multi-modal recommendation task, we propose tofine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)techniques. Extensive evaluations across various datasets validate theeffectiveness of MLLM-MSR, showcasing its superior ability to capture and adaptto the evolving dynamics of user preferences.</description><author>Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong</author><pubDate>Mon, 13 Jan 2025 17:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09698v5</guid></item><item><title>RGB-D Indiscernible Object Counting in Underwater Scenes</title><link>http://arxiv.org/abs/2304.11677v2</link><description>Recently, indiscernible/camouflaged scene understanding has attracted lots ofresearch attention in the vision community. We further advance the frontier ofthis field by systematically studying a new challenge named indiscernibleobject counting (IOC), the goal of which is to count objects that are blendedwith respect to their surroundings. Due to a lack of appropriate IOC datasets,we present a large-scale dataset IOCfish5K which contains a total of 5,637high-resolution images and 659,024 annotated center points. Our datasetconsists of a large number of indiscernible objects (mainly fish) in underwaterscenes, making the annotation process all the more challenging. IOCfish5K issuperior to existing datasets with indiscernible scenes because of its largerscale, higher image resolutions, more annotations, and denser scenes. All theseaspects make it the most challenging dataset for IOC so far, supportingprogress in this area. Benefiting from the recent advancements of depthestimation foundation models, we construct high-quality depth maps forIOCfish5K by generating pseudo labels using the Depth Anything V2 model. TheRGB-D version of IOCfish5K is named IOCfish5K-D. For benchmarking purposes onIOCfish5K, we select 14 mainstream methods for object counting and carefullyevaluate them. For multimodal IOCfish5K-D, we evaluate other 4 popularmultimodal counting methods. Furthermore, we propose IOCFormer, a new strongbaseline that combines density and regression branches in a unified frameworkand can effectively tackle object counting under concealed scenes. We alsopropose IOCFormer-D to enable the effective usage of depth modality in helpingdetect and count objects hidden in their environments. Experiments show thatIOCFormer and IOCFormer-D achieve state-of-the-art scores on IOCfish5K andIOCfish5K-D, respectively.</description><author>Guolei Sun, Xiaogang Cheng, Zhaochong An, Xiaokang Wang, Yun Liu, Deng-Ping Fan, Ming-Ming Cheng, Luc Van Gool</author><pubDate>Mon, 13 Jan 2025 17:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11677v2</guid></item><item><title>CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets with Sparse Multi-Baseline Data</title><link>http://arxiv.org/abs/2406.04158v3</link><description>Multi-baseline Synthetic Aperture Radar (SAR) three-dimensional (3D)tomography is a crucial remote sensing technique that provides 3D resolutionunavailable in conventional SAR imaging. However, achieving high-qualityimaging typically requires multi-angle or full-aperture data, resulting insignificant imaging costs. Recent advancements in sparse 3D SAR, which rely ondata from limited apertures, have gained attention as a cost-effectivealternative. Notably, deep learning techniques have markedly enhanced theimaging quality of sparse 3D SAR. Despite these advancements, existing methodsprimarily depend on high-resolution radar images for supervising the trainingof deep neural networks (DNNs). This exclusive dependence on single-modal dataprevents the introduction of complementary information from other data sources,limiting further improvements in imaging performance. In this paper, weintroduce a Cross-Modal 3D-SAR Reconstruction Network (CMAR-Net) to enhance 3DSAR imaging by integrating heterogeneous information. Leveraging cross-modalsupervision from 2D optical images and error transfer guaranteed bydifferentiable rendering, CMAR-Net achieves efficient training and reconstructshighly sparse multi-baseline SAR data into visually structured and accurate 3Dimages, particularly for vehicle targets. Extensive experiments on simulatedand real-world datasets demonstrate that CMAR-Net significantly outperformsSOTA sparse reconstruction algorithms based on compressed sensing (CS) and deeplearning (DL). Furthermore, our method eliminates the need for time-consumingfull-aperture data preprocessing and relies solely on computer-rendered opticalimages, significantly reducing dataset construction costs. This work highlightsthe potential of deep learning for multi-baseline SAR 3D imaging and introducesa novel framework for radar imaging research through cross-modal learning.</description><author>Da Li, Guoqiang Zhao, Houjun Sun, Jiacheng Bao</author><pubDate>Mon, 13 Jan 2025 17:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04158v3</guid></item><item><title>The importance of visual modelling languages in generative software engineering</title><link>http://arxiv.org/abs/2411.17976v3</link><description>Multimodal GPTs represent a watershed in the interplay between SoftwareEngineering and Generative Artificial Intelligence. GPT-4 accepts image andtext inputs, rather than simply natural language. We investigate relevant usecases stemming from these enhanced capabilities of GPT-4. To the best of ourknowledge, no other work has investigated similar use cases involving SoftwareEngineering tasks carried out via multimodal GPTs prompted with a mix ofdiagrams and natural language.</description><author>Roberto Rossi</author><pubDate>Mon, 13 Jan 2025 17:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17976v3</guid></item><item><title>The Paradox of Success in Evolutionary and Bioinspired Optimization: Revisiting Critical Issues, Key Studies, and Methodological Pathways</title><link>http://arxiv.org/abs/2501.07515v1</link><description>Evolutionary and bioinspired computation are crucial for efficientlyaddressing complex optimization problems across diverse application domains. Bymimicking processes observed in nature, like evolution itself, these algorithmsoffer innovative solutions beyond the reach of traditional optimizationmethods. They excel at finding near-optimal solutions in large, complex searchspaces, making them invaluable in numerous fields. However, both areas areplagued by challenges at their core, including inadequate benchmarking,problem-specific overfitting, insufficient theoretical grounding, andsuperfluous proposals justified only by their biological metaphor. Thisoverview recapitulates and analyzes in depth the criticisms concerning the lackof innovation and rigor in experimental studies within the field. To this end,we examine the judgmental positions of the existing literature in an informedattempt to guide the research community toward directions of solid contributionand advancement in these areas. We summarize guidelines for the design ofevolutionary and bioinspired optimizers, the development of experimentalcomparisons, and the derivation of novel proposals that take a step further inthe field. We provide a brief note on automating the process of creating thesealgorithms, which may help align metaheuristic optimization research with itsprimary objective (solving real-world problems), provided that our identifiedpathways are followed. Our conclusions underscore the need for a sustained pushtowards innovation and the enforcement of methodological rigor in prospectivestudies to fully realize the potential of these advanced computationaltechniques.</description><author>Daniel Molina, Javier Del Ser, Javier Poyatos, Francisco Herrera</author><pubDate>Mon, 13 Jan 2025 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07515v1</guid></item><item><title>FlashRNN: Optimizing Traditional RNNs on Modern Hardware</title><link>http://arxiv.org/abs/2412.07752v2</link><description>While Transformers and other sequence-parallelizable neural networkarchitectures seem like the current state of the art in sequence modeling, theyspecifically lack state-tracking capabilities. These are important fortime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,as well as modern variants like sLSTM do have these capabilities at the cost ofstrictly sequential processing. While this is often seen as a stronglimitation, we show how fast these networks can get with ourhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to theregister level on modern GPUs. We extend traditional RNNs with aparallelization variant that processes multiple RNNs of smaller hidden state inparallel, similar to the head-wise processing in Transformers. To enableflexibility on different GPU variants, we introduce a new optimizationframework for hardware-internal cache sizes, memory and compute handling. Itmodels the hardware in a setting using polyhedral-like constraints, includingthe notion of divisibility. This speeds up the solution process in ourConstrINT library for general integer constraint satisfaction problems (integerCSPs). We show that our kernels can achieve 50x speed-ups over a vanillaPyTorch implementation and allow 40x larger hidden sizes compared to our Tritonimplementation. Our open-source kernels and the optimization library arereleased here to boost research in the direction of state-tracking enabled RNNsand sequence modeling: \url{https://github.com/NX-AI/flashrnn}</description><author>Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter</author><pubDate>Mon, 13 Jan 2025 17:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07752v2</guid></item><item><title>Improving DeFi Accessibility through Efficient Liquidity Provisioning with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2501.07508v1</link><description>This paper applies deep reinforcement learning (DRL) to optimize liquidityprovisioning in Uniswap v3, a decentralized finance (DeFi) protocolimplementing an automated market maker (AMM) model with concentrated liquidity.We model the liquidity provision task as a Markov Decision Process (MDP) andtrain an active liquidity provider (LP) agent using the Proximal PolicyOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positionsby using information about price dynamics to balance fee maximization andimpermanent loss mitigation. We use a rolling window approach for training andtesting, reflecting realistic market conditions and regime shifts. This studycompares the data-driven performance of the DRL-based strategy against commonheuristics adopted by small retail LP actors that do not systematically modifytheir liquidity positions. By promoting more efficient liquidity management,this work aims to make DeFi markets more accessible and inclusive for a broaderrange of participants. Through a data-driven approach to liquidity management,this work seeks to contribute to the ongoing development of more efficient anduser-friendly DeFi markets.</description><author>Haonan Xu, Alessio Brini</author><pubDate>Mon, 13 Jan 2025 17:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07508v1</guid></item><item><title>Inductive Learning of Robot Task Knowledge from Raw Data and Online Expert Feedback</title><link>http://arxiv.org/abs/2501.07507v1</link><description>The increasing level of autonomy of robots poses challenges of trust andsocial acceptance, especially in human-robot interaction scenarios. Thisrequires an interpretable implementation of robotic cognitive capabilities,possibly based on formal methods as logics for the definition of taskspecifications. However, prior knowledge is often unavailable in complexrealistic scenarios. In this paper, we propose an offline algorithm based on inductive logicprogramming from noisy examples to extract task specifications (i.e., actionpreconditions, constraints and effects) directly from raw data of fewheterogeneous (i.e., not repetitive) robotic executions. Our algorithmleverages on the output of any unsupervised action identification algorithmfrom video-kinematic recordings. Combining it with the definition of verybasic, almost task-agnostic, commonsense concepts about the environment, whichcontribute to the interpretability of our methodology, we are able to learnlogical axioms encoding preconditions of actions, as well as their effects inthe event calculus paradigm. Since the quality of learned specificationsdepends mainly on the accuracy of the action identification algorithm, we alsopropose an online framework for incremental refinement of task knowledge fromuser feedback, guaranteeing safe execution. Results in a standard manipulationtask and benchmark for user training in the safety-critical surgical roboticscenario, show the robustness, data- and time-efficiency of our methodology,with promising results towards the scalability in more complex domains.</description><author>Daniele Meli, Paolo Fiorini</author><pubDate>Mon, 13 Jan 2025 17:25:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07507v1</guid></item><item><title>Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance</title><link>http://arxiv.org/abs/2501.05379v2</link><description>Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) inreconstructing detailed 3D scenes within multi-view setups and the emergence oflarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-basedmethod utilizing a human face foundation model as guidance with just a singleimage as input. To achieve that, we extend such a model for diverse-view humanhead generation by fine-tuning on synthetic data and modifying itsconditioning. Our avatars maintain a dense correspondence with a human facemesh template, allowing blendshape-based expression generation. This isachieved through a modified 3DGS approach, connectivity regularizers, and astrategic initialization tailored for our task. Additionally, we propose anoptional efficient SDS-based correction step to refine the blendshapeexpressions, enhancing realism and diversity. Experiments demonstrate thatArc2Avatar achieves state-of-the-art realism and identity preservation,effectively addressing color issues by allowing the use of very low guidance,enabled by our strong identity prior and initialization strategy, withoutcompromising detail. Please visit https://arc2avatar.github.io for moreresources.</description><author>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou</author><pubDate>Mon, 13 Jan 2025 17:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05379v2</guid></item><item><title>RbRL2.0: Integrated Reward and Policy Learning for Rating-based Reinforcement Learning</title><link>http://arxiv.org/abs/2501.07502v1</link><description>Reinforcement learning (RL), a common tool in decision making, learnspolicies from various experiences based on the associated cumulativereturn/rewards without treating them differently. On the contrary, humans oftenlearn to distinguish from different levels of performance and extract theunderlying trends towards improving their decision making for best performance.Motivated by this, this paper proposes a novel RL method that mimics humans'decision making process by differentiating among collected experiences foreffective policy learning. The main idea is to extract important directionalinformation from experiences with different performance levels, named ratings,so that policies can be updated towards desired deviation from theseexperiences with different ratings. Specifically, we propose a new policy lossfunction that penalizes distribution similarities between the current policyand failed experiences with different ratings, and assign different weights tothe penalty terms based on the rating classes. Meanwhile, reward learning fromthese rated samples can be integrated with the new policy loss towards anintegrated reward and policy learning from rated samples. Optimizing theintegrated reward and policy loss function will lead to the discovery ofdirections for policy improvement towards maximizing cumulative rewards andpenalizing most from the lowest performance level while least from the highestperformance level. To evaluate the effectiveness of the proposed method, wepresent results for experiments on a few typical environments that showimproved convergence and overall performance over the existing rating-basedreinforcement learning method with only reward learning.</description><author>Mingkang Wu, Devin White, Vernon Lawhern, Nicholas R. Waytowich, Yongcan Cao</author><pubDate>Mon, 13 Jan 2025 17:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07502v1</guid></item><item><title>Three-view Focal Length Recovery From Homographies</title><link>http://arxiv.org/abs/2501.07499v1</link><description>In this paper, we propose a novel approach for recovering focal lengths fromthree-view homographies. By examining the consistency of normal vectors betweentwo homographies, we derive new explicit constraints between the focal lengthsand homographies using an elimination technique. We demonstrate that three-viewhomographies provide two additional constraints, enabling the recovery of oneor two focal lengths. We discuss four possible cases, including three camerashaving an unknown equal focal length, three cameras having two differentunknown focal lengths, three cameras where one focal length is known, and theother two cameras have equal or different unknown focal lengths. All theproblems can be converted into solving polynomials in one or two unknowns,which can be efficiently solved using Sturm sequence or hidden variabletechnique. Evaluation using both synthetic and real data shows that theproposed solvers are both faster and more accurate than methods relying onexisting two-view solvers. The code and data are available onhttps://github.com/kocurvik/hf</description><author>Yaqing Ding, Viktor Kocur, Zuzana Berger Haladová, Qianliang Wu, Shen Cai, Jian Yang, Zuzana Kukelova</author><pubDate>Mon, 13 Jan 2025 17:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07499v1</guid></item><item><title>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</title><link>http://arxiv.org/abs/2401.10815v2</link><description>Language-supervised pre-training has proven to be a valuable method forextracting semantically meaningful features from images, serving as afoundational element in multimodal systems within the computer vision andmedical imaging domains. However, the computed features are limited by theinformation contained in the text, which is particularly problematic in medicalimaging, where the findings described by radiologists focus on specificobservations. This challenge is compounded by the scarcity of pairedimaging-text data due to concerns over leakage of personal health information.In this work, we fundamentally challenge the prevailing reliance on languagesupervision for learning general-purpose biomedical imaging encoders. Weintroduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodalbiomedical imaging data that obtains similar or greater performance thanstate-of-the-art biomedical language-supervised models on a diverse range ofbenchmarks. Specifically, the quality of learned representations is evaluatedon standard imaging tasks (classification and semantic segmentation), and avision-language alignment task (text report generation from images). To furtherdemonstrate the drawback of language supervision, we show that features fromRAD-DINO correlate with other medical records (e.g., sex or age) better thanlanguage-supervised models, which are generally not mentioned in radiologyreports. Finally, we conduct a series of ablations determining the factors inRAD-DINO's performance; notably, we observe that RAD-DINO's downstreamperformance scales well with the quantity and diversity of training data,demonstrating that image-only supervision is a scalable approach for training afoundational biomedical image encoder. Model weights of RAD-DINO trained onpublicly available datasets are available athttps://huggingface.co/microsoft/rad-dino.</description><author>Fernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, Ozan Oktay</author><pubDate>Mon, 13 Jan 2025 17:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10815v2</guid></item><item><title>Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method</title><link>http://arxiv.org/abs/2501.07496v1</link><description>Weakly supervised violence detection refers to the technique of trainingmodels to identify violent segments in videos using only video-level labels.Among these approaches, multimodal violence detection, which integratesmodalities such as audio and optical flow, holds great potential. Existingmethods in this domain primarily focus on designing multimodal fusion models toaddress modality discrepancies. In contrast, we take a different approach;leveraging the inherent discrepancies across modalities in violence eventrepresentation to propose a novel multimodal semantic feature alignment method.This method sparsely maps the semantic features of local, transient, and lessinformative modalities ( such as audio and optical flow ) into the moreinformative RGB semantic feature space. Through an iterative process, themethod identifies the suitable no-zero feature matching subspace and aligns themodality-specific event representations based on this subspace, enabling thefull exploitation of information from all modalities during the subsequentmodality fusion stage. Building on this, we design a new weakly supervisedviolence detection framework that consists of unimodal multiple-instancelearning for extracting unimodal semantic features, multimodal alignment,multimodal fusion, and final detection. Experimental results on benchmarkdatasets demonstrate the effectiveness of our method, achieving an averageprecision (AP) of 86.07% on the XD-Violence dataset. Our code is available athttps://github.com/xjpp2016/MAVD.</description><author>Wenping Jin, Li Zhu, Jing Sun</author><pubDate>Mon, 13 Jan 2025 17:14:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07496v1</guid></item><item><title>Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models</title><link>http://arxiv.org/abs/2405.14496v5</link><description>Learning the unique directed acyclic graph corresponding to an unknown causalmodel is a challenging task. Methods based on functional causal models canidentify a unique graph, but either suffer from the curse of dimensionality orimpose strong parametric assumptions. To address these challenges, we propose anovel hybrid approach for global causal discovery in observational data thatleverages local causal substructures. We first present a topological sortingalgorithm that leverages ancestral relationships in linear structural causalmodels to establish a compact top-down hierarchical ordering, encoding morecausal information than linear orderings produced by existing methods. Wedemonstrate that this approach generalizes to nonlinear settings with arbitrarynoise. We then introduce a nonparametric constraint-based algorithm that prunesspurious edges by searching for local conditioning sets, achieving greateraccuracy than current methods. We provide theoretical guarantees forcorrectness and worst-case polynomial time complexities, with empiricalvalidation on synthetic data.</description><author>Sujai Hiremath, Jacqueline R. M. A. Maasch, Mengxiao Gao, Promit Ghosal, Kyra Gan</author><pubDate>Mon, 13 Jan 2025 17:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14496v5</guid></item><item><title>Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards</title><link>http://arxiv.org/abs/2501.07493v1</link><description>It is now common to evaluate Large Language Models (LLMs) by having humansmanually vote to evaluate model outputs, in contrast to typical benchmarks thatevaluate knowledge or skill at some particular task. Chatbot Arena, the mostpopular benchmark of this type, ranks models by asking users to select thebetter response between two randomly selected models (without revealing whichmodel was responsible for the generations). These platforms are widely trustedas a fair and accurate measure of LLM capabilities. In this paper, we show thatif bot protection and other defenses are not implemented, these voting-basedbenchmarks are potentially vulnerable to adversarial manipulation.Specifically, we show that an attacker can alter the leaderboard (to promotetheir favorite model or demote competitors) at the cost of roughly a thousandvotes (verified in a simulated, offline version of Chatbot Arena). Our attackconsists of two steps: first, we show how an attacker can determine which modelwas used to generate a given reply with more than $95\%$ accuracy; and then,the attacker can use this information to consistently vote for (or against) atarget model. Working with the Chatbot Arena developers, we identify, propose,and implement mitigations to improve the robustness of Chatbot Arena againstadversarial manipulation, which, based on our analysis, substantially increasesthe cost of such attacks. Some of these defenses were present before ourcollaboration, such as bot protection with Cloudflare, malicious userdetection, and rate limiting. Others, including reCAPTCHA and login are beingintegrated to strengthen the security in Chatbot Arena.</description><author>Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, Chiyuan Zhang</author><pubDate>Mon, 13 Jan 2025 17:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07493v1</guid></item><item><title>Data and System Perspectives of Sustainable Artificial Intelligence</title><link>http://arxiv.org/abs/2501.07487v1</link><description>Sustainable AI is a subfield of AI for concerning developing and using AIsystems in ways of aiming to reduce environmental impact and achievesustainability. Sustainable AI is increasingly important given that training ofand inference with AI models such as large langrage models are consuming alarge amount of computing power. In this article, we discuss current issues,opportunities and example solutions for addressing these issues, and futurechallenges to tackle, from the data and system perspectives, related to dataacquisition, data processing, and AI model training and inference.</description><author>Tao Xie, David Harel, Dezhi Ran, Zhenwen Li, Maoliang Li, Zhi Yang, Leye Wang, Xiang Chen, Ying Zhang, Wentao Zhang, Meng Li, Chen Zhang, Linyi Li, Assaf Marron</author><pubDate>Mon, 13 Jan 2025 17:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07487v1</guid></item><item><title>Smart Learning in the 21st Century: Advancing Constructionism Across Three Digital Epochs</title><link>http://arxiv.org/abs/2501.07486v1</link><description>This article explores the evolution of constructionism as an educationalframework, tracing its relevance and transformation across three pivotal eras:the advent of personal computing, the networked society, and the current era ofgenerative AI. Rooted in Seymour Papert constructionist philosophy, this studyexamines how constructionist principles align with the expanding role ofdigital technology in personal and collective learning. We discuss thetransformation of educational environments from hierarchical instructionism toconstructionist models that emphasize learner autonomy and interactive,creative engagement. Central to this analysis is the concept of an expandedpersonality, wherein digital tools and AI integration fundamentally reshapeindividual self-perception and social interactions. By integratingconstructionism into the paradigm of smart education, we propose it as afoundational approach to personalized and democratized learning. Our findingsunderscore constructionism enduring relevance in navigating the complexities oftechnology-driven education, providing insights for educators and policymakersseeking to harness digital innovations to foster adaptive, student-centeredlearning experiences.</description><author>Ilya Levin, Alexei L. Semenov, Mikael Gorsky</author><pubDate>Mon, 13 Jan 2025 17:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07486v1</guid></item><item><title>Controlling Equational Reasoning in Large Language Models with Prompt Interventions</title><link>http://arxiv.org/abs/2307.09998v5</link><description>This paper investigates how hallucination rates in Large Language Models(LLMs) may be controlled via a symbolic data generation framework, exploring afundamental relationship between the rate of certain mathematical errors andtypes of input intervention. Specifically, we systematically generate data fora derivation generation task using a symbolic engine, applying targetedinterventions to prompts to perturb features of mathematical derivations suchas the surface forms of symbols, equational tree structures, and mathematicalcontext. We then evaluate the effect of prompt interventions across a range ofLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Ourexperiments suggest that T5-Large can outperform the few-shot performance ofGPT-4 on various evaluation sets generated via the framework. However, anextensive evaluation based on human analysis, template-based error detection,and text generation metrics reveals model weaknesses beyond what thereference-based metrics singularly describe. We use these results to tiecharacteristic distributional footprints of interventions to the humanevaluation of LLM derivation quality, potentially leading to significantcontrol over fine-grained mathematical capabilities of language models withrespect to specific types of errors.</description><author>Jordan Meadows, Marco Valentino, Andre Freitas</author><pubDate>Mon, 13 Jan 2025 17:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09998v5</guid></item><item><title>A Unified Approach to Extract Interpretable Rules from Tree Ensembles via Integer Programming</title><link>http://arxiv.org/abs/2407.00843v3</link><description>Tree ensembles are very popular machine learning models, known for theireffectiveness in supervised classification and regression tasks. Theirperformance derives from aggregating predictions of multiple decision trees,which are renowned for their interpretability properties. However, treeensemble models do not reliably exhibit interpretable output. Our work aims toextract an optimized list of rules from a trained tree ensemble, providing theuser with a condensed, interpretable model that retains most of the predictivepower of the full model. Our approach consists of solving a set partitioningproblem formulated through Integer Programming. The proposed method works witheither tabular or time series data, for both classification and regressiontasks, and its flexible formulation can include any arbitrary loss orregularization functions. Our extensive computational experiments offerstatistically significant evidence that our method is competitive with otherrule extraction methods in terms of predictive performance and fidelity towardsthe tree ensemble. Moreover, we empirically show that the proposed methodeffectively extracts interpretable rules from tree ensemble that are designedfor time series data.</description><author>Lorenzo Bonasera, Emilio Carrizosa</author><pubDate>Mon, 13 Jan 2025 16:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00843v3</guid></item><item><title>TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language Models</title><link>http://arxiv.org/abs/2501.07482v1</link><description>In a rapidly evolving knowledge landscape and the increasing adoption oflarge language models, a need has emerged to keep these models continuouslyupdated with current events. While existing benchmarks evaluate general factualrecall, they often overlook two critical aspects: the ability of models tointegrate evolving knowledge through continual learning and the significantregional disparities in their performance. To address these gaps, we introducethe Timely Events Benchmark (TiEBe), a dataset containing over 11,000question-answer pairs focused on globally and regionally significant events.TiEBe leverages structured retrospective data from Wikipedia, enablingcontinuous updates to assess LLMs' knowledge of evolving global affairs andtheir understanding of events across different regions. Our benchmarkdemonstrates that LLMs exhibit substantial geographic disparities in factualrecall, emphasizing the need for more balanced global knowledge representation.Furthermore, TiEBe serves as a tool for evaluating continual learningstrategies, providing insights into models' ability to acquire new informationwithout forgetting past knowledge.</description><author>Thales Sales Almeida, Giovana Kerche Bonás, João Guilherme Alves Santos, Hugo Abonizio, Rodrigo Nogueira</author><pubDate>Mon, 13 Jan 2025 16:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07482v1</guid></item><item><title>Agentic Copyright Watermarking against Adversarial Evidence Forgery with Purification-Agnostic Curriculum Proxy Learning</title><link>http://arxiv.org/abs/2409.01541v2</link><description>With the proliferation of AI agents in various domains, protecting theownership of AI models has become crucial due to the significant investment intheir development. Unauthorized use and illegal distribution of these modelspose serious threats to intellectual property, necessitating effectivecopyright protection measures. Model watermarking has emerged as a keytechnique to address this issue, embedding ownership information within modelsto assert rightful ownership during copyright disputes. This paper presentsseveral contributions to model watermarking: a self-authenticating black-boxwatermarking protocol using hash techniques, a study on evidence forgeryattacks using adversarial perturbations, a proposed defense involving apurification step to counter adversarial attacks, and a purification-agnosticcurriculum proxy learning method to enhance watermark robustness and modelperformance. Experimental results demonstrate the effectiveness of theseapproaches in improving the security, reliability, and performance ofwatermarked models.</description><author>Erjin Bao, Ching-Chun Chang, Hanrui Wang, Isao Echizen</author><pubDate>Mon, 13 Jan 2025 16:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01541v2</guid></item><item><title>Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering</title><link>http://arxiv.org/abs/2410.16314v3</link><description>Large language models have transformed AI, yet reliably controlling theiroutputs remains a challenge. This paper explores activation engineering, whereoutputs of pre-trained LLMs are controlled by manipulating their activations atinference time. Unlike traditional methods using a single steering vector, weintroduce conceptors - mathematical constructs that represent sets ofactivation vectors as ellipsoidal regions. Conceptors act as soft projectionmatrices and offer more precise control over complex activation patterns. Ourexperiments demonstrate that conceptors outperform traditional methods acrossmultiple steering tasks. We further use Boolean operations on conceptors forcombined steering goals that empirically outperform additively combiningsteering vectors on a set of tasks. These results highlight conceptors as apromising tool for more effective steering of LLMs. Our code is available ongithub.com/jorispos/conceptorsteering.</description><author>Joris Postmus, Steven Abreu</author><pubDate>Mon, 13 Jan 2025 16:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16314v3</guid></item><item><title>3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh</title><link>http://arxiv.org/abs/2501.07478v1</link><description>3D Gaussian Splatting (3DGS) excels at producing highly detailed 3Dreconstructions, but these scenes often require specialised renderers foreffective visualisation. In contrast, point clouds are a widely used 3Drepresentation and are compatible with most popular 3D processing software, yetconverting 3DGS scenes into point clouds is a complex challenge. In this workwe introduce 3DGS-to-PC, a flexible and highly customisable framework that iscapable of transforming 3DGS scenes into dense, high-accuracy point clouds. Wesample points probabilistically from each Gaussian as a 3D density function. Weadditionally threshold new points using the Mahalanobis distance to theGaussian centre, preventing extreme outliers. The result is a point cloud thatclosely represents the shape encoded into the 3D Gaussian scene. IndividualGaussians use spherical harmonics to adapt colours depending on view, and eachpoint may contribute only subtle colour hints to the resulting rendered scene.To avoid spurious or incorrect colours that do not fit with the final pointcloud, we recalculate Gaussian colours via a customised image renderingapproach, assigning each Gaussian the colour of the pixel to which itcontributes most across all views. 3DGS-to-PC also supports mesh generationthrough Poisson Surface Reconstruction, applied to points sampled frompredicted surface Gaussians. This allows coloured meshes to be generated from3DGS scenes without the need for re-training. This package is highlycustomisable and capability of simple integration into existing 3DGS pipelines.3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloudand surface-based formats.</description><author>Lewis A G Stuart, Michael P Pound</author><pubDate>Mon, 13 Jan 2025 16:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07478v1</guid></item><item><title>Estimating Musical Surprisal in Audio</title><link>http://arxiv.org/abs/2501.07474v1</link><description>In modeling musical surprisal expectancy with computational methods, it hasbeen proposed to use the information content (IC) of one-step predictions froman autoregressive model as a proxy for surprisal in symbolic music. With anappropriately chosen model, the IC of musical events has been shown tocorrelate with human perception of surprise and complexity aspects, includingtonal and rhythmic complexity. This work investigates whether an analogousmethodology can be applied to music audio. We train an autoregressiveTransformer model to predict compressed latent audio representations of apretrained autoencoder network. We verify learning effects by estimating thedecrease in IC with repetitions. We investigate the mean IC of musical segmenttypes (e.g., A or B) and find that segment types appearing later in a piecehave a higher IC than earlier ones on average. We investigate the IC's relationto audio and musical features and find it correlated with timbral variationsand loudness and, to a lesser extent, dissonance, rhythmic complexity, andonset density related to audio and musical features. Finally, we investigate ifthe IC can predict EEG responses to songs and thus model humans' surprisal inmusic. We provide code for our method on github.com/sonycslparis/audioic.</description><author>Mathias Rose Bjare, Giorgia Cantisani, Stefan Lattner, Gerhard Widmer</author><pubDate>Mon, 13 Jan 2025 16:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07474v1</guid></item><item><title>Rethinking Decoders for Transformer-based Semantic Segmentation: A Compression Perspective</title><link>http://arxiv.org/abs/2411.03033v2</link><description>State-of-the-art methods for Transformer-based semantic segmentationtypically adopt Transformer decoders that are used to extract additionalembeddings from image embeddings via cross-attention, refine either or bothtypes of embeddings via self-attention, and project image embeddings onto theadditional embeddings via dot-product. Despite their remarkable success, theseempirical designs still lack theoretical justifications or interpretations,thus hindering potentially principled improvements. In this paper, we arguethat there are fundamental connections between semantic segmentation andcompression, especially between the Transformer decoders and PrincipalComponent Analysis (PCA). From such a perspective, we derive a white-box, fullyattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with theinterpretations as follows: 1) the self-attention operator refines imageembeddings to construct an ideal principal subspace that aligns with thesupervision and retains most information; 2) the cross-attention operator seeksto find a low-rank approximation of the refined image embeddings, which isexpected to be a set of orthonormal bases of the principal subspace andcorresponds to the predefined classes; 3) the dot-product operation yieldscompact representation for image embeddings as segmentation masks. Experimentsconducted on dataset ADE20K find that DEPICT consistently outperforms itsblack-box counterpart, Segmenter, and it is light weight and more robust.</description><author>Qishuai Wen, Chun-Guang Li</author><pubDate>Mon, 13 Jan 2025 16:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03033v2</guid></item><item><title>A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities</title><link>http://arxiv.org/abs/2501.07468v1</link><description>Healthcare systems worldwide face persistent challenges in efficiency,accessibility, and personalization. Powered by modern AI technologies such asmultimodal large language models and world models, Embodied AI (EmAI)represents a transformative frontier, offering enhanced autonomy and theability to interact with the physical world to address these challenges. As aninterdisciplinary and rapidly evolving research domain, "EmAI in healthcare"spans diverse fields such as algorithms, robotics, and biomedicine. Thiscomplexity underscores the importance of timely reviews and analyses to trackadvancements, address challenges, and foster cross-disciplinary collaboration.In this paper, we provide a comprehensive overview of the "brain" of EmAI forhealthcare, wherein we introduce foundational AI algorithms for perception,actuation, planning, and memory, and focus on presenting the healthcareapplications spanning clinical interventions, daily care &amp; companionship,infrastructure support, and biomedical research. Despite its promise, thedevelopment of EmAI for healthcare is hindered by critical challenges such assafety concerns, gaps between simulation platforms and real-world applications,the absence of standardized benchmarks, and uneven progress acrossinterdisciplinary domains. We discuss the technical barriers and exploreethical considerations, offering a forward-looking perspective on the future ofEmAI in healthcare. A hierarchical framework of intelligent levels for EmAIsystems is also introduced to guide further development. By providingsystematic insights, this work aims to inspire innovation and practicalapplications, paving the way for a new era of intelligent, patient-centeredhealthcare.</description><author>Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen</author><pubDate>Mon, 13 Jan 2025 16:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07468v1</guid></item><item><title>Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is Not AGI</title><link>http://arxiv.org/abs/2501.07458v1</link><description>OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposedto measure intelligence. This raises the question whether systems based onLarge Language Models (LLMs), particularly o3, demonstrate intelligence andprogress towards artificial general intelligence (AGI). Building on thedistinction between skills and intelligence made by Fran\c{c}ois Chollet, thecreator of ARC-AGI, a new understanding of intelligence is introduced: an agentis the more intelligent, the more efficiently it can achieve the more diversegoals in the more diverse worlds with the less knowledge. An analysis of theARC-AGI benchmark shows that its tasks represent a very specific type ofproblem that can be solved by massive trialling of combinations of predefinedoperations. This method is also applied by o3, achieving its high score throughthe extensive use of computing power. However, for most problems in thephysical world and in the human domain, solutions cannot be tested in advanceand predefined operations are not available. Consequently, massive trialling ofpredefined operations, as o3 does, cannot be a basis for AGI - instead, newapproaches are required that can reliably solve a wide variety of problemswithout existing skills. To support this development, a new benchmark forintelligence is outlined that covers a much higher diversity of unknown tasksto be solved, thus enabling a comprehensive assessment of intelligence and ofprogress towards AGI.</description><author>Rolf Pfister, Hansueli Jud</author><pubDate>Mon, 13 Jan 2025 16:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07458v1</guid></item><item><title>ScVLM: Enhancing Vision-Language Model for Safety-Critical Event Understanding</title><link>http://arxiv.org/abs/2410.00982v2</link><description>Accurately identifying, understanding and describing traffic safety-criticalevents (SCEs), including crashes, tire strikes, and near-crashes, is crucialfor advanced driver assistance systems, automated driving systems, and trafficsafety. As SCEs are rare events, most general vision-language models (VLMs)have not been trained sufficiently to link SCE videos and narratives, whichcould lead to hallucinations and missing key safety characteristics. Here, weintroduce ScVLM, a novel hybrid methodology that integrates supervised andcontrastive learning techniques to classify the severity and types of SCEs, aswell as to generate narrative descriptions of SCEs. This approach utilizesclassification to enhance VLMs' comprehension of driving videos and improve therationality of event descriptions. The proposed approach is trained on andevaluated by more than 8,600 SCEs from the Second Strategic Highway ResearchProgram Naturalistic Driving Study dataset, the largest publicly accessibledriving dataset with videos and SCE annotations. The results demonstrate thesuperiority of the proposed approach in generating contextually accurate eventdescriptions and mitigating VLM hallucinations. The code will be available athttps://github.com/datadrivenwheels/ScVLM.</description><author>Liang Shi, Boyu Jiang, Tong Zeng, Feng Guo</author><pubDate>Mon, 13 Jan 2025 16:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00982v2</guid></item><item><title>A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion</title><link>http://arxiv.org/abs/2501.07451v1</link><description>Model compression is essential in the deployment of large Computer Visionmodels on embedded devices. However, static optimization techniques (e.g.pruning, quantization, etc.) neglect the fact that different inputs havedifferent complexities, thus requiring different amount of computations.Dynamic Neural Networks allow to condition the number of computations to thespecific input. The current literature on the topic is very extensive andfragmented. We present a comprehensive survey that synthesizes and unifiesexisting Dynamic Neural Networks research in the context of Computer Vision.Additionally, we provide a logical taxonomy based on which component of thenetwork is adaptive: the output, the computation graph or the input.Furthermore, we argue that Dynamic Neural Networks are particularly beneficialin the context of Sensor Fusion for better adaptivity, noise reduction andinformation prioritization. We present preliminary works in this direction.</description><author>Fabio Montello, Ronja Güldenring, Simone Scardapane, Lazaros Nalpantidis</author><pubDate>Mon, 13 Jan 2025 16:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07451v1</guid></item><item><title>Automation of Quantum Dot Measurement Analysis via Explainable Machine Learning</title><link>http://arxiv.org/abs/2402.13699v5</link><description>The rapid development of quantum dot (QD) devices for quantum computing hasnecessitated more efficient and automated methods for device characterizationand tuning. This work demonstrates the feasibility and advantages of applyingexplainable machine learning techniques to the analysis of quantum dotmeasurements, paving the way for further advances in automated and transparentQD device tuning. Many of the measurements acquired during the tuning processcome in the form of images that need to be properly analyzed to guide thesubsequent tuning steps. By design, features present in such images capturecertain behaviors or states of the measured QD devices. When consideredcarefully, such features can aid the control and calibration of QD devices. Animportant example of such images are so-called $\textit{triangle plots}$, whichvisually represent current flow and reveal characteristics important for QDdevice calibration. While image-based classification tools, such asconvolutional neural networks (CNNs), can be used to verify whether a givenmeasurement is $\textit{good}$ and thus warrants the initiation of the nextphase of tuning, they do not provide any insights into how the device should beadjusted in the case of $\textit{bad}$ images. This is because CNNs sacrificeprediction and model intelligibility for high accuracy. To ameliorate thistrade-off, a recent study introduced an image vectorization approach thatrelies on the Gabor wavelet transform (Schug $\textit{et al.}$ 2024$\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternativevectorization method that involves mathematical modeling of synthetic trianglesto mimic the experimental data. Using explainable boosting machines, we showthat this new method offers superior explainability of model prediction withoutsacrificing accuracy.</description><author>Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak</author><pubDate>Mon, 13 Jan 2025 16:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13699v5</guid></item><item><title>PrecipDiff: Leveraging image diffusion models to enhance satellite-based precipitation observations</title><link>http://arxiv.org/abs/2501.07447v1</link><description>A recent report from the World Meteorological Organization (WMO) highlightsthat water-related disasters have caused the highest human losses among naturaldisasters over the past 50 years, with over 91\% of deaths occurring inlow-income countries. This disparity is largely due to the lack of adequateground monitoring stations, such as weather surveillance radars (WSR), whichare expensive to install. For example, while the US and Europe combined possessover 600 WSRs, Africa, despite having almost one and half times their landmass,has fewer than 40. To address this issue, satellite-based observations offer aglobal, near-real-time monitoring solution. However, they face severalchallenges like accuracy, bias, and low spatial resolution. This studyleverages the power of diffusion models and residual learning to address theselimitations in a unified framework. We introduce the first diffusion model forcorrecting the inconsistency between different precipitation products. Ourmethod demonstrates the effectiveness in downscaling satellite precipitationestimates from 10 km to 1 km resolution. Extensive experiments conducted in theSeattle region demonstrate significant improvements in accuracy, biasreduction, and spatial detail. Importantly, our approach achieves these resultsusing only precipitation data, showcasing the potential of a purely computervision-based approach for enhancing satellite precipitation products and pavingthe way for further advancements in this domain.</description><author>Ting-Yu Dai, Hayato Ushijima-Mwesigwa</author><pubDate>Mon, 13 Jan 2025 16:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07447v1</guid></item><item><title>Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport</title><link>http://arxiv.org/abs/2501.07446v1</link><description>We consider synthesis and analysis of probability measures using theentropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorndivergence. The synthesis problem consists of computing the barycenter, withrespect to these costs, of $m$ reference measures given a set of coefficientsbelonging to the $m$-dimensional simplex. The analysis problem consists offinding the coefficients for the closest barycenter in the Wasserstein-2distance to a given measure $\mu$. Under the weakest assumptions on themeasures thus far in the literature, we compute the derivative of theentropy-regularized Wasserstein-2 cost. We leverage this to establish acharacterization of regularized barycenters as solutions to a fixed-pointequation for the average of the entropic maps from the barycenter to thereference measures. This characterization yields a finite-dimensional, convex,quadratic program for solving the analysis problem when $\mu$ is a barycenter.It is shown that these coordinates, as well as the value of the barycenterfunctional, can be estimated from samples with dimension-independent rates ofconvergence, a hallmark of entropy-regularized optimal transport, and we verifythese rates experimentally. We also establish that barycentric coordinates arestable with respect to perturbations in the Wasserstein-2 metric, suggesting arobustness of these coefficients to corruptions. We employ the barycentriccoefficients as features for classification of corrupted point cloud data, andshow that compared to neural network baselines, our approach is more efficientin small training data regimes.</description><author>Brendan Mallery, James M. Murphy, Shuchin Aeron</author><pubDate>Mon, 13 Jan 2025 16:16:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07446v1</guid></item><item><title>Online inductive learning from answer sets for efficient reinforcement learning exploration</title><link>http://arxiv.org/abs/2501.07445v1</link><description>This paper presents a novel approach combining inductive logic programmingwith reinforcement learning to improve training performance and explainability.We exploit inductive learning of answer set programs from noisy examples tolearn a set of logical rules representing an explainable approximation of theagent policy at each batch of experience. We then perform answer set reasoningon the learned rules to guide the exploration of the learning agent at the nextbatch, without requiring inefficient reward shaping and preserving optimalitywith soft bias. The entire procedure is conducted during the online executionof the reinforcement learning algorithm. We preliminarily validate the efficacyof our approach by integrating it into the Q-learning algorithm for the Pac-Manscenario in two maps of increasing complexity. Our methodology produces asignificant boost in the discounted return achieved by the agent, even in thefirst batches of training. Moreover, inductive learning does not compromise thecomputational time required by Q-learning and learned rules quickly converge toan explanation of the agent policy.</description><author>Celeste Veronese, Daniele Meli, Alessandro Farinelli</author><pubDate>Mon, 13 Jan 2025 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07445v1</guid></item><item><title>Attention when you need</title><link>http://arxiv.org/abs/2501.07440v1</link><description>Being attentive to task-relevant features can improve task performance, butpaying attention comes with its own metabolic cost. Therefore, strategicallocation of attention is crucial in performing the task efficiently. Thiswork aims to understand this strategy. Recently, de Gee et al. conductedexperiments involving mice performing an auditory sustained attention-valuetask. This task required the mice to exert attention to identify whether ahigh-order acoustic feature was present amid the noise. By varying the trialduration and reward magnitude, the task allows us to investigate how an agentshould strategically deploy their attention to maximize their benefits andminimize their costs. In our work, we develop a reinforcement learning-basednormative model of the mice to understand how it balances attention costagainst its benefits. The model is such that at each moment the mice can choosebetween two levels of attention and decide when to take costly actions thatcould obtain rewards. Our model suggests that efficient use of attentionalresources involves alternating blocks of high attention with blocks of lowattention. In the extreme case where the agent disregards sensory input duringlow attention states, we see that high attention is used rhythmically. Ourmodel provides evidence about how one should deploy attention as a function oftask utility, signal statistics, and how attention affects sensory evidence.</description><author>Lokesh Boominathan, Yizhou Chen, Matthew McGinley, Xaq Pitkow</author><pubDate>Mon, 13 Jan 2025 16:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07440v1</guid></item><item><title>Class Distance Weighted Cross Entropy Loss for Classification of Disease Severity</title><link>http://arxiv.org/abs/2412.01246v3</link><description>Assessing disease severity with ordinal classes, where each class reflectsincreasing severity levels, benefits from loss functions designed for thisordinal structure. Traditional categorical loss functions, like Cross-Entropy(CE), often perform suboptimally in these scenarios. To address this, wepropose a novel loss function, Class Distance Weighted Cross-Entropy (CDW-CE),which penalizes misclassifications more severely when the predicted and actualclasses are farther apart. We evaluated CDW-CE using various deeparchitectures, comparing its performance against several categorical andordinal loss functions. To assess the quality of latent representations, weused t-distributed stochastic neighbor embedding (t-SNE) and uniform manifoldapproximation and projection (UMAP) visualizations, quantified the clusteringquality using the Silhouette Score, and compared Class Activation Maps (CAM)generated by models trained with CDW-CE and CE loss. Feedback from domainexperts was incorporated to evaluate how well model attention aligns withexpert opinion. Our results show that CDW-CE consistently improves performancein ordinal image classification tasks. It achieves higher Silhouette Scores,indicating better class discrimination capability, and its CAM visualizationsshow a stronger focus on clinically significant regions, as validated by domainexperts. Receiver operator characteristics (ROC) curves and the area under thecurve (AUC) scores highlight that CDW-CE outperforms other loss functions,including prominent ordinal loss functions from the literature.</description><author>Gorkem Polat, Ümit Mert Çağlar, Alptekin Temizel</author><pubDate>Mon, 13 Jan 2025 16:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01246v3</guid></item><item><title>Pairwise Comparisons without Stochastic Transitivity: Model, Theory and Applications</title><link>http://arxiv.org/abs/2501.07437v1</link><description>Most statistical models for pairwise comparisons, including the Bradley-Terry(BT) and Thurstone models and many extensions, make a relatively strongassumption of stochastic transitivity. This assumption imposes the existence ofan unobserved global ranking among all the players/teams/items and monotoneconstraints on the comparison probabilities implied by the global ranking.However, the stochastic transitivity assumption does not hold in manyreal-world scenarios of pairwise comparisons, especially games involvingmultiple skills or strategies. As a result, models relying on this assumptioncan have suboptimal predictive performance. In this paper, we propose a generalfamily of statistical models for pairwise comparison data without a stochastictransitivity assumption, substantially extending the BT and Thurstone models.In this model, the pairwise probabilities are determined by a (approximately)low-dimensional skew-symmetric matrix. Likelihood-based estimation methods andcomputational algorithms are developed, which allow for sparse data with only asmall proportion of observed pairs. Theoretical analysis shows that theproposed estimator achieves minimax-rate optimality, which adapts effectivelyto the sparsity level of the data. The spectral theory for skew-symmetricmatrices plays a crucial role in the implementation and theoretical analysis.The proposed method's superiority against the BT model, along with its broadapplicability across diverse scenarios, is further supported by simulations andreal data analysis.</description><author>Sze Ming Lee, Yunxiao Chen</author><pubDate>Mon, 13 Jan 2025 16:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07437v1</guid></item><item><title>Guided SAM: Label-Efficient Part Segmentation</title><link>http://arxiv.org/abs/2501.07434v1</link><description>Localizing object parts precisely is essential for tasks such as objectrecognition and robotic manipulation. Recent part segmentation methods requireextensive training data and labor-intensive annotations. Segment-Anything Model(SAM) has demonstrated good performance on a wide range of segmentationproblems, but requires (manual) positional prompts to guide it where tosegment. Furthermore, since it has been trained on full objects instead ofobject parts, it is prone to over-segmentation of parts. To address this, wepropose a novel approach that guides SAM towards the relevant object parts. Ourmethod learns positional prompts from coarse patch annotations that are easierand cheaper to acquire. We train classifiers on image patches to identify partclasses and aggregate patches into regions of interest (ROIs) with positionalprompts. SAM is conditioned on these ROIs and prompts. This approach, termed`Guided SAM', enhances efficiency and reduces manual effort, allowing effectivepart segmentation with minimal labeled data. We demonstrate the efficacy ofGuided SAM on a dataset of car parts, improving the average IoU on state of theart models from 0.37 to 0.49 with annotations that are on average five timesmore efficient to acquire.</description><author>S. B. van Rooij, G. J. Burghouts</author><pubDate>Mon, 13 Jan 2025 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07434v1</guid></item><item><title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title><link>http://arxiv.org/abs/2411.17645v2</link><description>The use of machine learning and AI on electronic health records (EHRs) holdssubstantial potential for clinical insight. However, this approach faceschallenges due to data heterogeneity, sparsity, temporal misalignment, andlimited labeled outcomes. In this context, we leverage a linked EHR dataset ofapproximately one million de-identified individuals from Bristol, NorthSomerset, and South Gloucestershire, UK, to characterize urinary tractinfections (UTIs). We implemented a data pre-processing and curation pipelinethat transforms the raw EHR data into a structured format suitable fordeveloping predictive models focused on data fairness, accountability andtransparency. Given the limited availability and biases of ground truth UTIoutcomes, we introduce a UTI risk estimation framework informed by clinicalexpertise to estimate UTI risk across individual patient timelines. PairwiseXGBoost models are trained using this framework to differentiate UTI riskcategories with explainable AI techniques applied to identify key predictorsand support interpretability. Our findings reveal differences in clinical anddemographic predictors across risk groups. While this study highlights thepotential of AI-driven insights to support UTI clinical decision-making,further investigation of patient sub-strata and extensive validation are neededto ensure robustness and applicability in clinical practice.</description><author>Yujie Dai, Brian Sullivan, Axel Montout, Amy Dillon, Chris Waller, Peter Acs, Rachel Denholm, Philip Williams, Alastair D Hay, Raul Santos-Rodriguez, Andrew Dowsey</author><pubDate>Mon, 13 Jan 2025 16:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17645v2</guid></item><item><title>Empirical Evaluation of the Implicit Hitting Set Approach for Weighted CSPs</title><link>http://arxiv.org/abs/2501.07432v1</link><description>SAT technology has proven to be surprisingly effective in a large variety ofdomains. However, for the Weighted CSP problem dedicated algorithms have alwaysbeen superior. One approach not well-studied so far is the use of SAT inconjunction with the Implicit Hitting Set approach. In this work, we exploresome alternatives to the existing algorithm of reference. The alternatives,mostly borrowed from related boolean frameworks, consider trade-offs for thetwo main components of the IHS approach: the computation of low-cost hittingvectors, and their transformation into high-cost cores. For each one, wepropose 4 levels of intensity. Since we also test the usefulness of costfunction merging, our experiments consider 32 different implementations. Ourempirical study shows that for WCSP it is not easy to identify the bestalternative. Nevertheless, the cost-function merging encoding and extractingmaximal cores seems to be a robust approach.</description><author>Aleksandra Petrova, Javier Larrosa, Emma Rollón</author><pubDate>Mon, 13 Jan 2025 15:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07432v1</guid></item><item><title>Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation</title><link>http://arxiv.org/abs/2501.07430v1</link><description>Despite success in volume-to-volume translations in medical images, mostexisting models struggle to effectively capture the inherent volumetricdistribution using 3D representations. The current state-of-the-art approachcombines multiple 2D-based networks through weighted averaging, therebyneglecting the 3D spatial structures. Directly training 3D models in medicalimaging presents significant challenges due to high computational demands andthe need for large-scale datasets. To address these challenges, we introduceDiff-Ensembler, a novel hybrid 2D-3D model for efficient and effectivevolumetric translations by ensembling perpendicularly trained 2D diffusionmodels with a 3D network in each diffusion step. Moreover, our model cannaturally be used to ensemble diffusion models conditioned on differentmodalities, allowing flexible and accurate fusion of input conditions.Extensive experiments demonstrate that Diff-Ensembler attains superior accuracyand volumetric realism in 3D medical image super-resolution and modalitytranslation. We further demonstrate the strength of our model's volumetricrealism using tumor segmentation as a downstream task.</description><author>Xiyue Zhu, Dou Hoon Kwark, Ruike Zhu, Kaiwen Hong, Yiqi Tao, Shirui Luo, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko</author><pubDate>Mon, 13 Jan 2025 15:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07430v1</guid></item><item><title>Distance Measure Based on an Embedding of the Manifold of K-Component Gaussian Mixture Models into the Manifold of Symmetric Positive Definite Matrices</title><link>http://arxiv.org/abs/2501.07429v1</link><description>In this paper, a distance between the Gaussian Mixture Models(GMMs) isobtained based on an embedding of the K-component Gaussian Mixture Model intothe manifold of the symmetric positive definite matrices. Proof of embedding ofK-component GMMs into the manifold of symmetric positive definite matrices isgiven and shown that it is a submanifold. Then, proved that the manifold ofGMMs with the pullback of induced metric is isometric to the submanifold withthe induced metric. Through this embedding we obtain a general lower bound forthe Fisher-Rao metric. This lower bound is a distance measure on the manifoldof GMMs and we employ it for the similarity measure of GMMs. The effectivenessof this framework is demonstrated through an experiment on standard machinelearning benchmarks, achieving accuracy of 98%, 92%, and 93.33% on the UIUC,KTH-TIPS, and UMD texture recognition datasets respectively.</description><author>Amit Vishwakarma, KS Subrahamanian Moosath</author><pubDate>Mon, 13 Jan 2025 15:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07429v1</guid></item><item><title>FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</title><link>http://arxiv.org/abs/2501.00843v2</link><description>In this work, we investigate four different fusion methods for associatingdetections to tracklets in multi-object visual tracking. In addition toconsidering strong cues such as motion and appearance information, we alsoconsider weak cues such as height intersection-over-union (height-IoU) andtracklet confidence information in the data association using different fusionmethods. These fusion methods include minimum, weighted sum based on IoU,Kalman filter (KF) gating, and hadamard product of costs due to the differentcues. We conduct extensive evaluations on validation sets of MOT17, MOT20 andDanceTrack datasets, and find out that the choice of a fusion method is key fordata association in multi-object visual tracking. We hope that thisinvestigative work helps the computer vision research community to use theright fusion method for data association in multi-object visual tracking.</description><author>Nathanael L. Baisa</author><pubDate>Mon, 13 Jan 2025 15:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00843v2</guid></item><item><title>MVICAD2: Multi-View Independent Component Analysis with Delays and Dilations</title><link>http://arxiv.org/abs/2501.07426v1</link><description>Machine learning techniques in multi-view settings face significantchallenges, particularly when integrating heterogeneous data, aligning featurespaces, and managing view-specific biases. These issues are prominent inneuroscience, where data from multiple subjects exposed to the same stimuli areanalyzed to uncover brain activity dynamics. In magnetoencephalography (MEG),where signals are captured at the scalp level, estimating the brain'sunderlying sources is crucial, especially in group studies where sources areassumed to be similar for all subjects. Common methods, such as Multi-ViewIndependent Component Analysis (MVICA), assume identical sources acrosssubjects, but this assumption is often too restrictive due to individualvariability and age-related changes. Multi-View Independent Component Analysiswith Delays (MVICAD) addresses this by allowing sources to differ up to atemporal delay. However, temporal dilation effects, particularly in auditorystimuli, are common in brain dynamics, making the estimation of time delaysalone insufficient. To address this, we propose Multi-View IndependentComponent Analysis with Delays and Dilations (MVICAD2), which allows sources todiffer across subjects in both temporal delays and dilations. We present amodel with identifiable sources, derive an approximation of its likelihood inclosed form, and use regularization and optimization techniques to enhanceperformance. Through simulations, we demonstrate that MVICAD2 outperformsexisting multi-view ICA methods. We further validate its effectiveness usingthe Cam-CAN dataset, and showing how delays and dilations are related to aging.</description><author>Ambroise Heurtebise, Omar Chehab, Pierre Ablin, Alexandre Gramfort</author><pubDate>Mon, 13 Jan 2025 15:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07426v1</guid></item><item><title>An Investigation into Seasonal Variations in Energy Forecasting for Student Residences</title><link>http://arxiv.org/abs/2501.07423v1</link><description>This research provides an in-depth evaluation of various machine learningmodels for energy forecasting, focusing on the unique challenges of seasonalvariations in student residential settings. The study assesses the performanceof baseline models, such as LSTM and GRU, alongside state-of-the-artforecasting methods, including Autoregressive Feedforward Neural Networks,Transformers, and hybrid approaches. Special attention is given to predictingenergy consumption amidst challenges like seasonal patterns, vacations,meteorological changes, and irregular human activities that cause suddenfluctuations in usage. The findings reveal that no single model consistentlyoutperforms others across all seasons, emphasizing the need for season-specificmodel selection or tailored designs. Notably, the proposed Hyper Network basedLSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonalvariations, effectively capturing abrupt changes in energy consumption duringsummer months. This study advances the energy forecasting field by emphasizingthe critical role of seasonal dynamics and model-specific behavior in achievingaccurate predictions.</description><author>Muhammad Umair Danish, Mathumitha Sureshkumar, Thanuri Fonseka, Umeshika Uthayakumar, Vinura Galwaduge</author><pubDate>Mon, 13 Jan 2025 15:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07423v1</guid></item><item><title>Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs</title><link>http://arxiv.org/abs/2409.11547v2</link><description>In this paper, we evaluate the creative fiction writing abilities of afine-tuned small language model (SLM), BART-large, and compare its performanceto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Ourevaluation consists of two experiments: (i) a human study in which 68participants rated short stories from humans and the SLM on grammaticality,relevance, creativity, and attractiveness, and (ii) a qualitative linguisticanalysis examining the textual characteristics of stories produced by eachmodel. In the first experiment, BART-large outscored average human writersoverall (2.11 vs. 1.85), a 14% relative improvement, though the slight humanadvantage in creativity was not statistically significant. In the secondexperiment, qualitative analysis showed that while GPT-4o demonstratednear-perfect coherence and used less cliche phrases, it tended to produce morepredictable language, with only 3% of its synopses featuring surprisingassociations (compared to 15% for BART). These findings highlight how modelsize and fine-tuning influence the balance between creativity, fluency, andcoherence in creative writing tasks, and demonstrate that smaller models can,in certain contexts, rival both humans and larger models.</description><author>Guillermo Marco, Luz Rello, Julio Gonzalo</author><pubDate>Mon, 13 Jan 2025 15:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11547v2</guid></item><item><title>Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes</title><link>http://arxiv.org/abs/2501.05226v2</link><description>We introduce a single-view reconstruction technique of volumetric fields inwhich multiple light scattering effects are omnipresent, such as in clouds. Wemodel the unknown distribution of volumetric fields using an unconditionaldiffusion model trained on a novel benchmark dataset comprising 1,000synthetically simulated volumetric density fields. The neural diffusion modelis trained on the latent codes of a novel, diffusion-friendly, monoplanarrepresentation. The generative model is used to incorporate a tailoredparametric diffusion posterior sampling technique into different reconstructiontasks. A physically-based differentiable volume renderer is employed to providegradients with respect to light transport in the latent space. This stands incontrast to classic NeRF approaches and makes the reconstructions betteraligned with observed data. Through various experiments, we demonstratesingle-view reconstruction of volumetric clouds at a previously unattainablequality.</description><author>Ludwic Leonard, Nils Thuerey, Ruediger Westermann</author><pubDate>Mon, 13 Jan 2025 15:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05226v2</guid></item><item><title>Initial Findings on Sensor based Open Vocabulary Activity Recognition via Text Embedding Inversion</title><link>http://arxiv.org/abs/2501.07408v1</link><description>Conventional human activity recognition (HAR) relies on classifiers trainedto predict discrete activity classes, inherently limiting recognition toactivities explicitly present in the training set. Such classifiers wouldinvariably fail, putting zero likelihood, when encountering unseen activities.We propose Open Vocabulary HAR (OV-HAR), a framework that overcomes thislimitation by first converting each activity into natural language and breakingit into a sequence of elementary motions. This descriptive text is then encodedinto a fixed-size embedding. The model is trained to regress this embedding,which is subsequently decoded back into natural language using a pre-trainedembedding inversion model. Unlike other works that rely on auto-regressivelarge language models (LLMs) at their core, OV-HAR achieves open vocabularyrecognition without the computational overhead of such models. The generatedtext can be transformed into a single activity class using LLM promptengineering. We have evaluated our approach on different modalities, includingvision (pose), IMU, and pressure sensors, demonstrating robust generalizationacross unseen activities and modalities, offering a fundamentally differentparadigm from contemporary classifiers.</description><author>Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz</author><pubDate>Mon, 13 Jan 2025 15:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07408v1</guid></item><item><title>Are queries and keys always relevant? A case study on Transformer wave functions</title><link>http://arxiv.org/abs/2405.18874v2</link><description>The dot product attention mechanism, originally designed for natural languageprocessing tasks, is a cornerstone of modern Transformers. It adeptly capturessemantic relationships between word pairs in sentences by computing asimilarity overlap between queries and keys. In this work, we explore thesuitability of Transformers, focusing on their attention mechanisms, in thespecific domain of the parametrization of variational wave functions toapproximate ground states of quantum many-body spin Hamiltonians. Specifically,we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenbergmodel, a common benchmark in the field of quantum many-body systems on lattice.By comparing the performance of standard attention mechanisms with a simplifiedversion that excludes queries and keys, relying solely on positions, we achievecompetitive results while reducing computational cost and parameter usage.Furthermore, through the analysis of the attention maps generated by standardattention mechanisms, we show that the attention weights become effectivelyinput-independent at the end of the optimization. We support the numericalresults with analytical calculations, providing physical insights of whyqueries and keys should be, in principle, omitted from the attention mechanismwhen studying large systems.</description><author>Riccardo Rende, Luciano Loris Viteritti</author><pubDate>Mon, 13 Jan 2025 15:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18874v2</guid></item><item><title>PROTECT: Protein circadian time prediction using unsupervised learning</title><link>http://arxiv.org/abs/2501.07405v1</link><description>Circadian rhythms regulate the physiology and behavior of humans and animals.Despite advancements in understanding these rhythms and predicting circadianphases at the transcriptional level, predicting circadian phases from proteomicdata remains elusive. This challenge is largely due to the scarcity of timelabels in proteomic datasets, which are often characterized by small samplesizes, high dimensionality, and significant noise. Furthermore, existingmethods for predicting circadian phases from transcriptomic data typically relyon prior knowledge of known rhythmic genes, making them unsuitable forproteomic datasets. To address this gap, we developed a novel computationalmethod using unsupervised deep learning techniques to predict circadian samplephases from proteomic data without requiring time labels or prior knowledge ofproteins or genes. Our model involves a two-stage training process optimizedfor robust circadian phase prediction: an initial greedy one-layer-at-a-timepre-training which generates informative initial parameters followed byfine-tuning. During fine-tuning, a specialized loss function guides the modelto align protein expression levels with circadian patterns, enabling it toaccurately capture the underlying rhythmic structure within the data. We testedour method on both time-labeled and unlabeled proteomic data. For labeled data,we compared our predictions to the known time labels, achieving high accuracy,while for unlabeled human datasets, including postmortem brain regions andurine samples, we explored circadian disruptions. Notably, our analysisidentified disruptions in rhythmic proteins between Alzheimer's disease andcontrol subjects across these samples.</description><author>Aram Ansary Ogholbake, Qiang Cheng</author><pubDate>Mon, 13 Jan 2025 15:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07405v1</guid></item><item><title>Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</title><link>http://arxiv.org/abs/2410.08926v3</link><description>We explore the transformative potential of SAM 2, a vision foundation model,in advancing gaze estimation and eye tracking technologies. By significantlyreducing annotation time, lowering technical barriers through its ease ofdeployment, and enhancing segmentation accuracy, SAM 2 addresses criticalchallenges faced by researchers and practitioners. Utilizing its zero-shotsegmentation capabilities with minimal user input-a single click per video-wetested SAM 2 on over 14 million eye images from diverse datasets, includingvirtual reality setups and the world's largest unified dataset recorded usingwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matchesthe performance of domain-specific models trained solely on eye images,achieving competitive mean Intersection over Union (mIoU) scores of up to 93%without fine-tuning. Additionally, we provide our code and segmentation masksfor these widely used datasets to promote further research.</description><author>Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci</author><pubDate>Mon, 13 Jan 2025 15:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08926v3</guid></item><item><title>Derivation of effective gradient flow equations and dynamical truncation of training data in Deep Learning</title><link>http://arxiv.org/abs/2501.07400v1</link><description>We derive explicit equations governing the cumulative biases and weights inDeep Learning with ReLU activation function, based on gradient descent for theEuclidean cost in the input layer, and under the assumption that the weightsare, in a precise sense, adapted to the coordinate system distinguished by theactivations. We show that gradient descent corresponds to a dynamical processin the input layer, whereby clusters of data are progressively reduced incomplexity ("truncated") at an exponential rate that increases with the numberof data points that have already been truncated. We provide a detaileddiscussion of several types of solutions to the gradient flow equations. A mainmotivation for this work is to shed light on the interpretability question insupervised learning.</description><author>Thomas Chen</author><pubDate>Mon, 13 Jan 2025 15:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07400v1</guid></item><item><title>OCORD: Open-Campus Object Removal Dataset</title><link>http://arxiv.org/abs/2501.07397v1</link><description>The rapid advancements in generative models, particularly diffusion-basedtechniques, have revolutionized image inpainting tasks by enabling thegeneration of high-fidelity and diverse content. However, object removalremains under-explored as a specific subset of inpainting, facing challengessuch as inadequate semantic understanding and the unintended generation ofartifacts. Existing datasets for object removal often rely on synthetic data,which fails to align with real-world scenarios, limiting model performance.Although some real-world datasets address these issues partially, they sufferfrom scalability, annotation inefficiencies, and limited realism in physicalphenomena such as lighting and shadows. To address these limitations, thispaper introduces a novel approach to object removal by constructing ahigh-resolution real-world dataset through long-duration video capture withfixed camera settings. Leveraging advanced tools such as Grounding-DINO,Segment-Anything-Model, and MASA for automated annotation, we provides image,background, and mask pairs while significantly reducing annotation time andlabor. With our efficient annotation pipeline, we release the first fully open,high-resolution real-world dataset for object removal, and improved performancein object removal tasks through fine-tuning of pre-trained diffusion models.</description><author>Shuo Zhang, Runpu Wei, Kongming Liang</author><pubDate>Mon, 13 Jan 2025 15:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07397v1</guid></item><item><title>Zero-Shot Scene Understanding for Automatic Target Recognition Using Large Vision-Language Models</title><link>http://arxiv.org/abs/2501.07396v1</link><description>Automatic target recognition (ATR) plays a critical role in tasks such asnavigation and surveillance, where safety and accuracy are paramount. Inextreme use cases, such as military applications, these factors are oftenchallenged due to the presence of unknown terrains, environmental conditions,and novel object categories. Current object detectors, including open-worlddetectors, lack the ability to confidently recognize novel objects or operatein unknown environments, as they have not been exposed to these new conditions.However, Large Vision-Language Models (LVLMs) exhibit emergent properties thatenable them to recognize objects in varying conditions in a zero-shot manner.Despite this, LVLMs struggle to localize objects effectively within a scene. Toaddress these limitations, we propose a novel pipeline that combines thedetection capabilities of open-world detectors with the recognition confidenceof LVLMs, creating a robust system for zero-shot ATR of novel classes andunknown domains. In this study, we compare the performance of various LVLMs forrecognizing military vehicles, which are often underrepresented in trainingdatasets. Additionally, we examine the impact of factors such as distancerange, modality, and prompting methods on the recognition performance,providing insights into the development of more reliable ATR systems for novelconditions and classes.</description><author>Yasiru Ranasinghe, Vibashan VS, James Uplinger, Celso De Melo, Vishal M. Patel</author><pubDate>Mon, 13 Jan 2025 15:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07396v1</guid></item><item><title>The Essentials of AI for Life and Society: An AI Literacy Course for the University Community</title><link>http://arxiv.org/abs/2501.07392v1</link><description>We describe the development of a one-credit course to promote AI literacy atThe University of Texas at Austin. In response to a call for the rapiddeployment of class to serve a broad audience in Fall of 2023, we designed a14-week seminar-style course that incorporated an interdisciplinary group ofspeakers who lectured on topics ranging from the fundamentals of AI to societalconcerns including disinformation and employment. University students, faculty,and staff, and even community members outside of the University, were invitedto enroll in this online offering: The Essentials of AI for Life and Society.We collected feedback from course participants through weekly reflections and afinal survey. Satisfyingly, we found that attendees reported gains in their AIliteracy. We sought critical feedback through quantitative and qualitativeanalysis, which uncovered challenges in designing a course for this generalaudience. We utilized the course feedback to design a three-credit version ofthe course that is being offered in Fall of 2024. The lessons we learned andour plans for this new iteration may serve as a guide to instructors designingAI courses for a broad audience.</description><author>Joydeep Biswas, Don Fussell, Peter Stone, Kristin Patterson, Kristen Procko, Lea Sabatini, Zifan Xu</author><pubDate>Mon, 13 Jan 2025 15:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07392v1</guid></item><item><title>Enhancing Retrieval-Augmented Generation: A Study of Best Practices</title><link>http://arxiv.org/abs/2501.07391v1</link><description>Retrieval-Augmented Generation (RAG) systems have recently shown remarkableadvancements by integrating retrieval mechanisms into language models,enhancing their ability to produce more accurate and contextually relevantresponses. However, the influence of various components and configurationswithin RAG systems remains underexplored. A comprehensive understanding ofthese elements is essential for tailoring RAG systems to complex retrievaltasks and ensuring optimal performance across diverse applications. In thispaper, we develop several advanced RAG system designs that incorporate queryexpansion, various novel retrieval strategies, and a novel ContrastiveIn-Context Learning RAG. Our study systematically investigates key factors,including language model size, prompt design, document chunk size, knowledgebase size, retrieval stride, query expansion techniques, Contrastive In-ContextLearning knowledge bases, multilingual knowledge bases, and Focus Moderetrieving relevant context at sentence-level. Through extensiveexperimentation, we provide a detailed analysis of how these factors influenceresponse quality. Our findings offer actionable insights for developing RAGsystems, striking a balance between contextual richness andretrieval-generation efficiency, thereby paving the way for more adaptable andhigh-performing RAG frameworks in diverse real-world scenarios. Our code andimplementation details are publicly available.</description><author>Siran Li, Linus Stenzel, Carsten Eickhoff, Seyed Ali Bahrainian</author><pubDate>Mon, 13 Jan 2025 15:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07391v1</guid></item><item><title>Kolmogorov-Arnold Network for Remote Sensing Image Semantic Segmentation</title><link>http://arxiv.org/abs/2501.07390v1</link><description>Semantic segmentation plays a crucial role in remote sensing applications,where the accurate extraction and representation of features are essential forhigh-quality results. Despite the widespread use of encoder-decoderarchitectures, existing methods often struggle with fully utilizing thehigh-dimensional features extracted by the encoder and efficiently recoveringdetailed information during decoding. To address these problems, we propose anovel semantic segmentation network, namely DeepKANSeg, including two keyinnovations based on the emerging Kolmogorov Arnold Network (KAN). Notably, theadvantage of KAN lies in its ability to decompose high-dimensional complexfunctions into univariate transformations, enabling efficient and flexiblerepresentation of intricate relationships in data. First, we introduce aKAN-based deep feature refinement module, namely DeepKAN to effectively capturecomplex spatial and rich semantic relationships from high-dimensional features.Second, we replace the traditional multi-layer perceptron (MLP) layers in theglobal-local combined decoder with KAN-based linear layers, namely GLKAN. Thismodule enhances the decoder's ability to capture fine-grained details duringdecoding. To evaluate the effectiveness of the proposed method, experiments areconducted on two well-known fine-resolution remote sensing benchmark datasets,namely ISPRS Vaihingen and ISPRS Potsdam. The results demonstrate that theKAN-enhanced segmentation model achieves superior performance in terms ofaccuracy compared to state-of-the-art methods. They highlight the potential ofKANs as a powerful alternative to traditional architectures in semanticsegmentation tasks. Moreover, the explicit univariate decomposition providesimproved interpretability, which is particularly beneficial for applicationsrequiring explainable learning in remote sensing.</description><author>Xianping Ma, Ziyao Wang, Yin Hu, Xiaokang Zhang, Man-On Pun</author><pubDate>Mon, 13 Jan 2025 15:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07390v1</guid></item><item><title>Information-Theoretic Dual Memory System for Continual Learning</title><link>http://arxiv.org/abs/2501.07382v1</link><description>Continuously acquiring new knowledge from a dynamic environment is afundamental capability for animals, facilitating their survival and ability toaddress various challenges. This capability is referred to as continuallearning, which focuses on the ability to learn a sequence of tasks without thedetriment of previous knowledge. A prevalent strategy to tackle continuallearning involves selecting and storing numerous essential data samples fromprior tasks within a fixed-size memory buffer. However, the majority of currentmemory-based techniques typically utilize a single memory buffer, which poseschallenges in concurrently managing newly acquired and previously learnedsamples. Drawing inspiration from the Complementary Learning Systems (CLS)theory, which defines rapid and gradual learning mechanisms for processinginformation, we propose an innovative dual memory system called theInformation-Theoretic Dual Memory System (ITDMS). This system comprises a fastmemory buffer designed to retain temporary and novel samples, alongside a slowmemory buffer dedicated to preserving critical and informative samples. Thefast memory buffer is optimized employing an efficient reservoir samplingprocess. Furthermore, we introduce a novel information-theoretic memoryoptimization strategy that selectively identifies and retains diverse andinformative data samples for the slow memory buffer. Additionally, we propose anovel balanced sample selection procedure that automatically identifies andeliminates redundant memorized samples, thus freeing up memory capacity for newdata acquisitions, which can deal with a growing array of tasks. Ourmethodology is rigorously assessed through a series of continual learningexperiments, with empirical results underscoring the effectiveness of theproposed system.</description><author>RunQing Wu, KaiHui Huang, HanYi Zhang, QiHe Liu, GuoJin Yu, JingSong Deng, Fei Ye</author><pubDate>Mon, 13 Jan 2025 15:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07382v1</guid></item><item><title>Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2402.02429v3</link><description>As a marriage between offline RL and meta-RL, the advent of offlinemeta-reinforcement learning (OMRL) has shown great promise in enabling RLagents to multi-task and quickly adapt while acquiring knowledge safely. Amongwhich, context-based OMRL (COMRL) as a popular paradigm, aims to learn auniversal policy conditioned on effective task representations. In this work,by examining several key milestones in the field of COMRL, we propose tointegrate these seemingly independent methodologies into a unified framework.Most importantly, we show that the pre-existing COMRL algorithms areessentially optimizing the same mutual information objective between the taskvariable $M$ and its latent representation $Z$ by implementing variousapproximate bounds. Such theoretical insight offers ample design freedom fornovel algorithms. As demonstrations, we propose a supervised and aself-supervised implementation of $I(Z; M)$, and empirically show that thecorresponding optimization algorithms exhibit remarkable generalization acrossa broad spectrum of RL benchmarks, context shift scenarios, data qualities anddeep learning architectures. This work lays the information theoreticfoundation for COMRL methods, leading to a better understanding of taskrepresentation learning in the context of reinforcement learning. Given itsgenerality, we envision our framework as a promising offline pre-trainingparadigm of foundation models for decision making.</description><author>Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Yang Yu, Junqiao Zhao, Pheng-Ann Heng</author><pubDate>Mon, 13 Jan 2025 14:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02429v3</guid></item><item><title>Distributed Representations Enable Robust Multi-Timescale Symbolic Computation in Neuromorphic Hardware</title><link>http://arxiv.org/abs/2405.01305v3</link><description>Programming recurrent spiking neural networks (RSNNs) to robustly performmulti-timescale computation remains a difficult challenge. To address this, wedescribe a single-shot weight learning scheme to embed robust multi-timescaledynamics into attractor-based RSNNs, by exploiting the properties ofhigh-dimensional distributed representations. We embed finite state machinesinto the RSNN dynamics by superimposing a symmetric autoassociative weightmatrix and asymmetric transition terms, which are each formed by the vectorbinding of an input and heteroassociative outer-products between states. Ourapproach is validated through simulations with highly nonideal weights; anexperimental closed-loop memristive hardware setup; and on Loihi 2, where itscales seamlessly to large state machines. This work introduces a scalableapproach to embed robust symbolic computation through recurrent dynamics intoneuromorphic hardware, without requiring parameter fine-tuning or significantplatform-specific optimisation. Moreover, it demonstrates that distributedsymbolic representations serve as a highly capable representation-invariantlanguage for cognitive algorithms in neuromorphic hardware.</description><author>Madison Cotteret, Hugh Greatorex, Alpha Renner, Junren Chen, Emre Neftci, Huaqiang Wu, Giacomo Indiveri, Martin Ziegler, Elisabetta Chicca</author><pubDate>Mon, 13 Jan 2025 14:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01305v3</guid></item><item><title>FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2501.07378v1</link><description>Medical image segmentation is challenging due to the diversity of medicalimages and the lack of labeled data, which motivates recent developments infederated semi-supervised learning (FSSL) to leverage a large amount ofunlabeled data from multiple centers for model training without sharing rawdata. However, what remains under-explored in FSSL is the domain shift problemwhich may cause suboptimal model aggregation and low effectivity of theutilization of unlabeled data, eventually leading to unsatisfactory performancein unseen domains. In this paper, we explore this previously ignored scenario,namely domain generalized federated semi-supervised learning (FedSemiDG), whichaims to learn a model in a distributed manner from multiple domains withlimited labeled data and abundant unlabeled data such that the model cangeneralize well to unseen domains. We present a novel framework, FederatedGeneralization-Aware SemiSupervised Learning (FGASL), to address the challengesin FedSemiDG by effectively tackling critical issues at both global and locallevels. Globally, we introduce Generalization-Aware Aggregation (GAA),assigning adaptive weights to local models based on their generalizationperformance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement(DR) strategy to combine global and domain-specific knowledge, generating morereliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)enforces feature consistency under perturbations, promoting domain-invariantlearning. Extensive experiments on three medical segmentation tasks (cardiacMRI, spine MRI and bladder cancer MRI) demonstrate that our methodsignificantly outperforms state-of-the-art FSSL and domain generalizationapproaches, achieving robust generalization on unseen domains.</description><author>Zhipeng Deng, Zhe Xu, Tsuyoshi Isshiki, Yefeng Zheng</author><pubDate>Mon, 13 Jan 2025 14:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07378v1</guid></item><item><title>Synthetic Data for Portfolios: A Throw of the Dice Will Never Abolish Chance</title><link>http://arxiv.org/abs/2501.03993v2</link><description>Simulation methods have always been instrumental in finance, and data-drivenmethods with minimal model specification, commonly referred to as generativemodels, have attracted increasing attention, especially after the success ofdeep learning in a broad range of fields. However, the adoption of these modelsin financial applications has not kept pace with the growing interest, probablydue to the unique complexities and challenges of financial markets. This paperaims to contribute to a deeper understanding of the limitations of generativemodels, particularly in portfolio and risk management. To this end, we begin bypresenting theoretical results on the importance of initial sample size, andpoint out the potential pitfalls of generating far more data than originallyavailable. We then highlight the inseparable nature of model development andthe desired use case by touching on a paradox: generic generative modelsinherently care less about what is important for constructing portfolios (inparticular the long-short ones). Based on these findings, we propose a pipelinefor the generation of multivariate returns that meets conventional evaluationstandards on a large universe of US equities while being compliant withstylized facts observed in asset returns and turning around the pitfalls wepreviously identified. Moreover, we insist on the need for more delicateevaluation methods, and suggest, through an example of mean-reversionstrategies, a method designed to identify poor models for a given applicationbased on regurgitative training, i.e. retraining the model using the data ithas itself generated, which is commonly referred to in statistics asidentifiability.</description><author>Adil Rengim Cetingoz, Charles-Albert Lehalle</author><pubDate>Mon, 13 Jan 2025 14:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03993v2</guid></item><item><title>Accelerating genetic optimization of nonlinear model predictive control by learning optimal search space size</title><link>http://arxiv.org/abs/2305.08094v2</link><description>Genetic algorithm (GA) is typically used to solve nonlinear model predictivecontrol's optimization problem. However, the size of the search space in whichthe GA searches for the optimal control inputs is crucial for its applicabilityto fast-response systems. This paper proposes accelerating the geneticoptimization of NMPC by learning optimal search space size. The approach trainsa multivariate regression model to adaptively predict the best smallest size ofthe search space in every control cycle. The proposed approach reduces the GA'scomputational time, improves the chance of convergence to better controlinputs, and provides a stable and feasible solution. The proposed approach wasevaluated on three nonlinear systems and compared to four other evolutionaryalgorithms implemented in a processor-in-the-loop fashion. The results showthat the proposed approach provides a 17-45\% reduction in computational timeand increases the convergence rate by 35-47\%. The source code is available onGitHub.</description><author>Eslam Mostafa, Hussein A. Aly, Ahmed Elliethy</author><pubDate>Mon, 13 Jan 2025 14:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08094v2</guid></item><item><title>Constructing and explaining machine learning models for chemistry: example of the exploration and design of boron-based Lewis acids</title><link>http://arxiv.org/abs/2501.01576v2</link><description>The integration of machine learning (ML) into chemistry offers transformativepotential in the design of molecules with targeted properties. However, thefocus has often been on creating highly efficient predictive models, sometimesat the expense of interpretability. In this study, we leverage explainable AItechniques to explore the rational design of boron-based Lewis acids, whichplay a pivotal role in organic reactions due to their electron-cceptingproperties. Using Fluoride Ion Affinity as a proxy for Lewis acidity, wedeveloped interpretable ML models based on chemically meaningful descriptors,including ab initio computed features and substituent-based parameters derivedfrom the Hammett linear free-energy relationship. By constraining the chemicalspace to well-defined molecular scaffolds, we achieved highly accuratepredictions (mean absolute error &lt; 6 kJ/mol), surpassing conventional black-boxdeep learning models in low-data regimes. Interpretability analyses of themodels shed light on the origin of Lewis acidity in these compounds andidentified actionable levers to modulate it through the nature and positioningof substituents on the molecular scaffold. This work bridges ML and chemist'sway of thinking, demonstrating how explainable models can inspire moleculardesign and enhance scientific understanding of chemical reactivity.</description><author>Juliette Fenogli, Laurence Grimaud, Rodolphe Vuilleumier</author><pubDate>Mon, 13 Jan 2025 14:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01576v2</guid></item><item><title>A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive Coverage Optimization</title><link>http://arxiv.org/abs/2501.07375v1</link><description>Coverage optimization generally involves deploying a set of facilities (e.g.,sensors) to best satisfy the demands of specified points, with wideapplications in fields such as location science and sensor networks. Inpractical applications, coverage optimization focuses on target coverage, whichis typically formulated as Mixed-Variable Optimization Problems (MVOPs) due tocomplex real-world constraints. Meanwhile, high-fidelity discretization andvisibility analysis may bring additional calculations, which significantlyincreases the computational cost. These factors pose significant challenges forfitness evaluations (FEs) in canonical Evolutionary Algorithms (EAs), andevolve the coverage problem into an Expensive Mixed-Variable OptimizationProblem (EMVOP). To address these issues, we propose the RankNet-InspiredSurrogate-assisted Hybrid Metaheuristic (RI-SHM), an extension of our previouswork. RI-SHM integrates three key components: (1) a RankNet-based pairwiseglobal surrogate that innovatively predicts rankings between pairs ofindividuals, bypassing the challenges of fitness estimation in discontinuoussolution space; (2) a surrogate-assisted local Estimation of DistributionAlgorithm (EDA) that enhances local exploitation and helps escape from localoptima; and (3) a fitness diversity-driven switching strategy that dynamicallybalances exploration and exploitation. Experiments demonstrate that ouralgorithm can effectively handle large-scale coverage optimization tasks of upto 300 dimensions and more than 1,800 targets within desirable runtime.Compared to state-of-the-art algorithms for EMVOPs, RI-SHM consistentlyoutperforms them by up to 56.5$\%$ across all tested instances.</description><author>Tongyu Wu, Changhao Miao, Yuntian Zhang, Chen Chen</author><pubDate>Mon, 13 Jan 2025 14:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07375v1</guid></item><item><title>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</title><link>http://arxiv.org/abs/2412.05271v4</link><description>We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)series that builds upon InternVL 2.0, maintaining its core model architecturewhile introducing significant enhancements in training and testing strategiesas well as data quality. In this work, we delve into the relationship betweenmodel scaling and performance, systematically exploring the performance trendsin vision encoders, language models, dataset sizes, and test-timeconfigurations. Through extensive evaluations on a wide range of benchmarks,including multi-discipline reasoning, document understanding, multi-image /video understanding, real-world comprehension, multimodal hallucinationdetection, visual grounding, multilingual capabilities, and pure languageprocessing, InternVL 2.5 exhibits competitive performance, rivaling leadingcommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model isthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasingstrong potential for test-time scaling. We hope this model contributes to theopen-source community by setting new standards for developing and applyingmultimodal AI systems. HuggingFace demo seehttps://huggingface.co/spaces/OpenGVLab/InternVL</description><author>Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang</author><pubDate>Mon, 13 Jan 2025 14:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05271v4</guid></item><item><title>Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving Linear and Angular Momentum for Dynamical Systems</title><link>http://arxiv.org/abs/2501.07373v1</link><description>Accurate, interpretable, and real-time modeling of multi-body dynamicalsystems is essential for predicting behaviors and inferring physical propertiesin natural and engineered environments. Traditional physics-based models facescalability challenges and are computationally demanding, while data-drivenapproaches like Graph Neural Networks (GNNs) often lack physical consistency,interpretability, and generalization. In this paper, we propose Dynami-CALGraphNet, a Physics-Informed Graph Neural Network that integrates the learningcapabilities of GNNs with physics-based inductive biases to address theselimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear andangular momentum for interacting nodes using edge-local reference frames thatare equivariant to rotational symmetries, invariant to translations, andequivariant to node permutations. This design ensures physically consistentpredictions of node dynamics while offering interpretable, edge-wise linear andangular impulses resulting from pairwise interactions. Evaluated on a 3Dgranular system with inelastic collisions, Dynami-CAL GraphNet demonstratesstable error accumulation over extended rollouts, effective extrapolations tounseen configurations, and robust handling of heterogeneous interactions andexternal forces. Dynami-CAL GraphNet offers significant advantages in fieldsrequiring accurate, interpretable, and real-time modeling of complex multi-bodydynamical systems, such as robotics, aerospace engineering, and materialsscience. By providing physically consistent and scalable predictions thatadhere to fundamental conservation laws, it enables the inference of forces andmoments while efficiently handling heterogeneous interactions and externalforces.</description><author>Vinay Sharma, Olga Fink</author><pubDate>Mon, 13 Jan 2025 14:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07373v1</guid></item><item><title>Simulating the Hubbard Model with Equivariant Normalizing Flows</title><link>http://arxiv.org/abs/2501.07371v1</link><description>Generative models, particularly normalizing flows, have shown exceptionalperformance in learning probability distributions across various domains ofphysics, including statistical mechanics, collider physics, and lattice fieldtheory. In the context of lattice field theory, normalizing flows have beensuccessfully applied to accurately learn the Boltzmann distribution, enabling arange of tasks such as direct estimation of thermodynamic observables andsampling independent and identically distributed (i.i.d.) configurations. In this work, we present a proof-of-concept demonstration that normalizingflows can be used to learn the Boltzmann distribution for the Hubbard model.This model is widely employed to study the electronic structure of graphene andother carbon nanomaterials. State-of-the-art numerical simulations of theHubbard model, such as those based on Hybrid Monte Carlo (HMC) methods, oftensuffer from ergodicity issues, potentially leading to biased estimates ofphysical observables. Our numerical experiments demonstrate that leveragingi.i.d.\ sampling from the normalizing flow effectively addresses these issues.</description><author>Dominic Schuh, Janik Kreit, Evan Berkowitz, Lena Funcke, Thomas Luu, Kim A. Nicoli, Marcel Rodekamp</author><pubDate>Mon, 13 Jan 2025 14:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07371v1</guid></item><item><title>Project Tracyn: Generative Artificial Intelligence based Peripherals Trace Synthesizer</title><link>http://arxiv.org/abs/2411.06376v2</link><description>Peripheral Component Interconnect Express (PCIe) is the de facto interconnectstandard for high-speed peripherals and CPUs. Prototyping and optimizing PCIedevices for emerging scenarios is an ongoing challenge. Since Transaction LayerPackets (TLPs) capture device-CPU interactions, it is crucial to analyze andgenerate realistic TLP traces for effective device design and optimization.Generative AI offers a promising approach for creating intricate, custom TLPtraces necessary for PCIe hardware and software development. However, existingmodels often generate impractical traces due to the absence of PCIe-specificconstraints, such as TLP ordering and causality. This paper presents Phantom,the first framework that treats TLP trace generation as a generative AI problemwhile incorporating PCIe-specific constraints. We validate Phantom'seffectiveness by generating TLP traces for an actual PCIe network interfacecard. Experimental results show that Phantom produces practical, large-scaleTLP traces, significantly outperforming existing models, with improvements ofup to 1000$\times$ in task-specific metrics and up to 2.19$\times$ in FrechetInception Distance (FID) compared to backbone-only methods.</description><author>Zhibai Huang, Yihan Shen, Yongchen Xie, Zhixiang Wei, Yun wang, Fangxin Liu, Tao Song, Zhengwei Qi</author><pubDate>Mon, 13 Jan 2025 14:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06376v2</guid></item><item><title>Design of 2D Skyrmionic Metamaterial Through Controlled Assembly</title><link>http://arxiv.org/abs/2402.10874v2</link><description>Despite extensive research on magnetic skyrmions and antiskyrmions, asignificant challenge remains in crafting nontrivial high-order skyrmionictextures with varying, or even tailor-made, topologies. We address thischallenge, by focusing on a construction pathway of skyrmionic metamaterialswithin a monolayer thin film and suggest several skyrmionic metamaterials thatare surprisingly stable, i.e., long-lived, due to a self-stabilizationmechanism. This makes these new textures promising for applications. Central toour approach is the concept of 'simulated controlled assembly', in short, aprotocol inspired by 'click chemistry' that allows for positioning topologicalmagnetic structures where one likes, and then allowing for energy minimizationto elucidate the stability. Utilizing high-throughput atomistic-spin-dynamicsimulations alongside state-of-the-art AI-driven tools, we have isolatedskyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium(Q=0). These entities serve as foundational 'skyrmionic building blocks' toform the here reported intricate textures. In this work, two key contributionsare introduced to the field of skyrmionic systems. First, we present a a novelcombination of atomistic spin dynamics simulations and controlled assemblyprotocols for the stabilization and investigation of new topological magnets.Second, using the aforementioned methods we report on the discovery ofskyrmionic metamaterials.</description><author>Qichen Xu, Zhuanglin Shen, Alexander Edström, I. P. Miranda, Zhiwei Lu, Anders Bergman, Danny Thonig, Wanjian Yin, Olle Eriksson, Anna Delin</author><pubDate>Mon, 13 Jan 2025 14:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10874v2</guid></item><item><title>BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation</title><link>http://arxiv.org/abs/2412.09718v2</link><description>The emergence of large pre-trained vision-language models (VLMs) represents aparadigm shift in machine learning, with unprecedented results in a broad spanof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibitedremarkable zero-shot and transfer learning capabilities in classification. Totransfer CLIP to downstream tasks, adapters constitute a parameter-efficientapproach that avoids backpropagation through the large model (unlike relatedprompt learning methods). However, CLIP adapters have been developed to targetdiscriminative performance, and the quality of their uncertainty estimates hasbeen overlooked. In this work we show that the discriminative performance ofstate-of-the-art CLIP adapters does not always correlate with their uncertaintyestimation capabilities, which are essential for a safe deployment inreal-world scenarios. We also demonstrate that one of such adapters is obtainedthrough MAP inference from a more general probabilistic framework. Based onthis observation we introduce BayesAdapter, which leverages Bayesian inferenceto estimate a full probability distribution instead of a single point, bettercapturing the variability inherent in the parameter space. In a comprehensiveempirical evaluation we show that our approach obtains high quality uncertaintyestimates in the predictions, standing out in calibration and selectiveclassification. Our code will be publicly available upon acceptance of thepaper.</description><author>Pablo Morales-Álvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, Jose Dolz</author><pubDate>Mon, 13 Jan 2025 14:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09718v2</guid></item><item><title>GIM: A Million-scale Benchmark for Generative Image Manipulation Detection and Localization</title><link>http://arxiv.org/abs/2406.16531v2</link><description>The extraordinary ability of generative models emerges as a new trend inimage editing and generating realistic images, posing a serious threat to thetrustworthiness of multimedia data and driving the research of imagemanipulation detection and location (IMDL). However, the lack of a large-scaledata foundation makes the IMDL task unattainable. In this paper, we build alocal manipulation data generation pipeline that integrates the powerfulcapabilities of SAM, LLM, and generative models. Upon this basis, we proposethe GIM dataset, which has the following advantages: 1) Large scale, GIMincludes over one million pairs of AI-manipulated images and real images. 2)Rich image content, GIM encompasses a broad range of image classes. 3) Diversegenerative manipulation, the images are manipulated images withstate-of-the-art generators and various manipulation tasks. The aforementionedadvantages allow for a more comprehensive evaluation of IMDL methods, extendingtheir applicability to diverse images. We introduce the GIM benchmark with twosettings to evaluate existing IMDL methods. In addition, we propose a novelIMDL framework, termed GIMFormer, which consists of a ShadowTracer,Frequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)module. Extensive experiments on the GIM demonstrate that GIMFormer surpassesthe previous state-of-the-art approach on two different benchmarks.</description><author>Yirui Chen, Xudong Huang, Quan Zhang, Wei Li, Mingjian Zhu, Qiangyu Yan, Simiao Li, Hanting Chen, Hailin Hu, Jie Yang, Wei Liu, Jie Hu</author><pubDate>Mon, 13 Jan 2025 14:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16531v2</guid></item></channel></rss>