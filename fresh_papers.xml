<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Mar 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HAIFIT: Human-Centered AI for Fashion Image Translation</title><link>http://arxiv.org/abs/2403.08651v2</link><description>In the realm of fashion design, sketches serve as the canvas for expressingan artist's distinctive drawing style and creative vision, capturing intricatedetails like stroke variations and texture nuances. The advent ofsketch-to-image cross-modal translation technology has notably aided designers.However, existing methods often compromise these sketch details during imagegeneration, resulting in images that deviate from the designer's intendedconcept. This limitation hampers the ability to offer designers a precisepreview of the final output. To overcome this challenge, we introduce HAIFIT, anovel approach that transforms sketches into high-fidelity, lifelike clothingimages by integrating multi-scale features and capturing extensive feature mapdependencies from diverse perspectives. Through extensive qualitative andquantitative evaluations conducted on our self-collected dataset, our methoddemonstrates superior performance compared to existing methods in generatingphotorealistic clothing images. Our method excels in preserving the distinctivestyle and intricate details essential for fashion design applications.</description><author>Jianan Jiang, Xinglin Li, Weiren Yu, Di Wu</author><pubDate>Mon, 25 Mar 2024 15:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08651v2</guid></item><item><title>Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</title><link>http://arxiv.org/abs/2403.16812v1</link><description>In AI-assisted decision-making, humans often passively review AI's suggestionand decide whether to accept or reject it as a whole. In such a paradigm,humans are found to rarely trigger analytical thinking and face difficulties incommunicating the nuances of conflicting opinions to the AI when disagreementsoccur. To tackle this challenge, we propose Human-AI Deliberation, a novelframework to promote human reflection and discussion on conflicting human-AIopinions in decision-making. Based on theories in human deliberation, thisframework engages humans and AI in dimension-level opinion elicitation,deliberative discussion, and decision updates. To empower AI with deliberativecapabilities, we designed Deliberative AI, which leverages large languagemodels (LLMs) as a bridge between humans and domain-specific models to enableflexible conversational interactions and faithful information provision. Anexploratory evaluation on a graduate admissions task shows that Deliberative AIoutperforms conventional explainable AI (XAI) assistants in improving humans'appropriate reliance and task performance. Based on a mixed-methods analysis ofparticipant behavior, perception, user experience, and open-ended feedback, wedraw implications for future AI-assisted decision tool design.</description><author>Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming Yin, Xiaojuan Ma</author><pubDate>Mon, 25 Mar 2024 15:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16812v1</guid></item><item><title>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</title><link>http://arxiv.org/abs/2403.16809v1</link><description>The increasing prevalence of Cyber-Physical Systems and the Internet ofThings (CPS-IoT) applications and Foundation Models are enabling newapplications that leverage real-time control of the environment. For example,real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systemscan reduce its usage when not needed for the comfort of human occupants, hencereducing energy consumption. Collecting real-time feedback on human preferencesin such human-in-the-loop (HITL) systems, however, is difficult in practice. Wepropose the use of large language models (LLMs) to deal with the challenges ofdynamic environments and difficult-to-obtain data in CPS optimization. In thispaper, we present a case study that employs LLM agents to mimic the behaviorsand thermal preferences of various population groups (e.g. young families, theelderly) in a shopping mall. The aggregated thermal preferences are integratedinto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, whichemploys the LLM as a dynamic simulation of the physical environment to learnhow to balance between energy savings and occupant comfort. Our results showthat LLMs are capable of simulating complex population movements within largeopen spaces. Besides, AitL-RL demonstrates superior performance compared to thepopular existing policy of set point control, suggesting that adaptive andpersonalized decision-making is critical for efficient optimization in CPS-IoTapplications. Through this case study, we demonstrate the potential ofintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance systemadaptability and efficiency. The project's code can be found on our GitHubrepository.</description><author>Hanqing Yang, Marie Siew, Carlee Joe-Wong</author><pubDate>Mon, 25 Mar 2024 15:32:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16809v1</guid></item><item><title>Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products</title><link>http://arxiv.org/abs/2403.16808v1</link><description>In December 2023, the European Parliament provisionally agreed on the EU AIAct. This unprecedented regulatory framework for AI systems lays out guidelinesto ensure the safety, legality, and trustworthiness of AI products. This paperpresents a methodology for interpreting the EU AI Act requirements forhigh-risk AI systems by leveraging product quality models. We first propose anextended product quality model for AI systems, incorporating attributesrelevant to the Act not covered by current quality models. We map the Actrequirements to relevant quality attributes with the goal of refining them intomeasurable characteristics. We then propose a contract-based approach to derivetechnical requirements at the stakeholder level. This facilitates thedevelopment and assessment of AI systems that not only adhere to establishedquality standards, but also comply with the regulatory requirements outlined inthe Act for high-risk (including safety-critical) AI systems. We demonstratethe applicability of this methodology on an exemplary automotive supply chainuse case, where several stakeholders interact to achieve EU AI Act compliance.</description><author>J. Kelly, S. Ali Zafar, L. Heidemann, J. Zacchi, D. Espinoza, N. Mata</author><pubDate>Mon, 25 Mar 2024 15:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16808v1</guid></item><item><title>SeMoLi: What Moves Together Belongs Together</title><link>http://arxiv.org/abs/2402.19463v2</link><description>We tackle semi-supervised object detection based on motion cues. Recentresults suggest that heuristic-based clustering methods in conjunction withobject trackers can be used to pseudo-label instances of moving objects and usethese as supervisory signals to train 3D object detectors in Lidar data withoutmanual supervision. We re-think this approach and suggest that both, objectdetection, as well as motion-inspired pseudo-labeling, can be tackled in adata-driven manner. We leverage recent advances in scene flow estimation toobtain point trajectories from which we extract long-term, class-agnosticmotion patterns. Revisiting correlation clustering in the context of messagepassing networks, we learn to group those motion patterns to cluster points toobject instances. By estimating the full extent of the objects, we obtainper-scan 3D bounding boxes that we use to supervise a Lidar object detectionnetwork. Our method not only outperforms prior heuristic-based approaches (57.5AP, +14 improvement over prior work), more importantly, we show we canpseudo-label and train object detectors across datasets.</description><author>Jenny Seidenschwarz, Aljoša Ošep, Francesco Ferroni, Simon Lucey, Laura Leal-Taixé</author><pubDate>Mon, 25 Mar 2024 15:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19463v2</guid></item><item><title>TEI2GO: A Multilingual Approach for Fast Temporal Expression Identification</title><link>http://arxiv.org/abs/2403.16804v1</link><description>Temporal expression identification is crucial for understanding texts writtenin natural language. Although highly effective systems such as HeidelTimeexist, their limited runtime performance hampers adoption in large-scaleapplications and production environments. In this paper, we introduce theTEI2GO models, matching HeidelTime's effectiveness but with significantlyimproved runtime, supporting six languages, and achieving state-of-the-artresults in four of them. To train the TEI2GO models, we used a combination ofmanually annotated reference corpus and developed ``Professor HeidelTime'', acomprehensive weakly labeled corpus of news texts annotated with HeidelTime.This corpus comprises a total of $138,069$ documents (over six languages) with$1,050,921$ temporal expressions, the largest open-source annotated dataset fortemporal expression identification to date. By describing how the models wereproduced, we aim to encourage the research community to further explore,refine, and extend the set of models to additional languages and domains. Code,annotations, and models are openly available for community exploration and use.The models are conveniently on HuggingFace for seamless integration andapplication.</description><author>Hugo Sousa, Ricardo Campos, Alípio Jorge</author><pubDate>Mon, 25 Mar 2024 15:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16804v1</guid></item><item><title>Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning</title><link>http://arxiv.org/abs/2403.16803v1</link><description>Object reconstruction is relevant for many autonomous robotic tasks thatrequire interaction with the environment. A key challenge in such scenarios isplanning view configurations to collect informative measurements forreconstructing an initially unknown object. One-shot view planning enablesefficient data collection by predicting view configurations and planning theglobally shortest path connecting all views at once. However, geometric priorsabout the object are required to conduct one-shot view planning. In this work,we propose a novel one-shot view planning approach that utilizes the powerful3D generation capabilities of diffusion models as priors. By incorporating suchgeometric priors into our pipeline, we achieve effective one-shot view planningstarting with only a single RGB image of the object to be reconstructed. Ourplanning experiments in simulation and real-world setups indicate that ourapproach balances well between object reconstruction quality and movement cost.</description><author>Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz</author><pubDate>Mon, 25 Mar 2024 15:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16803v1</guid></item><item><title>Cluster-Based Normalization Layer for Neural Networks</title><link>http://arxiv.org/abs/2403.16798v1</link><description>Deep learning faces significant challenges during the training of neuralnetworks, including internal covariate shift, label shift, vanishing/explodinggradients, overfitting, and computational complexity. While conventionalnormalization methods, such as Batch Normalization, aim to tackle some of theseissues, they often depend on assumptions that constrain their adaptability.Mixture Normalization faces computational hurdles in its pursuit of handlingmultiple Gaussian distributions. This paper introduces Cluster-BasedNormalization (CB-Norm) in two variants - Supervised Cluster-BasedNormalization (SCB-Norm) and Unsupervised Cluster-Based Normalization(UCB-Norm) - proposing a groundbreaking one-step normalization approach.CB-Norm leverages a Gaussian mixture model to specifically address challengesrelated to gradient stability and learning acceleration. For SCB-Norm, asupervised variant, the novel mechanism involves introducing predefined datapartitioning, termed clusters, to normalize activations based on the assignedcluster. This cluster-driven approach creates a space that conforms to aGaussian mixture model. On the other hand, UCB-Norm, an unsupervisedcounterpart, dynamically clusters neuron activations during training, adaptingto task-specific challenges without relying on predefined data partitions(clusters). This dual approach ensures flexibility in addressing diverselearning scenarios. CB-Norm innovatively uses a one-step normalizationapproach, where parameters of each mixture component (cluster in activationspace) serve as weights for deep neural networks. This adaptive clusteringprocess tackles both clustering and resolution of deep neural network tasksconcurrently during training, signifying a notable advancement in the field.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Mon, 25 Mar 2024 15:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16798v1</guid></item><item><title>CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation</title><link>http://arxiv.org/abs/2403.16794v1</link><description>Curb detection is an important function in intelligent driving and can beused to determine drivable areas of the road. However, curbs are difficult todetect due to the complex road environment. This paper introduces CurbNet, anovel framework for curb detection, leveraging point cloud segmentation.Addressing the dearth of comprehensive curb datasets and the absence of 3Dannotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,which represents the largest and most categorically diverse collection of curbpoint clouds currently available. Recognizing that curbs are primarilycharacterized by height variations, our approach harnesses spatially-rich 3Dpoint clouds for training. To tackle the challenges presented by the unevendistribution of curb features on the xy-plane and their reliance on z-axishigh-frequency features, we introduce the multi-scale and channel attention(MSCA) module, a bespoke solution designed to optimize detection performance.Moreover, we propose an adaptive weighted loss function group, specificallyformulated to counteract the imbalance in the distribution of curb point cloudsrelative to other categories. Our extensive experimentation on 2 major datasetshas yielded results that surpass existing benchmarks set by leading curbdetection and point cloud segmentation models. By integrating multi-clusteringand curve fitting techniques in our post-processing stage, we havesubstantially reduced noise in curb detection, thereby enhancing precision to0.8744. Notably, CurbNet has achieved an exceptional average metrics of over0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.Furthermore, corroborative real-world experiments and dataset analyzes mutuallyvalidate each other, solidifying CurbNet's superior detection proficiency andits robust generalizability.</description><author>Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu</author><pubDate>Mon, 25 Mar 2024 15:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16794v1</guid></item><item><title>Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback</title><link>http://arxiv.org/abs/2403.16792v1</link><description>Large language models (LLMs) have shown remarkable progress in automated codegeneration. Yet, incorporating LLM-based code generation into real-lifesoftware projects poses challenges, as the generated code may contain errors inAPI usage, class, data structure, or missing project-specific information. Asmuch of this project-specific context cannot fit into the prompts of LLMs, wemust find ways to allow the model to explore the project-level code context. Tothis end, this paper puts forward a novel approach, termed ProCoder, whichiteratively refines the project-level code context for precise code generation,guided by the compiler feedback. In particular, ProCoder first leveragescompiler techniques to identify a mismatch between the generated code and theproject's context. It then iteratively aligns and fixes the identified errorsusing information extracted from the code repository. We integrate ProCoderwith two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), andapply it to Python code generation. Experimental results show that ProCodersignificantly improves the vanilla LLMs by over 80% in generating codedependent on project context, and consistently outperforms the existingretrieval-based code generation baselines.</description><author>Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin</author><pubDate>Mon, 25 Mar 2024 15:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16792v1</guid></item><item><title>Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise</title><link>http://arxiv.org/abs/2403.16790v1</link><description>Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much inthe realm of generative AI. Despite their high performance, there is room forimprovement, especially in terms of sample fidelity by utilizing statisticalproperties that impose structural integrity, such as isotropy. Minimizing themean squared error between the additive and predicted noise alone does notimpose constraints on the predicted noise to be isotropic. Thus, we weremotivated to utilize the isotropy of the additive noise as a constraint on theobjective function to enhance the fidelity of DDPMs. Our approach is simple andcan be applied to any DDPM variant. We validate our approach by presentingexperiments conducted on four synthetic 2D datasets as well as on unconditionalimage generation. As demonstrated by the results, the incorporation of thisconstraint improves the fidelity metrics, Precision and Density for the 2Ddatasets as well as for the unconditional image generation.</description><author>Dilum Fernando, Dhananjaya jayasundara, Roshan Godaliyadda, Chaminda Bandara, Parakrama Ekanayake, Vijitha Herath</author><pubDate>Mon, 25 Mar 2024 15:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16790v1</guid></item><item><title>HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation</title><link>http://arxiv.org/abs/2403.16788v1</link><description>Event-based semantic segmentation has gained popularity due to its capabilityto deal with scenarios under high-speed motion and extreme lighting conditions,which cannot be addressed by conventional RGB cameras. Since it is hard toannotate event data, previous approaches rely on event-to-image reconstructionto obtain pseudo labels for training. However, this will inevitably introducenoise, and learning from noisy pseudo labels, especially when generated from asingle source, may reinforce the errors. This drawback is also calledconfirmation bias in pseudo-labeling. In this paper, we propose a novel hybridpseudo-labeling framework for unsupervised event-based semantic segmentation,HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, wefirst employ a plain unsupervised domain adaptation framework as our baseline,which can generate a set of pseudo labels through self-training. Then, weincorporate offline event-to-image reconstruction into the framework, andobtain another set of pseudo labels by predicting segmentation maps on thereconstructed images. A noisy label learning strategy is designed to mix thetwo sets of pseudo labels and enhance the quality. Moreover, we propose a softprototypical alignment module to further improve the consistency of targetdomain features. Extensive experiments show that our proposed methodoutperforms existing state-of-the-art methods by a large margin on theDSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpassesseveral supervised methods.</description><author>Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang, Gerald Schaefer, Hui Fang, Bin Zhao, Xuelong Li</author><pubDate>Mon, 25 Mar 2024 15:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16788v1</guid></item><item><title>The Anatomy of Adversarial Attacks: Concept-based XAI Dissection</title><link>http://arxiv.org/abs/2403.16782v1</link><description>Adversarial attacks (AAs) pose a significant threat to the reliability androbustness of deep neural networks. While the impact of these attacks on modelpredictions has been extensively studied, their effect on the learnedrepresentations and concepts within these models remains largely unexplored. Inthis work, we perform an in-depth analysis of the influence of AAs on theconcepts learned by convolutional neural networks (CNNs) using eXplainableartificial intelligence (XAI) techniques. Through an extensive set ofexperiments across various network architectures and targeted AA techniques, weunveil several key findings. First, AAs induce substantial alterations in theconcept composition within the feature space, introducing new concepts ormodifying existing ones. Second, the adversarial perturbation itself can belinearly decomposed into a set of latent vector components, with a subset ofthese being responsible for the attack's success. Notably, we discover thatthese components are target-specific, i.e., are similar for a given targetclass throughout different AA techniques and starting classes. Our findingsprovide valuable insights into the nature of AAs and their impact on learnedrepresentations, paving the way for the development of more robust andinterpretable deep learning models, as well as effective defenses againstadversarial threats.</description><author>Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade</author><pubDate>Mon, 25 Mar 2024 14:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16782v1</guid></item><item><title>Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?</title><link>http://arxiv.org/abs/2403.16777v1</link><description>Multilingual pretraining and fine-tuning have remarkably succeeded in variousnatural language processing tasks. Transferring representations from onelanguage to another is especially crucial for cross-lingual learning. One canexpect machine translation objectives to be well suited to fostering suchcapabilities, as they involve the explicit alignment of semantically equivalentsentences from different languages. This paper investigates the potentialbenefits of employing machine translation as a continued training objective toenhance language representation learning, bridging multilingual pretraining andcross-lingual applications. We study this question through two lenses: aquantitative evaluation of the performance of existing models and an analysisof their latent representations. Our results show that, contrary toexpectations, machine translation as the continued training fails to enhancecross-lingual representation learning in multiple cross-lingual naturallanguage understanding tasks. We conclude that explicit sentence-levelalignment in the cross-lingual scenario is detrimental to cross-lingualtransfer pretraining, which has important implications for future cross-lingualtransfer studies. We furthermore provide evidence through similarity measuresand investigation of parameters that this lack of positive influence is due tooutput separability -- which we argue is of use for machine translation butdetrimental elsewhere.</description><author>Shaoxiong Ji, Timothee Mickus, Vincent Segonne, Jörg Tiedemann</author><pubDate>Mon, 25 Mar 2024 14:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16777v1</guid></item><item><title>Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases</title><link>http://arxiv.org/abs/2403.16776v1</link><description>Anatomical atlases are widely used for population analysis. Conditionalatlases target a particular sub-population defined via certain conditions (e.g.demographics or pathologies) and allow for the investigation of fine-grainedanatomical differences - such as morphological changes correlated with age.Existing approaches use either registration-based methods that are unable tohandle large anatomical variations or generative models, which can suffer fromtraining instabilities and hallucinations. To overcome these limitations, weuse latent diffusion models to generate deformation fields, which transform ageneral population atlas into one representing a specific sub-population. Bygenerating a deformation field and registering the conditional atlas to aneighbourhood of images, we ensure structural plausibility and avoidhallucinations, which can occur during direct image synthesis. We compare ourmethod to several state-of-the-art atlas generation methods in experimentsusing 5000 brain as well as whole-body MR images from UK Biobank. Our methodgenerates highly realistic atlases with smooth transformations and highanatomical fidelity, outperforming the baselines.</description><author>Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin Menten, Tamara Mueller, Daniel Rueckert</author><pubDate>Mon, 25 Mar 2024 14:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16776v1</guid></item><item><title>Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation</title><link>http://arxiv.org/abs/2403.16771v1</link><description>The widespread online communication in a modern multilingual world hasprovided opportunities to blend more than one language (aka code-mixedlanguage) in a single utterance. This has resulted a formidable challenge forthe computational models due to the scarcity of annotated data and presence ofnoise. A potential solution to mitigate the data scarcity problem inlow-resource setup is to leverage existing data in resource-rich languagethrough translation. In this paper, we tackle the problem of code-mixed(Hinglish and Bengalish) to English machine translation. First, wesynthetically develop HINMIX, a parallel corpus of Hinglish to English, with~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbationbased joint-training model that learns to handle noise in the real-worldcode-mixed text by parameter sharing across clean and noisy words. Further, weshow the adaptability of RCMT in a zero-shot setup for Bengalish to Englishtranslation. Our evaluation and comprehensive analyses qualitatively andquantitatively demonstrate the superiority of RCMT over state-of-the-artcode-mixed and robust translation methods.</description><author>Kartik, Sanjana Soni, Anoop Kunchukuttan, Tanmoy Chakraborty, Md Shad Akhtar</author><pubDate>Mon, 25 Mar 2024 14:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16771v1</guid></item><item><title>DeepKnowledge: Generalisation-Driven Deep Learning Testing</title><link>http://arxiv.org/abs/2403.16768v1</link><description>Despite their unprecedented success, DNNs are notoriously fragile to smallshifts in data distribution, demanding effective testing techniques that canassess their dependability. Despite recent advances in DNN testing, there is alack of systematic testing approaches that assess the DNN's capability togeneralise and operate comparably beyond data in their training distribution.We address this gap with DeepKnowledge, a systematic testing methodology forDNN-based systems founded on the theory of knowledge generalisation, which aimsto enhance DNN robustness and reduce the residual risk of 'black box' models.Conforming to this theory, DeepKnowledge posits that core computational DNNunits, termed Transfer Knowledge neurons, can generalise under domain shift.DeepKnowledge provides an objective confidence measurement on testingactivities of DNN given data distribution shifts and uses this information toinstrument a generalisation-informed test adequacy criterion to check thetransfer knowledge capacity of a test set. Our empirical evaluation of severalDNNs, across multiple datasets and state-of-the-art adversarial generationtechniques demonstrates the usefulness and effectiveness of DeepKnowledge andits ability to support the engineering of more dependable DNNs. We reportimprovements of up to 10 percentage points over state-of-the-art coveragecriteria for detecting adversarial attacks on several benchmarks, includingMNIST, SVHN, and CIFAR.</description><author>Sondess Missaoui, Simos Gerasimou, Nikolaos Matragkas</author><pubDate>Mon, 25 Mar 2024 14:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16768v1</guid></item><item><title>Geometric Generative Models based on Morphological Equivariant PDEs and GANs</title><link>http://arxiv.org/abs/2403.14897v2</link><description>Content and image generation consist in creating or generating data fromnoisy information by extracting specific features such as texture, edges, andother thin image structures. We are interested here in generative models, andtwo main problems are addressed. Firstly, the improvements of specific featureextraction while accounting at multiscale levels intrinsic geometric features;and secondly, the equivariance of the network to reduce its complexity andprovide a geometric interpretability. To proceed, we propose a geometricgenerative model based on an equivariant partial differential equation (PDE)for group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built onmorphology operators and generative adversarial networks (GANs). Equivariantmorphological PDE layers are composed of multiscale dilations and erosionsformulated in Riemannian manifolds, while group symmetries are defined on a Liegroup. We take advantage of the Lie group structure to properly integrate theequivariance in layers, and are able to use the Riemannian metric to solve themultiscale morphological operations. Each point of the Lie group is associatedwith a unique point in the manifold, which helps us derive a metric on theRiemannian manifold from a tensor field invariant under the Lie group so thatthe induced metric has the same symmetries. The proposed geometricmorphological GAN (GM-GAN) is obtained by using the proposed morphologicalequivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.GM-GAN is evaluated on MNIST data and compared with GANs. Preliminary resultsshow that GM-GAN model outperforms classical GAN.</description><author>El Hadji S. Diop, Thierno Fall, Alioune Mbengue, Mohamed Daoudi</author><pubDate>Mon, 25 Mar 2024 14:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14897v2</guid></item><item><title>As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli</title><link>http://arxiv.org/abs/2403.16760v1</link><description>As synthetic media becomes progressively more realistic and barriers to usingit continue to lower, the technology has been increasingly utilized formalicious purposes, from financial fraud to nonconsensual pornography. Today,the principal defense against being misled by synthetic media relies on theability of the human observer to visually and auditorily discern between realand fake. However, it remains unclear just how vulnerable people actually areto deceptive synthetic media in the course of their day to day lives. Weconducted a perceptual study with 1276 participants to assess how accuratepeople were at distinguishing synthetic images, audio only, video only, andaudiovisual stimuli from authentic. To reflect the circumstances under whichpeople would likely encounter synthetic media in the wild, testing conditionsand stimuli emulated a typical online platform, while all synthetic media usedin the survey was sourced from publicly accessible generative AI technology. We find that overall, participants struggled to meaningfully discern betweensynthetic and authentic content. We also find that detection performanceworsens when the stimuli contains synthetic content as compared to authenticcontent, images featuring human faces as compared to non face objects, a singlemodality as compared to multimodal stimuli, mixed authenticity as compared tobeing fully synthetic for audiovisual stimuli, and features foreign languagesas compared to languages the observer is fluent in. Finally, we also find thatprior knowledge of synthetic media does not meaningfully impact their detectionperformance. Collectively, these results indicate that people are highlysusceptible to being tricked by synthetic media in their daily lives and thathuman perceptual detection capabilities can no longer be relied upon as aneffective counterdefense.</description><author>Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly</author><pubDate>Mon, 25 Mar 2024 14:39:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16760v1</guid></item><item><title>Bi-objective Optimization in Role Mining</title><link>http://arxiv.org/abs/2403.16757v1</link><description>Role mining is a technique used to derive a role-based authorization policyfrom an existing policy. Given a set of users $U$, a set of permissions $P$ anda user-permission authorization relation $\mahtit{UPA}\subseteq U\times P$, arole mining algorithm seeks to compute a set of roles $R$, a user-roleauthorization relation $\mathit{UA}\subseteq U\times R$ and a permission-roleauthorization relation $\mathit{PA}\subseteq R\times P$, such that thecomposition of $\mathit{UA}$ and $\mathit{PA}$ is close (in some appropriatesense) to $\mathit{UPA}$. In this paper, we first introduce the Generalized Noise Role Mining problem(GNRM) -- a generalization of the MinNoise Role Mining problem -- which webelieve has considerable practical relevance. Extending work of Fomin et al.,we show that GNRM is fixed parameter tractable, with parameter $r + k$, where$r$ is the number of roles in the solution and $k$ is the number ofdiscrepancies between $\mathit{UPA}$ and the relation defined by thecomposition of $\mathit{UA}$ and $\mathit{PA}$. We further introduce abi-objective optimization variant of GNRM, where we wish to minimize both $r$and $k$ subject to upper bounds $r\le \bar{r}$ and $k\le \bar{k}$, where$\bar{r}$ and $\bar{k}$ are constants. We show that the Pareto front of thisbi-objective optimization problem (BO-GNRM) can be computed in fixed-parametertractable time with parameter $\bar{r}+\bar{k}$. We then report the results of our experimental work using the integerprogramming solver Gurobi to solve instances of BO-GNRM. Our key findings arethat (a) we obtained strong support that Gurobi's performance isfixed-parameter tractable, (b) our results suggest that our techniques may beuseful for role mining in practice, based on our experiments in the context ofthree well-known real-world authorization policies.</description><author>Jason Crampton, Eduard Eiben, Gregory Gutin, Daniel Karapetyan, Diptapriyo Majumdar</author><pubDate>Mon, 25 Mar 2024 14:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16757v1</guid></item><item><title>Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation</title><link>http://arxiv.org/abs/2403.11722v2</link><description>We propose a novel quaternionic time-series compression methodology where wedivide a long time-series into segments of data, extract the min, max, mean andstandard deviation of these chunks as representative features and encapsulatethem in a quaternion, yielding a quaternion valued time-series. Thistime-series is processed using quaternion valued neural network layers, wherewe aim to preserve the relation between these features through the usage of theHamilton product. To train this quaternion neural network, we derive quaternionbackpropagation employing the GHR calculus, which is required for a validproduct and chain rule in quaternion space. Furthermore, we investigate theconnection between the derived update rules and automatic differentiation. Weapply our proposed compression method on the Tennessee Eastman Dataset, wherewe perform fault classification using the compressed data in two settings: afully supervised one and in a semi supervised, contrastive learning setting.Both times, we were able to outperform real valued counterparts as well as twobaseline models: one with the uncompressed time-series as the input and theother with a regular downsampling using the mean. Further, we could improve theclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.</description><author>Johannes Pöppelbaum, Andreas Schwung</author><pubDate>Mon, 25 Mar 2024 14:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11722v2</guid></item><item><title>Towards Precise 3D Human Pose Estimation with Multi-Perspective Spatial-Temporal Relational Transformers</title><link>http://arxiv.org/abs/2401.16700v2</link><description>3D human pose estimation captures the human joint points in three-dimensionalspace while keeping the depth information and physical structure. That isessential for applications that require precise pose information, such ashuman-computer interaction, scene understanding, and rehabilitation training.Due to the challenges in data collection, mainstream datasets of 3D human poseestimation are primarily composed of multi-view video data collected inlaboratory environments, which contains rich spatial-temporal correlationinformation besides the image frame content. Given the remarkableself-attention mechanism of transformers, capable of capturing thespatial-temporal correlation from multi-view video datasets, we propose amulti-stage framework for 3D sequence-to-sequence (seq2seq) human posedetection. Firstly, the spatial module represents the human pose feature byintra-image content, while the frame-image relation module extracts temporalrelationships and 3D spatial positional relationship features between themulti-perspective images. Secondly, the self-attention mechanism is adopted toeliminate the interference from non-human body parts and reduce computingresources. Our method is evaluated on Human3.6M, a popular 3D human posedetection dataset. Experimental results demonstrate that our approach achievesstate-of-the-art performance on this dataset. The source code will be availableat https://github.com/WUJINHUAN/3D-human-pose.</description><author>Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang</author><pubDate>Mon, 25 Mar 2024 14:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16700v2</guid></item><item><title>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment</title><link>http://arxiv.org/abs/2402.04599v2</link><description>Video sequences exhibit significant nuisance variations (undesired effects)of speed of actions, temporal locations, and subjects' poses, leading totemporal-viewpoint misalignment when comparing two sets of frames or evaluatingthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmeraviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3Dskeleton sequences whose camera and subjects' poses can be easily manipulatedin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), wherematching well temporal blocks (temporal chunks that make up a sequence) ofsupport-query sequence pairs (by factoring out nuisance variations) isessential due to limited samples of novel classes. Given a query sequence, wecreate its several views by simulating several camera locations. For a supportsequence, we match it with view-simulated query sequences, as in the popularDynamic Time Warping (DTW). Specifically, each support temporal block can bematched to the query temporal block with the same or adjacent (next) temporalindex, and adjacent camera views to achieve joint local temporal-viewpointwarping. JEANIE selects the smallest distance among matching paths withdifferent temporal-viewpoint warping patterns, an advantage over DTW which onlyperforms temporal alignment. We also propose an unsupervised FSAR akin toclustering of sequences with JEANIE as a distance measure. JEANIE achievesstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3DMultiview Activity II on supervised and unsupervised FSAR, and theirmeta-learning inspired fusion.</description><author>Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz</author><pubDate>Mon, 25 Mar 2024 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04599v2</guid></item><item><title>An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models</title><link>http://arxiv.org/abs/2403.06764v2</link><description>In this study, we identify the inefficient attention phenomena in LargeVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,QwenVL-Chat and Video-LLaVA. We find out that the attention computation overvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,suggesting a need for a sparser approach compared to textual data handling. Tothis end, we introduce FastV, a versatile plug-and-play method designed tooptimize computational efficiency by learning adaptive attention patterns inearly layers and pruning visual tokens in subsequent ones. Our evaluationsdemonstrate FastV's ability to dramatically reduce computational costs (e.g., a45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in awide range of image and video understanding tasks. The computational efficiencyand performance trade-off of FastV are highly customizable andpareto-efficient. It can compress the FLOPs of a 13B-parameter model to achievea lower budget than that of a 7B-parameter model, while still maintainingsuperior performance. We believe FastV has practical values for deployment ofLVLMs in edge devices and commercial models. Code is released athttps://github.com/pkunlp-icler/FastV.</description><author>Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang</author><pubDate>Mon, 25 Mar 2024 14:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06764v2</guid></item><item><title>MambaIR: A Simple Baseline for Image Restoration with State-Space Model</title><link>http://arxiv.org/abs/2402.15648v2</link><description>Recent years have seen significant advancements in image restoration, largelyattributed to the development of modern deep neural networks, such as CNNs andTransformers. However, existing restoration backbones often face the dilemmabetween global receptive fields and efficient computation, hindering theirapplication in practice. Recently, the Selective Structured State Space Model,especially the improved version Mamba, has shown great potential for long-rangedependency modeling with linear complexity, which offers a way to resolve theabove dilemma. However, the standard Mamba still faces certain challenges inlow-level vision such as local pixel forgetting and channel redundancy. In thiswork, we introduce a simple but effective baseline, named MambaIR, whichintroduces both local enhancement and channel attention to improve the vanillaMamba. In this way, our MambaIR takes advantage of the local pixel similarityand reduces the channel redundancy. Extensive experiments demonstrate thesuperiority of our method, for example, MambaIR outperforms SwinIR by up to0.45dB on image SR, using similar computational cost but with a globalreceptive field. Code is available at \url{https://github.com/csguoh/MambaIR}.</description><author>Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, Shu-Tao Xia</author><pubDate>Mon, 25 Mar 2024 14:27:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15648v2</guid></item><item><title>All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification</title><link>http://arxiv.org/abs/2403.16750v1</link><description>Modern hardware designs have grown increasingly efficient and complex.However, they are often susceptible to Common Weakness Enumerations (CWEs).This paper is focused on the formal verification of CWEs in a dataset ofhardware designs written in SystemVerilog from Regenerative ArtificialIntelligence (AI) powered by Large Language Models (LLMs). We applied formalverification to categorize each hardware design as vulnerable or CWE-free. Thisdataset was generated by 4 different LLMs and features a unique set of designsfor each of the 10 CWEs we target in our paper. We have associated theidentified vulnerabilities with CWE numbers for a dataset of 60,000 generatedSystemVerilog Register Transfer Level (RTL) code. It was also found that mostLLMs are not aware of any hardware CWEs; hence they are usually not consideredwhen generating the hardware code. Our study reveals that approximately 60% ofthe hardware designs generated by LLMs are prone to CWEs, posing potentialsafety and security risks. The dataset could be ideal for training LLMs andMachine Learning (ML) algorithms to abstain from generating CWE-prone hardwaredesigns.</description><author>Deepak Narayan Gadde, Aman Kumar, Thomas Nalapat, Evgenii Rezunov, Fabio Cappellini</author><pubDate>Mon, 25 Mar 2024 14:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16750v1</guid></item><item><title>Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback</title><link>http://arxiv.org/abs/2402.02423v2</link><description>Reinforcement Learning with Human Feedback (RLHF) has received significantattention for performing tasks without the need for costly manual reward designby aligning human preferences. It is crucial to consider diverse human feedbacktypes and various learning methods in different environments. However,quantifying progress in RLHF with diverse feedback is challenging due to thelack of standardized annotation platforms and widely used unified benchmarks.To bridge this gap, we introduce Uni-RLHF, a comprehensive systemimplementation tailored for RLHF. It aims to provide a complete workflow fromreal human feedback, fostering progress in the development of practicalproblems. Uni-RLHF contains three packages: 1) a universal multi-feedbackannotation platform, 2) large-scale crowdsourced feedback datasets, and 3)modular offline RLHF baseline implementations. Uni-RLHF develops auser-friendly annotation interface tailored to various feedback types,compatible with a wide range of mainstream RL environments. We then establish asystematic pipeline of crowdsourced annotations, resulting in large-scaleannotated datasets comprising more than 15 million steps across 30+ populartasks. Through extensive experiments, the results in the collected datasetsdemonstrate competitive performance compared to those from well-designed manualrewards. We evaluate various design choices and offer insights into theirstrengths and potential areas of improvement. We wish to build valuableopen-source platforms, datasets, and baselines to facilitate the development ofmore robust and reliable RLHF solutions based on realistic human feedback. Thewebsite is available at https://uni-rlhf.github.io/.</description><author>Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng</author><pubDate>Mon, 25 Mar 2024 14:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02423v2</guid></item><item><title>Text-Conditioned Resampler For Long Form Video Understanding</title><link>http://arxiv.org/abs/2312.11897v2</link><description>In this paper we present a text-conditioned video resampler (TCR) module thatuses a pre-trained and frozen visual encoder and large language model (LLM) toprocess long video sequences for a task. TCR localises relevant visual featuresfrom the video given a text condition and provides them to a LLM to generate atext response. Due to its lightweight design and use of cross-attention, TCRcan process more than 100 frames at a time with plain attention and withoutoptimised implementations. We make the following contributions: (i) we design atransformer-based sampling architecture that can process long videosconditioned on a task, together with a training method that enables it tobridge pre-trained visual and language models; (ii) we identify tasks thatcould benefit from longer video perception; and (iii) we empirically validateits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,and the EGO4D-LTA challenge.</description><author>Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, Federico Tombari</author><pubDate>Mon, 25 Mar 2024 14:15:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11897v2</guid></item><item><title>Creating a Digital Twin of Spinal Surgery: A Proof of Concept</title><link>http://arxiv.org/abs/2403.16736v1</link><description>Surgery digitalization is the process of creating a virtual replica ofreal-world surgery, also referred to as a surgical digital twin (SDT). It hassignificant applications in various fields such as education and training,surgical planning, and automation of surgical tasks. Given their detailedrepresentations of surgical procedures, SDTs are an ideal foundation formachine learning methods, enabling automatic generation of training data. Inrobotic surgery, SDTs can provide realistic virtual environments in whichrobots may learn through trial and error. In this paper, we present a proof ofconcept (PoC) for surgery digitalization that is applied to an ex-vivo spinalsurgery performed in realistic conditions. The proposed digitalization focuseson the acquisition and modelling of the geometry and appearance of the entiresurgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction ofthe surgeon, a high-end camera for 3D reconstruction of the anatomy, aninfrared stereo camera for surgical instrument tracking, and a laser scannerfor 3D reconstruction of the operating room and data fusion. We justify theproposed methodology, discuss the challenges faced and further extensions ofour prototype. While our PoC partially relies on manual data curation, its highquality and great potential motivate the development of automated methods forthe creation of SDTs. The quality of our SDT can be assessed in a renderedvideo available at https://youtu.be/LqVaWGgaTMY .</description><author>Jonas Hein, Frederic Giraud, Lilian Calvet, Alexander Schwarz, Nicola Alessandro Cavalcanti, Sergey Prokudin, Mazda Farshad, Siyu Tang, Marc Pollefeys, Fabio Carrillo, Philipp Fürnstahl</author><pubDate>Mon, 25 Mar 2024 14:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16736v1</guid></item><item><title>Enabling Uncertainty Estimation in Iterative Neural Networks</title><link>http://arxiv.org/abs/2403.16732v1</link><description>Turning pass-through network architectures into iterative ones, which usetheir own output as input, is a well-known approach for boosting performance.In this paper, we argue that such architectures offer an additional benefit:The convergence rate of their successive outputs is highly correlated with theaccuracy of the value to which they converge. Thus, we can use the convergencerate as a useful proxy for uncertainty. This results in an approach touncertainty estimation that provides state-of-the-art estimates at a much lowercomputational cost than techniques like Ensembles, and without requiring anymodifications to the original iterative model. We demonstrate its practicalvalue by embedding it in two application domains: road detection in aerialimages and the estimation of aerodynamic properties of 2D and 3D shapes.</description><author>Nikita Durasov, Doruk Oner, Jonathan Donier, Hieu Le, Pascal Fua</author><pubDate>Mon, 25 Mar 2024 14:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16732v1</guid></item><item><title>Improving Diffusion Models's Data-Corruption Resistance using Scheduled Pseudo-Huber Loss</title><link>http://arxiv.org/abs/2403.16728v1</link><description>Diffusion models are known to be vulnerable to outliers in training data. Inthis paper we study an alternative diffusion loss function, which can preservethe high quality of generated data like the original squared $L_{2}$ loss whileat the same time being robust to outliers. We propose to use pseudo-Huber lossfunction with a time-dependent parameter to allow for the trade-off betweenrobustness on the most vulnerable early reverse-diffusion steps and finedetails restoration on the final steps. We show that pseudo-Huber loss with thetime-dependent parameter exhibits better performance on corrupted datasets inboth image and audio domains. In addition, the loss function we propose canpotentially help diffusion models to resist dataset corruption while notrequiring data filtering or purification compared to conventional trainingalgorithms.</description><author>Artem Khrapov, Vadim Popov, Tasnima Sadekova, Assel Yermekova, Mikhail Kudinov</author><pubDate>Mon, 25 Mar 2024 14:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16728v1</guid></item><item><title>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling</title><link>http://arxiv.org/abs/2401.00374v3</link><description>We propose EMAGE, a framework to generate full-body human gestures from audioand masked gestures, encompassing facial, local body, hands, and globalmovements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a newmesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body withFLAME head parameters and further refines the modeling of head, neck, andfinger movements, offering a community-standardized, high-quality 3D motioncaptured dataset. EMAGE leverages masked body gesture priors during training toboost inference performance. It involves a Masked Audio Gesture Transformer,facilitating joint training on audio-to-gesture generation and masked gesturereconstruction to effectively encode audio and body gesture hints. Encoded bodyhints from masked gestures are then separately employed to generate facial andbody movements. Moreover, EMAGE adaptively merges speech features from theaudio's rhythm and content and utilizes four compositional VQ-VAEs to enhancethe results' fidelity and diversity. Experiments demonstrate that EMAGEgenerates holistic gestures with state-of-the-art performance and is flexiblein accepting predefined spatial-temporal gesture inputs, generating complete,audio-synchronized results. Our code and dataset are available athttps://pantomatrix.github.io/EMAGE/</description><author>Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black</author><pubDate>Mon, 25 Mar 2024 14:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00374v3</guid></item><item><title>BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis</title><link>http://arxiv.org/abs/2402.07310v2</link><description>This paper presents BioNeRF, a biologically plausible architecture thatmodels scenes in a 3D representation and synthesizes new views through radiancefields. Since NeRF relies on the network weights to store the scene's3-dimensional representation, BioNeRF implements a cognitive-inspired mechanismthat fuses inputs from multiple sources into a memory-like structure, improvingthe storing capacity and extracting more intrinsic and correlated information.BioNeRF also mimics a behavior observed in pyramidal cells concerningcontextual information, in which the memory is provided as the context andcombined with the inputs of two subsequent neural models, one responsible forproducing the volumetric densities and the other the colors used to render thescene. Experimental results show that BioNeRF outperforms state-of-the-artresults concerning a quality measure that encodes human perception in twodatasets: real-world images and synthetic data.</description><author>Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, João Paulo Papa</author><pubDate>Mon, 25 Mar 2024 13:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07310v2</guid></item><item><title>Towards a Formalisation of Value-based Actions and Consequentialist Ethics</title><link>http://arxiv.org/abs/2403.16719v1</link><description>Agents act to bring about a state of the world that is more compatible withtheir personal or institutional values. To formalise this intuition, the paperproposes an action framework based on the STRIPS formalisation. Technically,the contribution expresses actions in terms of Value-based Formal Reasoning(VFR), which provides a set of propositions derived from an Agent's valueprofile and the Agent's assessment of propositions with respect to the profile.Conceptually, the contribution provides a computational framework for a form ofconsequentialist ethics which is satisficing, luralistic, act-based, andpreferential.</description><author>Adam Wyner, Tomasz Zurek, DOrota Stachura-Zurek</author><pubDate>Mon, 25 Mar 2024 13:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16719v1</guid></item><item><title>LOCOST: State-Space Models for Long Document Abstractive Summarization</title><link>http://arxiv.org/abs/2401.17919v3</link><description>State-space models are a low-complexity alternative to transformers forencoding long sequences and capturing long-term dependencies. We proposeLOCOST: an encoder-decoder architecture based on state-space models forconditional text generation with long context inputs. With a computationalcomplexity of $O(L \log L)$, this architecture can handle significantly longersequences than state-of-the-art models that are based on sparse attentionpatterns. We evaluate our model on a series of long document abstractivesummarization tasks. The model reaches a performance level that is 93-96%comparable to the top-performing sparse transformers of the same size whilesaving up to 50% memory during training and up to 87% during inference.Additionally, LOCOST effectively handles input texts exceeding 600K tokens atinference time, setting new state-of-the-art results on full-book summarizationand opening new perspectives for long input processing.</description><author>Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari</author><pubDate>Mon, 25 Mar 2024 13:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17919v3</guid></item><item><title>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</title><link>http://arxiv.org/abs/2403.02969v2</link><description>Multimodal Large Language Model (MLLMs) leverages Large Language Models as acognitive framework for diverse visual-language tasks. Recent efforts have beenmade to equip MLLMs with visual perceiving and grounding capabilities. However,there still remains a gap in providing fine-grained pixel-level perceptions andextending interactions beyond text-specific inputs. In this work, we propose{\bf{AnyRef}}, a general MLLM model that can generate pixel-wise objectperceptions and natural language descriptions from multi-modality references,such as texts, boxes, images, or audio. This innovation empowers users withgreater flexibility to engage with the model beyond textual and regionalprompts, without modality-specific designs. Through our proposed refocusingmechanism, the generated grounding output is guided to better focus on thereferenced object, implicitly incorporating additional pixel-level supervision.This simple modification utilizes attention scores generated during theinference of LLM, eliminating the need for extra computations while exhibitingperformance enhancements in both grounding masks and referring expressions.With only publicly available training data, our model achieves state-of-the-artresults across multiple benchmarks, including diverse modality referringsegmentation and region-level referring expression generation.</description><author>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie</author><pubDate>Mon, 25 Mar 2024 13:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02969v2</guid></item><item><title>One-Shot Domain Incremental Learning</title><link>http://arxiv.org/abs/2403.16707v1</link><description>Domain incremental learning (DIL) has been discussed in previous studies ondeep neural network models for classification. In DIL, we assume that sampleson new domains are observed over time. The models must classify inputs on alldomains. In practice, however, we may encounter a situation where we need toperform DIL under the constraint that the samples on the new domain areobserved only infrequently. Therefore, in this study, we consider the extremecase where we have only one sample from the new domain, which we call one-shotDIL. We first empirically show that existing DIL methods do not work well inone-shot DIL. We have analyzed the reason for this failure through variousinvestigations. According to our analysis, we clarify that the difficulty ofone-shot DIL is caused by the statistics in the batch normalization layers.Therefore, we propose a technique regarding these statistics and demonstratethe effectiveness of our technique through experiments on open datasets.</description><author>Yasushi Esaki, Satoshi Koide, Takuro Kutsuna</author><pubDate>Mon, 25 Mar 2024 13:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16707v1</guid></item><item><title>Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!</title><link>http://arxiv.org/abs/2403.10963v2</link><description>While Transformer-based neural machine translation (NMT) is very effective inhigh-resource settings, many languages lack the necessary large parallelcorpora to benefit from it. In the context of low-resource (LR) MT between twoclosely-related languages, a natural intuition is to seek benefits fromstructural "shortcuts", such as copying subwords from the source to the target,given that such language pairs often share a considerable number of identicalwords, cognates, and borrowings. We test Pointer-Generator Networks for thispurpose for six language pairs over a variety of resource ranges, and find weakimprovements for most settings. However, analysis shows that the model does notshow greater improvements for closely-related vs. more distant language pairs,or for lower resource ranges, and that the models do not exhibit the expectedusage of the mechanism for shared subwords. Our discussion of the reasons forthis behaviour highlights several general challenges for LR NMT, such as moderntokenization strategies, noisy real-world conditions, and linguisticcomplexities. We call for better scrutiny of linguistically motivatedimprovements to NMT given the blackbox nature of Transformer models, as well asfor a focus on the above problems in the field.</description><author>Niyati Bafna, Philipp Koehn, David Yarowsky</author><pubDate>Mon, 25 Mar 2024 13:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10963v2</guid></item><item><title>ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search</title><link>http://arxiv.org/abs/2403.16702v1</link><description>Retrieval-based code question answering seeks to match user queries innatural language to relevant code snippets. Previous approaches typically relyon pretraining models using crafted bi-modal and uni-modal datasets to aligntext and code representations. In this paper, we introduce ProCQA, alarge-scale programming question answering dataset extracted from theStackOverflow community, offering naturally structured mixed-modal QA pairs. Tovalidate its effectiveness, we propose a modality-agnostic contrastivepre-training approach to improve the alignment of text and code representationsof current code language models. Compared to previous models that primarilyemploy bimodal and unimodal pairs extracted from CodeSearchNet forpre-training, our model exhibits significant performance improvements across awide range of code retrieval benchmarks.</description><author>Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong</author><pubDate>Mon, 25 Mar 2024 13:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16702v1</guid></item><item><title>DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization</title><link>http://arxiv.org/abs/2403.16697v1</link><description>Source-Free Domain Generalization (SFDG) aims to develop a model that worksfor unseen target domains without relying on any source domain. Recent work,PromptStyler, employs text prompts to simulate different distribution shifts inthe joint vision-language space, allowing the model to generalize effectivelyto unseen domains without using any images. However, 1) PromptStyler's stylegeneration strategy has limitations, as all style patterns are fixed after thefirst training phase. This leads to the training set in the second trainingphase being restricted to a limited set of styles. Additionally, 2) the frozentext encoder in PromptStyler result in the encoder's output varying with thestyle of the input text prompts, making it difficult for the model to learndomain-invariant features. In this paper, we introduce Dynamic PromptStyler(DPStyler), comprising Style Generation and Style Removal modules to addressthese issues. The Style Generation module refreshes all styles at everytraining epoch, while the Style Removal module eliminates variations in theencoder's output features caused by input styles. Moreover, since the StyleGeneration module, responsible for generating style word vectors using randomsampling or style mixing, makes the model sensitive to input text prompts, weintroduce a model ensemble method to mitigate this sensitivity. Extensiveexperiments demonstrate that our framework outperforms state-of-the-art methodson benchmark datasets.</description><author>Yunlong Tang, Yuxuan Wan, Lei Qi, Xin Geng</author><pubDate>Mon, 25 Mar 2024 13:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16697v1</guid></item><item><title>Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer</title><link>http://arxiv.org/abs/2403.16695v1</link><description>Prostate cancer is a dominant health concern calling for advanced diagnostictools. Utilizing digital pathology and artificial intelligence, this studyexplores the potential of 11 deep neural network architectures for automatedGleason grading in prostate carcinoma focusing on comparing traditional andrecent architectures. A standardized image classification pipeline, based onthe AUCMEDI framework, facilitated robust evaluation using an in-house datasetconsisting of 34,264 annotated tissue tiles. The results indicated varyingsensitivity across architectures, with ConvNeXt demonstrating the strongestperformance. Notably, newer architectures achieved superior performance, eventhough with challenges in differentiating closely related Gleason grades. TheConvNeXt model was capable of learning a balance between complexity andgeneralizability. Overall, this study lays the groundwork for enhanced Gleasongrading systems, potentially improving diagnostic efficiency for prostatecancer.</description><author>Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Daniel Hieber, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Frank Kramer, Iñaki Soto-Rey, Johannes Raffler</author><pubDate>Mon, 25 Mar 2024 13:26:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16695v1</guid></item><item><title>Synapse: Learning Preferential Concepts from Visual Demonstrations</title><link>http://arxiv.org/abs/2403.16689v1</link><description>This paper addresses the problem of preference learning, which aims to learnuser-specific preferences (e.g., "good parking spot", "convenient drop-offlocation") from visual input. Despite its similarity to learning factualconcepts (e.g., "red cube"), preference learning is a fundamentally harderproblem due to its subjective nature and the paucity of person-specifictraining data. We address this problem using a new framework called Synapse,which is a neuro-symbolic approach designed to efficiently learn preferentialconcepts from limited demonstrations. Synapse represents preferences asneuro-symbolic programs in a domain-specific language (DSL) that operates overimages, and leverages a novel combination of visual parsing, large languagemodels, and program synthesis to learn programs representing individualpreferences. We evaluate Synapse through extensive experimentation including auser case study focusing on mobility-related concepts in mobile robotics andautonomous driving. Our evaluation demonstrates that Synapse significantlyoutperforms existing baselines as well as its own ablations. The code and otherdetails can be found on the project website https://amrl.cs.utexas.edu/synapse .</description><author>Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas</author><pubDate>Mon, 25 Mar 2024 13:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16689v1</guid></item><item><title>Optimal convex $M$-estimation via score matching</title><link>http://arxiv.org/abs/2403.16688v1</link><description>In the context of linear regression, we construct a data-driven convex lossfunction with respect to which empirical risk minimisation yields optimalasymptotic variance in the downstream estimation of the regressioncoefficients. Our semiparametric approach targets the best decreasingapproximation of the derivative of the log-density of the noise distribution.At the population level, this fitting process is a nonparametric extension ofscore matching, corresponding to a log-concave projection of the noisedistribution with respect to the Fisher divergence. The procedure iscomputationally efficient, and we prove that our procedure attains the minimalasymptotic covariance among all convex $M$-estimators. As an example of anon-log-concave setting, for Cauchy errors, the optimal convex loss function isHuber-like, and our procedure yields an asymptotic efficiency greater than 0.87relative to the oracle maximum likelihood estimator of the regressioncoefficients that uses knowledge of this error distribution; in this sense, weobtain robustness without sacrificing much efficiency. Numerical experimentsconfirm the practical merits of our proposal.</description><author>Oliver Y. Feng, Yu-Chun Kao, Min Xu, Richard J. Samworth</author><pubDate>Mon, 25 Mar 2024 13:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16688v1</guid></item><item><title>Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography</title><link>http://arxiv.org/abs/2403.16687v1</link><description>In recent years, the rapid development of artificial intelligence technology,especially the emergence of large language models (LLMs) such as ChatGPT, haspresented significant prospects for application in the field of education. LLMspossess the capability to interpret knowledge, answer questions, and considercontext, thus providing support for dialogic teaching to students. Therefore,an examination of the capacity of LLMs to effectively fulfill instructionalroles, thereby facilitating student learning akin to human educators withindialogic teaching scenarios, is an exceptionally valuable research topic. Thisresearch recruited 34 undergraduate students as participants, who were randomlydivided into two groups. The experimental group engaged in dialogic teachingusing ChatGPT, while the control group interacted with human teachers. Bothgroups learned the histogram equalization unit in the information-relatedcourse "Digital Image Processing". The research findings show comparable scoresbetween the two groups on the retention test. However, students who engaged indialogue with ChatGPT exhibited lower performance on the transfer test.Electroencephalography data revealed that students who interacted with ChatGPTexhibited higher levels of cognitive activity, suggesting that ChatGPT couldhelp students establish a knowledge foundation and stimulate cognitiveactivity. However, its strengths on promoting students. knowledge applicationand creativity were insignificant. Based upon the research findings, it isevident that ChatGPT cannot fully excel in fulfilling teaching tasks in thedialogue teaching in information related courses. Combining ChatGPT withtraditional human teachers might be a more ideal approach. The synergistic useof both can provide students with more comprehensive learning support, thuscontributing to enhancing the quality of teaching.</description><author>Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Senqing Qi, Taotao Long, Bao Ge</author><pubDate>Mon, 25 Mar 2024 13:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16687v1</guid></item><item><title>ToXCL: A Unified Framework for Toxic Speech Detection and Explanation</title><link>http://arxiv.org/abs/2403.16685v1</link><description>The proliferation of online toxic speech is a pertinent problem posingthreats to demographic groups. While explicit toxic speech contains offensivelexical signals, implicit one consists of coded or indirect language.Therefore, it is crucial for models not only to detect implicit toxic speechbut also to explain its toxicity. This draws a unique need for unifiedframeworks that can effectively detect and explain implicit toxic speech. Priorworks mainly formulated the task of toxic speech detection and explanation as atext generation problem. Nonetheless, models trained using this strategy can beprone to suffer from the consequent error propagation problem. Moreover, ourexperiments reveal that the detection results of such models are much lowerthan those that focus only on the detection task. To bridge these gaps, weintroduce ToXCL, a unified framework for the detection and explanation ofimplicit toxic speech. Our model consists of three modules: a (i) Target GroupGenerator to generate the targeted demographic group(s) of a given post; an(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicittoxic speech and is boosted by a (iii) Teacher Classifier via knowledgedistillation, and the decoder generates the necessary explanation. ToXCLachieves new state-of-the-art effectiveness, and outperforms baselinessignificantly.</description><author>Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan</author><pubDate>Mon, 25 Mar 2024 13:21:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16685v1</guid></item><item><title>A note on generalization bounds for losses with finite moments</title><link>http://arxiv.org/abs/2403.16681v1</link><description>This paper studies the truncation method from Alquier [1] to derivehigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.Assuming that the $p$-th moment is bounded, the resulting bounds interpolatebetween a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\to \infty$ and the loss is essentially bounded. Moreover, the paper derives ahigh-probability PAC-Bayes bound for losses with a bounded variance. This boundhas an exponentially better dependence on the confidence parameter and thedependency measure than previous bounds in the literature. Finally, the paperextends all results to guarantees in expectation and single-draw PAC-Bayes. Inorder to so, it obtains analogues of the PAC-Bayes fast rate bound for boundedlosses from [2] in these settings.</description><author>Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund</author><pubDate>Mon, 25 Mar 2024 13:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16681v1</guid></item><item><title>Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics</title><link>http://arxiv.org/abs/2403.16680v1</link><description>Learning physical simulations has been an essential and central aspect ofmany recent research efforts in machine learning, particularly forNavier-Stokes-based fluid mechanics. Classic numerical solvers havetraditionally been computationally expensive and challenging to use in inverseproblems, whereas Neural solvers aim to address both concerns through machinelearning. We propose a general formulation for continuous convolutions usingseparable basis functions as a superset of existing methods and evaluate alarge set of basis functions in the context of (a) a compressible 1D SPHsimulation, (b) a weakly compressible 2D SPH simulation, and (c) anincompressible 2D SPH Simulation. We demonstrate that even and odd symmetriesincluded in the basis functions are key aspects of stability and accuracy. Ourbroad evaluation shows that Fourier-based continuous convolutions outperformall other architectures regarding accuracy and generalization. Finally, usingthese Fourier-based networks, we show that prior inductive biases, such aswindow functions, are no longer necessary. An implementation of our approach,as well as complete datasets and solver implementations, is available athttps://github.com/tum-pbs/SFBC.</description><author>Rene Winchenbach, Nils Thuerey</author><pubDate>Mon, 25 Mar 2024 13:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16680v1</guid></item><item><title>DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks</title><link>http://arxiv.org/abs/2403.16678v1</link><description>Advances in digital pathology and artificial intelligence (AI) offerpromising opportunities for clinical decision support and enhancing diagnosticworkflows. Previous studies already demonstrated AI's potential for automatedGleason grading, but lack state-of-the-art methodology and model reusability.To address this issue, we propose DeepGleason: an open-source deep neuralnetwork based image classification system for automated Gleason grading usingwhole-slide histopathology images from prostate tissue sections. Implementedwith the standardized AUCMEDI framework, our tool employs a tile-wiseclassification approach utilizing fine-tuned image preprocessing techniques incombination with a ConvNeXt architecture which was compared to variousstate-of-the-art architectures. The neural network model was trained andvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostatecarcinoma slides. We demonstrated that DeepGleason is capable of highlyaccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,AUC of 0.991, and Accuracy of 0.974. The internal architecture comparisonrevealed that the ConvNeXt model was superior performance-wise on our datasetto established and other modern architectures like transformers. Furthermore,we were able to outperform the current state-of-the-art in tile-wisefine-classification with a sensitivity and specificity of 0.94 and 0.98 forbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vsGleason 4 &amp; 5 classification, respectively. Our tool contributes to the wideradoption of AI-based Gleason grading within the research community and pavesthe way for broader clinical application of deep learning models in digitalpathology. DeepGleason is open-source and publicly available for researchapplication in the following Git repository:https://github.com/frankkramer-lab/DeepGleason.</description><author>Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler</author><pubDate>Mon, 25 Mar 2024 13:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16678v1</guid></item><item><title>FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression</title><link>http://arxiv.org/abs/2403.16677v1</link><description>Nanosatellite constellations equipped with sensors capturing large geographicregions provide unprecedented opportunities for Earth observation. Asconstellation sizes increase, network contention poses a downlink bottleneck.Orbital Edge Computing (OEC) leverages limited onboard compute resources toreduce transfer costs by processing the raw captures at the source. However,current solutions have limited practicability due to reliance on crudefiltering methods or over-prioritizing particular downstream tasks. This work presents FOOL, an OEC-native and task-agnostic feature compressionmethod that preserves prediction performance. FOOL partitions high-resolutionsatellite imagery to maximize throughput. Further, it embeds context andleverages inter-tile dependencies to lower transfer costs with negligibleoverhead. While FOOL is a feature compressor, it can recover images withcompetitive scores on perceptual quality measures at lower bitrates. Weextensively evaluate transfer cost reduction by including the peculiarity ofintermittently available network connections in low earth orbit. Lastly, wetest the feasibility of our system for standardized nanosatellite form factors.We demonstrate that FOOL permits downlinking over 100x the data volume withoutrelying on prior information on the downstream tasks.</description><author>Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar</author><pubDate>Mon, 25 Mar 2024 13:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16677v1</guid></item><item><title>Understanding the Functional Roles of Modelling Components in Spiking Neural Networks</title><link>http://arxiv.org/abs/2403.16674v1</link><description>Spiking neural networks (SNNs), inspired by the neural circuits of the brain,are promising in achieving high computational efficiency with biologicalfidelity. Nevertheless, it is quite difficult to optimize SNNs because thefunctional roles of their modelling components remain unclear. By designing andevaluating several variants of the classic model, we systematically investigatethe functional roles of key modelling components, leakage, reset, andrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensiveexperiments, we demonstrate how these components influence the accuracy,generalization, and robustness of SNNs. Specifically, we find that the leakageplays a crucial role in balancing memory retention and robustness, the resetmechanism is essential for uninterrupted temporal processing and computationalefficiency, and the recurrence enriches the capability to model complexdynamics at a cost of robustness degradation. With these interestingobservations, we provide optimization suggestions for enhancing the performanceof SNNs in different scenarios. This work deepens the understanding of how SNNswork, which offers valuable guidance for the development of more effective androbust neuromorphic models.</description><author>Huifeng Yin, Hanle Zheng, Jiayi Mao, Siyuan Ding, Xing Liu, Mingkun Xu, Yifan Hu, Jing Pei, Lei Deng</author><pubDate>Mon, 25 Mar 2024 13:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16674v1</guid></item><item><title>Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network</title><link>http://arxiv.org/abs/2403.16669v1</link><description>Visual detection of Micro Air Vehicles (MAVs) has attracted increasingattention in recent years due to its important application in various tasks.The existing methods for MAV detection assume that the training set and testingset have the same distribution. As a result, when deployed in new domains, thedetectors would have a significant performance degradation due to domaindiscrepancy. In this paper, we study the problem of cross-domain MAV detection.The contributions of this paper are threefold. 1) We propose aMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation andrealistic images. Compared to other existing datasets, the proposed one is morecomprehensive in the sense that it covers rich scenes, diverse MAV types, andvarious viewing angles. A new benchmark for cross-domain MAV detection isproposed based on the proposed dataset. 2) We propose a Noise SuppressionNetwork (NSN) based on the framework of pseudo-labeling and a large-to-smalltraining procedure. To reduce the challenging pseudo-label noises, two novelmodules are designed in this network. The first is a prior-based curriculumlearning module for allocating adaptive thresholds for pseudo labels withdifferent difficulties. The second is a masked copy-paste augmentation modulefor pasting truly-labeled MAVs on unlabeled target images and thus decreasingpseudo-label noises. 3) Extensive experimental results verify the superiorperformance of the proposed method compared to the state-of-the-art ones. Inparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) onthe tasks of simulation-to-real adaptation, cross-scene adaptation, andcross-camera adaptation, respectively.</description><author>Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao</author><pubDate>Mon, 25 Mar 2024 13:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16669v1</guid></item><item><title>Who is bragging more online? A large scale analysis of bragging in social media</title><link>http://arxiv.org/abs/2403.16668v1</link><description>Bragging is the act of uttering statements that are likely to be positivelyviewed by others and it is extensively employed in human communication with theaim to build a positive self-image of oneself. Social media is a naturalplatform for users to employ bragging in order to gain admiration, respect,attention and followers from their audiences. Yet, little is known about thescale of bragging online and its characteristics. This paper employscomputational sociolinguistics methods to conduct the first large scale studyof bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,temporal dynamics and impact of demographic factors. Our study shows that theprevalence of bragging decreases over time within the same population of users.In addition, younger, more educated and popular users in the U.S. are morelikely to brag. Finally, we conduct an extensive linguistics analysis to unveilspecific bragging themes associated with different user traits.</description><author>Mali Jin, Daniel Preoţiuc-Pietro, A. Seza Doğruöz, Nikolaos Aletras</author><pubDate>Mon, 25 Mar 2024 13:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16668v1</guid></item><item><title>A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study</title><link>http://arxiv.org/abs/2403.02930v2</link><description>We present a detailed replication study of the BASS framework, an abstractivesummarization system based on the notion of Unified Semantic Graphs. Ourinvestigation includes challenges in replicating key components and an ablationstudy to systematically isolate error sources rooted in replicating novelcomponents. Our findings reveal discrepancies in performance compared to theoriginal work. We highlight the significance of paying careful attention evento reasonably omitted details for replicating advanced frameworks like BASS,and emphasize key practices for writing replicable papers.</description><author>Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert</author><pubDate>Mon, 25 Mar 2024 13:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02930v2</guid></item><item><title>Boosting Adversarial Transferability by Block Shuffle and Rotation</title><link>http://arxiv.org/abs/2308.10299v3</link><description>Adversarial examples mislead deep neural networks with imperceptibleperturbations and have brought significant threats to deep learning. Animportant aspect is their transferability, which refers to their ability todeceive other models, thus enabling attacks in the black-box setting. Thoughvarious methods have been proposed to boost transferability, the performancestill falls short compared with white-box attacks. In this work, we observethat existing input transformation based attacks, one of the mainstreamtransfer-based attacks, result in different attention heatmaps on variousmodels, which might limit the transferability. We also find that breaking theintrinsic relation of the image can disrupt the attention heatmap of theoriginal image. Based on this finding, we propose a novel input transformationbased attack called block shuffle and rotation (BSR). Specifically, BSR splitsthe input image into several blocks, then randomly shuffles and rotates theseblocks to construct a set of new images for gradient calculation. Empiricalevaluations on the ImageNet dataset demonstrate that BSR could achievesignificantly better transferability than the existing input transformationbased methods under single-model and ensemble-model settings. Combining BSRwith the current input transformation method can further improve thetransferability, which significantly outperforms the state-of-the-art methods.Code is available at https://github.com/Trustworthy-AI-Group/BSR</description><author>Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang</author><pubDate>Mon, 25 Mar 2024 13:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10299v3</guid></item><item><title>Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization</title><link>http://arxiv.org/abs/2403.16667v1</link><description>Portfolio optimization involves determining the optimal allocation ofportfolio assets in order to maximize a given investment objective.Traditionally, some form of mean-variance optimization is used with the aim ofmaximizing returns while minimizing risk, however, more recently, deepreinforcement learning formulations have been explored. Increasingly, investorshave demonstrated an interest in incorporating ESG objectives when makinginvestment decisions, and modifications to the classical mean-varianceoptimization framework have been developed. In this work, we study the use ofdeep reinforcement learning for responsible portfolio optimization, byincorporating ESG states and objectives, and provide comparisons againstmodified mean-variance approaches. Our results show that deep reinforcementlearning policies can provide competitive performance against mean-varianceapproaches for responsible portfolio allocation across additive andmultiplicative utility functions of financial and ESG responsibilityobjectives.</description><author>Fernando Acero, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore, Daniele Magazzeni, Manuela Veloso</author><pubDate>Mon, 25 Mar 2024 13:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16667v1</guid></item><item><title>Word4Per: Zero-shot Composed Person Retrieval</title><link>http://arxiv.org/abs/2311.16515v2</link><description>Searching for specific person has great social benefits and security value,and it often involves a combination of visual and textual information.Conventional person retrieval methods, whether image-based or text-based,usually fall short in effectively harnessing both types of information, leadingto the loss of accuracy. In this paper, a whole new task called Composed PersonRetrieval (CPR) is proposed to jointly utilize both image and text informationfor target person retrieval. However, the supervised CPR requires very costlymanual annotation dataset, while there are currently no available resources. Tomitigate this issue, we firstly introduce the Zero-shot Composed PersonRetrieval (ZS-CPR), which leverages existing domain-related data to resolve theCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, wepropose a two-stage learning framework, Word4Per, where a lightweight TextualInversion Network (TINet) and a text-based person retrieval model based onfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learnedwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text ComposedPerson Retrieval (ITCPR) dataset is built as the benchmark to assess theperformance of the proposed Word4Per framework. Extensive experiments underboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPRtask, surpassing the comparative methods by over 10\%. The code and ITCPRdataset will be publicly available athttps://github.com/Delong-liu-bupt/Word4Per.</description><author>Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong</author><pubDate>Mon, 25 Mar 2024 13:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16515v2</guid></item><item><title>Revisiting the Sleeping Beauty problem</title><link>http://arxiv.org/abs/2403.16666v1</link><description>The Sleeping Beauty problem is a probability riddle with no definite solutionfor more than two decades and its solution is of great interest in many fieldsof knowledge. There are two main competing solutions to the problem: the halferapproach, and the thirder approach. The main reason for disagreement in theliterature is connected to the use of different probability spaces to representthe same probabilistic riddle. In this work, we analyse the problem from amathematical perspective, identifying probability distributions induceddirectly from the thought experiment's rules. The precise choices ofprobability spaces provide both halfer and thirder solutions to the problem. Totry and decide on which approach to follow, a criterion involving theinformation available to Sleeping Beauty is proposed.</description><author>Paulo S. Piva, Gabriel Ruffolo</author><pubDate>Mon, 25 Mar 2024 13:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16666v1</guid></item><item><title>An Analysis of Linear Time Series Forecasting Models</title><link>http://arxiv.org/abs/2403.14587v2</link><description>Despite their simplicity, linear models perform well at time seriesforecasting, even when pitted against deeper and more expensive models. Anumber of variations to the linear model have been proposed, often includingsome form of feature normalisation that improves model generalisation. In thispaper we analyse the sets of functions expressible using these linear modelarchitectures. In so doing we show that several popular variants of linearmodels for time series forecasting are equivalent and functionallyindistinguishable from standard, unconstrained linear regression. Wecharacterise the model classes for each linear variant. We demonstrate thateach model can be reinterpreted as unconstrained linear regression over asuitably augmented feature set, and therefore admit closed-form solutions whenusing a mean-squared loss function. We provide experimental evidence that themodels under inspection learn nearly identical solutions, and finallydemonstrate that the simpler closed form solutions are superior forecastersacross 72% of test settings.</description><author>William Toner, Luke Darlow</author><pubDate>Mon, 25 Mar 2024 13:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14587v2</guid></item><item><title>A Huber Loss Minimization Approach to Byzantine Robust Federated Learning</title><link>http://arxiv.org/abs/2308.12581v2</link><description>Federated learning systems are susceptible to adversarial attacks. To combatthis, we introduce a novel aggregator based on Huber loss minimization, andprovide a comprehensive theoretical analysis. Under independent and identicallydistributed (i.i.d) assumption, our approach has several advantages compared toexisting methods. Firstly, it has optimal dependence on $\epsilon$, whichstands for the ratio of attacked clients. Secondly, our approach does not needprecise knowledge of $\epsilon$. Thirdly, it allows different clients to haveunequal data sizes. We then broaden our analysis to include non-i.i.d data,such that clients have slightly different distributions.</description><author>Puning Zhao, Fei Yu, Zhiguo Wan</author><pubDate>Mon, 25 Mar 2024 12:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12581v2</guid></item><item><title>RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict</title><link>http://arxiv.org/abs/2403.16662v1</link><description>Fact-checking is the task of verifying the factuality of a given claim byexamining the available evidence. High-quality evidence plays a vital role inenhancing fact-checking systems and facilitating the generation of explanationsthat are understandable to humans. However, the provision of both sufficientand relevant evidence for explainable fact-checking systems poses a challenge.To tackle this challenge, we propose a method based on a Large Language Modelto automatically retrieve and summarize evidence from the Web. Furthermore, weconstruct RU22Fact, a novel multilingual explainable fact-checking dataset onthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-worldclaims, optimized evidence, and referenced explanation. To establish a baselinefor our dataset, we also develop an end-to-end explainable fact-checking systemto verify claims and generate explanations. Experimental results demonstratethe prospect of optimized evidence in increasing fact-checking performance andalso indicate the possibility of further progress in the end-to-end claimverification and explanation generation tasks.</description><author>Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin</author><pubDate>Mon, 25 Mar 2024 12:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16662v1</guid></item><item><title>Design-Space Exploration of SNN Models using Application-Specific Multi-Core Architectures</title><link>http://arxiv.org/abs/2403.12061v2</link><description>With the motivation and the difficulties that currently exist incomprehending and utilizing the promising features of SNNs, we proposed a novelrun-time multi-core architecture-based simulator called "RAVSim" (RuntimeAnalysis and Visualization Simulator), a cutting-edge SNN simulator, developedusing LabVIEW and it is publicly available on their website as an officialmodule. RAVSim is a runtime virtual simulation environment tool that enablesthe user to interact with the model, observe its behavior of outputconcentration, and modify the set of parametric values at any time while thesimulation is in execution. Recently some popular tools have been presented,but we believe that none of the tools allow users to interact with the modelsimulation in run time.</description><author>Sanaullah, Shamini Koravuna, Ulrich Rückert, Thorsten Jungeblut</author><pubDate>Mon, 25 Mar 2024 12:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12061v2</guid></item><item><title>LongHeads: Multi-Head Attention is Secretly a Long Context Processor</title><link>http://arxiv.org/abs/2402.10685v2</link><description>Large language models (LLMs) have achieved impressive performance in numerousdomains but often struggle to process lengthy inputs effectively andefficiently due to limited length generalization and attention's quadraticcomputational demands. Many sought to mitigate this by restricting theattention window within the pre-trained length. However, these methodsintroduce new issues such as ignoring the middle context and requiringadditional training. To address these problems, we propose LongHeads, atraining-free framework that enhances LLM's long context ability by unlockingmulti-head attention's untapped potential. Instead of allowing each head toattend to the full sentence, which struggles with generalizing to longersequences due to out-of-distribution (OOD) issues, we allow each head toprocess in-distribution length by selecting and attending to important contextchunks. To this end, we propose a chunk selection strategy that relies on theinherent correlation between the query and the key representations, efficientlydistributing context chunks to different heads. In this way, each head ensuresit can effectively process attended tokens within the trained length, whiledifferent heads in different layers can collectively process longer contexts.LongHeads works efficiently in linear time, fits seamlessly with many LLMs thatuse relative positional encoding. LongHeads achieves 100% accuracy at the 128klength on passkey retrieval task, verifying LongHeads's efficacy in extendingthe usable context window for existing models. We release our code athttps://github.com/LuLuLuyi/LongHeads .</description><author>Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Mon, 25 Mar 2024 12:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10685v2</guid></item><item><title>Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning</title><link>http://arxiv.org/abs/2402.05305v2</link><description>The advancement of knowledge distillation has played a crucial role inenabling the transfer of knowledge from larger teacher models to smaller andmore efficient student models, and is particularly beneficial for online andresource-constrained applications. The effectiveness of the student modelheavily relies on the quality of the distilled knowledge received from theteacher. Given the accessibility of unlabelled remote sensing data,semi-supervised learning has become a prevalent strategy for enhancing modelperformance. However, relying solely on semi-supervised learning with smallermodels may be insufficient due to their limited capacity for featureextraction. This limitation restricts their ability to exploit training data.To address this issue, we propose an integrated approach that combinesknowledge distillation and semi-supervised learning methods. This hybridapproach leverages the robust capabilities of large models to effectivelyutilise large unlabelled data whilst subsequently providing the small studentmodel with rich and informative features for enhancement. The proposedsemi-supervised learning-based knowledge distillation (SSLKD) approachdemonstrates a notable improvement in the performance of the student model, inthe application of road segmentation, surpassing the effectiveness oftraditional semi-supervised learning methods.</description><author>Wanli Ma, Oktay Karakus, Paul L. Rosin</author><pubDate>Mon, 25 Mar 2024 12:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05305v2</guid></item><item><title>Graph Augmentation for Recommendation</title><link>http://arxiv.org/abs/2403.16656v1</link><description>Graph augmentation with contrastive learning has gained significant attentionin the field of recommendation systems due to its ability to learn expressiveuser representations, even when labeled data is limited. However, directlyapplying existing GCL models to real-world recommendation environments poseschallenges. There are two primary issues to address. Firstly, the lack ofconsideration for data noise in contrastive learning can result in noisyself-supervised signals, leading to degraded performance. Secondly, manyexisting GCL approaches rely on graph neural network (GNN) architectures, whichcan suffer from over-smoothing problems due to non-adaptive message passing. Toaddress these challenges, we propose a principled framework called GraphAug.This framework introduces a robust data augmentor that generates denoisedself-supervised signals, enhancing recommender systems. The GraphAug frameworkincorporates a graph information bottleneck (GIB)-regularized augmentationparadigm, which automatically distills informative self-supervision informationand adaptively adjusts contrastive view generation. Through rigorousexperimentation on real-world datasets, we thoroughly assessed the performanceof our novel GraphAug model. The outcomes consistently unveil its superiorityover existing baseline methods. The source code for our model is publiclyavailable at: https://github.com/HKUDS/GraphAug.</description><author>Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen</author><pubDate>Mon, 25 Mar 2024 12:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16656v1</guid></item><item><title>Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems</title><link>http://arxiv.org/abs/2308.02935v2</link><description>Autonomous driving systems have extended the spectrum of Web of Things forintelligent vehicles and have become an important component of the Webecosystem. Similar to traditional Web-based applications, fairness is anessential aspect for ensuring the high quality of autonomous driving systems,particularly in the context of pedestrian detectors within them. However, thereis an absence in the literature of a comprehensive assessment of the fairnessof current Deep Learning (DL)-based pedestrian detectors. To fill the gap, weevaluate eight widely-explored DL-based pedestrian detectors across demographicgroups on large-scale real-world datasets. To enable a thorough fairnessevaluation, we provide extensive annotations for the datasets, resulting in8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tonelabels. Our findings reveal significant fairness issues related to age. Theundetected proportions for adults are 20.14% lower compared to children.Furthermore, we explore how various driving scenarios affect the fairness ofpedestrian detectors. We find that the bias may exacerbate for children andfemales towards low brightness and low contrast.</description><author>Xinyue Li, Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Ying Zhang, Xuanzhe Liu</author><pubDate>Mon, 25 Mar 2024 12:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02935v2</guid></item><item><title>Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT</title><link>http://arxiv.org/abs/2403.16655v1</link><description>Text continues to remain a relevant form of representation for information.Text documents are created either in digital native platforms or through theconversion of other media files such as images and speech. While the digitalnative text is invariably obtained through physical or virtual keyboards,technologies such as OCR and speech recognition are utilized to transform theimages and speech signals into text content. All these variety of mechanisms oftext generation also introduce errors into the captured text. This project aims at analyzing different kinds of error that occurs in textdocuments. The work employs two of the advanced deep neural network-basedlanguage models, namely, BART and MarianMT, to rectify the anomalies present inthe text. Transfer learning of these models with available dataset is performedto finetune their capacity for error correction. A comparative study isconducted to investigate the effectiveness of these models in handling each ofthe defined error categories. It is observed that while both models can bringdown the erroneous sentences by 20+%, BART can handle spelling errors farbetter (24.6%) than grammatical errors (8.8%).</description><author>Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS</author><pubDate>Mon, 25 Mar 2024 12:45:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16655v1</guid></item><item><title>A Novel Loss Function-based Support Vector Machine for Binary Classification</title><link>http://arxiv.org/abs/2403.16654v1</link><description>The previous support vector machine(SVM) including $0/1$ loss SVM, hinge lossSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked thedegree of penalty for the correctly classified samples within the margin. Thisoversight affects the generalization ability of the SVM classifier to someextent. To address this limitation, from the perspective of confidence margin,we propose a novel Slide loss function ($\ell_s$) to construct the supportvector machine classifier($\ell_s$-SVM). By introducing the concept of proximalstationary point, and utilizing the property of Lipschitz continuity, we derivethe first-order optimality conditions for $\ell_s$-SVM. Based on this, wedefine the $\ell_s$ support vectors and working set of $\ell_s$-SVM. Toefficiently handle $\ell_s$-SVM, we devise a fast alternating direction methodof multipliers with the working set ($\ell_s$-ADMM), and provide theconvergence analysis. The numerical experiments on real world datasets confirmthe robustness and effectiveness of the proposed method.</description><author>Yan Li, Liping Zhang</author><pubDate>Mon, 25 Mar 2024 12:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16654v1</guid></item><item><title>Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation</title><link>http://arxiv.org/abs/2402.11922v3</link><description>Spatio-temporal modeling is foundational for smart city applications, yet itis often hindered by data scarcity in many cities and regions. To bridge thisgap, we propose a novel generative pre-training framework, GPD, forspatio-temporal few-shot learning with urban knowledge transfer. Unlikeconventional approaches that heavily rely on common feature extraction orintricate few-shot learning designs, our solution takes a novel approach byperforming generative pre-training on a collection of neural network parametersoptimized with data from source cities. We recast spatio-temporal few-shotlearning as pre-training a generative diffusion model, which generates tailoredneural networks guided by prompts, allowing for adaptability to diverse datadistributions and city-specific characteristics. GPD employs aTransformer-based denoising diffusion model, which is model-agnostic tointegrate with powerful spatio-temporal neural networks. By addressingchallenges arising from data gaps and the complexity of generalizing knowledgeacross cities, our framework consistently outperforms state-of-the-artbaselines on multiple real-world datasets for tasks such as traffic speedprediction and crowd flow prediction. The implementation of our approach isavailable: https://github.com/tsinghua-fib-lab/GPD.</description><author>Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li</author><pubDate>Mon, 25 Mar 2024 12:39:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11922v3</guid></item><item><title>CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment</title><link>http://arxiv.org/abs/2403.16649v1</link><description>Reinforcement learning from human feedback (RLHF) is a crucial technique inaligning large language models (LLMs) with human preferences, ensuring theseLLMs behave in beneficial and comprehensible ways to users. However, alongstanding challenge in human alignment techniques based on reinforcementlearning lies in their inherent complexity and difficulty in training. Toaddress this challenge, we present a simple yet effective Contrastive LearningFramework for Human Alignment (CLHA) to align LLMs with human preferencesdirectly. CLHA employs a novel rescoring strategy to evaluate the noise withinthe data by considering its inherent quality and dynamically adjusting thetraining process. Simultaneously, CLHA utilizes pairwise contrastive loss andadaptive supervised fine-tuning loss to adaptively modify the likelihood ofgenerating responses, ensuring enhanced alignment with human preferences. Usingadvanced methods, CLHA surpasses other algorithms, showcasing superiorperformance in terms of reward model scores, automatic evaluations, and humanassessments on the widely used ``\textit{Helpful and Harmless}'' dataset.</description><author>Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao, Chengming Li, Xiping Hu, Ruifeng Xu</author><pubDate>Mon, 25 Mar 2024 12:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16649v1</guid></item><item><title>HiFi-123: Towards High-fidelity One Image to 3D Content Generation</title><link>http://arxiv.org/abs/2310.06744v2</link><description>Recent advances in diffusion models have enabled 3D generation from a singleimage. However, current methods often produce suboptimal results for novelviews, with blurred textures and deviations from the reference image, limitingtheir practical applications. In this paper, we introduce HiFi-123, a methoddesigned for high-fidelity and multi-view consistent 3D generation. Ourcontributions are twofold: First, we propose a Reference-Guided Novel ViewEnhancement (RGNV) technique that significantly improves the fidelity ofdiffusion-based zero-shot novel view synthesis methods. Second, capitalizing onthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.When incorporated into the optimization-based image-to-3D pipeline, our methodsignificantly improves 3D generation quality, achieving state-of-the-artperformance. Comprehensive evaluations demonstrate the effectiveness of ourapproach over existing methods, both qualitatively and quantitatively. Videoresults are available on the project page.</description><author>Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Wenbo Hu, Long Quan, Ying Shan, Yonghong Tian</author><pubDate>Mon, 25 Mar 2024 12:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06744v2</guid></item><item><title>Clustering Propagation for Universal Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.16646v1</link><description>Prominent solutions for medical image segmentation are typically tailored forautomatic or interactive setups, posing challenges in facilitating progressachieved in one task to another.$_{\!}$ This$_{\!}$ also$_{\!}$necessitates$_{\!}$ separate$_{\!}$ models for each task, duplicating bothtraining time and parameters.$_{\!}$ To$_{\!}$ address$_{\!}$ above$_{\!}$issues,$_{\!}$ we$_{\!}$ introduce$_{\!}$ S2VNet,$_{\!}$ a$_{\!}$universal$_{\!}$ framework$_{\!}$ that$_{\!}$ leverages$_{\!}$Slice-to-Volume$_{\!}$ propagation$_{\!}$ to$_{\!}$ unify automatic/interactivesegmentation within a single model and one training session. Inspired byclustering-based segmentation techniques, S2VNet makes full use of theslice-wise structure of volumetric data by initializing cluster centers fromthe cluster$_{\!}$ results$_{\!}$ of$_{\!}$ previous$_{\!}$ slice.$_{\!}$ Thisenables knowledge acquired from prior slices to assist in the segmentation ofthe current slice, further efficiently bridging the communication betweenremote slices using mere 2D networks. Moreover, such a framework readilyaccommodates interactive segmentation with no architectural change, simply byinitializing centroids from user inputs. S2VNet distinguishes itself by swiftinference speeds and reduced memory consumption compared to prevailing 3Dsolutions. It can also handle multi-class interactions with each of themserving to initialize different centroids. Experiments on three benchmarksdemonstrate S2VNet surpasses task-specified solutions on bothautomatic/interactive setups.</description><author>Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang</author><pubDate>Mon, 25 Mar 2024 12:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16646v1</guid></item><item><title>Bridging the Sim-to-Real Gap with Bayesian Inference</title><link>http://arxiv.org/abs/2403.16644v1</link><description>We present SIM-FSVGD for learning robot dynamics from data. As opposed totraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., inthe form of simulators, to regularize the training of neural network models.While learning accurate dynamics already in the low data regime, SIM-FSVGDscales and excels also when more data is available. We empirically show thatlearning with implicit physical priors results in accurate mean modelestimation as well as precise uncertainty quantification. We demonstrate theeffectiveness of SIM-FSVGD in bridging the sim-to-real gap on ahigh-performance RC racecar system. Using model-based RL, we demonstrate ahighly dynamic parking maneuver with drifting, using less than half the datacompared to the state of the art.</description><author>Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause</author><pubDate>Mon, 25 Mar 2024 12:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16644v1</guid></item><item><title>Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution</title><link>http://arxiv.org/abs/2403.16643v1</link><description>Artifact-free super-resolution (SR) aims to translate low-resolution imagesinto their high-resolution counterparts with a strict integrity of the originalcontent, eliminating any distortions or synthetic details. While traditionaldiffusion-based SR techniques have demonstrated remarkable abilities to enhanceimage detail, they are prone to artifact introduction during iterativeprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,deviate from the true structure of the source image, thus challenging theintegrity of the super-resolution process. In this work, we proposeSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method thatdelves into the latent space to effectively identify and mitigate thepropagation of artifacts. Our SARGD begins by using an artifact detector toidentify implausible pixels, creating a binary mask that highlights artifacts.Following this, the Reality Guidance Refinement (RGR) process refines artifactsby integrating this mask with realistic latent representations, improvingalignment with the original image. Nonetheless, initial realistic-latentrepresentations from lower-quality images result in over-smoothing in the finaloutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.It dynamically computes a reality score, enhancing the sharpness of therealistic latent. These alternating mechanisms collectively achieveartifact-free super-resolution. Extensive experiments demonstrate thesuperiority of our method, delivering detailed artifact-free high-resolutionimages while reducing sampling steps by 2X. We release our code athttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.</description><author>Qingping Zheng, Ling Zheng, Yuanfan Guo, Ying Li, Songcen Xu, Jiankang Deng, Hang Xu</author><pubDate>Mon, 25 Mar 2024 12:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16643v1</guid></item><item><title>Multi-Scale Texture Loss for CT denoising with GANs</title><link>http://arxiv.org/abs/2403.16640v1</link><description>Generative Adversarial Networks (GANs) have proved as a powerful frameworkfor denoising applications in medical imaging. However, GAN-based denoisingalgorithms still suffer from limitations in capturing complex relationshipswithin the images. In this regard, the loss function plays a crucial role inguiding the image generation process, encompassing how much a synthetic imagediffers from a real image. To grasp highly complex and non-linear texturalrelationships in the training process, this work presents a loss function thatleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrenceMatrix (GLCM). Although the recent advances in deep learning have demonstratedsuperior performance in classification and detection tasks, we hypothesize thatits information content can be valuable when integrated into GANs' training. Tothis end, we propose a differentiable implementation of the GLCM suited forgradient-based optimization. Our approach also introduces a self-attentionlayer that dynamically aggregates the multi-scale texture information extractedfrom the images. We validate our approach by carrying out extensive experimentsin the context of low-dose CT denoising, a challenging application that aims toenhance the quality of noisy CT scans. We utilize three publicly availabledatasets, including one simulated and two real datasets. The results arepromising as compared to other well-established loss functions, being alsoconsistent across three different GAN architectures. The code is available at:https://github.com/FrancescoDiFeola/DenoTextureLoss</description><author>Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda</author><pubDate>Mon, 25 Mar 2024 12:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16640v1</guid></item><item><title>AI-Generated Video Detection via Spatio-Temporal Anomaly Learning</title><link>http://arxiv.org/abs/2403.16638v1</link><description>The advancement of generation models has led to the emergence of highlyrealistic artificial intelligence (AI)-generated videos. Malicious users caneasily create non-existent videos to spread false information. This letterproposes an effective AI-generated video detection (AIGVDet) scheme bycapturing the forensic traces with a two-branch spatio-temporal convolutionalneural network (CNN). Specifically, two ResNet sub-detectors are learnedseparately for identifying the anomalies in spatical and optical flow domains,respectively. Results of such sub-detectors are fused to further enhance thediscrimination ability. A large-scale generated video dataset (GVD) isconstructed as a benchmark for model training and evaluation. Extensiveexperimental results verify the high generalization and robustness of ourAIGVDet scheme. Code and dataset will be available athttps://github.com/multimediaFor/AIGVDet.</description><author>Jianfa Bai, Man Lin, Gang Cao</author><pubDate>Mon, 25 Mar 2024 12:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16638v1</guid></item><item><title>SVGDreamer: Text Guided SVG Generation with Diffusion Model</title><link>http://arxiv.org/abs/2312.16476v4</link><description>Recently, text-guided scalable vector graphics (SVGs) synthesis has shownpromise in domains such as iconography and sketch. However, existingtext-to-SVG generation methods lack editability and struggle with visualquality and result diversity. To address these limitations, we propose a noveltext-guided vector graphics synthesis method called SVGDreamer. SVGDreamerincorporates a semantic-driven image vectorization (SIVE) process that enablesthe decomposition of synthesis into foreground objects and background, therebyenhancing editability. Specifically, the SIVE process introduce attention-basedprimitive control and an attention-mask loss function for effective control andmanipulation of individual elements. Additionally, we propose a VectorizedParticle-based Score Distillation (VPSD) approach to tackle the challenges ofshape over-smoothing, color over-saturation, limited diversity in results, andslow convergence in existing text-to-SVG generation methods. VPSD models SVGsas distributions of control points and colors to counteract over-smoothing andover-saturation. Furthermore, VPSD leverages a reward model to reweight vectorparticles, which improves aesthetic appeal and accelerates convergence.Extensive experiments have been conducted to validate the effectiveness ofSVGDreamer, demonstrating its superiority over baseline methods in terms ofeditability, visual quality, and diversity. The code and demo of SVGDreamer canbe found at https://ximinng.github.io/SVGDreamer-project/</description><author>Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu</author><pubDate>Mon, 25 Mar 2024 12:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16476v4</guid></item><item><title>V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster</title><link>http://arxiv.org/abs/2403.16635v1</link><description>The objective of the collaborative vehicle-to-everything perception task isto enhance the individual vehicle's perception capability through messagecommunication among neighboring traffic agents. Previous methods focus onachieving optimal performance within bandwidth limitations and typically adoptBEV maps as the basic collaborative message units. However, we demonstrate thatcollaboration with dense representations is plagued by object featuredestruction during message packing, inefficient message aggregation forlong-range collaboration, and implicit structure representation communication.To tackle these issues, we introduce a brand new message unit, namely pointcluster, designed to represent the scene sparsely with a combination oflow-level structure information and high-level semantic information. The pointcluster inherently preserves object information while packing messages, withweak relevance to the collaboration range, and supports explicit structuremodeling. Building upon this representation, we propose a novel frameworkV2X-PC for collaborative perception. This framework includes a Point ClusterPacking (PCP) module to keep object feature and manage bandwidth through themanipulation of cluster point numbers. As for effective message aggregation, wepropose a Point Cluster Aggregation (PCA) module to match and merge pointclusters associated with the same object. To further handle time latency andpose errors encountered in real-world scenarios, we propose parameter-freesolutions that can adapt to different noisy levels without finetuning.Experiments on two widely recognized collaborative perception benchmarksshowcase the superior performance of our method compared to the previousstate-of-the-art approaches relying on BEV maps.</description><author>Si Liu, Zihan Ding, Jiahui Fu, Hongyu Li, Siheng Chen, Shifeng Zhang, Xu Zhou</author><pubDate>Mon, 25 Mar 2024 12:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16635v1</guid></item><item><title>A comparative analysis of embedding models for patent similarity</title><link>http://arxiv.org/abs/2403.16630v1</link><description>This paper makes two contributions to the field of text-based patentsimilarity. First, it compares the performance of different kinds ofpatent-specific pretrained embedding models, namely static word embeddings(such as word2vec and doc2vec models) and contextual word embeddings (such astransformers based models), on the task of patent similarity calculation.Second, it compares specifically the performance of Sentence Transformers(SBERT) architectures with different training phases on the patent similaritytask. To assess the models' performance, we use information about patentinterferences, a phenomenon in which two or more patent claims belonging todifferent patent applications are proven to be overlapping by patent examiners.Therefore, we use these interferences cases as a proxy for maximum similaritybetween two patents, treating them as ground-truth to evaluate the performanceof the different embedding models. Our results point out that, first, PatentSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformerarchitecture proposed in this research, outperforms the currentstate-of-the-art in patent similarity. Second, they show that, in some cases,large static models performances are still comparable to contextual ones whentrained on extensive data; thus, we believe that the superiority in theperformance of contextual embeddings may not be related to the actualarchitecture but rather to the way the training phase is performed.</description><author>Grazia Sveva Ascione, Valerio Sterzi</author><pubDate>Mon, 25 Mar 2024 12:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16630v1</guid></item><item><title>SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</title><link>http://arxiv.org/abs/2403.16627v1</link><description>Recent advancements in diffusion models have positioned them at the forefrontof image generation. Despite their superior performance, diffusion models arenot without drawbacks; they are characterized by complex architectures andsubstantial computational demands, resulting in significant latency due totheir iterative sampling process. To mitigate these limitations, we introduce adual approach involving model miniaturization and a reduction in samplingsteps, aimed at significantly decreasing model latency. Our methodologyleverages knowledge distillation to streamline the U-Net and image decoderarchitectures, and introduces an innovative one-step DM training technique thatutilizes feature matching and score distillation. We present two models,SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,respectively. Moreover, our training approach offers promising applications inimage-conditioned control, facilitating efficient image-to-image translation.</description><author>Yuda Song, Zehao Sun, Xuanwu Yin</author><pubDate>Mon, 25 Mar 2024 12:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16627v1</guid></item><item><title>The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies</title><link>http://arxiv.org/abs/2310.07471v2</link><description>Blockchain promises to enhance distributed machine learning (ML) approachessuch as federated learning (FL) by providing further decentralization,security, immutability, and trust, which are key properties for enablingcollaborative intelligence in next-generation applications. Nonetheless, theintrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leadsto an uncharted setting for FL, whereby the concepts of FL round and globalmodel become meaningless, as devices' synchronization is lost without thefigure of a central orchestrating server. In this paper, we study the practicalimplications of outsourcing the orchestration of FL to a democratic settingsuch as in a blockchain. In particular, we focus on the effects that modelstaleness and inconsistencies, endorsed by blockchains' modus operandi, have onthe training procedure held by FL devices asynchronously. Using simulation, weevaluate the blockchained FL operation by applying two different ML models(ranging from low to high complexity) on the well-known MNIST and CIFAR-10datasets, respectively, and focus on the accuracy and timeliness of thesolutions. Our results show the high impact of model inconsistencies on theaccuracy of the models (up to a ~35% decrease in prediction accuracy), whichunderscores the importance of properly designing blockchain systems based onthe characteristics of the underlying FL application.</description><author>Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini</author><pubDate>Mon, 25 Mar 2024 12:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07471v2</guid></item><item><title>Variational Bayes image restoration with compressive autoencoders</title><link>http://arxiv.org/abs/2311.17744v2</link><description>Regularization of inverse problems is of paramount importance incomputational imaging. The ability of neural networks to learn efficient imagerepresentations has been recently exploited to design powerful data-drivenregularizers. While state-of-the-art plug-and-play methods rely on an implicitregularization provided by neural denoisers, alternative Bayesian approachesconsider Maximum A Posteriori (MAP) estimation in the latent space of agenerative model, thus with an explicit regularization. However,state-of-the-art deep generative models require a huge amount of training datacompared to denoisers. Besides, their complexity hampers the optimizationinvolved in latent MAP derivation. In this work, we first propose to usecompressive autoencoders instead. These networks, which can be seen asvariational autoencoders with a flexible latent prior, are smaller and easierto train than state-of-the-art generative models. As a second contribution, weintroduce the Variational Bayes Latent Estimation (VBLE) algorithm, whichperforms latent estimation within the framework of variational inference.Thanks to a simple yet efficient parameterization of the variational posterior,VBLE allows for fast and easy (approximate) posterior sampling. Experimentalresults on image datasets BSD and FFHQ demonstrate that VBLE reaches similarperformance than state-of-the-art plug-and-play methods, while being able toquantify uncertainties faster than other existing posterior samplingtechniques.</description><author>Maud Biquard, Marie Chabert, Thomas Oberlin</author><pubDate>Mon, 25 Mar 2024 12:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17744v2</guid></item><item><title>Mask Grounding for Referring Image Segmentation</title><link>http://arxiv.org/abs/2312.12198v2</link><description>Referring Image Segmentation (RIS) is a challenging task that requires analgorithm to segment objects referred by free-form language expressions.Despite significant progress in recent years, most state-of-the-art (SOTA)methods still suffer from considerable language-image modality gap at the pixeland word level. These methods generally 1) rely on sentence-level languagefeatures for language-image alignment and 2) lack explicit training supervisionfor fine-grained visual grounding. Consequently, they exhibit weak object-levelcorrespondence between visual and language features. Without well-groundedfeatures, prior methods struggle to understand complex expressions that requirestrong reasoning over relationships among multiple objects, especially whendealing with rarely used or ambiguous clauses. To tackle this challenge, weintroduce a novel Mask Grounding auxiliary task that significantly improvesvisual grounding within language features, by explicitly teaching the model tolearn fine-grained correspondence between masked textual tokens and theirmatching visual objects. Mask Grounding can be directly used on prior RISmethods and consistently bring improvements. Furthermore, to holisticallyaddress the modality gap, we also design a cross-modal alignment loss and anaccompanying alignment module. These additions work synergistically with MaskGrounding. With all these techniques, our comprehensive approach culminates inMagNet (Mask-grounded Network), an architecture that significantly outperformsprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstratingour method's effectiveness in addressing current limitations of RIS algorithms.Our code and pre-trained weights will be released.</description><author>Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong Qiu, Gao Huang</author><pubDate>Mon, 25 Mar 2024 12:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12198v2</guid></item><item><title>A unified front-end framework for English text-to-speech synthesis</title><link>http://arxiv.org/abs/2305.10666v3</link><description>The front-end is a critical component of English text-to-speech (TTS)systems, responsible for extracting linguistic features that are essential fora text-to-speech model to synthesize speech, such as prosodies and phonemes.The English TTS front-end typically consists of a text normalization (TN)module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme(G2P) module. However, current research on the English TTS front-end focusessolely on individual modules, neglecting the interdependence between them andresulting in sub-optimal performance for each module. Therefore, this paperproposes a unified front-end framework that captures the dependencies among theEnglish TTS front-end modules. Extensive experiments have demonstrated that theproposed method achieves state-of-the-art (SOTA) performance in all modules.</description><author>Zelin Ying, Chen Li, Yu Dong, Qiuqiang Kong, Qiao Tian, Yuanyuan Huo, Yuxuan Wang</author><pubDate>Mon, 25 Mar 2024 11:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10666v3</guid></item><item><title>Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure</title><link>http://arxiv.org/abs/2308.03472v2</link><description>Renewable energy generation is of utmost importance for globaldecarbonization. Forecasting renewable energies, particularly wind energy, ischallenging due to the inherent uncertainty in wind energy generation, whichdepends on weather conditions. Recent advances in hierarchical forecastingthrough reconciliation have demonstrated a significant increase in the qualityof wind energy forecasts for short-term periods. We leverage thecross-sectional and temporal hierarchical structure of turbines in wind farmsand build cross-temporal hierarchies to further investigate how integratedcross-sectional and temporal dimensions can add value to forecast accuracy inwind farms. We found that cross-temporal reconciliation was superior toindividual cross-sectional reconciliation at multiple temporal aggregations.Additionally, machine learning based forecasts that were cross-temporallyreconciled demonstrated high accuracy at coarser temporal granularities, whichmay encourage adoption for short-term wind forecasts. Empirically, we provideinsights for decision-makers on the best methods for forecasting high-frequencywind data across different forecasting horizons and levels.</description><author>Lucas English, Mahdi Abolghasemi</author><pubDate>Mon, 25 Mar 2024 11:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03472v2</guid></item><item><title>Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations</title><link>http://arxiv.org/abs/2402.00591v3</link><description>This paper presents sandra, a neuro-symbolic reasoner combining vectorialrepresentations with deductive reasoning. Sandra builds a vector spaceconstrained by an ontology and performs reasoning over it. The geometric natureof the reasoner allows its combination with neural networks, bridging the gapwith symbolic knowledge representations. Sandra is based on the Description andSituation (DnS) ontology design pattern, a formalization of frame semantics.Given a set of facts (a situation) it allows to infer all possible perspectives(descriptions) that can provide a plausible interpretation for it, even inpresence of incomplete information. We prove that our method is correct withrespect to the DnS model. We experiment with two different tasks and theirstandard benchmarks, demonstrating that, without increasing complexity, sandra(i) outperforms all the baselines (ii) provides interpretability in theclassification process, and (iii) allows control over the vector space, whichis designed a priori.</description><author>Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti</author><pubDate>Mon, 25 Mar 2024 11:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00591v3</guid></item><item><title>On the Relationship between Skill Neurons and Robustness in Prompt Tuning</title><link>http://arxiv.org/abs/2309.12263v2</link><description>Prompt Tuning is a popular parameter-efficient finetuning method forpre-trained large language models (PLMs). Based on experiments with RoBERTa, ithas been suggested that Prompt Tuning activates specific neurons in thetransformer's feed-forward networks, that are highly predictive and selectivefor the given task. In this paper, we study the robustness of Prompt Tuning inrelation to these "skill neurons", using RoBERTa and T5. We show that promptstuned for a specific task are transferable to tasks of the same type but arenot very robust to adversarial data. While prompts tuned for RoBERTa yieldbelow-chance performance on adversarial data, prompts tuned for T5 are slightlymore robust and retain above-chance performance in two out of three cases. Atthe same time, we replicate the finding that skill neurons exist in RoBERTa andfurther show that skill neurons also exist in T5. Interestingly, the skillneurons of T5 determined on non-adversarial data are also among the mostpredictive neurons on the adversarial data, which is not the case for RoBERTa.We conclude that higher adversarial robustness may be related to a model'sability to consistently activate the relevant skill neurons on adversarialdata.</description><author>Leon Ackermann, Xenia Ohmer</author><pubDate>Mon, 25 Mar 2024 11:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12263v2</guid></item><item><title>Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts</title><link>http://arxiv.org/abs/2403.16614v1</link><description>Tasks such as semantic search and clustering on crisis-related social mediatexts enhance our comprehension of crisis discourse, aiding decision-making andtargeted interventions. Pre-trained language models have advanced performancein crisis informatics, but their contextual embeddings lack semanticmeaningfulness. Although the CrisisTransformers family includes a sentenceencoder to address the semanticity issue, it remains monolingual, processingonly English texts. Furthermore, employing separate models for differentlanguages leads to embeddings in distinct vector spaces, introducing challengeswhen comparing semantic similarities between multi-lingual texts. Therefore, wepropose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embedcrisis-related social media texts for over 50 languages, such that texts withsimilar meanings are in close proximity within the same vector space,irrespective of language diversity. Results in sentence encoding and sentencematching tasks are promising, suggesting these models could serve as robustbaselines when embedding multi-lingual crisis-related social media texts. Themodels are publicly available at: https://huggingface.co/crisistransformers.</description><author>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera</author><pubDate>Mon, 25 Mar 2024 11:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16614v1</guid></item><item><title>Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting</title><link>http://arxiv.org/abs/2403.16612v1</link><description>Seasonal forecasting is a crucial task when it comes to detecting the extremeheat and colds that occur due to climate change. Confidence in the predictionsshould be reliable since a small increase in the temperatures in a year has abig impact on the world. Calibration of the neural networks provides a way toensure our confidence in the predictions. However, calibrating regressionmodels is an under-researched topic, especially in forecasters. We calibrate aUNet++ based architecture, which was shown to outperform physics-based modelsin temperature anomalies. We show that with a slight trade-off betweenprediction error and calibration error, it is possible to get more reliable andsharper forecasts. We believe that calibration should be an important part ofsafety-critical machine learning applications such as weather forecasters.</description><author>Busra Asan, Abdullah Akgul, Alper Unal, Melih Kandemir, Gozde Unal</author><pubDate>Mon, 25 Mar 2024 11:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16612v1</guid></item><item><title>Distributed collaborative anomalous sound detection by embedding sharing</title><link>http://arxiv.org/abs/2403.16610v1</link><description>To develop a machine sound monitoring system, a method for detectinganomalous sound is proposed. In this paper, we explore a method for multipleclients to collaboratively learn an anomalous sound detection model whilekeeping their raw data private from each other. In the context of industrialmachine anomalous sound detection, each client possesses data from differentmachines or different operational states, making it challenging to learnthrough federated learning or split learning. In our proposed method, eachclient calculates embeddings using a common pre-trained model developed forsound data classification, and these calculated embeddings are aggregated onthe server to perform anomalous sound detection through outlier exposure.Experiments showed that our proposed method improves the AUC of anomalous sounddetection by an average of 6.8%.</description><author>Kota Dohi, Yohei Kawaguchi</author><pubDate>Mon, 25 Mar 2024 11:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16610v1</guid></item><item><title>Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units</title><link>http://arxiv.org/abs/2403.16609v1</link><description>Successful conversations often rest on common understanding, where allparties are on the same page about the information being shared. This process,known as conversational grounding, is crucial for building trustworthy dialogsystems that can accurately keep track of and recall the shared information.The proficiencies of an agent in grounding the conveyed informationsignificantly contribute to building a reliable dialog system. Despite recentadvancements in dialog systems, there exists a noticeable deficit in theirgrounding capabilities. Traum provided a framework for conversational groundingintroducing Grounding Acts and Grounding Units, but substantial progress,especially in the realm of Large Language Models, remains lacking. To bridgethis gap, we present the annotation of two dialog corpora employing GroundingActs, Grounding Units, and a measure of their degree of grounding. We discussour key findings during the annotation and also provide a baseline model totest the performance of current Language Models in categorizing the groundingacts of the dialogs. Our work aims to provide a useful resource for furtherresearch in making conversations with machines better understood and morereliable in natural day-to-day collaborative dialogs.</description><author>Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell</author><pubDate>Mon, 25 Mar 2024 11:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16609v1</guid></item><item><title>Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus</title><link>http://arxiv.org/abs/2403.16607v1</link><description>Addressing the challenge of data scarcity in industrial domains, transferlearning emerges as a pivotal paradigm. This work introduces Style Filter, atailored methodology for industrial contexts. By selectively filtering sourcedomain data before knowledge transfer, Style Filter reduces the quantity ofdata while maintaining or even enhancing the performance of transfer learningstrategy. Offering label-free operation, minimal reliance on prior knowledge,independence from specific models, and re-utilization, Style Filter isevaluated on authentic industrial datasets, highlighting its effectiveness whenemployed before conventional transfer strategies in the deep learning domain.The results underscore the effectiveness of Style Filter in real-worldindustrial applications.</description><author>Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li</author><pubDate>Mon, 25 Mar 2024 11:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16607v1</guid></item><item><title>SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation</title><link>http://arxiv.org/abs/2403.16605v1</link><description>In recent years, semantic segmentation has become a pivotal tool inprocessing and interpreting satellite imagery. Yet, a prevalent limitation ofsupervised learning techniques remains the need for extensive manualannotations by experts. In this work, we explore the potential of generativeimage diffusion to address the scarcity of annotated data in earth observationtasks. The main idea is to learn the joint data manifold of images and labels,leveraging recent advancements in denoising diffusion probabilistic models. Tothe best of our knowledge, we are the first to generate both images andcorresponding masks for satellite segmentation. We find that the obtained pairsnot only display high quality in fine-scale features but also ensure a widesampling diversity. Both aspects are crucial for earth observation data, wheresemantic classes can vary severely in scale and occurrence frequency. We employthe novel data instances for downstream segmentation, as a form of dataaugmentation. In our experiments, we provide comparisons to prior works basedon discriminative diffusion models or GANs. We demonstrate that integratinggenerated samples yields significant quantitative improvements for satellitesemantic segmentation -- both compared to baselines and when training only onthe original data.</description><author>Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixé</author><pubDate>Mon, 25 Mar 2024 11:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16605v1</guid></item><item><title>EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.16594v1</link><description>Deploying deep learning (DL) models in medical applications relies onpredictive performance and other critical factors, such as conveyingtrustworthy predictive uncertainty. Uncertainty estimation (UE) methods providepotential solutions for evaluating prediction reliability and improving themodel confidence calibration. Despite increasing interest in UE, challengespersist, such as the need for explicit methods to capture aleatoric uncertaintyand align uncertainty estimates with real-life disagreements among domainexperts. This paper proposes an Expert Disagreement-Guided UncertaintyEstimation (EDUE) for medical image segmentation. By leveraging variability inground-truth annotations from multiple raters, we guide the model duringtraining and incorporate random sampling-based strategies to enhancecalibration confidence. Our method achieves 55% and 23% improvement incorrelation on average with expert disagreements at the image and pixel levels,respectively, better calibration, and competitive segmentation performancecompared to the state-of-the-art deep ensembles, requiring only a singleforward pass.</description><author>Kudaibergen Abutalip, Numan Saeed, Ikboljon Sobirov, Vincent Andrearczyk, Adrien Depeursinge, Mohammad Yaqub</author><pubDate>Mon, 25 Mar 2024 11:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16594v1</guid></item><item><title>Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing</title><link>http://arxiv.org/abs/2403.14828v2</link><description>Fashion illustration is a crucial medium for designers to convey theircreative vision and transform design concepts into tangible representationsthat showcase the interplay between clothing and the human body. In the contextof fashion design, computer vision techniques have the potential to enhance andstreamline the design process. Departing from prior research primarily focusedon virtual try-on, this paper tackles the task of multimodal-conditionedfashion image editing. Our approach aims to generate human-centric fashionimages guided by multimodal prompts, including text, human body poses, garmentsketches, and fabric textures. To address this problem, we propose extendinglatent diffusion models to incorporate these multiple modalities and modifyingthe structure of the denoising network, taking multimodal prompts as input. Tocondition the proposed architecture on fabric textures, we employ textualinversion techniques and let diverse cross-attention layers of the denoisingnetwork attend to textual and texture information, thus incorporating differentgranularity conditioning details. Given the lack of datasets for the task, weextend two existing fashion datasets, Dress Code and VITON-HD, with multimodalannotations. Experimental evaluations demonstrate the effectiveness of ourproposed approach in terms of realism and coherence concerning the providedmultimodal inputs.</description><author>Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara</author><pubDate>Mon, 25 Mar 2024 11:12:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14828v2</guid></item><item><title>TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques</title><link>http://arxiv.org/abs/2403.16592v1</link><description>The Large Language Models (LLMs) exhibit remarkable ability to generatefluent content across a wide spectrum of user queries. However, this capabilityhas raised concerns regarding misinformation and personal information leakage.In this paper, we present our methods for the SemEval2024 Task8, aiming todetect machine-generated text across various domains in both mono-lingual andmulti-lingual contexts. Our study comprehensively analyzes various methods todetect machine-generated text, including statistical, neural, and pre-trainedmodel approaches. We also detail our experimental setup and perform a in-deptherror analysis to evaluate the effectiveness of these methods. Our methodsobtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% forsubtask-B. Furthermore, we also highlight the challenges and essential factorsfor consideration in future studies.</description><author>Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala</author><pubDate>Mon, 25 Mar 2024 11:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16592v1</guid></item><item><title>Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy</title><link>http://arxiv.org/abs/2403.16591v1</link><description>The swift evolution of machine learning has led to emergence of variousdefinitions of privacy due to the threats it poses to privacy, including theconcept of local differential privacy (LDP). Although widely embraced andutilized across numerous domains, this conventional approach to measure privacystill exhibits certain limitations, spanning from failure to preventinferential disclosure to lack of consideration for the adversary's backgroundknowledge. In this comprehensive study, we introduce Bayesian privacy and delveinto the intricate relationship between local differential privacy and itsBayesian counterparts, unveiling novel insights into utility-privacytrade-offs. We introduce a framework that encapsulates both attack and defensestrategies, highlighting their interplay and effectiveness. Our theoreticalcontributions are anchored in the rigorous definitions and relationshipsbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),encapsulated by equations $\epsilon_{p,a} \leq\frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} +\epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDPestablished under uniform prior distribution. These relationships fortify ourunderstanding of the privacy guarantees provided by various mechanisms, leadingto the realization that a mechanism satisfying $\xi$-LDP also confers$\xi$-MBP, and vice versa. Our work not only lays the groundwork for futureempirical exploration but also promises to enhance the design ofprivacy-preserving algorithms that do not compromise on utility, therebyfostering the development of trustworthy machine learning solutions.</description><author>Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin</author><pubDate>Mon, 25 Mar 2024 11:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16591v1</guid></item><item><title>Can Large Language Models (or Humans) Distill Text?</title><link>http://arxiv.org/abs/2403.16584v1</link><description>We investigate the potential of large language models (LLMs) to distill text:to remove the textual traces of an undesired forbidden variable. We employ arange of LLMs with varying architectures and training approaches to distilltext by identifying and removing information about the target variable whilepreserving other relevant signals. Our findings shed light on the strengths andlimitations of LLMs in addressing the distillation and provide insights intothe strategies for leveraging these models in computational social scienceinvestigations involving text data. In particular, we show that in the strongtest of removing sentiment, the statistical association between the processedtext and sentiment is still clearly detectable to machine learning classifierspost-LLM-distillation. Furthermore, we find that human annotators also struggleto distill sentiment while preserving other semantic content. This suggeststhere may be limited separability between concept variables in some textcontexts, highlighting limitations of methods relying on text-leveltransformations and also raising questions about the robustness of distillationmethods that achieve statistical independence in representation space if thisis difficult for human coders operating on raw text to attain.</description><author>Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson</author><pubDate>Mon, 25 Mar 2024 10:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16584v1</guid></item><item><title>In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data</title><link>http://arxiv.org/abs/2403.16582v1</link><description>Crop classification is of critical importance due to its role in studyingcrop pattern changes, resource management, and carbon sequestration. Whenemploying data-driven techniques for its prediction, utilizing various temporaldata sources is necessary. Deep learning models have proven to be effective forthis task by mapping time series data to high-level representation forprediction. However, they face substantial challenges when dealing withmultiple input patterns. The literature offers limited guidance for Multi-ViewLearning (MVL) scenarios, as it has primarily focused on exploring fusionstrategies with specific encoders and validating them in local regions. Incontrast, we investigate the impact of simultaneous selection of the fusionstrategy and the encoder architecture evaluated on a global-scale cropland andcrop-type classifications. We use a range of five fusion strategies (Input,Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. Thevalidation is on the CropHarvest dataset that provides optical, radar, andweather time series, and topographic information as input data. We found thatin scenarios with a limited number of labeled samples, a unique configurationis insufficient for all the cases. Instead, a specialized combination,including encoder and fusion strategy, should be meticulously sought. Tostreamline this search process, we suggest initially identifying the optimalencoder architecture tailored for a particular fusion strategy, and thendetermining the most suitable fusion strategy for the classification task. Weprovide a technical framework for researchers exploring crop classification orrelated tasks through a MVL approach.</description><author>Francisco Mena, Diego Arenas, Andreas Dengel</author><pubDate>Mon, 25 Mar 2024 10:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16582v1</guid></item><item><title>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2403.16578v1</link><description>Medical image segmentation models adapting to new tasks in a training-freemanner through in-context learning is an exciting advancement. Universalsegmentation models aim to generalize across the diverse modality of medicalimages, yet their effectiveness often diminishes when applied toout-of-distribution (OOD) data modalities and tasks, requiring intricatefine-tuning of model for optimal performance. For addressing this challenge, weintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) forimage segmentation. Unlike existing methods, SegICL has the capability toemploy text-guided segmentation and conduct in-context learning with a smallset of image-mask pairs, eliminating the need for training the model fromscratch or fine-tuning for OOD tasks (including OOD modality and dataset).Extensive experimental validation of SegICL demonstrates a positive correlationbetween the number of prompt samples and segmentation performance on OODmodalities and tasks. This indicates that SegICL effectively address newsegmentation tasks based on contextual information. Additionally, SegICL alsoexhibits comparable segmentation performance to mainstream models on OOD andin-distribution tasks. Our code will be released soon.</description><author>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang</author><pubDate>Mon, 25 Mar 2024 10:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16578v1</guid></item><item><title>LightIt: Illumination Modeling and Control for Diffusion Models</title><link>http://arxiv.org/abs/2403.10615v2</link><description>We introduce LightIt, a method for explicit illumination control for imagegeneration. Recent generative methods lack lighting control, which is crucialto numerous artistic aspects of image generation such as setting the overallmood or cinematic appearance. To overcome these limitations, we propose tocondition the generation on shading and normal maps. We model the lighting withsingle bounce shading, which includes cast shadows. We first train a shadingestimation module to generate a dataset of real-world images and shading pairs.Then, we train a control network using the estimated shading and normals asinput. Our method demonstrates high-quality image generation and lightingcontrol in numerous scenes. Additionally, we use our generated dataset to trainan identity-preserving relighting model, conditioned on an image and a targetshading. Our method is the first that enables the generation of images withcontrollable, consistent lighting and performs on par with specializedrelighting state-of-the-art methods.</description><author>Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy</author><pubDate>Mon, 25 Mar 2024 10:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10615v2</guid></item></channel></rss>