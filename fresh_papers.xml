<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 06 Jun 2023 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Neuralangelo: High-Fidelity Neural Surface Reconstruction</title><link>http://arxiv.org/abs/2306.03092v1</link><description>Neural surface reconstruction has been shown to be powerful for recoveringdense 3D surfaces via image-based neural rendering. However, current methodsstruggle to recover detailed structures of real-world scenes. To address theissue, we present Neuralangelo, which combines the representation power ofmulti-resolution 3D hash grids with neural surface rendering. Two keyingredients enable our approach: (1) numerical gradients for computinghigher-order derivatives as a smoothing operation and (2) coarse-to-fineoptimization on the hash grids controlling different levels of details. Evenwithout auxiliary inputs such as depth, Neuralangelo can effectively recoverdense 3D surface structures from multi-view images with fidelity significantlysurpassing previous methods, enabling detailed large-scale scene reconstructionfrom RGB video captures.</description><author>Zhaoshuo Li, Thomas MÃ¼ller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin</author><pubDate>Mon, 05 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03092v1</guid></item><item><title>RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems</title><link>http://arxiv.org/abs/2306.03091v1</link><description>Large Language Models (LLMs) have greatly advanced code auto-completionsystems, with a potential for substantial productivity enhancements fordevelopers. However, current benchmarks mainly focus on single-file tasks,leaving an assessment gap for more complex, real-world, multi-file programmingscenarios. To fill this gap, we introduce RepoBench, a new benchmarkspecifically designed for evaluating repository-level code auto-completionsystems. RepoBench consists of three interconnected evaluation tasks:RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P(Pipeline). Each task respectively measures the system's ability to retrievethe most relevant code snippets from other files as cross-file context, predictthe next line of code with cross-file and in-file context, and handle complextasks that require a combination of both retrieval and next-line prediction.RepoBench aims to facilitate a more complete comparison of performance andencouraging continuous improvement in auto-completion systems. RepoBench ispublicly available at https://github.com/Leolty/repobench.</description><author>Tianyang Liu, Canwen Xu, Julian McAuley</author><pubDate>Mon, 05 Jun 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03091v1</guid></item><item><title>Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction</title><link>http://arxiv.org/abs/2306.03090v1</link><description>Coaching, which involves classroom observation and expert feedback, is awidespread and fundamental part of teacher training. However, the majority ofteachers do not have access to consistent, high quality coaching due to limitedresources and access to expertise. We explore whether generative AI couldbecome a cost-effective complement to expert feedback by serving as anautomated teacher coach. In doing so, we propose three teacher coaching tasksfor generative AI: (A) scoring transcript segments based on classroomobservation instruments, (B) identifying highlights and missed opportunitiesfor good instructional strategies, and (C) providing actionable suggestions foreliciting more student reasoning. We recruit expert math teachers to evaluatethe zero-shot performance of ChatGPT on each of these tasks for elementary mathclassroom transcripts. Our results reveal that ChatGPT generates responses thatare relevant to improving instruction, but they are often not novel orinsightful. For example, 82% of the model's suggestions point to places in thetranscript where the teacher is already implementing that suggestion. Our workhighlights the challenges of producing insightful, novel and truthful feedbackfor teachers while paving the way for future research to address theseobstacles and improve the capacity of generative AI to coach teachers.</description><author>Rose E. Wang, Dorottya Demszky</author><pubDate>Mon, 05 Jun 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03090v1</guid></item><item><title>Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models</title><link>http://arxiv.org/abs/2306.03089v1</link><description>A long standing goal in neuroscience has been to elucidate the functionalorganization of the brain. Within higher visual cortex, functional accountshave remained relatively coarse, focusing on regions of interest (ROIs) andtaking the form of selectivity for broad categories such as faces, places,bodies, food, or words. Because the identification of such ROIs has typicallyrelied on manually assembled stimulus sets consisting of isolated objects innon-ecological contexts, exploring functional organization without robust apriori hypotheses has been challenging. To overcome these limitations, weintroduce a data-driven approach in which we synthesize images predicted toactivate a given brain region using paired natural images and fMRI recordings,bypassing the need for category-specific stimuli. Our approach -- BrainDiffusion for Visual Exploration ("BrainDiVE") -- builds on recent generativemethods by combining large-scale diffusion models with brain-guided imagesynthesis. Validating our method, we demonstrate the ability to synthesizepreferred images with appropriate semantic specificity for well-characterizedcategory-selective ROIs. We then show that BrainDiVE can characterizedifferences between ROIs selective for the same high-level category. Finally weidentify novel functional subdivisions within these ROIs, validated withbehavioral data. These results advance our understanding of the fine-grainedfunctional organization of human visual cortex, and provide well-specifiedconstraints for further examination of cortical organization usinghypothesis-driven methods.</description><author>Andrew F. Luo, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr</author><pubDate>Mon, 05 Jun 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03089v1</guid></item><item><title>DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics</title><link>http://arxiv.org/abs/2306.03088v1</link><description>Functional brain dynamics is supported by parallel and overlapping functionalnetwork modes that are associated with specific neural circuits. Decomposingthese network modes from fMRI data and finding their temporal characteristicsis challenging due to their time-varying nature and the non-linearity of thefunctional dynamics. Dynamic Mode Decomposition (DMD) algorithms have beenquite popular for solving this decomposition problem in recent years. In thiswork, we apply GraphDMD -- an extension of the DMD for network data -- toextract the dynamic network modes and their temporal characteristics from thefMRI time series in an interpretable manner. GraphDMD, however, regards theunderlying system as a linear dynamical system that is sub-optimal forextracting the network modes from non-linear functional data. In this work, wedevelop a generalized version of the GraphDMD algorithm -- DeepGraphDMD --applicable to arbitrary non-linear graph dynamical systems. DeepGraphDMD is anautoencoder-based deep learning model that learns Koopman eigenfunctions forgraph data and embeds the non-linear graph dynamics into a latent linear space.We show the effectiveness of our method in both simulated data and the HCPresting-state fMRI data. In the HCP data, DeepGraphDMD provides novel insightsinto cognitive brain functions by discovering two major network modes relatedto fluid and crystallized intelligence.</description><author>Md Asadullah Turja, Martin Styner, Guorong Wu</author><pubDate>Mon, 05 Jun 2023 18:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03088v1</guid></item><item><title>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</title><link>http://arxiv.org/abs/2306.00937v2</link><description>Constructing AI models that respond to text instructions is challenging,especially for sequential decision-making tasks. This work introduces aninstruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1,demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effectivefor creating instruction-following sequential decision-making agents. STEVE-1is trained in two steps: adapting the pretrained VPT model to follow commandsin MineCLIP's latent space, then training a prior to predict latent codes fromtext. This allows us to finetune VPT through self-supervised behavioral cloningand hindsight relabeling, bypassing the need for costly human text annotations.By leveraging pretrained models like VPT and MineCLIP and employing bestpractices from text-conditioned image generation, STEVE-1 costs just $60 totrain and can follow a wide range of short-horizon open-ended text and visualinstructions in Minecraft. STEVE-1 sets a new bar for open-ended instructionfollowing in Minecraft with low-level controls (mouse and keyboard) and rawpixel inputs, far outperforming previous baselines. We provide experimentalevidence highlighting key factors for downstream performance, includingpretraining, classifier-free guidance, and data scaling. All resources,including our model weights, training scripts, and evaluation tools are madeavailable for further research.</description><author>Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila McIlraith</author><pubDate>Mon, 05 Jun 2023 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00937v2</guid></item><item><title>FilFL: Client Filtering for Optimized Client Participation in Federated Learning</title><link>http://arxiv.org/abs/2302.06599v2</link><description>Federated learning is an emerging machine learning paradigm that enablesclients to train collaboratively without exchanging local data. The clientsparticipating in the training process have a crucial impact on the convergencerate, learning efficiency, and model generalization. In this work, we proposeFilFL, a new approach to optimizing client participation and training byintroducing client filtering. FilFL periodically filters the available clientsto identify a subset that maximizes a combinatorial objective function using anefficient greedy filtering algorithm. From this filtered-in subset, clients arethen selected for the training process. We provide a thorough analysis of FilFLconvergence in a heterogeneous setting and evaluate its performance acrossdiverse vision and language tasks and realistic federated scenarios withtime-varying client availability. Our empirical results demonstrate severalbenefits of our approach, including improved learning efficiency, fasterconvergence, and up to 10 percentage points higher test accuracy compared toscenarios where client filtering is not utilized.</description><author>Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini, Marco Canini</author><pubDate>Mon, 05 Jun 2023 18:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06599v2</guid></item><item><title>MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion</title><link>http://arxiv.org/abs/2306.03083v1</link><description>We present MotionDiffuser, a diffusion based representation for the jointdistribution of future trajectories over multiple agents. Such representationhas several key advantages: first, our model learns a highly multimodaldistribution that captures diverse future outcomes. Second, the simplepredictor design requires only a single L2 loss training objective, and doesnot depend on trajectory anchors. Third, our model is capable of learning thejoint distribution for the motion of multiple agents in a permutation-invariantmanner. Furthermore, we utilize a compressed trajectory representation via PCA,which improves model performance and allows for efficient computation of theexact sample log probability. Subsequently, we propose a general constrainedsampling framework that enables controlled trajectory sampling based ondifferentiable cost functions. This strategy enables a host of applicationssuch as enforcing rules and physical priors, or creating tailored simulationscenarios. MotionDiffuser can be combined with existing backbone architecturesto achieve top motion forecasting results. We obtain state-of-the-art resultsfor multi-agent motion prediction on the Waymo Open Motion Dataset.</description><author>Chiyu Max Jiang, Andre Cornman, Cheolho Park, Ben Sapp, Yin Zhou, Dragomir Anguelov</author><pubDate>Mon, 05 Jun 2023 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03083v1</guid></item><item><title>InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</title><link>http://arxiv.org/abs/2306.03082v1</link><description>Large language models~(LLMs) are instruction followers, but it can bechallenging to find the best instruction for different situations, especiallyfor black-box LLMs on which backpropagation is forbidden. Instead of directlyoptimizing the discrete instruction, we optimize a low-dimensional soft promptapplied to an open-source LLM to generate the instruction for the black-boxLLM. On each iteration of the proposed method, which we call InstructZero, asoft prompt is converted into an instruction using the open-source LLM, whichis then submitted to the black-box LLM for zero-shot evaluation, and theperformance is sent to Bayesian optimization to produce new soft promptsimproving the zero-shot performance. We evaluate InstructZero on differentcombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Ourresults show that InstructZero outperforms SOTA auto-instruction methods acrossa variety of downstream tasks. Our code and data are publicly available athttps://github.com/Lichang-Chen/InstructZero.</description><author>Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, Tianyi Zhou</author><pubDate>Mon, 05 Jun 2023 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03082v1</guid></item><item><title>PaLI: A Jointly-Scaled Multilingual Language-Image Model</title><link>http://arxiv.org/abs/2209.06794v4</link><description>Effective scaling and a flexible task interface enable large language modelsto excel at many tasks. We present PaLI (Pathways Language and Image model), amodel that extends this approach to the joint modeling of language and vision.PaLI generates text based on visual and textual inputs, and with this interfaceperforms many vision, language, and multimodal tasks, in many languages. Totrain PaLI, we make use of large pre-trained encoder-decoder language modelsand Vision Transformers (ViTs). This allows us to capitalize on their existingcapabilities and leverage the substantial cost of training them. We find thatjoint scaling of the vision and language components is important. Sinceexisting Transformers for language are much larger than their visioncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify thebenefits from even larger-capacity vision models. To train PaLI, we create alarge multilingual mix of pretraining tasks, based on a new image-text trainingset containing 10B images and texts in over 100 languages. PaLI achievesstate-of-the-art in multiple vision and language tasks (such as captioning,visual question-answering, scene-text understanding), while retaining a simple,modular, and scalable design.</description><author>Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut</author><pubDate>Mon, 05 Jun 2023 18:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06794v4</guid></item><item><title>Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs</title><link>http://arxiv.org/abs/2306.03081v1</link><description>Even after fine-tuning and reinforcement learning, large language models(LLMs) can be difficult, if not impossible, to control reliably with promptsalone. We propose a new inference-time approach to enforcing syntactic andsemantic constraints on the outputs of LLMs, called sequential Monte Carlo(SMC) steering. The key idea is to specify language generation tasks asposterior inference problems in a class of discrete probabilistic sequencemodels, and replace standard decoding with sequential Monte Carlo inference.For a computational cost similar to that of beam search, SMC can steer LLMs tosolve diverse tasks, including infilling, generation under syntacticconstraints, and prompt intersection. To facilitate experimentation with SMCsteering, we present a probabilistic programming library, LLaMPPL(https://github.com/probcomp/LLaMPPL), for concisely specifying new generationtasks as language model probabilistic programs, and automating steering ofLLaMA-family Transformers.</description><author>Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka</author><pubDate>Mon, 05 Jun 2023 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03081v1</guid></item><item><title>Machine Learning and Statistical Approaches to Measuring Similarity of Political Parties</title><link>http://arxiv.org/abs/2306.03079v1</link><description>Mapping political party systems to metric policy spaces is one of the majormethodological problems in political science. At present, in most politicalscience project this task is performed by domain experts relying on purelyqualitative assessments, with all the attendant problems of subjectivity andlabor intensiveness. We consider how advances in natural language processing,including large transformer-based language models, can be applied to solve thatissue. We apply a number of texts similarity measures to party politicalprograms, analyze how they correlate with each other, and -- in the absence ofa satisfactory benchmark -- evaluate them against other measures, includingthose based on expert surveys, voting records, electoral patterns, andcandidate networks. Finally, we consider the prospects of relying on thosemethods to correct, supplement, and eventually replace expert judgments.</description><author>Daria Boratyn, Damian Brzyski, Beata Kosowska-GÄstoÅ, Jan Rybicki, Wojciech SÅomczyÅski, Dariusz Stolicki</author><pubDate>Mon, 05 Jun 2023 18:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03079v1</guid></item><item><title>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</title><link>http://arxiv.org/abs/2306.03078v1</link><description>Recent advances in large language model (LLM) pretraining have led tohigh-quality LLMs with impressive abilities. By compressing such LLMs viaquantization to 3-4 bits per parameter, they can fit into memory-limiteddevices such as laptops and mobile phones, enabling personalized use. However,quantization down to 3-4 bits per parameter usually leads to moderate-to-highaccuracy losses, especially for smaller models in the 1-10B parameter range,which are well-suited for edge deployments. To address this accuracy issue, weintroduce the Sparse-Quantized Representation (SpQR), a new compressed formatand quantization technique which enables for the first time near-losslesscompression of LLMs across model scales, while reaching similar compressionlevels to previous methods. SpQR works by identifying and isolating outlierweights, which cause particularly-large quantization errors, and storing themin higher precision, while compressing all other weights to 3-4 bits, andachieves relative accuracy losses of less than 1% in perplexity forhighly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33Bparameter LLM on a single 24 GB consumer GPU without any performancedegradation at 15% speedup thus making powerful LLMs available to consumerwithout any downsides. SpQR comes with efficient algorithms for both encodingweights into its format, as well as decoding them efficiently at runtime.Specifically, we provide an efficient GPU inference algorithm for SpQR whichyields faster inference than 16-bit baselines at similar accuracy, whileenabling memory compression gains of more than 4x.</description><author>Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh</author><pubDate>Mon, 05 Jun 2023 18:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03078v1</guid></item><item><title>Sensitivity-Aware Finetuning for Accuracy Recovery on Deep Learning Hardware</title><link>http://arxiv.org/abs/2306.03076v1</link><description>Existing methods to recover model accuracy on analog-digital hardware in thepresence of quantization and analog noise include noise-injection training.However, it can be slow in practice, incurring high computational costs, evenwhen starting from pretrained models. We introduce the Sensitivity-AwareFinetuning (SAFT) approach that identifies noise sensitive layers in a model,and uses the information to freeze specific layers for noise-injectiontraining. Our results show that SAFT achieves comparable accuracy tonoise-injection training and is 2x to 8x faster.</description><author>Lakshmi Nair, Darius Bunandar</author><pubDate>Mon, 05 Jun 2023 18:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03076v1</guid></item><item><title>A General Perspective on Objectives of Reinforcement Learning</title><link>http://arxiv.org/abs/2306.03074v1</link><description>In this lecture, we present a general perspective on reinforcement learning(RL) objectives, where we show three versions of objectives. The first versionis the standard definition of objective in RL literature. Then we extend thestandard definition to the $\lambda$-return version, which unifies the standarddefinition of objective. Finally, we propose a general objective that unifiesthe previous two versions. The last version provides a high level to understandof RL's objective, where it shows a fundamental formulation that connects somewidely used RL techniques (e.g., TD$(\lambda)$ and GAE), and this objective canbe potentially applied to extensive RL algorithms.</description><author>Long Yang</author><pubDate>Mon, 05 Jun 2023 18:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03074v1</guid></item><item><title>In-context Example Selection with Influences</title><link>http://arxiv.org/abs/2302.11042v2</link><description>In-context learning (ICL) is a powerful paradigm emerged from large languagemodels (LLMs). Despite its promises, ICL performance is known to be highlysensitive to input examples. In this work, we use $\textit{in-contextinfluences}$ to analyze few-shot ICL performance directly from the in-contextexamples. Our proposed influence-based example selection method can identifyboth positive and negative examples, outperforming several baselines whenevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$performance gap between using the most negative in-context examples compared tothe most positive. In a case study, we apply our influence-based framework toquantify the phenomena of recency bias in example ordering for few-shot ICL.</description><author>Tai Nguyen, Eric Wong</author><pubDate>Mon, 05 Jun 2023 18:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11042v2</guid></item><item><title>Explore to Generalize in Zero-Shot RL</title><link>http://arxiv.org/abs/2306.03072v1</link><description>We study zero-shot generalization in reinforcement learning - optimizing apolicy on a set of training tasks such that it will perform well on a similarbut unseen test task. To mitigate overfitting, previous work explored differentnotions of invariance to the task. However, on problems such as the ProcGenMaze, an adequate solution that is invariant to the task visualization does notexist, and therefore invariance-based approaches fail. Our insight is thatlearning a policy that $\textit{explores}$ the domain effectively is harder tomemorize than a policy that maximizes reward for a specific task, and thereforewe expect such learned behavior to generalize well; we indeed demonstrate thisempirically on several domains that are difficult for invariance-basedapproaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds onthis insight: We train an additional ensemble of agents that optimize reward.At test time, either the ensemble agrees on an action, and we generalize well,or we take exploratory actions, which are guaranteed to generalize and drive usto a novel part of the state space, where the ensemble may potentially agreeagain. We show that our approach is the state-of-the-art on several tasks inthe ProcGen challenge that have so far eluded effective generalization. Forexample, we demonstrate a success rate of $82\%$ on the Maze task and $74\%$ onHeist with $200$ training levels.</description><author>Ev Zisselman, Itai Lavie, Daniel Soudry, Aviv Tamar</author><pubDate>Mon, 05 Jun 2023 18:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03072v1</guid></item><item><title>CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks</title><link>http://arxiv.org/abs/2204.10965v5</link><description>In this paper, we propose CLIP-Dissect, a new technique to automaticallydescribe the function of individual hidden neurons inside vision networks.CLIP-Dissect leverages recent advances in multimodal vision/language models tolabel internal neurons with open-ended concepts without the need for anylabeled data or human examples. We show that CLIP-Dissect provides moreaccurate descriptions than existing methods for last layer neurons where theground-truth is available as well as qualitatively good descriptions for hiddenlayer neurons. In addition, our method is very flexible: it is model agnostic,can easily handle new concepts and can be extended to take advantage of bettermultimodal models in the future. Finally CLIP-Dissect is computationallyefficient and can label all neurons from five layers of ResNet-50 in just 4minutes, which is more than 10 times faster than existing methods. Our code isavailable at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally,crowdsourced user study results are available at Appendix B to further supportthe effectiveness of our method.</description><author>Tuomas Oikarinen, Tsui-Wei Weng</author><pubDate>Mon, 05 Jun 2023 18:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.10965v5</guid></item><item><title>Interactive Editing for Text Summarization</title><link>http://arxiv.org/abs/2306.03067v1</link><description>Summarizing lengthy documents is a common and essential task in our dailylives. Although recent advancements in neural summarization models can assistin crafting general-purpose summaries, human writers often have specificrequirements that call for a more customized approach. To address this need, weintroduce REVISE (Refinement and Editing via Iterative SummarizationEnhancement), an innovative framework designed to facilitate iterative editingand refinement of draft summaries by human writers. Within our framework,writers can effortlessly modify unsatisfactory segments at any location orlength and provide optional starting phrases -- our system will generatecoherent alternatives that seamlessly integrate with the existing summary. Atits core, REVISE incorporates a modified fill-in-the-middle model with theencoder-decoder architecture while developing novel evaluation metrics tailoredfor the summarization task. In essence, our framework empowers users to createhigh-quality, personalized summaries by effectively harnessing both humanexpertise and AI capabilities, ultimately transforming the summarizationprocess into a truly collaborative and adaptive experience.</description><author>Yujia Xie, Xun Wang, Si-Qing Chen, Wayne Xiong, Pengcheng He</author><pubDate>Mon, 05 Jun 2023 18:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03067v1</guid></item><item><title>Of Mice and Mates: Automated Classification and Modelling of Mouse Behaviour in Groups using a Single Model across Cages</title><link>http://arxiv.org/abs/2306.03066v1</link><description>Behavioural experiments often happen in specialised arenas, but this mayconfound the analysis. To address this issue, we provide tools to study mice inthe homecage environment, equipping biologists with the possibility to capturethe temporal aspect of the individual's behaviour and model the interaction andinterdependence between cage-mates with minimal human intervention. We developthe Activity Labelling Module (ALM) to automatically classify mouse behaviourfrom video, and a novel Group Behaviour Model (GBM) for summarising their jointbehaviour across cages, using a permutation matrix to match the mouseidentities in each cage to the model. We also release two datasets, ABODe fortraining behaviour classifiers and IMADGE for modelling behaviour.</description><author>Michael P. J. Camilleri, Rasneer S. Bains, Christopher K. I. Williams</author><pubDate>Mon, 05 Jun 2023 18:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03066v1</guid></item><item><title>LibAUC: A Deep Learning Library for X-Risk Optimization</title><link>http://arxiv.org/abs/2306.03065v1</link><description>This paper introduces the award-winning deep learning (DL) library calledLibAUC for implementing state-of-the-art algorithms towards optimizing a familyof risk functions named X-risks. X-risks refer to a family of compositionalfunctions in which the loss function of each data point is defined in a waythat contrasts the data point with a large number of others. They have broadapplications in AI for solving classical and emerging problems, including butnot limited to classification for imbalanced data (CID), learning to rank(LTR), and contrastive learning of representations (CLR). The motivation ofdeveloping LibAUC is to address the convergence issues of existing librariesfor solving these problems. In particular, existing libraries may not convergeor require very large mini-batch sizes in order to attain good performance forthese problems, due to the usage of the standard mini-batch technique in theempirical risk minimization (ERM) framework. Our library is for deep X-riskoptimization (DXO) that has achieved great success in solving a variety oftasks for CID, LTR and CLR. The contributions of this paper include: (1) Itintroduces a new mini-batch based pipeline for implementing DXO algorithms,which differs from existing DL pipeline in the design of controlled datasamplers and dynamic mini-batch losses; (2) It provides extensive benchmarkingexperiments for ablation studies and comparison with existing libraries. TheLibAUC library features scalable performance for millions of items to becontrasted, faster and better convergence than existing libraries foroptimizing X-risks, seamless PyTorch deployment and versatile APIs for variousloss optimization. Our library is available to the open source community athttps://github.com/Optimization-AI/LibAUC, to facilitate further academicresearch and industrial applications.</description><author>Zhuoning Yuan, Dixian Zhu, Zi-Hao Qiu, Gang Li, Xuanhui Wang, Tianbao Yang</author><pubDate>Mon, 05 Jun 2023 18:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03065v1</guid></item><item><title>Robust 3D-aware Object Classification via Discriminative Render-and-Compare</title><link>http://arxiv.org/abs/2305.14668v2</link><description>In real-world applications, it is essential to jointly estimate the 3D objectpose and class label of objects, i.e., to perform 3D-aware classification.Whilecurrent approaches for either image classification or pose estimation can beextended to 3D-aware classification, we observe that they are inherentlylimited: 1) Their performance is much lower compared to the respectivesingle-task models, and 2) they are not robust in out-of-distribution (OOD)scenarios. Our main contribution is a novel architecture for 3D-awareclassification, which builds upon a recent work and performs comparably tosingle-task models while being highly robust. In our method, an object categoryis represented as a 3D cuboid mesh composed of feature vectors at each meshvertex. Using differentiable rendering, we estimate the 3D object pose byminimizing the reconstruction error between the mesh and the featurerepresentation of the target image. Object classification is then performed bycomparing the reconstruction losses across object categories. Notably, theneural texture of the mesh is trained in a discriminative manner to enhance theclassification performance while also avoiding local optima in thereconstruction loss. Furthermore, we show how our method and feed-forwardneural networks can be combined to scale the render-and-compare approach tolarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,and OOD-CV show that our method outperforms all baselines at 3D-awareclassification by a wide margin in terms of performance and robustness.</description><author>Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam Kortylewski</author><pubDate>Mon, 05 Jun 2023 18:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14668v2</guid></item><item><title>Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2210.06274v2</link><description>We introduce hybrid execution in multi-agent reinforcement learning (MARL), anew paradigm in which agents aim to successfully complete cooperative taskswith arbitrary communication levels at execution time by taking advantage ofinformation-sharing among the agents. Under hybrid execution, the communicationlevel can range from a setting in which no communication is allowed betweenagents (fully decentralized), to a setting featuring full communication (fullycentralized), but the agents do not know beforehand which communication levelthey will encounter at execution time. To formalize our setting, we define anew class of multi-agent partially observable Markov decision processes(POMDPs) that we name hybrid-POMDPs, which explicitly model a communicationprocess between the agents. We contribute MARO, an approach that makes use ofan auto-regressive predictive model, trained in a centralized manner, toestimate missing agents' observations at execution time. We evaluate MARO onstandard scenarios and extensions of previous benchmarks tailored to emphasizethe negative impact of partial observability in MARL. Experimental results showthat our method consistently outperforms relevant baselines, allowing agents toact with faulty communication while successfully exploiting shared information.</description><author>Pedro P. Santos, Diogo S. Carvalho, Miguel Vasco, Alberto Sardinha, Pedro A. Santos, Ana Paiva, Francisco S. Melo</author><pubDate>Mon, 05 Jun 2023 18:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06274v2</guid></item><item><title>DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection</title><link>http://arxiv.org/abs/2305.03716v2</link><description>Fine-grained 3D object detection is a core ability for agents to understandtheir 3D environment and interact with surrounding objects. However, currentmethods and benchmarks mainly focus on relatively large stuff. 3D objectdetectors still struggle on small objects due to weak geometric information.With in-depth study, we find increasing the spatial resolution of the featuremaps significantly boosts the performance of 3D small object detection. Andmore interestingly, though the computational overhead increases dramaticallywith resolution, the growth mainly comes from the upsampling operation of thedecoder. Inspired by this, we present a high-resolution multi-level detectorwith dynamic spatial pruning named DSPDet3D, which detects objects from largeto small by iterative upsampling and meanwhile prunes the spatialrepresentation of the scene at regions where there is no smaller object to bedetected in higher resolution. We organize two benchmarks on ScanNet andTO-SCENE dataset to evaluate the ability of fine-grained 3D object detection,where our DSPDet3D improves the detection performance of small objects to a newlevel while achieving leading inference speed compared with existing 3D objectdetection methods. Moreover, DSPDet3D trained with only ScanNet rooms cangeneralize well to scenes in larger scale. It takes less than 2s for DSPDet3Dto directly process a whole house or building consisting of dozens of roomswhile detecting out almost all objects, ranging from bottles to beds, on asingle RTX 3090 GPU. Project page: https://xuxw98.github.io/DSPDet3D/.</description><author>Xiuwei Xu, Zhihao Sun, Ziwei Wang, Hongmin Liu, Jie Zhou, Jiwen Lu</author><pubDate>Mon, 05 Jun 2023 18:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03716v2</guid></item><item><title>Label-Free Concept Bottleneck Models</title><link>http://arxiv.org/abs/2304.06129v2</link><description>Concept bottleneck models (CBM) are a popular way of creating moreinterpretable neural networks by having hidden layer neurons correspond tohuman-understandable concepts. However, existing CBMs and their variants havetwo crucial limitations: first, they need to collect labeled data for each ofthe predefined concepts, which is time consuming and labor intensive; second,the accuracy of a CBM is often significantly lower than that of a standardneural network, especially on more complex datasets. This poor performancecreates a barrier for adopting CBMs in practical real world applications.Motivated by these challenges, we propose Label-free CBM which is a novelframework to transform any neural network into an interpretable CBM withoutlabeled concept data, while retaining a high accuracy. Our Label-free CBM hasmany advantages, it is: scalable - we present the first CBM scaled to ImageNet,efficient - creating a CBM takes only a few hours even for very large datasets,and automated - training it for a new dataset requires minimal human effort.Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM.Finally, in Appendix B we conduct a large scale user evaluation of theinterpretability of our method.</description><author>Tuomas Oikarinen, Subhro Das, Lam M. Nguyen, Tsui-Wei Weng</author><pubDate>Mon, 05 Jun 2023 18:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06129v2</guid></item><item><title>Structured Voronoi Sampling</title><link>http://arxiv.org/abs/2306.03061v1</link><description>Recently, there has been a growing interest in the development ofgradient-based sampling algorithms for text generation, especially in thecontext of controlled generation. However, there exists a lack of theoreticallygrounded and principled approaches for this task. In this paper, we take animportant step toward building a principled approach for sampling from languagemodels with gradient-based methods. We use discrete distributions given bylanguage models to define densities and develop an algorithm based onHamiltonian Monte Carlo to sample from them. We name our gradient-basedtechnique Structured Voronoi Sampling (SVS). In an experimental setup where thereference distribution is known, we show that the empirical distribution of SVSsamples is closer to the reference distribution compared to alternativesampling schemes. Furthermore, in a controlled generation task, SVS is able togenerate fluent and diverse samples while following the control targetssignificantly better than other methods.</description><author>Afra Amini, Li Du, Ryan Cotterell</author><pubDate>Mon, 05 Jun 2023 18:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03061v1</guid></item><item><title>Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion</title><link>http://arxiv.org/abs/2306.03055v1</link><description>Using Japanese honorifics is challenging because it requires not onlyknowledge of the grammatical rules but also contextual information, such associal relationships. It remains unclear whether pre-trained large languagemodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyzethis, we introduce an honorific conversion task that considers socialrelationships among people mentioned in a conversation. We construct a Japanesehonorifics dataset from problem templates of various sentence structures toinvestigate the syntactic generalization capacity of GPT-3, one of the leadingLLMs, on this task under two settings: fine-tuning and prompt learning. Ourresults showed that the fine-tuned GPT-3 performed better in a context-awarehonorific conversion task than the prompt-based one. The fine-tuned modeldemonstrated overall syntactic generalizability towards compound honorificsentences, except when tested with the data involving direct speech.</description><author>Ryo Sekizawa, Hitomi Yanaka</author><pubDate>Mon, 05 Jun 2023 18:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03055v1</guid></item><item><title>Discriminative Adversarial Privacy: Balancing Accuracy and Membership Privacy in Neural Networks</title><link>http://arxiv.org/abs/2306.03054v1</link><description>The remarkable proliferation of deep learning across various industries hasunderscored the importance of data privacy and security in AI pipelines. As theevolution of sophisticated Membership Inference Attacks (MIAs) threatens thesecrecy of individual-specific information used for training deep learningmodels, Differential Privacy (DP) raises as one of the most utilized techniquesto protect models against malicious attacks. However, despite its proventheoretical properties, DP can significantly hamper model performance andincrease training time, turning its use impractical in real-world scenarios.Tackling this issue, we present Discriminative Adversarial Privacy (DAP), anovel learning technique designed to address the limitations of DP by achievinga balance between model performance, speed, and privacy. DAP relies onadversarial training based on a novel loss function able to minimise theprediction error while maximising the MIA's error. In addition, we introduce anovel metric named Accuracy Over Privacy (AOP) to capture theperformance-privacy trade-off. Finally, to validate our claims, we compare DAPwith diverse DP scenarios, providing an analysis of the results fromperformance, time, and privacy preservation perspectives.</description><author>Eugenio Lomurno, Alberto Archetti, Francesca Ausonio, Matteo Matteucci</author><pubDate>Mon, 05 Jun 2023 18:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03054v1</guid></item><item><title>Forecasting Crude Oil Prices Using Reservoir Computing Models</title><link>http://arxiv.org/abs/2306.03052v1</link><description>Accurate crude oil price prediction is crucial for financial decision-making.We propose a novel reservoir computing model for forecasting crude oil prices.It outperforms popular deep learning methods in most scenarios, as demonstratedthrough rigorous evaluation using daily closing price data from major stockmarket indices. Our model's competitive advantage is further validated bycomparing it with recent deep-learning approaches. This study introducesinnovative reservoir computing models for predicting crude oil prices, withpractical implications for financial practitioners. By leveraging advancedtechniques, market participants can enhance decision-making and gain valuableinsights into crude oil market dynamics.</description><author>Kaushal Kumar</author><pubDate>Mon, 05 Jun 2023 18:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03052v1</guid></item><item><title>ELEV-VISION: Automated Lowest Floor Elevation Estimation from Segmenting Street View Images</title><link>http://arxiv.org/abs/2306.03050v1</link><description>We propose an automated lowest floor elevation (LFE) estimation algorithmbased on computer vision techniques to leverage the latent information instreet view images. Flood depth-damage models use a combination of LFE andflood depth for determining flood risk and extent of damage to properties. Weused image segmentation for detecting door bottoms and roadside edges fromGoogle Street View images. The characteristic of equirectangular projectionwith constant spacing representation of horizontal and vertical angles allowsextraction of the pitch angle from the camera to the door bottom. The depthfrom the camera to the door bottom was obtained from the depthmap paired withthe Google Street View image. LFEs were calculated from the pitch angle and thedepth. The testbed for application of the proposed method is Meyerland (HarrisCounty, Texas). The results show that the proposed method achieved meanabsolute error of 0.190 m (1.18 %) in estimating LFE. The height differencebetween the street and the lowest floor (HDSL) was estimated to provideinformation for flood damage estimation. The proposed automatic LFE estimationalgorithm using Street View images and image segmentation provides a rapid andcost-effective method for LFE estimation compared with the surveys using totalstation theodolite and unmanned aerial systems. By obtaining more accurate andup-to-date LFE data using the proposed method, city planners, emergencyplanners and insurance companies could make a more precise estimation of flooddamage.</description><author>Yu-Hsuan Ho, Cheng-Chun Lee, Nicholas D. Diaz, Samuel D. Brody, Ali Mostafavi</author><pubDate>Mon, 05 Jun 2023 18:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03050v1</guid></item><item><title>From Robustness to Explainability and Back Again</title><link>http://arxiv.org/abs/2306.03048v1</link><description>In contrast with ad-hoc methods for eXplainable Artificial Intelligence(XAI), formal explainability offers important guarantees of rigor. However,formal explainability is hindered by poor scalability for some families ofclassifiers, the most significant being neural networks. As a result, there areconcerns as to whether formal explainability might serve to complement otherapproaches in delivering trustworthy AI. This paper addresses the limitation ofscalability of formal explainability, and proposes novel algorithms forcomputing formal explanations. The novel algorithm computes explanations byanswering instead a number of robustness queries, and such that the number ofsuch queries is at most linear on the number of features. Consequently, theproposed algorithm establishes a direct relationship between the practicalcomplexity of formal explainability and that of robustness. More importantly,the paper generalizes the definition of formal explanation, thereby allowingthe use of robustness tools that are based on different distance norms, andalso by reasoning in terms of some target degree of robustness. The experimentsvalidate the practical efficiency of the proposed approach.</description><author>Xuanxiang Huang, Joao Marques-Silva</author><pubDate>Mon, 05 Jun 2023 18:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03048v1</guid></item><item><title>NusaCrowd: Open Source Initiative for Indonesian NLP Resources</title><link>http://arxiv.org/abs/2212.09648v3</link><description>We present NusaCrowd, a collaborative initiative to collect and unifyexisting resources for Indonesian languages, including opening access topreviously non-public resources. Through this initiative, we have broughttogether 137 datasets and 118 standardized data loaders. The quality of thedatasets has been assessed manually and automatically, and their value isdemonstrated through multiple experiments. NusaCrowd's data collection enablesthe creation of the first zero-shot benchmarks for natural languageunderstanding and generation in Indonesian and the local languages ofIndonesia. Furthermore, NusaCrowd brings the creation of the first multilingualautomatic speech recognition benchmark in Indonesian and the local languages ofIndonesia. Our work strives to advance natural language processing (NLP)research for languages that are under-represented despite being widely spoken.</description><author>Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Fajri Koto, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmonangan, Ika Alfina, Muhammad Satrio Wicaksono, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Akbar Septiandri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Ryan Ignatius, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul Muslim Karo Karo, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Herry Sujaini, Sakriani Sakti, Ayu Purwar</author><pubDate>Mon, 05 Jun 2023 18:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09648v3</guid></item><item><title>Echoes of Biases: How Stigmatizing Language Affects AI Performance</title><link>http://arxiv.org/abs/2305.10201v3</link><description>Electronic health records (EHRs) serve as an essential data source for theenvisioned artificial intelligence (AI)-driven transformation in healthcare.However, clinician biases reflected in EHR notes can lead to AI modelsinheriting and amplifying these biases, perpetuating health disparities. Thisstudy investigates the impact of stigmatizing language (SL) in EHR notes onmortality prediction using a Transformer-based deep learning model andexplainable AI (XAI) techniques. Our findings demonstrate that SL written byclinicians adversely affects AI performance, particularly so for blackpatients, highlighting SL as a source of racial disparity in AI modeldevelopment. To explore an operationally efficient way to mitigate SL's impact,we investigate patterns in the generation of SL through a clinicians'collaborative network, identifying central clinicians as having a strongerimpact on racial disparity in the AI model. We find that removing SL written bycentral clinicians is a more efficient bias reduction strategy than eliminatingall SL in the entire corpus of data. This study provides actionable insightsfor responsible AI development and contributes to understanding clinicianbehavior and EHR note writing in healthcare.</description><author>Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal</author><pubDate>Mon, 05 Jun 2023 18:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10201v3</guid></item><item><title>SERT: A Transfomer Based Model for Spatio-Temporal Sensor Data with Missing Values for Environmental Monitoring</title><link>http://arxiv.org/abs/2306.03042v1</link><description>Environmental monitoring is crucial to our understanding of climate change,biodiversity loss and pollution. The availability of large-scalespatio-temporal data from sources such as sensors and satellites allows us todevelop sophisticated models for forecasting and understanding key drivers.However, the data collected from sensors often contain missing values due tofaulty equipment or maintenance issues. The missing values rarely occursimultaneously leading to data that are multivariate misaligned sparse timeseries. We propose two models that are capable of performing multivariatespatio-temporal forecasting while handling missing data naturally without theneed for imputation. The first model is a transformer-based model, which wename SERT (Spatio-temporal Encoder Representations from Transformers). Thesecond is a simpler model named SST-ANN (Sparse Spatio-Temporal ArtificialNeural Network) which is capable of providing interpretable results. We conductextensive experiments on two different datasets for multivariatespatio-temporal forecasting and show that our models have competitive orsuperior performance to those at the state-of-the-art.</description><author>Amin Shoari Nejad, RocÃ­o Alaiz-RodrÃ­guez, Gerard D. McCarthy, Brian Kelleher, Anthony Grey, Andrew Parnell</author><pubDate>Mon, 05 Jun 2023 18:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03042v1</guid></item><item><title>PFNs4BO: In-Context Learning for Bayesian Optimization</title><link>http://arxiv.org/abs/2305.17535v2</link><description>In this paper, we use Prior-data Fitted Networks (PFNs) as a flexiblesurrogate for Bayesian Optimization (BO). PFNs are neural processes that aretrained to approximate the posterior predictive distribution (PPD) throughin-context learning on any prior distribution that can be efficiently sampledfrom. We describe how this flexibility can be exploited for surrogate modelingin BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, anda Bayesian Neural Network (BNN). In addition, we show how to incorporatefurther information into the prior, such as allowing hints about the positionof optima (user priors), ignoring irrelevant dimensions, and performingnon-myopic BO by learning the acquisition function. The flexibility underlyingthese extensions opens up vast possibilities for using PFNs for BO. Wedemonstrate the usefulness of PFNs for BO in a large-scale evaluation onartificial GP samples and three different hyperparameter optimization testbeds:HPO-B, Bayesmark, and PD1. We publish code alongside trained models athttps://github.com/automl/PFNs4BO.</description><author>Samuel MÃ¼ller, Matthias Feurer, Noah Hollmann, Frank Hutter</author><pubDate>Mon, 05 Jun 2023 18:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17535v2</guid></item><item><title>Learning Similarity among Users for Personalized Session-Based Recommendation from hierarchical structure of User-Session-Item</title><link>http://arxiv.org/abs/2306.03040v1</link><description>The task of the session-based recommendation is to predict the nextinteraction of the user based on the anonymized user's behavior pattern. Andpersonalized version of this system is a promising research field due to itsavailability to deal with user information. However, there's a problem that theuser's preferences and historical sessions were not considered in the typicalsession-based recommendation since it concentrates only on user-iteminteraction. In addition, the existing personalized session-basedrecommendation model has a limited capability in that it only considers thepreference of the current user without considering those of similar users. Itmeans there can be the loss of information included within the hierarchicaldata structure of the user-session-item. To tackle with this problem, wepropose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).To model global historical sessions of users, we propose UserGraph that has twotypes of nodes - ItemNode and UserNode. We then connect the nodes with threetypes of edges. The first type of edges connects ItemNode as chronologicalorder, and the second connects ItemNode to UserNode, and the last connectsUserNode to ItemNode. With these user embeddings, we propose additionalcontrastive loss, that makes users with similar intention be close to eachother in the vector space. we apply graph neural network on these UserGraph andupdate nodes. Experimental results on two real-world datasets demonstrate thatour method outperforms some state-of-the-art approaches.</description><author>Jisoo Cha, Haemin Jeong, Wooju Kim</author><pubDate>Mon, 05 Jun 2023 18:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03040v1</guid></item><item><title>HeadSculpt: Crafting 3D Head Avatars with Text</title><link>http://arxiv.org/abs/2306.03038v1</link><description>Recently, text-guided 3D generative methods have made remarkable advancementsin producing high-quality textures and geometry, capitalizing on theproliferation of large vision-language and image diffusion models. However,existing methods still struggle to create high-fidelity 3D head avatars in twoaspects: (1) They rely mostly on a pre-trained text-to-image diffusion modelwhilst missing the necessary 3D awareness and head priors. This makes themprone to inconsistency and geometric distortions in the generated avatars. (2)They fall short in fine-grained editing. This is primarily due to the inheritedlimitations from the pre-trained 2D image diffusion models, which become morepronounced when it comes to 3D head avatars. In this work, we address thesechallenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculptfor crafting (i.e., generating and editing) 3D head avatars from textualprompts. Specifically, we first equip the diffusion model with 3D awareness byleveraging landmark-based control and a learned textual embedding representingthe back view appearance of heads, enabling 3D-consistent head avatargenerations. We further propose a novel identity-aware editing scoredistillation strategy to optimize a textured mesh with a high-resolutiondifferentiable rendering technique. This enables identity preservation whilefollowing the editing instruction. We showcase HeadSculpt's superior fidelityand editing capabilities through comprehensive experiments and comparisons withexisting methods.</description><author>Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong</author><pubDate>Mon, 05 Jun 2023 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03038v1</guid></item><item><title>Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination</title><link>http://arxiv.org/abs/2306.03034v1</link><description>Achieving coordination between humans and artificial intelligence inscenarios involving previously unencountered humans remains a substantialobstacle within Zero-Shot Human-AI Coordination, which aims to develop AIagents capable of efficiently working alongside previously unknown humanteammates. Traditional algorithms have aimed to collaborate with humans byoptimizing fixed objectives within a population, fostering diversity instrategies and behaviors. However, these techniques may lead to learning lossand an inability to cooperate with specific strategies within the population, aphenomenon named cooperative incompatibility. To mitigate this issue, weintroduce the Cooperative Open-ended LEarning (COLE) framework, whichformulates open-ended objectives in cooperative games with two players usingperspectives of graph theory to evaluate and pinpoint the cooperative capacityof each strategy. We put forth a practical algorithm incorporating insightsfrom game theory and graph theory, e.g., Shapley Value and Centrality. We alsoshow that COLE could effectively overcome the cooperative incompatibility fromtheoretical and empirical analysis. Subsequently, we created an onlineOvercooked human-AI experiment platform, the COLE platform, which enables easycustomization of questionnaires, model weights, and other aspects. Utilizingthe COLE platform, we enlist 130 participants for human experiments. Ourfindings reveal a preference for our approach over state-of-the-art methodsusing a variety of subjective metrics. Moreover, objective experimentaloutcomes in the Overcooked game environment indicate that our method surpassesexisting ones when coordinating with previously unencountered AI agents and thehuman proxy model. Our code and demo are publicly available athttps://sites.google.com/view/cole-2023.</description><author>Yang Li, Shao Zhang, Jichen Sun, Wenhao Zhang, Yali Du, Ying Wen, Xinbing Wang, Wei Pan</author><pubDate>Mon, 05 Jun 2023 17:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03034v1</guid></item><item><title>Classification of Edge-dependent Labels of Nodes in Hypergraphs</title><link>http://arxiv.org/abs/2306.03032v1</link><description>A hypergraph is a data structure composed of nodes and hyperedges, where eachhyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedgesize, hypergraphs represent group interactions (e.g., co-authorship by morethan two authors) more naturally and accurately than ordinary graphs.Interestingly, many real-world systems modeled as hypergraphs containedge-dependent node labels, i.e., node labels that vary depending onhyperedges. For example, on co-authorship datasets, the same author (i.e., anode) can be the primary author in a paper (i.e., a hyperedge) but thecorresponding author in another paper (i.e., another hyperedge). In this work, we introduce a classification of edge-dependent node labels asa new problem. This problem can be used as a benchmark task for hypergraphneural networks, which recently have attracted great attention, and also theusefulness of edge-dependent node labels has been verified in variousapplications. To tackle this problem, we propose WHATsNet, a novel hypergraphneural network that represents the same node differently depending on thehyperedges it participates in by reflecting its varying importance in thehyperedges. To this end, WHATsNet models the relations between nodes withineach hyperedge, using their relative centrality as positional encodings. In ourexperiments, we demonstrate that WHATsNet significantly and consistentlyoutperforms ten competitors on six real-world hypergraphs, and we also showsuccessful applications of WHATsNet to (a) ranking aggregation, (b) nodeclustering, and (c) product return prediction.</description><author>Minyoung Choe, Sunwoo Kim, Jaemin Yoo, Kijung Shin</author><pubDate>Mon, 05 Jun 2023 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03032v1</guid></item><item><title>Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset</title><link>http://arxiv.org/abs/2306.03030v1</link><description>Recent advancements in large language models (LLMs) have transformed thefield of question answering (QA). However, evaluating LLMs in the medical fieldis challenging due to the lack of standardized and comprehensive datasets. Toaddress this gap, we introduce CMExam, sourced from the Chinese NationalMedical Licensing Examination. CMExam consists of 60K+ multiple-choicequestions for standardized and objective evaluations, as well as solutionexplanations for model reasoning evaluation in an open-ended manner. Forin-depth analyses of LLMs, we invited medical professionals to label fiveadditional question-wise annotations, including disease groups, clinicaldepartments, medical disciplines, areas of competency, and question difficultylevels. Alongside the dataset, we further conducted thorough experiments withrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4had the best accuracy of 61.5% and a weighted F1 score of 0.616. These resultshighlight a great disparity when compared to human accuracy, which stood at71.6%. For explanation tasks, while LLMs could generate relevant reasoning anddemonstrate improved performance after finetuning, they fall short of a desiredstandard, indicating ample room for improvement. To the best of our knowledge,CMExam is the first Chinese medical exam dataset to provide comprehensivemedical annotations. The experiments and findings of LLM evaluation alsoprovide valuable insights into the challenges and potential solutions indeveloping Chinese medical QA systems and LLM evaluation pipelines. The datasetand relevant code are available at https://github.com/williamliujl/CMExam.</description><author>Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi Li</author><pubDate>Mon, 05 Jun 2023 17:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03030v1</guid></item><item><title>AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and Practices</title><link>http://arxiv.org/abs/2306.03025v1</link><description>Cone-beam computed tomography (CBCT) is a popular imaging modality indentistry for diagnosing and planning treatment for a variety of oral diseaseswith the ability to produce detailed, three-dimensional images of the teeth,jawbones, and surrounding structures. CBCT imaging has emerged as an essentialdiagnostic tool in dentistry. CBCT imaging has seen significant improvements interms of its diagnostic value, as well as its accuracy and efficiency, with themost recent development of artificial intelligence (AI) techniques. This paperreviews recent AI trends and practices in dental CBCT imaging. AI has been usedfor lesion detection, malocclusion classification, measurement of buccal bonethickness, and classification and segmentation of teeth, alveolar bones,mandibles, landmarks, contours, and pharyngeal airways using CBCT images.Mainly machine learning algorithms, deep learning algorithms, andsuper-resolution techniques are used for these tasks. This review focuses onthe potential of AI techniques to transform CBCT imaging in dentistry, whichwould improve both diagnosis and treatment planning. Finally, we discuss thechallenges and limitations of artificial intelligence in dentistry and CBCTimaging.</description><author>Saba Sarwar, Suraiya Jabin</author><pubDate>Mon, 05 Jun 2023 17:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03025v1</guid></item><item><title>Cooperative Open-ended Learning Framework for Zero-shot Coordination</title><link>http://arxiv.org/abs/2302.04831v3</link><description>Zero-shot coordination in cooperative artificial intelligence (AI) remains asignificant challenge, which means effectively coordinating with a wide rangeof unseen partners. Previous algorithms have attempted to address thischallenge by optimizing fixed objectives within a population to improvestrategy or behaviour diversity. However, these approaches can result in a lossof learning and an inability to cooperate with certain strategies within thepopulation, known as cooperative incompatibility. To address this issue, wepropose the Cooperative Open-ended LEarning (COLE) framework, which constructsopen-ended objectives in cooperative games with two players from theperspective of graph theory to assess and identify the cooperative ability ofeach strategy. We further specify the framework and propose a practicalalgorithm that leverages knowledge from game theory and graph theory.Furthermore, an analysis of the learning process of the algorithm shows that itcan efficiently overcome cooperative incompatibility. The experimental resultsin the Overcooked game environment demonstrate that our method outperformscurrent state-of-the-art methods when coordinating with different-levelpartners. Our demo is available at https://sites.google.com/view/cole-2023.</description><author>Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, Wei Pan</author><pubDate>Mon, 05 Jun 2023 17:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04831v3</guid></item><item><title>PokemonChat: Auditing ChatGPT for PokÃ©mon Universe Knowledge</title><link>http://arxiv.org/abs/2306.03024v1</link><description>The recently released ChatGPT model demonstrates unprecedented capabilitiesin zero-shot question-answering. In this work, we probe ChatGPT for itsconversational understanding and introduce a conversational framework(protocol) that can be adopted in future studies. The Pok\'emon universe servesas an ideal testing ground for auditing ChatGPT's reasoning capabilities due toits closed world assumption. After bringing ChatGPT's background knowledge (onthe Pok\'emon universe) to light, we test its reasoning process when usingthese concepts in battle scenarios. We then evaluate its ability to acquire newknowledge and include it in its reasoning process. Our ultimate goal is toassess ChatGPT's ability to generalize, combine features, and to acquire andreason over newly introduced knowledge from human feedback. We find thatChatGPT has prior knowledge of the Pokemon universe, which can reason upon inbattle scenarios to a great extent, even when new information is introduced.The model performs better with collaborative feedback and if there is aninitial phase of information retrieval, but also hallucinates occasionally andis susceptible to adversarial attacks.</description><author>Laura Cabello, Jiaang Li, Ilias Chalkidis</author><pubDate>Mon, 05 Jun 2023 17:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03024v1</guid></item><item><title>Transformer and Snowball Graph Convolution Learning for Brain functional network Classification</title><link>http://arxiv.org/abs/2303.16132v2</link><description>Advanced deep learning methods, especially graph neural networks (GNNs), areincreasingly expected to learn from brain functional network data and identifythe functional connections between brain disorder and health. In this paper, weproposed a novel Transformer and snowball encoding networks (TSEN) for brainfunctional network classification, which introduced Transformer architecturewith graph snowball connection into GNNs for learning whole-graphrepresentation. TSEN combined graph snowball connection with graph Transformerby snowball encoding layers, which enhanced the power to capture multi-scaleinformation and global patterns of brain functional networks. TSEN alsointroduced snowball graph convolution as position embedding in Transformerstructure, which was a simple yet effective method for capturing local patternsnaturally. We evaluated the proposed model by two large-scale brain functionalnetwork datasets, and the results demonstrated that TSEN outperformed thestate-of-the-art GNN models and the graph-transformer based GNN models.</description><author>Jinlong Hu, Yangmin Huang, Shoubin Dong</author><pubDate>Mon, 05 Jun 2023 17:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16132v2</guid></item><item><title>Interpretable Alzheimer's Disease Classification Via a Contrastive Diffusion Autoencoder</title><link>http://arxiv.org/abs/2306.03022v1</link><description>In visual object classification, humans often justify their choices bycomparing objects to prototypical examples within that class. We may thereforeincrease the interpretability of deep learning models by imbuing them with asimilar style of reasoning. In this work, we apply this principle byclassifying Alzheimer's Disease based on the similarity of images to trainingexamples within the latent space. We use a contrastive loss combined with adiffusion autoencoder backbone, to produce a semantically meaningful latentspace, such that neighbouring latents have similar image-level features. Weachieve a classification accuracy comparable to black box approaches on adataset of 2D MRI images, whilst producing human interpretable modelexplanations. Therefore, this work stands as a contribution to the pertinentdevelopment of accurate and interpretable deep learning within medical imaging.</description><author>Ayodeji Ijishakin, Ahmed Abdulaal, Adamos Hadjivasiliou, Sophie Martin, James Cole</author><pubDate>Mon, 05 Jun 2023 17:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03022v1</guid></item><item><title>Automating Style Analysis and Visualization With Explainable AI -- Case Studies on Brand Recognition</title><link>http://arxiv.org/abs/2306.03021v1</link><description>Incorporating style-related objectives into shape design has been centrallyimportant to maximize product appeal. However, stylistic features such asaesthetics and semantic attributes are hard to codify even for experts. Assuch, algorithmic style capture and reuse have not fully benefited fromautomated data-driven methodologies due to the challenging nature of designdescribability. This paper proposes an AI-driven method to fully automate thediscovery of brand-related features. Our approach introduces BIGNet, a two-tierBrand Identification Graph Neural Network (GNN) to classify and analyze scalarvector graphics (SVG). First, to tackle the scarcity of vectorized productimages, this research proposes two data acquisition workflows: parametricmodeling from small curve-based datasets, and vectorization from largepixel-based datasets. Secondly, this study constructs a novel hierarchical GNNarchitecture to learn from both SVG's curve-level and chunk-level parameters.In the first case study, BIGNet not only classifies phone brands but alsocaptures brand-related features across multiple scales, such as the location ofthe lens, the height-width ratio, and the screen-frame gap, as confirmed by AIevaluation. In the second study, this paper showcases the generalizability ofBIGNet learning from a vectorized car image dataset and validates theconsistency and robustness of its predictions given four scenarios. The resultsmatch the difference commonly observed in luxury vs. economy brands in theautomobile market. Finally, this paper also visualizes the activation mapsgenerated from a convolutional neural network and shows BIGNet's advantage ofbeing a more human-friendly, explainable, and explicit style-capturing agent.Code and dataset can be found on Github: 1. Phone case study: github.com/parksandrecfan/bignet-phone 2. Car casestudy: github.com/parksandrecfan/bignet-car</description><author>Yu-hsuan Chen, Levent Burak Kara, Jonathan Cagan</author><pubDate>Mon, 05 Jun 2023 17:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03021v1</guid></item><item><title>Quantification of Uncertainties in Deep Learning-based Environment Perception</title><link>http://arxiv.org/abs/2306.03018v1</link><description>In this work, we introduce a novel Deep Learning-based method to perceive theenvironment of a vehicle based on radar scans while accounting foruncertainties in its predictions. The environment of the host vehicle issegmented into equally sized grid cells which are classified individually.Complementary to the segmentation output, our Deep Learning-based algorithm iscapable of differentiating uncertainties in its predictions as being related toan inadequate model (epistemic uncertainty) or noisy data (aleatoricuncertainty). To this end, weights are described as probability distributionsaccounting for uncertainties in the model parameters. Distributions are learnedin a supervised fashion using gradient descent. We prove that uncertainties inthe model output correlate with the precision of its predictions. Compared toprevious concepts, we show superior performance of our approach to reliablyperceive the environment of a vehicle.</description><author>Marco Braun, Moritz Luszek, Jan Siegemund, Kevin Kollek, Anton Kummert</author><pubDate>Mon, 05 Jun 2023 17:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03018v1</guid></item><item><title>Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2112.04571v4</link><description>A main research goal in various studies is to use an observational data setand provide a new set of counterfactual guidelines that can yield causalimprovements. Dynamic Treatment Regimes (DTRs) are widely studied to formalizethis process. However, available methods in finding optimal DTRs often rely onassumptions that are violated in real-world applications (e.g., medicaldecision-making or public policy), especially when (a) the existence ofunobserved confounders cannot be ignored, and (b) the unobserved confoundersare time-varying (e.g., affected by previous actions). When such assumptionsare violated, one often faces ambiguity regarding the underlying causal model.This ambiguity is inevitable, since the dynamics of unobserved confounders andtheir causal impact on the observed part of the data cannot be understood fromthe observed data. Motivated by a case study of finding superior treatmentregimes for patients who underwent transplantation in our partner hospital andfaced a medical condition known as New Onset Diabetes After Transplantation(NODAT), we extend DTRs to a new class termed Ambiguous Dynamic TreatmentRegimes (ADTRs), in which the causal impact of treatment regimes is evaluatedbased on a "cloud" of causal models. We then connect ADTRs to AmbiguousPartially Observable Mark Decision Processes (APOMDPs) and developReinforcement Learning methods, which enable using the observed data toefficiently learn an optimal treatment regime. We establish theoretical resultsfor these learning methods, including (weak) consistency and asymptoticnormality. We further evaluate the performance of these learning methods bothin our case study and in simulation experiments.</description><author>Soroush Saghafian</author><pubDate>Mon, 05 Jun 2023 17:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.04571v4</guid></item><item><title>On the Behavior of Intrusive and Non-intrusive Speech Enhancement Metrics in Predictive and Generative Settings</title><link>http://arxiv.org/abs/2306.03014v1</link><description>Since its inception, the field of deep speech enhancement has been dominatedby predictive (discriminative) approaches, such as spectral mapping or masking.Recently, however, novel generative approaches have been applied to speechenhancement, attaining good denoising performance with high subjective qualityscores. At the same time, advances in deep learning also allowed for thecreation of neural network-based metrics, which have desirable traits such asbeing able to work without a reference (non-intrusively). Since generativelyenhanced speech tends to exhibit radically different residual distortions, itsevaluation using instrumental speech metrics may behave differently compared topredictively enhanced speech. In this paper, we evaluate the performance of thesame speech enhancement backbone trained under predictive and generativeparadigms on a variety of metrics and show that intrusive and non-intrusivemeasures correlate differently for each paradigm. This analysis motivates thesearch for metrics that can together paint a complete and unbiased picture ofspeech enhancement performance, irrespective of the model's training process.</description><author>Danilo de Oliveira, Julius Richter, Jean-Marie Lemercier, Tal Peer, Timo Gerkmann</author><pubDate>Mon, 05 Jun 2023 17:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03014v1</guid></item><item><title>Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning</title><link>http://arxiv.org/abs/2306.03013v1</link><description>Malicious server (MS) attacks have enabled the scaling of data stealing infederated learning to large batch sizes and secure aggregation, settingspreviously considered private. However, many concerns regarding client-sidedetectability of MS attacks were raised, questioning their practicality oncethey are publicly known. In this work, for the first time, we thoroughly studythe problem of client-side detectability.We demonstrate that most prior MSattacks, which fundamentally rely on one of two key principles, are detectableby principled client-side checks. Further, we formulate desiderata forpractical MS attacks and propose SEER, a novel attack framework that satisfiesall desiderata, while stealing user data from gradients of realistic networks,even for large batch sizes (up to 512 in our experiments) and under secureaggregation. The key insight of SEER is the use of a secret decoder, which isjointly trained with the shared model. Our work represents a promising firststep towards more principled treatment of MS attacks, paving the way forrealistic data stealing that can compromise user privacy in real-worlddeployments.</description><author>Kostadin Garov, Dimitar I. Dimitrov, Nikola JovanoviÄ, Martin Vechev</author><pubDate>Mon, 05 Jun 2023 17:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03013v1</guid></item><item><title>Interval Load Forecasting for Individual Households in the Presence of Electric Vehicle Charging</title><link>http://arxiv.org/abs/2306.03010v1</link><description>The transition to Electric Vehicles (EV) in place of traditional internalcombustion engines is increasing societal demand for electricity. The abilityto integrate the additional demand from EV charging into forecastingelectricity demand is critical for maintaining the reliability of electricitygeneration and distribution. Load forecasting studies typically excludehouseholds with home EV charging, focusing on offices, schools, and publiccharging stations. Moreover, they provide point forecasts which do not offerinformation about prediction uncertainty. Consequently, this paper proposes theLong Short-Term Memory Bayesian Neural Networks (LSTM-BNNs) for household loadforecasting in presence of EV charging. The approach takes advantage of theLSTM model to capture the time dependencies and uses the dropout layer withBayesian inference to generate prediction intervals. Results show that theproposed LSTM-BNNs achieve accuracy similar to point forecasts with theadvantage of prediction intervals. Moreover, the impact of lockdowns related tothe COVID-19 pandemic on the load forecasting model is examined, and theanalysis shows that there is no major change in the model performance as, forthe considered households, the randomness of the EV charging outweighs thechange due to pandemic.</description><author>Raiden Skala, Mohamed Ahmed T. A. Elgalhud, Katarina Grolinger, Syed Mir</author><pubDate>Mon, 05 Jun 2023 17:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03010v1</guid></item><item><title>Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance</title><link>http://arxiv.org/abs/2206.03787v3</link><description>Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms ofexploration such as the additive action noise often used in continuous controldomains. Typically, the scaling factor of this action noise is chosen as ahyper-parameter and is kept constant during training. In this paper, we focuson action noise in off-policy deep reinforcement learning for continuouscontrol. We analyze how the learned policy is impacted by the noise type, noisescale, and impact scaling factor reduction schedule. We consider the two mostprominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, andperform a vast experimental campaign by systematically varying the noise typeand scale parameter, and by measuring variables of interest like the expectedreturn of the policy and the state-space coverage during exploration. For thelatter, we propose a novel state-space coverage measure$\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimationartifacts caused by points close to the state-space boundary thanpreviously-proposed measures. Larger noise scales generally increasestate-space coverage. However, we found that increasing the space coverageusing a larger noise scale is often not beneficial. On the contrary, reducingthe noise scale over the training process reduces the variance and generallyimproves the learning performance. We conclude that the best noise type andscale are environment dependent, and based on our observations derive heuristicrules for guiding the choice of the action noise as a starting point forfurther optimization.</description><author>Jakob Hollenstein, Sayantan Auddy, Matteo Saveriano, Erwan Renaudo, Justus Piater</author><pubDate>Mon, 05 Jun 2023 17:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03787v3</guid></item><item><title>A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets</title><link>http://arxiv.org/abs/2305.18486v2</link><description>The development of large language models (LLMs) such as ChatGPT has brought alot of attention recently. However, their evaluation in the benchmark academicdatasets remains under-explored due to the difficulty of evaluating thegenerative outputs produced by this model against the ground truth. In thispaper, we aim to present a thorough evaluation of ChatGPT's performance ondiverse academic datasets, covering tasks like question-answering, textsummarization, code generation, commonsense reasoning, mathematicalproblem-solving, machine translation, bias detection, and ethicalconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze255K responses it generates in these datasets. This makes our work the largestevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validatethe strengths and weaknesses of ChatGPT in various tasks and provide insightsfor future research using LLMs. We also report a new emergent ability to followmulti-query instructions that we mostly found in ChatGPT and otherinstruction-tuned models. Our extensive evaluation shows that even thoughChatGPT is capable of performing a wide variety of tasks, and may obtainimpressive performance in several benchmark datasets, it is still far fromachieving the ability to reliably solve many challenging tasks. By providing athorough assessment of ChatGPT's performance across diverse NLP tasks, thispaper sets the stage for a targeted deployment of ChatGPT-like LLMs inreal-world applications.</description><author>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang</author><pubDate>Mon, 05 Jun 2023 17:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18486v2</guid></item><item><title>Using Sequences of Life-events to Predict Human Lives</title><link>http://arxiv.org/abs/2306.03009v1</link><description>Over the past decade, machine learning has revolutionized computers' abilityto analyze text through flexible computational models. Due to their structuralsimilarity to written language, transformer-based architectures have also shownpromise as tools to make sense of a range of multi-variate sequences fromprotein-structures, music, electronic health records to weather-forecasts. Wecan also represent human lives in a way that shares this structural similarityto language. From one perspective, lives are simply sequences of events: Peopleare born, visit the pediatrician, start school, move to a new location, getmarried, and so on. Here, we exploit this similarity to adapt innovations fromnatural language processing to examine the evolution and predictability ofhuman lives based on detailed event sequences. We do this by drawing onarguably the most comprehensive registry data in existence, available for anentire nation of more than six million individuals across decades. Our datainclude information about life-events related to health, education, occupation,income, address, and working hours, recorded with day-to-day resolution. Wecreate embeddings of life-events in a single vector space showing that thisembedding space is robust and highly structured. Our models allow us to predictdiverse outcomes ranging from early mortality to personality nuances,outperforming state-of-the-art models by a wide margin. Using methods forinterpreting deep learning models, we probe the algorithm to understand thefactors that enable our predictions. Our framework allows researchers toidentify new potential mechanisms that impact life outcomes and associatedpossibilities for personalized interventions.</description><author>Germans Savcisens, Tina Eliassi-Rad, Lars Kai Hansen, Laust Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, Sune Lehmann</author><pubDate>Mon, 05 Jun 2023 17:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03009v1</guid></item><item><title>Nonparametric Iterative Machine Teaching</title><link>http://arxiv.org/abs/2306.03007v1</link><description>In this paper, we consider the problem of Iterative Machine Teaching (IMT),where the teacher provides examples to the learner iteratively such that thelearner can achieve fast convergence to a target model. However, existing IMTalgorithms are solely based on parameterized families of target models. Theymainly focus on convergence in the parameter space, resulting in difficultywhen the target models are defined to be functions without dependency onparameters. To address such a limitation, we study a more general task --Nonparametric Iterative Machine Teaching (NIMT), which aims to teachnonparametric target models to learners in an iterative fashion. Unlikeparametric IMT that merely operates in the parameter space, we cast NIMT as afunctional optimization problem in the function space. To solve it, we proposeboth random and greedy functional teaching algorithms. We obtain the iterativeteaching dimension (ITD) of the random teaching algorithm under properassumptions, which serves as a uniform upper bound of ITD in NIMT. Further, thegreedy teaching algorithm has a significantly lower ITD, which reaches atighter upper bound of ITD in NIMT. Finally, we verify the correctness of ourtheoretical findings with extensive experiments in nonparametric scenarios.</description><author>Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok</author><pubDate>Mon, 05 Jun 2023 17:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03007v1</guid></item><item><title>Unveiling the Two-Faced Truth: Disentangling Morphed Identities for Face Morphing Detection</title><link>http://arxiv.org/abs/2306.03002v1</link><description>Morphing attacks keep threatening biometric systems, especially facerecognition systems. Over time they have become simpler to perform and morerealistic, as such, the usage of deep learning systems to detect these attackshas grown. At the same time, there is a constant concern regarding the lack ofinterpretability of deep learning models. Balancing performance andinterpretability has been a difficult task for scientists. However, byleveraging domain information and proving some constraints, we have been ableto develop IDistill, an interpretable method with state-of-the-art performancethat provides information on both the identity separation on morph samples andtheir contribution to the final prediction. The domain information is learnt byan autoencoder and distilled to a classifier system in order to teach it toseparate identity information. When compared to other methods in the literatureit outperforms them in three out of five databases and is competitive in theremaining.</description><author>Eduarda Caldeira, Pedro C. Neto, Tiago GonÃ§alves, Naser Damer, Ana F. Sequeira, Jaime S. Cardoso</author><pubDate>Mon, 05 Jun 2023 17:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03002v1</guid></item><item><title>BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance Fields</title><link>http://arxiv.org/abs/2306.03000v1</link><description>Neural rendering combines ideas from classical computer graphics and machinelearning to synthesize images from real-world observations. NeRF, short forNeural Radiance Fields, is a recent innovation that uses AI algorithms tocreate 3D objects from 2D images. By leveraging an interpolation approach, NeRFcan produce new 3D reconstructed views of complicated scenes. Rather thandirectly restoring the whole 3D scene geometry, NeRF generates a volumetricrepresentation called a ``radiance field,'' which is capable of creating colorand density for every point within the relevant 3D space. The broad appeal andnotoriety of NeRF make it imperative to examine the existing research on thetopic comprehensively. While previous surveys on 3D rendering have primarilyfocused on traditional computer vision-based or deep learning-based approaches,only a handful of them discuss the potential of NeRF. However, such surveyshave predominantly focused on NeRF's early contributions and have not exploredits full potential. NeRF is a relatively new technique continuously beinginvestigated for its capabilities and limitations. This survey reviews recentadvances in NeRF and categorizes them according to their architectural designs,especially in the field of novel view synthesis.</description><author>AKM Shahariar Azad Rabby, Chengcui Zhang</author><pubDate>Mon, 05 Jun 2023 17:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03000v1</guid></item><item><title>Over-the-Air Federated Learning in Satellite systems</title><link>http://arxiv.org/abs/2306.02996v1</link><description>Federated learning in satellites offers several advantages. Firstly, itensures data privacy and security, as sensitive data remains on the satellitesand is not transmitted to a central location. This is particularly importantwhen dealing with sensitive or classified information. Secondly, federatedlearning allows satellites to collectively learn from a diverse set of datasources, benefiting from the distributed knowledge across the satellitenetwork. Lastly, the use of federated learning reduces the communicationbandwidth requirements between satellites and the central server, as only modelupdates are exchanged instead of raw data. By leveraging federated learning,satellites can collaborate and continuously improve their machine learningmodels while preserving data privacy and minimizing communication overhead.This enables the development of more intelligent and efficient satellitesystems for various applications, such as Earth observation, weatherforecasting, and space exploration.</description><author>Edward Akito Carlos, Raphael Pinard, Mitra Hassani</author><pubDate>Mon, 05 Jun 2023 17:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02996v1</guid></item><item><title>Long-range UAV Thermal Geo-localization with Satellite Imagery</title><link>http://arxiv.org/abs/2306.02994v1</link><description>Onboard sensors, such as cameras and thermal sensors, have emerged aseffective alternatives to Global Positioning System (GPS) for geo-localizationin Unmanned Aerial Vehicle (UAV) navigation. Since GPS can suffer from signalloss and spoofing problems, researchers have explored camera-based techniquessuch as Visual Geo-localization (VG) using satellite imagery. Additionally,thermal geo-localization (TG) has become crucial for long-range UAV flights inlow-illumination environments. This paper proposes a novel thermalgeo-localization framework using satellite imagery, which includes multipledomain adaptation methods to address the limited availability of paired thermaland satellite images. The experimental results demonstrate the effectiveness ofthe proposed approach in achieving reliable thermal geo-localizationperformance, even in thermal images with indistinct self-similar features. Weevaluate our approach on real data collected onboard a UAV. We also release thecode and \textit{Boson-nighttime}, a dataset of paired satellite-thermal andunpaired satellite images for thermal geo-localization with satellite imagery.To the best of our knowledge, this work is the first to propose a thermalgeo-localization method using satellite imagery in long-range flights.</description><author>Jiuhong Xiao, Daniel Tortei, Eloy Roura, Giuseppe Loianno</author><pubDate>Mon, 05 Jun 2023 17:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02994v1</guid></item><item><title>Integrated Sensing, Computation, and Communication for UAV-assisted Federated Edge Learning</title><link>http://arxiv.org/abs/2306.02990v1</link><description>Federated edge learning (FEEL) enables privacy-preserving model trainingthrough periodic communication between edge devices and the server. UnmannedAerial Vehicle (UAV)-mounted edge devices are particularly advantageous forFEEL due to their flexibility and mobility in efficient data collection. InUAV-assisted FEEL, sensing, computation, and communication are coupled andcompete for limited onboard resources, and UAV deployment also affects sensingand communication performance. Therefore, the joint design of UAV deploymentand resource allocation is crucial to achieving the optimal trainingperformance. In this paper, we address the problem of joint UAV deploymentdesign and resource allocation for FEEL via a concrete case study of humanmotion recognition based on wireless sensing. We first analyze the impact ofUAV deployment on the sensing quality and identify a threshold value for thesensing elevation angle that guarantees a satisfactory quality of data samples.Due to the non-ideal sensing channels, we consider the probabilistic sensingmodel, where the successful sensing probability of each UAV is determined byits position. Then, we derive the upper bound of the FEEL training loss as afunction of the sensing probability. Theoretical results suggest that theconvergence rate can be improved if UAVs have a uniform successful sensingprobability. Based on this analysis, we formulate a training time minimizationproblem by jointly optimizing UAV deployment, integrated sensing, computation,and communication (ISCC) resources under a desirable optimality gap constraint.To solve this challenging mixed-integer non-convex problem, we apply thealternating optimization technique, and propose the bandwidth, batch size, andposition optimization (BBPO) scheme to optimize these three decision variablesalternately.</description><author>Yao Tang, Guangxu Zhu, Wei Xu, Man Hon Cheung, Tat-Ming Lok, Shuguang Cui</author><pubDate>Mon, 05 Jun 2023 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02990v1</guid></item><item><title>Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models</title><link>http://arxiv.org/abs/2306.02986v1</link><description>Large annotated datasets are required for training deep learning models, butin medical imaging data sharing is often complicated due to ethics,anonymization and data protection legislation (e.g. the general data protectionregulation (GDPR)). Generative AI models, such as generative adversarialnetworks (GANs) and diffusion models, can today produce very realisticsynthetic images, and can potentially facilitate data sharing as GDPR shouldnot apply for medical images which do not belong to a specific person. However,in order to share synthetic images it must first be demonstrated that they canbe used for training different networks with acceptable performance. Here, wetherefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1-3)and a diffusion model for the task of brain tumor segmentation. Our resultsshow that segmentation networks trained on synthetic images reach Dice scoresthat are 80\% - 90\% of Dice scores when training with real images, but thatmemorization of the training images can be a problem for diffusion models ifthe original dataset is too small. Furthermore, we demonstrate that commonmetrics for evaluating synthetic images, Fr\'echet inception distance (FID) andinception score (IS), do not correlate well with the obtained performance whenusing the synthetic images for training segmentation networks.</description><author>Muhammad Usman Akbar, MÃ¥ns Larsson, Anders Eklund</author><pubDate>Mon, 05 Jun 2023 16:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02986v1</guid></item><item><title>Evaluating Inter-Bilingual Semantic Parsing for Indian Languages</title><link>http://arxiv.org/abs/2304.13005v2</link><description>Despite significant progress in Natural Language Generation for Indianlanguages (IndicNLP), there is a lack of datasets around complex structuredtasks such as semantic parsing. One reason for this imminent gap is thecomplexity of the logical form, which makes English to multilingual translationdifficult. The process involves alignment of logical forms, intents and slotswith translated unstructured utterance. To address this, we propose anInter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinctIndian languages. We highlight the proposed task's practicality, and evaluateexisting multilingual seq2seq models across several train-test strategies. Ourexperiment reveals a high correlation across performance of originalmultilingual semantic parsing datasets (such as mTOP, multilingual TOP andmultiATIS++) and our proposed IE-SEMPARSE suite.</description><author>Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan</author><pubDate>Mon, 05 Jun 2023 16:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13005v2</guid></item><item><title>Representation-agnostic distance-driven perturbation for optimizing ill-conditioned problems</title><link>http://arxiv.org/abs/2306.02985v1</link><description>Locality is a crucial property for efficiently optimising black-box problemswith randomized search heuristics. However, in practical applications, it isnot likely to always find such a genotype encoding of candidate solutions thatthis property is upheld with respect to the Hamming distance. At the same time,it may be possible to use domain-specific knowledge to define a metric withlocality property. We propose two mutation operators to solve such optimizationproblems more efficiently using the metric. The first operator assumes priorknowledge about the distance, the second operator uses the distance as a blackbox. Those operators apply an estimation of distribution algorithm to find thebest mutant according to the defined in the paper function, which employs thegiven distance. For pseudo-boolean and integer optimization problems, weexperimentally show that both mutation operators speed up the search on most ofthe functions when applied in considered evolutionary algorithms and randomlocal search. Moreover, those operators can be applied in any randomized searchheuristic which uses perturbations. However, our mutation operators increasewall-clock time and so are helpful in practice when distance is (much) cheaperto compute than the real objective function.</description><author>Kirill Antonov, Anna V. Kononova, Thomas BÃ¤ck, Niki van Stein</author><pubDate>Mon, 05 Jun 2023 16:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02985v1</guid></item><item><title>A Deep Learning Approach Utilizing Covariance Matrix Analysis for the ISBI Edited MRS Reconstruction Challenge</title><link>http://arxiv.org/abs/2306.02984v1</link><description>This work proposes a method to accelerate the acquisition of high-qualityedited magnetic resonance spectroscopy (MRS) scans using machine learningmodels taking the sample covariance matrix as input. The method is invariant tothe number of transients and robust to noisy input data for both synthetic aswell as in-vivo scenarios.</description><author>Julian P. Merkofer, Dennis M. J. van de Sande, Sina Amirrajab, Gerhard S. Drenthen, Mitko Veta, Jacobus F. A. Jansen, Marcel Breeuwer, Ruud J. G. van Sloun</author><pubDate>Mon, 05 Jun 2023 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02984v1</guid></item><item><title>PolyVoice: Language Models for Speech to Speech Translation</title><link>http://arxiv.org/abs/2306.02982v1</link><description>We propose PolyVoice, a language model-based framework for speech-to-speechtranslation (S2ST) system. Our framework consists of two language models: atranslation language model and a speech synthesis language model. We usediscretized speech units, which are generated in a fully unsupervised way, andthus our framework can be used for unwritten languages. For the speechsynthesis part, we adopt the existing VALL-E X approach and build a unit-basedaudio language model. This grants our framework the ability to preserve thevoice characteristics and the speaking style of the original speech. We examineour system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanishpairs. Experimental results show that our system can generate speech with hightranslation quality and audio quality. Speech samples are available athttps://speechtranslation.github.io/polyvoice.</description><author>Qianqian Dong, Zhiying Huang, Chen Xu, Yunlong Zhao, Kexin Wang, Xuxin Cheng, Tom Ko, Qiao Tian, Tang Li, Fengpeng Yue, Ye Bai, Xi Chen, Lu Lu, Zejun Ma, Yuping Wang, Mingxuan Wang, Yuxuan Wang</author><pubDate>Mon, 05 Jun 2023 16:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02982v1</guid></item><item><title>KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations</title><link>http://arxiv.org/abs/2306.02980v1</link><description>While recent works have been considerably improving the quality of thenatural language explanations (NLEs) generated by a model to justify itspredictions, there is very limited research in detecting and alleviatinginconsistencies among generated NLEs. In this work, we leverage externalknowledge bases to significantly improve on an existing adversarial attack fordetecting inconsistent NLEs. We apply our attack to high-performing NLE modelsand show that models with higher NLE quality do not necessarily generate fewerinconsistencies. Moreover, we propose an off-the-shelf mitigation method toalleviate inconsistencies by grounding the model into external backgroundknowledge. Our method decreases the inconsistencies of previous high-performingNLE models as detected by our attack.</description><author>Myeongjun Jang, Bodhisattwa Prasad Majumder, Julian McAuley, Thomas Lukasiewicz, Oana-Maria Camburu</author><pubDate>Mon, 05 Jun 2023 16:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02980v1</guid></item><item><title>The Chai Platform's AI Safety Framework</title><link>http://arxiv.org/abs/2306.02979v1</link><description>Chai empowers users to create and interact with customized chatbots, offeringunique and engaging experiences. Despite the exciting prospects, the workrecognizes the inherent challenges of a commitment to modern safety standards.Therefore, this paper presents the integrated AI safety principles into Chai toprioritize user safety, data protection, and ethical technology use. The paperspecifically explores the multidimensional domain of AI safety research,demonstrating its application in Chai's conversational chatbot platform. Itpresents Chai's AI safety principles, informed by well-established AI researchcentres and adapted for chat AI. This work proposes the following safetyframework: Content Safeguarding; Stability and Robustness; and OperationalTransparency and Traceability. The subsequent implementation of theseprinciples is outlined, followed by an experimental analysis of Chai's AIsafety framework's real-world impact. We emphasise the significance ofconscientious application of AI safety principles and robust safety measures.The successful implementation of the safe AI framework in Chai indicates thepracticality of mitigating potential risks for responsible and ethical use ofAI technologies. The ultimate vision is a transformative AI tool fosteringprogress and innovation while prioritizing user safety and ethical standards.</description><author>Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp</author><pubDate>Mon, 05 Jun 2023 16:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02979v1</guid></item><item><title>Which Argumentative Aspects of Hate Speech in Social Media can be reliably identified?</title><link>http://arxiv.org/abs/2306.02978v1</link><description>With the increasing diversity of use cases of large language models, a moreinformative treatment of texts seems necessary. An argumentative analysis couldfoster a more reasoned usage of chatbots, text completion mechanisms or otherapplications. However, it is unclear which aspects of argumentation can bereliably identified and integrated in language models. In this paper, wepresent an empirical assessment of the reliability with which differentargumentative aspects can be automatically identified in hate speech in socialmedia. We have enriched the Hateval corpus (Basile et al. 2019) with a manualannotation of some argumentative components, adapted from Wagemans (2016)'sPeriodic Table of Arguments. We show that some components can be identifiedwith reasonable reliability. For those that present a high error ratio, weanalyze the patterns of disagreement between expert annotators and errors inautomatic procedures, and we propose adaptations of those categories that canbe more reliably reproduced.</description><author>DamiÃ¡n Furman, Pablo Torres, JosÃ© A. RodrÃ­guez, Diego Letzen, Vanina MartÃ­nez, Laura Alonso Alemany</author><pubDate>Mon, 05 Jun 2023 16:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02978v1</guid></item><item><title>Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</title><link>http://arxiv.org/abs/2302.11552v2</link><description>Since their introduction, diffusion models have quickly become the prevailingapproach to generative modeling in many domains. They can be interpreted aslearning the gradients of a time-varying sequence of log-probability densityfunctions. This interpretation has motivated classifier-based andclassifier-free guidance as methods for post-hoc control of diffusion models.In this work, we build upon these ideas using the score-based interpretation ofdiffusion models, and explore alternative ways to condition, modify, and reusediffusion models for tasks involving compositional generation and guidance. Inparticular, we investigate why certain types of composition fail using currenttechniques and present a number of solutions. We conclude that the sampler (notthe model) is responsible for this failure and propose new samplers, inspiredby MCMC, which enable successful compositional generation. Further, we proposean energy-based parameterization of diffusion models which enables the use ofnew compositional operators and more sophisticated, Metropolis-correctedsamplers. Intriguingly we find these samplers lead to notable improvements incompositional generation across a wide set of problems such asclassifier-guided ImageNet modeling and compositional text-to-image generation.</description><author>Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl</author><pubDate>Mon, 05 Jun 2023 16:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11552v2</guid></item><item><title>XAudit : A Theoretical Look at Auditing with Explanations</title><link>http://arxiv.org/abs/2206.04740v3</link><description>Responsible use of machine learning requires models to be audited forundesirable properties. While a body of work has proposed using explanationsfor auditing, how to do so and why has remained relatively ill-understood. Thiswork formalizes the role of explanations in auditing and investigates if andhow model explanations can help audits. Specifically, we proposeexplanation-based algorithms for auditing linear classifiers and decision treesfor feature sensitivity. Our results illustrate that Counterfactualexplanations are extremely helpful for auditing. While Anchors and decisionpaths may not be as beneficial in the worst-case, in the average-case they doaid a lot.</description><author>Chhavi Yadav, Michal Moshkovitz, Kamalika Chaudhuri</author><pubDate>Mon, 05 Jun 2023 16:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04740v3</guid></item><item><title>Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System</title><link>http://arxiv.org/abs/2306.02972v1</link><description>Speech representation learning with self-supervised algorithms has resultedin notable performance boosts in many downstream tasks. Recent work combinedself-supervised learning (SSL) and visually grounded speech (VGS) processingmechanisms for representation learning. The joint training with SSL and VGSmechanisms provides the opportunity to utilize both unlabeled speech andspeech-related visual information based on data availability. This has shown toenhance the quality of learned representations, especially at encodingsemantic- and lexical-level knowledge. In this work, we further study the jointoptimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-tasklearning system. We explore a set of training scenarios to understand howspeech representations are shared or transferred between the two tasks, andwhat is the optimal training strategy for cross-modal semantic retrieval andphoneme discrimination performance. As a result, we find that sequentialtraining with wav2vec 2.0 first and VGS next provides higher performance onaudio-visual retrieval compared to simultaneous optimization of both learningmechanisms. However, the parallel SSL-VGS training reduces the effects ofcatastrophic forgetting when switching between optimization criteria. Moreover,the results suggest that phonemic representations learned through the VGSmechanism may generalize better across datasets compared to those learned withSSL.</description><author>Khazar Khorrami, MarÃ­a Andrea Cruz BlandÃ³n, Tuomas Virtanen, Okko RÃ¤sÃ¤nen</author><pubDate>Mon, 05 Jun 2023 16:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02972v1</guid></item><item><title>Online Learning with Feedback Graphs: The True Shape of Regret</title><link>http://arxiv.org/abs/2306.02971v1</link><description>Sequential learning with feedback graphs is a natural extension of themulti-armed bandit problem where the problem is equipped with an underlyinggraph structure that provides additional information - playing an actionreveals the losses of all the neighbors of the action. This problem wasintroduced by \citet{mannor2011} and received considerable attention in recentyears. It is generally stated in the literature that the minimax regret ratefor this problem is of order $\sqrt{\alpha T}$, where $\alpha$ is theindependence number of the graph, and $T$ is the time horizon. However, this isproven only when the number of rounds $T$ is larger than $\alpha^3$, whichposes a significant restriction for the usability of this result in largegraphs. In this paper, we define a new quantity $R^*$, called the \emph{problemcomplexity}, and prove that the minimax regret is proportional to $R^*$ for anygraph and time horizon $T$. Introducing an intricate exploration strategy, wedefine the \mainAlgorithm algorithm that achieves the minimax optimal regretbound and becomes the first provably optimal algorithm for this setting, evenif $T$ is smaller than $\alpha^3$.</description><author>TomÃ¡Å¡ KocÃ¡k, Alexandra Carpentier</author><pubDate>Mon, 05 Jun 2023 16:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02971v1</guid></item><item><title>Robust incremental learning pipelines for temporal tabular datasets with distribution shifts</title><link>http://arxiv.org/abs/2303.07925v5</link><description>In this paper, we present a robust incremental learning model for regressiontasks on temporal tabular datasets. Using commonly available tabular andtime-series prediction models as building blocks, a machine-learning model isbuilt incrementally to adapt to distributional shifts in data. Using theconcept of self-similarity, the model uses only two basic building blocks ofmachine learning models, gradient boosting decision trees and neural networksto build models for any required complexity. The model is efficient as nospecialised neural architectures are used and each model building block can beindependently trained in parallel. The model is demonstrated to have robustperformances under adverse situations such as regime changes, fat-taileddistributions and low signal-to-noise ratios. Model robustness are studiedunder different hyper-parameters and complexities.</description><author>Thomas Wong, Mauricio Barahona</author><pubDate>Mon, 05 Jun 2023 16:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07925v5</guid></item><item><title>\textit{spred}: Solving $L_1$ Penalty with SGD</title><link>http://arxiv.org/abs/2210.01212v3</link><description>We propose to minimize a generic differentiable objective with $L_1$constraint using a simple reparametrization and straightforward stochasticgradient descent. Our proposal is the direct generalization of previous ideasthat the $L_1$ penalty may be equivalent to a differentiable reparametrizationwith weight decay. We prove that the proposed method, \textit{spred}, is anexact differentiable solver of $L_1$ and that the reparametrization trick iscompletely ``benign" for a generic nonconvex function. Practically, wedemonstrate the usefulness of the method in (1) training sparse neural networksto perform gene selection tasks, which involves finding relevant features in avery high dimensional space, and (2) neural network compression task, to whichprevious attempts at applying the $L_1$-penalty have been unsuccessful.Conceptually, our result bridges the gap between the sparsity in deep learningand conventional statistical learning.</description><author>Liu Ziyin, Zihao Wang</author><pubDate>Mon, 05 Jun 2023 16:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01212v3</guid></item><item><title>Time Interpret: a Unified Model Interpretability Library for Time Series</title><link>http://arxiv.org/abs/2306.02968v1</link><description>We introduce $\texttt{time_interpret}$, a library designed as an extension ofCaptum, with a specific focus on temporal data. As such, this libraryimplements several feature attribution methods that can be used to explainpredictions made by any Pytorch model. $\texttt{time_interpret}$ also providesseveral synthetic and real world time series datasets, various PyTorch models,as well as a set of methods to evaluate feature attributions. Moreover, whilebeing primarily developed to explain predictions based on temporal data, someof its components have a different application, including for instance methodsexplaining predictions made by language models. In this paper, we give ageneral introduction of this library. We also present several previouslyunpublished feature attribution methods, which have been developed along with$\texttt{time_interpret}$.</description><author>Joseph Enguehard</author><pubDate>Mon, 05 Jun 2023 16:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02968v1</guid></item><item><title>MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation</title><link>http://arxiv.org/abs/2302.09048v2</link><description>This work introduces MiDi, a novel diffusion model for jointly generatingmolecular graphs and their corresponding 3D arrangement of atoms. Unlikeexisting methods that rely on predefined rules to determine molecular bondsbased on the 3D conformation, MiDi offers an end-to-end differentiable approachthat streamlines the molecule generation process. Our experimental resultsdemonstrate the effectiveness of this approach. On the challenging GEOM-DRUGSdataset, MiDi generates 92% of stable molecules, against 6% for the previousEDM model that uses interatomic distances for bond prediction, and 40% usingEDM followed by an algorithm that directly optimize bond orders for validity.Our code is available at github.com/cvignac/MiDi.</description><author>Clement Vignac, Nagham Osman, Laura Toni, Pascal Frossard</author><pubDate>Mon, 05 Jun 2023 16:26:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09048v2</guid></item><item><title>Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation</title><link>http://arxiv.org/abs/2306.02960v1</link><description>Event-based cameras offer a low-power alternative to frame-based cameras forcapturing high-speed motion and high dynamic range scenes. They provideasynchronous streams of sparse events. Spiking Neural Networks (SNNs) withtheir asynchronous event-driven compute, show great potential for extractingthe spatio-temporal features from these event streams. In contrast, thestandard Analog Neural Networks (ANNs1) fail to process event data effectively.However, training SNNs is difficult due to additional trainable parameters(thresholds and leaks), vanishing spikes at deeper layers, non-differentiablebinary activation function etc. Moreover, an additional data structure"membrane potential" responsible for keeping track of temporal information,must be fetched and updated at every timestep in SNNs. To overcome these, wepropose a novel SNN-ANN hybrid architecture that combines the strengths ofboth. Specifically, we leverage the asynchronous compute capabilities of SNNlayers to effectively extract the input temporal information. While the ANNlayers offer trouble-free training and implementation on standard machinelearning hardware such as GPUs. We provide extensive experimental analysis forassigning each layer to be spiking or analog in nature, leading to a networkconfiguration optimized for performance and ease of training. We evaluate ourhybrid architectures for optical flow estimation using event-data on DSEC-flowand Mutli-Vehicle Stereo Event-Camera (MVSEC) datasets. The results indicatethat our configured hybrid architectures outperform the state-of-the-artANN-only, SNN-only and past hybrid architectures both in terms of accuracy andefficiency. Specifically, our hybrid architecture exhibit a 31% and 24.8% loweraverage endpoint error (AEE) at 2.1x and 3.1x lower energy, compared to anSNN-only architecture on DSEC and MVSEC datasets, respectively.</description><author>Shubham Negi, Deepika Sharma, Adarsh Kumar Kosta, Kaushik Roy</author><pubDate>Mon, 05 Jun 2023 16:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02960v1</guid></item><item><title>Complex Preferences for Different Convergent Priors in Discrete Graph Diffusion</title><link>http://arxiv.org/abs/2306.02957v1</link><description>Diffusion models have achieved state-of-the-art performance in generatingmany different kinds of data, including images, text, and videos. Despite theirsuccess, there has been limited research on how the underlying diffusionprocess and the final convergent prior can affect generative performance; thisresearch has also been limited to continuous data types and a score-baseddiffusion framework. To fill this gap, we explore how different discretediffusion kernels (which converge to different prior distributions) affect theperformance of diffusion models for graphs. To this end, we developed a novelformulation of a family of discrete diffusion kernels which are easilyadjustable to converge to different Bernoulli priors, and we study the effectof these different kernels on generative performance. We show that the qualityof generated graphs is sensitive to the prior used, and that the optimal choicecannot be explained by obvious statistics or metrics, which challenges theintuitions which previous works have suggested.</description><author>Alex M. Tseng, Nathaniel Diamant, Tommaso Biancalani, Gabriele Scalia</author><pubDate>Mon, 05 Jun 2023 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02957v1</guid></item><item><title>Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields</title><link>http://arxiv.org/abs/2306.02956v1</link><description>We introduce Explicit Neural Surfaces (ENS), an efficient surfacereconstruction method that learns an explicitly defined continuous surface frommultiple views. We use a series of neural deformation fields to progressivelytransform a continuous input surface to a target shape. By sampling meshes asdiscrete surface proxies, we train the deformation fields through efficientdifferentiable rasterization, and attain a mesh-independent and smooth surfacerepresentation. By using Laplace-Beltrami eigenfunctions as an intrinsicpositional encoding alongside standard extrinsic Fourier features, our approachcan capture fine surface details. ENS trains 1 to 2 orders of magnitude fasterand can extract meshes of higher quality compared to implicit representations,whilst maintaining competitive surface reconstruction performance and real-timecapabilities. Finally, we apply our approach to learn a collection of objectsin a single model, and achieve disentangled interpolations between differentshapes, their surface details, and textures.</description><author>Thomas Walker, Octave Mariotti, Amir Vaxman, Hakan Bilen</author><pubDate>Mon, 05 Jun 2023 16:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02956v1</guid></item><item><title>A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires</title><link>http://arxiv.org/abs/2306.02955v1</link><description>Social media is one of the most highly sought resources for analyzingcharacteristics of the language by its users. In particular, many researchersutilized various linguistic features of mental health problems from socialmedia. However, existing approaches to detecting mental disorders face criticalchallenges, such as the scarcity of high-quality data or the trade-off betweenaddressing the complexity of models and presenting interpretable resultsgrounded in expert domain knowledge. To address these challenges, we design asimple but flexible model that preserves domain-based interpretability. Wepropose a novel approach that captures the semantic meanings directly from thetext and compares them to symptom-related descriptions. Experimental resultsdemonstrate that our model outperforms relevant baselines on various mentaldisorder detection tasks. Our detailed analysis shows that the proposed modelis effective at leveraging domain knowledge, transferable to other mentaldisorders, and providing interpretable detection results.</description><author>Hoyun Song, Jisu Shin, Huije Lee, Jong C. Park</author><pubDate>Mon, 05 Jun 2023 16:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02955v1</guid></item><item><title>Exploring the Connection between Robust and Generative Models</title><link>http://arxiv.org/abs/2304.04033v4</link><description>We offer a study that connects robust discriminative classifiers trained withadversarial training (AT) with generative modeling in the form of Energy-basedModels (EBM). We do so by decomposing the loss of a discriminative classifierand showing that the discriminative model is also aware of the input datadensity. Though a common assumption is that adversarial points leave themanifold of the input data, our study finds out that, surprisingly, untargetedadversarial points in the input space are very likely under the generativemodel hidden inside the discriminative classifier -- have low energy in theEBM. We present two evidence: untargeted attacks are even more likely than thenatural data and their likelihood increases as the attack strength increases.This allows us to easily detect them and craft a novel attack calledHigh-Energy PGD that fools the classifier yet has energy similar to the dataset. The code is available at github.com/senad96/Robust-Generative</description><author>Senad Beadini, Iacopo Masi</author><pubDate>Mon, 05 Jun 2023 16:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04033v4</guid></item><item><title>Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions</title><link>http://arxiv.org/abs/2208.05844v2</link><description>We study the problem of adaptively identifying patient subpopulations thatbenefit from a given treatment during a confirmatory clinical trial. This typeof adaptive clinical trial has been thoroughly studied in biostatistics, buthas been allowed only limited adaptivity so far. Here, we aim to relaxclassical restrictions on such designs and investigate how to incorporate ideasfrom the recent machine learning literature on adaptive and onlineexperimentation to make trials more flexible and efficient. We find that theunique characteristics of the subpopulation selection problem -- mostimportantly that (i) one is usually interested in finding subpopulations withany treatment benefit (and not necessarily the single subgroup with largesteffect) given a limited budget and that (ii) effectiveness only has to bedemonstrated across the subpopulation on average -- give rise to interestingchallenges and new desiderata when designing algorithmic solutions. Building onthese findings, we propose AdaGGI and AdaGCPI, two meta-algorithms forsubpopulation construction. We empirically investigate their performance acrossa range of simulation scenarios and derive insights into their (dis)advantagesacross different settings.</description><author>Alicia Curth, Alihan HÃ¼yÃ¼k, Mihaela van der Schaar</author><pubDate>Mon, 05 Jun 2023 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.05844v2</guid></item><item><title>Color-aware Deep Temporal Backdrop Duplex Matting System</title><link>http://arxiv.org/abs/2306.02954v1</link><description>Deep learning-based alpha matting showed tremendous improvements in recentyears, yet, feature film production studios still rely on classical chromakeying including costly post-production steps. This perceived discrepancy canbe explained by some missing links necessary for production which are currentlynot adequately addressed in the alpha matting community, in particularforeground color estimation or color spill compensation. We propose a neuralnetwork-based temporal multi-backdrop production system that combinesbeneficial features from chroma keying and alpha matting. Given two consecutiveframes with different background colors, our one-encoder-dual-decoder networkpredicts foreground colors and alpha values using a patch-based overlap-blendapproach. The system is able to handle imprecise backdrops, dynamic cameras,and dynamic foregrounds and has no restrictions on foreground colors. Wecompare our method to state-of-the-art algorithms using benchmark datasets anda video sequence captured by a demonstrator setup. We verify that a dualbackdrop input is superior to the usually applied trimap-based approach. Inaddition, the proposed studio set is actor friendly, and produces high-quality,temporal consistent alpha and color estimations that include a superior colorspill compensation.</description><author>Hendrik Hachmann, Bodo Rosenhahn</author><pubDate>Mon, 05 Jun 2023 16:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02954v1</guid></item><item><title>INDigo: An INN-Guided Probabilistic Diffusion Algorithm for Inverse Problems</title><link>http://arxiv.org/abs/2306.02949v1</link><description>Recently it has been shown that using diffusion models for inverse problemscan lead to remarkable results. However, these approaches require a closed-formexpression of the degradation model and can not support complex degradations.To overcome this limitation, we propose a method (INDigo) that combinesinvertible neural networks (INN) and diffusion models for general inverseproblems. Specifically, we train the forward process of INN to simulate anarbitrary degradation process and use the inverse as a reconstruction process.During the diffusion sampling process, we impose an additional data-consistencystep that minimizes the distance between the intermediate result and theINN-optimized result at every iteration, where the INN-optimized image iscomposed of the coarse information given by the observed degraded image and thedetails generated by the diffusion process. With the help of INN, our algorithmeffectively estimates the details lost in the degradation process and is nolonger limited by the requirement of knowing the closed-form expression of thedegradation model. Experiments demonstrate that our algorithm obtainscompetitive results compared with recently leading methods both quantitativelyand visually. Moreover, our algorithm performs well on more complex degradationmodels and real-world low-quality images.</description><author>Di You, Andreas Floros, Pier Luigi Dragotti</author><pubDate>Mon, 05 Jun 2023 16:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02949v1</guid></item><item><title>Random Distribution Shift in Refugee Placement: Strategies for Building Robust Models</title><link>http://arxiv.org/abs/2306.02948v1</link><description>Algorithmic assignment of refugees and asylum seekers to locations withinhost countries has gained attention in recent years, with implementations inthe US and Switzerland. These approaches use data on past arrivals to generatemachine learning models that can be used (along with assignment algorithms) tomatch families to locations, with the goal of maximizing a policy-relevantintegration outcome such as employment status after a certain duration.Existing implementations and research train models to predict the policyoutcome directly, and use these predictions in the assignment procedure.However, the merits of this approach, particularly in non-stationary settings,has not been previously explored. This study proposes and compares threedifferent modeling strategies: the standard approach described above, anapproach that uses newer data and proxy outcomes, and a hybrid approach. Weshow that the hybrid approach is robust to both distribution shift and weakproxy relationships -- the failure points of the other two methods,respectively. We compare these approaches empirically using data on asylumseekers in the Netherlands. Surprisingly, we find that both the proxy andhybrid approaches out-perform the standard approach in practice. These insightssupport the development of a real-world recommendation tool currently used byNGOs and government agencies.</description><author>Kirk Bansak, Elisabeth Paulson, Dominik RothenhÃ¤usler</author><pubDate>Mon, 05 Jun 2023 16:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02948v1</guid></item><item><title>Continual Learning with Pretrained Backbones by Tuning in the Input Space</title><link>http://arxiv.org/abs/2306.02947v1</link><description>The intrinsic difficulty in adapting deep learning models to non-stationaryenvironments limits the applicability of neural networks to real-world tasks.This issue is critical in practical supervised learning settings, such as theones in which a pre-trained model computes projections toward a latent spacewhere different task predictors are sequentially learned over time. As a matterof fact, incrementally fine-tuning the whole model to better adapt to new tasksusually results in catastrophic forgetting, with decreasing performance overthe past experiences and losing valuable knowledge from the pre-training stage.In this paper, we propose a novel strategy to make the fine-tuning proceduremore effective, by avoiding to update the pre-trained part of the network andlearning not only the usual classification head, but also a set ofnewly-introduced learnable parameters that are responsible for transforming theinput data. This process allows the network to effectively leverage thepre-training knowledge and find a good trade-off between plasticity andstability with modest computational efforts, thus especially suitable foron-the-edge settings. Our experiments on four image classification problems ina continual learning setting confirm the quality of the proposed approach whencompared to several fine-tuning procedures and to popular continual learningmethods.</description><author>Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci, Tinne Tuytelaars</author><pubDate>Mon, 05 Jun 2023 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02947v1</guid></item><item><title>Linking generative semi-supervised learning and generative open-set recognition</title><link>http://arxiv.org/abs/2303.11702v2</link><description>This study investigates the relationship between semi-supervised learning(SSL) and open-set recognition (OSR) in the context of generative adversarialnetworks (GANs). Although no previous study has formally linked SSL and OSR,their respective methods share striking similarities. Specifically, SSL-GANsand OSR-GANs require their generators to produce samples in the complementaryspace. Subsequently, by regularising networks with generated samples, both SSLand OSR classifiers generalize the open space. To demonstrate the connectionbetween SSL and OSR, we theoretically and experimentally comparestate-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Ourresults indicate that the SSL optimised margin-GANs, which have a strongerfoundation in literature, set the new standard for the combined SSL-OSR taskand achieves new state-of-other art results in certain general OSR experiments.However, the OSR optimised adversarial reciprocal point (ARP)-GANs stillslightly out-performed margin-GANs at other OSR experiments. This resultindicates unique insights for the combined optimisation task of SSL-OSR.</description><author>Emile Reyn Engelbrecht, Johan du Preez</author><pubDate>Mon, 05 Jun 2023 16:11:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11702v2</guid></item><item><title>Tutel: Adaptive Mixture-of-Experts at Scale</title><link>http://arxiv.org/abs/2206.03382v2</link><description>Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deeplearning models to trillion-plus parameters with fixed computational cost. Thealgorithmic performance of MoE relies on its token routing mechanism thatforwards each input token to the right sub-models or experts. While tokenrouting dynamically determines the amount of expert workload at runtime,existing systems suffer inefficient computation due to their static execution,namely static parallelism and pipelining, which does not adapt to the dynamicworkload. We present Flex, a highly scalable stack design and implementationfor MoE with dynamically adaptive parallelism and pipelining. Flex designs anidentical layout for distributing MoE model parameters and input data, whichcan be leveraged by all possible parallelism or pipelining methods without anymathematical inequivalence or tensor migration overhead. This enables adaptiveparallelism/pipelining optimization at zero cost during runtime. Based on thiskey design, Flex also implements various MoE acceleration techniques.Aggregating all techniques, Flex finally delivers huge speedup at any scale --4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs,respectively, over the previous state-of-the-art. Our evaluation shows thatFlex efficiently and effectively runs a real-world MoE-based model namedSwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer visionarchitecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55xand 2.11x speedup in training and inference over Fairseq, respectively. Oneffectiveness, the SwinV2-MoE model achieves superior accuracy in bothpre-training and down-stream computer vision tasks such as COCO objectdetection than the counterpart dense model, indicating the readiness of Flexfor end-to-end real-world model training and inference.</description><author>Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong</author><pubDate>Mon, 05 Jun 2023 16:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03382v2</guid></item><item><title>Deep Weakly-supervised Anomaly Detection</title><link>http://arxiv.org/abs/1910.13601v4</link><description>Recent semi-supervised anomaly detection methods that are trained using smalllabeled anomaly examples and large unlabeled data (mostly normal data) haveshown largely improved performance over unsupervised methods. However, thesemethods often focus on fitting abnormalities illustrated by the given anomalyexamples only (i.e.,, seen anomalies), and consequently they fail to generalizeto those that are not, i.e., new types/classes of anomaly unseen duringtraining. To detect both seen and unseen anomalies, we introduce a novel deepweakly-supervised approach, namely Pairwise Relation prediction Network(PReNet), that learns pairwise relation features and anomaly scores bypredicting the relation of any two randomly sampled training instances, inwhich the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, orunlabeled-unlabeled. Since unlabeled instances are mostly normal, the relationprediction enforces a joint learning of anomaly-anomaly, anomaly-normal, andnormal-normal pairwise discriminative patterns, respectively. PReNet can thendetect any seen/unseen abnormalities that fit the learned pairwise abnormalpatterns, or deviate from the normal patterns. Further, this pairwise approachalso seamlessly and significantly augments the training anomaly data. Empiricalresults on 12 real-world datasets show that PReNet significantly outperformsnine competing methods in detecting seen and unseen anomalies. We alsotheoretically and empirically justify the robustness of our model w.r.t.anomaly contamination in the unlabeled data. The code is available athttps://github.com/mala-lab/PReNet.</description><author>Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel</author><pubDate>Mon, 05 Jun 2023 16:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.13601v4</guid></item><item><title>Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm</title><link>http://arxiv.org/abs/2306.02939v1</link><description>This paper presents a new generalization error analysis for the DecentralizedStochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability.The obtained results largely improve upon state-of-the-art results, and eveninvalidate their claims that the communication graph has a detrimental effecton generalization. For instance, we show that in convex settings, D-SGD has thesame generalization bounds as the classical SGD algorithm, no matter the choiceof graph. We exhibit that this counter-intuitive result comes from consideringthe average of local parameters, which hides a final global averaging stepincompatible with the decentralized scenario. In light of this observation, weadvocate to analyze the supremum over local parameters and show that in thiscase, the graph does have an impact on the generalization. Unlike priorresults, our analysis yields non-vacuous bounds even for non-connected graphs.</description><author>Batiste Le Bars, AurÃ©lien Bellet, Marc Tommasi</author><pubDate>Mon, 05 Jun 2023 16:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02939v1</guid></item><item><title>Topology Optimization via Machine Learning and Deep Learning: A Review</title><link>http://arxiv.org/abs/2210.10782v2</link><description>Topology optimization (TO) is a method of deriving an optimal design thatsatisfies a given load and boundary conditions within a design domain. Thismethod enables effective design without initial design, but has been limited inuse due to high computational costs. At the same time, machine learning (ML)methodology including deep learning has made great progress in the 21stcentury, and accordingly, many studies have been conducted to enable effectiveand rapid optimization by applying ML to TO. Therefore, this study reviews andanalyzes previous research on ML-based TO (MLTO). Two different perspectives ofMLTO are used to review studies: (1) TO and (2) ML perspectives. The TOperspective addresses "why" to use ML for TO, while the ML perspectiveaddresses "how" to apply ML to TO. In addition, the limitations of current MLTOresearch and future research directions are examined.</description><author>Seungyeon Shin, Dongju Shin, Namwoo Kang</author><pubDate>Mon, 05 Jun 2023 16:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10782v2</guid></item><item><title>Deep surrogate accelerated delayed-acceptance HMC for Bayesian inference of spatio-temporal heat fluxes in rotating disc systems</title><link>http://arxiv.org/abs/2204.02272v2</link><description>We introduce a deep learning accelerated methodology to solve PDE-basedBayesian inverse problems with guaranteed accuracy. This is motivated by theill-posed problem of inferring a spatio-temporal heat-flux parameter known asthe Biot number given temperature data, however the methodology isgeneralisable to other settings. To accelerate Bayesian inference, we develop anovel training scheme that uses data to adaptively train a neural-networksurrogate simulating the parametric forward model. By simultaneouslyidentifying an approximate posterior distribution over the Biot number, andweighting a physics-informed training loss according to this, our approachapproximates forward and inverse solution together without any need forexternal solves. Using a random Chebyshev series, we outline how to approximatea Gaussian process prior, and using the surrogate we apply Hamiltonian MonteCarlo (HMC) to sample from the posterior distribution. We derive convergence ofthe surrogate posterior to the true posterior distribution in the Hellingermetric as our adaptive loss approaches zero. Additionally, we describe how thissurrogate-accelerated HMC approach can be combined with traditional PDE solversin a delayed-acceptance scheme to a-priori control the posterior accuracy. Thisovercomes a major limitation of deep learning-based surrogate approaches, whichdo not achieve guaranteed accuracy a-priori due to their non-convex training.Biot number calculations are involved in turbo-machinery design, which issafety critical and highly regulated, therefore it is important that ourresults have such mathematical guarantees. Our approach achieves fast mixing inhigh dimensions whilst retaining the convergence guarantees of a traditionalPDE solver, and without the burden of evaluating this solver for proposals thatare likely to be rejected. Numerical results are given using real and simulateddata.</description><author>Teo Deveney, Eike Mueller, Tony Shardlow</author><pubDate>Mon, 05 Jun 2023 15:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.02272v2</guid></item><item><title>Linkless Link Prediction via Relational Distillation</title><link>http://arxiv.org/abs/2210.05801v3</link><description>Graph Neural Networks (GNNs) have shown exceptional performance in the taskof link prediction. Despite their effectiveness, the high latency brought bynon-trivial neighborhood data dependency limits GNNs in practical deployments.Conversely, the known efficient MLPs are much less effective than GNNs due tothe lack of relational knowledge. In this work, to combine the advantages ofGNNs and MLPs, we start with exploring direct knowledge distillation (KD)methods for link prediction, i.e., predicted logit-based matching and noderepresentation-based matching. Upon observing direct KD analogs do not performwell for link prediction, we propose a relational KD framework, Linkless LinkPrediction (LLP), to distill knowledge for link prediction with MLPs. Unlikesimple KD methods that match independent link logits or node representations,LLP distills relational knowledge that is centered around each (anchor) node tothe student MLP. Specifically, we propose rank-based matching anddistribution-based matching strategies that complement each other. Extensiveexperiments demonstrate that LLP boosts the link prediction performance of MLPswith significant margins, and even outperforms the teacher GNNs on 7 out of 8benchmarks. LLP also achieves a 70.68x speedup in link prediction inferencecompared to GNNs on the large-scale OGB dataset.</description><author>Zhichun Guo, William Shiao, Shichang Zhang, Yozen Liu, Nitesh V. Chawla, Neil Shah, Tong Zhao</author><pubDate>Mon, 05 Jun 2023 15:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05801v3</guid></item><item><title>Causal Discovery using Bayesian Model Selection</title><link>http://arxiv.org/abs/2306.02931v1</link><description>With only observational data on two variables, and without other assumptions,it is not possible to infer which one causes the other. Much of the causalliterature has focused on guaranteeing identifiability of causal direction instatistical models for datasets where strong assumptions hold, such as additivenoise or restrictions on parameter count. These methods are then subsequentlytested on realistic datasets, most of which violate their assumptions. Buildingon previous attempts, we show how to use causal assumptions within the Bayesianframework. This allows us to specify models with realistic assumptions, whilealso encoding independent causal mechanisms, leading to an asymmetry betweenthe causal directions. Identifying causal direction then becomes a Bayesianmodel selection problem. We analyse why Bayesian model selection works forknown identifiable cases and flexible model classes, while also providingcorrectness guarantees about its behaviour. To demonstrate our approach, weconstruct a Bayesian non-parametric model that can flexibly model the joint. Wethen outperform previous methods on a wide range of benchmark datasets withvarying data generating assumptions showing the usefulness of our method.</description><author>Anish Dhir, Mark van der Wilk</author><pubDate>Mon, 05 Jun 2023 15:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02931v1</guid></item><item><title>Human Spine Motion Capture using Perforated Kinesiology Tape</title><link>http://arxiv.org/abs/2306.02930v1</link><description>In this work, we present a marker-based multi-view spine tracking method thatis specifically adjusted to the requirements for movements in sports. A maximalfocus is on the accurate detection of markers and fast usage of the system. Forthis task, we take advantage of the prior knowledge of the arrangement of dotsin perforated kinesiology tape. We detect the tape and its dots using a MaskR-CNN and a blob detector. Here, we can focus on detection only while skippingany image-based feature encoding or matching. We conduct a reasoning in 3D by alinear program and Markov random fields, in which the structure of thekinesiology tape is modeled and the shape of the spine is optimized. Incomparison to state-of-the-art systems, we demonstrate that our system achieveshigh precision and marker density, is robust against occlusions, and capable ofcapturing fast movements.</description><author>Hendrik Hachmann, Bodo Rosenhahn</author><pubDate>Mon, 05 Jun 2023 15:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02930v1</guid></item><item><title>Weakly-Supervised Conditional Embedding for Referred Visual Search</title><link>http://arxiv.org/abs/2306.02928v1</link><description>This paper presents a new approach to image similarity search in the contextof fashion, a domain with inherent ambiguity due to the multiple ways in whichimages can be considered similar. We introduce the concept of Referred VisualSearch (RVS), where users provide additional information to define the desiredsimilarity. We present a new dataset, LAION-RVS-Fashion, consisting of 272Kfashion products with 842K images extracted from LAION, designed explicitly forthis task. We then propose an innovative method for learning conditionalembeddings using weakly-supervised training, achieving a 6% increase in Recallat one (R@1) against a gallery with 2M distractors, compared to classicalapproaches based on explicit attention and filtering. The proposed methoddemonstrates robustness, maintaining similar R@1 when dealing with 2.5 times asmany distractors as the baseline methods. We believe this is a step forward inthe emerging field of Referred Visual Search both in terms of accessible dataand approach. Code, data and models are available athttps://www.github.com/Simon-Lepage/CondViT-LRVSF .</description><author>Simon Lepage, JÃ©rÃ©mie Mary, David Picard</author><pubDate>Mon, 05 Jun 2023 15:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02928v1</guid></item><item><title>MidMed: Towards Mixed-Type Dialogues for Medical Consultation</title><link>http://arxiv.org/abs/2306.02923v1</link><description>Most medical dialogue systems assume that patients have clear goals (medicinequerying, surgical operation querying, etc.) before medical consultation.However, in many real scenarios, due to the lack of medical knowledge, it isusually difficult for patients to determine clear goals with all necessaryslots. In this paper, we identify this challenge as how to construct medicalconsultation dialogue systems to help patients clarify their goals. To mitigatethis challenge, we propose a novel task and create a human-to-human mixed-typemedical consultation dialogue corpus, termed MidMed, covering five dialoguetypes: task-oriented dialogue for diagnosis, recommendation, knowledge-groundeddialogue, QA, and chitchat. MidMed covers four departments(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175dialogues. Furthermore, we build baselines on MidMed and propose aninstruction-guiding medical dialogue generation framework, termed InsMed, toaddress this task. Experimental results show the effectiveness of InsMed.</description><author>Xiaoming Shi, Zeming Liu, Chuan Wang, Haitao Leng, Kui Xue, Xiaofan Zhang, Shaoting Zhang</author><pubDate>Mon, 05 Jun 2023 15:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02923v1</guid></item><item><title>MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning</title><link>http://arxiv.org/abs/2210.04183v2</link><description>Multimodal representation learning has shown promising improvements onvarious vision-language tasks. Most existing methods excel at buildingglobal-level alignment between vision and language while lacking effectivefine-grained image-text interaction. In this paper, we propose a jointly maskedmultimodal modeling method to learn fine-grained multimodal representations.Our method performs joint masking on image-text input and integrates bothimplicit and explicit targets for the masked signals to recover. The implicittarget provides a unified and debiased objective for vision and language, wherethe model predicts latent multimodal representations of the unmasked input. Theexplicit target further enriches the multimodal representations by recoveringhigh-level and semantically meaningful information: momentum visual features ofimage patches and concepts of word tokens. Through such a masked modelingprocess, our model not only learns fine-grained multimodal interaction, butalso avoids the semantic gap between high-level representations and low- ormid-level prediction targets (e.g. image pixels), thus producing semanticallyrich multimodal representations that perform well on both zero-shot andfine-tuned settings. Our pre-trained model (named MAMO) achievesstate-of-the-art performance on various downstream vision-language tasks,including image-text retrieval, visual question answering, visual reasoning,and weakly-supervised visual grounding.</description><author>Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu</author><pubDate>Mon, 05 Jun 2023 15:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.04183v2</guid></item><item><title>Zero shot framework for satellite image restoration</title><link>http://arxiv.org/abs/2306.02921v1</link><description>Satellite images are typically subject to multiple distortions. Differentfactors affect the quality of satellite images, including changes inatmosphere, surface reflectance, sun illumination, viewing geometries etc.,limiting its application to downstream tasks. In supervised networks, theavailability of paired datasets is a strong assumption. Consequently, manyunsupervised algorithms have been proposed to address this problem. Thesemethods synthetically generate a large dataset of degraded images using imageformation models. A neural network is then trained with an adversarial loss todiscriminate between images from distorted and clean domains. However, thesemethods yield suboptimal performance when tested on real images that do notnecessarily conform to the generation mechanism. Also, they require a largeamount of training data and are rendered unsuitable when only a few images areavailable. We propose a distortion disentanglement and knowledge distillationframework for satellite image restoration to address these important issues.Our algorithm requires only two images: the distorted satellite image to berestored and a reference image with similar semantics. Specifically, we firstpropose a mechanism to disentangle distortion. This enables us to generateimages with varying degrees of distortion using the disentangled distortion andthe reference image. We then propose the use of knowledge distillation to traina restoration network using the generated image pairs. As a final step, thedistorted image is passed through the restoration network to get the finaloutput. Ablation studies show that our proposed mechanism successfullydisentangles distortion.</description><author>Praveen Kandula, A. N. Rajagopalan</author><pubDate>Mon, 05 Jun 2023 15:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02921v1</guid></item><item><title>Second Language Acquisition of Neural Language Models</title><link>http://arxiv.org/abs/2306.02920v1</link><description>With the success of neural language models (LMs), their language acquisitionhas gained much attention. This work sheds light on the second language (L2)acquisition of LMs, while previous work has typically explored their firstlanguage (L1) acquisition. Specifically, we trained bilingual LMs with ascenario similar to human L2 acquisition and analyzed their cross-lingualtransfer from linguistic perspectives. Our exploratory experiments demonstratedthat the L1 pretraining accelerated their linguistic generalization in L2, andlanguage transfer configurations (e.g., the L1 choice, and presence of paralleltexts) substantially affected their generalizations. These clarify their(non-)human-like L2 acquisition in particular aspects.</description><author>Miyu Oba, Tatsuki Kuribayashi, Hiroki Ouchi, Taro Watanabe</author><pubDate>Mon, 05 Jun 2023 15:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02920v1</guid></item></channel></rss>