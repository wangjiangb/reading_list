<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 24 May 2023 06:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</title><link>http://arxiv.org/abs/2305.14345v1</link><description>Deep generative models have been recently extended to synthesizing 3D digitalhumans. However, previous approaches treat clothed humans as a single chunk ofgeometry without considering the compositionality of clothing and accessories.As a result, individual items cannot be naturally composed into novelidentities, leading to limited expressiveness and controllability of generative3D avatars. While several methods attempt to address this by leveragingsynthetic data, the interaction between humans and objects is not authentic dueto the domain gap, and manual asset creation is difficult to scale for a widevariety of objects. In this work, we present a novel framework for learning acompositional generative model of humans and objects (backpacks, coats,scarves, and more) from real-world 3D scans. Our compositional model isinteraction-aware, meaning the spatial relationship between humans and objects,and the mutual shape change by physical contact is fully incorporated. The keychallenge is that, since humans and objects are in contact, their 3D scans aremerged into a single piece. To decompose them without manual annotations, wepropose to leverage two sets of 3D scans of a single person with and withoutobjects. Our approach learns to decompose objects and naturally compose themback into a generative human model in an unsupervised manner. Despite oursimple setup requiring only the capture of a single subject with objects, ourexperiments demonstrate the strong generalization of our model by enabling thenatural composition of objects to diverse identities in various poses and thecomposition of multiple objects, which is unseen in training data.</description><author>Taeksoo Kim, Shunsuke Saito, Hanbyul Joo</author><pubDate>Tue, 23 May 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14345v1</guid></item><item><title>Siamese Masked Autoencoders</title><link>http://arxiv.org/abs/2305.14344v1</link><description>Establishing correspondence between images or scenes is a significantchallenge in computer vision, especially given occlusions, viewpoint changes,and varying object appearances. In this paper, we present Siamese MaskedAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) forlearning visual correspondence from videos. SiamMAE operates on pairs ofrandomly sampled video frames and asymmetrically masks them. These frames areprocessed independently by an encoder network, and a decoder composed of asequence of cross-attention layers is tasked with predicting the missingpatches in the future frame. By masking a large fraction ($95\%$) of patches inthe future frame while leaving the past frame unchanged, SiamMAE encourages thenetwork to focus on object motion and learn object-centric representations.Despite its conceptual simplicity, features learned via SiamMAE outperformstate-of-the-art self-supervised methods on video object segmentation, posekeypoint propagation, and semantic part propagation tasks. SiamMAE achievescompetitive results without relying on data augmentation, handcraftedtracking-based pretext tasks, or other techniques to prevent representationalcollapse.</description><author>Agrim Gupta, Jiajun Wu, Jia Deng, Li Fei-Fei</author><pubDate>Tue, 23 May 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14344v1</guid></item><item><title>Video Prediction Models as Rewards for Reinforcement Learning</title><link>http://arxiv.org/abs/2305.14343v1</link><description>Specifying reward signals that allow agents to learn complex behaviors is along-standing challenge in reinforcement learning. A promising approach is toextract preferences for behaviors from unlabeled videos, which are widelyavailable on the internet. We present Video Prediction Rewards (VIPER), analgorithm that leverages pretrained video prediction models as action-freereward signals for reinforcement learning. Specifically, we first train anautoregressive transformer on expert videos and then use the video predictionlikelihoods as reward signals for a reinforcement learning agent. VIPER enablesexpert-level control without programmatic task rewards across a wide range ofDMC, Atari, and RLBench tasks. Moreover, generalization of the video predictionmodel allows us to derive rewards for an out-of-distribution environment whereno expert data is available, enabling cross-embodiment generalization fortabletop manipulation. We see our work as starting point for scalable rewardspecification from unlabeled videos that will benefit from the rapid advancesin generative modeling. Source code and datasets are available on the projectwebsite: https://escontrela.me</description><author>Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, Pieter Abbeel</author><pubDate>Tue, 23 May 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14343v1</guid></item><item><title>Deep Patch Visual Odometry</title><link>http://arxiv.org/abs/2208.04726v2</link><description>We propose Deep Patch Visual Odometry (DPVO), a new deep learning system formonocular Visual Odometry (VO). DPVO uses a novel recurrent networkarchitecture designed for tracking image patches across time. Recent approachesto VO have significantly improved the state-of-the-art accuracy by using deepnetworks to predict dense flow between video frames. However, using dense flowincurs a large computational cost, making these previous methods impracticalfor many use cases. Despite this, it has been assumed that dense flow isimportant as it provides additional redundancy against incorrect matches. DPVOdisproves this assumption, showing that it is possible to get the best accuracyand efficiency by exploiting the advantages of sparse patch-based matching overdense flow. DPVO introduces a novel recurrent update operator for patch basedcorrespondence coupled with differentiable bundle adjustment. On Standardbenchmarks, DPVO outperforms all prior work, including the learning-basedstate-of-the-art VO-system (DROID) using a third of the memory while running 3xfaster on average. Code is available at https://github.com/princeton-vl/DPVO</description><author>Zachary Teed, Lahav Lipson, Jia Deng</author><pubDate>Tue, 23 May 2023 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.04726v2</guid></item><item><title>Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</title><link>http://arxiv.org/abs/2305.14342v1</link><description>Given the massive cost of language model pre-training, a non-trivialimprovement of the optimization algorithm would lead to a material reduction onthe time and cost of training. Adam and its variants have been state-of-the-artfor years, and more sophisticated second-order (Hessian-based) optimizers oftenincur too much per-step overhead. In this paper, we propose Sophia,Second-order Clipped Stochastic Optimization, a simple scalable second-orderoptimizer that uses a light-weight estimate of the diagonal Hessian as thepre-conditioner. The update is the moving average of the gradients divided bythe moving average of the estimated Hessian, followed by element-wise clipping.The clipping controls the worst-case update size and tames the negative impactof non-convexity and rapid change of Hessian along the trajectory. Sophia onlyestimates the diagonal Hessian every handful of iterations, which hasnegligible average per-step time and memory overhead. On language modeling withGPT-2 models of sizes ranging from 125M to 770M, Sophia achieves a 2x speed-upcompared with Adam in the number of steps, total compute, and wall-clock time.Theoretically, we show that Sophia adapts to the curvature in differentcomponents of the parameters, which can be highly heterogeneous for languagemodeling tasks. Our run-time bound does not depend on the condition number ofthe loss.</description><author>Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma</author><pubDate>Tue, 23 May 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14342v1</guid></item><item><title>APPLS: A Meta-evaluation Testbed for Plain Language Summarization</title><link>http://arxiv.org/abs/2305.14341v1</link><description>While there has been significant development of models for Plain LanguageSummarization (PLS), evaluation remains a challenge. This is in part becausePLS involves multiple, interrelated language transformations (e.g., addingbackground explanations, removing specialized terminology). No metrics areexplicitly engineered for PLS, and the suitability of other text generationevaluation metrics remains unclear. To address these concerns, our studypresents a granular meta-evaluation testbed, APPLS, designed to evaluateexisting metrics for PLS. Drawing on insights from previous research, we definecontrolled perturbations for our testbed along four criteria that a metric ofplain language should capture: informativeness, simplification, coherence, andfaithfulness. Our analysis of metrics using this testbed reveals that currentmetrics fail to capture simplification, signaling a crucial gap. In response,we introduce POMME, a novel metric designed to assess text simplification inPLS. We demonstrate its correlation with simplification perturbations andvalidate across a variety of datasets. Our research contributes the firstmeta-evaluation testbed for PLS and a comprehensive evaluation of existingmetrics, offering insights with relevance to other text generation tasks.</description><author>Yue Guo, Tal August, Gondy Leroy, Trevor Cohen, Lucy Lu Wang</author><pubDate>Tue, 23 May 2023 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14341v1</guid></item><item><title>Anchor Prediction: Automatic Refinement of Internet Links</title><link>http://arxiv.org/abs/2305.14337v1</link><description>Internet links enable users to deepen their understanding of a topic byproviding convenient access to related information. However, the majority oflinks are unanchored -- they link to a target webpage as a whole, and readersmay expend considerable effort localizing the specific parts of the targetwebpage that enrich their understanding of the link's source context. To helpreaders effectively find information in linked webpages, we introduce the taskof anchor prediction, where the goal is to identify the specific part of thelinked target webpage that is most related to the source linking context. Werelease the AuthorAnchors dataset, a collection of 34K naturally-occurringanchored links, which reflect relevance judgments by the authors of the sourcearticle. To model reader relevance judgments, we annotate and releaseReaderAnchors, an evaluation set of anchors that readers find useful. Ouranalysis shows that effective anchor prediction often requires jointlyreasoning over lengthy source and target webpages to determine their implicitrelations and identify parts of the target webpage that are related but notredundant. We benchmark a performant T5-based ranking approach to establishbaseline performance on the task, finding ample room for improvement.</description><author>Nelson F. Liu, Kenton Lee, Kristina Toutanova</author><pubDate>Tue, 23 May 2023 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14337v1</guid></item><item><title>Schema-Driven Information Extraction from Heterogeneous Tables</title><link>http://arxiv.org/abs/2305.14336v1</link><description>In this paper, we explore the question of whether language models (LLMs) cansupport cost-efficient information extraction from complex tables. We introduceschema-driven information extraction, a new task that uses LLMs to transformtabular data into structured records following a human-authored schema. Toassess various LLM's capabilities on this task, we develop a benchmark composedof tables from three diverse domains: machine learning papers, chemistrytables, and webpages. Accompanying the benchmark, we present InstrucTE, a tableextraction method based on instruction-tuned LLMs. This method necessitatesonly a human-constructed extraction schema, and incorporates an error-recoverystrategy. Notably, InstrucTE demonstrates competitive performance withouttask-specific labels, achieving an F1 score ranging from 72.3 to 95.7.Moreover, we validate the feasibility of distilling more compact tableextraction models to minimize extraction costs and reduce API reliance. Thisstudy paves the way for the future development of instruction-following modelsfor cost-efficient table extraction.</description><author>Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter</author><pubDate>Tue, 23 May 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14336v1</guid></item><item><title>Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence</title><link>http://arxiv.org/abs/2305.14334v1</link><description>Diffusion models have been shown to be capable of generating high-qualityimages, suggesting that they could contain meaningful internal representations.Unfortunately, the feature maps that encode a diffusion model's internalinformation are spread not only over layers of the network, but also overdiffusion timesteps, making it challenging to extract useful descriptors. Wepropose Diffusion Hyperfeatures, a framework for consolidating multi-scale andmulti-timestep feature maps into per-pixel feature descriptors that can be usedfor downstream tasks. These descriptors can be extracted for both synthetic andreal images using the generation and inversion processes. We evaluate theutility of our Diffusion Hyperfeatures on the task of semantic keypointcorrespondence: our method achieves superior performance on the SPair-71k realimage benchmark. We also demonstrate that our method is flexible andtransferable: our feature aggregation network trained on the inversion featuresof real image pairs can be used on the generation features of synthetic imagepairs with unseen objects and compositions. Our code is available at\url{https://diffusion-hyperfeatures.github.io}.</description><author>Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, Trevor Darrell</author><pubDate>Tue, 23 May 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14334v1</guid></item><item><title>Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud Semantic Segmentation</title><link>http://arxiv.org/abs/2305.14335v1</link><description>In this work, we address the challenging task of few-shot and zero-shot 3Dpoint cloud semantic segmentation. The success of few-shot semanticsegmentation in 2D computer vision is mainly driven by the pre-training onlarge-scale datasets like imagenet. The feature extractor pre-trained onlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, thedevelopment of 3D deep learning is hindered by the limited volume and instancemodality of datasets due to the significant cost of 3D data collection andannotation. This results in less representative features and large intra-classfeature variation for few-shot 3D point cloud segmentation. As a consequence,directly extending existing popular prototypical methods of 2D few-shotclassification/segmentation into 3D point cloud segmentation won't work as wellas in 2D domain. To address this issue, we propose a Query-Guided PrototypeAdaption (QGPA) module to adapt the prototype from support point clouds featurespace to query point clouds feature space. With such prototype adaption, wegreatly alleviate the issue of large feature intra-class variation in pointcloud and significantly improve the performance of few-shot 3D segmentation.Besides, to enhance the representation of prototypes, we introduce aSelf-Reconstruction (SR) module that enables prototype to reconstruct thesupport mask as well as possible. Moreover, we further consider zero-shot 3Dpoint cloud semantic segmentation where there is no support sample. To thisend, we introduce category words as semantic information and propose asemantic-visual projection model to bridge the semantic and visual spaces. Ourproposed method surpasses state-of-the-art algorithms by a considerable 7.90%and 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,respectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.</description><author>Shuting He, Xudong Jiang, Wei Jiang, Henghui Ding</author><pubDate>Tue, 23 May 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14335v1</guid></item><item><title>Automatic Model Selection with Large Language Models for Reasoning</title><link>http://arxiv.org/abs/2305.14333v1</link><description>Chain-of-Thought and Program-Aided Language Models represent two distinctreasoning methods, each with its own strengths and weaknesses. We demonstratethat it is possible to combine the best of both worlds by using differentmodels for different problems, employing a large language model (LLM) toperform model selection. Through a theoretical analysis, we discover that theperformance improvement is determined by the differences between the combinedmethods and the success rate of choosing the correct model. On eight reasoningdatasets, our proposed approach shows significant improvements. Furthermore, weachieve new state-of-the-art results on GSM8K and SVAMP with accuracies of96.5% and 93.7%, respectively. Our code is publicly available athttps://github.com/XuZhao0/Model-Selection-Reasoning.</description><author>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie</author><pubDate>Tue, 23 May 2023 18:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14333v1</guid></item><item><title>Evaluating and Modeling Attribution for Cross-Lingual Question Answering</title><link>http://arxiv.org/abs/2305.14332v1</link><description>Trustworthy answer content is abundant in many high-resource languages and isinstantly accessible through question answering systems, yet this content canbe hard to access for those that do not speak these languages. The leap forwardin cross-lingual modeling quality offered by generative language models offersmuch promise, yet their raw generations often fall short in factuality. Toimprove trustworthiness in these systems, a promising direction is to attributethe answer to a retrieved source, possibly in a content-rich language differentfrom the query. Our work is the first to study attribution for cross-lingualquestion answering. First, we collect data in 5 languages to assess theattribution level of a state-of-the-art cross-lingual QA system. To oursurprise, we find that a substantial portion of the answers is not attributableto any retrieved passages (up to 50% of answers exactly matching a goldreference) despite the system being able to attend directly to the retrievedtext. Second, to address this poor attribution level, we experiment with a widerange of attribution detection techniques. We find that Natural LanguageInference models and PaLM 2 fine-tuned on a very small amount of attributiondata can accurately detect attribution. Based on these models, we improve theattribution level of a cross-lingual question-answering system. Overall, weshow that current academic generative cross-lingual QA systems have substantialshortcomings in attribution and we build tooling to mitigate these issues.</description><author>Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, Xinyi Wang</author><pubDate>Tue, 23 May 2023 18:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14332v1</guid></item><item><title>What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems</title><link>http://arxiv.org/abs/2305.14331v1</link><description>AI systems have shown impressive performance at answering questions byretrieving relevant context. However, with the increasingly large models, it isimpossible and often undesirable to constrain models' knowledge or reasoning toonly the retrieved context. This leads to a mismatch between the informationthat these models access to derive the answer and the information available tothe user consuming the AI predictions to assess the AI predicted answer. Inthis work, we study how users interact with AI systems in absence of sufficientinformation to assess AI predictions. Further, we ask the question of whetheradding the requisite background alleviates the concerns around over-reliance inAI predictions. Our study reveals that users rely on AI predictions even in theabsence of sufficient information needed to assess its correctness. Providingthe relevant background, however, helps users catch AI errors better, reducingover-reliance on incorrect AI predictions. On the flip side, backgroundinformation also increases users' confidence in their correct as well asincorrect judgments. Contrary to common expectation, aiding a user's perusal ofthe context and the background through highlights is not helpful in alleviatingthe issue of over-confidence stemming from availability of more information.Our work aims to highlight the gap between how NLP developers perceiveinformational need in human-AI interaction and the actual human interactionwith the information available to them.</description><author>Navita Goyal, Eleftheria Briakou, Amanda Liu, Connor Baumler, Claire Bonial, Jeffrey Micher, Clare R. Voss, Marine Carpuat, Hal Daumé III</author><pubDate>Tue, 23 May 2023 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14331v1</guid></item><item><title>Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation</title><link>http://arxiv.org/abs/2305.14330v1</link><description>In the paradigm of AI-generated content (AIGC), there has been increasingattention in extending pre-trained text-to-image (T2I) models to text-to-video(T2V) generation. Despite their effectiveness, these frameworks face challengesin maintaining consistent narratives and handling rapid shifts in scenecomposition or object placement from a single user prompt. This paperintroduces a new framework, dubbed DirecT2V, which leverages instruction-tunedlarge language models (LLMs) to generate frame-by-frame descriptions from asingle abstract user prompt. DirecT2V utilizes LLM directors to divide userinputs into separate prompts for each frame, enabling the inclusion oftime-varying content and facilitating consistent video generation. To maintaintemporal consistency and prevent object collapse, we propose a novel valuemapping method and dual-softmax filtering. Extensive experimental resultsvalidate the effectiveness of the DirecT2V framework in producing visuallycoherent and consistent videos from abstract user prompts, addressing thechallenges of zero-shot video generation.</description><author>Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong Shin, Seungryong Kim</author><pubDate>Tue, 23 May 2023 18:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14330v1</guid></item><item><title>Empowering LLM-based Machine Translation with Cultural Awareness</title><link>http://arxiv.org/abs/2305.14328v1</link><description>Traditional neural machine translation (NMT) systems often fail to translatesentences that contain culturally specific information. Most previous NMTmethods have incorporated external cultural knowledge during training, whichrequires fine-tuning on low-frequency items specific to the culture. Recentin-context learning utilizes lightweight prompts to guide large language models(LLMs) to perform machine translation, however, whether such an approach worksin terms of injecting culture awareness into machine translation remainsunclear. To this end, we introduce a new data curation pipeline to construct aculturally relevant parallel corpus, enriched with annotations ofcultural-specific entities. Additionally, we design simple but effectiveprompting strategies to assist this LLM-based translation. Extensiveexperiments show that our approaches can largely help incorporate culturalknowledge into LLM-based machine translation, outperforming traditional NMTsystems in translating cultural-specific sentences.</description><author>Binwei Yao, Ming Jiang, Diyi Yang, Junjie Hu</author><pubDate>Tue, 23 May 2023 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14328v1</guid></item><item><title>Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation</title><link>http://arxiv.org/abs/2305.14327v1</link><description>Instruction tuning has emerged to enhance the capabilities of large languagemodels (LLMs) in providing appropriate outputs based on input instructions.However, existing methods for collecting instruction-tuning data suffer fromlimitations in scalability and affordability. In this paper, we proposeDynosaur, a dynamic growth paradigm for instruction-tuning data curation. Builtupon the metadata of existing NLP datasets, we generate multiple taskinstructions applicable to various NLP datasets and determine the relevant datafields for constructing instruction-tuning data with LLMs. Dynosaur offersseveral advantages: 1) lower generation costs (less than $12 for generating800K instruction-tuning data), 2) good quality of instruction-tuning data(better performance than Alpaca and Instruction GPT-4 on Super-NI withcomparable data sizes), and 3) the ability to grow dynamically by incorporatingnew datasets from Huggingface Datasets Platform. We further investigatecontinual learning as an approach to learning with the ever-growinginstruction-tuning dataset. We demonstrate that replay methods not only helpmitigate forgetting issues but help generalize to unseen tasks better. As anovel continual learning scenario for instruction tuning, selecting tasks basedon instruction representations can be an effective replaying strategy. Code anddata are released at \url{https://github.com/WadeYin9712/Dynosaur}.</description><author>Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, Kai-Wei Chang</author><pubDate>Tue, 23 May 2023 18:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14327v1</guid></item><item><title>Self-Supervision is All You Need for Solving Rubik's Cube</title><link>http://arxiv.org/abs/2106.03157v5</link><description>Existing combinatorial search methods are often complex and require somelevel of expertise. This work introduces a simple and efficient deep learningmethod for solving combinatorial problems with a predefined goal, representedby Rubik's Cube. We demonstrate that, for such problems, training a deep neuralnetwork on random scrambles branching from the goal state is sufficient toachieve near-optimal solutions. When tested on Rubik's Cube, 15 Puzzle, and7$\times$7 Lights Out, our method outperformed the previous state-of-the-artmethod DeepCubeA, improving the trade-off between solution optimality andcomputational cost, despite significantly less training data. Furthermore, weinvestigate the scaling law of our Rubik's Cube solver with respect to modelsize and training data volume.</description><author>Kyo Takano</author><pubDate>Tue, 23 May 2023 18:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.03157v5</guid></item><item><title>TalkUp: A Novel Dataset Paving the Way for Understanding Empowering Language</title><link>http://arxiv.org/abs/2305.14326v1</link><description>Empowering language is important in many real-world contexts, from educationto workplace dynamics to healthcare. Though language technologies are growingmore prevalent in these contexts, empowerment has not been studied in NLP, andmoreover, it is inherently challenging to operationalize because of its subtle,implicit nature. This work presents the first computational exploration ofempowering language. We first define empowerment detection as a new task,grounding it in linguistic and social psychology literature. We thencrowdsource a novel dataset of Reddit posts labeled for empowerment, reasonswhy these posts are empowering to readers, and the social relationships betweenposters and readers. Our preliminary analyses show that this dataset, which wecall TalkUp, can be used to train language models that capture empowering anddisempowering language. More broadly, as it is rich with the ambiguities anddiverse interpretations of real-world language, TalkUp provides an avenue toexplore implication, presuppositions, and how social context influences themeaning of language.</description><author>Lucille Njoo, Chan Young Park, Octavia Stappart, Marvin Thielk, Yi Chu, Yulia Tsvetkov</author><pubDate>Tue, 23 May 2023 18:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14326v1</guid></item><item><title>Improving Factuality and Reasoning in Language Models through Multiagent Debate</title><link>http://arxiv.org/abs/2305.14325v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities inlanguage generation, understanding, and few-shot learning in recent years. Anextensive body of work has explored how their performance may be furtherimproved through the tools of prompting, ranging from verification,self-consistency, or intermediate scratchpads. In this paper, we present acomplementary approach to improve language responses where multiple languagemodel instances propose and debate their individual responses and reasoningprocesses over multiple rounds to arrive at a common final answer. Our findingsindicate that this approach significantly enhances mathematical and strategicreasoning across a number of tasks. We also demonstrate that our approachimproves the factual validity of generated content, reducing fallacious answersand hallucinations that contemporary models are prone to. Our approach may bedirectly applied to existing black-box models and uses identical procedure andprompts for all tasks we investigate. Overall, our findings suggest that such"society of minds" approach has the potential to significantly advance thecapabilities of LLMs and pave the way for further breakthroughs in languagegeneration and understanding.</description><author>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch</author><pubDate>Tue, 23 May 2023 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14325v1</guid></item><item><title>Ties Matter: Modifying Kendall's Tau for Modern Metric Meta-Evaluation</title><link>http://arxiv.org/abs/2305.14324v1</link><description>Kendall's tau is frequently used to meta-evaluate how well machinetranslation (MT) evaluation metrics score individual translations. Its focus onpairwise score comparisons is intuitive but raises the question of how tiesshould be handled, a gray area that has motivated different variants in theliterature. We demonstrate that, in settings like modern MT meta-evaluation,existing variants have weaknesses arising from their handling of ties, and insome situations can even be gamed. We propose a novel variant that givesmetrics credit for correctly predicting ties, as well as an optimizationprocedure that automatically introduces ties into metric scores, enabling faircomparison between metrics that do and do not predict ties. We argue andprovide experimental evidence that these modifications lead to fairerKendall-based assessments of metric performance.</description><author>Daniel Deutsch, George Foster, Markus Freitag</author><pubDate>Tue, 23 May 2023 18:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14324v1</guid></item><item><title>ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on\\ Chat-based Large Language Models</title><link>http://arxiv.org/abs/2305.14323v1</link><description>Although large language models (LLMs) have achieved excellent performance ina variety of evaluation benchmarks, they still struggle in complex reasoningtasks which require specific knowledge and multi-hop reasoning. To improve thereasoning abilities, we propose \textbf{ChatCoT}, a tool-augmentedchain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we modelthe chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilizetools in a more natural way through chatting. At each turn, LLMs can eitherinteract with tools or perform the reasoning. %framework to decompose reasoningand manipulate tools for complex reasoning tasks via multi-round dialogue. %Ouridea is to decompose the complex problems into several sub-problems and solvethese sub-problems with the help of external tools. Our approach caneffectively leverage the multi-turn conversation ability of chat-based LLMs,and integrate the thought chain following and tools manipulation in a unifiedway. Specially, we initialize the early turns of the conversation by the tools,tasks and reasoning format, and propose an iterative \emph{tool-augmentedreasoning} step to perform step-by-step tool-augmented reasoning. Theexperiment results on two complex reasoning datasets (MATH and HotpotQA) haveshown the effectiveness of ChatCoT on complex reasoning tasks, achieving a6.8\% relative improvement over the state-of-the-art baseline. %and ChatCoT canachieve performance on Math. Our code and data are available at:\url{https://github.com/RUCAIBOX/ChatCoT}.</description><author>Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen</author><pubDate>Tue, 23 May 2023 18:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14323v1</guid></item><item><title>RET-LLM: Towards a General Read-Write Memory for Large Language Models</title><link>http://arxiv.org/abs/2305.14322v1</link><description>Large language models (LLMs) have significantly advanced the field of naturallanguage processing (NLP) through their extensive parameters and comprehensivedata utilization. However, existing LLMs lack a dedicated memory unit, limitingtheir ability to explicitly store and retrieve knowledge for various tasks. Inthis paper, we propose RET-LLM a novel framework that equips LLMs with ageneral write-read memory unit, allowing them to extract, store, and recallknowledge from the text as needed for task performance. Inspired by Davidsoniansemantics theory, we extract and save knowledge in the form of triplets. Thememory unit is designed to be scalable, aggregatable, updatable, andinterpretable. Through qualitative evaluations, we demonstrate the superiorityof our proposed framework over baseline approaches in question answering tasks.Moreover, our framework exhibits robust performance in handling temporal-basedquestion answering tasks, showcasing its ability to effectively managetime-dependent information.</description><author>Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze</author><pubDate>Tue, 23 May 2023 18:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14322v1</guid></item><item><title>ConGraT: Self-Supervised Contrastive Pretraining for Joint Graph and Text Embeddings</title><link>http://arxiv.org/abs/2305.14321v1</link><description>We propose ConGraT(Contrastive Graph-Text pretraining), a general,self-supervised method for jointly learning separate representations of textsand nodes in a parent (or ``supervening'') graph, where each text is associatedwith one of the nodes. Datasets fitting this paradigm are common, from socialmedia (users and posts), to citation networks over articles, to link graphsover web pages. We expand on prior work by providing a general,self-supervised, joint pretraining method, one which does not depend onparticular dataset structure or a specific task. Our method uses two separateencoders for graph nodes and texts, which are trained to align theirrepresentations within a common latent space. Training uses a batch-wisecontrastive learning objective inspired by prior work on joint text and imageencoding. As graphs are more structured objects than images, we also extend thetraining objective to incorporate information about node similarity andplausible next guesses in matching nodes and texts. Experiments on variousdatasets reveal that ConGraT outperforms strong baselines on various downstreamtasks, including node and text category classification and link prediction.Code and certain datasets are available athttps://github.com/wwbrannon/congrat.</description><author>William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, Deb Roy</author><pubDate>Tue, 23 May 2023 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14321v1</guid></item><item><title>CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation</title><link>http://arxiv.org/abs/2305.14318v1</link><description>Large Language Models (LLMs) have demonstrated significant progress inutilizing external APIs as tools for various tasks. However, their tool-usingability is limited by the availability of suitable APIs and the instability ofimplicit reasoning, particularly when simultaneously engaging in reasoningabout plans and actual calculations. To address these limitations, we proposeCREATOR, a novel framework that empowers LLMs to create their own tools throughdocumentation and code realization. CREATOR disentangles the LLM's ability intotwo distinct phases: abstract tool creation and concrete decision execution,which results in improved LLM performance. We evaluate CREATOR on twoestablished benchmarks: MATH, which consists of challenging math competitionproblems, and TabMWP, which includes diverse tabular contents forproblem-solving. Remarkably, CREATOR significantly outperforms existingchain-of-thought (CoT), program-of-thought (PoT), and tool-using baselines onthese two benchmarks. Additionally, we present a new dataset, CreationChallenge, comprising 2K diverse questions, to highlight the necessity andbenefits of LLMs' tool creation ability in effectively addressing theseproblems. Furthermore, our research reveals that leveraging LLMs as toolcreators facilitates knowledge transfer, and LLMs exhibit varying levels oftool creation abilities, enabling them to flexibly tackle diverse situations.Our study represents a promising avenue for maximizing the potential of LLMsand advancing toward truly intelligent and adaptable AI systems.</description><author>Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji</author><pubDate>Tue, 23 May 2023 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14318v1</guid></item><item><title>Boosting Discriminative Visual Representation Learning with Scenario-Agnostic Mixup</title><link>http://arxiv.org/abs/2111.15454v3</link><description>Mixup is a well-known data-dependent augmentation technique for DNNs,consisting of two sub-tasks: mixup generation and classification. However, therecent dominant online training method confines mixup to supervised learning(SL), and the objective of the generation sub-task is limited to selectedsample pairs instead of the whole data manifold, which might cause trivialsolutions. To overcome such limitations, we comprehensively study the objectiveof mixup generation and propose \textbf{S}cenario-\textbf{A}gnostic\textbf{Mix}up (SAMix) for both SL and Self-supervised Learning (SSL)scenarios. Specifically, we hypothesize and verify the objective function ofmixup generation as optimizing local smoothness between two mixed classessubject to global discrimination from other classes. Accordingly, we propose$\eta$-balanced mixup loss for complementary learning of the twosub-objectives. Meanwhile, a label-free generation sub-network is designed,which effectively provides non-trivial mixup samples and improves transferableabilities. Moreover, to reduce the computational cost of online training, wefurther introduce a pre-trained version, SAMix$^\mathcal{P}$, achieving morefavorable efficiency and generalizability. Extensive experiments on nine SL andSSL benchmarks demonstrate the consistent superiority and versatility of SAMixcompared with existing methods.</description><author>Siyuan Li, Zicheng Liu, Zedong Wang, Di Wu, Zihan Liu, Stan Z. Li</author><pubDate>Tue, 23 May 2023 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.15454v3</guid></item><item><title>QLoRA: Efficient Finetuning of Quantized LLMs</title><link>http://arxiv.org/abs/2305.14314v1</link><description>We present QLoRA, an efficient finetuning approach that reduces memory usageenough to finetune a 65B parameter model on a single 48GB GPU while preservingfull 16-bit finetuning task performance. QLoRA backpropagates gradients througha frozen, 4-bit quantized pretrained language model into Low RankAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms allprevious openly released models on the Vicuna benchmark, reaching 99.3% of theperformance level of ChatGPT while only requiring 24 hours of finetuning on asingle GPU. QLoRA introduces a number of innovations to save memory withoutsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that isinformation theoretically optimal for normally distributed weights (b) doublequantization to reduce the average memory footprint by quantizing thequantization constants, and (c) paged optimziers to manage memory spikes. Weuse QLoRA to finetune more than 1,000 models, providing a detailed analysis ofinstruction following and chatbot performance across 8 instruction datasets,multiple model types (LLaMA, T5), and model scales that would be infeasible torun with regular finetuning (e.g. 33B and 65B parameter models). Our resultsshow that QLoRA finetuning on a small high-quality dataset leads tostate-of-the-art results, even when using smaller models than the previousSoTA. We provide a detailed analysis of chatbot performance based on both humanand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonablealternative to human evaluation. Furthermore, we find that current chatbotbenchmarks are not trustworthy to accurately evaluate the performance levels ofchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared toChatGPT. We release all of our models and code, including CUDA kernels for4-bit training.</description><author>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer</author><pubDate>Tue, 23 May 2023 18:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14314v1</guid></item><item><title>Text-guided 3D Human Generation from 2D Collections</title><link>http://arxiv.org/abs/2305.14312v1</link><description>3D human modeling has been widely used for engaging interaction in gaming,film, and animation. The customization of these characters is crucial forcreativity and scalability, which highlights the importance of controllability.In this work, we introduce Text-guided 3D Human Generation (\texttt{T3H}),where a model is to generate a 3D human, guided by the fashion description.There are two goals: 1) the 3D human should render articulately, and 2) itsoutfit is controlled by the given text. To address this \texttt{T3H} task, wepropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attentionto fuse compositional human rendering with the extracted fashion semantics.Each human body part perceives relevant textual guidance as its visualpatterns. We incorporate the human prior and semantic discrimination to enhance3D geometry transformation and fine-grained consistency, enabling it to learnfrom 2D collections for data efficiency. We conduct evaluations on DeepFashionand SHHQ with diverse fashion attributes covering the shape, fabric, and colorof upper and lower clothing. Extensive experiments demonstrate that CCHachieves superior results for \texttt{T3H} with high efficiency.</description><author>Tsu-Jui Fu, Wenhan Xiong, Yixin Nie, Jingyu Liu, Barlas Oğuz, William Yang Wang</author><pubDate>Tue, 23 May 2023 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14312v1</guid></item><item><title>Statistical Indistinguishability of Learning Algorithms</title><link>http://arxiv.org/abs/2305.14311v1</link><description>When two different parties use the same learning rule on their own data, howcan we test whether the distributions of the two outcomes are similar? In thispaper, we study the similarity of outcomes of learning rules through the lensof the Total Variation (TV) distance of distributions. We say that a learningrule is TV indistinguishable if the expected TV distance between the posteriordistributions of its outputs, executed on two training data sets drawnindependently from the same distribution, is small. We first investigate thelearnability of hypothesis classes using TV indistinguishable learners. Ourmain results are information-theoretic equivalences between TVindistinguishability and existing algorithmic stability notions such asreplicability and approximate differential privacy. Then, we providestatistical amplification and boosting algorithms for TV indistinguishablelearners.</description><author>Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas</author><pubDate>Tue, 23 May 2023 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14311v1</guid></item><item><title>Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science</title><link>http://arxiv.org/abs/2305.14310v1</link><description>Instruction-tuned Large Language Models (LLMs) have exhibited impressivelanguage understanding and the capacity to generate responses that followspecific instructions. However, due to the computational demands associatedwith training these models, their applications often rely on zero-shotsettings. In this paper, we evaluate the zero-shot performance of two publiclyaccessible LLMs, ChatGPT and OpenAssistant, in the context of ComputationalSocial Science classification tasks, while also investigating the effects ofvarious prompting strategies. Our experiment considers the impact of promptcomplexity, including the effect of incorporating label definitions into theprompt, using synonyms for label names, and the influence of integrating pastmemories during the foundation model training. The findings indicate that in azero-shot setting, the current LLMs are unable to match the performance ofsmaller, fine-tuned baseline transformer models (such as BERT). Additionally,we find that different prompting strategies can significantly affectclassification accuracy, with variations in accuracy and F1 scores exceeding10%.</description><author>Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos Aletras, Carolina Scarton, Kalina Bontcheva, Xingyi Song</author><pubDate>Tue, 23 May 2023 18:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14310v1</guid></item><item><title>Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models</title><link>http://arxiv.org/abs/2305.14307v1</link><description>Debiasing methods that seek to mitigate the tendency of Language Models (LMs)to occasionally output toxic or inappropriate text have recently gainedtraction. In this paper, we propose a standardized protocol which distinguishesmethods that yield not only desirable results, but are also consistent withtheir mechanisms and specifications. For example, we ask, given a debiasingmethod that is developed to reduce toxicity in LMs, if the definition oftoxicity used by the debiasing method is reversed, would the debiasing resultsalso be reversed? We used such considerations to devise three criteria for ournew protocol: Specification Polarity, Specification Importance, and DomainTransferability. As a case study, we apply our protocol to a popular debiasingmethod, Self-Debiasing, and compare it to one we propose, called InstructiveDebiasing, and demonstrate that consistency is as important an aspect todebiasing viability as is simply a desirable result. We show that our protocolprovides essential insights into the generalizability and interpretability ofdebiasing methods that may otherwise go overlooked.</description><author>Robert Morabito, Jad Kabbara, Ali Emami</author><pubDate>Tue, 23 May 2023 18:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14307v1</guid></item><item><title>Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications in Large-scale Point Clouds</title><link>http://arxiv.org/abs/2305.14306v1</link><description>While point-based neural architectures have demonstrated their efficacy, thetime-consuming sampler currently prevents them from performing real-timereasoning on scene-level point clouds. Existing methods attempt to overcomethis issue by using random sampling strategy instead of the commonly-adoptedfarthest point sampling~(FPS), but at the expense of lower performance. So theeffectiveness/efficiency trade-off remains under-explored. In this paper, wereveal the key to high-quality sampling is ensuring an even spacing betweenpoints in the subset, which can be naturally obtained through a grid. Based onthis insight, we propose a hierarchical adaptive voxel-guided point samplerwith linear complexity and high parallelization for real-time applications.Extensive experiments on large-scale point cloud detection and segmentationtasks demonstrate that our method achieves competitive performance with themost powerful FPS, at an amazing speed that is more than 100 times faster. Thisbreakthrough in efficiency addresses the bottleneck of the sampling step whenhandling scene-level point clouds. Furthermore, our sampler can be easilyintegrated into existing models and achieves a 20$\sim$80\% reduction inruntime with minimal effort. The code will be available athttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt</description><author>Junyuan Ouyang, Xiao Liu, Haoyao Chen</author><pubDate>Tue, 23 May 2023 18:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14306v1</guid></item><item><title>QTSumm: A New Benchmark for Query-Focused Table Summarization</title><link>http://arxiv.org/abs/2305.14303v1</link><description>People primarily consult tables to conduct data analysis or answer specificquestions. Text generation systems that can provide accurate table summariestailored to users' information needs can facilitate more efficient access torelevant data insights. However, existing table-to-text generation studiesprimarily focus on converting tabular data into coherent statements, ratherthan addressing information-seeking purposes. In this paper, we define a newquery-focused table summarization task, where text generation models have toperform human-like reasoning and analysis over the given table to generate atailored summary, and we introduce a new benchmark named QTSumm for this task.QTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tableson diverse topics. Moreover, we investigate state-of-the-art models (i.e., textgeneration, table-to-text generation, and large language models) on the QTSummdataset. Experimental results and manual analysis reveal that our benchmarkpresents significant challenges in table-to-text generation for futureresearch.</description><author>Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev</author><pubDate>Tue, 23 May 2023 18:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14303v1</guid></item><item><title>VIP5: Towards Multimodal Foundation Models for Recommendation</title><link>http://arxiv.org/abs/2305.14302v1</link><description>Computer Vision (CV), Natural Language Processing (NLP), and RecommenderSystems (RecSys) are three prominent AI applications that have traditionallydeveloped independently, resulting in disparate modeling and engineeringmethodologies. This has impeded the ability for these fields to directlybenefit from each other's advancements. With the increasing availability ofmultimodal data on the web, there is a growing need to consider variousmodalities when making recommendations for users. With the recent emergence offoundation models, large language models have emerged as a potentialgeneral-purpose interface for unifying different modalities and problemformulations. In light of this, we propose the development of a multimodalfoundation model by considering both visual and textual modalities under the P5recommendation paradigm (VIP5) to unify various modalities and recommendationtasks. This will enable the processing of vision, language, and personalizationinformation in a shared architecture for improved recommendations. To achievethis, we introduce multimodal personalized prompts to accommodate multiplemodalities under a shared format. Additionally, we propose aparameter-efficient training method for foundation models, which involvesfreezing the backbone and fine-tuning lightweight adapters, resulting inimproved recommendation performance and increased efficiency in terms oftraining time and memory usage.</description><author>Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang</author><pubDate>Tue, 23 May 2023 18:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14302v1</guid></item><item><title>A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network</title><link>http://arxiv.org/abs/2305.14301v1</link><description>Hematoxylin and Eosin (H&amp;E) staining is a widely used sample preparationprocedure for enhancing the saturation of tissue sections and the contrastbetween nuclei and cytoplasm in histology images for medical diagnostics.However, various factors, such as the differences in the reagents used, resultin high variability in the colors of the stains actually recorded. Thisvariability poses a challenge in achieving generalization for machine-learningbased computer-aided diagnostic tools. To desensitize the learned models tostain variations, we propose the Generative Stain Augmentation Network (G-SAN)-- a GAN-based framework that augments a collection of cell images withsimulated yet realistic stain variations. At its core, G-SAN uses a novel andhighly computationally efficient Laplacian Pyramid (LP) based generatorarchitecture, that is capable of disentangling stain from cell morphology.Through the task of patch classification and nucleus segmentation, we show thatusing G-SAN-augmented training data provides on average 15.7% improvement in F1score and 7.3% improvement in panoptic quality, respectively. Our code isavailable at https://github.com/lifangda01/GSAN-Demo.</description><author>Fangda Li, Zhiqiang Hu, Wen Chen, Avinash Kak</author><pubDate>Tue, 23 May 2023 18:43:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14301v1</guid></item><item><title>TaDSE: Template-aware Dialogue Sentence Embeddings</title><link>http://arxiv.org/abs/2305.14299v1</link><description>Learning high quality sentence embeddings from dialogues has drawn increasingattentions as it is essential to solve a variety of dialogue-oriented taskswith low annotation cost. However, directly annotating and gathering utterancerelationships in conversations are difficult, while token-level annotations,\eg, entities, slots and templates, are much easier to obtain. General sentenceembedding methods are usually sentence-level self-supervised frameworks andcannot utilize token-level extra knowledge. In this paper, we introduceTemplate-aware Dialogue Sentence Embedding (TaDSE), a novel augmentation methodthat utilizes template information to effectively learn utterancerepresentation via self-supervised contrastive learning framework. TaDSEaugments each sentence with its corresponding template and then conductspairwise contrastive learning over both sentence and template. We furtherenhance the effect with a synthetically augmented dataset that enhancesutterance-template relation, in which entity detection (slot-filling) is apreliminary step. We evaluate TaDSE performance on five downstream benchmarkdatasets. The experiment results show that TaDSE achieves significantimprovements over previous SOTA methods, along with a consistent IntentClassification task performance improvement margin. We further introduce anovel analytic instrument of Semantic Compression method, for which we discovera correlation with uniformity and alignment. Our code will be released soon.</description><author>Minsik Oh, Jiwei Li, Guoyin Wang</author><pubDate>Tue, 23 May 2023 18:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14299v1</guid></item><item><title>MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking</title><link>http://arxiv.org/abs/2305.14298v1</link><description>Although end-to-end multi-object trackers like MOTR enjoy the merits ofsimplicity, they suffer from the conflict between detection and associationseriously, resulting in unsatisfactory convergence dynamics. While MOTRv2partly addresses this problem, it demands an additional detection network forassistance. In this work, we serve as the first to reveal that this conflictarises from the unfair label assignment between detect queries and trackqueries during training, where these detect queries recognize targets and trackqueries associate them. Based on this observation, we propose MOTRv3, whichbalances the label assignment process using the developed release-fetchsupervision strategy. In this strategy, labels are first released for detectionand gradually fetched back for association. Besides, another two strategiesnamed pseudo label distillation and track group denoising are designed tofurther improve the supervision for detection and association. Without theassistance of an extra detection network during inference, MOTRv3 achievesimpressive performance across diverse benchmarks, e.g., MOT17, DanceTrack.</description><author>En Yu, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, Wenbing Tao</author><pubDate>Tue, 23 May 2023 18:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14298v1</guid></item><item><title>USB: A Unified Summarization Benchmark Across Tasks and Domains</title><link>http://arxiv.org/abs/2305.14296v1</link><description>An abundance of datasets exist for training and evaluating models on the taskof summary generation.However, these datasets are often derived heuristically,and lack sufficient annotations to support research into all aspects ofsummarization, such as evidence extraction and controllable summarization. Weintroduce a benchmark comprising 8 tasks that require multi-dimensionalunderstanding of summarization, e.g., surfacing evidence for a summary,assessing its correctness, and gauging its relevance to different topics. Wecompare various methods on this benchmark and discover that on multiple tasks,moderately-sized fine-tuned models consistently outperform much larger few-shotprompted language models. For factuality related tasks, we also evaluateexisting heuristics to create training data and find that training on themperforms worse than training on $20\times$ less human-labeled data. Ourbenchmark consists of data from 6 different domains, allowing us to studycross-domain performance of trained models. We find that for some tasks, theamount of training data matters more than the domain where it comes from, whilefor other tasks training specifically on data from the target domain, even iflimited, is more beneficial. Our work fulfills the need for a well-annotatedsummarization benchmark with diverse tasks, and provides useful insights aboutthe impact of the quality, size and domain of training data.</description><author>Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Byron C. Wallace, Jeffrey P. Bigham, Zachary C. Lipton</author><pubDate>Tue, 23 May 2023 18:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14296v1</guid></item><item><title>WebIE: Faithful and Robust Information Extraction on the Web</title><link>http://arxiv.org/abs/2305.14293v1</link><description>Extracting structured and grounded fact triples from raw text is afundamental task in Information Extraction (IE). Existing IE datasets aretypically collected from Wikipedia articles, using hyperlinks to link entitiesto the Wikidata knowledge base. However, models trained only on Wikipedia havelimitations when applied to web domains, which often contain noisy text or textthat does not have any factual information. We present WebIE, the firstlarge-scale, entity-linked closed IE dataset consisting of 1.6M sentencesautomatically collected from the English Common Crawl corpus. WebIE alsoincludes negative examples, i.e. sentences without fact triples, to betterreflect the data on the web. We annotate ~25K triples from WebIE throughcrowdsourcing and introduce mWebIE, a translation of the annotated set in fourother languages: French, Spanish, Portuguese, and Hindi. We evaluate thein-domain, out-of-domain, and zero-shot cross-lingual performance of generativeIE models and find models trained on WebIE show better generalisability. Wealso propose three training strategies that use entity linking as an auxiliarytask. Our experiments show that adding Entity-Linking objectives improves thefaithfulness of our generative IE models.</description><author>Chenxi Whitehouse, Clara Vania, Alham Fikri Aji, Christos Christodoulopoulos, Andrea Pierleoni</author><pubDate>Tue, 23 May 2023 18:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14293v1</guid></item><item><title>WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia</title><link>http://arxiv.org/abs/2305.14292v1</link><description>Despite recent advances in Large Language Models (LLMs), users still cannottrust the information provided in their responses. LLMs cannot speak accuratelyabout events that occurred after their training, which are often topics ofgreat interest to users, and, as we show in this paper, they are highly proneto hallucination when talking about less popular (tail) topics. This paperpresents WikiChat, a few-shot LLM-based chatbot that is grounded with liveinformation from Wikipedia. Through many iterations of experimentation, we havecrafte a pipeline based on information retrieval that (1) uses LLMs to suggestinteresting and relevant facts that are individually verified againstWikipedia, (2) retrieves additional up-to-date information, and (3) composescoherent and engaging time-aware responses. We propose a novel hybridhuman-and-LLM evaluation methodology to analyze the factuality andconversationality of LLM-based chatbots. We focus on evaluating important butpreviously neglected issues such as conversing about recent and tail topics. Weevaluate WikiChat against strong fine-tuned and LLM-based baselines across adiverse set of conversation topics. We find that WikiChat outperforms allbaselines in terms of the factual accuracy of its claims, by up to 12.1%, 28.3%and 32.7% on head, recent and tail topics, while matching GPT-3.5 in terms ofproviding natural, relevant, non-repetitive and informational responses.</description><author>Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam</author><pubDate>Tue, 23 May 2023 18:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14292v1</guid></item><item><title>Evaluation of African American Language Bias in Natural Language Generation</title><link>http://arxiv.org/abs/2305.14291v1</link><description>We evaluate how well LLMs understand African American Language (AAL) incomparison to their performance on White Mainstream English (WME), theencouraged "standard" form of English taught in American classrooms. We measureLLM performance using automatic metrics and human judgments for two tasks: acounterpart generation task, where a model generates AAL (or WME) given WME (orAAL), and a masked span prediction (MSP) task, where models predict a phrasethat was removed from their input. Our contributions include: (1) evaluation ofsix pre-trained, large language models on the two language generation tasks;(2) a novel dataset of AAL text from multiple contexts (social media, hip-hoplyrics, focus groups, and linguistic interviews) with human-annotatedcounterparts in WME; and (3) documentation of model performance gaps thatsuggest bias and identification of trends in lack of understanding of AALfeatures.</description><author>Nicholas Deas, Jessi Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, Kathleen McKeown</author><pubDate>Tue, 23 May 2023 18:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14291v1</guid></item><item><title>LLM-powered Data Augmentation for Enhanced Crosslingual Performance</title><link>http://arxiv.org/abs/2305.14288v1</link><description>This paper aims to explore the potential of leveraging Large Language Models(LLMs) for data augmentation in crosslingual commonsense reasoning datasets,where the available training data is extremely limited. To achieve this, weemploy several LLMs including Dolly-v2, StableVicuna, ChatGPT, and GPT-4 toaugment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, weassess the effectiveness of fine-tuning smaller crosslingual models, mBERT andXLMR, using the synthesised data. We compare the performance of training withdata generated in English and target languages, as well as translating theEnglish-generated data into the target languages. Our experiments reveal theoverall advantages of incorporating data generated by LLMs. Training onsynthetic data generated by GPT-4, whether English or multilingual, improvesperformance consistently compared to the baseline. Other models also exhibit anoverall increase in performance, however, their effectiveness decreases in somesettings. We also ask native speakers to evaluate the naturalness and logicalsoundness of the generated examples for different languages. Human evaluationreveals that LLMs like ChatGPT and GPT-4 excel at generating natural text inmost languages, except a few such as Tamil. Moreover, ChatGPT trails behind ingenerating plausible alternatives in comparison to the original dataset, whileGPT-4 demonstrates competitive logic consistency in the synthesised data.</description><author>Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji</author><pubDate>Tue, 23 May 2023 18:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14288v1</guid></item><item><title>Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics</title><link>http://arxiv.org/abs/2305.14286v1</link><description>Neural networks are emerging as a tool for scalable data-driven simulation ofhigh-dimensional dynamical systems, especially in settings where numericalmethods are infeasible or computationally expensive. Notably, it has been shownthat incorporating domain symmetries in deterministic neural simulators cansubstantially improve their accuracy, sample efficiency, and parameterefficiency. However, to incorporate symmetries in probabilistic neuralsimulators that can simulate stochastic phenomena, we need a model thatproduces equivariant distributions over trajectories, rather than equivariantfunction approximations. In this paper, we propose Equivariant ProbabilisticNeural Simulation (EPNS), a framework for autoregressive probabilistic modelingof equivariant distributions over system evolutions. We use EPNS to designmodels for a stochastic n-body system and stochastic cellular dynamics. Ourresults show that EPNS considerably outperforms existing neural network-basedmethods for probabilistic simulation. More specifically, we demonstrate thatincorporating equivariance in EPNS improves simulation quality, dataefficiency, rollout stability, and uncertainty quantification. We conclude thatEPNS is a promising method for efficient and effective data-drivenprobabilistic simulation in a diverse range of domains.</description><author>Koen Minartz, Yoeri Poels, Simon Koop, Vlado Menkovski</author><pubDate>Tue, 23 May 2023 18:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14286v1</guid></item><item><title>Query Rewriting for Retrieval-Augmented Large Language Models</title><link>http://arxiv.org/abs/2305.14283v1</link><description>Large Language Models (LLMs) play a powerful \textit{Reader} of the\textit{Retrieve-then-Read} pipeline, making great progress in knowledge-basedopen-domain tasks. This work introduces a new framework,\textit{Rewrite-Retrieve-Read} that improves the retrieval-augmented methodfrom the perspective of the query rewriting. Prior studies mostly contribute toadapt the retriever or stimulate the reader. Different from them, our approachpay attention of the query adaptation. Because the original query can not bealways optimal to retrieve for the LLM, especially in the real world.(1) Wefirst prompt an LLM to rewrite the queries, then conduct retrieval-augmentedreading. (2) We further apply a small language model as a trainable rewriter,which rewrite the search query to cater to the frozen retriever and the LLMreader. To fine-tune the rewriter, we first use a pseudo data to conductsupervised warm-up training. Then the \textit{Retrieve-then-Read} pipeline ismodeled as a reinforcement learning context. The rewriter is further trained asa policy model by maximize the reward of the pipeline performance. Evaluationis performed on two downstream tasks, open-domain QA and multiple choice. Ourframework is proved effective and scalable.</description><author>Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan</author><pubDate>Tue, 23 May 2023 18:27:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14283v1</guid></item><item><title>INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback</title><link>http://arxiv.org/abs/2305.14282v1</link><description>The field of automatic evaluation of text generation made tremendous progressin the last few years. In particular, since the advent of neural metrics, likeCOMET, BLEURT, and SEScore2, the newest generation of metrics show a highcorrelation with human judgment. Unfortunately, quality scores generated withneural metrics are not interpretable, and it is unclear which part of thegeneration output is criticized by the metrics. To address this limitation, wepresent INSTRUCTSCORE, an open-source, explainable evaluation metric for textgeneration. By harnessing both explicit human instruction and the implicitknowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metricthat can produce a diagnostic report aligned with human judgment. We evaluateINSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpassesother LLM-based baselines, including those based on 175B GPT3. Impressively,our INSTRUCTSCORE, even without direct supervision from human-rated data,achieves performance levels on par with state-of-the-art metrics like COMET22,which was fine-tuned on human ratings.</description><author>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, Lei Li</author><pubDate>Tue, 23 May 2023 18:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14282v1</guid></item><item><title>Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining</title><link>http://arxiv.org/abs/2305.14281v1</link><description>Recent work in vision-and-language pretraining has investigated supervisedsignals from object detection data to learn better, fine-grained multimodalrepresentations. In this work, we take a step further and explore how we addsupervision from small-scale visual relation data. In particular, we proposetwo pretraining approaches to contextualise visual entities in a multimodalsetup. With verbalised scene graphs, we transform visual relation triplets intostructured captions, and treat them as additional views of images. With maskedrelation prediction, we further encourage relating entities from visuallymasked contexts. When applied to strong baselines pretrained on large amountsof Web data, zero-shot evaluations on both coarse-grained and fine-grainedtasks show the efficacy of our methods in learning multimodal representationsfrom weakly-supervised relations data.</description><author>Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks</author><pubDate>Tue, 23 May 2023 18:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14281v1</guid></item><item><title>Pixel Representations for Multilingual Translation and Data-efficient Cross-lingual Transfer</title><link>http://arxiv.org/abs/2305.14280v1</link><description>We introduce and demonstrate how to effectively train multilingual machinetranslation models with pixel representations. We experiment with two differentdata settings with a variety of language and script coverage, and showperformance competitive with subword embeddings. We analyze various propertiesof pixel representations to better understand where they provide potentialbenefits and the impact of different scripts and data representations. Weobserve that these properties not only enable seamless cross-lingual transferto unseen scripts, but make pixel representations more data-efficient thanalternatives such as vocabulary expansion. We hope this work contributes tomore extensible multilingual models for all languages and scripts.</description><author>Elizabeth Salesky, Neha Verma, Philipp Koehn, Matt Post</author><pubDate>Tue, 23 May 2023 18:26:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14280v1</guid></item><item><title>PhotoMat: A Material Generator Learned from Single Flash Photos</title><link>http://arxiv.org/abs/2305.12296v2</link><description>Authoring high-quality digital materials is key to realism in 3D rendering.Previous generative models for materials have been trained exclusively onsynthetic data; such data is limited in availability and has a visual gap toreal materials. We circumvent this limitation by proposing PhotoMat: the firstmaterial generator trained exclusively on real photos of material samplescaptured using a cell phone camera with flash. Supervision on individualmaterial maps is not available in this setting. Instead, we train a generatorfor a neural material representation that is rendered with a learned relightingmodule to create arbitrarily lit RGB images; these are compared against realphotos using a discriminator. We then train a material maps estimator to decodematerial reflectance properties from the neural material representation. Wetrain PhotoMat with a new dataset of 12,000 material photos captured withhandheld phone cameras under flash lighting. We demonstrate that our generatedmaterials have better visual quality than previous material generators trainedon synthetic data. Moreover, we can fit analytical material models to closelymatch these generated neural materials, thus allowing for further editing anduse in 3D rendering.</description><author>Xilong Zhou, Miloš Hašan, Valentin Deschaintre, Paul Guerrero, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Nima Khademi Kalantari</author><pubDate>Tue, 23 May 2023 18:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12296v2</guid></item><item><title>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs</title><link>http://arxiv.org/abs/2305.14279v1</link><description>Large language models (LLMs) have achieved widespread success on a variety ofin-context few-shot tasks, but this success is typically evaluated viacorrectness rather than consistency. We argue that self-consistency is animportant criteria for valid multi-step reasoning and propose two types ofself-consistency that are particularly important for multi-step logic --hypothetical consistency (the ability for a model to predict what its outputwould be in a hypothetical other context) and compositional consistency(consistency of a model's outputs for a compositional task even when anintermediate step is replaced with the model's output for that step). Wedemonstrate that four sizes of the GPT-3 model exhibit poor consistency ratesacross both types of consistency on four different tasks (Wikipedia,DailyDialog, arithmetic, and GeoQuery).</description><author>Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, Kyunghyun Cho</author><pubDate>Tue, 23 May 2023 18:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14279v1</guid></item><item><title>Variational Inference with Coverage Guarantees</title><link>http://arxiv.org/abs/2305.14275v1</link><description>Amortized variational inference produces a posterior approximator that cancompute a posterior approximation given any new observation. Unfortunately,there are few guarantees about the quality of these approximate posteriors. Wepropose Conformalized Amortized Neural Variational Inference (CANVI), aprocedure that is scalable, easily implemented, and provides guaranteedmarginal coverage. Given a collection of candidate amortized posteriorapproximators, CANVI constructs conformalized predictors based on eachcandidate, compares the predictors using a metric known as predictiveefficiency, and returns the most efficient predictor. CANVI ensures that theresulting predictor constructs regions that contain the truth with highprobability (exactly how high is prespecified by the user). CANVI is agnosticto design decisions in formulating the candidate approximators and onlyrequires access to samples from the forward model, permitting its use inlikelihood-free settings. We prove lower bounds on the predictive efficiency ofthe regions produced by CANVI and explore how the quality of a posteriorapproximation relates to the predictive efficiency of prediction regions basedon that approximation. Finally, we demonstrate the accurate calibration andhigh predictive efficiency of CANVI on a suite of simulation-based inferencebenchmark tasks and an important scientific task: analyzing galaxy emissionspectra.</description><author>Yash Patel, Declan McNamara, Jackson Loper, Jeffrey Regier, Ambuj Tewari</author><pubDate>Tue, 23 May 2023 18:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14275v1</guid></item><item><title>Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers</title><link>http://arxiv.org/abs/2305.14269v1</link><description>With the increasing availability of depth sensors, multimodal frameworks thatcombine color information with depth data are attracting increasing interest.In the challenging task of semantic segmentation, depth maps allow todistinguish between similarly colored objects at different depths and provideuseful geometric cues. On the other side, ground truth data for semanticsegmentation is burdensome to be provided and thus domain adaptation is anothersignificant research area. Specifically, we address the challenging source-freedomain adaptation setting where the adaptation is performed without reusingsource data. We propose MISFIT: MultImodal Source-Free Information fusionTransformer, a depth-aware framework which injects depth information into asegmentation module based on vision transformers at multiple stages, namely atthe input, feature and output levels. Color and depth style transfer helpsearly-stage domain alignment while re-wiring self-attention between modalitiescreates mixed features allowing the extraction of better semantic content.Furthermore, a depth-based entropy minimization strategy is also proposed toadaptively weight regions at different distances. Our framework, which is alsothe first approach using vision transformers for source-free semanticsegmentation, shows noticeable performance improvements with respect tostandard strategies.</description><author>Giulia Rizzoli, Donald Shenaj, Pietro Zanuttigh</author><pubDate>Tue, 23 May 2023 18:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14269v1</guid></item><item><title>Masked Path Modeling for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2305.14268v1</link><description>Vision-and-language navigation (VLN) agents are trained to navigate inreal-world environments by following natural language instructions. A majorchallenge in VLN is the limited availability of training data, which hindersthe models' ability to generalize effectively. Previous approaches haveattempted to address this issue by introducing additional supervision duringtraining, often requiring costly human-annotated data that restrictsscalability. In this paper, we introduce a masked path modeling (MPM)objective, which pretrains an agent using self-collected data for downstreamnavigation tasks. Our proposed method involves allowing the agent to activelyexplore navigation environments without a specific goal and collect the pathsit traverses. Subsequently, we train the agent on this collected data toreconstruct the original path given a randomly masked subpath. This way, theagent can actively accumulate a diverse and substantial amount of data whilelearning conditional action generation. To evaluate the effectiveness of ourtechnique, we conduct experiments on various VLN datasets and demonstrate theversatility of MPM across different levels of instruction complexity. Ourresults exhibit significant improvements in success rates, with enhancements of1.32\%, 1.05\%, and 1.19\% on the val-unseen split of the Room-to-Room,Room-for-Room, and Room-across-Room datasets, respectively. Furthermore, weconduct an analysis that highlights the potential for additional improvementswhen the agent is allowed to explore unseen environments prior to testing.</description><author>Zi-Yi Dou, Feng Gao, Nanyun Peng</author><pubDate>Tue, 23 May 2023 18:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14268v1</guid></item><item><title>SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models</title><link>http://arxiv.org/abs/2305.14267v1</link><description>A potent class of generative models known as Diffusion Probabilistic Models(DPMs) has become prominent. A forward diffusion process adds gradually noiseto data, while a model learns to gradually denoise. Sampling from pre-trainedDPMs is obtained by solving differential equations (DE) defined by the learntmodel, a process which has shown to be prohibitively slow. Numerous efforts onspeeding-up this process have consisted on crafting powerful ODE solvers.Despite being quick, such solvers do not usually reach the optimal qualityachieved by available slow SDE solvers. Our goal is to propose SDE solvers thatreach optimal quality without requiring several hundreds or thousands of NFEsto achieve that goal. In this work, we propose Stochastic ExponentialDerivative-free Solvers (SEEDS), improving and generalizing ExponentialIntegrator approaches to the stochastic case on several frameworks. Aftercarefully analyzing the formulation of exact solutions of diffusion SDEs, wecraft SEEDS to analytically compute the linear part of such solutions. Inspiredby the Exponential Time-Differencing method, SEEDS uses a novel treatment ofthe stochastic components of solutions, enabling the analytical computation oftheir variance, and contains high-order terms allowing to reach optimal qualitysampling $\sim3$-$5\times$ faster than previous SDE methods. We validate ourapproach on several image generation benchmarks, showing that SEEDS outperformsor is competitive with previous SDE solvers. Contrary to the latter, SEEDS arederivative and training free, and we fully prove strong convergence guaranteesfor them.</description><author>Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, Nader Masmoudi</author><pubDate>Tue, 23 May 2023 18:19:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14267v1</guid></item><item><title>Active Learning Principles for In-Context Learning with Large Language Models</title><link>http://arxiv.org/abs/2305.14264v1</link><description>The remarkable advancements in large language models (LLMs) havesignificantly enhanced the performance in few-shot learning settings. By usingonly a small number of labeled examples, referred to as demonstrations, LLMscan effectively grasp the task at hand through in-context learning. However,the process of selecting appropriate demonstrations has received limitedattention in prior work. This paper addresses the issue of identifying the mostinformative demonstrations for few-shot learning by approaching it as apool-based Active Learning (AL) problem over a single iteration. Our objectiveis to investigate how AL algorithms can serve as effective demonstrationselection methods for in-context learning. We compare various standard ALalgorithms based on uncertainty, diversity, and similarity, and consistentlyobserve that the latter outperforms all other methods, including randomsampling. Notably, uncertainty sampling, despite its success in conventionalsupervised learning scenarios, performs poorly in this context. Our extensiveexperimentation involving a diverse range of GPT and OPT models across $24$classification and multi-choice tasks, coupled with thorough analysis,unambiguously demonstrates that in-context example selection through ALprioritizes high-quality examples that exhibit low uncertainty and bearsimilarity to the test examples.</description><author>Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane Dwivedi-Yu</author><pubDate>Tue, 23 May 2023 18:16:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14264v1</guid></item><item><title>LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages</title><link>http://arxiv.org/abs/2305.14263v1</link><description>Knowing the language of an input text/audio is a necessary first step forusing almost every natural language processing (NLP) tool such as taggers,parsers, or translation systems. Language identification is a well-studiedproblem, sometimes even considered solved; in reality, most of the world's 7000languages are not supported by current systems. This lack of representationaffects large-scale data mining efforts and further exacerbates data shortagefor low-resource languages. We take a step towards tackling the data bottleneckby compiling a corpus of over 50K parallel children's stories in 350+ languagesand dialects, and the computation bottleneck by building lightweighthierarchical models for language identification. Our data can serve asbenchmark data for language identification of short texts and for understudiedtranslation directions such as those between Indian or African languages. Ourproposed method, Hierarchical LIMIT, uses limited computation to expandcoverage into excluded languages while maintaining prediction quality.</description><author>Milind Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos</author><pubDate>Tue, 23 May 2023 18:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14263v1</guid></item><item><title>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval</title><link>http://arxiv.org/abs/2304.01982v2</link><description>Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]allow token-level interactions between queries and documents, and hence achievestate of the art on many information retrieval benchmarks. However, theirnon-linear scoring function cannot be scaled to millions of documents,necessitating a three-stage process for inference: retrieving initialcandidates via token retrieval, accessing all token vectors, and scoring theinitial candidate documents. The non-linear scoring function is applied overall token vectors of each candidate document, making the inference processcomplicated and slow. In this paper, we aim to simplify the multi-vectorretrieval by rethinking the role of token retrieval. We present XTR,ConteXtualized Token Retriever, which introduces a simple, yet novel, objectivefunction that encourages the model to retrieve the most important documenttokens first. The improvement to token retrieval allows XTR to rank candidatesonly using the retrieved tokens rather than all tokens in the document, andenables a newly designed scoring stage that is two-to-three orders of magnitudecheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances thestate-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysisconfirms our decision to revisit the token retrieval stage, as XTR demonstratesmuch better recall of the token retrieval stage compared to ColBERT.</description><author>Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, Vincent Y. Zhao</author><pubDate>Tue, 23 May 2023 18:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01982v2</guid></item><item><title>R2H: Building Multimodal Navigation Helpers that Respond to Help</title><link>http://arxiv.org/abs/2305.14260v1</link><description>The ability to assist humans during a navigation task in a supportive role iscrucial for intelligent agents. Such agents, equipped with environmentknowledge and conversational abilities, can guide individuals throughunfamiliar terrains by generating natural language responses to theirinquiries, grounded in the visual information of their surroundings. However,these multimodal conversational navigation helpers are still underdeveloped.This paper proposes a new benchmark, Respond to Help (R2H), to build multimodalnavigation helpers that can respond to help, based on existing dialog-basedembodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History(RDH), which assesses the helper agent's ability to generate informativeresponses based on a given dialog history, and (2) Respond during Interaction(RdI), which evaluates the helper agent's ability to maintain effective andconsistent cooperation with a task performer agent during navigation inreal-time. Furthermore, we propose a novel task-oriented multimodal responsegeneration model that can see and respond, named SeeRee, as the navigationhelper to guide the task performer in embodied tasks. Through both automaticand human evaluations, we show that SeeRee produces more effective andinformative responses than baseline methods in assisting the task performerwith different navigation tasks. Project website:https://sites.google.com/view/respond2help/home.</description><author>Yue Fan, Kaizhi Zheng, Jing Gu, Xin Eric Wang</author><pubDate>Tue, 23 May 2023 18:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14260v1</guid></item><item><title>Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery</title><link>http://arxiv.org/abs/2305.14259v1</link><description>Literature-Based Discovery (LBD) aims to discover new scientific knowledge bymining papers and generating hypotheses. Standard LBD is limited to predictingpairwise relations between discrete concepts (e.g., drug-disease links). LBDalso ignores critical contexts like experimental settings (e.g., a specificpatient population where a drug is evaluated) and background knowledge andmotivations that human scientists consider (e.g., to find a drug candidatewithout specific side effects). We address these limitations with a novelformulation of contextualized-LBD (C-LBD): generating scientific hypotheses innatural language, while grounding them in a context that controls thehypothesis search space. We present a new modeling framework using retrieval of``inspirations'' from a heterogeneous network of citations and knowledge graphrelations, and create a new dataset derived from papers. In automated and humanevaluations, our models improve over baselines, including powerful largelanguage models (LLMs), but also reveal challenges on the road to buildingmachines that generate new scientific knowledge.</description><author>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope</author><pubDate>Tue, 23 May 2023 18:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14259v1</guid></item><item><title>Weakly Supervised AUC Optimization: A Unified Partial AUC Approach</title><link>http://arxiv.org/abs/2305.14258v1</link><description>Since acquiring perfect supervision is usually difficult, real-world machinelearning tasks often confront inaccurate, incomplete, or inexact supervision,collectively referred to as weak supervision. In this work, we present WSAUC, aunified framework for weakly supervised AUC optimization problems, which coversnoisy label learning, positive-unlabeled learning, multi-instance learning, andsemi-supervised learning scenarios. Within the WSAUC framework, we first framethe AUC optimization problems in various weakly supervised scenarios as acommon formulation of minimizing the AUC risk on contaminated sets, anddemonstrate that the empirical risk minimization problems are consistent withthe true AUC. Then, we introduce a new type of partial AUC, specifically, thereversed partial AUC (rpAUC), which serves as a robust training objective forAUC maximization in the presence of contaminated labels. WSAUC offers auniversal solution for AUC optimization in various weakly supervised scenariosby maximizing the empirical rpAUC. Theoretical and experimental results undermultiple settings support the effectiveness of WSAUC on a range of weaklysupervised AUC optimization tasks.</description><author>Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou</author><pubDate>Tue, 23 May 2023 18:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14258v1</guid></item><item><title>Hierarchical Prompting Assists Large Language Model on Web Navigation</title><link>http://arxiv.org/abs/2305.14257v1</link><description>Large language models (LLMs) struggle on processing complicated observationsin interactive decision making. To alleviate this issue, we propose a simplehierarchical prompting approach. Diverging from previous prompting approachesthat always put the \emph{full} observation~(\eg a web page) to the prompt, wepropose to first construct an action-aware observation which is more\emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actorprompt then predicts the next action based on the summarized history. While ourmethod has broad applicability, we particularly demonstrate its efficacy in thecomplex domain of web navigation where a full observation often containsredundant and irrelevant information. Our approach outperforms the previousstate-of-the-art prompting mechanism with the same LLM by 6.2\% on task successrate, demonstrating its potential on interactive decision making tasks withlong observation traces.</description><author>Abishek Sridhar, Robert Lo, Frank F. Xu, Hao Zhu, Shuyan Zhou</author><pubDate>Tue, 23 May 2023 18:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14257v1</guid></item><item><title>Linear Cross-Lingual Mapping of Sentence Embeddings</title><link>http://arxiv.org/abs/2305.14256v1</link><description>Semantics of a sentence is defined with much less ambiguity than semantics ofa single word, and it should be better preserved by translation to anotherlanguage. If multilingual sentence embeddings intend to represent sentencesemantics, then the similarity between embeddings of any two sentences must beinvariant with respect to translation. Based on this suggestion, we consider asimple linear cross-lingual mapping as a possible improvement of themultilingual embeddings. We also consider deviation from orthogonalityconditions as a measure of deficiency of the embeddings.</description><author>Oleg Vasilyev, Fumika Isono, John Bohannon</author><pubDate>Tue, 23 May 2023 18:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14256v1</guid></item><item><title>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</title><link>http://arxiv.org/abs/2305.14251v1</link><description>Evaluating the factuality of long-form text generated by large languagemodels (LMs) is non-trivial because (1) generations often contain a mixture ofsupported and unsupported pieces of information, making binary judgments ofquality inadequate, and (2) human evaluation is time-consuming and costly. Inthis paper, we introduce FActScore (Factual precision in Atomicity Score), anew evaluation that breaks a generation into a series of atomic facts andcomputes the percentage of atomic facts supported by a reliable knowledgesource. We conduct an extensive human evaluation to obtain FActScores of peoplebiographies generated by several state-of-the-art commercial LMs --InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and reportnew analysis demonstrating the need for such a fine-grained score (e.g.,ChatGPT only achieves 58%). Since human evaluation is costly, we also introducean automated model that estimates FActScore, using retrieval and a stronglanguage model, with less than a 2% error rate. Finally, we use this automatedmetric to evaluate 6,500 generations from a new set of 13 recent LMs that wouldhave cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPTare more factual than public models, and Vicuna and Alpaca are some of the bestpublic models.</description><author>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi</author><pubDate>Tue, 23 May 2023 18:06:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14251v1</guid></item><item><title>Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling</title><link>http://arxiv.org/abs/2304.05365v4</link><description>There is a growing interest in using reinforcement learning (RL) topersonalize sequences of treatments in digital health to support users inadopting healthier behaviors. Such sequential decision-making problems involvedecisions about when to treat and how to treat based on the user's context(e.g., prior activity level, location, etc.). Online RL is a promisingdata-driven approach for this problem as it learns based on each user'shistorical responses and uses that knowledge to personalize these decisions.However, to decide whether the RL algorithm should be included in an``optimized'' intervention for real-world deployment, we must assess the dataevidence indicating that the RL algorithm is actually personalizing thetreatments to its users. Due to the stochasticity in the RL algorithm, one mayget a false impression that it is learning in certain states and using thislearning to provide specific treatments. We use a working definition ofpersonalization and introduce a resampling-based methodology for investigatingwhether the personalization exhibited by the RL algorithm is an artifact of theRL algorithm stochasticity. We illustrate our methodology with a case study byanalyzing the data from a physical activity clinical trial called HeartSteps,which included the use of an online RL algorithm. We demonstrate how ourapproach enhances data-driven truth-in-advertising of algorithm personalizationboth across all users as well as within specific users in the study.</description><author>Susobhan Ghosh, Raphael Kim, Prasidh Chhabria, Raaz Dwivedi, Predrag Klasnja, Peng Liao, Kelly Zhang, Susan Murphy</author><pubDate>Tue, 23 May 2023 18:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05365v4</guid></item><item><title>Language Models with Rationality</title><link>http://arxiv.org/abs/2305.14250v1</link><description>While large language models (LLMs) are proficient at question-answering (QA),the dependencies between their answers and other "beliefs" they may have aboutthe world are typically unstated, and may even be in conflict. Our goal is touncover such dependencies and reduce inconsistencies among them, so thatanswers are supported by faithful, system-believed chains of reasoning drawnfrom a consistent network of beliefs. Our approach, which we call REFLEX, is toadd a "rational", self-reflecting layer on top of the LLM. First, given aquestion, we construct a belief graph using a backward-chaining process tomaterialize relevant model "beliefs" (including beliefs about answercandidates) and the inferential relationships between them. Second, we identifyand minimize contradictions in that graph using a formal constraint reasoner.We find that REFLEX significantly improves consistency (by 8%-11% absolute)without harming overall answer accuracy, resulting in answers supported byfaithful chains of reasoning drawn from a more consistent belief system. Thissuggests a new style of system architecture, in which an LLM extended with arational layer of self-reflection can repair latent inconsistencies within theLLM alone.</description><author>Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schutze, Peter Clark</author><pubDate>Tue, 23 May 2023 18:04:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14250v1</guid></item><item><title>Evaluation of the MACE Force Field Architecture: from Medicinal Chemistry to Materials Science</title><link>http://arxiv.org/abs/2305.14247v1</link><description>The MACE architecture represents the state of the art in the field of machinelearning force fields for a variety of in-domain, extrapolation and low-dataregime tasks. In this paper, we further evaluate MACE by fitting models forpublished benchmark datasets. We show that MACE generally outperformsalternatives for a wide range of systems from amorphous carbon and generalsmall molecule organic chemistry to large molecules and liquid water. Wedemonstrate the capabilities of the model on tasks ranging from constrainedgeometry optimisation to molecular dynamics simulations and find excellentperformance across all tested domains. We show that MACE is very dataefficient, and can reproduce experimental molecular vibrational spectra whentrained on as few as 50 randomly selected reference configurations. We furtherdemonstrate that the strictly local atom-centered model is sufficient for suchtasks even in the case of large molecules and weakly interacting molecularassemblies.</description><author>David Peter Kovacs, Ilyes Batatia, Eszter Sara Arany, Gabor Csanyi</author><pubDate>Tue, 23 May 2023 18:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14247v1</guid></item><item><title>Modeling Empathic Similarity in Personal Narratives</title><link>http://arxiv.org/abs/2305.14246v1</link><description>The most meaningful connections between people are often fostered throughexpression of shared vulnerability and emotional experiences in personalnarratives. We introduce a new task of identifying similarity in personalstories based on empathic resonance, i.e., the extent to which two peopleempathize with each others' experiences, as opposed to raw semantic or lexicalsimilarity, as has predominantly been studied in NLP. Using insights fromsocial psychology, we craft a framework that operationalizes empathicsimilarity in terms of three key features of stories: main events, emotionaltrajectories, and overall morals or takeaways. We create EmpathicStories, adataset of 1,500 personal stories annotated with our empathic similarityfeatures, and 2,000 pairs of stories annotated with empathic similarity scores.Using our dataset, we fine-tune a model to compute empathic similarity of storypairs, and show that this outperforms semantic similarity models on automatedcorrelation and retrieval metrics. Through a user study with 150 participants,we also assess the effect our model has on retrieving stories that usersempathize with, compared to naive semantic similarity-based retrieval, and findthat participants empathized significantly more with stories retrieved by ourmodel. Our work has strong implications for the use of empathy-aware models tofoster human connection and empathy between people.</description><author>Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, Cynthia Breazeal</author><pubDate>Tue, 23 May 2023 18:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14246v1</guid></item><item><title>Spatial-temporal Prompt Learning for Federated Weather Forecasting</title><link>http://arxiv.org/abs/2305.14244v1</link><description>Federated weather forecasting is a promising collaborative learning frameworkfor analyzing meteorological data across participants from different countriesand regions, thus embodying a global-scale real-time weather data predictiveanalytics platform to tackle climate change. This paper is to model themeteorological data in a federated setting where many distributed low-resourcedsensors are deployed in different locations. Specifically, we model thespatial-temporal weather data into a federated prompt learning framework thatleverages lightweight prompts to share meaningful representation and structuralknowledge among participants. Prompts-based communication allows the server toestablish the structural topology relationships among participants and furtherexplore the complex spatial-temporal correlations without transmitting privatedata while mitigating communication overhead. Moreover, in addition to aglobally shared large model at the server, our proposed method enables eachparticipant to acquire a personalized model that is highly customized to tackleclimate changes in a specific geographic area. We have demonstrated theeffectiveness of our method on classical weather forecasting tasks by utilizingthree spatial-temporal multivariate time-series weather data.</description><author>Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, Jing Jiang</author><pubDate>Tue, 23 May 2023 17:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14244v1</guid></item><item><title>Training Transitive and Commutative Multimodal Transformers with LoReTTa</title><link>http://arxiv.org/abs/2305.14243v1</link><description>Collecting a multimodal dataset with two paired modalities A and B or B and Cis difficult in practice. Obtaining a dataset with three aligned modalities A,B, and C is even more challenging. For example, some public medical datasetshave only genetic sequences and microscopic images for one patient, and onlygenetic sequences and radiological images for another - but no dataset includesboth microscopic and radiological images for the same patient. This makes itdifficult to integrate and combine all modalities into a large pre-trainedneural network. We introduce LoReTTa (Linking mOdalities with a tRansitive andcommutativE pre-Training sTrAtegy) to address this understudied problem. Ourself-supervised framework combines causal masked modeling with the rules ofcommutativity and transitivity to transition within and between differentmodalities. Thus, it can model the relation A -&gt; C with A -&gt; B -&gt; C. Given adataset containing only the disjoint combinations (A, B) and (B, C), we showthat a transformer pre-trained with LoReTTa can handle any modality combinationat inference time, including the never-seen pair (A, C) and the triplet (A, B,C). We evaluate our approach on a multimodal dataset derived from MNISTcontaining speech, vision, and language, as well as a real-world medicaldataset containing mRNA, miRNA, and RPPA samples from TCGA. Compared totraditional pre-training methods, we observe up to a 100-point reduction inperplexity for autoregressive generation tasks and up to a 15% improvement inclassification accuracy for previously unseen modality pairs during thepre-training phase.</description><author>Manuel Tran, Amal Lahiani, Yashin Dicente Cid, Fabian J. Theis, Tingying Peng, Eldad Klaiman</author><pubDate>Tue, 23 May 2023 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14243v1</guid></item><item><title>Revisiting Machine Translation for Cross-lingual Classification</title><link>http://arxiv.org/abs/2305.14240v1</link><description>Machine Translation (MT) has been widely used for cross-lingualclassification, either by translating the test set into English and runninginference with a monolingual model (translate-test), or translating thetraining set into the target languages and finetuning a multilingual model(translate-train). However, most research in the area focuses on themultilingual models rather than the MT component. We show that, by using astronger MT system and mitigating the mismatch between training on originaltext and running inference on machine translated text, translate-test can dosubstantially better than previously assumed. The optimal approach, however, ishighly task dependent, as we identify various sources of cross-lingual transfergap that affect different tasks and approaches differently. Our work calls intoquestion the dominance of multilingual models for cross-lingual classification,and prompts to pay more attention to MT-based baselines.</description><author>Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, Luke Zettlemoyer</author><pubDate>Tue, 23 May 2023 17:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14240v1</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v1</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we investigate a new learningparadigm of text summarization models that considers the LLMs as the referenceor the gold-standard oracle on commonly used summarization datasets such as theCNN/DailyMail dataset. To examine the standard practices that are aligned withthe new learning setting, we propose a novel training method that is based oncontrastive learning with LLMs as a summarization quality evaluator. For thisreward-based training method, we investigate two different methods of utilizingLLMs for summary quality evaluation, namely GPTScore and GPTRank. Ourexperiments on the CNN/DailyMail dataset demonstrate that smaller summarizationmodels trained by our proposed method can achieve performance equal to orsurpass that of the reference LLMs, as evaluated by the LLMs themselves. Thisunderscores the efficacy of our proposed paradigm in enhancing modelperformance over the standard maximum likelihood estimation (MLE) trainingmethod, and its efficiency since it only requires a small budget to access theLLMs. We release the training scripts, model outputs, and LLM-based evaluationresults to facilitate future studies.</description><author>Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Tue, 23 May 2023 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v1</guid></item><item><title>HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision</title><link>http://arxiv.org/abs/2305.14237v1</link><description>Explainable multi-hop question answering (QA) not only predicts answers butalso identifies rationales, i. e. subsets of input sentences used to derive theanswers. This problem has been extensively studied under the supervisedsetting, where both answer and rationale annotations are given. Becauserationale annotations are expensive to collect and not always available, recentefforts have been devoted to developing methods that do not rely on supervisionfor rationales. However, such methods have limited capacities in modelinginteractions between sentences, let alone reasoning across multiple documents.This work proposes a principled, probabilistic approach for trainingexplainable multi-hop QA systems without rationale supervision. Our approachperforms multi-hop reasoning by explicitly modeling rationales as sets,enabling the model to capture interactions between documents and sentenceswithin a document. Experimental results show that our approach is more accurateat selecting rationales than the previous methods, while maintaining similaraccuracy in predicting answers.</description><author>Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush</author><pubDate>Tue, 23 May 2023 17:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14237v1</guid></item><item><title>REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos</title><link>http://arxiv.org/abs/2305.14236v1</link><description>Reconstructing dynamic 3D garment surfaces with open boundaries frommonocular videos is an important problem as it provides a practical andlow-cost solution for clothes digitization. Recent neural rendering methodsachieve high-quality dynamic clothed human reconstruction results frommonocular video, but these methods cannot separate the garment surface from thebody. Moreover, despite existing garment reconstruction methods based onfeature curve representation demonstrating impressive results for garmentreconstruction from a single image, they struggle to generate temporallyconsistent surfaces for the video input. To address the above limitations, inthis paper, we formulate this task as an optimization problem of 3D garmentfeature curves and surface reconstruction from monocular video. We introduce anovel approach, called REC-MV, to jointly optimize the explicit feature curvesand the implicit signed distance field (SDF) of the garments. Then the opengarment meshes can be extracted via garment template registration in thecanonical space. Experiments on multiple casually captured datasets show thatour approach outperforms existing methods and can produce high-quality dynamicgarment surfaces. The source code is available athttps://github.com/GAP-LAB-CUHK-SZ/REC-MV.</description><author>Lingteng Qiu, Guanying Chen, Jiapeng Zhou, Mutian Xu, Junle Wang, Xiaoguang Han</author><pubDate>Tue, 23 May 2023 17:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14236v1</guid></item><item><title>Multilingual Large Language Models Are Not (Yet) Code-Switchers</title><link>http://arxiv.org/abs/2305.14235v1</link><description>Multilingual Large Language Models (LLMs) have recently shown greatcapability in various tasks, exhibiting state-of-the-art performance usingfew-shot or zero-shot prompting methods. While these models have beenextensively studied in tasks where inputs are assumed to be in a singlelanguage, less attention has been paid to exploring their performance wheninputs involve code-switching (CSW). In this paper, we provide an extensiveempirical study of various multilingual LLMs and benchmark their performance inthree tasks: sentiment analysis, machine translation, and word-level languageidentification. Our findings indicate that despite multilingual LLMs showingpromising outcomes in certain tasks when using zero-/few-shot prompting, theirperformance still falls short on average when compared to smaller finetunedmodels. We argue that LLMs that are "multilingual" are not necessarilycode-switching compatible and extensive future research is required to fullybridge this gap.</description><author>Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Alham Fikri Aji</author><pubDate>Tue, 23 May 2023 17:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14235v1</guid></item><item><title>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</title><link>http://arxiv.org/abs/2305.14233v1</link><description>Fine-tuning on instruction data has been widely validated as an effectivepractice for implementing chat language models like ChatGPT. Scaling thediversity and quality of such data, although straightforward, stands a greatchance of leading to improved performance. This paper aims to improve the upperbound of open-source models further. We first provide a systematicallydesigned, diverse, informative, large-scale dataset of instructionalconversations, UltraChat, which does not involve human queries. Our objectiveis to capture the breadth of interactions that a human might have with an AIassistant and employs a comprehensive framework to generate multi-turnconversation iteratively. UltraChat contains 1.5 million high-qualitymulti-turn dialogues and covers a wide range of topics and instructions. Ourstatistical analysis of UltraChat reveals its superiority in various keymetrics, including scale, average length, diversity, coherence, etc.,solidifying its position as a leading open-source dataset. Building uponUltraChat, we fine-tune a LLaMA model to create a powerful conversationalmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistentlyoutperforms other open-source models, including Vicuna, the previouslyrecognized state-of-the-art open-source model. The dataset and the model willbe publicly released\footnote{\url{https://github.com/thunlp/UltraChat}}.</description><author>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou</author><pubDate>Tue, 23 May 2023 17:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14233v1</guid></item><item><title>Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding</title><link>http://arxiv.org/abs/2305.14232v1</link><description>Scientific literature understanding tasks have gained significant attentiondue to their potential to accelerate scientific discovery. Pre-trained languagemodels (LMs) have shown effectiveness in these tasks, especially when tuned viacontrastive learning. However, jointly utilizing pre-training data acrossmultiple heterogeneous tasks (e.g., extreme classification, citationprediction, and literature search) remains largely unexplored. To bridge thisgap, we propose a multi-task contrastive learning framework, SciMult, with afocus on facilitating common knowledge sharing across different scientificliterature understanding tasks while preventing task-specific skills frominterfering with each other. To be specific, we explore two techniques --task-aware specialization and instruction tuning. The former adopts aMixture-of-Experts Transformer architecture with task-aware sub-layers; thelatter prepends task-specific instructions to the input text so as to producetask-aware outputs. Extensive experiments on a comprehensive collection ofbenchmark datasets verify the effectiveness of our task-aware specializationstrategy in various tasks, where we outperform state-of-the-art scientific LMs.</description><author>Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, Jianfeng Gao</author><pubDate>Tue, 23 May 2023 17:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14232v1</guid></item><item><title>Exploring Representational Disparities Between Multilingual and Bilingual Translation Models</title><link>http://arxiv.org/abs/2305.14230v1</link><description>Multilingual machine translation has proven immensely useful for low-resourceand zero-shot language pairs. However, language pairs in multilingual modelssometimes see worse performance than in bilingual models, especially whentranslating in a one-to-many setting. To understand why, we examine thegeometric differences in the representations from bilingual models versus thosefrom one-to-many multilingual models. Specifically, we evaluate the isotropy ofthe representations, to measure how well they utilize the dimensions in theirunderlying vector space. Using the same evaluation data in both models, we findthat multilingual model decoder representations tend to be less isotropic thanbilingual model decoder representations. Additionally, we show that much of theanisotropy in multilingual decoder representations can be attributed tomodeling language-specific information, therefore limiting remainingrepresentational capacity.</description><author>Neha Verma, Kenton Murray, Kevin Duh</author><pubDate>Tue, 23 May 2023 17:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14230v1</guid></item><item><title>Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer</title><link>http://arxiv.org/abs/2305.09126v2</link><description>A novel problem of improving causal effect estimation accuracy with the helpof knowledge transfer under the same covariate (or feature) space setting,i.e., homogeneous transfer learning (TL), is studied, referred to as theTransfer Causal Learning (TCL) problem. While most recent efforts in adaptingTL techniques to estimate average causal effect (ACE) have been focused on theheterogeneous covariate space setting, those methods are inadequate fortackling the TCL problem since their algorithm designs are based on thedecomposition into shared and domain-specific covariate spaces. To address thisissue, we propose a generic framework called $\ell_1$-TCL, which incorporates$\ell_1$ regularized TL for nuisance parameter estimation and downstreamplug-in ACE estimators, including outcome regression, inverse probabilityweighted, and doubly robust estimators. Most importantly, with the help ofLasso for high-dimensional regression, we establish non-asymptotic recoveryguarantees for the generalized linear model (GLM) under the sparsity assumptionfor the proposed $\ell_1$-TCL. From an empirical perspective, $\ell_1$-TCL is ageneric learning framework that can incorporate not only GLM but also manyrecently developed non-parametric methods, which can enhance robustness tomodel mis-specification. We demonstrate this empirical benefit throughextensive numerical simulation by incorporating both GLM and recent neuralnetwork-based approaches in $\ell_1$-TCL, which shows improved performancecompared with existing TL approaches for ACE estimation. Furthermore, our$\ell_1$-TCL framework is subsequently applied to a real study, revealing thatvasopressor therapy could prevent 28-day mortality within septic patients,which all baseline approaches fail to show.</description><author>Song Wei, Ronald Moore, Hanyu Zhang, Yao Xie, Rishikesan Kamaleswaran</author><pubDate>Tue, 23 May 2023 17:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09126v2</guid></item><item><title>Provably Learning Object-Centric Representations</title><link>http://arxiv.org/abs/2305.14229v1</link><description>Learning structured representations of the visual world in terms of objectspromises to significantly improve the generalization abilities of currentmachine learning models. While recent efforts to this end have shown promisingempirical progress, a theoretical account of when unsupervised object-centricrepresentation learning is possible is still lacking. Consequently,understanding the reasons for the success of existing object-centric methods aswell as designing new theoretically grounded methods remains challenging. Inthe present work, we analyze when object-centric representations can provablybe learned without supervision. To this end, we first introduce two assumptionson the generative process for scenes comprised of several objects, which wecall compositionality and irreducibility. Under this generative process, weprove that the ground-truth object representations can be identified by aninvertible and compositional inference model, even in the presence ofdependencies between objects. We empirically validate our results throughexperiments on synthetic data. Finally, we provide evidence that our theoryholds predictive power for existing object-centric models by showing a closecorrespondence between models' compositionality and invertibility and theirempirical identifiability.</description><author>Jack Brady, Roland S. Zimmermann, Yash Sharma, Bernhard Schölkopf, Julius von Kügelgen, Wieland Brendel</author><pubDate>Tue, 23 May 2023 17:44:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14229v1</guid></item><item><title>ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media</title><link>http://arxiv.org/abs/2305.14225v1</link><description>Considerable advancements have been made to tackle the misrepresentation ofinformation derived from reference articles in the domains of fact-checking andfaithful summarization. However, an unaddressed aspect remains - theidentification of social media posts that manipulate information withinassociated news articles. This task presents a significant challenge, primarilydue to the prevalence of personal opinions in such posts. We present a noveltask, identifying manipulation of news on social media, which aims to detectmanipulation in social media posts and identify manipulated or insertedinformation. To study this task, we have proposed a data collection schema andcurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets andcorresponding articles. Our analysis demonstrates that this task is highlychallenging, with large language models (LLMs) yielding unsatisfactoryperformance. Additionally, we have developed a simple yet effective basic modelthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we haveconducted an exploratory analysis of human-written tweets, unveiling intriguingconnections between manipulation and the domain and factuality of newsarticles, as well as revealing that manipulated sentences are more likely toencapsulate the main story or consequences of a news outlet.</description><author>Kung-Hsiang Huang, Hou Pong Chan, Kathleen McKeown, Heng Ji</author><pubDate>Tue, 23 May 2023 17:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14225v1</guid></item><item><title>Large Language Models are Better Reasoners with Self-Verification</title><link>http://arxiv.org/abs/2212.09561v3</link><description>Recently, with the chain of thought (CoT) prompting, large language models(LLMs), e.g., GPT-3, have shown strong reasoning ability in several naturallanguage processing tasks such as arithmetic, commonsense, and logicalreasoning. However, LLMs with CoT require multi-step prompting and multi-tokenprediction, which is highly sensitive to individual mistakes and vulnerable toerror accumulation. The above issues make the LLMs need the ability to verifythe answers. In fact, after inferring conclusions in some thinking decisiontasks, people often check them by re-verifying steps to avoid some mistakes. Inthis paper, we propose and prove that LLMs also have similar self-verificationabilities. We take the conclusion obtained by CoT as one of the conditions forsolving the original problem. By taking turns masking the original conditionsand predicting their results, we calculate an explainable answer verificationscore based on whether the re-predicted conditions are correct. Experimentalresults demonstrate that the proposed method can improve the reasoningperformance on various arithmetic, commonsense, and logical reasoning datasets.</description><author>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao</author><pubDate>Tue, 23 May 2023 17:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09561v3</guid></item><item><title>Granger Causal Chain Discovery for Sepsis-Associated Derangements via Continuous-Time Hawkes Processes</title><link>http://arxiv.org/abs/2209.04480v5</link><description>Modern health care systems are conducting continuous, automated surveillanceof the electronic medical record (EMR) to identify adverse events withincreasing frequency; however, many events such as sepsis do not haveelucidated prodromes (i.e., event chains) that can be used to identify andintercept the adverse event early in its course. Clinically relevant andinterpretable results require a framework that can (i) infer temporalinteractions across multiple patient features found in EMR data (e.g., Labs,vital signs, etc.) and (ii) identify patterns that precede and are specific toan impending adverse event (e.g., sepsis). In this work, we propose a linearmultivariate Hawkes process model, coupled with ReLU link function, to recovera Granger Causal (GC) graph with both exciting and inhibiting effects. Wedevelop a scalable two-phase gradient-based method to obtain a maximumsurrogate-likelihood estimator, which is shown to be effective via extensivenumerical simulation. Our method is subsequently extended to a data set ofpatients admitted to Grady hospital system in Atlanta, GA, USA, where theestimated GC graph identifies several highly interpretable GC chains thatprecede sepsis. The code is available at\url{https://github.com/SongWei-GT/two-phase-MHP}.</description><author>Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran</author><pubDate>Tue, 23 May 2023 17:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04480v5</guid></item><item><title>mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations</title><link>http://arxiv.org/abs/2305.14224v1</link><description>Multilingual sequence-to-sequence models perform poorly with increasedlanguage coverage and fail to consistently generate text in the correct targetlanguage in few-shot settings. To address these challenges, we propose mmT5, amodular multilingual sequence-to-sequence model. mmT5 utilizeslanguage-specific modules during pre-training, which disentanglelanguage-specific information from language-agnostic information. We identifyrepresentation drift during fine-tuning as a key limitation of modulargenerative models and develop strategies that enable effective zero-shottransfer. Our model outperforms mT5 at the same parameter sizes by a largemargin on representative natural language understanding and generation tasks in40+ languages. Compared to mT5, mmT5 raises the rate of generating text in thecorrect language under zero-shot settings from 7% to 99%, thereby greatlyalleviating the source language hallucination problem.</description><author>Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, Sebastian Ruder</author><pubDate>Tue, 23 May 2023 17:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14224v1</guid></item><item><title>Co-Learning Empirical Games and World Models</title><link>http://arxiv.org/abs/2305.14223v1</link><description>Game-based decision-making involves reasoning over both world dynamics andstrategic interactions among the agents. Typically, empirical models capturingthese respective aspects are learned and used separately. We investigate thepotential gain from co-learning these elements: a world model for dynamics andan empirical game for strategic interactions. Empirical games drive worldmodels toward a broader consideration of possible game dynamics induced by adiversity of strategy profiles. Conversely, world models guide empirical gamesto efficiently discover new strategies through planning. We demonstrate thesebenefits first independently, then in combination as realized by a newalgorithm, Dyna-PSRO, that co-learns an empirical game and a world model. Whencompared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO isfound to compute lower regret solutions on partially observable general-sumgames. In our experiments, Dyna-PSRO also requires substantially fewerexperiences than PSRO, a key algorithmic advantage for settings wherecollecting player-game interaction data is a cost-limiting factor.</description><author>Max Olan Smith, Michael P. Wellman</author><pubDate>Tue, 23 May 2023 17:37:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14223v1</guid></item><item><title>Using In-Context Learning to Improve Dialogue Safety</title><link>http://arxiv.org/abs/2302.00871v2</link><description>While large neural-based conversational models have become increasinglyproficient dialogue agents, recent work has highlighted safety issues withthese systems. For example, these systems can be goaded into generating toxiccontent, which often perpetuates social biases or stereotypes. We investigate aretrieval-based method for reducing bias and toxicity in responses fromchatbots. It uses in-context learning to steer a model towards safergenerations. Concretely, to generate a response to an unsafe dialogue context,we retrieve demonstrations of safe responses to similar dialogue contexts. Wefind our method performs competitively with strong baselines without requiringtraining. For instance, using automatic evaluation, we find our best fine-tunedbaseline only generates safe responses to unsafe dialogue contexts fromDiaSafety 4.04% more than our approach. Finally, we also propose a re-rankingprocedure which can further improve response safeness.</description><author>Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, Dilek Hakkani-Tür</author><pubDate>Tue, 23 May 2023 17:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00871v2</guid></item><item><title>Question Answering as Programming for Solving Time-Sensitive Questions</title><link>http://arxiv.org/abs/2305.14221v1</link><description>In this work we try to apply Large Language Models (LLMs) to reframe theQuestion Answering task as Programming (QAaP). Due to the inherent dynamicnature of the real world, factual questions frequently involve a symbolicconstraint: time, solving these questions necessitates not only extensive worldknowledge, but also advanced reasoning ability to satisfy the temporalconstraints. Despite the remarkable intelligence exhibited by LLMs in variousNLP tasks, our experiments reveal that the aforementioned problems continue topose a significant challenge to existing LLMs. To solve these time-sensitivefactual questions, considering that modern LLMs possess superior ability inboth natural language understanding and programming,we endeavor to leverageLLMs to represent diversely expressed text as well-structured code, and therebygrasp the desired knowledge along with the underlying symbolic constraint.</description><author>Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang</author><pubDate>Tue, 23 May 2023 17:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14221v1</guid></item><item><title>DUBLIN -- Document Understanding By Language-Image Network</title><link>http://arxiv.org/abs/2305.14218v1</link><description>Visual document understanding is a complex task that involves analyzing boththe text and the visual elements in document images. Existing models often relyon manual feature engineering or domain-specific pipelines, which limit theirgeneralization ability across different document types and languages. In thispaper, we propose DUBLIN, which is pretrained on webpages using three novelobjectives that leverage the spatial and semantic information in the documentimages: Masked Document Content Generation Task, Bounding Box Task, andRendered Question Answering Task. We evaluate our model on several benchmarks,such as Web-Based Structural Reading Comprehension, Document Visual QuestionAnswering, Key Information Extraction, Diagram Understanding, and TableQuestion Answering. We show that our model achieves competitive or betterresults than the state-of-the-art models on these tasks. In particular, we showthat DUBLIN is the first pixel-based model to achieve an EM of 77.75 and F1 of84.25 on the WebSRC dataset. We also show that our model outperforms thecurrent pixel-based SOTA models on DocVQA and AI2D datasets by significantmargins, 2% and 21% increase in performance, respectively. Also, DUBLIN is thefirst ever pixel-based model which achieves comparable to text-based SOTAmethods on XFUND dataset for Semantic Entity Recognition showcasing itsmultilingual capability. Moreover, we create new baselines for text-baseddatasets by rendering them as document images and applying this model.</description><author>Kriti Aggarwal, Aditi Khandelwal, Kumar Tanmay, Owais Mohammed Khan, Qiang Liu, Monojit Choudhury, Subhojit Som, Vishrav Chaudhary, Saurabh Tiwary</author><pubDate>Tue, 23 May 2023 17:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14218v1</guid></item><item><title>A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces</title><link>http://arxiv.org/abs/2211.07292v2</link><description>Recent advancements in the domain of text-to-image synthesis have culminatedin a multitude of enhancements pertaining to quality, fidelity, and diversity.Contemporary techniques enable the generation of highly intricate visuals whichrapidly approach near-photorealistic quality. Nevertheless, as progress isachieved, the complexity of these methodologies increases, consequentlyintensifying the comprehension barrier between individuals within the field andthose external to it. In an endeavor to mitigate this disparity, we propose a streamlined approachfor text-to-image generation, which encompasses both the training paradigm andthe sampling process. Despite its remarkable simplicity, our method yieldsaesthetically pleasing images with few sampling iterations, allows forintriguing ways for conditioning the model, and imparts advantages absent instate-of-the-art techniques. To demonstrate the efficacy of this approach inachieving outcomes comparable to existing works, we have trained a one-billionparameter text-conditional model, which we refer to as "Paella". In theinterest of fostering future exploration in this field, we have made our sourcecode and models publicly accessible for the research community.</description><author>Dominic Rampas, Pablo Pernias, Marc Aubreville</author><pubDate>Tue, 23 May 2023 17:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07292v2</guid></item><item><title>Constrained Proximal Policy Optimization</title><link>http://arxiv.org/abs/2305.14216v1</link><description>The problem of constrained reinforcement learning (CRL) holds significantimportance as it provides a framework for addressing critical safetysatisfaction concerns in the field of reinforcement learning (RL). However,with the introduction of constraint satisfaction, the current CRL methodsnecessitate the utilization of second-order optimization or primal-dualframeworks with additional Lagrangian multipliers, resulting in increasedcomplexity and inefficiency during implementation. To address these issues, wepropose a novel first-order feasible method named Constrained Proximal PolicyOptimization (CPPO). By treating the CRL problem as a probabilistic inferenceproblem, our approach integrates the Expectation-Maximization framework tosolve it through two steps: 1) calculating the optimal policy distributionwithin the feasible region (E-step), and 2) conducting a first-order update toadjust the current policy towards the optimal policy obtained in the E-step(M-step). We establish the relationship between the probability ratios and KLdivergence to convert the E-step into a convex optimization problem.Furthermore, we develop an iterative heuristic algorithm from a geometricperspective to solve this problem. Additionally, we introduce a conservativeupdate mechanism to overcome the constraint violation issue that occurs in theexisting feasible region method. Empirical evaluations conducted in complex anduncertain environments validate the effectiveness of our proposed method, as itperforms at least as well as other baselines.</description><author>Chengbin Xuan, Feng Zhang, Faliang Yin, Hak-Keung Lam</author><pubDate>Tue, 23 May 2023 17:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14216v1</guid></item><item><title>Exploring Chain-of-Thought Style Prompting for Text-to-SQL</title><link>http://arxiv.org/abs/2305.14215v1</link><description>Conventional supervised approaches for text-to-SQL parsing often requirelarge amounts of annotated data, which is costly to obtain in practice.Recently, in-context learning with large language models (LLMs) has caughtincreasing attention due to its superior few-shot performance in a wide rangeof tasks. However, most attempts to use in-context learning for text-to-SQLparsing still lag behind supervised methods. We hypothesize that theunder-performance is because text-to-SQL parsing requires complex, multi-stepreasoning. In this paper, we systematically study how to enhance the reasoningability of LLMs for text-to-SQL parsing through chain-of-thought (CoT) stylepromptings including CoT prompting and Least-to-Most prompting. Our experimentsdemonstrate that iterative prompting as in Least-to-Most prompting may beunnecessary for text-to-SQL parsing and directly applying existing CoT styleprompting methods leads to error propagation issues. By improving multi-stepreasoning while avoiding much detailed information in the reasoning steps whichmay lead to error propagation, our new method outperforms existing ones by 2.4point absolute gains on the Spider development set.</description><author>Chang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, Huan Sun</author><pubDate>Tue, 23 May 2023 17:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14215v1</guid></item><item><title>CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models</title><link>http://arxiv.org/abs/2305.14214v1</link><description>While many languages possess processes of joining two or more words to createcompound words, previous studies have been typically limited only to languageswith excessively productive compound formation (e.g., German, Dutch) and thereis no public dataset containing compound and non-compound words across a largenumber of languages. In this work, we systematically study decompounding, thetask of splitting compound words into their constituents, at a wide scale. Wefirst address the data gap by introducing a dataset of 255k compound andnon-compound words across 56 diverse languages obtained from Wiktionary. Wethen use this dataset to evaluate an array of Large Language Models (LLMs) onthe decompounding task. We find that LLMs perform poorly, especially on wordswhich are tokenized unfavorably by subword tokenization. We thus introduce anovel methodology to train dedicated models for decompounding. The proposedtwo-stage procedure relies on a fully self-supervised objective in the firststage, while the second, supervised learning stage optionally fine-tunes themodel on the annotated Wiktionary data. Our self-supervised models outperformthe prior best unsupervised decompounding models by 13.9% accuracy on average.Our fine-tuned models outperform all prior (language-specific) decompoundingtools. Furthermore, we use our models to leverage decompounding during thecreation of a subword tokenizer, which we refer to as CompoundPiece.CompoundPiece tokenizes compound words more favorably on average, leading toimproved performance on decompounding over an otherwise equivalent model usingSentencePiece tokenization.</description><author>Benjamin Minixhofer, Jonas Pfeiffer, Ivan Vulić</author><pubDate>Tue, 23 May 2023 17:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14214v1</guid></item><item><title>Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database</title><link>http://arxiv.org/abs/2305.14211v1</link><description>In Textual question answering (TQA) systems, complex questions often requireretrieving multiple textual fact chains with multiple reasoning steps. Whileexisting benchmarks are limited to single-chain or single-hop retrievalscenarios. In this paper, we propose to conduct Graph-Hop -- a novelmulti-chains and multi-hops retrieval and reasoning paradigm in complexquestion answering. We construct a new benchmark called ReasonGraphQA, whichprovides explicit and fine-grained evidence graphs for complex questions tosupport interpretable reasoning, comprehensive and detailed reasoning. AndReasonGraphQA also shows an advantage in reasoning diversity and scale.Moreover, We propose a strong graph-hop baseline called Bidirectional GraphRetrieval (BGR) method for generating an explanation graph of textual evidencein knowledge reasoning and question answering. We have thoroughly evaluatedexisting evidence retrieval and reasoning models on the ReasonGraphQA.Experiments highlight Graph-Hop is a promising direction for answering complexquestions, but it still has certain limitations. We have further studiedmitigation strategies to meet these challenges and discuss future directions.</description><author>Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao</author><pubDate>Tue, 23 May 2023 17:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14211v1</guid></item><item><title>Skill-Based Few-Shot Selection for In-Context Learning</title><link>http://arxiv.org/abs/2305.14210v1</link><description>In-Context learning is the paradigm that adapts large language models todownstream tasks by providing a few examples. Few-shot selection -- selectingappropriate examples for each test instance separately -- is important forin-context learning. In this paper, we propose Skill-KNN, a skill-basedfew-shot selection method for in-context learning. The key advantages ofSkill-KNN include: (1) it addresses the problem that existing methods based onpre-trained embeddings can be easily biased by surface natural languagefeatures that are not important for the target task; (2) it does not requiretraining or fine-tuning of any models, making it suitable for frequentlyexpanding or changing example banks. The key insight is to optimize the inputsfed into the embedding model, rather than tuning the model itself. Technically,Skill-KNN generates the skill-based representations for each test case andcandidate example by utilizing a pre-processing few-shot prompting, thuseliminating unimportant surface features. Experimental results across fourcross-domain semantic parsing tasks and four backbone models show thatSkill-KNN significantly outperforms existing methods.</description><author>Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Weizhu Chen, Jian-Guang Lou</author><pubDate>Tue, 23 May 2023 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14210v1</guid></item><item><title>Domain Private Transformers</title><link>http://arxiv.org/abs/2305.14208v1</link><description>Large, general purpose language models have demonstrated impressiveperformance across many different conversational domains. While multi-domainlanguage models achieve low overall perplexity, their outputs are notguaranteed to stay within the domain of a given input prompt. This paperproposes domain privacy as a novel way to quantify how likely a conditionallanguage model will leak across domains. We also develop policy functions basedon token-level domain classification, and propose an efficient fine-tuningmethod to improve the trained model's domain privacy. Experiments on membershipinference attacks show that our proposed method has comparable resiliency tomethods adapted from recent literature on differentially private languagemodels.</description><author>Anmol Kabra, Ethan R. Elenberg</author><pubDate>Tue, 23 May 2023 17:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14208v1</guid></item><item><title>SAD: Segment Any RGBD</title><link>http://arxiv.org/abs/2305.14207v1</link><description>The Segment Anything Model (SAM) has demonstrated its effectiveness insegmenting any part of 2D RGB images. However, SAM exhibits a stronger emphasison texture information while paying less attention to geometry information whensegmenting RGB images. To address this limitation, we propose the Segment AnyRGBD (SAD) model, which is specifically designed to extract geometryinformation directly from images. Inspired by the natural ability of humans toidentify objects through the visualization of depth maps, SAD utilizes SAM tosegment the rendered depth map, thus providing cues with enhanced geometryinformation and mitigating the issue of over-segmentation. We further includethe open-vocabulary semantic segmentation in our framework, so that the 3Dpanoptic segmentation is fulfilled. The project is available onhttps://github.com/Jun-CEN/SegmentAnyRGBD.</description><author>Jun Cen, Yizheng Wu, Kewei Wang, Xingyi Li, Jingkang Yang, Yixuan Pei, Lingdong Kong, Ziwei Liu, Qifeng Chen</author><pubDate>Tue, 23 May 2023 17:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14207v1</guid></item><item><title>Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements</title><link>http://arxiv.org/abs/2305.03695v2</link><description>Despite the much discussed capabilities of today's language models, they arestill prone to silly and unexpected commonsense failures. We consider aretrospective verification approach that reflects on the correctness of LMoutputs, and introduce Vera, a general-purpose model that estimates theplausibility of declarative statements based on commonsense knowledge. Trainedon ~7M commonsense statements created from 19 QA datasets and two large-scaleknowledge bases, and with a combination of three training objectives, Vera is aversatile model that effectively separates correct from incorrect statementsacross diverse commonsense domains. When applied to solving commonsenseproblems in the verification format, Vera substantially outperforms existingmodels that can be repurposed for commonsense verification, and it furtherexhibits generalization capabilities to unseen tasks and provideswell-calibrated outputs. We find that Vera excels at filtering LM-generatedcommonsense knowledge and is useful in detecting erroneous commonsensestatements generated by models like ChatGPT in real-world settings.</description><author>Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi</author><pubDate>Tue, 23 May 2023 17:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03695v2</guid></item><item><title>$μ$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge</title><link>http://arxiv.org/abs/2305.14205v1</link><description>Cross-lingual summarization consists of generating a summary in one languagegiven an input document in a different language, allowing for the disseminationof relevant content across speakers of other languages. However, this taskremains challenging, mainly because of the need for cross-lingual datasets andthe compounded difficulty of summarizing and translating. This work presents$\mu$PLAN, an approach to cross-lingual summarization that uses an intermediateplanning step as a cross-lingual bridge. We formulate the plan as a sequence ofentities that captures the conceptualization of the summary, i.e. identifyingthe salient content and expressing in which order to present the information,separate from the surface form. Using a multilingual knowledge base, we alignthe entities to their canonical designation across languages. $\mu$PLAN modelsfirst learn to generate the plan and then continue generating the summaryconditioned on the plan and the input. We evaluate our methodology on theXWikis dataset on cross-lingual pairs across four languages and demonstratethat this planning objective achieves state-of-the-art performance in terms ofROUGE and faithfulness scores. Moreover, this planning approach improves thezero-shot transfer to new cross-lingual language pairs compared to non-planningbaselines.</description><author>Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo, Priyanka Agrawal, Constanza Fierro, Shashi Narayan, Mirella Lapata</author><pubDate>Tue, 23 May 2023 17:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14205v1</guid></item><item><title>RECKONING: Reasoning through Dynamic Knowledge Encoding</title><link>http://arxiv.org/abs/2305.06349v2</link><description>Recent studies on transformer-based language models show that they can answerquestions by reasoning over knowledge provided as part of the context (i.e.,in-context reasoning). However, since the available knowledge is often notfiltered for a particular question, in-context reasoning can be sensitive todistractor facts, additional content that is irrelevant to a question but thatmay be relevant for a different question (i.e., not necessarily random noise).In these situations, the model fails to distinguish the knowledge that isnecessary to answer the question, leading to spurious reasoning and degradedperformance. This reasoning failure contrasts with the model's apparent abilityto distinguish its contextual knowledge from all the knowledge it has memorizedduring pre-training. Following this observation, we propose teaching the modelto reason more robustly by folding the provided contextual knowledge into themodel's parameters before presenting it with a question. Our method, RECKONING,is a bi-level learning algorithm that teaches language models to reason byupdating their parametric knowledge through back-propagation, allowing them tothen answer questions using the updated parameters. During training, the innerloop rapidly adapts a copy of the model weights to encode contextual knowledgeinto its parameters. In the outer loop, the model learns to use the updatedweights to reproduce and answer reasoning questions about the memorizedknowledge. Our experiments on two multi-hop reasoning datasets show thatRECKONING's performance improves over the in-context reasoning baseline (by upto 4.5%). We also find that compared to in-context reasoning, RECKONINGgeneralizes better to longer reasoning chains unseen during training, is morerobust to distractors in the context, and is more computationally efficientwhen multiple questions are asked about the same knowledge.</description><author>Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut</author><pubDate>Tue, 23 May 2023 17:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06349v2</guid></item><item><title>Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning</title><link>http://arxiv.org/abs/2305.14203v1</link><description>This paper presents a novel metric learning approach to address theperformance gap between normal and silent speech in visual speech recognition(VSR). The difference in lip movements between the two poses a challenge forexisting VSR models, which exhibit degraded accuracy when applied to silentspeech. To solve this issue and tackle the scarcity of training data for silentspeech, we propose to leverage the shared literal content between normal andsilent speech and present a metric learning approach based on visemes.Specifically, we aim to map the input of two speech types close to each otherin a latent space if they have similar viseme representations. By minimizingthe Kullback-Leibler divergence of the predicted viseme probabilitydistributions between and within the two speech types, our model effectivelylearns and predicts viseme identities. Our evaluation demonstrates that ourmethod improves the accuracy of silent VSR, even when limited training data isavailable.</description><author>Sara Kashiwagi, Keitaro Tanaka, Qi Feng, Shigeo Morishima</author><pubDate>Tue, 23 May 2023 17:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14203v1</guid></item><item><title>Complementing GPT-3 with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata</title><link>http://arxiv.org/abs/2305.14202v1</link><description>As the largest knowledge base, Wikidata is a massive source of knowledge,complementing large language models with well-structured data. In this paper,we present WikiWebQuestions, a high-quality knowledge base question answeringbenchmark for Wikidata. This new benchmark uses real-world human data withSPARQL annotation to facilitate a more accurate comparison with large languagemodels utilizing the up-to-date answers from Wikidata. Additionally, a baselinefor this benchmark is established with an effective training data synthesismethodology and WikiSP, a Seq2Seq semantic parser, that handles large noisyknowledge graphs. Experimental results illustrate the effectiveness of thismethodology, achieving 69% and 59% answer accuracy in the dev set and test set,respectively. We showed that we can pair semantic parsers with GPT-3 to providea combination of verifiable results and qualified guesses that can provideuseful answers to 97% of the questions in the dev set of our benchmark.</description><author>Silei Xu, Theo Culhane, Meng-Hsi Wu, Sina J. Semnani, Monica S. Lam</author><pubDate>Tue, 23 May 2023 17:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14202v1</guid></item><item><title>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</title><link>http://arxiv.org/abs/2305.14201v1</link><description>We introduce Goat, a fine-tuned LLaMA model that significantly outperformsGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generateddataset, Goat achieves state-of-the-art performance on BIG-bench arithmeticsub-task. In particular, the zero-shot Goat-7B matches or even surpasses theaccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achievenear-perfect accuracy on large-number addition and subtraction throughsupervised fine-tuning only, which is almost impossible with previouspretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attributeGoat's exceptional performance to LLaMA's consistent tokenization of numbers.To tackle more challenging tasks like large-number multiplication and division,we propose an approach that classifies tasks based on their learnability, andsubsequently decomposes unlearnable tasks, such as multi-digit multiplicationand division, into a series of learnable tasks by leveraging basic arithmeticprinciples. We thoroughly examine the performance of our model, offering acomprehensive evaluation of the effectiveness of our proposed decompositionsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAMGPU, facilitating reproducibility for other researchers. We release our model,dataset, and the Python script for dataset generation.</description><author>Tiedong Liu, Bryan Kian Hsiang Low</author><pubDate>Tue, 23 May 2023 17:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14201v1</guid></item><item><title>Accessing Higher Dimensions for Unsupervised Word Translation</title><link>http://arxiv.org/abs/2305.14200v1</link><description>The striking ability of unsupervised word translation has been demonstratedwith the help of word vectors / pretraining; however, they require largeamounts of data and usually fails if the data come from different domains. Wepropose coocmap, a method that can use either high-dimensional co-occurrencecounts or their lower-dimensional approximations. Freed from the limits of lowdimensions, we show that relying on low-dimensional vectors and theirincidental properties miss out on better denoising methods and useful worldknowledge in high dimensions, thus stunting the potential of the data. Ourresults show that unsupervised translation can be achieved more easily androbustly than previously thought -- less than 80MB and minutes of CPU time isrequired to achieve over 50\% accuracy for English to Finnish, Hungarian, andChinese translations when trained on similar data; even under domain mismatch,we show coocmap still works fully unsupervised on English NewsCrawl to ChineseWikipedia and English Europarl to Spanish Wikipedia, among others. Theseresults challenge prevailing assumptions on the necessity and superiority oflow-dimensional vectors, and suggest that similarly processed co-occurrencescan outperform dense vectors on other tasks too.</description><author>Sida I. Wang</author><pubDate>Tue, 23 May 2023 17:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14200v1</guid></item></channel></rss>