<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 10 Oct 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing</title><link>http://arxiv.org/abs/2310.05922v1</link><description>Text-to-video editing aims to edit the visual appearance of a source videoconditional on textual prompts. A major challenge in this task is to ensurethat all frames in the edited video are visually consistent. Most recent worksapply advanced text-to-image diffusion models to this task by inflating 2Dspatial attention in the U-Net into spatio-temporal attention. Althoughtemporal context can be added through spatio-temporal attention, it mayintroduce some irrelevant information for each patch and therefore causeinconsistency in the edited video. In this paper, for the first time, weintroduce optical flow into the attention module in the diffusion model's U-Netto address the inconsistency issue for text-to-video editing. Our method,FLATTEN, enforces the patches on the same flow path across different frames toattend to each other in the attention module, thus improving the visualconsistency in the edited videos. Additionally, our method is training-free andcan be seamlessly integrated into any diffusion-based text-to-video editingmethods and improve their visual consistency. Experiment results on existingtext-to-video editing benchmarks show that our proposed method achieves the newstate-of-the-art performance. In particular, our method excels in maintainingthe visual consistency in the edited videos.</description><author>Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He</author><pubDate>Mon, 09 Oct 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05922v1</guid></item><item><title>Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions</title><link>http://arxiv.org/abs/2310.05921v1</link><description>We introduce Conformal Decision Theory, a framework for producing safeautonomous decisions despite imperfect machine learning predictions. Examplesof such decisions are ubiquitous, from robot planning algorithms that rely onpedestrian predictions, to calibrating autonomous manufacturing to exhibit highthroughput and low error, to the choice of trusting a nominal policy versusswitching to a safe backup policy at run-time. The decisions produced by ouralgorithms are safe in the sense that they come with provable statisticalguarantees of having low risk without any assumptions on the world modelwhatsoever; the observations need not be I.I.D. and can even be adversarial.The theory extends results from conformal prediction to calibrate decisionsdirectly, without requiring the construction of prediction sets. Experimentsdemonstrate the utility of our approach in robot motion planning around humans,automated stock trading, and robot manufacturin</description><author>Jordan Lekeufack, Anastasios A. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik</author><pubDate>Mon, 09 Oct 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05921v1</guid></item><item><title>SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation</title><link>http://arxiv.org/abs/2310.05920v1</link><description>The ability to detect objects in images at varying scales has played apivotal role in the design of modern object detectors. Despite considerableprogress in removing handcrafted components using transformers, multi-scalefeature maps remain a key factor for their empirical success, even with a plainbackbone like the Vision Transformer (ViT). In this paper, we show that thisreliance on feature pyramids is unnecessary and a transformer-based detectorwith scale-aware attention enables the plain detector `SimPLR' whose backboneand detection head both operate on single-scale features. The plainarchitecture allows SimPLR to effectively take advantages of self-supervisedlearning and scaling approaches with ViTs, yielding strong performance comparedto multi-scale counterparts. We demonstrate through our experiments that whenscaling to larger backbones, SimPLR indicates better performance thanend-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), whileconsistently being faster. The code will be released.</description><author>Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek</author><pubDate>Mon, 09 Oct 2023 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05920v1</guid></item><item><title>Few-Shot Spoken Language Understanding via Joint Speech-Text Models</title><link>http://arxiv.org/abs/2310.05919v1</link><description>Recent work on speech representation models jointly pre-trained with text hasdemonstrated the potential of improving speech representations by encodingspeech and text in a shared space. In this paper, we leverage such sharedrepresentations to address the persistent challenge of limited dataavailability in spoken language understanding tasks. By employing a pre-trainedspeech-text model, we find that models fine-tuned on text can be effectivelytransferred to speech testing data. With as little as 1 hour of labeled speechdata, our proposed approach achieves comparable performance on spoken languageunderstanding tasks (specifically, sentiment analysis and named entityrecognition) when compared to previous methods using speech-only pre-trainedmodels fine-tuned on 10 times more data. Beyond the proof-of-concept study, wealso analyze the latent representations. We find that the bottom layers ofspeech-text models are largely task-agnostic and align speech and textrepresentations into a shared space, while the top layers are moretask-specific.</description><author>Chung-Ming Chien, Mingjiamei Zhang, Ju-Chieh Chou, Karen Livescu</author><pubDate>Mon, 09 Oct 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05919v1</guid></item><item><title>Grokking as Compression: A Nonlinear Complexity Perspective</title><link>http://arxiv.org/abs/2310.05918v1</link><description>We attribute grokking, the phenomenon where generalization is much delayedafter memorization, to compression. To do so, we define linear mapping number(LMN) to measure network complexity, which is a generalized version of linearregion number for ReLU networks. LMN can nicely characterize neural networkcompression before generalization. Although the $L_2$ norm has been a popularchoice for characterizing model complexity, we argue in favor of LMN for anumber of reasons: (1) LMN can be naturally interpreted asinformation/computation, while $L_2$ cannot. (2) In the compression phase, LMNhas linear relations with test losses, while $L_2$ is correlated with testlosses in a complicated nonlinear way. (3) LMN also reveals an intriguingphenomenon of the XOR network switching between two generalization solutions,while $L_2$ does not. Besides explaining grokking, we argue that LMN is apromising candidate as the neural network version of the Kolmogorov complexitysince it explicitly considers local or conditioned linear computations alignedwith the nature of modern artificial neural networks.</description><author>Ziming Liu, Ziqian Zhong, Max Tegmark</author><pubDate>Mon, 09 Oct 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05918v1</guid></item><item><title>Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic Clothing Driven by Sparse RGB-D Input</title><link>http://arxiv.org/abs/2310.05917v1</link><description>Clothing is an important part of human appearance but challenging to model inphotorealistic avatars. In this work we present avatars with dynamically movingloose clothing that can be faithfully driven by sparse RGB-D inputs as well asbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)algorithm that can efficiently track the coarse garment shape given sparsedepth input. Given the coarse tracking results, the input RGB-D images are thenremapped to texel-aligned features, which are fed into the drivable avatarmodels to faithfully reconstruct appearance details. We evaluate our methodagainst recent image-driven synthesis baselines, and conduct a comprehensiveanalysis of the N-ICP algorithm. We demonstrate that our method can generalizeto a novel testing environment, while preserving the ability to producehigh-fidelity and faithful clothing dynamics and appearance.</description><author>Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov</author><pubDate>Mon, 09 Oct 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05917v1</guid></item><item><title>Interpreting CLIP's Image Representation via Text-Based Decomposition</title><link>http://arxiv.org/abs/2310.05916v1</link><description>We investigate the CLIP image encoder by analyzing how individual modelcomponents affect the final representation. We decompose the imagerepresentation as a sum across individual image patches, model layers, andattention heads, and use CLIP's text representation to interpret the summands.Interpreting the attention heads, we characterize each head's role byautomatically finding text representations that span its output space, whichreveals property-specific roles for many heads (e.g. location or shape). Next,interpreting the image patches, we uncover an emergent spatial localizationwithin CLIP. Finally, we use this understanding to remove spurious featuresfrom CLIP and to create a strong zero-shot image segmenter. Our resultsindicate that a scalable understanding of transformer models is attainable andcan be used to repair and improve models.</description><author>Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt</author><pubDate>Mon, 09 Oct 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05916v1</guid></item><item><title>FireAct: Toward Language Agent Fine-tuning</title><link>http://arxiv.org/abs/2310.05915v1</link><description>Recent efforts have augmented language models (LMs) with external tools orenvironments, leading to the development of language agents that can reason andact. However, most of these agents rely on few-shot prompting techniques withoff-the-shelf LMs. In this paper, we investigate and argue for the overlookeddirection of fine-tuning LMs to obtain language agents. Using a setup ofquestion answering (QA) with a Google search API, we explore a variety of baseLMs, prompting methods, fine-tuning data, and QA tasks, and find languageagents are consistently improved after fine-tuning their backbone LMs. Forexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,a novel approach to fine-tuning LMs with trajectories from multiple tasks andprompting methods, and show having more diverse fine-tuning data can furtherimprove agents. Along with other findings regarding scaling effects,robustness, generalization, efficiency and cost, our work establishescomprehensive benefits of fine-tuning LMs for agents, and provides an initialset of experimental designs, insights, as well as open questions towardlanguage agent fine-tuning.</description><author>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao</author><pubDate>Mon, 09 Oct 2023 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05915v1</guid></item><item><title>NEFTune: Noisy Embeddings Improve Instruction Finetuning</title><link>http://arxiv.org/abs/2310.05914v1</link><description>We show that language model finetuning can be improved, sometimesdramatically, with a simple augmentation. NEFTune adds noise to the embeddingvectors during training. Standard finetuning of LLaMA-2-7B using Alpacaachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.NEFTune also improves over strong baselines on modern instruction datasets.Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%improvement, and with OpenPlatypus an 8% improvement. Even powerful modelsfurther refined with RLHF such as LLaMA-2-Chat benefit from additional trainingwith NEFTune.</description><author>Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</author><pubDate>Mon, 09 Oct 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05914v1</guid></item><item><title>Booster: a Benchmark for Depth from Images of Specular and Transparent Surfaces</title><link>http://arxiv.org/abs/2301.08245v2</link><description>Estimating depth from images nowadays yields outstanding results, both interms of in-domain accuracy and generalization. However, we identify two mainchallenges that remain open in this field: dealing with non-Lambertianmaterials and effectively processing high-resolution images. Purposely, wepropose a novel dataset that includes accurate and dense ground-truth labels athigh resolution, featuring scenes containing several specular and transparentsurfaces. Our acquisition pipeline leverages a novel deep space-time stereoframework, enabling easy and accurate labeling with sub-pixel precision. Thedataset is composed of 606 samples collected in 85 different scenes, eachsample includes both a high-resolution pair (12 Mpx) as well as an unbalancedstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devicesthat mount sensors with different resolutions. Additionally, we providemanually annotated material segmentation masks and 15K unlabeled samples. Thedataset is composed of a train set and two test sets, the latter devoted to theevaluation of stereo and monocular depth estimation networks. Our experimentshighlight the open challenges and future research directions in this field.</description><author>Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano</author><pubDate>Mon, 09 Oct 2023 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08245v2</guid></item><item><title>SALMON: Self-Alignment with Principle-Following Reward Models</title><link>http://arxiv.org/abs/2310.05910v1</link><description>Supervised Fine-Tuning (SFT) on response demonstrations combined withReinforcement Learning from Human Feedback (RLHF) constitutes a powerfulparadigm for aligning LLM-based AI agents. However, a significant limitation ofsuch an approach is its dependency on high-quality human annotations, makingits application to intricate tasks challenging due to difficulties in obtainingconsistent response demonstrations and in-distribution response preferences.This paper presents a novel approach, namely SALMON (Self-ALignMent withprinciple-fOllowiNg reward models), to align base language models with minimalhuman supervision, using only a small set of human-defined principles, yetachieving superior performance. Central to our approach is aprinciple-following reward model. Trained on synthetic preference data, thismodel can generate reward scores based on arbitrary human-defined principles.By merely adjusting these principles during the RL training phase, we gain fullcontrol over the preferences with the reward model, subsequently influencingthe behavior of the RL-trained policies, and eliminating the reliance on thecollection of online human preferences. Applying our method to the LLaMA-2-70bbase language model, we developed an AI assistant named Dromedary-2. With only6 exemplars for in-context learning and 31 human-defined principles,Dromedary-2 significantly surpasses the performance of several state-of-the-artAI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We haveopen-sourced the code and model weights to encourage further research intoaligning LLM-based AI agents with enhanced supervision efficiency, improvedcontrollability, and scalable oversight.</description><author>Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</author><pubDate>Mon, 09 Oct 2023 18:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05910v1</guid></item><item><title>AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation</title><link>http://arxiv.org/abs/2306.00977v2</link><description>During interactive segmentation, a model and a user work together todelineate objects of interest in a 3D point cloud. In an iterative process, themodel assigns each data point to an object (or the background), while the usercorrects errors in the resulting segmentation and feeds them back into themodel. The current best practice formulates the problem as binaryclassification and segments objects one at a time. The model expects the userto provide positive clicks to indicate regions wrongly assigned to thebackground and negative clicks on regions wrongly assigned to the object.Sequentially visiting objects is wasteful since it disregards synergies betweenobjects: a positive click for a given object can, by definition, serve as anegative click for nearby objects. Moreover, a direct competition betweenadjacent objects can speed up the identification of their common boundary. Weintroduce AGILE3D, an efficient, attention-based model that (1) supportssimultaneous segmentation of multiple 3D objects, (2) yields more accuratesegmentation masks with fewer user clicks, and (3) offers faster inference. Ourcore idea is to encode user clicks as spatial-temporal queries and enableexplicit interactions between click queries as well as between them and the 3Dscene through a click attention module. Every time new clicks are added, weonly need to run a lightweight decoder that produces updated segmentationmasks. In experiments with four different 3D point cloud datasets, AGILE3D setsa new state-of-the-art. Moreover, we also verify its practicality in real-worldsetups with real user studies.</description><author>Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, Theodora Kontogianni</author><pubDate>Mon, 09 Oct 2023 18:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00977v2</guid></item><item><title>TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models</title><link>http://arxiv.org/abs/2310.05905v1</link><description>The full potential of large pretrained models remains largely untapped incontrol domains like robotics. This is mainly because of the scarcity of dataand the computational challenges associated with training or fine-tuning theselarge models for such applications. Prior work mainly emphasizes effectivepretraining of large models for decision-making, with little exploration intohow to perform data-efficient continual adaptation of these models for newtasks. Recognizing these constraints, we introduce TAIL (Task-specific Adaptersfor Imitation Learning), a framework for efficient adaptation to new controltasks. Inspired by recent advancements in parameter-efficient fine-tuning inlanguage domains, we explore efficient fine-tuning techniques -- e.g.,Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL toadapt large pretrained models for new tasks with limited demonstration data.Our extensive experiments in large-scale language-conditioned manipulationtasks comparing prevalent parameter-efficient fine-tuning techniques andadaptation baselines suggest that TAIL with LoRA can achieve the bestpost-adaptation performance with only 1\% of the trainable parameters of fullfine-tuning, while avoiding catastrophic forgetting and preserving adaptationplasticity in continual learning settings.</description><author>Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, Rasool Fakoor</author><pubDate>Mon, 09 Oct 2023 18:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05905v1</guid></item><item><title>Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design</title><link>http://arxiv.org/abs/2310.04343v2</link><description>Proteins are macromolecules responsible for essential functions in almost allliving organisms. Designing reasonable proteins with desired functions iscrucial. A protein's sequence and structure are strongly correlated and theytogether determine its function. In this paper, we propose NAEPro, a model tojointly design Protein sequence and structure based on automatically detectedfunctional sites. NAEPro is powered by an interleaving network of attention andequivariant layers, which can capture global correlation in a whole sequenceand local influence from nearest amino acids in three dimensional (3D) space.Such an architecture facilitates effective yet economic message passing at twolevels. We evaluate our model and several strong baselines on two proteindatasets, $\beta$-lactamase and myoglobin. Experimental results show that ourmodel consistently achieves the highest amino acid recovery rate, TM-score, andthe lowest RMSD among all competitors. These findings prove the capability ofour model to design protein sequences and structures that closely resembletheir natural counterparts. Furthermore, in-depth analysis further confirms ourmodel's ability to generate highly effective proteins capable of binding totheir target metallocofactors. We provide code, data and models in Github.</description><author>Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Yang Yang, Lei Li</author><pubDate>Mon, 09 Oct 2023 18:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04343v2</guid></item><item><title>Finding Safe Zones of policies Markov Decision Processes</title><link>http://arxiv.org/abs/2202.11593v2</link><description>Given a policy of a Markov Decision Process, we define a SafeZone as a subsetof states, such that most of the policy's trajectories are confined to thissubset. The quality of a SafeZone is parameterized by the number of states andthe escape probability, i.e., the probability that a random trajectory willleave the subset. SafeZones are especially interesting when they have a smallnumber of states and low escape probability. We study the complexity of findingoptimal SafeZones, and show that in general, the problem is computationallyhard. Our main result is a bi-criteria approximation learning algorithm with afactor of almost $2$ approximation for both the escape probability and SafeZonesize, using a polynomial size sample complexity.</description><author>Lee Cohen, Yishay Mansour, Michal Moshkovitz</author><pubDate>Mon, 09 Oct 2023 18:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.11593v2</guid></item><item><title>Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</title><link>http://arxiv.org/abs/2310.02244v4</link><description>By classifying infinite-width neural networks and identifying the *optimal*limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,for *widthwise hyperparameter transfer*, i.e., predicting optimalhyperparameters of wide neural networks from narrow ones. Here we investigatethe analogous classification for *depthwise parametrizations* of deep residualnetworks (resnets). We classify depthwise parametrizations of block multiplierand learning rate by their infinite-width-then-depth limits. In resnets whereeach block has only one layer, we identify a unique optimal parametrization,called Depth-$\mu$P that extends $\mu$P and show empirically it admitsdepthwise hyperparameter transfer. We identify *feature diversity* as a crucialfactor in deep networks, and Depth-$\mu$P can be characterized as maximizingboth feature learning and feature diversity. Exploiting this, we find thatabsolute value, among all homogeneous nonlinearities, maximizes featurediversity and indeed empirically leads to significantly better performance.However, if each block is deeper (such as modern transformers), then we findfundamental limitations in all possible infinite-depth limits of suchparametrizations, which we illustrate both theoretically and empirically onsimple networks as well as Megatron transformer trained on Common Crawl.</description><author>Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou</author><pubDate>Mon, 09 Oct 2023 18:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02244v4</guid></item><item><title>Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network</title><link>http://arxiv.org/abs/2310.05900v1</link><description>Quantum error-correction is a prerequisite for reliable quantum computation.Towards this goal, we present a recurrent, transformer-based neural networkwhich learns to decode the surface code, the leading quantum error-correctioncode. Our decoder outperforms state-of-the-art algorithmic decoders onreal-world data from Google's Sycamore quantum processor for distance 3 and 5surface codes. On distances up to 11, the decoder maintains its advantage onsimulated data with realistic noise including cross-talk, leakage, and analogreadout signals, and sustains its accuracy far beyond the 25 cycles it wastrained on. Our work illustrates the ability of machine learning to go beyondhuman-designed algorithms by learning from data directly, highlighting machinelearning as a strong contender for decoding in quantum computers.</description><author>Johannes Bausch, Andrew W Senior, Francisco J H Heras, Thomas Edlich, Alex Davies, Michael Newman, Cody Jones, Kevin Satzinger, Murphy Yuezhen Niu, Sam Blackwell, George Holland, Dvir Kafri, Juan Atalaya, Craig Gidney, Demis Hassabis, Sergio Boixo, Hartmut Neven, Pushmeet Kohli</author><pubDate>Mon, 09 Oct 2023 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05900v1</guid></item><item><title>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts</title><link>http://arxiv.org/abs/2310.05898v1</link><description>Lion (Evolved Sign Momentum), a new optimizer discovered through programsearch, has shown promising results in training large AI models. It performscomparably or favorably to AdamW but with greater memory efficiency. As we canexpect from the results of a random search program, Lion incorporates elementsfrom several existing algorithms, including signed momentum, decoupled weightdecay, Polak, and Nesterov momentum, but does not fit into any existingcategory of theoretically grounded optimizers. Thus, even though Lion appearsto perform well as a general-purpose optimizer for a wide range of tasks, itstheoretical basis remains uncertain. This lack of theoretical clarity limitsopportunities to further enhance and expand Lion's efficacy. This work aims to demystify Lion. Based on both continuous-time anddiscrete-time analysis, we demonstrate that Lion is a theoretically novel andprincipled approach for minimizing a general loss function $f(x)$ whileenforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves thisthrough the incorporation of decoupled weight decay, where $\lambda$ representsthe weight decay coefficient. Our analysis is made possible by the developmentof a new Lyapunov function for the Lion updates. It applies to a broader familyof Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion isreplaced by the subgradient of a convex function $\kappa$, leading to thesolution of a general composite optimization problem of $\min_x f(x) +\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lionand pave the way for further improvements and extensions of Lion-relatedalgorithms.</description><author>Lizhang Chen, Bo Liu, Kaizhao Liang, Qiang Liu</author><pubDate>Mon, 09 Oct 2023 18:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05898v1</guid></item><item><title>A Generalization Bound of Deep Neural Networks for Dependent Data</title><link>http://arxiv.org/abs/2310.05892v1</link><description>Existing generalization bounds for deep neural networks require data to beindependent and identically distributed (iid). This assumption may not hold inreal-life applications such as evolutionary biology, infectious diseaseepidemiology, and stock price prediction. This work establishes ageneralization bound of feed-forward neural networks for non-stationary$\phi$-mixing data.</description><author>Quan Huu Do, Binh T. Nguyen, Lam Si Tung Ho</author><pubDate>Mon, 09 Oct 2023 18:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05892v1</guid></item><item><title>Streaming Anchor Loss: Augmenting Supervision with Temporal Significance</title><link>http://arxiv.org/abs/2310.05886v1</link><description>Streaming neural network models for fast frame-wise responses to variousspeech and sensory signals are widely adopted on resource-constrainedplatforms. Hence, increasing the learning capacity of such streaming models(i.e., by adding more parameters) to improve the predictive power may not beviable for real-world tasks. In this work, we propose a new loss, StreamingAnchor Loss (SAL), to better utilize the given learning capacity by encouragingthe model to learn more from essential frames. More specifically, our SAL andits focal variations dynamically modulate the frame-wise cross entropy lossbased on the importance of the corresponding frames so that a higher losspenalty is assigned for frames within the temporal proximity of semanticallycritical events. Therefore, our loss ensures that the model training focuses onpredicting the relatively rare but task-relevant frames. Experimental resultswith standard lightweight convolutional and recurrent streaming networks onthree different speech based detection tasks demonstrate that SAL enables themodel to learn the overall task more effectively with improved accuracy andlatency, without any additional data, model parameters, or architecturalchanges.</description><author>Utkarsh, Sarawgi, John Berkowitz, Vineet Garg, Arnav Kundu, Minsik Cho, Sai Srujana Buddi, Saurabh Adya, Ahmed Tewfik</author><pubDate>Mon, 09 Oct 2023 18:28:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05886v1</guid></item><item><title>A Meta-Learning Perspective on Transformers for Causal Language Modeling</title><link>http://arxiv.org/abs/2310.05884v1</link><description>The Transformer architecture has become prominent in developing large causallanguage models. However, mechanisms to explain its capabilities are not wellunderstood. Focused on the training process, here we establish a meta-learningview of the Transformer architecture when trained for the causal languagemodeling task, by explicating an inner optimization process that may happenwithin the Transformer. Further, from within the inner optimization, wediscover and theoretically analyze a special characteristic of the norms oflearned token representations within Transformer-based causal language models.Our analysis is supported by experiments conducted on pre-trained largelanguage models and real-world data.</description><author>Xinbo Wu, Lav R. Varshney</author><pubDate>Mon, 09 Oct 2023 18:27:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05884v1</guid></item><item><title>Recurrent Hypernetworks are Surprisingly Strong in Meta-RL</title><link>http://arxiv.org/abs/2309.14970v2</link><description>Deep reinforcement learning (RL) is notoriously impractical to deploy due tosample inefficiency. Meta-RL directly addresses this sample inefficiency bylearning to perform few-shot learning when a distribution of related tasks isavailable for meta-training. While many specialized meta-RL methods have beenproposed, recent work suggests that end-to-end learning in conjunction with anoff-the-shelf sequential model, such as a recurrent network, is a surprisinglystrong baseline. However, such claims have been controversial due to limitedsupporting evidence, particularly in the face of prior work establishingprecisely the opposite. In this paper, we conduct an empirical investigation.While we likewise find that a recurrent network can achieve strong performance,we demonstrate that the use of hypernetworks is crucial to maximizing theirpotential. Surprisingly, when combined with hypernetworks, the recurrentbaselines that are far simpler than existing specialized methods actuallyachieve the strongest performance of all methods evaluated.</description><author>Jacob Beck, Risto Vuorio, Zheng Xiong, Shimon Whiteson</author><pubDate>Mon, 09 Oct 2023 18:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14970v2</guid></item><item><title>Controllable Chest X-Ray Report Generation from Longitudinal Representations</title><link>http://arxiv.org/abs/2310.05881v1</link><description>Radiology reports are detailed text descriptions of the content of medicalscans. Each report describes the presence/absence and location of relevantclinical findings, commonly including comparison with prior exams of the samepatient to describe how they evolved. Radiology reporting is a time-consumingprocess, and scan results are often subject to delays. One strategy to speed upreporting is to integrate automated reporting systems, however clinicaldeployment requires high accuracy and interpretability. Previous approaches toautomated radiology reporting generally do not provide the prior study asinput, precluding comparison which is required for clinical accuracy in sometypes of scans, and offer only unreliable methods of interpretability.Therefore, leveraging an existing visual input format of anatomical tokens, weintroduce two novel aspects: (1) longitudinal representation learning -- weinput the prior scan as an additional input, proposing a method to align,concatenate and fuse the current and prior visual information into a jointlongitudinal representation which can be provided to the multimodal reportgeneration model; (2) sentence-anatomy dropout -- a training strategy forcontrollability in which the report generator model is trained to predict onlysentences from the original report which correspond to the subset of anatomicalregions given as input. We show through in-depth experiments on the MIMIC-CXRdataset how the proposed approach achieves state-of-the-art results whileenabling anatomy-wise controllable report generation.</description><author>Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeffrey Dalton, Alison Q O'Neil</author><pubDate>Mon, 09 Oct 2023 18:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05881v1</guid></item><item><title>Coarse-Graining Hamiltonian Systems Using WSINDy</title><link>http://arxiv.org/abs/2310.05879v1</link><description>The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy)has been demonstrated to offer coarse-graining capabilities in the context ofinteracting particle systems ( https://doi.org/10.1016/j.physd.2022.133406 ).In this work we extend this capability to the problem of coarse-grainingHamiltonian dynamics which possess approximate symmetries. Such approximatesymmetries often lead to the existence of a Hamiltonian system of reduceddimension that may be used to efficiently capture the dynamics of the relevantdegrees of freedom. Deriving such reduced systems, or approximating themnumerically, is an ongoing challenge. We demonstrate that WSINDy cansuccessfully identify this reduced Hamiltonian system in the presence of largeperturbations imparted from both the inexact nature of the symmetry andextrinsic noise. This is significant in part due to the nontrivial means bywhich such systems are derived analytically. WSINDy naturally preserves theHamiltonian structure by restricting to a trial basis of Hamiltonian vectorfields, and the methodology is computational efficient, often requiring only asingle trajectory to learn the full reduced Hamiltonian, and avoiding forwardsolves in the learning process. In this way, we argue that weak-form equationlearning is particularly well-suited for Hamiltonian coarse-graining. Usingnearly-periodic Hamiltonian systems as a prototypical class of systems withapproximate symmetries, we show that WSINDy robustly identifies the correctleading-order reduced system of dimension $2(N-1)$ or $N$ from the original$(2N)$-dimensional system, upon observation of the relevant degrees of freedom.We provide physically relevant examples, namely coupled oscillator dynamics,the H\'enon-Heiles system for stellar motion within a galaxy, and the dynamicsof charged particles.</description><author>Daniel A. Messenger, Joshua W. Burby, David M. Bortz</author><pubDate>Mon, 09 Oct 2023 18:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05879v1</guid></item><item><title>A Machine Learning Approach to Predicting Single Event Upsets</title><link>http://arxiv.org/abs/2310.05878v1</link><description>A single event upset (SEU) is a critical soft error that occurs insemiconductor devices on exposure to ionising particles from spaceenvironments. SEUs cause bit flips in the memory component of semiconductors.This creates a multitude of safety hazards as stored information becomes lessreliable. Currently, SEUs are only detected several hours after theiroccurrence. CREMER, the model presented in this paper, predicts SEUs in advanceusing machine learning. CREMER uses only positional data to predict SEUoccurrence, making it robust, inexpensive and scalable. Upon implementation,the improved reliability of memory devices will create a digitally saferenvironment onboard space vehicles.</description><author>Archit Gupta, Chong Yock Eng, Deon Lim Meng Wee, Rashna Analia Ahmed, See Min Sim</author><pubDate>Mon, 09 Oct 2023 18:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05878v1</guid></item><item><title>AI Systems of Concern</title><link>http://arxiv.org/abs/2310.05876v1</link><description>Concerns around future dangers from advanced AI often centre on systemshypothesised to have intrinsic characteristics such as agent-like behaviour,strategic awareness, and long-range planning. We label this cluster ofcharacteristics as "Property X". Most present AI systems are low in "PropertyX"; however, in the absence of deliberate steering, current research directionsmay rapidly lead to the emergence of highly capable AI systems that are alsohigh in "Property X". We argue that "Property X" characteristics areintrinsically dangerous, and when combined with greater capabilities willresult in AI systems for which safety and control is difficult to guarantee.Drawing on several scholars' alternative frameworks for possible AI researchtrajectories, we argue that most of the proposed benefits of advanced AI can beobtained by systems designed to minimise this property. We then proposeindicators and governance interventions to identify and limit the developmentof systems with risky "Property X" characteristics.</description><author>Kayla Matteucci, Shahar Avin, Fazl Barez, Seán Ó hÉigeartaigh</author><pubDate>Mon, 09 Oct 2023 18:15:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05876v1</guid></item><item><title>Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion Models</title><link>http://arxiv.org/abs/2310.05873v1</link><description>Fine-tuning diffusion models through personalized datasets is an acknowledgedmethod for improving generation quality across downstream tasks, which,however, often inadvertently generates unintended concepts such as watermarksand QR codes, attributed to the limitations in image sources and collectingmethods within specific downstream tasks. Existing solutions suffer fromeliminating these unintentionally learned implicit concepts, primarily due tothe dependency on the model's ability to recognize concepts that it actuallycannot discern. In this work, we introduce \methodname, a novel approach thatsuccessfully removes the implicit concepts with either an additional accessibleclassifier or detector model to encode geometric information of these conceptsinto text domain. Moreover, we propose \textit{Implicit Concept}, a novelimage-text dataset imbued with three implicit concepts (\ie, watermarks, QRcodes, and text) for training and evaluation. Experimental results demonstratethat \methodname not only identifies but also proficiently eradicates implicitconcepts, revealing a significant improvement over the existing methods. Theintegration of geometric information marks a substantial progression in theprecise removal of implicit concepts in diffusion models.</description><author>Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok</author><pubDate>Mon, 09 Oct 2023 18:13:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05873v1</guid></item><item><title>ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2310.05872v1</link><description>In our work, we explore the synergistic capabilities of pre-trainedvision-and-language models (VLMs) and large language models (LLMs) for visualcommonsense reasoning (VCR). We categorize the problem of VCR into visualcommonsense understanding (VCU) and visual commonsense inference (VCI). ForVCU, which involves perceiving the literal visual content, pre-trained VLMsexhibit strong cross-dataset generalization. On the other hand, in VCI, wherethe goal is to infer conclusions beyond image content, VLMs face difficulties.We find that a baseline where VLMs provide perception results (image captions)to LLMs leads to improved performance on VCI. However, we identify a challengewith VLMs' passive perception, which often misses crucial context information,leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, wesuggest a collaborative approach where LLMs, when uncertain about theirreasoning, actively direct VLMs to concentrate on and gather relevant visualelements to support potential commonsense inferences. In our method, namedViCor, pre-trained LLMs serve as problem classifiers to analyze the problemcategory, VLM commanders to leverage VLMs differently based on the problemclassification, and visual commonsense reasoners to answer the question. VLMswill perform visual recognition and understanding. We evaluate our framework ontwo VCR benchmark datasets and outperform all other methods that do not requirein-domain supervised fine-tuning.</description><author>Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang</author><pubDate>Mon, 09 Oct 2023 18:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05872v1</guid></item><item><title>UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning</title><link>http://arxiv.org/abs/2305.14761v2</link><description>Charts are very popular for analyzing data, visualizing key insights andanswering complex reasoning questions about data. To facilitate chart-baseddata analysis using natural language, several downstream tasks have beenintroduced recently such as chart question answering and chart summarization.However, most of the methods that solve these tasks use pretraining on languageor vision-language tasks that do not attempt to explicitly model the structureof the charts (e.g., how data is visually encoded and how chart elements arerelated to each other). To address this, we first build a large corpus ofcharts covering a wide variety of topics and visual styles. We then presentUniChart, a pretrained model for chart comprehension and reasoning. UniChartencodes the relevant text, data, and visual elements of charts and then uses achart-grounded text decoder to generate the expected output in naturallanguage. We propose several chart-specific pretraining tasks that include: (i)low-level tasks to extract the visual elements (e.g., bars, lines) and datafrom charts, and (ii) high-level tasks to acquire chart understanding andreasoning skills. We find that pretraining the model on a large corpus withchart-specific low- and high-level tasks followed by finetuning on threedown-streaming tasks results in state-of-the-art performance on threedownstream tasks.</description><author>Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, Shafiq Joty</author><pubDate>Mon, 09 Oct 2023 18:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14761v2</guid></item><item><title>Dynamic value alignment through preference aggregation of multiple objectives</title><link>http://arxiv.org/abs/2310.05871v1</link><description>The development of ethical AI systems is currently geared toward settingobjective functions that align with human objectives. However, finding suchfunctions remains a research challenge, while in RL, setting rewards by hand isa fairly standard approach. We present a methodology for dynamic valuealignment, where the values that are to be aligned with are dynamicallychanging, using a multiple-objective approach. We apply this approach to extendDeep $Q$-Learning to accommodate multiple objectives and evaluate this methodon a simplified two-leg intersection controlled by a switching agent.Ourapproach dynamically accommodates the preferences of drivers on the system andachieves better overall performance across three metrics (speeds, stops, andwaits) while integrating objectives that have competing or conflicting actions.</description><author>Marcin Korecki, Damian Dailisan, Cesare Carissimo</author><pubDate>Mon, 09 Oct 2023 18:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05871v1</guid></item><item><title>A Marketplace Price Anomaly Detection System at Scale</title><link>http://arxiv.org/abs/2310.04367v2</link><description>Online marketplaces execute large volume of price updates that are initiatedby individual marketplace sellers each day on the platform. This pricedemocratization comes with increasing challenges with data quality. Lack ofcentralized guardrails that are available for a traditional online retailercauses a higher likelihood for inaccurate prices to get published on thewebsite, leading to poor customer experience and potential for revenue loss. Wepresent MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labelingand Unsupervised Statistical-features), a scalable price anomaly detectionframework for a growing marketplace platform. The goal is to leverage proximityand historical price trends from unsupervised statistical features to generatean upper price bound. We build an ensemble of models to detect irregularitiesin price-based features, exclude irregular features and use optimized weightingscheme to build a reliable price bound in real-time pricing pipeline. Weobserved that our approach improves precise anchor coverage by up to 46.6% inhigh-vulnerability item subsets</description><author>Akshit Sarpal, Qiwen Kang, Fangping Huang, Yang Song, Lijie Wan</author><pubDate>Mon, 09 Oct 2023 18:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04367v2</guid></item><item><title>HyperAttention: Long-context Attention in Near-Linear Time</title><link>http://arxiv.org/abs/2310.05869v1</link><description>We present an approximate attention mechanism named HyperAttention to addressthe computational challenges posed by the growing complexity of long contextsused in Large Language Models (LLMs). Recent work suggests that in theworst-case scenario, quadratic time is necessary unless the entries of theattention matrix are bounded or the matrix has low stable rank. We introducetwo parameters which measure: (1) the max column norm in the normalizedattention matrix, and (2) the ratio of row norms in the unnormalized attentionmatrix after detecting and removing large entries. We use these fine-grainedparameters to capture the hardness of the problem. Despite previous lowerbounds, we are able to achieve a linear time sampling algorithm even when thematrix has unbounded entries or a large stable rank, provided the aboveparameters are small. HyperAttention features a modular design that easilyaccommodates integration of other fast low-level implementations, particularlyFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) toidentify large entries, HyperAttention outperforms existing methods, givingsignificant speed improvements compared to state-of-the-art solutions likeFlashAttention. We validate the empirical performance of HyperAttention on avariety of different long-context length datasets. For example, HyperAttentionmakes the inference time of ChatGLM2 50\% faster on 32k context length whileperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,with causal masking, HyperAttention offers 5-fold speedup on a single attentionlayer.</description><author>Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh</author><pubDate>Mon, 09 Oct 2023 18:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05869v1</guid></item><item><title>Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory</title><link>http://arxiv.org/abs/2310.05868v1</link><description>The brain has computational capabilities that surpass those of modernsystems, being able to solve complex problems efficiently in a simple way.Neuromorphic engineering aims to mimic biology in order to develop new systemscapable of incorporating such capabilities. Bio-inspired learning systemscontinue to be a challenge that must be solved, and much work needs to be donein this regard. Among all brain regions, the hippocampus stands out as anautoassociative short-term memory with the capacity to learn and recallmemories from any fragment of them. These characteristics make the hippocampusan ideal candidate for developing bio-inspired learning systems that, inaddition, resemble content-addressable memories. Therefore, in this work wepropose a bio-inspired spiking content-addressable memory model based on theCA3 region of the hippocampus with the ability to learn, forget and recallmemories, both orthogonal and non-orthogonal, from any fragment of them. Themodel was implemented on the SpiNNaker hardware platform using Spiking NeuralNetworks. A set of experiments based on functional, stress and applicabilitytests were performed to demonstrate its correct functioning. This work presentsthe first hardware implementation of a fully-functional bio-inspired spikinghippocampal content-addressable memory model, paving the way for thedevelopment of future more complex neuromorphic systems.</description><author>Daniel Casanueva-Morato, Alvaro Ayuso-Martinez, Juan P. Dominguez-Morales, Angel Jimenez-Fernandez, Gabriel Jimenez-Moreno</author><pubDate>Mon, 09 Oct 2023 18:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05868v1</guid></item><item><title>Domain-wise Invariant Learning for Panoptic Scene Graph Generation</title><link>http://arxiv.org/abs/2310.05867v1</link><description>Panoptic Scene Graph Generation (PSG) involves the detection of objects andthe prediction of their corresponding relationships (predicates). However, thepresence of biased predicate annotations poses a significant challenge for PSGmodels, as it hinders their ability to establish a clear decision boundaryamong different predicates. This issue substantially impedes the practicalutility and real-world applicability of PSG models. To address the intrinsicbias above, we propose a novel framework to infer potentially biasedannotations by measuring the predicate prediction risks within eachsubject-object pair (domain), and adaptively transfer the biased annotations toconsistent ones by learning invariant predicate representation embeddings.Experiments show that our method significantly improves the performance ofbenchmark models, achieving a new state-of-the-art performance, and shows greatgeneralization and effectiveness on PSG dataset.</description><author>Li Li, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann</author><pubDate>Mon, 09 Oct 2023 18:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05867v1</guid></item><item><title>A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots</title><link>http://arxiv.org/abs/2208.10765v2</link><description>Duckiebots are low-cost mobile robots that are widely used in the fields ofresearch and education. Although there are existing self-driving algorithms forthe Duckietown platform, they are either too complex or perform too poorly tonavigate a multi-lane track. Moreover, it is essential to give memory andcomputational resources to a Duckiebot so it can perform additional tasks suchas out-of-distribution input detection. In order to satisfy these constraints,we built a low-cost autonomous driving algorithm capable of driving on atwo-lane track. The algorithm uses traditional computer vision techniques toidentify the central lane on the track and obtain the relevant steering angle.The steering is then controlled by a PID controller that smoothens the movementof the Duckiebot. The performance of the algorithm was compared to that of theNeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all butone finalists. The two main contributions of our algorithm are its lowcomputational requirements and very quick set-up, with ongoing efforts to makeit more reliable.</description><author>Archit Gupta, Arvind Easwaran</author><pubDate>Mon, 09 Oct 2023 18:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10765v2</guid></item><item><title>Generative quantum machine learning via denoising diffusion probabilistic models</title><link>http://arxiv.org/abs/2310.05866v1</link><description>Deep generative models are key-enabling technology to computer vision, textgeneration and large language models. Denoising diffusion probabilistic models(DDPMs) have recently gained much attention due to their ability to generatediverse and high-quality samples in many computer vision tasks, as well as toincorporate flexible model architectures and relatively simple training scheme.Quantum generative models, empowered by entanglement and superposition, havebrought new insight to learning classical and quantum data. Inspired by theclassical counterpart, we propose the quantum denoising diffusion probabilisticmodels (QuDDPM) to enable efficiently trainable generative learning of quantumdata. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,while introduces multiple intermediate training tasks as interpolation betweenthe target distribution and noise to avoid barren plateau and guaranteeefficient training. We demonstrate QuDDPM's capability in learning correlatedquantum noise model and learning topological structure of nontrivialdistribution of quantum data.</description><author>Bingzhi Zhang, Peng Xu, Xiaohui Chen, Quntao Zhuang</author><pubDate>Mon, 09 Oct 2023 18:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05866v1</guid></item><item><title>Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction</title><link>http://arxiv.org/abs/2308.08653v2</link><description>Hyperspectral measurements from long range sensors can give a detailedpicture of the items, materials, and chemicals in a scene but analysis can bedifficult, slow, and expensive due to high spatial and spectral resolutions ofstate-of-the-art sensors. As such, sparsity is important to enable the futureof spectral compression and analytics. It has been observed that environmentaland atmospheric effects, including scattering, can produce nonlinear effectsposing challenges for existing source separation and compression methods. Wepresent a novel transformation into Hilbert spaces for pruning and constructingsparse representations via non-negative least squares minimization. Then weintroduce max likelihood compression vectors to decrease information loss. Ourapproach is benchmarked against standard pruning and least squares as well asdeep learning methods. Our methods are evaluated in terms of overall spectralreconstruction error and compression rate using real and synthetic data. Wefind that pruning least squares methods converge quickly unlike matchingpursuit methods. We find that Hilbert space pruning can reduce error by as muchas 40% of the error of standard pruning and also outperform neural networkautoencoders.</description><author>Michael G. Rawson, Timothy Doster</author><pubDate>Mon, 09 Oct 2023 18:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08653v2</guid></item><item><title>Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.05863v1</link><description>Audio-visual large language models (LLM) have drawn significant attention,yet the fine-grained combination of both input streams is ratherunder-explored, which is challenging but necessary for LLMs to understandgeneral video inputs. To this end, a fine-grained audio-visual jointrepresentation (FAVOR) learning framework for multimodal LLMs is proposed inthis paper, which extends a text-based LLM to simultaneously perceive speechand audio events in the audio input stream and images or videos in the visualinput stream, at the frame level. To fuse the audio and visual feature streamsinto joint representations and to align the joint space with the LLM inputembedding space, we propose a causal Q-Former structure with a causal attentionmodule to enhance the capture of causal relations of the audio-visual framesacross time. An audio-visual evaluation benchmark (AVEB) is also proposed whichcomprises six representative single-modal tasks with five cross-modal tasksreflecting audio-visual co-reasoning abilities. While achieving competitivesingle-modal performance on audio, speech and image tasks in AVEB, FAVORachieved over 20% accuracy improvements on the video question-answering taskwhen fine-grained information or temporal causal reasoning is required. FAVOR,in addition, demonstrated remarkable video comprehension and reasoningabilities on tasks that are unprecedented by other multimodal LLMs. Aninteractive demo of FAVOR is available athttps://github.com/the-anonymous-bs/FAVOR.git, and the training code and modelcheckpoints will be released upon acceptance.</description><author>Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</author><pubDate>Mon, 09 Oct 2023 18:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05863v1</guid></item><item><title>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</title><link>http://arxiv.org/abs/2310.05861v1</link><description>An increasing number of vision-language tasks can be handled with little tono training, i.e., in a zero and few-shot manner, by marrying large languagemodels (LLMs) to vision encoders, resulting in large vision-language models(LVLMs). While this has huge upsides, such as not requiring training data orcustom architectures, how an input is presented to a LVLM can have a majorimpact on zero-shot model performance. In particular, inputs phrased in anunderspecified way can result in incorrect answers due to factors like missingvisual information, complex implicit reasoning, or linguistic ambiguity.Therefore, adding visually grounded information to the input as a preemptiveclarification should improve model performance by reducing underspecification,e.g., by localizing objects and disambiguating references. Similarly, in theVQA setting, changing the way questions are framed can make them easier formodels to answer. To this end, we present Rephrase, Augment and Reason(RepARe), a gradient-free framework that extracts salient details about theimage using the underlying LVLM as a captioner and reasoner, in order topropose modifications to the original question. We then use the LVLM'sconfidence over a generated answer as an unsupervised scoring function toselect the rephrased question most likely to improve zero-shot performance.Focusing on two visual question answering tasks, we show that RepARe can resultin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%point increase on A-OKVQA. Additionally, we find that using gold answers fororacle question candidate selection achieves a substantial gain in VQA accuracyby up to 14.41%. Through extensive analysis, we demonstrate that outputs fromRepARe increase syntactic complexity, and effectively utilize vision-languageinteraction and the frozen language model in LVLMs.</description><author>Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Mon, 09 Oct 2023 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05861v1</guid></item><item><title>Knowledge Rumination for Pre-trained Language Models</title><link>http://arxiv.org/abs/2305.08732v2</link><description>Previous studies have revealed that vanilla pre-trained language models(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,several works have attempted to integrate external knowledge into PLMs.However, despite the promising outcome, we empirically observe that PLMs mayhave already encoded rich knowledge in their pre-trained parameters but fail tofully utilize them when applying them to knowledge-intensive tasks. In thispaper, we propose a new paradigm dubbed Knowledge Rumination to help thepre-trained language model utilize that related latent knowledge withoutretrieving it from the external corpus. By simply adding a prompt like "As faras I know" to the PLMs, we try to review related latent knowledge and injectthem back into the model for knowledge consolidation. We apply the proposedknowledge rumination to various language models, including RoBERTa, DeBERTa,and GPT-3. Experimental results on six commonsense reasoning tasks and GLUEbenchmarks demonstrate the effectiveness of our proposed approach, which provesthat the knowledge stored in PLMs can be better exploited to enhanceperformance. Code is available inhttps://github.com/zjunlp/knowledge-rumination.</description><author>Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 09 Oct 2023 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08732v2</guid></item><item><title>Equivariant Similarity for Vision-Language Foundation Models</title><link>http://arxiv.org/abs/2303.14465v2</link><description>This study explores the concept of equivariance in vision-language foundationmodels (VLMs), focusing specifically on the multimodal similarity function thatis not only the major training objective but also the core delivery to supportdownstream tasks. Unlike the existing image-text similarity objective whichonly categorizes matched pairs as similar and unmatched pairs as dissimilar,equivariance also requires similarity to vary faithfully according to thesemantic changes. This allows VLMs to generalize better to nuanced and unseenmultimodal compositions. However, modeling equivariance is challenging as theground truth of semantic change is difficult to collect. For example, given animage-text pair about a dog, it is unclear to what extent the similaritychanges when the pixel is changed from dog to cat? To this end, we proposeEqSim, a regularization loss that can be efficiently calculated from any twomatched training pairs and easily pluggable into existing image-text retrievalfine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, wepresent a new challenging benchmark EqBen. Compared to the existing evaluationsets, EqBen is the first to focus on "visual-minimal change". Extensiveexperiments show the lack of equivariance in current VLMs and validate theeffectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen.</description><author>Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang</author><pubDate>Mon, 09 Oct 2023 17:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14465v2</guid></item><item><title>DSAC-T: Distributional Soft Actor-Critic with Three Refinements</title><link>http://arxiv.org/abs/2310.05858v1</link><description>Reinforcement learning (RL) has proven to be highly effective in tacklingcomplex decision-making and control tasks. However, prevalent model-free RLmethods often face severe performance degradation due to the well-knownoverestimation issue. In response to this problem, we recently introduced anoff-policy RL algorithm, called distributional soft actor-critic (DSAC orDSAC-v1), which can effectively improve the value estimation accuracy bylearning a continuous Gaussian value distribution. Nonetheless, standard DSAChas its own shortcomings, including occasionally unstable learning processesand needs for task-specific reward scaling, which may hinder its overallperformance and adaptability in some special tasks. This paper furtherintroduces three important refinements to standard DSAC in order to addressthese shortcomings. These refinements consist of critic gradient adjusting,twin value distribution learning, and variance-based target return clipping.The modified RL algorithm is named as DSAC with three refinements (DSAC-T orDSAC-v2), and its performances are systematically evaluated on a diverse set ofbenchmark tasks. Without any task-specific hyperparameter tuning, DSAC-Tsurpasses a lot of mainstream model-free RL algorithms, including SAC, TD3,DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlikeits standard version, ensures a highly stable learning process and deliverssimilar performance across varying reward scales.</description><author>Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, Shengbo Eben Li</author><pubDate>Mon, 09 Oct 2023 17:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05858v1</guid></item><item><title>Improving Summarization with Human Edits</title><link>http://arxiv.org/abs/2310.05857v1</link><description>Recent work has shown the promise of learning with human feedback paradigmsto produce human-determined high-quality text. Existing works use humanfeedback to train large language models (LLMs) in general domain abstractivesummarization and have obtained summary quality exceeding traditionallikelihood training. In this paper, we focus on a less explored form of humanfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training(SALT), a novel technique to use both the human-edited and model-generated datatogether in the training loop. In addition, we demonstrate simulating HumanEdits with ground truth summaries coming from existing training data --Imitation edits, along with the model-generated summaries obtained after thetraining, to reduce the need for expensive human-edit data. In our experiments,we extend human feedback exploration from general domain summarization tomedical domain summarization. Our results demonstrate the effectiveness of SALTto improve the summary quality with Human and Imitation Edits.</description><author>Zonghai Yao, Benjamin J Schloss, Sai P. Selvaraj</author><pubDate>Mon, 09 Oct 2023 17:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05857v1</guid></item><item><title>On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks</title><link>http://arxiv.org/abs/2208.03835v2</link><description>As large-scale training regimes have gained popularity, the use of pretrainedmodels for downstream tasks has become common practice in machine learning.While pretraining has been shown to enhance the performance of models inpractice, the transfer of robustness properties from pretraining to downstreamtasks remains poorly understood. In this study, we demonstrate that therobustness of a linear predictor on downstream tasks can be constrained by therobustness of its underlying representation, regardless of the protocol usedfor pretraining. We prove (i) a bound on the loss that holds independent of anydownstream task, as well as (ii) a criterion for robust classification inparticular. We validate our theoretical results in practical applications, showhow our results can be used for calibrating expectations of downstreamrobustness, and when our results are useful for optimal transfer learning.Taken together, our results offer an initial step towards characterizing therequirements of the representation function for reliable post-adaptationperformance.</description><author>Laura Fee Nern, Harsh Raj, Maurice Georgi, Yash Sharma</author><pubDate>Mon, 09 Oct 2023 17:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.03835v2</guid></item><item><title>A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing</title><link>http://arxiv.org/abs/2310.03758v2</link><description>In generative compressed sensing (GCS), we want to recover a signal$\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using agenerative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typicallyan $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ representsthe radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements,most prior results are non-uniform, i.e., they hold with high probability for afixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In thispaper, we build a unified framework to derive uniform recovery guarantees fornonlinear GCS where the observation model is nonlinear and possiblydiscontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformlyquantized observations and single index models as canonical examples.Specifically, using a single realization of the sensing ensemble andgeneralized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can berecovered up to an $\ell_2$-error at most $\epsilon$ using roughly$\tilde{O}({k}/{\epsilon^2})$ samples, with omitted logarithmic factorstypically being dominated by $\log L$. Notably, this almost coincides withexisting non-uniform guarantees up to logarithmic factors, hence the uniformitycosts very little. As part of our technical contributions, we introduce theLipschitz approximation to handle discontinuous observation models. We alsodevelop a concentration inequality that produces tighter bounds for productprocesses whose index sets have low metric entropy. Experimental results arepresented to corroborate our theory.</description><author>Junren Chen, Jonathan Scarlett, Michael K. Ng, Zhaoqiang Liu</author><pubDate>Mon, 09 Oct 2023 17:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03758v2</guid></item><item><title>GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</title><link>http://arxiv.org/abs/2310.05845v1</link><description>The advancement of Large Language Models (LLMs) has remarkably pushed theboundaries towards artificial general intelligence (AGI), with theirexceptional ability on understanding diverse types of information, includingbut not limited to images and audio. Despite this progress, a critical gapremains in empowering LLMs to proficiently understand and reason on graph data.Recent studies underscore LLMs' underwhelming performance on fundamental graphreasoning tasks. In this paper, we endeavor to unearth the obstacles thatimpede LLMs in graph reasoning, pinpointing the common practice of convertinggraphs into natural language descriptions (Graph2Text) as a fundamentalbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneeringend-to-end approach that synergistically integrates graph learning models withLLMs. This synergy equips LLMs with the ability to proficiently interpret andreason on graph data, harnessing the superior expressive power of graphlearning models. Our empirical evaluations across four fundamental graphreasoning tasks validate the effectiveness of GraphLLM. The results exhibit asubstantial average accuracy enhancement of 54.44%, alongside a noteworthycontext reduction of 96.45% across various graph reasoning tasks.</description><author>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang</author><pubDate>Mon, 09 Oct 2023 17:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05845v1</guid></item><item><title>Robust Angular Synchronization via Directed Graph Neural Networks</title><link>http://arxiv.org/abs/2310.05842v1</link><description>The angular synchronization problem aims to accurately estimate (up to aconstant additive phase) a set of unknown angles $\theta_1, \dots,\theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets$\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example,sensor network localization, phase retrieval, and distributed clocksynchronization. An extension of the problem to the heterogeneous setting(dubbed $k$-synchronization) is to estimate $k$ groups of anglessimultaneously, given noisy observations (with unknown group assignment) fromeach group. Existing methods for angular synchronization usually perform poorlyin high-noise regimes, which are common in applications. In this paper, weleverage neural networks for the angular synchronization problem, and itsheterogeneous extension, by proposing GNNSync, a theoretically-groundedend-to-end trainable framework using directed graph neural networks. Inaddition, new loss functions are devised to encode synchronization objectives.Experimental results on extensive data sets demonstrate that GNNSync attainscompetitive, and often superior, performance against a comprehensive set ofbaselines for the angular synchronization problem and its extension, validatingthe robustness of GNNSync even at high noise levels.</description><author>Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu</author><pubDate>Mon, 09 Oct 2023 17:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05842v1</guid></item><item><title>Predicting Accident Severity: An Analysis Of Factors Affecting Accident Severity Using Random Forest Model</title><link>http://arxiv.org/abs/2310.05840v1</link><description>Road accidents have significant economic and societal costs, with a smallnumber of severe accidents accounting for a large portion of these costs.Predicting accident severity can help in the proactive approach to road safetyby identifying potential unsafe road conditions and taking well-informedactions to reduce the number of severe accidents. This study investigates theeffectiveness of the Random Forest machine learning algorithm for predictingthe severity of an accident. The model is trained on a dataset of accidentrecords from a large metropolitan area and evaluated using various metrics.Hyperparameters and feature selection are optimized to improve the model'sperformance. The results show that the Random Forest model is an effective toolfor predicting accident severity with an accuracy of over 80%. The study alsoidentifies the top six most important variables in the model, which includewind speed, pressure, humidity, visibility, clear conditions, and cloud cover.The fitted model has an Area Under the Curve of 80%, a recall of 79.2%, aprecision of 97.1%, and an F1 score of 87.3%. These results suggest that theproposed model has higher performance in explaining the target variable, whichis the accident severity class. Overall, the study provides evidence that theRandom Forest model is a viable and reliable tool for predicting accidentseverity and can be used to help reduce the number of fatalities and injuriesdue to road accidents in the United States</description><author>Adekunle Adefabi, Somtobe Olisah, Callistus Obunadike, Oluwatosin Oyetubo, Esther Taiwo, Edward Tella</author><pubDate>Mon, 09 Oct 2023 17:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05840v1</guid></item><item><title>Real-world Machine Learning Systems: A survey from a Data-Oriented Architecture Perspective</title><link>http://arxiv.org/abs/2302.04810v2</link><description>Machine Learning models are being deployed as parts of real-world systemswith the upsurge of interest in artificial intelligence. The design,implementation, and maintenance of such systems are challenged by real-worldenvironments that produce larger amounts of heterogeneous data and usersrequiring increasingly faster responses with efficient resource consumption.These requirements push prevalent software architectures to the limit whendeploying ML-based systems. Data-oriented Architecture (DOA) is an emergingconcept that equips systems better for integrating ML models. DOA extendscurrent architectures to create data-driven, loosely coupled, decentralised,open systems. Even though papers on deployed ML-based systems do not mentionDOA, their authors made design decisions that implicitly follow DOA. Thereasons why, how, and the extent to which DOA is adopted in these systems areunclear. Implicit design decisions limit the practitioners' knowledge of DOA todesign ML-based systems in the real world. This paper answers these questionsby surveying real-world deployments of ML-based systems. The survey shows thedesign decisions of the systems and the requirements these satisfy. Based onthe survey findings, we also formulate practical advice to facilitate thedeployment of ML-based systems. Finally, we outline open challenges todeploying DOA-based systems that integrate ML models.</description><author>Christian Cabrera, Andrei Paleyes, Pierre Thodoroff, Neil D. Lawrence</author><pubDate>Mon, 09 Oct 2023 17:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04810v2</guid></item><item><title>Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models</title><link>http://arxiv.org/abs/2305.19187v2</link><description>Large language models (LLMs) specializing in natural language generation(NLG) have recently started exhibiting promising capabilities across a varietyof domains. However, gauging the trustworthiness of responses generated by LLMsremains an open challenge, with limited research on uncertainty quantification(UQ) for NLG. Furthermore, existing literature typically assumes white-boxaccess to language models, which is becoming unrealistic either due to theclosed-source nature of the latest LLMs or computational constraints. In thiswork, we investigate UQ in NLG for black-box LLMs. We first differentiateuncertainty vs confidence: the former refers to the "dispersion" of thepotential predictions for a fixed input, and the latter refers to theconfidence on a particular prediction/generation. We then propose and compareseveral confidence/uncertainty metrics, applying them to selective NLG whereunreliable results could either be ignored or yielded for further assessment.Experiments were carried out with several popular LLMs on question-answeringdatasets (for evaluation purposes). Results reveal that a simple metric for thesemantic dispersion can be a reliable predictor of the quality of LLMresponses, providing valuable insights for practitioners on uncertaintymanagement when adopting LLMs. The code to replicate our experiments isavailable at https://github.com/zlin7/UQ-NLG.</description><author>Zhen Lin, Shubhendu Trivedi, Jimeng Sun</author><pubDate>Mon, 09 Oct 2023 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19187v2</guid></item><item><title>A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields</title><link>http://arxiv.org/abs/2310.05837v1</link><description>We present the first real-time method for inserting a rigid virtual objectinto a neural radiance field, which produces realistic lighting and shadowingeffects, as well as allows interactive manipulation of the object. Byexploiting the rich information about lighting and geometry in a NeRF, ourmethod overcomes several challenges of object insertion in augmented reality.For lighting estimation, we produce accurate, robust and 3D spatially-varyingincident lighting that combines the near-field lighting from NeRF and anenvironment lighting to account for sources not covered by the NeRF. Forocclusion, we blend the rendered virtual object with the background scene usingan opacity map integrated from the NeRF. For shadows, with a precomputed fieldof spherical signed distance field, we query the visibility term for any pointaround the virtual object, and cast soft, detailed shadows onto 3D surfaces.Compared with state-of-the-art techniques, our approach can insert virtualobject into scenes with superior fidelity, and has a great potential to befurther applied to augmented reality systems.</description><author>Keyang Ye, Hongzhi Wu, Xin Tong, Kun Zhou</author><pubDate>Mon, 09 Oct 2023 17:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05837v1</guid></item><item><title>Recent Advancements in Machine Learning For Cybercrime Prediction</title><link>http://arxiv.org/abs/2304.04819v2</link><description>Cybercrime is a growing threat to organizations and individuals worldwide,with criminals using sophisticated techniques to breach security systems andsteal sensitive data. This paper aims to comprehensively survey the latestadvancements in cybercrime prediction, highlighting the relevant research. Forthis purpose, we reviewed more than 150 research articles and discussed 50 mostrecent and appropriate ones. We start the review with some standard methodscybercriminals use and then focus on the latest machine and deep learningtechniques, which detect anomalous behavior and identify potential threats. Wealso discuss transfer learning, which allows models trained on one dataset tobe adapted for use on another dataset. We then focus on active andreinforcement learning as part of early-stage algorithmic research incybercrime prediction. Finally, we discuss critical innovations, research gaps,and future research opportunities in Cybercrime prediction. This paper presentsa holistic view of cutting-edge developments and publicly available datasets.</description><author>Lavanya Elluri, Varun Mandalapu, Piyush Vyas, Nirmalya Roy</author><pubDate>Mon, 09 Oct 2023 17:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04819v2</guid></item><item><title>TransHP: Image Classification with Hierarchical Prompting</title><link>http://arxiv.org/abs/2304.06385v3</link><description>This paper explores a hierarchical prompting mechanism for the hierarchicalimage classification (HIC) task. Different from prior HIC methods, ourhierarchical prompting is the first to explicitly inject ancestor-classinformation as a tokenized hint that benefits the descendant-classdiscrimination. We think it well imitates human visual recognition, i.e.,humans may use the ancestor class as a prompt to draw focus on the subtledifferences among descendant classes. We model this prompting mechanism into aTransformer with Hierarchical Prompting (TransHP). TransHP consists of threesteps: 1) learning a set of prompt tokens to represent the coarse (ancestor)classes, 2) on-the-fly predicting the coarse class of the input image at anintermediate block, and 3) injecting the prompt token of the predicted coarseclass into the intermediate feature. Though the parameters of TransHP maintainthe same for all input images, the injected coarse-class prompt conditions(modifies) the subsequent feature extraction and encourages a dynamic focus onrelatively subtle differences among the descendant classes. Extensiveexperiments show that TransHP improves image classification on accuracy (e.g.,improving ViT-B/16 by +2.83% ImageNet classification accuracy), training dataefficiency (e.g., +12.69% improvement under 10% ImageNet training data), andmodel explainability. Moreover, TransHP also performs favorably against priorHIC methods, showing that TransHP well exploits the hierarchical information.</description><author>Wenhao Wang, Yifan Sun, Wei Li, Yi Yang</author><pubDate>Mon, 09 Oct 2023 17:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06385v3</guid></item><item><title>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</title><link>http://arxiv.org/abs/2309.12284v3</link><description>Large language models (LLMs) have pushed the limits of natural languageunderstanding and exhibited excellent problem-solving ability. Despite thegreat success, most existing open-source LLMs (e.g., LLaMA-2) are still faraway from satisfactory for solving mathematical problem due to the complexreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tunedlanguage model that specializes in mathematical reasoning. Specifically, westart by bootstrapping mathematical questions by rewriting the question frommultiple perspectives without extra knowledge, which results in a new datasetcalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.Experimental results on two popular benchmarks (i.e., GSM8K and MATH) formathematical reasoning demonstrate that MetaMath outperforms a suite ofopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the samesize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all theMetaMathQA dataset, the MetaMath models with different model sizes and thetraining code for public use.</description><author>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu</author><pubDate>Mon, 09 Oct 2023 17:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12284v3</guid></item><item><title>A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models</title><link>http://arxiv.org/abs/2310.05833v1</link><description>Generative models, like large language models, are becoming increasinglyrelevant in our daily lives, yet a theoretical framework to assess theirgeneralization behavior and uncertainty does not exist. Particularly, theproblem of uncertainty estimation is commonly solved in an ad-hoc manner andtask dependent. For example, natural language approaches cannot be transferredto image generation. In this paper we introduce the firstbias-variance-covariance decomposition for kernel scores and their associatedentropy. We propose unbiased and consistent estimators for each quantity whichonly require generated samples but not the underlying model itself. As anapplication, we offer a generalization evaluation of diffusion models anddiscover how mode collapse of minority groups is a contrary phenomenon tooverfitting. Further, we demonstrate that variance and predictive kernelentropy are viable measures of uncertainty for image, audio, and languagegeneration. Specifically, our approach for uncertainty estimation is morepredictive of performance on CoQA and TriviaQA question answering datasets thanexisting baselines and can also be applied to closed-source models.</description><author>Sebastian G. Gruber, Florian Buettner</author><pubDate>Mon, 09 Oct 2023 17:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05833v1</guid></item><item><title>Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization</title><link>http://arxiv.org/abs/2310.04015v2</link><description>While personalized recommendations systems have become increasingly popular,ensuring user data protection remains a top concern in the development of theselearning systems. A common approach to enhancing privacy involves trainingmodels using anonymous data rather than individual data. In this paper, weexplore a natural technique called \emph{look-alike clustering}, which involvesreplacing sensitive features of individuals with the cluster's average values.We provide a precise analysis of how training models using anonymous clustercenters affects their generalization capabilities. We focus on an asymptoticregime where the size of the training set grows in proportion to the featuresdimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT)and allows us to theoretically understand the role of different modelcomponents on the generalization error. In addition, we demonstrate that incertain high-dimensional regimes, training over anonymous cluster centers actsas a regularization and improves generalization error of the trained models.Finally, we corroborate our asymptotic theory with finite-sample numericalexperiments where we observe a perfect match when the sample size is only oforder of a few hundreds.</description><author>Adel Javanmard, Vahab Mirrokni</author><pubDate>Mon, 09 Oct 2023 17:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04015v2</guid></item><item><title>Graph Theory Applications in Advanced Geospatial Research</title><link>http://arxiv.org/abs/2309.03249v2</link><description>Geospatial sciences include a wide range of applications, from environmentalmonitoring transportation to infrastructure planning, as well as location-basedanalysis and services. Graph theory algorithms in mathematics have emerged asindispensable tools in these domains due to their capability to model andanalyse spatial relationships efficiently. This article explores theapplications of graph theory algorithms in geospatial sciences, highlightingtheir role in network analysis, spatial connectivity, geographic informationsystems, and various other spatial problem-solving scenarios like digital twin.The article provides a comprehensive idea about graph theory's key concepts andalgorithms that assist the geospatial modelling processes and insights intoreal-world geospatial challenges and opportunities. It lists the extensiveresearch, innovative technologies and methodologies implemented in this domain.</description><author>Surajit Ghosh, Archita Mallick, Anuva Chowdhury, Kounik De Sarkar</author><pubDate>Mon, 09 Oct 2023 17:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03249v2</guid></item><item><title>Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning under A Unified View</title><link>http://arxiv.org/abs/2310.05829v1</link><description>Spatio-temporal predictive learning plays a crucial role in self-supervisedlearning, with wide-ranging applications across a diverse range of fields.Previous approaches for temporal modeling fall into two categories:recurrent-based and recurrent-free methods. The former, while meticulouslyprocessing frames one by one, neglect short-term spatio-temporal informationredundancies, leading to inefficiencies. The latter naively stack framessequentially, overlooking the inherent temporal dependencies. In this paper, were-examine the two dominant temporal modeling approaches within the realm ofspatio-temporal predictive learning, offering a unified perspective. Buildingupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictivelearning), an innovative framework that reconciles the recurrent-based andrecurrent-free methods by integrating both micro-temporal and macro-temporalscales. Extensive experiments on a wide range of spatio-temporal predictivelearning demonstrate that USTEP achieves significant improvements over existingtemporal modeling approaches, thereby establishing it as a robust solution fora wide range of spatio-temporal applications.</description><author>Cheng Tan, Jue Wang, Zhangyang Gao, Siyuan Li, Lirong Wu, Jun Xia, Stan Z. Li</author><pubDate>Mon, 09 Oct 2023 17:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05829v1</guid></item><item><title>Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting</title><link>http://arxiv.org/abs/2310.05824v1</link><description>Terminology correctness is important in the downstream application of machinetranslation, and a prevalent way to ensure this is to inject terminologyconstraints into a translation system. In our submission to the WMT 2023terminology translation task, we adopt a translate-then-refine approach whichcan be domain-independent and requires minimal manual efforts. We annotaterandom source words with pseudo-terminology translations obtained from wordalignment to first train a terminology-aware model. Further, we explore twopost-processing methods. First, we use an alignment process to discover whethera terminology constraint has been violated, and if so, we re-decode with theviolating word negatively constrained. Alternatively, we leverage a largelanguage model to refine a hypothesis by providing it with terminologyconstraints. Results show that our terminology-aware model learns toincorporate terminologies effectively, and the large language model refinementprocess can further improve terminology recall.</description><author>Nikolay Bogoychev, Pinzhen Chen</author><pubDate>Mon, 09 Oct 2023 17:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05824v1</guid></item><item><title>Pre-trained Spatial Priors on Multichannel NMF for Music Source Separation</title><link>http://arxiv.org/abs/2310.05821v1</link><description>This paper presents a novel approach to sound source separation thatleverages spatial information obtained during the recording setup. Our methodtrains a spatial mixing filter using solo passages to capture information aboutthe room impulse response and transducer response at each sensor location. Thispre-trained filter is then integrated into a multichannel non-negative matrixfactorization (MNMF) scheme to better capture the variances of different soundsources. The recording setup used in our experiments is the typical setup fororchestra recordings, with a main microphone and a close "cardioid" or"supercardioid" microphone for each section of the orchestra. This makes theproposed method applicable to many existing recordings. Experiments onpolyphonic ensembles demonstrate the effectiveness of the proposed framework inseparating individual sound sources, improving performance compared toconventional MNMF methods.</description><author>Pablo Cabanas-Molero, Antonio J. Munoz-Montoro, Julio Carabias-Orti, Pedro Vera-Candeas</author><pubDate>Mon, 09 Oct 2023 17:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05821v1</guid></item><item><title>SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese</title><link>http://arxiv.org/abs/2310.05818v1</link><description>Large language models (LLMs), like ChatGPT and GPT-4, have demonstratedremarkable abilities in natural language understanding and generation. However,alongside their positive impact on our daily tasks, they can also produceharmful content that negatively affects societal perceptions. To systematicallyassess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -a multi-round adversarial benchmark with 4912 open-ended questions coveringmore than 20 safety sub-dimensions. Adversarial human-model interactions andconversations significantly increase the challenges compared to existingmethods. Experiments on 13 major LLMs supporting Chinese yield the followinginsights: 1) Closed-source models outperform open-sourced ones in terms ofsafety; 2) Models released from China demonstrate comparable safety levels toLLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters cancompete effectively in terms of safety. By introducing SC-Safety, we aim topromote collaborative efforts to create safer and more trustworthy LLMs. Thebenchmark and findings provide guidance on model selection. Our benchmark canbe found at https://www.CLUEbenchmarks.com</description><author>Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue</author><pubDate>Mon, 09 Oct 2023 17:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05818v1</guid></item><item><title>Deep learning based projection domain metal segmentation for metal artifact reduction in cone beam computed tomography</title><link>http://arxiv.org/abs/2208.08288v2</link><description>Metal artifact correction is a challenging problem in cone beam computedtomography (CBCT) scanning. Metal implants inserted into the anatomy causesevere artifacts in reconstructed images. Widely used inpainting-based metalartifact reduction (MAR) methods require segmentation of metal traces in theprojections as a first step, which is a challenging task. One approach is touse a deep learning method to segment metals in the projections. However, thesuccess of deep learning methods is limited by the availability of realistictraining data. It is laborious and time consuming to get reliable ground truthannotations due to unclear implant boundaries and large numbers of projections.We propose to use X-ray simulations to generate synthetic metal segmentationtraining dataset from clinical CBCT scans. We compare the effect of simulationswith different numbers of photons and also compare several training strategiesto augment the available data. We compare our model's performance on realclinical scans with conventional region growing threshold-based MAR, movingmetal artifact reduction method, and a recent deep learning method. We showthat simulations with relatively small number of photons are suitable for themetal segmentation task and that training the deep learning model with fullsize and cropped projections together improves the robustness of the model. Weshow substantial improvement in the image quality affected by severe motion,voxel size under-sampling, and out-of-FOV metals. Our method can be easilyintegrated into the existing projection-based MAR pipeline to get improvedimage quality. This method can provide a novel paradigm to accurately segmentmetals in CBCT projections.</description><author>Harshit Agrawal, Ari Hietanen, Simo Särkkä</author><pubDate>Mon, 09 Oct 2023 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08288v2</guid></item><item><title>AI-Generated Images as Data Source: The Dawn of Synthetic Era</title><link>http://arxiv.org/abs/2310.01830v2</link><description>The advancement of visual intelligence is intrinsically tethered to theavailability of data. In parallel, generative Artificial Intelligence (AI) hasunlocked the potential to create synthetic images that closely resemblereal-world photographs, which prompts a compelling inquiry: how visualintelligence benefit from the advance of generative AI? This paper explores theinnovative concept of harnessing these AI-generated images as a new datasource, reshaping traditional model paradigms in visual intelligence. Incontrast to real data, AI-generated data sources exhibit remarkable advantages,including unmatched abundance and scalability, the rapid generation of vastdatasets, and the effortless simulation of edge cases. Built on the success ofgenerative AI models, we examines the potential of their generated data in arange of applications, from training machine learning models to simulatingscenarios for computational modeling, testing, and validation. We probe thetechnological foundations that support this groundbreaking use of generativeAI, engaging in an in-depth discussion on the ethical, legal, and practicalconsiderations that accompany this transformative paradigm shift. Through anexhaustive survey of current technologies and applications, this paper presentsa comprehensive view of the synthetic era in visual intelligence. A projectassociated with this paper can be found at https://github.com/mwxely/AIGS .</description><author>Zuhao Yang, Fangneng Zhan, Kunhao Liu, Muyu Xu, Shijian Lu</author><pubDate>Mon, 09 Oct 2023 17:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01830v2</guid></item><item><title>Defending Against Backdoor Attacks in Natural Language Generation</title><link>http://arxiv.org/abs/2106.01810v3</link><description>The frustratingly fragile nature of neural network models make currentnatural language generation (NLG) systems prone to backdoor attacks andgenerate malicious sequences that could be sexist or offensive. Unfortunately,little effort has been invested to how backdoor attacks can affect current NLGmodels and how to defend against these attacks. In this work, by giving aformal definition of backdoor attack and defense, we investigate this problemon two important NLG tasks, machine translation and dialog generation. Tailoredto the inherent nature of NLG models (e.g., producing a sequence of coherentwords given contexts), we design defending strategies against attacks. We findthat testing the backward probability of generating sources given targetsyields effective defense performance against all different types of attacks,and is able to handle the {\it one-to-many} issue in many NLG tasks such asdialog generation. We hope that this work can raise the awareness of backdoorrisks concealed in deep NLG systems and inspire more future work (both attackand defense) towards this direction.</description><author>Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, Tianwei Zhang</author><pubDate>Mon, 09 Oct 2023 16:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.01810v3</guid></item><item><title>Provably Convergent Data-Driven Convex-Nonconvex Regularization</title><link>http://arxiv.org/abs/2310.05812v1</link><description>An emerging new paradigm for solving inverse problems is via the use of deeplearning to learn a regularizer from data. This leads to high-quality results,but often at the cost of provable guarantees. In this work, we show howwell-posedness and convergent regularization arises within the convex-nonconvex(CNC) framework for inverse problems. We introduce a novel input weakly convexneural network (IWCNN) construction to adapt the method of learned adversarialregularization to the CNC framework. Empirically we show that our methodovercomes numerical issues of previous adversarial methods.</description><author>Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, Carola-Bibiane Schönlieb</author><pubDate>Mon, 09 Oct 2023 16:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05812v1</guid></item><item><title>Text Classification via Large Language Models</title><link>http://arxiv.org/abs/2305.08377v3</link><description>Despite the remarkable success of large-scale Language Models (LLMs) such asGPT-3, their performances still significantly underperform fine-tuned models inthe task of text classification. This is due to (1) the lack of reasoningability in addressing complex linguistic phenomena (e.g., intensification,contrast, irony etc); (2) limited number of tokens allowed in in-contextlearning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adoptsa progressive reasoning strategy tailored to addressing the complex linguisticphenomena involved in text classification: CARP first prompts LLMs to findsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),based on which a diagnostic reasoning process is induced for final decisions.To further address the limited-token issue, CARP uses a fine-tuned model on thesupervised dataset for $k$NN demonstration search in the in-context learning,allowing the model to take the advantage of both LLM's generalization abilityand the task-specific evidence provided by the full labeled dataset.Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-usedtext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) onAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performancecomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARPdelivers impressive abilities on low-resource and domain-adaptation setups.Specifically, using 16 examples per class, CARP achieves comparableperformances to supervised models with 1,024 examples per class.</description><author>Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang</author><pubDate>Mon, 09 Oct 2023 16:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08377v3</guid></item><item><title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model</title><link>http://arxiv.org/abs/2303.05118v4</link><description>The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research. Code has been made available at:https://github.com/GengDavid/SLCA.</description><author>Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei</author><pubDate>Mon, 09 Oct 2023 16:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05118v4</guid></item><item><title>COPR: Consistency-Oriented Pre-Ranking for Online Advertising</title><link>http://arxiv.org/abs/2306.03516v2</link><description>Cascading architecture has been widely adopted in large-scale advertisingsystems to balance efficiency and effectiveness. In this architecture, thepre-ranking model is expected to be a lightweight approximation of the rankingmodel, which handles more candidates with strict latency requirements. Due tothe gap in model capacity, the pre-ranking and ranking models usually generateinconsistent ranked results, thus hurting the overall system effectiveness. Theparadigm of score alignment is proposed to regularize their raw scores to beconsistent. However, it suffers from inevitable alignment errors and erroramplification by bids when applied in online advertising. To this end, weintroduce a consistency-oriented pre-ranking framework for online advertising,which employs a chunk-based sampling module and a plug-and-play rank alignmentmodule to explicitly optimize consistency of ECPM-ranked results. A $\DeltaNDCG$-based weighting mechanism is adopted to better distinguish the importanceof inter-chunk samples in optimization. Both online and offline experimentshave validated the superiority of our framework. When deployed in Taobaodisplay advertising system, it achieves an improvement of up to +12.3\% CTR and+5.6\% RPM.</description><author>Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, Bo Zheng</author><pubDate>Mon, 09 Oct 2023 16:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03516v2</guid></item><item><title>Pushing the Limits of ChatGPT on NLP Tasks</title><link>http://arxiv.org/abs/2306.09719v2</link><description>Despite the success of ChatGPT, its performances on most NLP tasks are stillwell below the supervised baselines. In this work, we looked into the causes,and discovered that its subpar performance was caused by the following factors:(1) token limit in the prompt does not allow for the full utilization of thesupervised datasets; (2) mismatch between the generation nature of ChatGPT andNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overlyfocus on certain keywords, etc. In this work, we propose a collection of general modules to address theseissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposedmodules include (1) a one-input-multiple-prompts strategy that employs multipleprompts for one input to accommodate more demonstrations; (2) using fine-tunedmodels for better demonstration retrieval; (3) transforming tasks to formatsthat are more tailored to the generation nature; (4) employing reasoningstrategies that are tailored to addressing the task-specific complexity; (5)the self-verification strategy to address the hallucination issue of LLMs; (6)the paraphrase strategy to improve the robustness of model predictions. We conduct experiments on 21 datasets of 10 representative NLP tasks,including question answering, commonsense reasoning, natural languageinference, sentiment analysis, named entity recognition, entity-relationextraction, event extraction, dependency parsing, semantic role labeling, andpart-of-speech tagging. Using the proposed assemble of techniques, we are ableto significantly boost the performance of ChatGPT on the selected NLP tasks,achieving performances comparable to or better than supervised baselines, oreven existing SOTA performances.</description><author>Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, Guoyin Wang</author><pubDate>Mon, 09 Oct 2023 16:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09719v2</guid></item><item><title>Sharing Information Between Machine Tools to Improve Surface Finish Forecasting</title><link>http://arxiv.org/abs/2310.05807v1</link><description>At present, most surface-quality prediction methods can only performsingle-task prediction which results in under-utilised datasets, repetitivework and increased experimental costs. To counter this, the authors propose aBayesian hierarchical model to predict surface-roughness measurements for aturning machining process. The hierarchical model is compared to multipleindependent Bayesian linear regression models to showcase the benefits ofpartial pooling in a machining setting with respect to prediction accuracy anduncertainty quantification.</description><author>Daniel R. Clarkson, Lawrence A. Bull, Tina A. Dardeno, Chandula T. Wickramarachchi, Elizabeth J. Cross, Timothy J. Rogers, Keith Worden, Nikolaos Dervilis, Aidan J. Hughes</author><pubDate>Mon, 09 Oct 2023 16:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05807v1</guid></item><item><title>Boosted Control Functions</title><link>http://arxiv.org/abs/2310.05805v1</link><description>Modern machine learning methods and the availability of large-scale dataopened the door to accurately predict target quantities from large sets ofcovariates. However, existing prediction methods can perform poorly when thetraining and testing data are different, especially in the presence of hiddenconfounding. While hidden confounding is well studied for causal effectestimation (e.g., instrumental variables), this is not the case for predictiontasks. This work aims to bridge this gap by addressing predictions underdifferent training and testing distributions in the presence of unobservedconfounding. In particular, we establish a novel connection between the fieldof distribution generalization from machine learning, and simultaneous equationmodels and control function from econometrics. Central to our contribution aresimultaneous equation models for distribution generalization (SIMDGs) whichdescribe the data-generating process under a set of distributional shifts.Within this framework, we propose a strong notion of invariance for apredictive model and compare it with existing (weaker) versions. Building onthe control function approach from instrumental variable regression, we proposethe boosted control function (BCF) as a target of inference and prove itsability to successfully predict even in intervened versions of the underlyingSIMDG. We provide necessary and sufficient conditions for identifying the BCFand show that it is worst-case optimal. We introduce the ControlTwicingalgorithm to estimate the BCF and analyze its predictive performance onsimulated and real world data.</description><author>Nicola Gnecco, Jonas Peters, Sebastian Engelke, Niklas Pfister</author><pubDate>Mon, 09 Oct 2023 16:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05805v1</guid></item><item><title>How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization</title><link>http://arxiv.org/abs/2310.01769v2</link><description>This paper rigorously shows how over-parameterization changes the convergencebehaviors of gradient descent (GD) for the matrix sensing problem, where thegoal is to recover an unknown low-rank ground-truth matrix from near-isotropiclinear measurements. First, we consider the symmetric setting with thesymmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is apositive semi-definite unknown matrix of rank $r \ll n$, and one uses asymmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n\times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$)where $T$ is the number of iterations. This is in stark contrast to theexact-parameterization scenario ($k=r$) where the convergence rate is $\exp(-\Omega (T))$. Next, we study asymmetric setting where $M^* \in\mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll\min\{n_1,n_2\}$, and one uses an asymmetric parameterization $FG^\top$ tolearn $M^*$ where $F \in \mathbb{R}^{n_1 \times k}$ and $G \in \mathbb{R}^{n_2\times k}$. Building on prior work, we give a global exact convergence resultof randomly initialized GD for the exact-parameterization case ($k=r$) with an$\exp (-\Omega(T))$ rate. Furthermore, we give the first global exactconvergence result for the over-parameterization case ($k&gt;r$) with an$\exp(-\Omega(\alpha^2 T))$ rate where $\alpha$ is the initialization scale.This linear convergence result in the over-parameterization case is especiallysignificant because one can apply the asymmetric parameterization to thesymmetric setting to speed up from $\Omega (1/T^2)$ to linear convergence. Onthe other hand, we propose a novel method that only modifies one step of GD andobtains a convergence rate independent of $\alpha$, recovering the rate in theexact-parameterization case.</description><author>Nuoya Xiong, Lijun Ding, Simon S. Du</author><pubDate>Mon, 09 Oct 2023 16:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01769v2</guid></item><item><title>Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis</title><link>http://arxiv.org/abs/2310.05804v1</link><description>Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing richinformation from multiple sources (e.g., language, video, and audio), thepotential sentiment-irrelevant and conflicting information across modalitiesmay hinder the performance from being further improved. To alleviate this, wepresent Adaptive Language-guided Multimodal Transformer (ALMT), whichincorporates an Adaptive Hyper-modality Learning (AHL) module to learn anirrelevance/conflict-suppressing representation from visual and audio featuresunder the guidance of language features at different scales. With the obtainedhyper-modality representation, the model can obtain a complementary and jointrepresentation through multimodal fusion for effective MSA. In practice, ALMTachieves state-of-the-art performance on several popular datasets (e.g., MOSI,MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity andnecessity of our irrelevance/conflict suppression mechanism.</description><author>Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu</author><pubDate>Mon, 09 Oct 2023 16:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05804v1</guid></item><item><title>PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator</title><link>http://arxiv.org/abs/2308.11534v3</link><description>The unparalleled performance of closed-sourced ChatGPT has sparked effortstowards its democratization, with notable strides made by leveraging real userand ChatGPT conversations, as evidenced by Vicuna. However, due to challengesin gathering conversations involving human participation, current endeavorslike Baize and UltraChat aim to automatically generate conversational data.They primarily rely on ChatGPT conducting roleplay to simulate human behaviorsbased on instructions rather than genuine learning from humans, resulting inlimited scope, diminished diversity, and an absence of genuine multi-roundconversational dynamics. To address the above issues, we target human questionsextracted from genuine human-machine conversations as a learning goal and traina user simulator called `Socratic' to produce a high-quality human-centricsynthetic conversation dataset. Subsequently, this dataset was used to trainour assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baselinemodels in both Vicuna-Bench and MT-Bench by pairwise comparison whenconsidering equivalent training set sizes, and manual evaluation also showsthat our model is highly competitive. Impressively, when fine-tuned with thelatest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and inAlpaca-Eval benchmark, it ranks second among 7B models, even beating somelarger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depthanalysis demonstrates the scalability and transferability of our approach. Thecode is available at https://github.com/FreedomIntelligence/PlatoLM.</description><author>Chuyi Kong, Yaxin Fan, Xiang Wan, Feng Jiang, Benyou Wang</author><pubDate>Mon, 09 Oct 2023 16:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11534v3</guid></item><item><title>LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2304.01933v3</link><description>The success of large language models (LLMs), like GPT-4 and ChatGPT, has ledto the development of numerous cost-effective and accessible alternatives thatare created by finetuning open-access LLMs with task-specific data (e.g.,ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuningmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedlyone of the most attractive topics, as it only requires fine-tuning a fewexternal parameters instead of the entire LLMs while achieving comparable oreven better performance. To enable further research on PEFT methods of LLMs,this paper presents LLM-Adapters, an easy-to-use framework that integratesvarious adapters into LLMs and can execute these adapter-based PEFT methods ofLLMs for different tasks. The framework includes state-of-the-art open-accessLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such asSeries adapters, Parallel adapter, Prompt-based learning andReparametrization-based methods. Moreover, we conduct extensive empiricalstudies on the impact of adapter types, placement locations, andhyper-parameters to the best design for each adapter-based methods. We evaluatethe effectiveness of the adapters on fourteen datasets from two differentreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The resultsdemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with fewextra trainable parameters yields comparable, and in some cases superior,performance to powerful LLMs (175B) in zero-shot inference on both reasoningtasks.</description><author>Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee</author><pubDate>Mon, 09 Oct 2023 16:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01933v3</guid></item><item><title>An operator preconditioning perspective on training in physics-informed machine learning</title><link>http://arxiv.org/abs/2310.05801v1</link><description>In this paper, we investigate the behavior of gradient descent algorithms inphysics-informed machine learning methods like PINNs, which minimize residualsconnected to partial differential equations (PDEs). Our key result is that thedifficulty in training these models is closely related to the conditioning of aspecific differential operator. This operator, in turn, is associated to theHermitian square of the differential operator of the underlying PDE. If thisoperator is ill-conditioned, it results in slow or infeasible training.Therefore, preconditioning this operator is crucial. We employ both rigorousmathematical analysis and empirical evaluations to investigate variousstrategies, explaining how they better condition this critical operator, andconsequently improve training.</description><author>Tim De Ryck, Florent Bonnet, Siddhartha Mishra, Emmanuel de Bézenac</author><pubDate>Mon, 09 Oct 2023 16:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05801v1</guid></item><item><title>Instruction Tuning for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2308.10792v4</link><description>This paper surveys research works in the quickly advancing field ofinstruction tuning (IT), a crucial technique to enhance the capabilities andcontrollability of large language models (LLMs). Instruction tuning refers tothe process of further training LLMs on a dataset consisting of\textsc{(instruction, output)} pairs in a supervised fashion, which bridges thegap between the next-word prediction objective of LLMs and the users' objectiveof having LLMs adhere to human instructions. In this work, we make a systematicreview of the literature, including the general methodology of IT, theconstruction of IT datasets, the training of IT models, and applications todifferent modalities, domains and applications, along with an analysis onaspects that influence the outcome of IT (e.g., generation of instructionoutputs, size of the instruction dataset, etc). We also review the potentialpitfalls of IT along with criticism against it, along with efforts pointing outcurrent deficiencies of existing strategies and suggest some avenues forfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey</description><author>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang</author><pubDate>Mon, 09 Oct 2023 16:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10792v4</guid></item><item><title>The First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss</title><link>http://arxiv.org/abs/2310.05799v1</link><description>The Cadenza project aims to improve the audio quality of music for those whohave a hearing loss. This is being done through a series of signal processingchallenges, to foster better and more inclusive technologies. In the firstround, two common listening scenarios are considered: listening to music overheadphones, and with a hearing aid in a car. The first scenario is cast as ademixing-remixing problem, where the music is decomposed into vocals, bass,drums and other components. These can then be intelligently remixed in apersonalized way, to increase the audio quality for a person who has a hearingloss. In the second scenario, music is coming from car loudspeakers, and themusic has to be enhanced to overcome the masking effect of the car noise. Thisis done by taking into account the music, the hearing ability of the listener,the hearing aid and the speed of the car. The audio quality of the submissionswill be evaluated using the Hearing Aid Audio Quality Index (HAAQI) forobjective assessment and by a panel of people with hearing loss for subjectiveevaluation.</description><author>Gerardo Roa Dabike, Scott Bannister, Jennifer Firth, Simone Graetzer, Rebecca Vos, Michael A. Akeroyd, Jon Barker, Trevor J. Cox, Bruno Fazenda, Alinka Greasley, William Whitmer</author><pubDate>Mon, 09 Oct 2023 16:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05799v1</guid></item><item><title>Are Large Language Models Post Hoc Explainers?</title><link>http://arxiv.org/abs/2310.05797v1</link><description>Large Language Models (LLMs) are increasingly used as powerful tools for aplethora of natural language processing (NLP) applications. A recentinnovation, in-context learning (ICL), enables LLMs to learn new tasks bysupplying a few examples in the prompt during inference time, therebyeliminating the need for model fine-tuning. While LLMs have been utilized inseveral applications, their applicability in explaining the behavior of othermodels remains relatively unexplored. Despite the growing number of newexplanation techniques, many require white-box access to the model and/or arecomputationally expensive, highlighting a need for next-generation post hocexplainers. In this work, we present the first framework to study theeffectiveness of LLMs in explaining other predictive models. More specifically,we propose a novel framework encompassing multiple prompting strategies: i)Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,and iv) Explanation-based ICL, with varying levels of information about theunderlying ML model and the local neighborhood of the test sample. We conductextensive experiments with real-world benchmark datasets to demonstrate thatLLM-generated explanations perform on par with state-of-the-art post hocexplainers using their ability to leverage ICL examples and their internalknowledge in generating model explanations. On average, across four datasetsand two ML models, we observe that LLMs identify the most important featurewith 72.19% accuracy, opening up new frontiers in explainable artificialintelligence (XAI) to explore LLM-based explanation frameworks.</description><author>Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</author><pubDate>Mon, 09 Oct 2023 16:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05797v1</guid></item><item><title>DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models</title><link>http://arxiv.org/abs/2310.05793v1</link><description>Diffusion models have gained prominence in generating high-quality sequencesof text. Nevertheless, current approaches predominantly represent discrete textwithin a continuous diffusion space, which incurs substantial computationaloverhead during training and results in slower sampling speeds. In this paper,we introduce a soft absorbing state that facilitates the diffusion model inlearning to reconstruct discrete mutations based on the underlying Gaussianspace, thereby enhancing its capacity to recover conditional signals. Duringthe sampling phase, we employ state-of-the-art ODE solvers within thecontinuous space to expedite the sampling process. Comprehensive experimentalevaluations reveal that our proposed method effectively accelerates thetraining convergence by 4x and generates samples of similar quality 800xfaster, rendering it significantly closer to practical application.\footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}</description><author>Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong</author><pubDate>Mon, 09 Oct 2023 16:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05793v1</guid></item><item><title>Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems</title><link>http://arxiv.org/abs/2310.05791v1</link><description>The recent program development industries have required problem-solvingabilities for engineers, especially application developers. However, AI-basededucation systems to help solve computer algorithm problems have not yetattracted attention, while most big tech companies require the ability to solvealgorithm problems including Google, Meta, and Amazon. The most useful guide tosolving algorithm problems might be guessing the category (tag) of the facingproblems. Therefore, our study addresses the task of predicting the algorithmtag as a useful tool for engineers and developers. Moreover, we also considerpredicting the difficulty levels of algorithm problems, which can be used asuseful guidance to calculate the required time to solve that problem. In thispaper, we present a real-world algorithm problem multi-task dataset, AMT, bymainly collecting problem samples from the most famous and large competitiveprogramming website Codeforces. To the best of our knowledge, our proposeddataset is the most large-scale dataset for predicting algorithm tags comparedto previous studies. Moreover, our work is the first to address predicting thedifficulty levels of algorithm problems. We present a deep learning-based novelmethod for simultaneously predicting algorithm tags and the difficulty levelsof an algorithm problem given. All datasets and source codes are available athttps://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.</description><author>Juntae Kim, Eunjung Cho, Dongwoo Kim, Dongbin Na</author><pubDate>Mon, 09 Oct 2023 16:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05791v1</guid></item><item><title>Ensemble Mask Networks</title><link>http://arxiv.org/abs/2309.06382v2</link><description>Can an $\mathbb{R}^n\rightarrow \mathbb{R}^n$ feedforward network learnmatrix-vector multiplication? This study introduces two mechanisms - flexiblemasking to take matrix inputs, and a unique network pruning to respect themask's dependency structure. Networks can approximate fixed operations such asmatrix-vector multiplication $\phi(A,x) \rightarrow Ax$, motivating themechanisms introduced with applications towards litmus-testing dependencies orinteraction order in graph-based models.</description><author>Jonny Luntzel</author><pubDate>Mon, 09 Oct 2023 16:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06382v2</guid></item><item><title>AdaptNet: Policy Adaptation for Physics-Based Character Control</title><link>http://arxiv.org/abs/2310.00239v2</link><description>Motivated by humans' ability to adapt skills in the learning of new ones,this paper presents AdaptNet, an approach for modifying the latent space ofexisting policies to allow new behaviors to be quickly learned from like tasksin comparison to learning from scratch. Building on top of a givenreinforcement learning controller, AdaptNet uses a two-tier hierarchy thataugments the original state embedding to support modest changes in a behaviorand further modifies the policy network layers to make more substantivechanges. The technique is shown to be effective for adapting existingphysics-based controllers to a wide range of new styles for locomotion, newtask targets, changes in character morphology and extensive changes inenvironment. Furthermore, it exhibits significant increase in learningefficiency, as indicated by greatly reduced training times when compared totraining from scratch or using other approaches that modify existing policies.Code is available at https://motion-lab.github.io/AdaptNet.</description><author>Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G. Kry, Michael Neff, Morgan McGuire, Ioannis Karamouzas, Victor Zordan</author><pubDate>Mon, 09 Oct 2023 16:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00239v2</guid></item><item><title>Efficient Hybrid Oversampling and Intelligent Undersampling for Imbalanced Big Data Classification</title><link>http://arxiv.org/abs/2310.05789v1</link><description>Imbalanced classification is a well-known challenge faced by many real-worldapplications. This issue occurs when the distribution of the target variable isskewed, leading to a prediction bias toward the majority class. With thearrival of the Big Data era, there is a pressing need for efficient solutionsto solve this problem. In this work, we present a novel resampling methodcalled SMOTENN that combines intelligent undersampling and oversampling using aMapReduce framework. Both procedures are performed on the same pass over thedata, conferring efficiency to the technique. The SMOTENN method iscomplemented with an efficient implementation of the neighborhoods related tothe minority samples. Our experimental results show the virtues of thisapproach, outperforming alternative resampling techniques for small- andmedium-sized datasets while achieving positive results on large datasets withreduced running times.</description><author>Carla Vairetti, José Luis Assadi, Sebastián Maldonado</author><pubDate>Mon, 09 Oct 2023 16:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05789v1</guid></item><item><title>Self-Supervised Training with Autoencoders for Visual Anomaly Detection</title><link>http://arxiv.org/abs/2206.11723v6</link><description>Deep autoencoders provide an effective tool for learning non-lineardimensionality reduction in an unsupervised way. Recently, they have been usedfor the task of anomaly detection in the visual domain. By optimizing for thereconstruction error using anomaly-free examples, the common belief is that acorresponding network should fail to accurately reconstruct anomalous regionsin the application phase. This goal is typically addressed by controlling thecapacity of the network, either by reducing the size of the bottleneck layer orby enforcing sparsity constraints on the activations. However, neither of thesetechniques does explicitly penalize reconstruction of anomalous signals oftenresulting in poor detection. We tackle this problem by adapting aself-supervised learning regime that allows the use of discriminativeinformation during training but focuses on the data manifold of normalexamples. We emphasize that inference with our approach is very efficientduring training and prediction requiring a single forward pass for each inputimage. Our experiments on the MVTec AD dataset demonstrate high detection andlocalization performance. On the texture-subset, in particular, our approachconsistently outperforms recent anomaly detection methods by a significantmargin.</description><author>Alexander Bauer, Shinichi Nakajima, Klaus-Robert Müller</author><pubDate>Mon, 09 Oct 2023 16:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11723v6</guid></item><item><title>StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model</title><link>http://arxiv.org/abs/2303.09268v2</link><description>Despite the progress made in the style transfer task, most previous workfocus on transferring only relatively simple features like color or texture,while missing more abstract concepts such as overall art expression orpainter-specific traits. However, these abstract semantics can be captured bymodels like DALL-E or CLIP, which have been trained using huge datasets ofimages and textual documents. In this paper, we propose StylerDALLE, a styletransfer method that exploits both of these models and uses natural language todescribe abstract art styles. Specifically, we formulate the language-guidedstyle transfer task as a non-autoregressive token sequence translation, i.e.,from input content image to output stylized image, in the discrete latent spaceof a large-scale pretrained vector-quantized tokenizer, e.g., the discretevariational auto-encoder (dVAE) of DALL-E. To incorporate style information, wepropose a Reinforcement Learning strategy with CLIP-based language supervisionthat ensures stylization and content preservation simultaneously. Experimentalresults demonstrate the superiority of our method, which can effectivelytransfer art styles using language instructions at different granularities.Code is available at https://github.com/zipengxuc/StylerDALLE.</description><author>Zipeng Xu, Enver Sangineto, Nicu Sebe</author><pubDate>Mon, 09 Oct 2023 16:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09268v2</guid></item><item><title>Joint object detection and re-identification for 3D obstacle multi-camera systems</title><link>http://arxiv.org/abs/2310.05785v1</link><description>In recent years, the field of autonomous driving has witnessed remarkableadvancements, driven by the integration of a multitude of sensors, includingcameras and LiDAR systems, in different prototypes. However, with theproliferation of sensor data comes the pressing need for more sophisticatedinformation processing techniques. This research paper introduces a novelmodification to an object detection network that uses camera and lidarinformation, incorporating an additional branch designed for the task ofre-identifying objects across adjacent cameras within the same vehicle whileelevating the quality of the baseline 3D object detection outcomes. Theproposed methodology employs a two-step detection pipeline: initially, anobject detection network is employed, followed by a 3D box estimator thatoperates on the filtered point cloud generated from the network's detections.Extensive experimental evaluations encompassing both 2D and 3D domains validatethe effectiveness of the proposed approach and the results underscore thesuperiority of this method over traditional Non-Maximum Suppression (NMS)techniques, with an improvement of more than 5\% in the car category in theoverlapping areas.</description><author>Irene Cortés, Jorge Beltrán, Arturo de la Escalera, Fernando García</author><pubDate>Mon, 09 Oct 2023 16:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05785v1</guid></item><item><title>Aligning Language Models with Human Preferences via a Bayesian Approach</title><link>http://arxiv.org/abs/2310.05782v1</link><description>In the quest to advance human-centric natural language generation (NLG)systems, ensuring alignment between NLG models and human preferences iscrucial. For this alignment, current popular methods leverage a reinforcementlearning (RL) approach with a reward model trained on feedback from humans.However, inherent disagreements due to the subjective nature of humanpreferences pose a significant challenge for training the reward model,resulting in a deterioration of the NLG performance. To tackle this issue,previous approaches typically rely on majority voting or averaging toconsolidate multiple inconsistent preferences into a merged one. Althoughstraightforward to understand and execute, such methods suffer from aninability to capture the nuanced degrees of disaggregation among humans and mayonly represent a specialized subset of individuals, thereby lacking the abilityto quantitatively disclose the universality of human preferences. To addressthis challenge, this paper proposes a novel approach, which employs a Bayesianframework to account for the distribution of disagreements among humanpreferences as training a preference model, and names it as d-PM. Besides,considering the RL strategy's inefficient and complex training process over thetraining efficiency, we further propose utilizing the contrastive learningstrategy to train the NLG model with the preference scores derived from thed-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,emotional support conversation and integrity "Rule-of-Thumb" generation, showthat our method consistently exceeds previous SOTA models in both automatic andhuman evaluations.</description><author>Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</author><pubDate>Mon, 09 Oct 2023 16:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05782v1</guid></item><item><title>Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions</title><link>http://arxiv.org/abs/2310.05779v1</link><description>The moderation of content on online platforms is usually non-transparent. OnWikipedia, however, this discussion is carried out publicly and the editors areencouraged to use the content moderation policies as explanations for makingmoderation decisions. Currently, only a few comments explicitly mention thosepolicies -- 20% of the English ones, but as few as 2% of the German and Turkishcomments. To aid in this process of understanding how content is moderated, weconstruct a novel multilingual dataset of Wikipedia editor discussions alongwith their reasoning in three languages. The dataset contains the stances ofthe editors (keep, delete, merge, comment), along with the stated reason, and acontent moderation policy, for each edit decision. We demonstrate that stanceand corresponding reason (policy) can be predicted jointly with a high degreeof accuracy, adding transparency to the decision-making process. We releaseboth our joint prediction models and the multilingual content moderationdataset for further research on automated transparent content moderation.</description><author>Lucie-Aimée Kaffee, Arnav Arora, Isabelle Augenstein</author><pubDate>Mon, 09 Oct 2023 16:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05779v1</guid></item><item><title>Towards Inferential Reproducibility of Machine Learning Research</title><link>http://arxiv.org/abs/2302.04054v7</link><description>Reliability of machine learning evaluation -- the consistency of observedevaluation scores across replicated model training runs -- is affected byseveral sources of nondeterminism which can be regarded as measurement noise.Current tendencies to remove noise in order to enforce reproducibility ofresearch results neglect inherent nondeterminism at the implementation leveland disregard crucial interaction effects between algorithmic noise factors anddata properties. This limits the scope of conclusions that can be drawn fromsuch experiments. Instead of removing noise, we propose to incorporate severalsources of variance, including their interaction with data properties, into ananalysis of significance and reliability of machine learning evaluation, withthe aim to draw inferences beyond particular instances of trained models. Weshow how to use linear mixed effects models (LMEMs) to analyze performanceevaluation scores, and to conduct statistical inference with a generalizedlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sourcesof noise like meta-parameter variations into statistical significance testing,and to assess performance differences conditional on data properties.Furthermore, a variance component analysis (VCA) enables the analysis of thecontribution of noise sources to overall variance and the computation of areliability coefficient by the ratio of substantial to total variance.</description><author>Michael Hagmann, Philipp Meier, Stefan Riezler</author><pubDate>Mon, 09 Oct 2023 16:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04054v7</guid></item><item><title>Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws</title><link>http://arxiv.org/abs/2303.03192v2</link><description>Symbolic Regression is the study of algorithms that automate the search foranalytic expressions that fit data. While recent advances in deep learning havegenerated renewed interest in such approaches, the development of symbolicregression methods has not been focused on physics, where we have importantadditional constraints due to the units associated with our data. Here wepresent $\Phi$-SO, a Physical Symbolic Optimization framework for recoveringanalytical symbolic expressions from physics data using deep reinforcementlearning techniques by learning units constraints. Our system is built, fromthe ground up, to propose solutions where the physical units are consistent byconstruction. This is useful not only in eliminating physically impossiblesolutions, but because the "grammatical" rules of dimensional analysis restrictenormously the freedom of the equation generator, thus vastly improvingperformance. The algorithm can be used to fit noiseless data, which can beuseful for instance when attempting to derive an analytical property of aphysical model, and it can also be used to obtain analytical approximations tonoisy data. We test our machinery on a standard benchmark of equations from theFeynman Lectures on Physics and other physics textbooks, achievingstate-of-the-art performance in the presence of noise (exceeding 0.1%) and showthat it is robust even in the presence of substantial (10%) noise. We showcaseits abilities on a panel of examples from astrophysics.</description><author>Wassim Tenachi, Rodrigo Ibata, Foivos I. Diakogiannis</author><pubDate>Mon, 09 Oct 2023 16:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03192v2</guid></item><item><title>Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching</title><link>http://arxiv.org/abs/2310.05773v1</link><description>The ultimate goal of Dataset Distillation is to synthesize a small syntheticdataset such that a model trained on this synthetic set will perform equallywell as a model trained on the full, real dataset. Until now, no method ofDataset Distillation has reached this completely lossless goal, in part due tothe fact that previous methods only remain effective when the total number ofsynthetic samples is extremely small. Since only so much information can becontained in such a small number of samples, it seems that to achieve trulyloss dataset distillation, we must develop a distillation method that remainseffective as the size of the synthetic dataset grows. In this work, we presentsuch an algorithm and elucidate why existing methods fail to generate larger,high-quality synthetic sets. Current state-of-the-art methods rely ontrajectory-matching, or optimizing the synthetic data to induce similarlong-term training dynamics as the real data. We empirically find that thetraining stage of the trajectories we choose to match (i.e., early or late)greatly affects the effectiveness of the distilled dataset. Specifically, earlytrajectories (where the teacher network learns easy patterns) work well for alow-cardinality synthetic set since there are fewer examples wherein todistribute the necessary information. Conversely, late trajectories (where theteacher network learns hard patterns) provide better signals for largersynthetic sets since there are now enough samples to represent the necessarycomplex patterns. Based on our findings, we propose to align the difficulty ofthe generated patterns with the size of the synthetic dataset. In doing so, wesuccessfully scale trajectory matching-based methods to larger syntheticdatasets, achieving lossless dataset distillation for the very first time. Codeand distilled datasets are available at https://gzyaftermath.github.io/DATM.</description><author>Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You</author><pubDate>Mon, 09 Oct 2023 15:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05773v1</guid></item><item><title>Foundation Models Meet Visualizations: Challenges and Opportunities</title><link>http://arxiv.org/abs/2310.05771v1</link><description>Recent studies have indicated that foundation models, such as BERT and GPT,excel in adapting to a variety of downstream tasks. This adaptability hasestablished them as the dominant force in building artificial intelligence (AI)systems. As visualization techniques intersect with these models, a newresearch paradigm emerges. This paper divides these intersections into two mainareas: visualizations for foundation models (VIS4FM) and foundation models forvisualizations (FM4VIS). In VIS4FM, we explore the primary role ofvisualizations in understanding, refining, and evaluating these intricatemodels. This addresses the pressing need for transparency, explainability,fairness, and robustness. Conversely, within FM4VIS, we highlight howfoundation models can be utilized to advance the visualization field itself.The confluence of foundation models and visualizations holds great promise, butit also comes with its own set of challenges. By highlighting these challengesand the growing opportunities, this paper seeks to provide a starting point forcontinued exploration in this promising avenue.</description><author>Weikai Yang, Mengchen Liu, Zheng Wang, Shixia Liu</author><pubDate>Mon, 09 Oct 2023 15:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05771v1</guid></item><item><title>DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network</title><link>http://arxiv.org/abs/2310.05768v1</link><description>Efficient and accurate detection of small objects in manufacturing settings,such as defects and cracks, is crucial for ensuring product quality and safety.To address this issue, we proposed a comprehensive strategy by synergizingFaster R-CNN with cutting-edge methods. By combining Faster R-CNN with FeaturePyramid Network, we enable the model to efficiently handle multi-scale featuresintrinsic to manufacturing environments. Additionally, Deformable Net is usedthat contorts and conforms to the geometric variations of defects, bringingprecision in detecting even the minuscule and complex features. Then, weincorporated an attention mechanism called Convolutional Block Attention Modulein each block of our base ResNet50 network to selectively emphasize informativefeatures and suppress less useful ones. After that we incorporated RoI Align,replacing RoI Pooling for finer region-of-interest alignment and finally theintegration of Focal Loss effectively handles class imbalance, crucial for raredefect occurrences. The rigorous evaluation of our model on both the NEU-DETand Pascal VOC datasets underscores its robust performance and generalizationcapabilities. On the NEU-DET dataset, our model exhibited a profoundunderstanding of steel defects, achieving state-of-the-art accuracy inidentifying various defects. Simultaneously, when evaluated on the Pascal VOCdataset, our model showcases its ability to detect objects across a widespectrum of categories within complex and small scenes.</description><author>Md Sohag Mia, Abdullah Al Bary Voban, Abu Bakor Hayat Arnob, Abdu Naim, Md Kawsar Ahmed, Md Shariful Islam</author><pubDate>Mon, 09 Oct 2023 15:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05768v1</guid></item><item><title>Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design</title><link>http://arxiv.org/abs/2310.05764v1</link><description>A significant amount of protein function requires binding small molecules,including enzymatic catalysis. As such, designing binding pockets for smallmolecules has several impactful applications ranging from drug synthesis toenergy storage. Towards this goal, we first develop HarmonicFlow, an improvedgenerative process over 3D protein-ligand binding structures based on ourself-conditioned flow matching objective. FlowSite extends this flow model tojointly generate a protein pocket's discrete residue types and the molecule'sbinding 3D structure. We show that HarmonicFlow improves upon thestate-of-the-art generative processes for docking in simplicity, generality,and performance. Enabled by this structure modeling, FlowSite designs bindingsites substantially better than baseline approaches and provides the firstgeneral solution for binding site design.</description><author>Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola</author><pubDate>Mon, 09 Oct 2023 15:45:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05764v1</guid></item><item><title>3D tomatoes' localisation with monocular cameras using histogram filters</title><link>http://arxiv.org/abs/2310.05762v1</link><description>Performing tasks in agriculture, such as fruit monitoring or harvesting,requires perceiving the objects' spatial position. RGB-D cameras are limitedunder open-field environments due to lightning interferences. Therefore, inthis study, we approach the use of Histogram Filters (Bayesian DiscreteFilters) to estimate the position of tomatoes in the tomato plant. Two kernelfilters were studied: the square kernel and the Gaussian kernel. Theimplemented algorithm was essayed in simulation, with and without Gaussiannoise and random noise, and in a testbed at laboratory conditions. Thealgorithm reported a mean absolute error lower than 10 mm in simulation and 20mm in the testbed at laboratory conditions with an assessing distance of about0.5 m. So, the results are viable for real environments and should be improvedat closer distances.</description><author>Sandro Costa Magalhães, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias</author><pubDate>Mon, 09 Oct 2023 15:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05762v1</guid></item><item><title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</title><link>http://arxiv.org/abs/2309.15112v4</link><description>We propose InternLM-XComposer, a vision-language large model that enablesadvanced image-text comprehension and composition. The innovative nature of ourmodel is highlighted by three appealing properties: 1) Interleaved Text-ImageComposition: InternLM-XComposer can effortlessly generate coherent andcontextual articles that seamlessly integrate images, providing a more engagingand immersive reading experience. Simply provide a title, and our system willgenerate the corresponding manuscript. It can intelligently identify the areasin the text where images would enhance the content and automatically insert themost appropriate visual candidates. 2) Comprehension with Rich MultilingualKnowledge: The text-image comprehension is empowered by training on extensivemulti-modal multilingual concepts with carefully crafted strategies, resultingin a deep understanding of visual content. 3) State-of-the-art Performance: Ourmodel consistently achieves state-of-the-art results across various mainstreambenchmarks for vision-language foundational models, including MME Benchmark,MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).Collectively, InternLM-XComposer seamlessly blends advanced text-imagecomprehension and composition, revolutionizing vision-language interaction andoffering new insights and opportunities. The InternLM-XComposer model serieswith 7B parameters are publicly available athttps://github.com/InternLM/InternLM-XComposer.</description><author>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Mon, 09 Oct 2023 15:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15112v4</guid></item><item><title>Nonlinear Correct and Smooth for Semi-Supervised Learning</title><link>http://arxiv.org/abs/2310.05757v1</link><description>Graph-based semi-supervised learning (GSSL) has been used successfully invarious applications. Existing methods leverage the graph structure and labeledsamples for classification. Label Propagation (LP) and Graph Neural Networks(GNNs) both iteratively pass messages on graphs, where LP propagates nodelabels through edges and GNN aggregates node features from the neighborhood.Recently, combining LP and GNN has led to improved performance. However,utilizing labels and features jointly in higher-order graphs has not beenexplored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), whichimproves the existing post-processing approach by incorporating non-linearityand higher-order representation into the residual propagation to handleintricate node relationships effectively. Systematic evaluations show that ourmethod achieves remarkable average improvements of 13.71% over base predictionand 2.16% over the state-of-the-art post-processing method on six commonly useddatasets. Comparisons and analyses show our method effectively utilizes labelsand features jointly in higher-order graphs to resolve challenging graphrelationships.</description><author>Yuanhang Shao, Xiuwen Liu</author><pubDate>Mon, 09 Oct 2023 15:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05757v1</guid></item><item><title>Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation</title><link>http://arxiv.org/abs/2307.09052v2</link><description>Deep neural network is a powerful tool for many tasks. Understanding why itis so successful and providing a mathematical explanation is an importantproblem and has been one popular research direction in past years. In theliterature of mathematical analysis of deep neural networks, a lot of works isdedicated to establishing representation theories. How to make connectionsbetween deep neural networks and mathematical algorithms is still underdevelopment. In this paper, we give an algorithmic explanation for deep neuralnetworks, especially in their connections with operator splitting. We show thatwith certain splitting strategies, operator-splitting methods have the samestructure as networks. Utilizing this connection and the Potts model for imagesegmentation, two networks inspired by operator-splitting methods are proposed.The two networks are essentially two operator-splitting algorithms solving thePotts model. Numerical experiments are presented to demonstrate theeffectiveness of the proposed networks.</description><author>Hao Liu, Xue-Cheng Tai, Raymond Chan</author><pubDate>Mon, 09 Oct 2023 15:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09052v2</guid></item><item><title>Deep Concept Removal</title><link>http://arxiv.org/abs/2310.05755v1</link><description>We address the problem of concept removal in deep neural networks, aiming tolearn representations that do not encode certain specified concepts (e.g.,gender etc.) We propose a novel method based on adversarial linear classifierstrained on a concept dataset, which helps to remove the targeted attributewhile maintaining model performance. Our approach Deep Concept Removalincorporates adversarial probing classifiers at various layers of the network,effectively addressing concept entanglement and improving out-of-distributiongeneralization. We also introduce an implicit gradient-based technique totackle the challenges associated with adversarial training using linearclassifiers. We evaluate the ability to remove a concept on a set of populardistributionally robust optimization (DRO) benchmarks with spuriouscorrelations, as well as out-of-distribution (OOD) generalization tasks.</description><author>Yegor Klochkov, Jean-Francois Ton, Ruocheng Guo, Yang Liu, Hang Li</author><pubDate>Mon, 09 Oct 2023 15:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05755v1</guid></item></channel></rss>