<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 16 Feb 2025 01:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF Architectures</title><link>http://arxiv.org/abs/2502.09623v1</link><description>Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm forrepresenting 3D objects and scenes by encoding shape and appearance informationinto the weights of a neural network. Recent works have shown how such weightscan be used as input to frameworks processing them to solve deep learningtasks. Yet, these frameworks can only process NeRFs with a specific, predefinedarchitecture. In this paper, we present the first framework that can ingestNeRFs with multiple architectures and perform inference on architectures unseenat training time. We achieve this goal by training a Graph Meta-Network in arepresentation learning framework. Moreover, we show how a contrastiveobjective is conducive to obtaining an architecture-agnostic latent space. Inexperiments on both MLP-based and tri-planar NeRFs, our approach demonstratesrobust performance in classification and retrieval tasks that either matches orexceeds that of existing frameworks constrained to single architectures, thusproviding the first architecture-agnostic method to perform tasks on NeRFs byprocessing their weights.</description><author>Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano</author><pubDate>Thu, 13 Feb 2025 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09623v1</guid></item><item><title>Theoretical Benefit and Limitation of Diffusion Language Model</title><link>http://arxiv.org/abs/2502.09622v1</link><description>Diffusion language models have emerged as a promising approach for textgeneration. One would naturally expect this method to be an efficientreplacement for autoregressive models since multiple tokens can be sampled inparallel during each diffusion step. However, its efficiency-accuracy trade-offis not yet well understood. In this paper, we present a rigorous theoreticalanalysis of a widely used type of diffusion language model, the MaskedDiffusion Model (MDM), and find that its effectiveness heavily depends on thetarget evaluation metric. Under mild conditions, we prove that when usingperplexity as the metric, MDMs can achieve near-optimal perplexity in samplingsteps regardless of sequence length, demonstrating that efficiency can beachieved without sacrificing performance. However, when using the sequenceerror rate--which is important for understanding the "correctness" of asequence, such as a reasoning chain--we show that the required sampling stepsmust scale linearly with sequence length to obtain "correct" sequences, therebyeliminating MDM's efficiency advantage over autoregressive models. Our analysisestablishes the first theoretical foundation for understanding the benefits andlimitations of MDMs. All theoretical findings are supported by empiricalstudies.</description><author>Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He</author><pubDate>Thu, 13 Feb 2025 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09622v1</guid></item><item><title>MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency</title><link>http://arxiv.org/abs/2502.09621v1</link><description>Answering questions with Chain-of-Thought (CoT) has significantly enhancedthe reasoning capabilities of Large Language Models (LLMs), yet its impact onLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depthinvestigation. In this paper, we introduce MME-CoT, a specialized benchmarkevaluating the CoT reasoning performance of LMMs, spanning six domains: math,science, OCR, logic, space-time, and general scenes. As the first comprehensivestudy in this area, we propose a thorough evaluation suite incorporating threenovel metrics that assess the reasoning quality, robustness, and efficiency ata fine-grained level. Leveraging curated high-quality data and a uniqueevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,uncovering several key insights: 1) Models with reflection mechanismdemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o anddemonstrating the highest quality results; 2) CoT prompting often degrades LMMperformance on perception-heavy tasks, suggesting a potentially harmfuloverthinking behavior; and 3) Although the CoT quality is high, LMMs withreflection exhibit significant inefficiency in both normal response andself-correction phases. We hope MME-CoT serves as a foundation for advancingmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/</description><author>Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li</author><pubDate>Thu, 13 Feb 2025 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09621v1</guid></item><item><title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title><link>http://arxiv.org/abs/2502.09620v1</link><description>Encoder-free architectures have been preliminarily explored in the 2D visualdomain, yet it remains an open question whether they can be effectively appliedto 3D understanding scenarios. In this paper, we present the firstcomprehensive investigation into the potential of encoder-free architectures toovercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).These challenges include the failure to adapt to varying point cloudresolutions and the point features from the encoder not meeting the semanticneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs toremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)We propose the LLM-embedded Semantic Encoding strategy in the pre-trainingstage, exploring the effects of various point cloud self-supervised losses. Andwe present the Hybrid Semantic Loss to extract high-level semantics. 2) Weintroduce the Hierarchical Geometry Aggregation strategy in the instructiontuning stage. This incorporates inductive bias into the LLM early layers tofocus on the local details of the point clouds. To the end, we present thefirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the currentstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on theclassification, captioning, and VQA tasks, respectively. Our resultsdemonstrate that the encoder-free architecture is highly promising forreplacing encoder-based architectures in the field of 3D understanding. Thecode is released at https://github.com/Ivan-Tang-3D/ENEL</description><author>Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao</author><pubDate>Thu, 13 Feb 2025 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09620v1</guid></item><item><title>Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights</title><link>http://arxiv.org/abs/2502.09619v1</link><description>With the increasing numbers of publicly available models, there are probablypretrained, online models for most tasks users require. However, current modelsearch methods are rudimentary, essentially a text-based search in thedocumentation, thus users cannot find the relevant models. This paper presentsProbeLog, a method for retrieving classification models that can recognize atarget concept, such as "Dog", without access to model metadata or trainingdata. Differently from previous probing methods, ProbeLog computes a descriptorfor each output dimension (logit) of each model, by observing its responses ona fixed set of inputs (probes). Our method supports both logit-based retrieval("find more logits like this") and zero-shot, text-based retrieval ("find alllogits corresponding to dogs"). As probing-based representations requiremultiple costly feedforward passes through the model, we develop a method,based on collaborative filtering, that reduces the cost of encodingrepositories by 3x. We demonstrate that ProbeLog achieves high retrievalaccuracy, both in real-world and fine-grained search tasks and is scalable tofull-size repositories.</description><author>Jonathan Kahana, Or Nathan, Eliahu Horwitz, Yedid Hoshen</author><pubDate>Thu, 13 Feb 2025 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09619v1</guid></item><item><title>Genetically programmable optical random neural networks</title><link>http://arxiv.org/abs/2403.12490v2</link><description>Today, machine learning tools, particularly artificial neural networks, havebecome crucial for diverse applications. However, current digital computingtools to train and deploy artificial neural networks often struggle withmassive data sizes and high power consumptions. Optical computing providesinherent parallelism accommodating high-resolution input data and performsfundamental operations with passive optical components. However, most of theoptical computing platforms suffer from relatively low accuracies for machinelearning tasks due to fixed connections while avoiding complex and sensitivetechniques. Here, we demonstrate a genetically programmable yet simple opticalneural network to achieve high performances with optical random projection. Bygenetically programming the orientation of the scattering medium which acts asa random projection kernel and only using 1% of the search space, our noveltechnique finds an optimum kernel and improves initial test accuracies by 8-41%for various machine learning tasks. Through numerical simulations andexperiments on a number of datasets, we validate the programmability andhigh-resolution sample processing capabilities of our design. Our opticalcomputing method presents a promising approach to achieve high performance inoptical neural networks with a simple and scalable design.</description><author>Bora Çarpınlıoğlu, Uğur Teğin</author><pubDate>Thu, 13 Feb 2025 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12490v2</guid></item><item><title>LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh</title><link>http://arxiv.org/abs/2502.09617v1</link><description>Generalizable rendering of an animatable human avatar from sparse inputsrelies on data priors and inductive biases extracted from training on largedata to avoid scene-specific optimization and to enable fast reconstruction.This raises two main challenges: First, unlike iterative gradient-basedadjustment in scene-specific optimization, generalizable methods mustreconstruct the human shape representation in a single pass at inference time.Second, rendering is preferably computationally efficient yet of highresolution. To address both challenges we augment the recently proposed dualshape representation, which combines the benefits of a mesh and Gaussianpoints, in two ways. To improve reconstruction, we propose an iterativefeedback update framework, which successively improves the canonical humanshape representation during reconstruction. To achieve computationallyefficient yet high-resolution rendering, we study a coupled-multi-resolutionGaussians-on-Mesh representation. We evaluate the proposed approach on thechallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs ananimatable representation from sparse inputs in less than 1s, renders viewswith 95.1FPS at $1024 \times 1024$, and achieves PSNR/LPIPS*/FID of24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art inrendering quality.</description><author>Jing Wen, Alexander G. Schwing, Shenlong Wang</author><pubDate>Thu, 13 Feb 2025 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09617v1</guid></item><item><title>Variational Rectified Flow Matching</title><link>http://arxiv.org/abs/2502.09616v1</link><description>We study Variational Rectified Flow Matching, a framework that enhancesclassic rectified flow matching by modeling multi-modal velocity vector-fields.At inference time, classic rectified flow matching 'moves' samples from asource distribution to the target distribution by solving an ordinarydifferential equation via integration along a velocity vector-field. Attraining time, the velocity vector-field is learnt by linearly interpolatingbetween coupled samples one drawn from the source and one drawn from the targetdistribution randomly. This leads to ''ground-truth'' velocity vector-fieldsthat point in different directions at the same location, i.e., the velocityvector-fields are multi-modal/ambiguous. However, since training uses astandard mean-squared-error loss, the learnt velocity vector-field averages''ground-truth'' directions and isn't multi-modal. In contrast, variationalrectified flow matching learns and samples from multi-modal flow directions. Weshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variationalrectified flow matching leads to compelling results.</description><author>Pengsheng Guo, Alexander G. Schwing</author><pubDate>Thu, 13 Feb 2025 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09616v1</guid></item><item><title>RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets</title><link>http://arxiv.org/abs/2502.09615v1</link><description>We present RigAnything, a novel autoregressive transformer-based model, whichmakes 3D assets rig-ready by probabilistically generating joints, skeletontopologies, and assigning skinning weights in a template-free manner. Unlikemost existing auto-rigging methods, which rely on predefined skeleton templateand are limited to specific categories like humanoid, RigAnything approachesthe rigging problem in an autoregressive manner, iteratively predicting thenext joint based on the global input shape and the previous prediction. Whileautoregressive models are typically used to generate sequential data,RigAnything extends their application to effectively learn and representskeletons, which are inherently tree structures. To achieve this, we organizethe joints in a breadth-first search (BFS) order, enabling the skeleton to bedefined as a sequence of 3D locations and the parent index. Furthermore, ourmodel improves the accuracy of position prediction by leveraging diffusionmodeling, ensuring precise and consistent placement of joints within thehierarchy. This formulation allows the autoregressive model to efficientlycapture both spatial and hierarchical relationships within the skeleton.Trained end-to-end on both RigNet and Objaverse datasets, RigAnythingdemonstrates state-of-the-art performance across diverse object types,including humanoids, quadrupeds, marine creatures, insects, and many more,surpassing prior methods in quality, robustness, generalizability, andefficiency. Please check our website for more details:https://www.liuisabella.com/RigAnything.</description><author>Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi</author><pubDate>Thu, 13 Feb 2025 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09615v1</guid></item><item><title>DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</title><link>http://arxiv.org/abs/2502.09614v1</link><description>We address the challenge of developing a generalizable neural trackingcontroller for dexterous manipulation from human references. This controlleraims to manage a dexterous robot hand to manipulate diverse objects for variouspurposes defined by kinematic human-object interactions. Developing such acontroller is complicated by the intricate contact dynamics of dexterousmanipulation and the need for adaptivity, generalizability, and robustness.Current reinforcement learning and trajectory optimization methods often fallshort due to their dependence on task-specific rewards or precise systemmodels. We introduce an approach that curates large-scale successful robottracking demonstrations, comprising pairs of human references and robotactions, to train a neural controller. Utilizing a data flywheel, weiteratively enhance the controller's performance, as well as the number andquality of successful tracking demonstrations. We exploit available trackingdemonstrations and carefully integrate reinforcement learning and imitationlearning to boost the controller's performance in dynamic environments. At thesame time, to obtain high-quality tracking demonstrations, we individuallyoptimize per-trajectory tracking by leveraging the learned tracking controllerin a homotopy optimization method. The homotopy optimization, mimickingchain-of-thought, aids in solving challenging trajectory tracking problems toincrease demonstration diversity. We showcase our success by training ageneralizable neural controller and evaluating it in both simulation and realworld. Our method achieves over a 10% improvement in success rates compared toleading baselines. The project website with animated results is available athttps://meowuu7.github.io/DexTrack/.</description><author>Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, Li Yi</author><pubDate>Thu, 13 Feb 2025 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09614v1</guid></item><item><title>Opening Articulated Objects in the Real World</title><link>http://arxiv.org/abs/2402.17767v2</link><description>What does it take to build mobile manipulation systems that can competentlyoperate on previously unseen objects in previously unseen environments? Thiswork answers this question using opening of articulated objects as a mobilemanipulation testbed. Specifically, our focus is on the end-to-end performanceon this task without any privileged information, i.e. the robot starts at alocation with the novel target articulated object in view, and has to approachthe object and successfully open it. We first develop a system for this task,and then conduct 100+ end-to-end system tests across 13 real world test sites.Our large-scale study reveals a number of surprising findings: a) modularsystems outperform end-to-end learned systems for this task, even when theend-to-end learned systems are trained on 1000+ demonstrations, b) perception,and not precise end-effector control, is the primary bottleneck to tasksuccess, and c) state-of-the-art articulation parameter estimation modelsdeveloped in isolation struggle when faced with robot-centric viewpoints.Overall, our findings highlight the limitations of developing components of thepipeline in isolation and underscore the need for system-level research,providing a pragmatic roadmap for building generalizable mobile manipulationsystems. Videos, code, and models are available on the project website:https://arjung128.github.io/opening-articulated-objects/</description><author>Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta</author><pubDate>Thu, 13 Feb 2025 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17767v2</guid></item><item><title>Latent Radiance Fields with 3D-aware 2D Representations</title><link>http://arxiv.org/abs/2502.09613v1</link><description>Latent 3D reconstruction has shown great promise in empowering 3D semanticunderstanding and 3D generation by distilling 2D features into the 3D space.However, existing approaches struggle with the domain gap between 2D featurespace and 3D representations, resulting in degraded rendering performance. Toaddress this challenge, we propose a novel framework that integrates 3Dawareness into the 2D latent space. The framework consists of three stages: (1)a correspondence-aware autoencoding method that enhances the 3D consistency of2D latent representations, (2) a latent radiance field (LRF) that lifts these3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field(VAE-RF) alignment strategy that improves image decoding from the rendered 2Drepresentations. Extensive experiments demonstrate that our method outperformsthe state-of-the-art latent 3D reconstruction approaches in terms of synthesisperformance and cross-dataset generalizability across diverse indoor andoutdoor scenes. To our knowledge, this is the first work showing the radiancefield representations constructed from 2D latent representations can yieldphotorealistic 3D reconstruction performance.</description><author>Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang</author><pubDate>Thu, 13 Feb 2025 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09613v1</guid></item><item><title>Transformers Learn Low Sensitivity Functions: Investigations and Implications</title><link>http://arxiv.org/abs/2403.06925v2</link><description>Transformers achieve state-of-the-art accuracy and robustness across manytasks, but an understanding of their inductive biases and how those biasesdiffer from other neural network architectures remains elusive. In this work,we identify the sensitivity of the model to token-wise random perturbations inthe input as a unified metric which explains the inductive bias of transformersacross different data modalities and distinguishes them from otherarchitectures. We show that transformers have lower sensitivity than MLPs,CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also showthat this low-sensitivity bias has important implications: i) lower sensitivitycorrelates with improved robustness; it can also be used as an efficientintervention to further improve the robustness of transformers; ii) itcorresponds to flatter minima in the loss landscape; and iii) it can serve as aprogress measure for grokking. We support these findings with theoreticalresults showing (weak) spectral bias of transformers in the NTK regime, andimproved robustness due to the lower sensitivity. The code is available athttps://github.com/estija/sensitivity.</description><author>Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan</author><pubDate>Thu, 13 Feb 2025 18:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06925v2</guid></item><item><title>Designing a Conditional Prior Distribution for Flow-Based Generative Models</title><link>http://arxiv.org/abs/2502.09611v1</link><description>Flow-based generative models have recently shown impressive performance forconditional generation tasks, such as text-to-image generation. However,current methods transform a general unimodal noise distribution to a specificmode of the target data distribution. As such, every point in the initialsource distribution can be mapped to every point in the target distribution,resulting in long average paths. To this end, in this work, we tap into anon-utilized property of conditional flow-based models: the ability to design anon-trivial prior distribution. Given an input condition, such as a textprompt, we first map it to a point lying in data space, representing an``average" data point with the minimal average distance to all data points ofthe same conditional mode (e.g., class). We then utilize the flow matchingformulation to map samples from a parametric distribution centered around thispoint to the conditional target distribution. Experimentally, our methodsignificantly improves training times and generation efficiency (FID, KID andCLIP alignment scores) compared to baselines, producing high quality samplesusing fewer sampling steps.</description><author>Noam Issachar, Mohammad Salama, Raanan Fattal, Sagie Benaim</author><pubDate>Thu, 13 Feb 2025 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09611v1</guid></item><item><title>Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2501.13904v3</link><description>Multimodal Large Language Models (LLMs) are pivotal in revolutionizingcustomer support and operations by integrating multiple modalities such astext, images, and audio. Federated Prompt Learning (FPL) is a recently proposedapproach that combines pre-trained multimodal LLMs such as vision-languagemodels with federated learning to create personalized, privacy-preserving AIsystems. However, balancing the competing goals of personalization,generalization, and privacy remains a significant challenge.Over-personalization can lead to overfitting, reducing generalizability, whilestringent privacy measures, such as differential privacy, can hinder bothpersonalization and generalization. In this paper, we propose a DifferentiallyPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge byleveraging a low-rank factorization scheme to capture generalization whilemaintaining a residual term that preserves expressiveness for personalization.To ensure privacy, we introduce a novel method where we apply localdifferential privacy to the two low-rank components of the local prompt, andglobal differential privacy to the global prompt. Our approach mitigates theimpact of privacy noise on the model performance while balancing the tradeoffbetween personalization and generalization. Extensive experiments demonstratethe effectiveness of our approach over other benchmarks.</description><author>Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</author><pubDate>Thu, 13 Feb 2025 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13904v3</guid></item><item><title>Score-of-Mixture Training: Training One-Step Generative Models Made Simple</title><link>http://arxiv.org/abs/2502.09609v1</link><description>We propose Score-of-Mixture Training (SMT), a novel framework for trainingone-step generative models by minimizing a class of divergences called the$\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the scoreof mixture distributions between real and fake samples across multiple noiselevels. Similar to consistency models, our approach supports both training fromscratch (SMT) and distillation using a pretrained diffusion model, which wecall Score-of-Mixture Distillation (SMD). It is simple to implement, requiresminimal hyperparameter tuning, and ensures stable training. Experiments onCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can evenoutperform existing methods.</description><author>Tejas Jayashankar, J. Jon Ryu, Gregory Wornell</author><pubDate>Thu, 13 Feb 2025 18:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09609v1</guid></item><item><title>Instance Segmentation of Scene Sketches Using Natural Image Priors</title><link>http://arxiv.org/abs/2502.09608v1</link><description>Sketch segmentation involves grouping pixels within a sketch that belong tothe same object or instance. It serves as a valuable tool for sketch editingtasks, such as moving, scaling, or removing specific components. While imagesegmentation models have demonstrated remarkable capabilities in recent years,sketches present unique challenges for these models due to their sparse natureand wide variation in styles. We introduce SketchSeg, a method for instancesegmentation of raster scene sketches. Our approach adapts state-of-the-artimage segmentation and object detection models to the sketch domain byemploying class-agnostic fine-tuning and refining segmentation masks usingdepth cues. Furthermore, our method organizes sketches into sorted layers,where occluded instances are inpainted, enabling advanced sketch editingapplications. As existing datasets in this domain lack variation in sketchstyles, we construct a synthetic scene sketch segmentation dataset featuringsketches with diverse brush strokes and varying levels of detail. We use thisdataset to demonstrate the robustness of our approach and will release it topromote further research in the field. Project webpage: https://sketchseg.github.io/sketch-seg/</description><author>Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala</author><pubDate>Thu, 13 Feb 2025 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09608v1</guid></item><item><title>Human-LLM Coevolution: Evidence from Academic Writing</title><link>http://arxiv.org/abs/2502.09606v1</link><description>With a statistical analysis of arXiv paper abstracts, we report a marked dropin the frequency of several words previously identified as overused by ChatGPT,such as "delve", starting soon after they were pointed out in early 2024. Thefrequency of certain other words favored by ChatGPT, such as "significant", hasinstead kept increasing. These phenomena suggest that some authors of academicpapers have adapted their use of large language models (LLMs), for example, byselecting outputs or applying modifications to the LLM-generated content. Suchcoevolution and cooperation of humans and LLMs thus introduce additionalchallenges to the detection of machine-generated text in real-world scenarios.Estimating the impact of LLMs on academic writing by examining word frequencyremains feasible, and more attention should be paid to words that were alreadyfrequently employed, including those that have decreased in frequency.</description><author>Mingmeng Geng, Roberto Trotta</author><pubDate>Thu, 13 Feb 2025 18:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09606v1</guid></item><item><title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title><link>http://arxiv.org/abs/2502.09604v1</link><description>We introduce SelfCite, a novel self-supervised approach that aligns LLMs togenerate high-quality, fine-grained, sentence-level citations for thestatements in their generated responses. Instead of only relying on costly andlabor-intensive annotations, SelfCite leverages a reward signal provided by theLLM itself through context ablation: If a citation is necessary, removing thecited text from the context should prevent the same response; if sufficient,retaining the cited text alone should preserve the same response. This rewardcan guide the inference-time best-of-N sampling strategy to improve citationquality significantly, as well as be used in preference optimization todirectly fine-tune the models for generating better citations. Theeffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3points on the LongBench-Cite benchmark across five long-form question answeringtasks.</description><author>Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih</author><pubDate>Thu, 13 Feb 2025 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09604v1</guid></item><item><title>CoT-Valve: Length-Compressible Chain-of-Thought Tuning</title><link>http://arxiv.org/abs/2502.09601v1</link><description>Chain-of-Thought significantly enhances a model's reasoning capability, butit also comes with a considerable increase in inference costs due to longchains. With the observation that the reasoning path can be easily compressedunder easy tasks but struggle on hard tasks, we explore the feasibility ofelastically controlling the length of reasoning paths with only one model,thereby reducing the inference overhead of reasoning models dynamically basedon task difficulty. We introduce a new tuning and inference strategy namedCoT-Valve, designed to allow models to generate reasoning chains of varyinglengths. To achieve this, we propose to identify a direction in the parameterspace that, when manipulated, can effectively control the length of generatedCoT. Moreover, we show that this property is valuable for compressing thereasoning chain. We construct datasets with chains from long to short for thesame questions and explore two enhanced strategies for CoT-Valve: (1) a preciselength-compressible CoT tuning method, and (2) a progressive chain lengthcompression approach. Our experiments show that CoT-Valve successfully enablescontrollability and compressibility of the chain and shows better performancethan the prompt-based control. We applied this method to QwQ-32B-Preview,reducing reasoning chains on GSM8K from 741 to 225 tokens with a minorperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, withonly one additional incorrect answer.</description><author>Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang</author><pubDate>Thu, 13 Feb 2025 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09601v1</guid></item><item><title>GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</title><link>http://arxiv.org/abs/2502.09598v1</link><description>The continuous operation of Earth-orbiting satellites generates vast andever-growing archives of Remote Sensing (RS) images. Natural language presentsan intuitive interface for accessing, querying, and interpreting the data fromsuch archives. However, existing Vision-Language Models (VLMs) arepredominantly trained on web-scraped, noisy image-text data, exhibiting limitedexposure to the specialized domain of RS. This deficiency results in poorperformance on RS-specific tasks, as commonly used datasets often lackdetailed, scientifically accurate textual descriptions and instead emphasizesolely on attributes like date and location. To bridge this critical gap, weintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, andmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curatedRS image-text pairs, representing a diverse range of RS modalities associatedto different spatial resolutions. Unlike existing vision-language datasets inRS, GAIA specifically focuses on capturing a diverse range of RS applications,providing unique information about environmental changes, natural disasters,and various other dynamic phenomena. The dataset provides a spatially andtemporally balanced distribution, spanning across the globe, covering the last25 years with a balanced temporal distribution of observations. GAIA'sconstruction involved a two-stage process: (1) targeted web-scraping of imagesand accompanying text from reputable RS-related sources, and (2) generation offive high-quality, scientifically grounded synthetic captions for each imageusing carefully crafted prompts that leverage the advanced vision-languagecapabilities of GPT-4o. Our extensive experiments, including fine-tuning ofCLIP and BLIP2 models, demonstrate that GAIA significantly improves performanceon RS image classification, cross-modal retrieval and image captioning tasks.</description><author>Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</author><pubDate>Thu, 13 Feb 2025 18:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09598v1</guid></item><item><title>Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs</title><link>http://arxiv.org/abs/2502.09597v1</link><description>Large Language Models (LLMs) are increasingly used as chatbots, yet theirability to personalize responses to user preferences remains limited. Weintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorizeand adhere to user preferences in a long-context conversational setting.PrefEval comprises 3,000 manually curated user preference and query pairsspanning 20 topics. PrefEval contains user personalization or preferenceinformation in both explicit and implicit forms, and evaluates LLM performanceusing a generation and a classification task. With PrefEval, we evaluated theaforementioned preference following capabilities of 10 open-source andproprietary LLMs in multi-session conversations with varying context lengths upto 100k tokens. We benchmark with various prompting, iterative feedback, andretrieval-augmented generation methods. Our benchmarking effort reveals thatstate-of-the-art LLMs face significant challenges in proactively followingusers' preferences during conversations. In particular, in zero-shot settings,preference following accuracy falls below 10% at merely 10 turns (~3k tokens)across most evaluated models. Even with advanced prompting and retrievalmethods, preference following still deteriorates in long-context conversations.Furthermore, we show that fine-tuning on PrefEval significantly improvesperformance. We believe PrefEval serves as a valuable resource for measuring,understanding, and enhancing LLMs' preference following abilities, paving theway for personalized conversational agents. Our code and dataset are availableat https://prefeval.github.io/.</description><author>Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin</author><pubDate>Thu, 13 Feb 2025 18:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09597v1</guid></item><item><title>KIMAs: A Configurable Knowledge Integrated Multi-Agent System</title><link>http://arxiv.org/abs/2502.09596v1</link><description>Knowledge-intensive conversations supported by large language models (LLMs)have become one of the most popular and helpful applications that can assistpeople in different aspects. Many current knowledge-intensive applications arecentered on retrieval-augmented generation (RAG) techniques. While manyopen-source RAG frameworks facilitate the development of RAG-basedapplications, they often fall short in handling practical scenarios complicatedby heterogeneous data in topics and formats, conversational context management,and the requirement of low-latency response times. This technical reportpresents a configurable knowledge integrated multi-agent system, KIMAs, toaddress these challenges. KIMAs features a flexible and configurable system forintegrating diverse knowledge sources with 1) context management and queryrewrite mechanisms to improve retrieval accuracy and multi-turn conversationalcoherency, 2) efficient knowledge routing and retrieval, 3) simple buteffective filter and reference generation mechanisms, and 4) optimizedparallelizable multi-agent pipeline execution. Our work provides a scalableframework for advancing the deployment of LLMs in real-world settings. To showhow KIMAs can help developers build knowledge-intensive applications withdifferent scales and emphases, we demonstrate how we configure the system tothree applications already running in practice with reliable performance.</description><author>Zitao Li, Fei Wei, Yuexiang Xie, Dawei Gao, Weirui Kuang, Zhijian Ma, Bingchen Qian, Yaliang Li, Bolin Ding</author><pubDate>Thu, 13 Feb 2025 18:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09596v1</guid></item><item><title>Censor Dependent Variational Inference</title><link>http://arxiv.org/abs/2502.09591v1</link><description>This paper provides a comprehensive analysis of variational inference inlatent variable models for survival analysis, emphasizing the distinctivechallenges associated with applying variational methods to survival data. Weidentify a critical weakness in the existing methodology, demonstrating how apoorly designed variational distribution may hinder the objective of survivalanalysis tasks--modeling time-to-event distributions. We prove that the optimalvariational distribution, which perfectly bounds the log-likelihood, may dependon the censoring mechanism. To address this issue, we propose censor-dependentvariational inference (CDVI), tailored for latent variable models in survivalanalysis. More practically, we introduce CD-CVAE, a V-structure VariationalAutoencoder (VAE) designed for the scalable implementation of CDVI. Furtherdiscussion extends some existing theories and training techniques to survivalanalysis. Extensive experiments validate our analysis and demonstratesignificant improvements in the estimation of individual survivaldistributions.</description><author>Chuanhui Liu, Xiao Wang</author><pubDate>Thu, 13 Feb 2025 18:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09591v1</guid></item><item><title>Logical forms complement probability in understanding language model (and human) performance</title><link>http://arxiv.org/abs/2502.09589v1</link><description>With the increasing interest in using large language models (LLMs) forplanning in natural language, understanding their behaviors becomes animportant research question. This work conducts a systematic investigation ofLLMs' ability to perform logical reasoning in natural language. We introduce acontrolled dataset of hypothetical and disjunctive syllogisms in propositionaland modal logic and use it as the testbed for understanding LLM performance.Our results lead to novel insights in predicting LLM behaviors: in addition tothe probability of input (Gonen et al., 2023; McCoy et al., 2024), logicalforms should be considered as orthogonal factors. In addition, we showsimilarities and differences between the logical reasoning performances ofhumans and LLMs by comparing LLM and human behavioral results.</description><author>Yixuan Wang, Freda Shi</author><pubDate>Thu, 13 Feb 2025 18:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09589v1</guid></item><item><title>Rolling Ahead Diffusion for Traffic Scene Simulation</title><link>http://arxiv.org/abs/2502.09587v1</link><description>Realistic driving simulation requires that NPCs not only mimic naturaldriving behaviors but also react to the behavior of other simulated agents.Recent developments in diffusion-based scenario generation focus on creatingdiverse and realistic traffic scenarios by jointly modelling the motion of allthe agents in the scene. However, these traffic scenarios do not react when themotion of agents deviates from their modelled trajectories. For example, theego-agent can be controlled by a stand along motion planner. To producereactive scenarios with joint scenario models, the model must regenerate thescenario at each timestep based on new observations in a Model PredictiveControl (MPC) fashion. Although reactive, this method is time-consuming, as onecomplete possible future for all NPCs is generated per simulation step.Alternatively, one can utilize an autoregressive model (AR) to predict only theimmediate next-step future for all NPCs. Although faster, this method lacks thecapability for advanced planning. We present a rolling diffusion based trafficscene generation model which mixes the benefits of both methods by predictingthe next step future and simultaneously predicting partially noised furtherfuture steps at the same time. We show that such model is efficient compared todiffusion model based AR, achieving a beneficial compromise between reactivityand computational efficiency.</description><author>Yunpeng Liu, Matthew Niedoba, William Harvey, Adam Scibior, Berend Zwartsenberg, Frank Wood</author><pubDate>Thu, 13 Feb 2025 18:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09587v1</guid></item><item><title>Learning to Coordinate with Experts</title><link>http://arxiv.org/abs/2502.09583v1</link><description>When deployed in dynamic environments, AI agents will inevitably encounterchallenges that exceed their individual capabilities. Leveraging assistancefrom expert agents-whether human or AI-can significantly enhance safety andperformance in such situations. However, querying experts is often costly,necessitating the development of agents that can efficiently request andutilize expert guidance. In this paper, we introduce a fundamental coordinationproblem called Learning to Yield and Request Control (YRC), where the objectiveis to learn a strategy that determines when to act autonomously and when toseek expert assistance. We consider a challenging practical setting in which anagent does not interact with experts during training but must adapt to novelenvironmental changes and expert interventions at test time. To facilitateempirical research, we introduce YRC-Bench, an open-source benchmark featuringdiverse domains. YRC-Bench provides a standardized Gym-like API, simulatedexperts, evaluation pipeline, and implementation of competitive baselines.Towards tackling the YRC problem, we propose a novel validation approach andinvestigate the performance of various learning methods across diverseenvironments, yielding insights that can guide future research.</description><author>Mohamad H. Danesh, Tu Trinh, Benjamin Plaut, Nguyen X. Khanh</author><pubDate>Thu, 13 Feb 2025 18:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09583v1</guid></item><item><title>OGBench: Benchmarking Offline Goal-Conditioned RL</title><link>http://arxiv.org/abs/2410.20092v2</link><description>Offline goal-conditioned reinforcement learning (GCRL) is a major problem inreinforcement learning (RL) because it provides a simple, unsupervised, anddomain-agnostic way to acquire diverse behaviors and representations fromunlabeled data without rewards. Despite the importance of this setting, we lacka standard benchmark that can systematically evaluate the capabilities ofoffline GCRL algorithms. In this work, we propose OGBench, a new, high-qualitybenchmark for algorithms research in offline goal-conditioned RL. OGBenchconsists of 8 types of environments, 85 datasets, and reference implementationsof 6 representative offline GCRL algorithms. We have designed these challengingand realistic environments and datasets to directly probe differentcapabilities of algorithms, such as stitching, long-horizon reasoning, and theability to handle high-dimensional inputs and stochasticity. Whilerepresentative algorithms may rank similarly on prior benchmarks, ourexperiments reveal stark strengths and weaknesses in these differentcapabilities, providing a strong foundation for building new algorithms.Project page: https://seohong.me/projects/ogbench</description><author>Seohong Park, Kevin Frans, Benjamin Eysenbach, Sergey Levine</author><pubDate>Thu, 13 Feb 2025 18:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20092v2</guid></item><item><title>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering</title><link>http://arxiv.org/abs/2502.09573v1</link><description>In this study, we tackle industry challenges in video content classificationby exploring and optimizing GPT-based models for zero-shot classificationacross seven critical categories of video quality. We contribute a novelapproach to improving GPT's performance through prompt optimization and policyrefinement, demonstrating that simplifying complex policies significantlyreduces false negatives. Additionally, we introduce a newdecomposition-aggregation-based prompt engineering technique, which outperformstraditional single-prompt methods. These experiments, conducted on realindustry problems, show that thoughtful prompt design can substantially enhanceGPT's performance without additional finetuning, offering an effective andscalable solution for improving video classification systems across variousdomains in industry.</description><author>Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu</author><pubDate>Thu, 13 Feb 2025 18:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09573v1</guid></item><item><title>DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra</title><link>http://arxiv.org/abs/2502.09571v1</link><description>Mass spectrometry plays a fundamental role in elucidating the structures ofunknown molecules and subsequent scientific discoveries. One formulation of thestructure elucidation task is the conditional $\textit{de novo}$ generation ofmolecular structure given a mass spectrum. Toward a more accurate and efficientscientific discovery pipeline for small molecules, we present DiffMS, aformula-restricted encoder-decoder generative network that achievesstate-of-the-art performance on this task. The encoder utilizes a transformerarchitecture and models mass spectra domain knowledge such as peak formulae andneutral losses, and the decoder is a discrete graph diffusion model restrictedby the heavy-atom composition of a known chemical formula. To develop a robustdecoder that bridges latent embeddings and molecular structures, we pretrainthe diffusion decoder with fingerprint-structure pairs, which are available invirtually infinite quantities, compared to structure-spectrum pairs that numberin the tens of thousands. Extensive experiments on established benchmarks showthat DiffMS outperforms existing models on $\textit{de novo}$ moleculegeneration. We provide several ablations to demonstrate the effectiveness ofour diffusion and pretraining approaches and show consistent performancescaling with increasing pretraining dataset size. DiffMS code is publiclyavailable at https://github.com/coleygroup/DiffMS.</description><author>Montgomery Bohde, Mrunali Manjrekar, Runzhong Wang, Shuiwang Ji, Connor W. Coley</author><pubDate>Thu, 13 Feb 2025 18:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09571v1</guid></item><item><title>Enhancing the Utility of Higher-Order Information in Relational Learning</title><link>http://arxiv.org/abs/2502.09570v1</link><description>Higher-order information is crucial for relational learning in many domainswhere relationships extend beyond pairwise interactions. Hypergraphs provide anatural framework for modeling such relationships, which has motivated recentextensions of graph neural net- work architectures to hypergraphs. However,comparisons between hypergraph architectures and standard graph-level modelsremain limited. In this work, we systematically evaluate a selection ofhypergraph-level and graph-level architectures, to determine theireffectiveness in leveraging higher-order information in relational learning.Our results show that graph-level architectures applied to hypergraphexpansions often outperform hypergraph- level ones, even on inputs that arenaturally parametrized as hypergraphs. As an alternative approach forleveraging higher-order information, we propose hypergraph-level encodingsbased on classical hypergraph characteristics. While these encodings do notsignificantly improve hypergraph architectures, they yield substantialperformance gains when combined with graph-level models. Our theoreticalanalysis shows that hypergraph-level encodings provably increase therepresentational power of message-passing graph neural networks beyond that oftheir graph-level counterparts.</description><author>Raphael Pellegrin, Lukas Fesser, Melanie Weber</author><pubDate>Thu, 13 Feb 2025 18:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09570v1</guid></item><item><title>Toward Universal Laws of Outlier Propagation</title><link>http://arxiv.org/abs/2502.08593v2</link><description>We argue that Algorithmic Information Theory (AIT) admits a principled way toquantify outliers in terms of so-called randomness deficiency. For theprobability distribution generated by a causal Bayesian network, we show thatthe randomness deficiency of the joint state decomposes into randomnessdeficiencies of each causal mechanism, subject to the Independence ofMechanisms Principle. Accordingly, anomalous joint observations can bequantitatively attributed to their root causes, i.e., the mechanisms thatbehaved anomalously. As an extension of Levin's law of randomness conservation,we show that weak outliers cannot cause strong ones when Independence ofMechanisms holds. We show how these information theoretic laws provide a betterunderstanding of the behaviour of outliers defined with respect to existingscores.</description><author>Aram Ebtekar, Yuhao Wang, Dominik Janzing</author><pubDate>Thu, 13 Feb 2025 18:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.08593v2</guid></item><item><title>Asymptotic Normality of Generalized Low-Rank Matrix Sensing via Riemannian Geometry</title><link>http://arxiv.org/abs/2407.10238v2</link><description>We prove an asymptotic normality guarantee for generalized low-rank matrixsensing -- i.e., matrix sensing under a general convex loss $\bar\ell(\langleX,M\rangle,y^*)$, where $M\in\mathbb{R}^{d\times d}$ is the unknown rank-$k$matrix, $X$ is a measurement matrix, and $y^*$ is the correspondingmeasurement. Our analysis relies on tools from Riemannian geometry to handledegeneracy of the Hessian of the loss due to rotational symmetry in theparameter space. In particular, we parameterize the manifold of low-rankmatrices by $\bar\theta\bar\theta^\top$, where$\bar\theta\in\mathbb{R}^{d\times k}$. Then, assuming the minimizer of theempirical loss $\bar\theta^0\in\mathbb{R}^{d\times k}$ is in a constant sizeball around the true parameters $\bar\theta^*$, we prove$\sqrt{n}(\phi^0-\phi^*)\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\to\infty$, where$\phi^0$ and $\phi^*$ are representations of $\bar\theta^*$ and $\bar\theta^0$in the horizontal space of the Riemannian quotient manifold$\mathbb{R}^{d\times k}/\text{O}(k)$, and $H^*$ is the Hessian of the true lossin the same representation.</description><author>Osbert Bastani</author><pubDate>Thu, 13 Feb 2025 18:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10238v2</guid></item><item><title>MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing</title><link>http://arxiv.org/abs/2502.09567v1</link><description>We introduce MorphNLI, a modular step-by-step approach to natural languageinference (NLI). When classifying the premise-hypothesis pairs into{entailment, contradiction, neutral}, we use a language model to generate thenecessary edits to incrementally transform (i.e., morph) the premise into thehypothesis. Then, using an off-the-shelf NLI model we track how the entailmentprogresses with these atomic changes, aggregating these intermediate labelsinto a final output. We demonstrate the advantages of our proposed methodparticularly in realistic cross-domain settings, where our method alwaysoutperforms strong baselines with improvements up to 12.6% (relative). Further,our proposed approach is explainable as the atomic edits can be used tounderstand the overall NLI label.</description><author>Vlad Andrei Negru, Robert Vacareanu, Camelia Lemnaru, Mihai Surdeanu, Rodica Potolea</author><pubDate>Thu, 13 Feb 2025 18:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09567v1</guid></item><item><title>Zero-shot generation of synthetic neurosurgical data with large language models</title><link>http://arxiv.org/abs/2502.09566v1</link><description>Clinical data is fundamental to advance neurosurgical research, but access isoften constrained by data availability, small sample sizes, privacyregulations, and resource-intensive preprocessing and de-identificationprocedures. Synthetic data offers a potential solution to challenges associatedwith accessing and using real-world data (RWD). This study aims to evaluate thecapability of zero-shot generation of synthetic neurosurgical data with a largelanguage model (LLM), GPT-4o, by benchmarking with the conditional tabulargenerative adversarial network (CTGAN). Synthetic datasets were compared toreal-world neurosurgical data to assess fidelity (means, proportions,distributions, and bivariate correlations), utility (ML classifier performanceon RWD), and privacy (duplication of records from RWD). The GPT-4o-generateddatasets matched or exceeded CTGAN performance, despite no fine-tuning oraccess to RWD for pre-training. Datasets demonstrated high univariate andbivariate fidelity to RWD without directly exposing any real patient records,even at amplified sample size. Training an ML classifier on GPT-4o-generateddata and testing on RWD for a binary prediction task showed an F1 score (0.706)with comparable performance to training on the CTGAN data (0.705) forpredicting postoperative functional status deterioration. GPT-4o demonstrated apromising ability to generate high-fidelity synthetic neurosurgical data. Thesefindings also indicate that data synthesized with GPT-4o can effectivelyaugment clinical data with small sample sizes, and train ML models forprediction of neurosurgical outcomes. Further investigation is necessary toimprove the preservation of distributional characteristics and boost classifierperformance.</description><author>Austin A. Barr, Eddie Guo, Emre Sezgin</author><pubDate>Thu, 13 Feb 2025 18:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09566v1</guid></item><item><title>Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery</title><link>http://arxiv.org/abs/2411.09101v2</link><description>Vision Transformers (ViT) have recently brought a new wave of research in thefield of computer vision. These models have performed particularly well inimage classification and segmentation. Research on semantic and instancesegmentation has accelerated with the introduction of the new architecture,with over 80% of the top 20 benchmarks for the iSAID dataset based on eitherthe ViT architecture or the attention mechanism behind its success. This paperfocuses on the heuristic comparison of three key factors of using (or notusing) ViT for semantic segmentation of remote sensing aerial images on theiSAID dataset. The experimental results observed during this research wereanalyzed based on three objectives. First, we studied the use of a weightedfused loss function to maximize the mean Intersection over Union (mIoU) scoreand Dice score while minimizing entropy or class representation loss. Second,we compared transfer learning on Meta's MaskFormer, a ViT-based semanticsegmentation model, against a generic UNet Convolutional Neural Network (CNN)based on mIoU, Dice scores, training efficiency, and inference time. Third, weexamined the trade-offs between the two models in comparison to currentstate-of-the-art segmentation models. We show that the novel combined weightedloss function significantly boosts the CNN model's performance compared totransfer learning with ViT. The code for this implementation can be found at:https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.</description><author>Ashim Dahal, Saydul Akbar Murad, Nick Rahimi</author><pubDate>Thu, 13 Feb 2025 18:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09101v2</guid></item><item><title>MDCrow: Automating Molecular Dynamics Workflows with Large Language Models</title><link>http://arxiv.org/abs/2502.09565v1</link><description>Molecular dynamics (MD) simulations are essential for understandingbiomolecular systems but remain challenging to automate. Recent advances inlarge language models (LLM) have demonstrated success in automating complexscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, anagentic LLM assistant capable of automating MD workflows. MDCrow useschain-of-thought over 40 expert-designed tools for handling and processingfiles, setting up simulations, analyzing the simulation outputs, and retrievingrelevant information from literature and databases. We assess MDCrow'sperformance across 25 tasks of varying required subtasks and difficulty, and weevaluate the agent's robustness to both difficulty and prompt style.\texttt{gpt-4o} is able to complete complex tasks with low variance, followedclosely by \texttt{llama3-405b}, a compelling open-source model. While promptstyle does not influence the best models' performance, it has significanteffects on smaller models.</description><author>Quintina Campbell, Sam Cox, Jorge Medina, Brittany Watterson, Andrew D. White</author><pubDate>Thu, 13 Feb 2025 18:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09565v1</guid></item><item><title>Diffusing DeBias: a Recipe for Turning a Bug into a Feature</title><link>http://arxiv.org/abs/2502.09564v1</link><description>Deep learning model effectiveness in classification tasks is often challengedby the quality and quantity of training data which, whenever containing strongspurious correlations between specific attributes and target labels, can resultin unrecoverable biases in model predictions. Tackling these biases is crucialin improving model generalization and trust, especially in real-worldscenarios. This paper presents Diffusing DeBias (DDB), a novel approach actingas a plug-in for common methods in model debiasing while exploiting theinherent bias-learning tendency of diffusion models. Our approach leveragesconditional diffusion models to generate synthetic bias-aligned images, used totrain a bias amplifier model, to be further employed as an auxiliary method indifferent unsupervised debiasing approaches. Our proposed method, which alsotackles the common issue of training set memorization typical of this type oftech- niques, beats current state-of-the-art in multiple benchmark datasets bysignificant margins, demonstrating its potential as a versatile and effectivetool for tackling dataset bias in deep learning applications.</description><author>Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino</author><pubDate>Thu, 13 Feb 2025 18:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09564v1</guid></item><item><title>Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction</title><link>http://arxiv.org/abs/2502.09563v1</link><description>In this paper, we present a self-calibrating framework that jointly optimizescamera parameters, lens distortion and 3D Gaussian representations, enablingaccurate and efficient scene reconstruction. In particular, our techniqueenables high-quality scene reconstruction from Large field-of-view (FOV)imagery taken with wide-angle lenses, allowing the scene to be modeled from asmaller number of images. Our approach introduces a novel method for modelingcomplex lens distortions using a hybrid network that combines invertibleresidual networks with explicit grids. This design effectively regularizes theoptimization process, achieving greater accuracy than conventional cameramodels. Additionally, we propose a cubemap-based resampling strategy to supportlarge FOV images without sacrificing resolution or introducing distortionartifacts. Our method is compatible with the fast rasterization of GaussianSplatting, adaptable to a wide variety of camera lens distortion, anddemonstrates state-of-the-art performance on both synthetic and real-worlddatasets.</description><author>Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec</author><pubDate>Thu, 13 Feb 2025 18:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09563v1</guid></item><item><title>Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos</title><link>http://arxiv.org/abs/2501.04001v2</link><description>This work presents Sa2VA, the first unified model for dense groundedunderstanding of both images and videos. Unlike existing multi-modal largelanguage models, which are often limited to specific modalities and tasks,Sa2VA supports a wide range of image and video tasks, including referringsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VAcombines SAM-2, a foundation video segmentation model, with LLaVA, an advancedvision-language model, and unifies text, image, and video into a shared LLMtoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2in producing precise masks, enabling a grounded, multi-modal understanding ofboth static and dynamic visual content. Additionally, we introduce Ref-SAV, anauto-labeled dataset containing over 72k object expressions in complex videoscenes, designed to boost model performance. We also manually validate 2k videoobjects in the Ref-SAV datasets to benchmark referring video objectsegmentation in complex environments. Experiments show that Sa2VA achievesstate-of-the-art across multiple tasks, particularly in referring video objectsegmentation, highlighting its potential for complex real-world applications.</description><author>Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang</author><pubDate>Thu, 13 Feb 2025 18:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.04001v2</guid></item><item><title>EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</title><link>http://arxiv.org/abs/2502.09560v1</link><description>Leveraging Multi-modal Large Language Models (MLLMs) to create embodiedagents offers a promising avenue for tackling real-world tasks. Whilelanguage-centric embodied agents have garnered substantial attention,MLLM-based embodied agents remain underexplored due to the lack ofcomprehensive evaluation frameworks. To bridge this gap, we introduceEmbodiedBench, an extensive benchmark designed to evaluate vision-drivenembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testingtasks across four environments, ranging from high-level semantic tasks (e.g.,household) to low-level tasks involving atomic actions (e.g., navigation andmanipulation); and (2) six meticulously curated subsets evaluating essentialagent capabilities like commonsense reasoning, complex instructionunderstanding, spatial awareness, visual perception, and long-term planning.Through extensive experiments, we evaluated 13 leading proprietary andopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excelat high-level tasks but struggle with low-level manipulation, with the bestmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides amultifaceted standardized evaluation platform that not only highlights existingchallenges but also offers valuable insights to advance MLLM-based embodiedagents. Our code is available at https://embodiedbench.github.io.</description><author>Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang</author><pubDate>Thu, 13 Feb 2025 18:11:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09560v1</guid></item><item><title>TransMLA: Multi-Head Latent Attention Is All You Need</title><link>http://arxiv.org/abs/2502.07864v2</link><description>Modern large language models (LLMs) often encounter communication bottleneckson current hardware, rather than purely computational constraints. Multi-headLatent Attention (MLA) tackles this challenge by using low-rank matrices in thekey-value (KV) layers, thereby allowing compressed latent KV states to becached. This approach significantly reduces the KV cache size relative totraditional multi-head attention, leading to faster inference. Moreover, MLAemploys an up-projection matrix to increase expressiveness, trading additionalcomputation for reduced communication overhead. Although MLA has demonstratedefficiency and effectiveness in Deepseek V2/V3/R1, many major model providersstill rely on Group Query Attention (GQA) and have not announced any plans toadopt MLA. In this paper, we show that GQA can always be represented by MLAwhile maintaining the same KV cache overhead, but the converse does not hold.To encourage broader use of MLA, we introduce TransMLA, a post-training methodthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,Mixtral) into MLA-based models. After conversion, the model can undergoadditional training to boost expressiveness without increasing the KV cachesize. Furthermore, we plan to develop MLA-specific inference accelerationtechniques to preserve low latency in transformed models, thus enabling moreefficient distillation of Deepseek R1.</description><author>Fanxu Meng, Zengwei Yao, Muhan Zhang</author><pubDate>Thu, 13 Feb 2025 18:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.07864v2</guid></item><item><title>SyntheticPop: Attacking Speaker Verification Systems With Synthetic VoicePops</title><link>http://arxiv.org/abs/2502.09553v1</link><description>Voice Authentication (VA), also known as Automatic Speaker Verification(ASV), is a widely adopted authentication method, particularly in automatedsystems like banking services, where it serves as a secondary layer of userauthentication. Despite its popularity, VA systems are vulnerable to variousattacks, including replay, impersonation, and the emerging threat of deepfakeaudio that mimics the voice of legitimate users. To mitigate these risks,several defense mechanisms have been proposed. One such solution, Voice Pops,aims to distinguish an individual's unique phoneme pronunciations during theenrollment process. While promising, the effectiveness of VA+VoicePop against abroader range of attacks, particularly logical or adversarial attacks, remainsinsufficiently explored. We propose a novel attack method, which we refer to asSyntheticPop, designed to target the phoneme recognition capabilities of theVA+VoicePop system. The SyntheticPop attack involves embedding synthetic "pop"noises into spoofed audio samples, significantly degrading the model'sperformance. We achieve an attack success rate of over 95% while poisoning 20%of the training dataset. Our experiments demonstrate that VA+VoicePop achieves69% accuracy under normal conditions, 37% accuracy when subjected to a baselinelabel flipping attack, and just 14% accuracy under our proposed SyntheticPopattack, emphasizing the effectiveness of our method.</description><author>Eshaq Jamdar, Amith Kamath Belman</author><pubDate>Thu, 13 Feb 2025 18:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09553v1</guid></item><item><title>Hello Again! LLM-powered Personalized Agent for Long-term Dialogue</title><link>http://arxiv.org/abs/2406.05925v2</link><description>Open-domain dialogue systems have seen remarkable advancements with thedevelopment of large language models (LLMs). Nonetheless, most existingdialogue systems predominantly focus on brief single-session interactions,neglecting the real-world demands for long-term companionship and personalizedinteractions with chatbots. Crucial to addressing this real-world need areevent summary and persona management, which enable reasoning for appropriatelong-term dialogue responses. Recent progress in the human-like cognitive andreasoning capabilities of LLMs suggests that LLM-based agents couldsignificantly enhance automated perception, decision-making, andproblem-solving. In response to this potential, we introduce a model-agnosticframework, the Long-term Dialogue Agent (LD-Agent), which incorporates threeindependently tunable modules dedicated to event perception, personaextraction, and response generation. For the event memory module, long andshort-term memory banks are employed to separately focus on historical andongoing sessions, while a topic-based retrieval mechanism is introduced toenhance the accuracy of memory retrieval. Furthermore, the persona moduleconducts dynamic persona modeling for both users and agents. The integration ofretrieved memories and extracted personas is subsequently fed into thegenerator to induce appropriate responses. The effectiveness, generality, andcross-domain capabilities of LD-Agent are empirically demonstrated acrossvarious illustrative benchmarks, models, and tasks. The code is released athttps://github.com/leolee99/LD-Agent.</description><author>Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua</author><pubDate>Thu, 13 Feb 2025 18:02:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05925v2</guid></item><item><title>Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for Remote Sensing Community</title><link>http://arxiv.org/abs/2408.09110v2</link><description>Object detection, particularly open-vocabulary object detection, plays acrucial role in Earth sciences, such as environmental monitoring, naturaldisaster assessment, and land-use planning. However, existing open-vocabularydetectors, primarily trained on natural-world images, struggle to generalize toremote sensing images due to a significant data domain gap. Thus, this paperaims to advance the development of open-vocabulary object detection in remotesensing community. To achieve this, we first reformulate the task as LocateAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.We then developed the LAE-Label Engine which collects, auto-annotates, andunifies up to 10 remote sensing datasets creating the LAE-1M - the firstlarge-scale remote sensing object detection dataset with broad categorycoverage. Using the LAE-1M, we further propose and train the novel LAE-DINOModel, the first open-vocabulary foundation object detector for the LAE task,featuring Dynamic Vocabulary Construction (DVC) and Visual-Guided Text PromptLearning (VisGT) modules. DVC dynamically constructs vocabulary for eachtraining batch, while VisGT maps visual features to semantic space, enhancingtext features. We comprehensively conduct experiments on established remotesensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-classLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset andthe effectiveness of the LAE-DINO method.</description><author>Jiancheng Pan, Yanxing Liu, Yuqian Fu, Muyuan Ma, Jiahao Li, Danda Pani Paudel, Luc Van Gool, Xiaomeng Huang</author><pubDate>Thu, 13 Feb 2025 18:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09110v2</guid></item><item><title>VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output</title><link>http://arxiv.org/abs/2502.04103v2</link><description>The rapid evolution of large language models (LLMs) has transformedhuman-computer interaction (HCI), but the interaction with LLMs is currentlymainly focused on text-based interactions, while other multi-model approachesremain under-explored. This paper introduces VTutor, an open-source SoftwareDevelopment Kit (SDK) that combines generative AI with advanced animationtechnologies to create engaging, adaptable, and realistic APAs for human-AImulti-media interactions. VTutor leverages LLMs for real-time personalizedfeedback, advanced lip synchronization for natural speech alignment, and WebGLrendering for seamless web integration. Supporting various 2D and 3D charactermodels, VTutor enables researchers and developers to design emotionallyresonant, contextually adaptive learning agents. This toolkit enhances learnerengagement, feedback receptivity, and human-AI interaction while promotingtrustworthy AI principles in education. VTutor sets a new standard fornext-generation APAs, offering an accessible, scalable solution for fosteringmeaningful and immersive human-AI interaction experiences. The VTutor projectis open-sourced and welcomes community-driven contributions and showcases.</description><author>Eason Chen, Chenyu Lin, Xinyi Tang, Aprille Xi, Canwen Wang, Jionghao Lin, Kenneth R Koedinger</author><pubDate>Thu, 13 Feb 2025 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04103v2</guid></item><item><title>WASP: A Weight-Space Approach to Detecting Learned Spuriousness</title><link>http://arxiv.org/abs/2410.18970v3</link><description>It is of crucial importance to train machine learning models such that theyclearly understand what defines each class in a given task. Though there is asum of works dedicated to identifying the spurious correlations featured by adataset that may impact the model's understanding of the classes, all currentapproaches rely solely on data or error analysis. That is, they cannot pointout spurious correlations learned by the model that are not already pointed outby the counterexamples featured in the validation or training sets. We proposea method that transcends this limitation, switching the focus from analyzing amodel's predictions to analyzing the model's weights, the mechanism behind themaking of the decisions, which proves to be more insightful. Our proposedWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing theweights of foundation models as they drift towards capturing various (spurious)correlations while being fine-tuned on a given dataset. We demonstrate thatdifferent from previous works, our method (i) can expose spurious correlationsfeatured by a dataset even when they are not exposed by training or validationcounterexamples, (ii) it works for multiple modalities such as image and text,and (iii) it can uncover previously untapped spurious correlations learned byImageNet-1k classifiers.</description><author>Cristian Daniel Păduraru, Antonio Bărbălau, Radu Filipescu, Andrei Liviu Nicolicioiu, Elena Burceanu</author><pubDate>Thu, 13 Feb 2025 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18970v3</guid></item><item><title>Evaluating Zero-Shot Long-Context LLM Compression</title><link>http://arxiv.org/abs/2406.06773v2</link><description>This study evaluates the effectiveness of zero-shot compression techniques onlarge language models (LLMs) under long-context. We identify the tendency forcomputational errors to increase under long-context when employing certaincompression methods. We propose a hypothesis to explain the varied behavior ofdifferent LLM compression techniques and explore remedies to mitigate theperformance decline observed in some techniques under long-context. This is acourse report for COS 598D Machine Learning and Systems by Prof. Kai Li atPrinceton University. Due to limited computational resources, our experimentswere conducted only on LLaMA-2-7B-32K.</description><author>Chenyu Wang, Yihan Wang, Kai Li</author><pubDate>Thu, 13 Feb 2025 17:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06773v2</guid></item><item><title>Fast Tensor Completion via Approximate Richardson Iteration</title><link>http://arxiv.org/abs/2502.09534v1</link><description>We study tensor completion (TC) through the lens of low-rank tensordecomposition (TD). Many TD algorithms use fast alternating minimizationmethods, which solve highly structured linear regression problems at each step(e.g., for CP, Tucker, and tensor-train decompositions). However, suchalgebraic structure is lost in TC regression problems, making direct extensionsunclear. To address this, we propose a lifting approach that approximatelysolves TC regression problems using structured TD regression algorithms asblackbox subroutines, enabling sublinear-time methods. We theoretically analyzethe convergence rate of our approximate Richardson iteration based algorithm,and we demonstrate on real-world tensors that its running time can be 100xfaster than direct methods for CP completion.</description><author>Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie</author><pubDate>Thu, 13 Feb 2025 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09534v1</guid></item><item><title>Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</title><link>http://arxiv.org/abs/2502.09533v1</link><description>Recent advances in conditional diffusion models have shown promise forgenerating realistic TalkingFace videos, yet challenges persist in achievingconsistent head movement, synchronized facial expressions, and accurate lipsynchronization over extended generations. To address these, we introduce the\textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel(\textbf{MCDM}), which utilizes both archived and current clip motion priors toenhance motion prediction and ensure temporal consistency. The model consistsof three key elements: (1) an archived-clip motion-prior that incorporateshistorical frames and a reference frame to preserve identity and context; (2) apresent-clip motion-prior diffusion model that captures multimodal causalityfor accurate predictions of head movements, lip sync, and expressions; and (3)a memory-efficient temporal attention mechanism that mitigates erroraccumulation by dynamically storing and updating motion features. We alsorelease the \textbf{TalkingFace-Wild} dataset, a multilingual collection ofover 200 hours of footage across 10 languages. Experimental results demonstratethe effectiveness of MCDM in maintaining identity and motion continuity forlong-term TalkingFace generation. Code, models, and datasets will be publiclyavailable.</description><author>Fei Shen, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua</author><pubDate>Thu, 13 Feb 2025 17:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09533v1</guid></item><item><title>Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages</title><link>http://arxiv.org/abs/2502.09532v1</link><description>Recent advances in generative AI have precipitated a proliferation of novelwriting assistants. These systems typically rely on multilingual large languagemodels (LLMs), providing globalized workers the ability to revise or creatediverse forms of content in different languages. However, there is substantialevidence indicating that the performance of multilingual LLMs varies betweenlanguages. Users who employ writing assistance for multiple languages aretherefore susceptible to disparate output quality. Importantly, recent researchhas shown that people tend to generalize algorithmic errors across independenttasks, violating the behavioral axiom of choice independence. In this paper, weanalyze whether user utilization of novel writing assistants in a charityadvertisement writing task is affected by the AI's performance in a secondlanguage. Furthermore, we quantify the extent to which these patterns translateinto the persuasiveness of generated charity advertisements, as well as therole of peoples' beliefs about LLM utilization in their donation choices. Ourresults provide evidence that writers who engage with an LLM-based writingassistant violate choice independence, as prior exposure to a Spanish LLMreduces subsequent utilization of an English LLM. While these patterns do notaffect the aggregate persuasiveness of the generated advertisements, people'sbeliefs about the source of an advertisement (human versus AI) do. Inparticular, Spanish-speaking female participants who believed that they read anAI-generated advertisement strongly adjusted their donation behavior downwards.Furthermore, people are generally not able to adequately differentiate betweenhuman-generated and LLM-generated ads. Our work has important implications forthe design, development, integration, and adoption of multilingual LLMs asassistive agents -- particularly in writing tasks.</description><author>Shreyan Biswas, Alexander Erlei, Ujwal Gadiraju</author><pubDate>Thu, 13 Feb 2025 17:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09532v1</guid></item><item><title>SteROI-D: System Design and Mapping for Stereo Depth Inference on Regions of Interest</title><link>http://arxiv.org/abs/2502.09528v1</link><description>Machine learning algorithms have enabled high quality stereo depth estimationto run on Augmented and Virtual Reality (AR/VR) devices. However, high energyconsumption across the full image processing stack prevents stereo depthalgorithms from running effectively on battery-limited devices. This paperintroduces SteROI-D, a full stereo depth system paired with a mappingmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsityat the system level to save energy. SteROI-D's flexible and heterogeneouscompute fabric supports diverse ROIs. Importantly, we introduce a systematicmapping methodology to effectively handle dynamic ROIs, thereby maximizingenergy savings. Using these techniques, our 28nm prototype SteROI-D designachieves up to 4.35x reduction in total system energy compared to a baselineASIC.</description><author>Jack Erhardt, Ziang Li, Reid Pinkham, Andrew Berkovich, Zhengya Zhang</author><pubDate>Thu, 13 Feb 2025 17:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09528v1</guid></item><item><title>Robust Learning of Multi-index Models via Iterative Subspace Approximation</title><link>http://arxiv.org/abs/2502.09525v1</link><description>We study the task of learning Multi-Index Models (MIMs) with label noiseunder the Gaussian distribution. A $K$-MIM is any function $f$ that onlydepends on a $K$-dimensional subspace. We focus on well-behaved MIMs withfinite ranges that satisfy certain regularity properties. Our main contributionis a general robust learner that is qualitatively optimal in the StatisticalQuery (SQ) model. Our algorithm iteratively constructs better approximations tothe defining subspace by computing low-degree moments conditional on theprojection to the subspace computed thus far, and adding directions withrelatively large empirical moments. This procedure efficiently finds a subspace$V$ so that $f(\mathbf{x})$ is close to a function of the projection of$\mathbf{x}$ onto $V$. Conversely, for functions for which these conditionalmoments do not help, we prove an SQ lower bound suggesting that no efficientlearner exists. As applications, we provide faster robust learners for the following conceptclasses: * {\bf Multiclass Linear Classifiers} We give a constant-factor approximateagnostic learner with sample complexity $N = O(d)2^{\mathrm{poly}(K/\epsilon)}$ and computational complexity $\mathrm{poly}(N,d)$. This is the first constant-factor agnostic learner for this class whosecomplexity is a fixed-degree polynomial in $d$. * {\bf Intersections of Halfspaces} We give an approximate agnostic learnerfor this class achieving 0-1 error $K \tilde{O}(\mathrm{OPT}) + \epsilon$ withsample complexity $N=O(d^2) 2^{\mathrm{poly}(K/\epsilon)}$ and computationalcomplexity $\mathrm{poly}(N ,d)$. This is the first agnostic learner for thisclass with near-linear error dependence and complexity a fixed-degreepolynomial in $d$. Furthermore, we show that in the presence of random classification noise, thecomplexity of our algorithm scales polynomially with $1/\epsilon$.</description><author>Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Nikos Zarifis</author><pubDate>Thu, 13 Feb 2025 17:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09525v1</guid></item><item><title>SQ-GAN: Semantic Image Communications Using Masked Vector Quantization</title><link>http://arxiv.org/abs/2502.09520v1</link><description>This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approachintegrating generative models to optimize image compression forsemantic/task-oriented communications. SQ-GAN employs off-the-shelf semanticsemantic segmentation and a new specifically developed semantic-conditionedadaptive mask module (SAMM) to selectively encode semantically significantfeatures of the images. SQ-GAN outperforms state-of-the-art image compressionschemes such as JPEG2000 and BPG across multiple metrics, including perceptualquality and semantic segmentation accuracy on the post-decoding reconstructedimage, at extreme low compression rates expressed in bits per pixel.</description><author>Francesco Pezone, Sergio Barbarossa, Giuseppe Caire</author><pubDate>Thu, 13 Feb 2025 17:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09520v1</guid></item><item><title>Salamandra Technical Report</title><link>http://arxiv.org/abs/2502.08489v2</link><description>This work introduces Salamandra, a suite of open-source decoder-only largelanguage models available in three different sizes: 2, 7, and 40 billionparameters. The models were trained from scratch on highly multilingual datathat comprises text in 35 European languages and code. Our carefully curatedcorpus is made exclusively from open-access data compiled from a wide varietyof sources. Along with the base models, supplementary checkpoints that werefine-tuned on public-domain instruction data are also released for chatapplications. Additionally, we also share our preliminary experiments onmultimodality, which serve as proof-of-concept to showcase potentialapplications for the Salamandra family. Our extensive evaluations onmultilingual benchmarks reveal that Salamandra has strong capabilities,achieving competitive performance when compared to similarly sized open-sourcemodels. We provide comprehensive evaluation results both on standard downstreamtasks as well as key aspects related to bias and safety.With this technicalreport, we intend to promote open science by sharing all the details behind ourdesign choices, data curation strategy and evaluation methodology. In additionto that, we deviate from the usual practice by making our training andevaluation scripts publicly accessible. We release all models under apermissive Apache 2.0 license in order to foster future research and facilitatecommercial use, thereby contributing to the open-source ecosystem of largelanguage models.</description><author>Aitor Gonzalez-Agirre, Marc Pàmies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, José Javier Saiz, Ferran Espuña, Jaume Prats, Javier Aula-Blasco, Mario Mina, Iñigo Pikabea, Adrián Rubio, Alexander Shvets, Anna Sallés, Iñaki Lacunza, Jorge Palomar, Júlia Falcão, Lucía Tormo, Luis Vasquez-Reina, Montserrat Marimon, Oriol Pareras, Valle Ruiz-Fernández, Marta Villegas</author><pubDate>Thu, 13 Feb 2025 17:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.08489v2</guid></item><item><title>Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books</title><link>http://arxiv.org/abs/2502.05331v2</link><description>Books, while often rich in cultural insights, can also mirror societal biasesof their eras - biases that Large Language Models (LLMs) may learn andperpetuate during training. We introduce a novel method to trace and quantifythese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising593 fictional books across seven decades (1950-2019), to track bias evolution.By fine-tuning LLMs on books from each decade and using targeted prompts, weexamine shifts in biases related to gender, sexual orientation, race, andreligion. Our findings indicate that LLMs trained on decade-specific booksmanifest biases reflective of their times, with both gradual trends and notableshifts. For example, model responses showed a progressive increase in theportrayal of women in leadership roles (from 8% to 22%) from the 1950s to2010s, with a significant uptick in the 1990s (from 4% to 12%), possiblyaligning with third-wave feminism. Same-sex relationship references increasedmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+visibility. Concerningly, negative portrayals of Islam rose sharply in the2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, wedemonstrate that these biases stem mainly from the books' content and not themodels' architecture or initial training. Our study offers a new perspective onsocietal bias trends by bridging AI, literary studies, and social scienceresearch.</description><author>Sangmitra Madhusudan, Robert Morabito, Skye Reid, Nikta Gohari Sadr, Ali Emami</author><pubDate>Thu, 13 Feb 2025 17:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05331v2</guid></item><item><title>Diffusion Models for Molecules: A Survey of Methods and Tasks</title><link>http://arxiv.org/abs/2502.09511v1</link><description>Generative tasks about molecules, including but not limited to moleculegeneration, are crucial for drug discovery and material design, and haveconsistently attracted significant attention. In recent years, diffusion modelshave emerged as an impressive class of deep generative models, sparkingextensive research and leading to numerous studies on their application tomolecular generative tasks. Despite the proliferation of related work, thereremains a notable lack of up-to-date and systematic surveys in this area.Particularly, due to the diversity of diffusion model formulations, moleculardata modalities, and generative task types, the research landscape ischallenging to navigate, hindering understanding and limiting the area'sgrowth. To address this, this paper conducts a comprehensive survey ofdiffusion model-based molecular generative methods. We systematically reviewthe research from the perspectives of methodological formulations, datamodalities, and task types, offering a novel taxonomy. This survey aims tofacilitate understanding and further flourishing development in this area. Therelevant papers are summarized at:https://github.com/AzureLeon1/awesome-molecular-diffusion-models.</description><author>Liang Wang, Chao Song, Zhiyuan Liu, Yu Rong, Qiang Liu, Shu Wu, Liang Wang</author><pubDate>Thu, 13 Feb 2025 17:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09511v1</guid></item><item><title>Measuring Human Contribution in AI-Assisted Content Generation</title><link>http://arxiv.org/abs/2408.14792v2</link><description>With the growing prevalence of generative artificial intelligence (AI), anincreasing amount of content is no longer exclusively generated by humans butby generative AI models with human guidance. This shift presents notablechallenges for the delineation of originality due to the varying degrees ofhuman contribution in AI-assisted works. This study raises the researchquestion of measuring human contribution in AI-assisted content generation andintroduces a framework to address this question that is grounded in informationtheory. By calculating mutual information between human input and AI-assistedoutput relative to self-information of AI-assisted output, we quantify theproportional information contribution of humans in content generation. Ourexperimental results demonstrate that the proposed measure effectivelydiscriminates between varying degrees of human contribution across multiplecreative domains. We hope that this work lays a foundation for measuring humancontributions in AI-assisted content generation in the era of generative AI.</description><author>Yueqi Xie, Tao Qi, Jingwei Yi, Xiyuan Yang, Ryan Whalen, Junming Huang, Qian Ding, Yu Xie, Xing Xie, Fangzhao Wu</author><pubDate>Thu, 13 Feb 2025 17:22:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14792v2</guid></item><item><title>EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling</title><link>http://arxiv.org/abs/2502.09509v1</link><description>Latent generative models have emerged as a leading approach for high-qualityimage synthesis. These models rely on an autoencoder to compress images into alatent space, followed by a generative model to learn the latent distribution.We identify that existing autoencoders lack equivariance to semantic-preservingtransformations like scaling and rotation, resulting in complex latent spacesthat hinder generative performance. To address this, we propose EQ-VAE, asimple regularization approach that enforces equivariance in the latent space,reducing its complexity without degrading reconstruction quality. By finetuningpre-trained autoencoders with EQ-VAE, we enhance the performance of severalstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.EQ-VAE is compatible with both continuous and discrete autoencoders, thusoffering a versatile enhancement for a wide range of latent generative models.Project page and code: https://eq-vae.github.io/.</description><author>Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis</author><pubDate>Thu, 13 Feb 2025 17:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09509v1</guid></item><item><title>When and How Does CLIP Enable Domain and Compositional Generalization?</title><link>http://arxiv.org/abs/2502.09507v1</link><description>The remarkable generalization performance of contrastive vision-languagemodels like CLIP is often attributed to the diversity of their trainingdistributions. However, key questions remain unanswered: Can CLIP generalize toan entirely unseen domain when trained on a diverse mixture of domains (domaingeneralization)? Can it generalize to unseen classes within partially seendomains (compositional generalization)? What factors affect suchgeneralization? To answer these questions, we trained CLIP models onsystematically constructed training distributions with controlled domaindiversity and object class exposure. Our experiments show that domain diversityis essential for both domain and compositional generalization, yetcompositional generalization can be surprisingly weaker than domaingeneralization when the training distribution contains a suboptimal subset ofthe test domain. Through data-centric and mechanistic analyses, we find thatsuccessful generalization requires learning of shared representations alreadyin intermediate layers and shared circuitry.</description><author>Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox</author><pubDate>Thu, 13 Feb 2025 17:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09507v1</guid></item><item><title>AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization</title><link>http://arxiv.org/abs/2502.09503v1</link><description>Transformer architectures have transformed AI applications but remain complexto customize for domain experts lacking low-level implementation expertise. Weintroduce AttentionSmithy, a modular software package that simplifiestransformer innovation by breaking down key components into reusable buildingblocks: attention modules, feed-forward networks, normalization layers, andpositional encodings. Users can rapidly prototype and evaluate transformervariants without extensive coding. Our framework supports four positionalencoding strategies and integrates with neural architecture search forautomated design. We validate AttentionSmithy by replicating the originaltransformer under resource constraints and optimizing translation performanceby combining positional encodings. Additionally, we demonstrate itsadaptability in gene-specific modeling, achieving over 95% accuracy in celltype classification. These case studies highlight AttentionSmithy's potentialto accelerate research across diverse fields by removing frameworkimplementation barriers.</description><author>Caleb Cranney, Jesse G. Meyer</author><pubDate>Thu, 13 Feb 2025 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09503v1</guid></item><item><title>Scalable First-order Method for Certifying Optimal k-Sparse GLMs</title><link>http://arxiv.org/abs/2502.09502v1</link><description>This paper investigates the problem of certifying optimality for sparsegeneralized linear models (GLMs), where sparsity is enforced through an$\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks cancertify optimality by pruning nodes using dual bounds, existing methods forcomputing these bounds are either computationally intensive or exhibit slowconvergence, limiting their scalability to large-scale problems. To addressthis challenge, we propose a first-order proximal gradient algorithm designedto solve the perspective relaxation of the problem within a BnB framework.Specifically, we formulate the relaxed problem as a composite optimizationproblem and demonstrate that the proximal operator of the non-smooth componentcan be computed exactly in log-linear time complexity, eliminating the need tosolve a computationally expensive second-order cone program. Furthermore, weintroduce a simple restart strategy that enhances convergence speed whilemaintaining low per-iteration complexity. Extensive experiments on syntheticand real-world datasets show that our approach significantly accelerates dualbound computations and is highly effective in providing optimality certificatesfor large-scale problems.</description><author>Jiachang Liu, Soroosh Shafiee, Andrea Lodi</author><pubDate>Thu, 13 Feb 2025 17:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09502v1</guid></item><item><title>Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery</title><link>http://arxiv.org/abs/2502.09501v1</link><description>This paper addresses generalized category discovery (GCD), the task ofclustering unlabeled data from potentially known or unknown categories with thehelp of labeled instances from each known category. Compared to traditionalsemi-supervised learning, GCD is more challenging because unlabeled data couldbe from novel categories not appearing in labeled data. Currentstate-of-the-art methods typically learn a parametric classifier assisted byself-distillation. While being effective, these methods do not make use ofcross-instance similarity to discover class-specific semantics which areessential for representation learning and category discovery. In this paper, werevisit the association-based paradigm and propose a Prior-constrainedAssociation Learning method to capture and learn the semantic relations withindata. In particular, the labeled data from known categories provides a uniqueprior for the association of unlabeled data. Unlike previous methods that onlyadopts the prior as a pre or post-clustering refinement, we fully incorporatethe prior into the association process, and let it constrain the associationtowards a reliable grouping outcome. The estimated semantic groups are utilizedthrough non-parametric prototypical contrast to enhance the representationlearning. A further combination of both parametric and non-parametricclassification complements each other and leads to a model that outperformsexisting methods by a significant margin. On multiple GCD benchmarks, weperform extensive experiments and validate the effectiveness of our proposedmethod.</description><author>Menglin Wang, Zhun Zhong, Xiaojin Gong</author><pubDate>Thu, 13 Feb 2025 17:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09501v1</guid></item><item><title>Rationalization Models for Text-to-SQL</title><link>http://arxiv.org/abs/2502.06759v2</link><description>We introduce a framework for generating Chain-of-Thought (CoT) rationales toenhance text-to-SQL model fine-tuning. These rationales consist of intermediateSQL statements and explanations, serving as incremental steps towardconstructing the final SQL query. The process begins with manually annotating asmall set of examples, which are then used to prompt a large language model inan iterative, dynamic few-shot knowledge distillation procedure from a teachermodel. A rationalization model is subsequently trained on the validateddecomposed queries, enabling extensive synthetic CoT annotations fortext-to-SQL datasets. To evaluate the approach, we fine-tune small languagemodels with and without these rationales on the BIRD dataset. Results indicatethat step-by-step query generation improves execution accuracy, especially formoderately and highly complex queries, while also enhancing explainability.</description><author>Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian</author><pubDate>Thu, 13 Feb 2025 17:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06759v2</guid></item><item><title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title><link>http://arxiv.org/abs/2502.08168v2</link><description>As a powerful all-weather Earth observation tool, synthetic aperture radar(SAR) remote sensing enables critical military reconnaissance, maritimesurveillance, and infrastructure monitoring. Although Vision language models(VLMs) have made remarkable progress in natural language processing and imageunderstanding, their applications remain limited in professional domains due toinsufficient domain expertise. This paper innovatively proposes the firstlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, whichcontains approximately 2 million high-quality image-text pairs, encompassesdiverse scenarios with detailed target annotations. This dataset not onlysupports several key tasks such as visual understanding and object detectiontasks, but also has unique innovative aspects: this study develop avisual-language dataset and benchmark for the SAR domain, enabling andevaluating VLMs' capabilities in SAR image interpretation, which provides aparadigmatic framework for constructing multimodal datasets across variousremote sensing vertical domains. Through experiments on 16 mainstream VLMs, theeffectiveness of the dataset has been fully verified. The project will bereleased at https://github.com/JimmyMa99/SARChat.</description><author>Zhiming Ma, Xiayang Xiao, Sihao Dong, Peidong Wang, HaiPeng Wang, Qingyun Pan</author><pubDate>Thu, 13 Feb 2025 17:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.08168v2</guid></item><item><title>Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting</title><link>http://arxiv.org/abs/2502.09500v1</link><description>Catastrophic forgetting -- the phenomenon of a neural network learning a taskt1 and losing the ability to perform it after being trained on some other taskt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,1989]. We present a method, Eidetic Learning, that provably solves catastrophicforgetting. A network trained with Eidetic Learning -- here, an EideticNet --requires no rehearsal or replay. We consider successive discrete tasks and showhow at inference time an EideticNet automatically routes new instances withoutauxiliary task information. An EideticNet bears a family resemblance to thesparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that networkcapacity is partitioned across tasks and the network itself performsdata-conditional routing. An EideticNet is easy to implement and train, isefficient, and has time and space complexity linear in the number ofparameters. The guarantee of our method holds for normalization layers ofmodern neural networks during both pre-training and fine-tuning. We show with avariety of network architectures and sets of tasks that EideticNets are immuneto forgetting. While the practical benefits of EideticNets are substantial, webelieve they can be benefit practitioners and theorists alike. The code fortraining EideticNets is available at\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.</description><author>Nicholas Dronen, Randall Balestriero</author><pubDate>Thu, 13 Feb 2025 17:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09500v1</guid></item><item><title>Improve LLM-based Automatic Essay Scoring with Linguistic Features</title><link>http://arxiv.org/abs/2502.09497v1</link><description>Automatic Essay Scoring (AES) assigns scores to student essays, reducing thegrading workload for instructors. Developing a scoring system capable ofhandling essays across diverse prompts is challenging due to the flexibilityand diverse nature of the writing task. Existing methods typically fall intotwo categories: supervised feature-based approaches and large language model(LLM)-based methods. Supervised feature-based approaches often achieve higherperformance but require resource-intensive training. In contrast, LLM-basedmethods are computationally efficient during inference but tend to suffer fromlower performance. This paper combines these approaches by incorporatinglinguistic features into LLM-based scoring. Experimental results show that thishybrid method outperforms baseline models for both in-domain and out-of-domainwriting prompts.</description><author>Zhaoyi Joey Hou, Alejandro Ciuba, Xiang Lorraine Li</author><pubDate>Thu, 13 Feb 2025 17:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09497v1</guid></item><item><title>Agent-OM: Leveraging LLM Agents for Ontology Matching</title><link>http://arxiv.org/abs/2312.00326v9</link><description>Ontology matching (OM) enables semantic interoperability between differentontologies and resolves their conceptual heterogeneity by aligning relatedentities. OM systems currently have two prevailing design paradigms:conventional knowledge-based expert systems and newer machine learning-basedpredictive systems. While large language models (LLMs) and LLM agents haverevolutionised data engineering and have been applied creatively in manydomains, their potential for OM remains underexplored. This study introduces anovel agent-powered LLM-based design paradigm for OM systems. Withconsideration of several specific challenges in leveraging LLM agents for OM,we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),consisting of two Siamese agents for retrieval and matching, with a set of OMtools. Our framework is implemented in a proof-of-concept system. Evaluationsof three Ontology Alignment Evaluation Initiative (OAEI) tracks overstate-of-the-art OM systems show that our system can achieve results very closeto the long-standing best performance on simple OM tasks and can significantlyimprove the performance on complex and few-shot OM tasks.</description><author>Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</author><pubDate>Thu, 13 Feb 2025 17:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00326v9</guid></item><item><title>HorNets: Learning from Discrete and Continuous Signals with Routing Neural Networks</title><link>http://arxiv.org/abs/2501.14346v2</link><description>Construction of neural network architectures suitable for learning from bothcontinuous and discrete tabular data is a challenging research endeavor.Contemporary high-dimensional tabular data sets are often characterized by arelatively small instance count, requiring data-efficient learning. We proposeHorNets (Horn Networks), a neural network architecture with state-of-the-artperformance on synthetic and real-life data sets from scarce-data tabulardomains. HorNets are based on a clipped polynomial-like activation function,extended by a custom discrete-continuous routing mechanism that decides whichpart of the neural network to optimize based on the input's cardinality. Byexplicitly modeling parts of the feature combination space or combining wholespace in a linear attention-like manner, HorNets dynamically decide which modeof operation is the most suitable for a given piece of data with no explicitsupervision. This architecture is one of the few approaches that reliablyretrieves logical clauses (including noisy XNOR) and achieves state-of-the-artclassification performance on 14 real-life biomedical high-dimensional datasets. HorNets are made freely available under a permissive license alongside asynthetic generator of categorical benchmarks.</description><author>Boshko Koloski, Nada Lavrač, Blaž Škrlj</author><pubDate>Thu, 13 Feb 2025 17:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.14346v2</guid></item><item><title>On Agnostic PAC Learning in the Small Error Regime</title><link>http://arxiv.org/abs/2502.09496v1</link><description>Binary classification in the classic PAC model exhibits a curious phenomenon:Empirical Risk Minimization (ERM) learners are suboptimal in the realizablecase yet optimal in the agnostic case. Roughly speaking, this owes itself tothe fact that non-realizable distributions $\mathcal{D}$ are simply moredifficult to learn than realizable distributions -- even when one discounts alearner's error by $\mathrm{err}(h^*_{\mathcal{D}})$, the error of the besthypothesis in $\mathcal{H}$ for $\mathcal{D}$. Thus, optimal agnostic learnersare permitted to incur excess error on (easier-to-learn) distributions$\mathcal{D}$ for which $\tau = \mathrm{err}(h^*_{\mathcal{D}})$ is small. Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses thisshortcoming by including $\tau$ itself as a parameter in the agnostic errorterm. In this more fine-grained model, they demonstrate tightness of the errorlower bound $\tau + \Omega \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} +\frac{d + \log(1 / \delta)}{m} \right)$ in a regime where $\tau &gt; d/m$, andleave open the question of whether there may be a higher lower bound when $\tau\approx d/m$, with $d$ denoting $\mathrm{VC}(\mathcal{H})$. In this work, weresolve this question by exhibiting a learner which achieves error $c \cdot\tau + O \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1/ \delta)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lowerbound when $\tau \approx d/m$. Further, our learner is computationallyefficient and is based upon careful aggregations of ERM classifiers, makingprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS`24). We leave open the interesting question of whether our approach can berefined to lower the constant from 2.1 to 1, which would completely settle thecomplexity of agnostic learning.</description><author>Julian Asilis, Mikael Møller Høgsgaard, Grigoris Velegkas</author><pubDate>Thu, 13 Feb 2025 17:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09496v1</guid></item><item><title>Cracking the Code: Enhancing Development finance understanding with artificial intelligence</title><link>http://arxiv.org/abs/2502.09495v1</link><description>Analyzing development projects is crucial for understanding donors aidstrategies, recipients priorities, and to assess development finance capacityto adress development issues by on-the-ground actions. In this area, theOrganisation for Economic Co-operation and Developments (OECD) CreditorReporting System (CRS) dataset is a reference data source. This datasetprovides a vast collection of project narratives from various sectors(approximately 5 million projects). While the OECD CRS provides a rich sourceof information on development strategies, it falls short in informing projectpurposes due to its reporting process based on donors self-declared mainobjectives and pre-defined industrial sectors. This research employs a novelapproach that combines Machine Learning (ML) techniques, specifically NaturalLanguage Processing (NLP), an innovative Python topic modeling technique calledBERTopic, to categorise (cluster) and label development projects based on theirnarrative descriptions. By revealing existing yet hidden topics of developmentfinance, this application of artificial intelligence enables a betterunderstanding of donor priorities and overall development funding and providesmethods to analyse public and private projects narratives.</description><author>Pierre Beaucoral</author><pubDate>Thu, 13 Feb 2025 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09495v1</guid></item><item><title>Communicating Likelihoods with Normalising Flows</title><link>http://arxiv.org/abs/2502.09494v1</link><description>We present a machine-learning-based workflow to model an unbinned likelihoodfrom its samples. A key advancement over existing approaches is the validationof the learned likelihood using rigorous statistical tests of the jointdistribution, such as the Kolmogorov-Smirnov test of the joint distribution.Our method enables the reliable communication of experimental andphenomenological likelihoods for subsequent analyses. We demonstrate itseffectiveness through three case studies in high-energy physics. To supportbroader adoption, we provide an open-source reference implementation, nabu.</description><author>Jack Y. Araz, Anja Beck, Méril Reboud, Michael Spannowsky, Danny van Dyk</author><pubDate>Thu, 13 Feb 2025 17:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09494v1</guid></item><item><title>Inverse Design with Dynamic Mode Decomposition</title><link>http://arxiv.org/abs/2502.09490v1</link><description>We introduce a computationally efficient method for the automation of inversedesign in science and engineering. Based on simple least-square regression, theunderlying dynamic mode decomposition algorithm can be used to construct alow-rank subspace spanning multiple experiments in parameter space. Theproposed inverse design dynamic mode composition (ID-DMD) algorithm leveragesthe computed low-dimensional subspace to enable fast digital design andoptimization on laptop-level computing, including the potential to prescribethe dynamics themselves. Moreover, the method is robust to noise, physicallyinterpretable, and can provide uncertainty quantification metrics. Thearchitecture can also efficiently scale to large-scale design problems usingrandomized algorithms in the ID-DMD. The simplicity of the method and itsimplementation are highly attractive in practice, and the ID-DMD has beendemonstrated to be an order of magnitude more accurate than competing methodswhile simultaneously being 3-5 orders faster on challenging engineering designproblems ranging from structural vibrations to fluid dynamics. Due to itsspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison withother leading machine learning methods represents a significant advancement indata-driven methods for inverse design and optimization, promising a paradigmshift in how to approach inverse design in practice.</description><author>Yunpeng Zhu, Liangliang Cheng, Anping Jing, Hanyu Huo, Ziqiang Lang, Bo Zhang, J. Nathan Kutz</author><pubDate>Thu, 13 Feb 2025 16:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09490v1</guid></item><item><title>Objective quantification of mood states using large language models</title><link>http://arxiv.org/abs/2502.09487v1</link><description>Emotional states influence human behaviour and cognition, leading to diversethought trajectories. Similarly, Large Language Models (LLMs) showcase anexcellent level of response consistency across wide-ranging contexts (prompts).We leverage these parallels to establish a framework for quantifying mentalstates. Our approach utilises self-report questionnaires that reliably assessthese states due to their inherent sensitivity to patterns of co-occurringresponses. Specifically, we recruited a large sample of participants (N=422) toinvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous setof depressive mood states measured with participants' open-ended responses to adepression questionnaire. We show LLM responses to held-out multiple-choicequestions, given participants' open-ended answers, correlate strongly (r:0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisationfrom mood representations. We explore a link between these representations andfactor analysis. Using ridge regression, we find depression-related subspaceswithin LLM hidden states. We show these subspaces to be predictive ofparticipants' "Depression" and "Somatic &amp; Emotional Distress" factor scores, aswell as suicidality severity. Overall, LLMs can provide quantitative measuresof mental states. The reliability of these hinges upon how informative thequestions we ask participants are. Used correctly, this approach couldsupplement mental state assessment in a variety of settings.</description><author>Jakub Onysk, Quentin Huys</author><pubDate>Thu, 13 Feb 2025 16:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09487v1</guid></item><item><title>PenTest++: Elevating Ethical Hacking with AI and Automation</title><link>http://arxiv.org/abs/2502.09484v1</link><description>Traditional ethical hacking relies on skilled professionals andtime-intensive command management, which limits its scalability and efficiency.To address these challenges, we introduce PenTest++, an AI-augmented systemthat integrates automation with generative AI (GenAI) to optimise ethicalhacking workflows. Developed in a controlled virtual environment, PenTest++streamlines critical penetration testing tasks, including reconnaissance,scanning, enumeration, exploitation, and documentation, while maintaining amodular and adaptable design. The system balances automation with humanoversight, ensuring informed decision-making at key stages, and offerssignificant benefits such as enhanced efficiency, scalability, andadaptability. However, it also raises ethical considerations, including privacyconcerns and the risks of AI-generated inaccuracies (hallucinations). Thisresearch underscores the potential of AI-driven systems like PenTest++ tocomplement human expertise in cybersecurity by automating routine tasks,enabling professionals to focus on strategic decision-making. By incorporatingrobust ethical safeguards and promoting ongoing refinement, PenTest++demonstrates how AI can be responsibly harnessed to address operational andethical challenges in the evolving cybersecurity landscape.</description><author>Haitham S. Al-Sinani, Chris J. Mitchell</author><pubDate>Thu, 13 Feb 2025 16:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09484v1</guid></item><item><title>Standardisation of Convex Ultrasound Data Through Geometric Analysis and Augmentation</title><link>http://arxiv.org/abs/2502.09482v1</link><description>The application of ultrasound in healthcare has seen increased diversity andimportance. Unlike other medical imaging modalities, ultrasound research anddevelopment has historically lagged, particularly in the case of applicationswith data-driven algorithms. A significant issue with ultrasound is the extremevariability of the images, due to the number of different machines availableand the possible combination of parameter settings. One outcome of this is thelack of standardised and benchmarking ultrasound datasets. The method proposedin this article is an approach to alleviating this issue of disorganisation.For this purpose, the issue of ultrasound data sparsity is examined and a novelperspective, approach, and solution is proposed; involving the extraction ofthe underlying ultrasound plane within the image and representing it usingannulus sector geometry. An application of this methodology is proposed, whichis the extraction of scan lines and the linearisation of convex planes.Validation of the robustness of the proposed method is performed on bothprivate and public data. The impact of deformation and the invertibility ofaugmentation using the estimated annulus sector parameters is also studied.Keywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.</description><author>Alistair Weld, Giovanni Faoro, Luke Dixon, Sophie Camp, Arianna Menciassi, Stamatia Giannarou</author><pubDate>Thu, 13 Feb 2025 16:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09482v1</guid></item><item><title>Mixed-curvature decision trees and random forests</title><link>http://arxiv.org/abs/2410.13879v2</link><description>Decision trees (DTs) and their random forest (RF) extensions are workhorsesof classification and regression in Euclidean spaces. However, algorithms forlearning in non-Euclidean spaces are still limited. We extend DT and RFalgorithms to product manifolds: Cartesian products of several hyperbolic,hyperspherical, or Euclidean components. Such manifolds handle heterogeneouscurvature while still factorizing neatly into simpler components, making themcompelling embedding spaces for complex datasets. Our novel angularreformulation of DTs respects the geometry of the product manifold, yieldingsplits that are geodesically convex, maximum-margin, and composable. In thespecial cases of single-component manifolds, our method simplifies to itsEuclidean or hyperbolic counterparts, or introduces hyperspherical DTalgorithms, depending on the curvature. We benchmark our method on variousclassification, regression, and link prediction tasks on synthetic data, graphembeddings, mixed-curvature variational autoencoder latent spaces, andempirical data. Compared to 7 other classifiers, product RFs ranked first on 25out of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlightsthe value of product RFs as straightforward yet powerful new tools for dataanalysis in product manifolds. Code for our paper is available athttps://github.com/pchlenski/manify.</description><author>Philippe Chlenski, Quentin Chu, Raiyan R. Khan, Kaizhu Du, Antonio Khalil Moretti, Itsik Pe'er</author><pubDate>Thu, 13 Feb 2025 16:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13879v2</guid></item><item><title>Assessing Generative AI value in a public sector context: evidence from a field experiment</title><link>http://arxiv.org/abs/2502.09479v1</link><description>The emergence of Generative AI (Gen AI) has motivated an interest inunderstanding how it could be used to enhance productivity across varioustasks. We add to research results for the performance impact of Gen AI oncomplex knowledge-based tasks in a public sector setting. In a pre-registeredexperiment, after establishing a baseline level of performance, we find mixedevidence for two types of composite tasks related to document understanding anddata analysis. For the Documents task, the treatment group using Gen AI had a17% improvement in answer quality scores (as judged by human evaluators) and a34% improvement in task completion time compared to a control group. For theData task, we find the Gen AI treatment group experienced a 12% reduction inquality scores and no significant difference in mean completion time comparedto the control group. These results suggest that the benefits of Gen AI may betask and potentially respondent dependent. We also discuss field notes andlessons learned, as well as supplementary insights from a post-trial survey andfeedback workshop with participants.</description><author>Trevor Fitzpatrick, Seamus Kelly, Patrick Carey, David Walsh, Ruairi Nugent</author><pubDate>Thu, 13 Feb 2025 16:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09479v1</guid></item><item><title>DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation Networks for Quantitative Nanomaterial Analysis through Differentiable Rendering and Generative Modelling</title><link>http://arxiv.org/abs/2502.09477v1</link><description>Nanomaterials exhibit distinctive properties governed by parameters such assize, shape, and surface characteristics, which critically influence theirapplications and interactions across technological, biological, andenvironmental contexts. Accurate quantification and understanding of thesematerials are essential for advancing research and innovation. In this regard,deep learning segmentation networks have emerged as powerful tools that enableautomated insights and replace subjective methods with precise quantitativeanalysis. However, their efficacy depends on representative annotated datasets,which are challenging to obtain due to the costly imaging of nanoparticles andthe labor-intensive nature of manual annotations. To overcome theselimitations, we introduce DiffRenderGAN, a novel generative model designed toproduce annotated synthetic data. By integrating a differentiable renderer intoa Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizestextural rendering parameters to generate realistic, annotated nanoparticleimages from non-annotated real microscopy images. This approach reduces theneed for manual intervention and enhances segmentation performance compared toexisting synthetic data methods by generating diverse and realistic data.Tested on multiple ion and electron microscopy cases, including titaniumdioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),DiffRenderGAN bridges the gap between synthetic and real data, advancing thequantification and understanding of complex nanomaterial systems.</description><author>Dennis Possart, Leonid Mill, Florian Vollnhals, Tor Hildebrand, Peter Suter, Mathis Hoffmann, Jonas Utz, Daniel Augsburger, Mareike Thies, Mingxuan Wu, Fabian Wagner, George Sarau, Silke Christiansen, Katharina Breininger</author><pubDate>Thu, 13 Feb 2025 16:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09477v1</guid></item><item><title>Conformal Predictive Portfolio Selection</title><link>http://arxiv.org/abs/2410.16333v2</link><description>This study examines portfolio selection using predictive models for portfolioreturns. Portfolio selection is a fundamental task in finance, and a variety ofmethods have been developed to achieve this goal. For instance, themean-variance approach constructs portfolios by balancing the trade-off betweenthe mean and variance of asset returns, while the quantile-based approachoptimizes portfolios by considering tail risk. These methods often depend ondistributional information estimated from historical data using predictivemodels, each of which carries its own uncertainty. To address this, we proposea framework for predictive portfolio selection via conformal prediction ,called \emph{Conformal Predictive Portfolio Selection} (CPPS). Our approachforecasts future portfolio returns, computes the corresponding predictionintervals, and selects the portfolio of interest based on these intervals. Theframework is flexible and can accommodate a wide range of predictive models,including autoregressive (AR) models, random forests, and neural networks. Wedemonstrate the effectiveness of the CPPS framework by applying it to an ARmodel and validate its performance through empirical studies, showing that itdelivers superior returns compared to simpler strategies.</description><author>Masahiro Kato</author><pubDate>Thu, 13 Feb 2025 16:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16333v2</guid></item><item><title>Learning to Predict Global Atrial Fibrillation Dynamics from Sparse Measurements</title><link>http://arxiv.org/abs/2502.09473v1</link><description>Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-alltreatment with limited success in persistent AF. This may be due to ourinability to map the dynamics of AF with the limited resolution and coverageprovided by sequential contact mapping catheters, preventing effective patientphenotyping for personalised, targeted ablation. Here we introduce FibMap, agraph recurrent neural network model that reconstructs global AF dynamics fromsparse measurements. Trained and validated on 51 non-contact whole atriarecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,achieving a 210% lower mean absolute error and an order of magnitude higherperformance in tracking phase singularities compared to baseline methods.Clinical utility of FibMap is demonstrated on real-world contact mappingrecordings, achieving reconstruction fidelity comparable to non-contactmapping. FibMap's state-spaces and patient-specific parameters offer insightsfor electrophenotyping AF. Integrating FibMap into clinical practice couldenable personalised AF care and improve outcomes.</description><author>Alexander Jenkins, Andrea Cini, Joseph Barker, Alexander Sharp, Arunashis Sau, Varun Valentine, Srushti Valasang, Xinyang Li, Tom Wong, Timothy Betts, Danilo Mandic, Cesare Alippi, Fu Siong Ng</author><pubDate>Thu, 13 Feb 2025 16:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09473v1</guid></item><item><title>Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits</title><link>http://arxiv.org/abs/2409.20440v2</link><description>Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regretfor adversarial as well as stochastic bandit problems and allow for astreamlined analysis. Nonetheless, FTRL algorithms require the solution of anoptimization problem in every iteration and are thus computationallychallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achievecomputational efficiency by perturbing the estimates of the rewards of thearms, but their regret analysis is cumbersome. We propose a new FTPL algorithmthat generates optimal policies for both adversarial and stochastic multi-armedbandits. Like FTRL, our algorithm admits a unified regret analysis, and similarto FTPL, it offers low computational costs. Unlike existing FTPL algorithmsthat rely on independent additive disturbances governed by a \textit{known}distribution, we allow for disturbances governed by an \textit{ambiguous}distribution that is only known to belong to a given set and propose aprinciple of optimism in the face of ambiguity. Consequently, our frameworkgeneralizes existing FTPL algorithms. It also encapsulates a broad range ofFTRL methods as special cases, including several optimal ones, which appears tobe impossible with current FTPL methods. Finally, we use techniques fromdiscrete choice theory to devise an efficient bisection algorithm for computingthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ timesfaster than standard FTRL algorithms that solve an optimization problem inevery iteration. Our results not only settle existing conjectures but alsoprovide new insights into the impact of perturbations by mapping FTRL to FTPL.</description><author>Mengmeng Li, Daniel Kuhn, Bahar Taşkesen</author><pubDate>Thu, 13 Feb 2025 16:35:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20440v2</guid></item><item><title>Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection</title><link>http://arxiv.org/abs/2502.09471v1</link><description>Accurately estimating the orientation of visual objects with compact rotatedbounding boxes (RBoxes) has become a prominent demand, which challengesexisting object detection paradigms that only use horizontal bounding boxes(HBoxes). To equip the detectors with orientation awareness, supervisedregression/classification modules have been introduced at the high cost ofrotation annotation. Meanwhile, some existing datasets with oriented objectsare already annotated with horizontal boxes or even single points. It becomesattractive yet remains open for effectively utilizing weaker single point andhorizontal annotations to train an oriented object detector (OOD). We developWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveragingvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in aunified fashion. By only using HBox for training, our Wholly-WOOD achievesperformance very close to that of the RBox-trained counterpart on remotesensing and other areas, significantly reducing the tedious efforts onlabor-intensive annotation for oriented objects. The source codes are availableat https://github.com/VisionXLab/whollywood (PyTorch-based) andhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).</description><author>Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan</author><pubDate>Thu, 13 Feb 2025 16:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09471v1</guid></item><item><title>Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks</title><link>http://arxiv.org/abs/2405.17163v2</link><description>The dynamics of information diffusion within graphs is a critical open issuethat heavily influences graph representation learning, especially whenconsidering long-range propagation. This calls for principled approaches thatcontrol and regulate the degree of propagation and dissipation of informationthroughout the neural flow. Motivated by this, we introduce (port-)HamiltonianDeep Graph Networks, a novel framework that models neural information flow ingraphs by building on the laws of conservation of Hamiltonian dynamicalsystems. We reconcile under a single theoretical and practical framework bothnon-dissipative long-range propagation and non-conservative behaviors,introducing tools from mechanical systems to gauge the equilibrium between thetwo components. Our approach can be applied to general message-passingarchitectures, and it provides theoretical guarantees on informationconservation in time. Empirical results prove the effectiveness of ourport-Hamiltonian scheme in pushing simple graph convolutional architectures tostate-of-the-art performance in long-range benchmarks.</description><author>Simon Heilig, Alessio Gravina, Alessandro Trenta, Claudio Gallicchio, Davide Bacciu</author><pubDate>Thu, 13 Feb 2025 16:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17163v2</guid></item><item><title>ArthroPhase: A Novel Dataset and Method for Phase Recognition in Arthroscopic Video</title><link>http://arxiv.org/abs/2502.07431v2</link><description>This study aims to advance surgical phase recognition in arthroscopicprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, byintroducing the first arthroscopy dataset and developing a noveltransformer-based model. We aim to establish a benchmark for arthroscopicsurgical phase recognition by leveraging spatio-temporal features to addressthe specific challenges of arthroscopic videos including limited field of view,occlusions, and visual distortions. We developed the ACL27 dataset, comprising27 videos of ACL surgeries, each labeled with surgical phases. Our modelemploys a transformer-based architecture, utilizing temporal-aware frame-wisefeature extraction through a ResNet-50 and transformer layers. This approachintegrates spatio-temporal features and introduces a Surgical Progress Index(SPI) to quantify surgery progression. The model's performance was evaluatedusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80datasets. The proposed model achieved an overall accuracy of 72.91% on theACL27 dataset. On the Cholec80 dataset, the model achieved a comparableperformance with the state-of-the-art methods with an accuracy of 92.4%. TheSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80datasets respectively, indicating reliable surgery progression estimation. Thisstudy introduces a significant advancement in surgical phase recognition forarthroscopy, providing a comprehensive dataset and a robust transformer-basedmodel. The results validate the model's effectiveness and generalizability,highlighting its potential to improve surgical training, real-time assistance,and operational efficiency in orthopedic surgery. The publicly availabledataset and code will facilitate future research and development in thiscritical field.</description><author>Ali Bahari Malayeri, Matthias Seibold, Nicola Cavalcanti, Jonas Hein, Sascha Jecklin, Lazaros Vlachopoulos, Sandro Fucentese, Sandro Hodel, Philipp Furnstahl</author><pubDate>Thu, 13 Feb 2025 16:32:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.07431v2</guid></item><item><title>Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation</title><link>http://arxiv.org/abs/2501.14679v3</link><description>Attention-based methods have demonstrated exceptional performance inmodelling long-range dependencies on spherical cortical surfaces, surpassingtraditional Geometric Deep Learning (GDL) models. However, their extensiveinference time and high memory demands pose challenges for application to largedatasets with limited computing resources. Inspired by the state space model incomputer vision, we introduce the attention-free Vision Mamba (Vim) tospherical surfaces, presenting a domain-agnostic architecture for analyzingdata on spherical manifolds. Our method achieves surface patching byrepresenting spherical data as a sequence of triangular patches derived from asubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated onmultiple neurodevelopmental phenotype regression tasks using cortical surfacemetrics from neonatal brains. Experimental results demonstrate that SiMoutperforms both attention- and GDL-based methods, delivering 4.8 times fasterinference and achieving 91.7% lower memory consumption compared to the SurfaceVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivityanalysis further underscores the potential of SiM to identify subtle cognitivedevelopmental patterns. The code is available athttps://github.com/Rongzhao-He/surface-vision-mamba.</description><author>Rongzhao He, Weihao Zheng, Leilei Zhao, Ying Wang, Dalin Zhu, Dan Wu, Bin Hu</author><pubDate>Thu, 13 Feb 2025 16:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.14679v3</guid></item><item><title>Proxy-informed Bayesian transfer learning with unknown sources</title><link>http://arxiv.org/abs/2411.03263v2</link><description>Generalization outside the scope of one's training data requires leveragingprior knowledge about the effects that transfer, and the effects that don't,between different data sources. Transfer learning is a framework for specifyingand refining this knowledge about sets of source (training) and target(prediction) data. A challenging open problem is addressing the empiricalphenomenon of negative transfer, whereby the transfer learner performs worse onthe target data after taking the source data into account than before. We firstintroduce a Bayesian perspective on negative transfer, and then a method toaddress it. The key insight from our formulation is that negative transfer canstem from misspecified prior information about non-transferable causes of thesource data. Our proposed method, proxy-informed robust method forprobabilistic transfer learning (PROMPT), does not require prior knowledge ofthe source data (the data sources may be "unknown"). PROMPT is thus applicablewhen differences between tasks are unobserved, such as in the presence oflatent confounders. Moreover, the learner need not have access to observationsin the target task (cannot "fine-tune"), and instead makes use of proxy(indirect) information. Our theoretical results show that the threat ofnegative transfer does not depend on the informativeness of the proxyinformation, highlighting the usefulness of PROMPT in cases where only noisyindirect information, such as human feedback, is available.</description><author>Sabina J. Sloman, Julien Martinelli, Samuel Kaski</author><pubDate>Thu, 13 Feb 2025 16:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03263v2</guid></item><item><title>Metamorphic Testing for Pose Estimation Systems</title><link>http://arxiv.org/abs/2502.09460v1</link><description>Pose estimation systems are used in a variety of fields, from sportsanalytics to livestock care. Given their potential impact, it is paramount tosystematically test their behaviour and potential for failure. This is acomplex task due to the oracle problem and the high cost of manual labellingnecessary to build ground truth keypoints. This problem is exacerbated by thefact that different applications require systems to focus on different subjects(e.g., human versus animal) or landmarks (e.g., only extremities versus wholebody and face), which makes labelled test data rarely reusable. To combat theseproblems we propose MET-POSE, a metamorphic testing framework for poseestimation systems that bypasses the need for manual annotation while assessingthe performance of these systems under different circumstances. MET-POSE thusallows users of pose estimation systems to assess the systems in conditionsthat more closely relate to their application without having to label an ad-hoctest dataset or rely only on available datasets, which may not be adapted totheir application domain. While we define MET-POSE in general terms, we alsopresent a non-exhaustive list of metamorphic rules that represent commonchallenges in computer vision applications, as well as a specific way toevaluate these rules. We then experimentally show the effectiveness of MET-POSEby applying it to Mediapipe Holistic, a state of the art human pose estimationsystem, with the FLIC and PHOENIX datasets. With these experiments, we outlinenumerous ways in which the outputs of MET-POSE can uncover faults in poseestimation systems at a similar or higher rate than classic testing using handlabelled data, and show that users can tailor the rule set they use to thefaults and level of accuracy relevant to their application.</description><author>Matias Duran, Thomas Laurent, Ellen Rushe, Anthony Ventresque</author><pubDate>Thu, 13 Feb 2025 16:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09460v1</guid></item><item><title>The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models</title><link>http://arxiv.org/abs/2502.09457v1</link><description>While reasoning and multilingual capabilities in Language Models (LMs) haveachieved remarkable progress in recent years, their integration into a unifiedparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoningrequires language models to handle logical reasoning across languages whileaddressing misalignment, biases, and challenges in low-resource settings. Thissurvey provides the first in-depth review of multilingual reasoning in LMs. Inthis survey, we provide a systematic overview of existing methods that leverageLMs for multilingual reasoning, specifically outlining the challenges,motivations, and foundational aspects of applying language models to reasonacross diverse languages. We provide an overview of the standard data resourcesused for training multilingual reasoning in LMs and the evaluation benchmarksemployed to assess their multilingual capabilities. Next, we analyze variousstate-of-the-art methods and their performance on these benchmarks. Finally, weexplore future research opportunities to improve multilingual reasoning in LMs,focusing on enhancing their ability to handle diverse languages and complexreasoning tasks.</description><author>Akash Ghosh, Debayan Datta, Sriparna Saha, Chirag Agarwal</author><pubDate>Thu, 13 Feb 2025 16:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09457v1</guid></item><item><title>Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes</title><link>http://arxiv.org/abs/2410.10790v2</link><description>Recent advancements in human motion synthesis have focused on specific typesof motions, such as human-scene interaction, locomotion or human-humaninteraction, however, there is a lack of a unified system capable of generatinga diverse combination of motion types. In response, we introduceSitcom-Crafter, a comprehensive and extendable system for human motiongeneration in 3D space, which can be guided by extensive plot contexts toenhance workflow efficiency for anime and game designers. The system iscomprised of eight modules, three of which are dedicated to motion generation,while the remaining five are augmentation modules that ensure consistent fusionof motion sequences and system functionality. Central to the generation modulesis our novel 3D scene-aware human-human interaction module, which addressescollision issues by synthesizing implicit 3D Signed Distance Function (SDF)points around motion spaces, thereby minimizing human-scene collisions withoutadditional data collection costs. Complementing this, our locomotion andhuman-scene interaction modules leverage existing methods to enrich thesystem's motion generation capabilities. Augmentation modules encompass plotcomprehension for command generation, motion synchronization for seamlessintegration of different motion types, hand pose retrieval to enhance motionrealism, motion collision revision to prevent human collisions, and 3Dretargeting to ensure visual fidelity. Experimental evaluations validate thesystem's ability to generate high-quality, diverse, and physically realisticmotions, underscoring its potential for advancing creative workflows. Projectpage: https://windvchen.github.io/Sitcom-Crafter.</description><author>Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang</author><pubDate>Thu, 13 Feb 2025 16:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10790v2</guid></item><item><title>4-LEGS: 4D Language Embedded Gaussian Splatting</title><link>http://arxiv.org/abs/2410.10719v3</link><description>The emergence of neural representations has revolutionized our means fordigitally viewing a wide range of 3D scenes, enabling the synthesis ofphotorealistic images rendered from novel views. Recently, several techniqueshave been proposed for connecting these low-level representations with thehigh-level semantics understanding embodied within the scene. These methodselevate the rich semantic understanding from 2D imagery to 3D representations,distilling high-dimensional spatial features onto 3D space. In our work, we areinterested in connecting language with a dynamic modeling of the world. We showhow to lift spatio-temporal features to a 4D representation based on 3DGaussian Splatting. This enables an interactive interface where the user canspatiotemporally localize events in the video from text prompts. We demonstrateour system on public 3D video datasets of people and animals performing variousactions.</description><author>Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor</author><pubDate>Thu, 13 Feb 2025 16:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10719v3</guid></item><item><title>Spiking Neural Networks for Temporal Processing: Status Quo and Future Prospects</title><link>http://arxiv.org/abs/2502.09449v1</link><description>Temporal processing is fundamental for both biological and artificialintelligence systems, as it enables the comprehension of dynamic environmentsand facilitates timely responses. Spiking Neural Networks (SNNs) excel inhandling such data with high efficiency, owing to their rich neuronal dynamicsand sparse activity patterns. Given the recent surge in the development ofSNNs, there is an urgent need for a comprehensive evaluation of their temporalprocessing capabilities. In this paper, we first conduct an in-depth assessmentof commonly used neuromorphic benchmarks, revealing critical limitations intheir ability to evaluate the temporal processing capabilities of SNNs. Tobridge this gap, we further introduce a benchmark suite consisting of threetemporal processing tasks characterized by rich temporal dynamics acrossmultiple timescales. Utilizing this benchmark suite, we perform a thoroughevaluation of recently introduced SNN approaches to elucidate the currentstatus of SNNs in temporal processing. Our findings indicate significantadvancements in recently developed spiking neuron models and neuralarchitectures regarding their temporal processing capabilities, while alsohighlighting a performance gap in handling long-range dependencies whencompared to state-of-the-art non-spiking models. Finally, we discuss the keychallenges and outline potential avenues for future research.</description><author>Chenxiang Ma, Xinyi Chen, Yanchen Li, Qu Yang, Yujie Wu, Guoqi Li, Gang Pan, Huajin Tang, Kay Chen Tan, Jibin Wu</author><pubDate>Thu, 13 Feb 2025 16:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09449v1</guid></item><item><title>Pixel-Level Reasoning Segmentation via Multi-turn Conversations</title><link>http://arxiv.org/abs/2502.09447v1</link><description>Existing visual perception systems focus on region-level segmentation insingle-turn dialogues, relying on complex and explicit query instructions. Suchsystems cannot reason at the pixel level and comprehend dynamic user intentthat changes over interaction. Our work tackles this issue by introducing anovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based onmulti-turn conversations, tracking evolving user intent via multi-turninteractions for fine-grained segmentation. To establish a benchmark for thisnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based onMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3kmulti-turn conversational scenarios with segmentation targets. Building onPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoningSegmentation framework, integrates pixel-level segmentation with robustmulti-turn conversation understanding, generating pixel-grounded explanationsaligned with user intent. The PRIST dataset and MIRSA framework fill the gap inpixel-level reasoning segmentation. Experimental results on the PRIST datasetdemonstrate that our method outperforms current segmentation-specific baselinesin terms of segmentation and LLM-based reasoning metrics. The code and data areavailable at: https://github.com/ccccai239/PixelRIST.</description><author>Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria</author><pubDate>Thu, 13 Feb 2025 16:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09447v1</guid></item><item><title>A Differentiable Rank-Based Objective For Better Feature Learning</title><link>http://arxiv.org/abs/2502.09445v1</link><description>In this paper, we leverage existing statistical methods to better understandfeature learning from data. We tackle this by modifying the model-free variableselection method, Feature Ordering by Conditional Independence (FOCI), which isintroduced in \cite{azadkia2021simple}. While FOCI is based on a non-parametriccoefficient of conditional dependence, we introduce its parametric,differentiable approximation. With this approximate coefficient of correlation,we present a new algorithm called difFOCI, which is applicable to a wider rangeof machine learning problems thanks to its differentiable nature and learnableparameters. We present difFOCI in three contexts: (1) as a variable selectionmethod with baseline comparisons to FOCI, (2) as a trainable model parametrizedwith a neural network, and (3) as a generic, widely applicable neural networkregularizer, one that improves feature learning with better management ofspurious correlations. We evaluate difFOCI on increasingly complex problemsranging from basic variable selection in toy examples to saliency mapcomparisons in convolutional networks. We then show how difFOCI can beincorporated in the context of fairness to facilitate classifications withoutrelying on sensitive data.</description><author>Krunoslav Lehman Pavasovic, David Lopez-Paz, Giulio Biroli, Levent Sagun</author><pubDate>Thu, 13 Feb 2025 16:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09445v1</guid></item><item><title>A Bias-Correction Decentralized Stochastic Gradient Algorithm with Momentum Acceleration</title><link>http://arxiv.org/abs/2501.19082v2</link><description>Distributed stochastic optimization algorithms can simultaneously processlarge-scale datasets, significantly accelerating model training. However, theireffectiveness is often hindered by the sparsity of distributed networks anddata heterogeneity. In this paper, we propose a momentum-accelerateddistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum(EDM), which mitigates the bias from data heterogeneity and incorporatesmomentum techniques commonly used in deep learning to enhance convergence rate.Our theoretical analysis demonstrates that the EDM algorithm convergessub-linearly to the neighborhood of the optimal solution, the radius of whichis irrespective of data heterogeneity, when applied to non-convex objectivefunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumptionthan strong convexity, it converges linearly to the target region. Our analysistechniques employed to handle momentum in complex distributed parameter updatestructures yield a sufficiently tight convergence upper bound, offering a newperspective for the theoretical analysis of other momentum-based distributedalgorithms.</description><author>Yuchen Hu, Xi Chen, Weidong Liu, Xiaojun Mao</author><pubDate>Thu, 13 Feb 2025 16:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19082v2</guid></item><item><title>Relational Conformal Prediction for Correlated Time Series</title><link>http://arxiv.org/abs/2502.09443v1</link><description>We address the problem of uncertainty quantification in time seriesforecasting by exploiting observations at correlated sequences. Relational deeplearning methods leveraging graph representations are among the most effectivetools for obtaining point estimates from spatiotemporal data and correlatedtime series. However, the problem of exploiting relational structures toestimate the uncertainty of such predictions has been largely overlooked in thesame context. To this end, we propose a novel distribution-free approach basedon the conformal prediction framework and quantile regression. Despite therecent applications of conformal prediction to sequential data, existingmethods operate independently on each target time series and do not account forrelationships among them when constructing the prediction interval. We fillthis void by introducing a novel conformal prediction method based on graphdeep learning operators. Our method, named Conformal Relational Prediction(CoRel), does not require the relational structure (graph) to be known as aprior and can be applied on top of any pre-trained time series predictor.Additionally, CoRel includes an adaptive component to handle non-exchangeabledata and changes in the input time series. Our approach provides accuratecoverage and archives state-of-the-art uncertainty quantification in relevantbenchmarks.</description><author>Andrea Cini, Alexander Jenkins, Danilo Mandic, Cesare Alippi, Filippo Maria Bianchi</author><pubDate>Thu, 13 Feb 2025 16:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09443v1</guid></item><item><title>On the Importance of Backbone to the Adversarial Robustness of Object Detectors</title><link>http://arxiv.org/abs/2305.17438v2</link><description>Object detection is a critical component of various security-sensitiveapplications, such as autonomous driving and video surveillance. However,existing object detectors are vulnerable to adversarial attacks, which poses asignificant challenge to their reliability and security. Through experiments,first, we found that existing works on improving the adversarial robustness ofobject detectors give a false sense of security. Second, we found thatadversarially pre-trained backbone networks were essential for enhancing theadversarial robustness of object detectors. We then proposed a simple yeteffective recipe for fast adversarial fine-tuning on object detectors withadversarially pre-trained backbones. Without any modifications to the structureof object detectors, our recipe achieved significantly better adversarialrobustness than previous works. Finally, we explored the potential of differentmodern object detector designs for improving adversarial robustness with ourrecipe and demonstrated interesting findings, which inspired us to designstate-of-the-art (SOTA) robust detectors. Our empirical results set a newmilestone for adversarially robust object detection. Code and trainedcheckpoints are available at https://github.com/thu-ml/oddefense.</description><author>Xiao Li, Hang Chen, Xiaolin Hu</author><pubDate>Thu, 13 Feb 2025 16:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17438v2</guid></item><item><title>Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection</title><link>http://arxiv.org/abs/2410.01404v2</link><description>Skins wrapping around our bodies, leathers covering over the sofa, sheetmetal coating the car - it suggests that objects are enclosed by a series ofcontinuous surfaces, which provides us with informative geometry prior forobjectness deduction. In this paper, we propose Gaussian-Det which leveragesGaussian Splatting as surface representation for multi-view based 3D objectdetection. Unlike existing monocular or NeRF-based methods which depict theobjects via discrete positional data, Gaussian-Det models the objects in acontinuous manner by formulating the input Gaussians as feature descriptors ona mass of partial surfaces. Furthermore, to address the numerous outliersinherently introduced by Gaussian splatting, we accordingly devise a ClosureInferring Module (CIM) for the comprehensive surface-based objectnessdeduction. CIM firstly estimates the probabilistic feature residuals forpartial surfaces given the underdetermined nature of Gaussian Splatting, whichare then coalesced into a holistic representation on the overall surfaceclosure of the object proposal. In this way, the surface informationGaussian-Det exploits serves as the prior on the quality and reliability ofobjectness and the information basis of proposal refinement. Experiments onboth synthetic and real-world datasets demonstrate that Gaussian-Detoutperforms various existing approaches, in terms of both average precision andrecall.</description><author>Hongru Yan, Yu Zheng, Yueqi Duan</author><pubDate>Thu, 13 Feb 2025 16:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01404v2</guid></item><item><title>KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI</title><link>http://arxiv.org/abs/2410.11415v2</link><description>A popular approach to neurosymbolic AI involves mapping logic formulas toarithmetic circuits (computation graphs consisting of sums and products) andpassing the outputs of a neural network through these circuits. This approachenforces symbolic constraints onto a neural network in a principled andend-to-end differentiable way. Unfortunately, arithmetic circuits arechallenging to run on modern AI accelerators as they exhibit a high degree ofirregular sparsity. To address this limitation, we introduce knowledge layers(KLay), a new data structure to represent arithmetic circuits that can beefficiently parallelized on GPUs. Moreover, we contribute two algorithms usedin the translation of traditional circuit representations to KLay and a furtheralgorithm that exploits parallelization opportunities during circuitevaluations. We empirically show that KLay achieves speedups of multiple ordersof magnitude over the state of the art, thereby paving the way towards scalingneurosymbolic AI to larger real-world applications.</description><author>Jaron Maene, Vincent Derkinderen, Pedro Zuidberg Dos Martires</author><pubDate>Thu, 13 Feb 2025 16:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11415v2</guid></item><item><title>Variable Stiffness for Robust Locomotion through Reinforcement Learning</title><link>http://arxiv.org/abs/2502.09436v1</link><description>Reinforcement-learned locomotion enables legged robots to perform highlydynamic motions but often accompanies time-consuming manual tuning of jointstiffness. This paper introduces a novel control paradigm that integratesvariable stiffness into the action space alongside joint positions, enablinggrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffnesspolicies, with grouping in per-leg stiffness (PLS), outperform position-basedcontrol in velocity tracking and push recovery. In contrast, HJLS excels inenergy efficiency. Furthermore, our method showcases robust walking behaviouron diverse outdoor terrains by sim-to-real transfer, although the policy issorely trained on a flat floor. Our approach simplifies design by eliminatingper-joint stiffness tuning while keeping competitive results with variousmetrics.</description><author>Dario Spoljaric, Yashuai Yan, Dongheui Lee</author><pubDate>Thu, 13 Feb 2025 16:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09436v1</guid></item></channel></rss>