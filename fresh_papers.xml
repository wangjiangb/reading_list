<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Sep 2023 06:00:44 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Extreme Parkour with Legged Robots</title><link>http://arxiv.org/abs/2309.14341v1</link><description>Humans can perform parkour by traversing obstacles in a highly dynamicfashion requiring precise eye-muscle coordination and movement. Getting robotsto do the same task requires overcoming similar challenges. Classically, thisis done by independently engineering perception, actuation, and control systemsto very low tolerances. This restricts them to tightly controlled settings suchas a predetermined obstacle course in labs. In contrast, humans are able tolearn parkour through practice without significantly changing their underlyingbiology. In this paper, we take a similar approach to developing robot parkouron a small low-cost robot with imprecise actuation and a single front-facingdepth camera for perception which is low-frequency, jittery, and prone toartifacts. We show how a single neural net policy operating directly from acamera image, trained in simulation with large-scale RL, can overcome imprecisesensing and actuation to output highly precise control behavior end-to-end. Weshow our robot can perform a high jump on obstacles 2x its height, long jumpacross gaps 2x its length, do a handstand and run across tilted ramps, andgeneralize to novel obstacle courses with different physical properties.Parkour videos at https://extreme-parkour.github.io/</description><author>Xuxin Cheng, Kexin Shi, Ananye Agarwal, Deepak Pathak</author><pubDate>Mon, 25 Sep 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14341v1</guid></item><item><title>Chop &amp; Learn: Recognizing and Generating Object-State Compositions</title><link>http://arxiv.org/abs/2309.14339v1</link><description>Recognizing and generating object-state compositions has been a challengingtask, especially when generalizing to unseen compositions. In this paper, westudy the task of cutting objects in different styles and the resulting objectstate changes. We propose a new benchmark suite Chop &amp; Learn, to accommodatethe needs of learning objects and different cut styles using multipleviewpoints. We also propose a new task of Compositional Image Generation, whichcan transfer learned cut styles to different objects, by generating novelobject-state images. Moreover, we also use the videos for Compositional ActionRecognition, and show valuable uses of this dataset for multiple video tasks.Project website: https://chopnlearn.github.io.</description><author>Nirat Saini, Hanyu Wang, Archana Swaminathan, Vinoj Jayasundara, Bo He, Kamal Gupta, Abhinav Shrivastava</author><pubDate>Mon, 25 Sep 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14339v1</guid></item><item><title>3D Indoor Instance Segmentation in an Open-World</title><link>http://arxiv.org/abs/2309.14338v1</link><description>Existing 3D instance segmentation methods typically assume that all semanticclasses to be segmented would be available during training and only seencategories are segmented at inference. We argue that such a closed-worldassumption is restrictive and explore for the first time 3D indoor instancesegmentation in an open-world setting, where the model is allowed todistinguish a set of known classes as well as identify an unknown object asunknown and then later incrementally learning the semantic category of theunknown when the corresponding category labels are available. To this end, weintroduce an open-world 3D indoor instance segmentation method, where anauto-labeling scheme is employed to produce pseudo-labels during training andinduce separation to separate known and unknown category labels. We furtherimprove the pseudo-labels quality at inference by adjusting the unknown classprobability based on the objectness score distribution. We also introducecarefully curated open-world splits leveraging realistic scenarios based oninherent object distribution, region-based indoor scene exploration andrandomness aspect of open-world classes. Extensive experiments reveal theefficacy of the proposed contributions leading to promising open-world 3Dinstance segmentation performance.</description><author>Mohamed El Amine Boudjoghra, Salwa K. Al Khatib, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</author><pubDate>Mon, 25 Sep 2023 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14338v1</guid></item><item><title>UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation</title><link>http://arxiv.org/abs/2309.14335v1</link><description>Human generation has achieved significant progress. Nonetheless, existingmethods still struggle to synthesize specific regions such as faces and hands.We argue that the main reason is rooted in the training data. A holistic humandataset inevitably has insufficient and low-resolution information on localparts. Therefore, we propose to use multi-source datasets with variousresolution images to jointly learn a high-resolution human generative model.However, multi-source data inherently a) contains different parts that do notspatially align into a coherent human, and b) comes with different scales. Totackle these challenges, we propose an end-to-end framework, UnitedHuman, thatempowers continuous GAN with the ability to effectively utilize multi-sourcedata for high-resolution human generation. Specifically, 1) we design aMulti-Source Spatial Transformer that spatially aligns multi-source images tofull-body space with a human parametric model. 2) Next, a continuous GAN isproposed with global-structural guidance and CutMix consistency. Patches fromdifferent datasets are then sampled and transformed to supervise the trainingof this scale-invariant generative model. Extensive experiments demonstratethat our model jointly learned from multi-source data achieves superior qualitythan those learned from a holistic dataset.</description><author>Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Wayne Wu, Ziwei Liu</author><pubDate>Mon, 25 Sep 2023 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14335v1</guid></item><item><title>Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points</title><link>http://arxiv.org/abs/2309.14334v1</link><description>We present a machine learning (ML)-assisted framework bridging manifoldlearning, neural networks, Gaussian processes, and Equation-Free multiscalemodeling, for (a) detecting tipping points in the emergent behavior of complexsystems, and (b) characterizing probabilities of rare events (here,catastrophic shifts) near them. Our illustrative example is an event-driven,stochastic agent-based model (ABM) describing the mimetic behavior of tradersin a simple financial market. Given high-dimensional spatiotemporal data --generated by the stochastic ABM -- we construct reduced-order models for theemergent dynamics at different scales: (a) mesoscopic Integro-PartialDifferential Equations (IPDEs); and (b) mean-field-type Stochastic DifferentialEquations (SDEs) embedded in a low-dimensional latent space, targeted to theneighborhood of the tipping point. We contrast the uses of the different modelsand the effort involved in learning them.</description><author>Gianluca Fabiani, Nikolaos Evangelou, Tianqi Cui, Juan M. Bello-Rivas, Cristina P. Martin-Linares, Constantinos Siettos, Ioannis G. Kevrekidis</author><pubDate>Mon, 25 Sep 2023 18:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14334v1</guid></item><item><title>LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference</title><link>http://arxiv.org/abs/2309.14331v1</link><description>The growth of Graph Convolution Network (GCN) model sizes has revolutionizednumerous applications, surpassing human performance in areas such as personalhealthcare and financial systems. The deployment of GCNs in the cloud raisesprivacy concerns due to potential adversarial attacks on client data. Toaddress security concerns, Privacy-Preserving Machine Learning (PPML) usingHomomorphic Encryption (HE) secures sensitive client data. However, itintroduces substantial computational overhead in practical applications. Totackle those challenges, we present LinGCN, a framework designed to reducemultiplication depth and optimize the performance of HE based GCN inference.LinGCN is structured around three key elements: (1) A differentiable structurallinearization algorithm, complemented by a parameterized discrete indicatorfunction, co-trained with model weights to meet the optimization goal. Thisstrategy promotes fine-grained node-level non-linear location selection,resulting in a model with minimized multiplication depth. (2) A compactnode-wise polynomial replacement policy with a second-order trainableactivation function, steered towards superior convergence by a two-leveldistillation approach from an all-ReLU based teacher model. (3) an enhanced HEsolution that enables finer-grained operator fusion for node-wise activationfunctions, further reducing multiplication level consumption in HE-basedinference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal thatLinGCN excels in latency, accuracy, and scalability for homomorphicallyencrypted inference, outperforming solutions such as CryptoGCN. Remarkably,LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preservingan inference accuracy of 75% and notably reducing multiplication depth.</description><author>Hongwu Peng, Ran Ran, Yukui Luo, Jiahui Zhao, Shaoyi Huang, Kiran Thorat, Tong Geng, Chenghong Wang, Xiaolin Xu, Wujie Wen, Caiwen Ding</author><pubDate>Mon, 25 Sep 2023 18:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14331v1</guid></item><item><title>Noise-in, Bias-out: Balanced and Real-time MoCap Solving</title><link>http://arxiv.org/abs/2309.14330v1</link><description>Real-time optical Motion Capture (MoCap) systems have not benefited from theadvances in modern data-driven modeling. In this work we apply machine learningto solve noisy unstructured marker estimates in real-time and deliver robustmarker-based MoCap even when using sparse affordable sensors. To achieve thiswe focus on a number of challenges related to model training, namely thesourcing of training data and their long-tailed distribution. Leveragingrepresentation learning we design a technique for imbalanced regression thatrequires no additional data or labels and improves the performance of our modelin rare and challenging poses. By relying on a unified representation, we showthat training such a model is not bound to high-end MoCap training dataacquisition, and exploit the advances in marker-less MoCap to acquire thenecessary data. Finally, we take a step towards richer and affordable MoCap byadapting a body model-based inverse kinematics solution to account formeasurement and inference uncertainty, further improving performance androbustness. Project page: https://moverseai.github.io/noise-tail</description><author>Georgios Albanis, Nikolaos Zioulis, Spyridon Thermos, Anargyros Chatzitofis, Kostas Kolomvatsos</author><pubDate>Mon, 25 Sep 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14330v1</guid></item><item><title>Learning to Drive Anywhere</title><link>http://arxiv.org/abs/2309.12295v2</link><description>Human drivers can seamlessly adapt their driving decisions acrossgeographical locations with diverse conditions and rules of the road, e.g.,left vs. right-hand traffic. In contrast, existing models for autonomousdriving have been thus far only deployed within restricted operational domains,i.e., without accounting for varying driving behaviors across locations ormodel scalability. In this work, we propose AnyD, a single geographically-awareconditional imitation learning (CIL) model that can efficiently learn fromheterogeneous and globally distributed data with dynamic environmental,traffic, and social characteristics. Our key insight is to introduce ahigh-capacity geo-location-based channel attention mechanism that effectivelyadapts to local nuances while also flexibly modeling similarities among regionsin a data-driven manner. By optimizing a contrastive imitation objective, ourproposed approach can efficiently scale across inherently imbalanced datadistributions and location-dependent events. We demonstrate the benefits of ourAnyD agent across multiple datasets, cities, and scalable deployment paradigms,i.e., centralized, semi-supervised, and distributed agent training.Specifically, AnyD outperforms CIL baselines by over 14% in open-loopevaluation and 30% in closed-loop testing on CARLA.</description><author>Ruizhao Zhu, Peng Huang, Eshed Ohn-Bar, Venkatesh Saligrama</author><pubDate>Mon, 25 Sep 2023 18:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12295v2</guid></item><item><title>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention</title><link>http://arxiv.org/abs/2309.14327v1</link><description>Most of the existing multi-modal models, hindered by their incapacity toadeptly manage interleaved image-and-text inputs in multi-image, multi-rounddialogues, face substantial constraints in resource allocation for training anddata accessibility, impacting their adaptability and scalability across variedinteraction realms. To address this, we present the DeepSpeed-VisualChatframework, designed to optimize Large Language Models (LLMs) by incorporatingmulti-modal capabilities, with a focus on enhancing the proficiency of LargeVision and Language Models in handling interleaved inputs. Our framework isnotable for (1) its open-source support for multi-round and multi-imagedialogues, (2) introducing an innovative multi-modal causal attentionmechanism, and (3) utilizing data blending techniques on existing datasets toassure seamless interactions in multi-round, multi-image conversations.Compared to existing frameworks, DeepSpeed-VisualChat shows superiorscalability up to 70B parameter language model size, representing a significantadvancement in multi-modal language models and setting a solid foundation forfuture explorations.</description><author>Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qi, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He</author><pubDate>Mon, 25 Sep 2023 18:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14327v1</guid></item><item><title>Futility and utility of a few ancillas for Pauli channel learning</title><link>http://arxiv.org/abs/2309.14326v1</link><description>In this paper we revisit one of the prototypical tasks for characterizing thestructure of noise in quantum devices, estimating the eigenvalues of an$n$-qubit Pauli noise channel. Prior work (Chen et al., 2022) establishedexponential lower bounds for this task for algorithms with limited quantummemory. We first improve upon their lower bounds and show: (1) Any algorithm without quantum memory must make $\Omega(2^n/\epsilon^2)$measurements to estimate each eigenvalue within error $\epsilon$. This is tightand implies the randomized benchmarking protocol is optimal, resolving an openquestion of (Flammia and Wallman, 2020). (2) Any algorithm with $\le k$ ancilla qubits of quantum memory must make$\Omega(2^{(n-k)/3})$ queries to the unknown channel. Crucially, unlike in(Chen et al., 2022), our bound holds even if arbitrary adaptive control andchannel concatenation are allowed. In fact these lower bounds, like those of (Chen et al., 2022), hold even forthe easier hypothesis testing problem of determining whether the underlyingchannel is completely depolarizing or has exactly one other nontrivialeigenvalue. Surprisingly, we show that: (3) With only $k=2$ ancilla qubits of quantum memory, there is an algorithmthat solves this hypothesis testing task with high probability using a singlemeasurement. Note that (3) does not contradict (2) as the protocol concatenatesexponentially many queries to the channel before the measurement. This resultsuggests a novel mechanism by which channel concatenation and $O(1)$ qubits ofquantum memory could work in tandem to yield striking speedups for quantumprocess learning that are not possible for quantum state learning.</description><author>Sitan Chen, Weiyuan Gong</author><pubDate>Mon, 25 Sep 2023 18:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14326v1</guid></item><item><title>Towards General-Purpose Text-Instruction-Guided Voice Conversion</title><link>http://arxiv.org/abs/2309.14324v1</link><description>This paper introduces a novel voice conversion (VC) model, guided by textinstructions such as "articulate slowly with a deep tone" or "speak in acheerful boyish voice". Unlike traditional methods that rely on referenceutterances to determine the attributes of the converted speech, our model addsversatility and specificity to voice conversion. The proposed VC model is aneural codec language model which processes a sequence of discrete codes,resulting in the code sequence of converted speech. It utilizes textinstructions as style prompts to modify the prosody and emotional informationof the given speech. In contrast to previous approaches, which often rely onemploying separate encoders like prosody and content encoders to handledifferent aspects of the source speech, our model handles various informationof speech in an end-to-end manner. Experiments have demonstrated the impressivecapabilities of our model in comprehending instructions and deliveringreasonable results.</description><author>Chun-Yi Kuan, Chen An Li, Tsu-Yuan Hsu, Tse-Yang Lin, Ho-Lam Chung, Kai-Wei Chang, Shuo-yiin Chang, Hung-yi Lee</author><pubDate>Mon, 25 Sep 2023 18:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14324v1</guid></item><item><title>Small-scale proxies for large-scale Transformer training instabilities</title><link>http://arxiv.org/abs/2309.14322v1</link><description>Teams that have trained large Transformer-based models have reported traininginstabilities at large scale that did not appear when training with the samehyperparameters at smaller scales. Although the causes of such instabilitiesare of scientific interest, the amount of resources required to reproduce themhas made investigation difficult. In this work, we seek ways to reproduce andstudy training stability and instability at smaller scales. First, we focus ontwo sources of training instability described in previous work: the growth oflogits in attention layers (Dehghani et al., 2023) and divergence of the outputlogits from the log probabilities (Chowdhery et al., 2022). By measuring therelationship between learning rate and loss across scales, we show that theseinstabilities also appear in small models when training at high learning rates,and that mitigations previously employed at large scales are equally effectivein this regime. This prompts us to investigate the extent to which other knownoptimizer and model interventions influence the sensitivity of the final lossto changes in the learning rate. To this end, we study methods such as warm-up,weight decay, and the $\mu$Param (Yang et al., 2022), and combine techniques totrain small models that achieve similar losses across orders of magnitude oflearning rate variation. Finally, to conclude our exploration we study twocases where instabilities can be predicted before they emerge by examining thescaling behavior of model activation and gradient norms.</description><author>Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, Simon Kornblith</author><pubDate>Mon, 25 Sep 2023 18:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14322v1</guid></item><item><title>Human-Assisted Continual Robot Learning with Foundation Models</title><link>http://arxiv.org/abs/2309.14321v1</link><description>Large Language Models (LLMs) have been shown to act like planners that candecompose high-level instructions into a sequence of executable instructions.However, current LLM-based planners are only able to operate with a fixed setof skills. We overcome this critical limitation and present a method for usingLLM-based planners to query new skills and teach robots these skills in a dataand time-efficient manner for rigid object manipulation. Our system can re-usenewly acquired skills for future tasks, demonstrating the potential of openworld and lifelong learning. We evaluate the proposed framework on multipletasks in simulation and the real world. Videos are available at:https://sites.google.com/mit.edu/halp-robot-learning.</description><author>Meenal Parakh, Alisha Fong, Anthony Simeonov, Abhishek Gupta, Tao Chen, Pulkit Agrawal</author><pubDate>Mon, 25 Sep 2023 18:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14321v1</guid></item><item><title>GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children</title><link>http://arxiv.org/abs/2305.16809v3</link><description>When caregivers ask open--ended questions to motivate dialogue with children,it facilitates the child's reading comprehension skills.Although there is scopefor use of technological tools, referred here as "intelligent tutoringsystems", to scaffold this process, it is currently unclear whether existingintelligent systems that generate human--language like questions is beneficial.Additionally, training data used in the development of these automated questiongeneration systems is typically sourced without attention to demographics, butpeople with different cultural backgrounds may ask different questions. As apart of a broader project to design an intelligent reading support app forLatinx children, we crowdsourced questions from Latinx caregivers andnoncaregivers as well as caregivers and noncaregivers from other demographics.We examine variations in question--asking within this dataset mediated byindividual, cultural, and contextual factors. We then design a system thatautomatically extracts templates from this data to generate open--endedquestions that are representative of those asked by Latinx caregivers.</description><author>Arun Balajiee Lekshmi Narayanan, Ligia E. Gomez, Martha Michelle Soto Fernandez, Tri Nguyen, Chris Blais, M. Adelaida Restrepo, Art Glenberg</author><pubDate>Mon, 25 Sep 2023 18:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16809v3</guid></item><item><title>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction</title><link>http://arxiv.org/abs/2309.14316v1</link><description>Large language models can store extensive world knowledge, often extractablethrough question-answering (e.g., "What is Abraham Lincoln's birthday?").However, it's unclear whether the model answers questions based on exposure toexact/similar questions during training, or if it genuinely extracts knowledgefrom the source (e.g., Wikipedia biographies). In this paper, we conduct an in-depth study of this problem using acontrolled set of semi-synthetic biography data. We uncover a relationshipbetween the model's knowledge extraction ability and different diversitymeasures of the training data. We conduct (nearly) linear probing, revealing astrong correlation between this relationship and whether the model (nearly)linearly encodes the knowledge attributes at the hidden embedding of the entitynames, or across the embeddings of other tokens in the training text.</description><author>Zeyuan Allen Zhu, Yuanzhi Li</author><pubDate>Mon, 25 Sep 2023 18:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14316v1</guid></item><item><title>PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs</title><link>http://arxiv.org/abs/2306.01913v3</link><description>Pre-training on large models is prevalent and emerging with the ever-growinguser-generated content in many machine learning application categories. It hasbeen recognized that learning contextual knowledge from the datasets depictinguser-content interaction plays a vital role in downstream tasks. Despiteseveral studies attempting to learn contextual knowledge via pre-trainingmethods, finding an optimal training objective and strategy for this type oftask remains a challenging problem. In this work, we contend that there are twodistinct aspects of contextual knowledge, namely the user-side and thecontent-side, for datasets where user-content interaction can be represented asa bipartite graph. To learn contextual knowledge, we propose a pre-trainingmethod that learns a bi-directional mapping between the spaces of the user-sideand the content-side. We formulate the training goal as a contrastive learningtask and propose a dual-Transformer architecture to encode the contextualknowledge. We evaluate the proposed method for the recommendation task. Theempirical studies have demonstrated that the proposed method outperformed allthe baselines with significant gains.</description><author>Xin Dai, Yujie Fan, Zhongfang Zhuang, Shubham Jain, Chin-Chia Michael Yeh, Junpeng Wang, Liang Wang, Yan Zheng, Prince Osei Aboagye, Wei Zhang</author><pubDate>Mon, 25 Sep 2023 18:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01913v3</guid></item><item><title>Multiple Different Explanations for Image Classifiers</title><link>http://arxiv.org/abs/2309.14309v1</link><description>Existing explanation tools for image classifiers usually give only one singleexplanation for an image. For many images, however, both humans and imageclassifiers accept more than one explanation for the image label. Thus,restricting the number of explanations to just one severely limits the insightinto the behavior of the classifier. In this paper, we describe an algorithmand a tool, REX, for computing multiple explanations of the output of ablack-box image classifier for a given image. Our algorithm uses a principledapproach based on causal theory. We analyse its theoretical complexity andprovide experimental results showing that REX finds multiple explanations on 7times more images than the previous work on the ImageNet-mini benchmark.</description><author>Hana Chockler, David A. Kelly, Daniel Kroening</author><pubDate>Mon, 25 Sep 2023 18:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14309v1</guid></item><item><title>A post-selection algorithm for improving dynamic ensemble selection methods</title><link>http://arxiv.org/abs/2309.14307v1</link><description>Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS)approach that aims to select an ensemble for each query sample during theselection phase. Even with the proposal of several DES approaches, noparticular DES technique is the best choice for different problems. Thus, wehypothesize that selecting the best DES approach per query instance can lead tobetter accuracy. To evaluate this idea, we introduce the Post-Selection DynamicEnsemble Selection (PS-DES) approach, a post-selection scheme that evaluatesensembles selected by several DES techniques using different metrics.Experimental results show that using accuracy as a metric to select theensembles, PS-DES performs better than individual DES techniques. PS-DES sourcecode is available in a GitHub repository</description><author>Paulo R. G. Cordeiro, George D. C. Cavalcanti, Rafael M. O. Cruz</author><pubDate>Mon, 25 Sep 2023 18:25:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14307v1</guid></item><item><title>DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning</title><link>http://arxiv.org/abs/2309.14306v1</link><description>3D motion estimation from cine cardiac magnetic resonance (CMR) images isimportant for the assessment of cardiac function and the diagnosis ofcardiovascular diseases. Current state-of-the art methods focus on estimatingdense pixel-/voxel-wise motion fields in image space, which ignores the factthat motion estimation is only relevant and useful within the anatomicalobjects of interest, e.g., the heart. In this work, we model the heart as a 3Dmesh consisting of epi- and endocardial surfaces. We propose a novel learningframework, DeepMesh, which propagates a template heart mesh to a subject spaceand estimates the 3D motion of the heart mesh from CMR images for individualsubjects. In DeepMesh, the heart mesh of the end-diastolic frame of anindividual subject is first reconstructed from the template mesh. Mesh-based 3Dmotion fields with respect to the end-diastolic frame are then estimated from2D short- and long-axis CMR images. By developing a differentiablemesh-to-image rasterizer, DeepMesh is able to leverage 2D shape informationfrom multiple anatomical views for 3D mesh reconstruction and mesh motionestimation. The proposed method estimates vertex-wise displacement and thusmaintains vertex correspondences between time frames, which is important forthe quantitative assessment of cardiac function across different subjects andpopulations. We evaluate DeepMesh on CMR images acquired from the UK Biobank.We focus on 3D motion estimation of the left ventricle in this work.Experimental results show that the proposed method quantitatively andqualitatively outperforms other image-based and mesh-based cardiac motiontracking methods.</description><author>Qingjie Meng, Wenjia Bai, Declan P O'Regan, and Daniel Rueckert</author><pubDate>Mon, 25 Sep 2023 18:24:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14306v1</guid></item><item><title>Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training</title><link>http://arxiv.org/abs/2302.14007v3</link><description>Masked Autoencoders (MAE) have shown promising performance in self-supervisedlearning for both 2D and 3D computer vision. However, existing MAE-stylemethods can only learn from the data of a single modality, i.e., either imagesor point clouds, which neglect the implicit semantic and geometric correlationbetween 2D and 3D. In this paper, we explore how the 2D modality can benefit 3Dmasked autoencoding, and propose Joint-MAE, a 2D-3D joint MAE framework forself-supervised 3D point cloud pre-training. Joint-MAE randomly masks an input3D point cloud and its projected 2D images, and then reconstructs the maskedinformation of the two modalities. For better cross-modal interaction, weconstruct our JointMAE by two hierarchical 2D-3D embedding modules, a jointencoder, and a joint decoder with modal-shared and model-specific decoders. Ontop of this, we further introduce two cross-modal strategies to boost the 3Drepresentation learning, which are local-aligned attention mechanisms for 2D-3Dsemantic cues, and a cross-reconstruction loss for 2D-3D geometric constraints.By our pre-training paradigm, Joint-MAE achieves superior performance onmultiple downstream tasks, e.g., 92.4% accuracy for linear SVM on ModelNet40and 86.07% accuracy on the hardest split of ScanObjectNN.</description><author>Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzhi Li, Pheng-Ann Heng</author><pubDate>Mon, 25 Sep 2023 18:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14007v3</guid></item><item><title>REPAIR: REnormalizing Permuted Activations for Interpolation Repair</title><link>http://arxiv.org/abs/2211.08403v3</link><description>In this paper we look into the conjecture of Entezari et al. (2021) whichstates that if the permutation invariance of neural networks is taken intoaccount, then there is likely no loss barrier to the linear interpolationbetween SGD solutions. First, we observe that neuron alignment methods aloneare insufficient to establish low-barrier linear connectivity between SGDsolutions due to a phenomenon we call variance collapse: interpolated deepnetworks suffer a collapse in the variance of their activations, causing poorperformance. Next, we propose REPAIR (REnormalizing Permuted Activations forInterpolation Repair) which mitigates variance collapse by rescaling thepreactivations of such interpolated networks. We explore the interactionbetween our method and the choice of normalization layer, network width, anddepth, and demonstrate that using REPAIR on top of neuron alignment methodsleads to 60%-100% relative barrier reduction across a wide variety ofarchitecture families and tasks. In particular, we report a 74% barrierreduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 onCIFAR10.</description><author>Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, Behnam Neyshabur</author><pubDate>Mon, 25 Sep 2023 18:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08403v3</guid></item><item><title>Overview of Class Activation Maps for Visualization Explainability</title><link>http://arxiv.org/abs/2309.14304v1</link><description>Recent research in deep learning methodology has led to a variety of complexmodelling techniques in computer vision (CV) that reach or even outperformhuman performance. Although these black-box deep learning models have obtainedastounding results, they are limited in their interpretability and transparencywhich are critical to take learning machines to the next step to include themin sensitive decision-support systems involving human supervision. Hence, thedevelopment of explainable techniques for computer vision (XCV) has recentlyattracted increasing attention. In the realm of XCV, Class Activation Maps(CAMs) have become widely recognized and utilized for enhancinginterpretability and insights into the decision-making process of deep learningmodels. This work presents a comprehensive overview of the evolution of ClassActivation Map methods over time. It also explores the metrics used forevaluating CAMs and introduces auxiliary techniques to improve the saliency ofthese methods. The overview concludes by proposing potential avenues for futureresearch in this evolving field.</description><author>Anh Pham Thi Minh</author><pubDate>Mon, 25 Sep 2023 18:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14304v1</guid></item><item><title>Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation</title><link>http://arxiv.org/abs/2309.14303v1</link><description>Preparing training data for deep vision models is a labor-intensive task. Toaddress this, generative models have emerged as an effective solution forgenerating synthetic data. While current generative models produce image-levelcategory labels, we propose a novel method for generating pixel-level semanticsegmentation labels using the text-to-image generative model Stable Diffusion(SD). By utilizing the text prompts, cross-attention, and self-attention of SD,we introduce three new techniques: \textit{class-prompt appending},\textit{class-prompt cross-attention}, and \textit{self-attentionexponentiation}. These techniques enable us to generate segmentation mapscorresponding to synthetic images. These maps serve as pseudo-labels fortraining semantic segmenters, eliminating the need for labor-intensivepixel-wise annotation. To account for the imperfections in our pseudo-labels,we incorporate uncertainty regions into the segmentation, allowing us todisregard loss from those regions. We conduct evaluations on two datasets,PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrentwork. Our benchmarks and code will be released athttps://github.com/VinAIResearch/Dataset-Diffusion</description><author>Quang Nguyen, Truong Vu, Anh Tran, Khoi Nguyen</author><pubDate>Mon, 25 Sep 2023 18:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14303v1</guid></item><item><title>Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures</title><link>http://arxiv.org/abs/2309.14298v1</link><description>We present improved algorithms with worst-case regret guarantees for thestochastic linear bandit problem. The widely used "optimism in the face ofuncertainty" principle reduces a stochastic bandit problem to the constructionof a confidence sequence for the unknown reward function. The performance ofthe resulting bandit algorithm depends on the size of the confidence sequence,with smaller confidence sets yielding better empirical performance and strongerregret guarantees. In this work, we use a novel tail bound for adaptivemartingale mixtures to construct confidence sequences which are suitable forstochastic bandits. These confidence sequences allow for efficient actionselection via convex programming. We prove that a linear bandit algorithm basedon our confidence sequences is guaranteed to achieve competitive worst-caseregret. We show that our confidence sequences are tighter than competitors,both empirically and theoretically. Finally, we demonstrate that our tighterconfidence sequences give improved performance in several hyperparameter tuningtasks.</description><author>Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</author><pubDate>Mon, 25 Sep 2023 18:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14298v1</guid></item><item><title>NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields</title><link>http://arxiv.org/abs/2309.14293v1</link><description>Neural radiance fields (NeRFs) enable high-quality novel view synthesis, buttheir prohibitively high computational complexity limits deployability,especially on resource-constrained platforms. To enable practical usage ofNeRFs, quality tuning is essential to reduce computational complexity, akin toadjustable graphics settings in video games. However while existing solutionsstrive for efficiency, they use one-size-fits-all architectures regardless ofscene complexity, although the same architecture may be unnecessarily large forsimple scenes but insufficient for complex ones. Thus as NeRFs become morewidely used for 3D visualization, there is a need to dynamically optimize theneural network component of NeRFs to achieve a balance between computationalcomplexity and specific targets for synthesis quality. Addressing this gap, weintroduce NAS-NeRF: a generative neural architecture search strategy uniquelytailored to generate NeRF architectures on a per-scene basis by optimizing thetrade-off between complexity and performance, while adhering to constraints oncomputational budget and minimum synthesis quality. Our experiments on theBlender synthetic dataset show the proposed NAS-NeRF can generate architecturesup to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to23$\times$ smaller, 22$\times$ fewer FLOPs, and 4.7$\times$ faster thanbaseline NeRFs with only a 5.3\% average SSIM drop. The source code for ourwork is also made publicly available athttps://saeejithnair.github.io/NAS-NeRF.</description><author>Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong</author><pubDate>Mon, 25 Sep 2023 18:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14293v1</guid></item><item><title>On the Non-Associativity of Analog Computations</title><link>http://arxiv.org/abs/2309.14292v1</link><description>The energy efficiency of analog forms of computing makes it one of the mostpromising candidates to deploy resource-hungry machine learning tasks onresource-constrained system such as mobile or embedded devices. However, it iswell known that for analog computations the safety net of discretization ismissing, thus all analog computations are exposed to a variety of imperfectionsof corresponding implementations. Examples include non-linearities, saturationeffect and various forms of noise. In this work, we observe that the orderingof input operands of an analog operation also has an impact on the outputresult, which essentially makes analog computations non-associative, eventhough the underlying operation might be mathematically associative. We conducta simple test by creating a model of a real analog processor which capturessuch ordering effects. With this model we assess the importance of ordering bycomparing the test accuracy of a neural network for keyword spotting, which istrained based either on an ordered model, on a non-ordered variant, and on realhardware. The results prove the existence of ordering effects as well as theirhigh impact, as neglecting ordering results in substantial accuracy drops.</description><author>Lisa Kuhn, Bernhard Klein, Holger Fröning</author><pubDate>Mon, 25 Sep 2023 18:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14292v1</guid></item><item><title>Tiled Multiplane Images for Practical 3D Photography</title><link>http://arxiv.org/abs/2309.14291v1</link><description>The task of synthesizing novel views from a single image has usefulapplications in virtual reality and mobile computing, and a number ofapproaches to the problem have been proposed in recent years. A MultiplaneImage (MPI) estimates the scene as a stack of RGBA layers, and can modelcomplex appearance effects, anti-alias depth errors and synthesize soft edgesbetter than methods that use textured meshes or layered depth images. Andunlike neural radiance fields, an MPI can be efficiently rendered on graphicshardware. However, MPIs are highly redundant and require a large number ofdepth layers to achieve plausible results. Based on the observation that thedepth complexity in local image regions is lower than that over the entireimage, we split an MPI into many small, tiled regions, each with only a fewdepth planes. We call this representation a Tiled Multiplane Image (TMPI). Wepropose a method for generating a TMPI with adaptive depth planes forsingle-view 3D photography in the wild. Our synthesized results are comparableto state-of-the-art single-view MPI methods while having lower computationaloverhead.</description><author>Numair Khan, Douglas Lanman, Lei Xiao</author><pubDate>Mon, 25 Sep 2023 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14291v1</guid></item><item><title>Inferential Theory for Granular Instrumental Variables in High Dimensions</title><link>http://arxiv.org/abs/2201.06605v2</link><description>The Granular Instrumental Variables (GIV) methodology exploits panels withfactor error structures to construct instruments to estimate structural timeseries models with endogeneity even after controlling for latent factors. Weextend the GIV methodology in several dimensions. First, we extend theidentification procedure to a large $N$ and large $T$ framework, which dependson the asymptotic Herfindahl index of the size distribution of $N$cross-sectional units. Second, we treat both the factors and loadings asunknown and show that the sampling error in the estimated instrument andfactors is negligible when considering the limiting distribution of thestructural parameters. Third, we show that the sampling error in thehigh-dimensional precision matrix is negligible in our estimation algorithm.Fourth, we overidentify the structural parameters with additional constructedinstruments, which leads to efficiency gains. Monte Carlo evidence is presentedto support our asymptotic theory and application to the global crude oil marketleads to new results.</description><author>Saman Banafti, Tae-Hwy Lee</author><pubDate>Mon, 25 Sep 2023 17:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.06605v2</guid></item><item><title>CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free</title><link>http://arxiv.org/abs/2309.14289v1</link><description>The emergence of CLIP has opened the way for open-world image perception. Thezero-shot classification capabilities of the model are impressive but areharder to use for dense tasks such as image segmentation. Several methods haveproposed different modifications and learning schemes to produce dense output.Instead, we propose in this work an open-vocabulary semantic segmentationmethod, dubbed CLIP-DIY, which does not require any additional training orannotations, but instead leverages existing unsupervised object localizationapproaches. In particular, CLIP-DIY is a multi-scale approach that directlyexploits CLIP classification abilities on patches of different sizes andaggregates the decision in a single map. We further guide the segmentationusing foreground/background scores obtained using unsupervised objectlocalization methods. With our method, we obtain state-of-the-art zero-shotsemantic segmentation results on PASCAL VOC and perform on par with the bestmethods on COCO.</description><author>Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni</author><pubDate>Mon, 25 Sep 2023 17:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14289v1</guid></item><item><title>MGL2Rank: Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion</title><link>http://arxiv.org/abs/2305.14375v2</link><description>Identifying important nodes with strong propagation capabilities in roadnetworks is a significant topic in the field of urban planning. However,existing methods for evaluating the importance of nodes in traffic networkconsider only topological information and traffic volumes, ignoring thediversity of characteristics in road networks, such as the number of lanes andaverage speed of road segments, limiting their performance. To solve thisproblem, we propose a graph learning-based framework (MGL2Rank) that integratesthe rich characteristics of road network for ranking the importance of nodes.In this framework, we first develop an embedding module that contains asampling algorithm (MGWalk) and an encoder network to learn latentrepresentation for each road segment. MGWalk utilizes multi-graph fusion tocapture the topology of the road network and establish associations among roadsegments based on their attributes. Then, we use the obtained noderepresentation to learn the importance ranking of road segments. Finally, weconstruct a synthetic dataset for ranking tasks based on the regional roadnetwork of Shenyang city, and our ranking results on this dataset demonstratethe effectiveness of our proposed method. The data and source code of MGL2Rankare available at https://github.com/ZJ726.</description><author>Ming Xu, Jing Zhang</author><pubDate>Mon, 25 Sep 2023 17:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14375v2</guid></item><item><title>Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation</title><link>http://arxiv.org/abs/2309.14282v1</link><description>Prototypical contrastive learning (PCL) has been widely used to learnclass-wise domain-invariant features recently. These methods are based on theassumption that the prototypes, which are represented as the central value ofthe same class in a certain domain, are domain-invariant. Since the prototypesof different domains have discrepancies as well, the class-wisedomain-invariant features learned from the source domain by PCL need to bealigned with the prototypes of other domains simultaneously. However, theprototypes of the same class in different domains may be different while theprototypes of different classes may be similar, which may affect the learningof class-wise domain-invariant features. Based on these observations, acalibration-based dual prototypical contrastive learning (CDPCL) approach isproposed to reduce the domain discrepancy between the learned class-wisefeatures and the prototypes of different domains for domain generalizationsemantic segmentation. It contains an uncertainty-guided PCL (UPCL) and ahard-weighted PCL (HPCL). Since the domain discrepancies of the prototypes ofdifferent classes may be different, we propose an uncertainty probabilitymatrix to represent the domain discrepancies of the prototypes of all theclasses. The UPCL estimates the uncertainty probability matrix to calibrate theweights of the prototypes during the PCL. Moreover, considering that theprototypes of different classes may be similar in some circumstances, whichmeans these prototypes are hard-aligned, the HPCL is proposed to generate ahard-weighted matrix to calibrate the weights of the hard-aligned prototypesduring the PCL. Extensive experiments demonstrate that our approach achievessuperior performance over current approaches on domain generalization semanticsegmentation tasks.</description><author>Muxin Liao, Shishun Tian, Yuhang Zhang, Guoguang Hua, Wenbin Zou, Xia Li</author><pubDate>Mon, 25 Sep 2023 17:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14282v1</guid></item><item><title>Cross-Class Feature Augmentation for Class Incremental Learning</title><link>http://arxiv.org/abs/2304.01899v2</link><description>We propose a novel class incremental learning approach by incorporating afeature augmentation technique motivated by adversarial attacks. We employ aclassifier learned in the past to complement training examples rather thansimply play a role as a teacher for knowledge distillation towards subsequentmodels. The proposed approach has a unique perspective to utilize the previousknowledge in class incremental learning since it augments features of arbitrarytarget classes using examples in other classes via adversarial attacks on apreviously learned classifier. By allowing the cross-class featureaugmentations, each class in the old tasks conveniently populates samples inthe feature space, which alleviates the collapse of the decision boundariescaused by sample deficiency for the previous tasks, especially when the numberof stored exemplars is small. This idea can be easily incorporated intoexisting class incremental learning algorithms without any architecturemodification. Extensive experiments on the standard benchmarks show that ourmethod consistently outperforms existing class incremental learning methods bysignificant margins in various scenarios, especially under an environment withan extremely limited memory budget.</description><author>Taehoon Kim, Jaeyoo Park, Bohyung Han</author><pubDate>Mon, 25 Sep 2023 17:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01899v2</guid></item><item><title>SINCERE: Supervised Information Noise-Contrastive Estimation REvisited</title><link>http://arxiv.org/abs/2309.14277v1</link><description>The information noise-contrastive estimation (InfoNCE) loss function providesthe basis of many self-supervised deep learning methods due to its strongempirical results and theoretic motivation. Previous work suggests a supervisedcontrastive (SupCon) loss to extend InfoNCE to learn from available classlabels. This SupCon loss has been widely-used due to reports of good empiricalperformance. However, in this work we suggest that the specific SupCon lossformulated by prior work has questionable theoretic justification, because itcan encourage images from the same class to repel one another in the learnedembedding space. This problematic behavior gets worse as the number of inputssharing one class label increases. We propose the Supervised InfoNCE REvisited(SINCERE) loss as a remedy. SINCERE is a theoretically justified solution for asupervised extension of InfoNCE that never causes images from the same class torepel one another. We further show that minimizing our new loss is equivalentto maximizing a bound on the KL divergence between class conditional embeddingdistributions. We compare SINCERE and SupCon losses in terms of learningtrajectories during pretraining and in ultimate linear classifier performanceafter finetuning. Our proposed SINCERE loss better separates embeddings fromdifferent classes during pretraining while delivering competitive accuracy.</description><author>Patrick Feeney, Michael C. Hughes</author><pubDate>Mon, 25 Sep 2023 17:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14277v1</guid></item><item><title>Hierarchical Integration Diffusion Model for Realistic Image Deblurring</title><link>http://arxiv.org/abs/2305.12966v4</link><description>Diffusion models (DMs) have recently been introduced in image deblurring andexhibited promising performance, particularly in terms of detailsreconstruction. However, the diffusion model requires a large number ofinference iterations to recover the clean image from pure Gaussian noise, whichconsumes massive computational resources. Moreover, the distributionsynthesized by the diffusion model is often misaligned with the target results,leading to restrictions in distortion-based metrics. To address the aboveissues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), forrealistic image deblurring. Specifically, we perform the DM in a highlycompacted latent space to generate the prior feature for the deblurringprocess. The deblurring process is implemented by a regression-based method toobtain better distortion accuracy. Meanwhile, the highly compact latent spaceensures the efficiency of the DM. Furthermore, we design the hierarchicalintegration module to fuse the prior into the regression-based model frommultiple scales, enabling better generalization in complex blurry scenarios.Comprehensive experiments on synthetic and real-world blur datasets demonstratethat our HI-Diff outperforms state-of-the-art methods. Code and trained modelsare available at https://github.com/zhengchen1999/HI-Diff.</description><author>Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, Linghe Kong, Xin Yuan</author><pubDate>Mon, 25 Sep 2023 17:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12966v4</guid></item><item><title>Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty</title><link>http://arxiv.org/abs/2309.14272v1</link><description>Global navigation satellite systems (GNSS) denied environments/conditionsrequire unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly.To this end, this study presents perception-and-energy-aware motion planningfor UAVs in GNSS-denied environments. The proposed planner solves thetrajectory planning problem by optimizing a cost function consisting of twoindices: the total energy consumption of a UAV and the perception quality oflight detection and ranging (LiDAR) sensor mounted on the UAV. Before onlinenavigation, a high-fidelity simulator acquires a flight dataset to learn energyconsumption for the UAV and heteroscedastic uncertainty associated with LiDARmeasurements, both as functions of the horizontal velocity of the UAV. Thelearned models enable the online planner to estimate energy consumption andperception quality, reducing UAV battery usage and localization errors.Simulation experiments in a photorealistic environment confirm that theproposed planner can address the trade-off between energy efficiency andperception quality under heteroscedastic uncertainty. The open-source code isreleased at https://gitlab.com/ReI08/perception-energy-planner.</description><author>Reiya Takemura, Genya Ishigami</author><pubDate>Mon, 25 Sep 2023 17:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14272v1</guid></item><item><title>Unsupervised correspondence with combined geometric learning and imaging for radiotherapy applications</title><link>http://arxiv.org/abs/2309.14269v1</link><description>The aim of this study was to develop a model to accurately identifycorresponding points between organ segmentations of different patients forradiotherapy applications. A model for simultaneous correspondence andinterpolation estimation in 3D shapes was trained with head and neck organsegmentations from planning CT scans. We then extended the original model toincorporate imaging information using two approaches: 1) extracting featuresdirectly from image patches, and 2) including the mean square error betweenpatches as part of the loss function. The correspondence and interpolationperformance were evaluated using the geodesic error, chamfer distance andconformal distortion metrics, as well as distances between anatomicallandmarks. Each of the models produced significantly better correspondencesthan the baseline non-rigid registration approach. The original model performedsimilarly to the model with direct inclusion of image features. The bestperforming model configuration incorporated imaging information as part of theloss function which produced more anatomically plausible correspondences. Wewill use the best performing model to identify corresponding anatomical pointson organs to improve spatial normalisation, an important step in outcomemodelling, or as an initialisation for anatomically informed registrations. Allour code is publicly available athttps://github.com/rrr-uom-projects/Unsup-RT-Corr-Net</description><author>Edward G. A. Henderson, Marcel van Herk, Andrew F. Green, Eliana M. Vasquez Osorio</author><pubDate>Mon, 25 Sep 2023 17:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14269v1</guid></item><item><title>Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments</title><link>http://arxiv.org/abs/2309.14267v1</link><description>Semantic facial attribute editing using pre-trained Generative AdversarialNetworks (GANs) has attracted a great deal of attention and effort fromresearchers in recent years. Due to the high quality of face images generatedby StyleGANs, much work has focused on the StyleGANs' latent space and theproposed methods for facial image editing. Although these methods have achievedsatisfying results for manipulating user-intended attributes, they have notfulfilled the goal of preserving the identity, which is an important challenge.We present ID-Style, a new architecture capable of addressing the problem ofidentity loss during attribute manipulation. The key components of ID-Styleinclude Learnable Global Direction (LGD), which finds a shared and semi-sparsedirection for each attribute, and an Instance-Aware Intensity Predictor (IAIP)network, which finetunes the global direction according to the input instance.Furthermore, we introduce two losses during training to enforce the LGD to findsemi-sparse semantic directions, which along with the IAIP, preserve theidentity of the input instance. Despite reducing the size of the network byroughly 95% as compared to similar state-of-the-art works, it outperformsbaselines by 10% and 7% in Identity preserving metric (FRS) and averageaccuracy of manipulation (mACC), respectively.</description><author>Najmeh Mohammadbagheri, Fardin Ayar, Ahmad Nickabadi, Reza Safabakhsh</author><pubDate>Mon, 25 Sep 2023 17:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14267v1</guid></item><item><title>Predicting the cardinality and maximum degree of a reduced Gröbner basis</title><link>http://arxiv.org/abs/2302.05364v2</link><description>We construct neural network regression models to predict key metrics ofcomplexity for Gr\"obner bases of binomial ideals. This work illustrates whypredictions with neural networks from Gr\"obner computations are not astraightforward process. Using two probabilistic models for random binomialideals, we generate and make available a large data set that is able to capturesufficient variability in Gr\"obner complexity. We use this data to trainneural networks and predict the cardinality of a reduced Gr\"obner basis andthe maximum total degree of its elements. While the cardinality predictionproblem is unlike classical problems tackled by machine learning, oursimulations show that neural networks, providing performance statistics such as$r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 =0.180$.</description><author>Shahrzad Jamshidi, Eric Kang, Sonja Petrović</author><pubDate>Mon, 25 Sep 2023 17:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05364v2</guid></item><item><title>Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</title><link>http://arxiv.org/abs/2309.14265v1</link><description>Despite the advances in robotics a large proportion of the of parts handlingtasks in the automotive industry's internal logistics are not automated butstill performed by humans. A key component to competitively automate theseprocesses is a 6D pose estimation that can handle a large number of differentparts, is adaptable to new parts with little manual effort, and is sufficientlyaccurate and robust with respect to industry requirements. In this context, thequestion arises as to the current status quo with respect to these measures. Toaddress this we built a representative 6D pose estimation pipeline withstate-of-the-art components from economically scalable real to synthetic datageneration to pose estimators and evaluated it on automotive parts with regardsto a realistic sequencing process. We found that using the data generationapproaches, the performance of the trained 6D pose estimators are promising,but do not meet industry requirements. We reveal that the reason for this isthe inability of the estimators to provide reliable uncertainties for theirposes, rather than the ability of to provide sufficiently accurate poses. Inthis context we further analyzed how RGB- and RGB-D-based approaches compareagainst this background and show that they are differently vulnerable to thedomain gap induced by synthetic data.</description><author>Philipp Quentin, Dino Knoll, Daniel Goehring</author><pubDate>Mon, 25 Sep 2023 17:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14265v1</guid></item><item><title>OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding</title><link>http://arxiv.org/abs/2309.14258v1</link><description>Event understanding aims at understanding the content and relationship ofevents within texts, which covers multiple complicated information extractiontasks: event detection, event argument extraction, and event relationextraction. To facilitate related research and application, we present an eventunderstanding toolkit OmniEvent, which features three desiderata: (1)Comprehensive. OmniEvent supports mainstream modeling paradigms of all theevent understanding tasks and the processing of 15 widely-used English andChinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuousevaluation pitfalls reported in Peng et al. (2023), which ensures faircomparisons between different models. (3) Easy-to-use. OmniEvent is designed tobe easily used by users with varying needs. We provide off-the-shelf modelsthat can be directly deployed as web services. The modular framework alsoenables users to easily implement and evaluate new event understanding modelswith OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publiclyreleased along with the demonstration website and video(https://omnievent.xlore.cn/).</description><author>Hao Peng, Xiaozhi Wang, Feng Yao, Zimu Wang, Chuzhao Zhu, Kaisheng Zeng, Lei Hou, Juanzi Li</author><pubDate>Mon, 25 Sep 2023 17:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14258v1</guid></item><item><title>A Weighted Prognostic Covariate Adjustment Method for Efficient and Powerful Treatment Effect Inferences in Randomized Controlled Trials</title><link>http://arxiv.org/abs/2309.14256v1</link><description>A crucial task for a randomized controlled trial (RCT) is to specify astatistical method that can yield an efficient estimator and powerful test forthe treatment effect. A novel and effective strategy to obtain efficient andpowerful treatment effect inferences is to incorporate predictions fromgenerative artificial intelligence (AI) algorithms into covariate adjustmentfor the regression analysis of a RCT. Training a generative AI algorithm onhistorical control data enables one to construct a digital twin generator (DTG)for RCT participants, which utilizes a participant's baseline covariates togenerate a probability distribution for their potential control outcome.Summaries of the probability distribution from the DTG are highly predictive ofthe trial outcome, and adjusting for these features via regression can thusimprove the quality of treatment effect inferences, while satisfying regulatoryguidelines on statistical analyses, for a RCT. However, a critical assumptionin this strategy is homoskedasticity, or constant variance of the outcomeconditional on the covariates. In the case of heteroskedasticity, existingcovariate adjustment methods yield inefficient estimators and underpoweredtests. We propose to address heteroskedasticity via a weighted prognosticcovariate adjustment methodology (Weighted PROCOVA) that adjusts for both themean and variance of the regression model using information obtained from theDTG. We prove that our method yields unbiased treatment effect estimators, anddemonstrate via comprehensive simulation studies and case studies fromAlzheimer's disease that it can reduce the variance of the treatment effectestimator, maintain the Type I error rate, and increase the power of the testfor the treatment effect from 80% to 85%~90% when the variances from the DTGcan explain 5%~10% of the variation in the RCT participants' outcomes.</description><author>Alyssa M. Vanderbeek, Anna A. Vidovszky, Jessica L. Ross, Arman Sabbaghi, Jonathan R. Walsh, Charles K. Fisher, the Critical Path for Alzheimer's Disease, the Alzheimer's Disease Neuroimaging Initiative, the European Prevention of Alzheimer's Disease, Consortium, the Alzheimer's Disease Cooperative Study</author><pubDate>Mon, 25 Sep 2023 17:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14256v1</guid></item><item><title>Expressive variational quantum circuits provide inherent privacy in federated learning</title><link>http://arxiv.org/abs/2309.13002v2</link><description>Federated learning has emerged as a viable distributed solution to trainmachine learning models without the actual need to share data with the centralaggregator. However, standard neural network-based federated learning modelshave been shown to be susceptible to data leakage from the gradients sharedwith the server. In this work, we introduce federated learning with variationalquantum circuit model built using expressive encoding maps coupled withoverparameterized ans\"atze. We show that expressive maps lead to inherentprivacy against gradient inversion attacks, while overparameterization ensuresmodel trainability. Our privacy framework centers on the complexity of solvingthe system of high-degree multivariate Chebyshev polynomials generated by thegradients of quantum circuit. We present compelling arguments highlighting theinherent difficulty in solving these equations, both in exact and approximatescenarios. Additionally, we delve into machine learning-based attack strategiesand establish a direct connection between overparameterization in the originalfederated learning model and underparameterization in the attack model.Furthermore, we provide numerical scaling arguments showcasing thatunderparameterization of the expressive map in the attack model leads to theloss landscape being swamped with exponentially many spurious local minimapoints, thus making it extremely hard to realize a successful attack. Thisprovides a strong claim, for the first time, that the nature of quantum machinelearning models inherently helps prevent data leakage in federated learning.</description><author>Niraj Kumar, Jamie Heredge, Changhao Li, Shaltiel Eloul, Shree Hari Sureshbabu, Marco Pistoia</author><pubDate>Mon, 25 Sep 2023 17:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13002v2</guid></item><item><title>Beyond Individual Input for Deep Anomaly Detection on Tabular Data</title><link>http://arxiv.org/abs/2305.15121v4</link><description>Anomaly detection is vital in many domains, such as finance, healthcare, andcybersecurity. In this paper, we propose a novel deep anomaly detection methodfor tabular data that leverages Non-Parametric Transformers (NPTs), a modelinitially proposed for supervised tasks, to capture both feature-feature andsample-sample dependencies. In a reconstruction-based framework, we train theNPT to reconstruct masked features of normal samples. In a non-parametricfashion, we leverage the whole training set during inference and use themodel's ability to reconstruct the masked features to generate an anomalyscore. To the best of our knowledge, this is the first work to successfullycombine feature-feature and sample-sample dependencies for anomaly detection ontabular datasets. Through extensive experiments on 31 benchmark tabulardatasets, we demonstrate that our method achieves state-of-the-art performance,outperforming existing methods by 1.7% and 1.2% in terms of F1-score and AUROC,respectively. Our ablation study provides evidence that modeling both types ofdependencies is crucial for anomaly detection on tabular data.</description><author>Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan</author><pubDate>Mon, 25 Sep 2023 17:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15121v4</guid></item><item><title>Prediction Model For Wordle Game Results With High Robustness</title><link>http://arxiv.org/abs/2309.14250v1</link><description>In this study, we delve into the dynamics of Wordle using data analysis andmachine learning. Our analysis initially focused on the correlation between thedate and the number of submitted results. Due to initial popularity bias, wemodeled stable data using an ARIMAX model with coefficient values of 9, 0, 2,and weekdays/weekends as the exogenous variable. We found no significantrelationship between word attributes and hard mode results. To predict word difficulty, we employed a Backpropagation Neural Network,overcoming overfitting via feature engineering. We also used K-meansclustering, optimized at five clusters, to categorize word difficultynumerically. Our findings indicate that on March 1st, 2023, around 12,884results will be submitted and the word "eerie" averages 4.8 attempts, fallinginto the hardest difficulty cluster. We further examined the percentage of loyal players and their propensity toundertake daily challenges. Our models underwent rigorous sensitivity analyses,including ADF, ACF, PACF tests, and cross-validation, confirming theirrobustness. Overall, our study provides a predictive framework for Wordlegameplay based on date or a given five-letter word. Results have beensummarized and submitted to the Puzzle Editor of the New York Times.</description><author>Jiaqi Weng, Chunlin Feng</author><pubDate>Mon, 25 Sep 2023 17:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14250v1</guid></item><item><title>Rethinking Internet Communication Through LLMs: How Close Are We?</title><link>http://arxiv.org/abs/2309.14247v1</link><description>In this paper, we rethink the way that communication among users over theInternet, one of the fundamental outcomes of the Internet evolution, takesplace. Instead of users communicating directly over the Internet, we explore anarchitecture that enables users to communicate with (query) Large LanguageModels (LLMs) that capture the cognition of users on the other end of thecommunication channel. We present an architecture to achieve such LLM-basedcommunication and we perform a reality check to assess how close we are todayto realizing such a communication architecture from a technical point of view.Finally, we discuss several research challenges and identify interestingdirections for future research.</description><author>Sifat Ut Taki, Spyridon Mastorakis</author><pubDate>Mon, 25 Sep 2023 17:07:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14247v1</guid></item><item><title>Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning</title><link>http://arxiv.org/abs/2309.14246v1</link><description>Deployment in hazardous environments requires robots to understand the risksassociated with their actions and movements to prevent accidents. Despite itsimportance, these risks are not explicitly modeled by currently deployedlocomotion controllers for legged robots. In this work, we propose a risksensitive locomotion training method employing distributional reinforcementlearning to consider safety explicitly. Instead of relying on a valueexpectation, we estimate the complete value distribution to account foruncertainty in the robot's interaction with the environment. The valuedistribution is consumed by a risk metric to extract risk sensitive valueestimates. These are integrated into Proximal Policy Optimization (PPO) toderive our method, Distributional Proximal Policy Optimization (DPPO). The riskpreference, ranging from risk-averse to risk-seeking, can be controlled by asingle parameter, which enables to adjust the robot's behavior dynamically.Importantly, our approach removes the need for additional reward functiontuning to achieve risk sensitivity. We show emergent risk sensitive locomotionbehavior in simulation and on the quadrupedal robot ANYmal.</description><author>Lukas Schneider, Jonas Frey, Takahiro Miki, Marco Hutter</author><pubDate>Mon, 25 Sep 2023 17:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14246v1</guid></item><item><title>Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review</title><link>http://arxiv.org/abs/2305.06739v2</link><description>Motion represents one of the major challenges in magnetic resonance imaging(MRI). Since the MR signal is acquired in frequency space, any motion of theimaged object leads to complex artefacts in the reconstructed image in additionto other MR imaging artefacts. Deep learning has been frequently proposed formotion correction at several stages of the reconstruction process. The widerange of MR acquisition sequences, anatomies and pathologies of interest, andmotion patterns (rigid vs. deformable and random vs. regular) makes acomprehensive solution unlikely. To facilitate the transfer of ideas betweendifferent applications, this review provides a detailed overview of proposedmethods for learning-based motion correction in MRI together with their commonchallenges and potentials. This review identifies differences and synergies inunderlying data usage, architectures, training and evaluation strategies. Wecritically discuss general trends and outline future directions, with the aimto enhance interaction between different application areas and research fields.</description><author>Veronika Spieker, Hannah Eichhorn, Kerstin Hammernik, Daniel Rueckert, Christine Preibisch, Dimitrios C. Karampinos, Julia A. Schnabel</author><pubDate>Mon, 25 Sep 2023 17:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06739v2</guid></item><item><title>Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation</title><link>http://arxiv.org/abs/2309.14243v1</link><description>Reinforcement learning (RL) algorithms face the challenge of limited dataefficiency, particularly when dealing with high-dimensional state spaces andlarge-scale problems. Most RL methods often rely solely on state transitioninformation within the same episode when updating the agent's Critic, which canlead to low data efficiency and sub-optimal training time consumption. Inspiredby human-like analogical reasoning abilities, we introduce a novel meshinformation propagation mechanism, termed the 'Imagination Mechanism (IM)',designed to significantly enhance the data efficiency of RL algorithms.Specifically, IM enables information generated by a single sample to beeffectively broadcasted to different states, instead of simply transmitting inthe same episode and it allows the model to better understand theinterdependencies between states and learn scarce sample information moreefficiently. To promote versatility, we extend the imagination mechanism tofunction as a plug-and-play module that can be seamlessly and fluidlyintegrated into other widely adopted RL models. Our experiments demonstratethat Imagination mechanism consistently boosts four mainstream SOTARL-algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin,ultimately leading to superior performance than before across various tasks.For access to our code and data, please visithttps://github.com/Zero-coder/FECAM.</description><author>Zihang Wang, Maowei Jiang</author><pubDate>Mon, 25 Sep 2023 17:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14243v1</guid></item><item><title>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</title><link>http://arxiv.org/abs/2307.12626v2</link><description>Multimodal reasoning is a critical component in the pursuit of artificialintelligence systems that exhibit human-like intelligence, especially whentackling complex tasks. While the chain-of-thought (CoT) technique has gainedconsiderable attention, the existing ScienceQA dataset, which focuses onmultimodal scientific questions and explanations from elementary and highschool textbooks, lacks a comprehensive evaluation of diverse approaches. Toaddress this gap, we present COCO Multi-Modal Reasoning(COCO-MMR) dataset, anovel dataset that encompasses an extensive collection of open-ended questions,rationales, and answers derived from the large object dataset COCO. Unlikeprevious datasets that rely on multiple-choice questions, our dataset pioneersthe use of open-ended questions in the context of multimodal CoT, introducing amore challenging problem that effectively assesses the reasoning capability ofCoT models. Through comprehensive evaluations and detailed analyses, we providevaluable insights and propose innovative techniques, including multi-hopcross-modal attention and sentence-level contrastive learning, to enhance theimage and text encoders. Extensive experiments demonstrate the efficacy of theproposed dataset and techniques, offering novel perspectives for advancingmultimodal reasoning. The data and code are available at\href{https://github.com/weijingxuan/COCO-MMR}{https://github.com/weijingxuan/COCO-MMR}.</description><author>Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li</author><pubDate>Mon, 25 Sep 2023 16:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12626v2</guid></item><item><title>Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets</title><link>http://arxiv.org/abs/2301.08688v2</link><description>We employ deep reinforcement learning (RL) to train an agent to successfullytranslate a high-frequency trading signal into a trading strategy that placesindividual limit orders. Based on the ABIDES limit order book simulator, webuild a reinforcement learning OpenAI gym environment and utilise it tosimulate a realistic trading environment for NASDAQ equities based on historicorder book messages. To train a trading agent that learns to maximise itstrading return in this environment, we use Deep Duelling Double Q-learning withthe APEX (asynchronous prioritised experience replay) architecture. The agentobserves the current limit order book state, its recent history, and ashort-term directional forecast. To investigate the performance of RL foradaptive trading independently from a concrete forecasting algorithm, we studythe performance of our approach utilising synthetic alpha signals obtained byperturbing forward-looking returns with varying levels of noise. Here, we findthat the RL agent learns an effective trading strategy for inventory managementand order placing that outperforms a heuristic benchmark trading strategyhaving access to the same signal.</description><author>Peer Nagy, Jan-Peter Calliess, Stefan Zohren</author><pubDate>Mon, 25 Sep 2023 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08688v2</guid></item><item><title>DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation</title><link>http://arxiv.org/abs/2305.19201v2</link><description>Neural radiance fields (NeRF) shows powerful performance in novel viewsynthesis and 3D geometry reconstruction, but it suffers from criticalperformance degradation when the number of known viewpoints is drasticallyreduced. Existing works attempt to overcome this problem by employing externalpriors, but their success is limited to certain types of scenes or datasets.Employing monocular depth estimation (MDE) networks, pretrained on large-scaleRGB-D datasets, with powerful generalization capability would be a key tosolving this problem: however, using MDE in conjunction with NeRF comes with anew set of challenges due to various ambiguity problems exhibited by monoculardepths. In this light, we propose a novel framework, dubbed D\"aRF, thatachieves robust NeRF reconstruction with a handful of real-world images bycombining the strengths of NeRF and monocular depth estimation through onlinecomplementary training. Our framework imposes the MDE network's powerfulgeometry prior to NeRF representation at both seen and unseen viewpoints toenhance its robustness and coherence. In addition, we overcome the ambiguityproblems of monocular depths through patch-wise scale-shift fitting andgeometry distillation, which adapts the MDE network to produce depths alignedaccurately with NeRF geometry. Experiments show our framework achievesstate-of-the-art results both quantitatively and qualitatively, demonstratingconsistent and reliable performance in both indoor and outdoor real-worlddatasets. Project page is available at https://ku-cvlab.github.io/DaRF/.</description><author>Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim</author><pubDate>Mon, 25 Sep 2023 16:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19201v2</guid></item><item><title>Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation</title><link>http://arxiv.org/abs/2309.14241v1</link><description>Contemporary domain adaptation offers a practical solution for achievingcross-domain transfer of semantic segmentation between labeled source data andunlabeled target data. These solutions have gained significant popularity;however, they require the model to be retrained when the test environmentchanges. This can result in unbearable costs in certain applications due to thetime-consuming training process and concerns regarding data privacy. One-shotdomain adaptation methods attempt to overcome these challenges by transferringthe pre-trained source model to the target domain using only one target data.Despite this, the referring style transfer module still faces issues withcomputation cost and over-fitting problems. To address this problem, we proposea novel framework called Informative Data Mining (IDM) that enables efficientone-shot domain adaptation for semantic segmentation. Specifically, IDMprovides an uncertainty-based selection criterion to identify the mostinformative samples, which facilitates quick adaptation and reduces redundanttraining. We then perform a model adaptation method using these selectedsamples, which includes patch-wise mixing and prototype-based informationmaximization to update the model. This approach effectively enhances adaptationand mitigates the overfitting problem. In general, we provide empiricalevidence of the effectiveness and efficiency of IDM. Our approach outperformsexisting methods and achieves a new state-of-the-art one-shot performance of56.7\%/55.4\% on the GTA5/SYNTHIA to Cityscapes adaptation tasks, respectively.The code will be released at \url{https://github.com/yxiwang/IDM}.</description><author>Yuxi Wang, Jian Liang, Jun Xiao, Shuqi Mei, Yuran Yang, Zhaoxiang Zhang</author><pubDate>Mon, 25 Sep 2023 16:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14241v1</guid></item><item><title>Learning to Abstain From Uninformative Data</title><link>http://arxiv.org/abs/2309.14240v1</link><description>Learning and decision-making in domains with naturally high noise-to-signalratio, such as Finance or Healthcare, is often challenging, while the stakesare very high. In this paper, we study the problem of learning and acting undera general noisy generative process. In this problem, the data distribution hasa significant proportion of uninformative samples with high noise in the label,while part of the data contains useful information represented by low labelnoise. This dichotomy is present during both training and inference, whichrequires the proper handling of uninformative data during both training andtesting. We propose a novel approach to learning under these conditions via aloss inspired by the selective learning theory. By minimizing this loss, themodel is guaranteed to make a near-optimal decision by distinguishinginformative data from uninformative data and making predictions. We build uponthe strength of our theoretical guarantees by describing an iterativealgorithm, which jointly optimizes both a predictor and a selector, andevaluates its empirical performance in a variety of settings.</description><author>Yikai Zhang, Songzhu Zheng, Mina Dalirrooyfard, Pengxiang Wu, Anderson Schneider, Anant Raj, Yuriy Nevmyvaka, Chao Chen</author><pubDate>Mon, 25 Sep 2023 16:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14240v1</guid></item><item><title>Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving</title><link>http://arxiv.org/abs/2309.14235v1</link><description>The deployment of autonomous vehicles (AVs) has faced hurdles due to thedominance of rare but critical corner cases within the long-tail distributionof driving scenarios, which negatively affects their overall performance. Toaddress this challenge, adversarial generation methods have emerged as a classof efficient approaches to synthesize safety-critical scenarios for AV testing.However, these generated scenarios are often underutilized for AV training,resulting in the potential for continual AV policy improvement remaininguntapped, along with a deficiency in the closed-loop design needed to achieveit. Therefore, we tailor the Stackelberg Driver Model (SDM) to accuratelycharacterize the hierarchical nature of vehicle interaction dynamics,facilitating iterative improvement by engaging background vehicles (BVs) and AVin a sequential game-like interaction paradigm. With AV acting as the leaderand BVs as followers, this leader-follower modeling ensures that AV wouldconsistently refine its policy, always taking into account the additionalinformation that BVs play the best response to challenge AV. Extensiveexperiments have shown that our algorithm exhibits superior performancecompared to several baselines especially in higher dimensional scenarios,leading to substantial advancements in AV capabilities while continuallygenerating progressively challenging scenarios.</description><author>Haoyi Niu, Qimao Chen, Yingyue Li, Jianming Hu</author><pubDate>Mon, 25 Sep 2023 16:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14235v1</guid></item><item><title>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions</title><link>http://arxiv.org/abs/2309.07875v2</link><description>Training large language models to follow instructions makes them performbetter on a wide range of tasks, generally becoming more helpful. However, aperfectly helpful model will follow even the most malicious instructions andreadily generate harmful content. In this paper, we raise concerns over thesafety of models that only emphasize helpfulness, not safety, in theirinstruction-tuning. We show that several popular instruction-tuned models arehighly unsafe. Moreover, we show that adding just 3% safety examples (a fewhundred demonstrations) in the training set when fine-tuning a model like LLaMAcan substantially improve their safety. Our safety-tuning does not make modelssignificantly less capable or helpful as measured by standard benchmarks.However, we do find a behavior of exaggerated safety, where too muchsafety-tuning makes models refuse to respond to reasonable prompts thatsuperficially resemble unsafe ones. Our study sheds light on trade-offs intraining LLMs to follow instructions and exhibit safe behavior.</description><author>Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou</author><pubDate>Mon, 25 Sep 2023 16:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07875v2</guid></item><item><title>Urdu Poetry Generated by Using Deep Learning Techniques</title><link>http://arxiv.org/abs/2309.14233v1</link><description>This study provides Urdu poetry generated using different deep-learningtechniques and algorithms. The data was collected through the Rekhta website,containing 1341 text files with several couplets. The data on poetry was notfrom any specific genre or poet. Instead, it was a collection of mixed Urdupoems and Ghazals. Different deep learning techniques, such as the modelapplied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU),have been used. Natural Language Processing (NLP) may be used in machinelearning to understand, analyze, and generate a language humans may use andunderstand. Much work has been done on generating poetry for differentlanguages using different techniques. The collection and use of data were alsodifferent for different researchers. The primary purpose of this project is toprovide a model that generates Urdu poems by using data completely, not bysampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, asin the base paper. The results have shown good accuracy in the poems generatedby the model.</description><author>Muhammad Shoaib Farooq, Ali Abbas</author><pubDate>Mon, 25 Sep 2023 16:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14233v1</guid></item><item><title>Combined sizing and layout optimization of truss structures via update Monte Carlo tree search (UMCTS) algorithm</title><link>http://arxiv.org/abs/2309.14231v1</link><description>The main concern of this study is to find the optimal design of trussstructures considering sizing and layout variables simultaneously. As comparedto purely sizing optimization problems, this problem is more challenging sincethe two types of variables involved are fundamentally different in nature. Inthis paper, a reinforcement learning method combining the update process andMonte Carlo tree search called the update Monte Carlo tree search (UMCTS) forsizing optimization problems is applied to solve combined sizing and layoutoptimization for truss structures. This study proposes a novel update processfor nodal coordinates with two features. (1) The allowed range of eachcoordinate varies in each round. (2) Accelerators for the number of entries inthe allowed range and iteration numbers are introduced to reduce thecomputation time. Furthermore, nodal coordinates and member areas aredetermined at the same time with only one search tree in each round. Thevalidation and efficiency of the UMCTS are tested on benchmark problems ofplanar and spatial trusses with discrete sizing variables and continuous layoutvariables. It is shown that the CPU time of the UMCTS is two times faster thanthe branch and bound method. The numerical results demonstrate that theproposed method stably achieves a better solution than other traditionalmethods.</description><author>Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura</author><pubDate>Mon, 25 Sep 2023 16:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14231v1</guid></item><item><title>Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem</title><link>http://arxiv.org/abs/2303.01543v2</link><description>We study the problem of learning a function that maps context observations(input) to parameters of a submodular function (output). Our motivating casestudy is a specific type of vehicle routing problem, in which a team ofUnmanned Ground Vehicles (UGVs) can serve as mobile charging stations torecharge a team of Unmanned Ground Vehicles (UAVs) that execute persistentmonitoring tasks. {We want to learn the mapping from observations of UAV taskroutes and wind field to the parameters of a submodular objective function,which describes the distribution of landing positions of the UAVs .}Traditionally, such a learning problem is solved independently as a predictionphase without considering the downstream task optimization phase. However, theloss function used in prediction may be misaligned with our final goal, i.e., agood routing decision. Good performance in the isolated prediction phase doesnot necessarily lead to good decisions in the downstream routing task. In thispaper, we propose a framework that incorporates task optimization as adifferentiable layer in the prediction phase. Our framework allows end-to-endtraining of the prediction model without using engineered intermediate lossthat is targeted only at the prediction performance. In the proposed framework,task optimization (submodular maximization) is made differentiable byintroducing stochastic perturbations into deterministic algorithms (i.e.,stochastic smoothing). We demonstrate the efficacy of the proposed frameworkusing synthetic data. Experimental results of the mobile charging stationrouting problem show that the proposed framework can result in better routingdecisions, e.g. the average number of UAVs recharged increases, compared to theprediction-optimization separate approach.</description><author>Guangyao Shi, Pratap Tokekar</author><pubDate>Mon, 25 Sep 2023 16:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01543v2</guid></item><item><title>SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation</title><link>http://arxiv.org/abs/2308.08143v2</link><description>The integration of different modalities, such as audio and visualinformation, plays a crucial role in human perception of the surroundingenvironment. Recent research has made significant progress in designing fusionmodules for audio-visual speech separation. However, they predominantly focuson multi-modal fusion architectures situated either at the top or bottompositions, rather than comprehensively considering multi-modal fusion atvarious hierarchical positions within the network. In this paper, we propose anovel model called self- and cross-attention network (SCANet), which leveragesthe attention mechanism for efficient audio-visual feature fusion. SCANetconsists of two types of attention blocks: self-attention (SA) andcross-attention (CA) blocks, where the CA blocks are distributed at the top(TCA), middle (MCA) and bottom (BCA) of SCANet. These blocks maintain theability to learn modality-specific features and enable the extraction ofdifferent semantics from audio-visual features. Comprehensive experiments onthree standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2)demonstrate the effectiveness of SCANet, outperforming existingstate-of-the-art (SOTA) methods while maintaining comparable inference time.</description><author>Kai Li, Runxuan Yang, Xiaolin Hu</author><pubDate>Mon, 25 Sep 2023 16:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08143v2</guid></item><item><title>Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models</title><link>http://arxiv.org/abs/2206.09563v4</link><description>Distributed maximization of a submodular function in the MapReduce model hasreceived much attention, culminating in two frameworks that allow a centralizedalgorithm to be run in the MR setting without loss of approximation, as long asthe centralized algorithm satisfies a certain consistency property - which hadonly been shown to be satisfied by the standard greedy and continous greedyalgorithms. A separate line of work has studied parallelizability of submodularmaximization in the adaptive complexity model, where each thread may haveaccess to the entire ground set. For the size-constrained maximization of amonotone and submodular function, we show that several sublinearly adaptivealgorithms satisfy the consistency property required to work in the MR setting,which yields highly practical parallelizable and distributed algorithms. Also,we develop the first linear-time distributed algorithm for this problem withconstant MR rounds. Finally, we provide a method to increase the maximumcardinality constraint for MR algorithms at the cost of additional MR rounds.</description><author>Tonmoy Dey, Yixin Chen, Alan Kuhnle</author><pubDate>Mon, 25 Sep 2023 16:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09563v4</guid></item><item><title>Accelerating Machine Learning Algorithms with Adaptive Sampling</title><link>http://arxiv.org/abs/2309.14221v1</link><description>The era of huge data necessitates highly efficient machine learningalgorithms. Many common machine learning algorithms, however, rely oncomputationally intensive subroutines that are prohibitively expensive on largedatasets. Oftentimes, existing techniques subsample the data or use othermethods to improve computational efficiency, at the expense of incurring someapproximation error. This thesis demonstrates that it is often sufficient,instead, to substitute computationally intensive subroutines with a specialkind of randomized counterparts that results in almost no degradation inquality.</description><author>Mo Tiwari</author><pubDate>Mon, 25 Sep 2023 16:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14221v1</guid></item><item><title>Chain-of-Verification Reduces Hallucination in Large Language Models</title><link>http://arxiv.org/abs/2309.11495v2</link><description>Generation of plausible yet incorrect factual information, termedhallucination, is an unsolved issue in large language models. We study theability of language models to deliberate on the responses they give in order tocorrect their mistakes. We develop the Chain-of-Verification (CoVe) methodwhereby the model first (i) drafts an initial response; then (ii) plansverification questions to fact-check its draft; (iii) answers those questionsindependently so the answers are not biased by other responses; and (iv)generates its final verified response. In experiments, we show CoVe decreaseshallucinations across a variety of tasks, from list-based questions fromWikidata, closed book MultiSpanQA and longform text generation.</description><author>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</author><pubDate>Mon, 25 Sep 2023 16:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11495v2</guid></item><item><title>MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation</title><link>http://arxiv.org/abs/2309.14216v1</link><description>Urban time series data forecasting featuring significant contributions tosustainable development is widely studied as an essential task of the smartcity. However, with the dramatic and rapid changes in the world environment,the assumption that data obey Independent Identically Distribution isundermined by the subsequent changes in data distribution, known as conceptdrift, leading to weak replicability and transferability of the model overunseen data. To address the issue, previous approaches typically retrain themodel, forcing it to fit the most recent observed data. However, retraining isproblematic in that it leads to model lag, consumption of resources, and modelre-invalidation, causing the drift problem to be not well solved in realisticscenarios. In this study, we propose a new urban time series prediction modelfor the concept drift problem, which encodes the drift by considering theperiodicity in the data and makes on-the-fly adjustments to the model based onthe drift using a meta-dynamic network. Experiments on real-world datasets showthat our design significantly outperforms state-of-the-art methods and can bewell generalized to existing prediction backbones by reducing their sensitivityto distribution changes.</description><author>Zekun Cai, Renhe Jiang, Xinyu Yang, Zhaonan Wang, Diansheng Guo, Hiroki Kobayashi, Xuan Song, Ryosuke Shibasaki</author><pubDate>Mon, 25 Sep 2023 16:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14216v1</guid></item><item><title>Travel Demand Forecasting: A Fair AI Approach</title><link>http://arxiv.org/abs/2303.01692v2</link><description>Artificial Intelligence (AI) and machine learning have been increasinglyadopted for travel demand forecasting. The AI-based travel demand forecastingmodels, though generate accurate predictions, may produce prediction biases andraise fairness issues. Using such biased models for decision-making may lead totransportation policies that exacerbate social inequalities. However, limitedstudies have been focused on addressing the fairness issues of these models.Therefore, in this study, we propose a novel methodology to developfairness-aware, highly-accurate travel demand forecasting models. Particularly,the proposed methodology can enhance the fairness of AI models for multipleprotected attributes (such as race and income) simultaneously. Specifically, weintroduce a new fairness regularization term, which is explicitly designed tomeasure the correlation between prediction accuracy and multiple protectedattributes, into the loss function of the travel demand forecasting model. Weconduct two case studies to evaluate the performance of the proposedmethodology using real-world ridesourcing-trip data in Chicago, IL and Austin,TX, respectively. Results highlight that our proposed methodology caneffectively enhance fairness for multiple protected attributes while preservingprediction accuracy. Additionally, we have compared our methodology with threestate-of-the-art methods that adopt the regularization term approach, and theresults demonstrate that our approach significantly outperforms them in bothpreserving prediction accuracy and enhancing fairness. This study can providetransportation professionals with a new tool to achieve fair and accuratetravel demand forecasting.</description><author>Xiaojian Zhang, Qian Ke, Xilei Zhao</author><pubDate>Mon, 25 Sep 2023 16:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01692v2</guid></item><item><title>Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward</title><link>http://arxiv.org/abs/2309.14213v1</link><description>This chapter explores the complex realm of autonomous cars, analyzing theirfundamental components and operational characteristics. The initial phase ofthe discussion is elucidating the internal mechanics of these automobiles,encompassing the crucial involvement of sensors, artificial intelligence (AI)identification systems, control mechanisms, and their integration withcloud-based servers within the framework of the Internet of Things (IoT). Itdelves into practical implementations of autonomous cars, emphasizing theirutilization in forecasting traffic patterns and transforming the dynamics oftransportation. The text also explores the topic of Robotic Process Automation(RPA), illustrating the impact of autonomous cars on different businessesthrough the automation of tasks. The primary focus of this investigation liesin the realm of cybersecurity, specifically in the context of autonomousvehicles. A comprehensive analysis will be conducted to explore various riskmanagement solutions aimed at protecting these vehicles from potential threatsincluding ethical, environmental, legal, professional, and social dimensions,offering a comprehensive perspective on their societal implications. Astrategic plan for addressing the challenges and proposing strategies foreffectively traversing the complex terrain of autonomous car systems,cybersecurity, hazards, and other concerns are some resources for acquiring anunderstanding of the intricate realm of autonomous cars and their ramificationsin contemporary society, supported by a comprehensive compilation of resourcesfor additional investigation. Keywords: RPA, Cyber Security, AV, Risk, Smart Cars</description><author>Md Aminul Islam, Sarah Alqahtani</author><pubDate>Mon, 25 Sep 2023 16:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14213v1</guid></item><item><title>QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds</title><link>http://arxiv.org/abs/2309.14211v1</link><description>This paper presents a novel framework to learn a concise geometric primitiverepresentation for 3D point clouds. Different from representing each type ofprimitive individually, we focus on the challenging problem of how to achieve aconcise and uniform representation robustly. We employ quadrics to representdiverse primitives with only 10 parameters and propose the first end-to-endlearning-based framework, namely QuadricsNet, to parse quadrics in pointclouds. The relationships between quadrics mathematical formulation andgeometric attributes, including the type, scale and pose, are insightfullyintegrated for effective supervision of QuaidricsNet. Besides, a novelpattern-comprehensive dataset with quadrics segments and objects is collectedfor training and evaluation. Experiments demonstrate the effectiveness of ourconcise representation and the robustness of QuadricsNet. Our code is availableat \url{https://github.com/MichaelWu99-lab/QuadricsNet}</description><author>Ji Wu, Huai Yu, Wen Yang, Gui-Song Xia</author><pubDate>Mon, 25 Sep 2023 16:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14211v1</guid></item><item><title>Continual Driving Policy Optimization with Closed-Loop Individualized Curricula</title><link>http://arxiv.org/abs/2309.14209v1</link><description>The safety of autonomous vehicles (AV) has been a long-standing top concern,stemming from the absence of rare and safety-critical scenarios in thelong-tail naturalistic driving distribution. To tackle this challenge, a surgeof research in scenario-based autonomous driving has emerged, with a focus ongenerating high-risk driving scenarios and applying them to conductsafety-critical testing of AV models. However, limited work has been exploredon the reuse of these extensive scenarios to iteratively improve AV models.Moreover, it remains intractable and challenging to filter through giganticscenario libraries collected from other AV models with distinct behaviors,attempting to extract transferable information for current AV improvement.Therefore, we develop a continual driving policy optimization frameworkfeaturing Closed-Loop Individualized Curricula (CLIC), which we factorize intoa set of standardized sub-modules for flexible implementation choices: AVEvaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as acollision prediction task, where it estimates the chance of AV failures inthese scenarios at each iteration. Subsequently, by re-sampling from historicalscenarios based on these failure probabilities, CLIC tailors individualizedcurricula for downstream training, aligning them with the evaluated capabilityof AV. Accordingly, CLIC not only maximizes the utilization of the vastpre-collected scenario library for closed-loop driving policy optimization butalso facilitates AV improvement by individualizing its training with morechallenging cases out of those poorly organized scenarios. Experimental resultsclearly indicate that CLIC surpasses other curriculum-based trainingstrategies, showing substantial improvement in managing risky scenarios, whilestill maintaining proficiency in handling simpler cases.</description><author>Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu</author><pubDate>Mon, 25 Sep 2023 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14209v1</guid></item><item><title>Framework based on complex networks to model and mine patient pathways</title><link>http://arxiv.org/abs/2309.14208v1</link><description>The automatic discovery of a model to represent the history of encounters ofa group of patients with the healthcare system -- the so-called ``pathway ofpatients'' -- is a new field of research that supports clinical andorganisational decisions to improve the quality and efficiency of the treatmentprovided. The pathways of patients with chronic conditions tend to varysignificantly from one person to another, have repetitive tasks, and demand theanalysis of multiple perspectives (interventions, diagnoses, medicalspecialities, among others) influencing the results. Therefore, modelling andmining those pathways is still a challenging task. In this work, we propose aframework comprising: (i) a pathway model based on a multi-aspect graph, (ii) anovel dissimilarity measurement to compare pathways taking the elapsed timeinto account, and (iii) a mining method based on traditional centralitymeasures to discover the most relevant steps of the pathways. We evaluated theframework using the study cases of pregnancy and diabetes, which revealed itsusefulness in finding clusters of similar pathways, representing them in aneasy-to-interpret way, and highlighting the most significant patterns accordingto multiple perspectives.</description><author>Caroline de Oliveira Costa Souza Rosa, Márcia Ito, Alex Borges Vieira, Klaus Wehmuth, Antônio Tadeu Azevedo Gomes</author><pubDate>Mon, 25 Sep 2023 16:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14208v1</guid></item><item><title>Automatic Animation of Hair Blowing in Still Portrait Photos</title><link>http://arxiv.org/abs/2309.14207v1</link><description>We propose a novel approach to animate human hair in a still portrait photo.Existing work has largely studied the animation of fluid elements such as waterand fire. However, hair animation for a real image remains underexplored, whichis a challenging problem, due to the high complexity of hair structure anddynamics. Considering the complexity of hair structure, we innovatively treathair wisp extraction as an instance segmentation problem, where a hair wisp isreferred to as an instance. With advanced instance segmentation networks, ourmethod extracts meaningful and natural hair wisps. Furthermore, we propose awisp-aware animation module that animates hair wisps with pleasing motionswithout noticeable artifacts. The extensive experiments show the superiority ofour method. Our method provides the most pleasing and compelling viewingexperience in the qualitative experiments and outperforms state-of-the-artstill-image animation methods by a large margin in the quantitative evaluation.Project url: \url{https://nevergiveu.github.io/AutomaticHairBlowing/}</description><author>Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem, Bing Li</author><pubDate>Mon, 25 Sep 2023 16:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14207v1</guid></item><item><title>A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents</title><link>http://arxiv.org/abs/2309.10563v2</link><description>Automatic legal judgment prediction and its explanation suffer from theproblem of long case documents exceeding tens of thousands of words, ingeneral, and having a non-uniform structure. Predicting judgments from suchdocuments and extracting their explanation becomes a challenging task, more soon documents with no structural annotation. We define this problem as "scarceannotated legal documents" and explore their lack of structural information andtheir long lengths with a deep-learning-based classification framework which wecall MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgmentprediction. We explore the adaptability of LLMs with multi-billion parameters(GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transferlearning capacity. Alongside this, we compare their performance andadaptability with MESc and the impact of combining embeddings from their lastlayers. For such hierarchical models, we also propose an explanation extractionalgorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;based on the input-occlusion sensitivity of the model, to explain thepredictions with the most relevant sentences from the document. We explorethese methods and test their effectiveness with extensive experiments andablation studies on legal documents from India, the European Union, and theUnited States with the ILDC dataset and a subset of the LexGLUE dataset. MEScachieves a minimum total performance gain of approximately 2 points overprevious state-of-the-art proposed methods, while ORSE applied on MESc achievesa total average gain of 50% over the baseline explainability scores.</description><author>Nishchal Prasad, Mohand Boughanem, Taoufik Dkaki</author><pubDate>Mon, 25 Sep 2023 16:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10563v2</guid></item><item><title>Detecting and Grounding Multi-Modal Media Manipulation and Beyond</title><link>http://arxiv.org/abs/2309.14203v1</link><description>Misinformation has become a pressing issue. Fake media, in both visual andtextual forms, is widespread on the web. While various deepfake detection andtext fake news detection methods have been proposed, they are only designed forsingle-modality forgery based on binary classification, let alone analyzing andreasoning subtle forgery traces across different modalities. In this paper, wehighlight a new research problem for multi-modal fake media, namely Detectingand Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not onlydetect the authenticity of multi-modal media, but also ground the manipulatedcontent, which requires deeper reasoning of multi-modal media manipulation. Tosupport a large-scale investigation, we construct the first DGM^4 dataset,where image-text pairs are manipulated by various approaches, with richannotation of diverse manipulations. Moreover, we propose a novel HierArchicalMulti-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture thefine-grained interaction between different modalities. HAMMER performs 1)manipulation-aware contrastive learning between two uni-modal encoders asshallow manipulation reasoning, and 2) modality-aware cross-attention bymulti-modal aggregator as deep manipulation reasoning. Dedicated manipulationdetection and grounding heads are integrated from shallow to deep levels basedon the interacted multi-modal information. To exploit more fine-grainedcontrastive learning for cross-modal semantic alignment, we further integrateManipulation-Aware Contrastive Loss with Local View and construct a moreadvanced model HAMMER++. Finally, we build an extensive benchmark and set uprigorous evaluation metrics for this new research problem. Comprehensiveexperiments demonstrate the superiority of HAMMER and HAMMER++.</description><author>Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, Ziwei Liu</author><pubDate>Mon, 25 Sep 2023 16:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14203v1</guid></item><item><title>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes</title><link>http://arxiv.org/abs/2306.12045v3</link><description>Developing computational models of neural response is crucial forunderstanding sensory processing and neural computations. Currentstate-of-the-art neural network methods use temporal filters to handle temporaldependencies, resulting in an unrealistic and inflexible processing paradigm.Meanwhile, these methods target trial-averaged firing rates and fail to captureimportant features in spike trains. This work presents the temporalconditioning spiking latent variable models (TeCoS-LVM) to simulate the neuralresponse to natural visual stimuli. We use spiking neurons to produce spikeoutputs that directly match the recorded trains. This approach helps to avoidlosing information embedded in the original spike trains. We exclude thetemporal dimension from the model parameter space and introduce a temporalconditioning operation to allow the model to adaptively explore and exploittemporal dependencies in stimuli sequences in a {\it natural paradigm}. We showthat TeCoS-LVM models can produce more realistic spike activities andaccurately fit spike statistics than powerful alternatives. Additionally,learned TeCoS-LVM models can generalize well to longer time scales. Overall,while remaining computationally tractable, our model effectively captures keyfeatures of neural coding systems. It thus provides a useful tool for buildingaccurate predictive computational accounts for various sensory perceptioncircuits.</description><author>Gehua Ma, Runhao Jiang, Rui Yan, Huajin Tang</author><pubDate>Mon, 25 Sep 2023 16:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12045v3</guid></item><item><title>LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation</title><link>http://arxiv.org/abs/2303.12233v2</link><description>Federated learning was introduced to enable machine learning over largedecentralized datasets while promising privacy by eliminating the need for datasharing. Despite this, prior work has shown that shared gradients often containprivate information and attackers can gain knowledge either through maliciousmodification of the architecture and parameters or by using optimization toapproximate user data from the shared gradients. However, prior datareconstruction attacks have been limited in setting and scale, as most workstarget FedSGD and limit the attack to single-client gradients. Many of theseattacks fail in the more practical setting of FedAVG or if updates areaggregated together using secure aggregation. Data reconstruction becomessignificantly more difficult, resulting in limited attack scale and/ordecreased reconstruction quality. When both FedAVG and secure aggregation areused, there is no current method that is able to attack multiple clientsconcurrently in a federated learning setting. In this work we introduce LOKI,an attack that overcomes previous limitations and also breaks the anonymity ofaggregation as the leaked data is identifiable and directly tied back to theclients they come from. Our design sends clients customized convolutionalparameters, and the weight gradients of data points between clients remainseparate even through aggregation. With FedAVG and aggregation across 100clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, andTiny ImageNet. Using only a single training round, LOKI is able to leak 76-86%of all data samples.</description><author>Joshua C. Zhao, Atul Sharma, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Salman Avestimehr, Saurabh Bagchi</author><pubDate>Mon, 25 Sep 2023 16:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12233v2</guid></item><item><title>How Much Temporal Long-Term Context is Needed for Action Segmentation?</title><link>http://arxiv.org/abs/2308.11358v2</link><description>Modeling long-term context in videos is crucial for many fine-grained tasksincluding temporal action segmentation. An interesting question that is stillopen is how much long-term temporal context is needed for optimal performance.While transformers can model the long-term context of a video, this becomescomputationally prohibitive for long videos. Recent works on temporal actionsegmentation thus combine temporal convolutional networks with self-attentionsthat are computed only for a local temporal window. While these approaches showgood results, their performance is limited by their inability to capture thefull context of a video. In this work, we try to answer how much long-termtemporal context is required for temporal action segmentation by introducing atransformer-based model that leverages sparse attention to capture the fullcontext of a video. We compare our model with the current state of the art onthree datasets for temporal action segmentation, namely 50Salads, Breakfast,and Assembly101. Our experiments show that modeling the full context of a videois necessary to obtain the best performance for temporal action segmentation.</description><author>Emad Bahrami, Gianpiero Francesca, Juergen Gall</author><pubDate>Mon, 25 Sep 2023 15:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11358v2</guid></item><item><title>(Predictable) Performance Bias in Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2309.14198v1</link><description>Background: With the ever-increasing amount of medical imaging data, thedemand for algorithms to assist clinicians has amplified. Unsupervised anomalydetection (UAD) models promise to aid in the crucial first step of diseasedetection. While previous studies have thoroughly explored fairness insupervised models in healthcare, for UAD, this has so far been unexplored. Methods: In this study, we evaluated how dataset composition regardingsubgroups manifests in disparate performance of UAD models along multipleprotected variables on three large-scale publicly available chest X-raydatasets. Our experiments were validated using two state-of-the-art UAD modelsfor medical images. Finally, we introduced a novel subgroup-AUROC (sAUROC)metric, which aids in quantifying fairness in machine learning. Findings: Our experiments revealed empirical "fairness laws" (similar to"scaling laws" for Transformers) for training-dataset composition: Linearrelationships between anomaly detection performance within a subpopulation andits representation in the training data. Our study further revealed performancedisparities, even in the case of balanced training data, and compound effectsthat exacerbate the drop in performance for subjects associated with multipleadversely affected groups. Interpretation: Our study quantified the disparate performance of UAD modelsagainst certain demographic subgroups. Importantly, we showed that thisunfairness cannot be mitigated by balanced representation alone. Instead, therepresentation of some subgroups seems harder to learn by UAD models than thatof others. The empirical fairness laws discovered in our study make disparateperformance in UAD models easier to estimate and aid in determining the mostdesirable dataset composition.</description><author>Felix Meissen, Svenja Breuer, Moritz Knolle, Alena Buyx, Ruth Müller, Georgios Kaissis, Benedikt Wiestler, Daniel Rückert</author><pubDate>Mon, 25 Sep 2023 15:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14198v1</guid></item><item><title>Learning Restricted Boltzmann Machines with greedy quantum search</title><link>http://arxiv.org/abs/2309.14196v1</link><description>Restricted Boltzmann Machines (RBMs) are widely used probabilistic undirectedgraphical models with visible and latent nodes, playing an important role instatistics and machine learning. The task of structure learning for RBMsinvolves inferring the underlying graph by using samples from the visiblenodes. Specifically, learning the two-hop neighbors of each visible node allowsfor the inference of the graph structure. Prior research has addressed thestructure learning problem for specific classes of RBMs, namely ferromagneticand locally consistent RBMs. In this paper, we extend the scope to the quantumcomputing domain and propose corresponding quantum algorithms for this problem.Our study demonstrates that the proposed quantum algorithms yield a polynomialspeedup compared to the classical algorithms for learning the structure ofthese two classes of RBMs.</description><author>Liming Zhao, Aman Agrawal, Patrick Rebentrost</author><pubDate>Mon, 25 Sep 2023 15:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14196v1</guid></item><item><title>Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition</title><link>http://arxiv.org/abs/2309.14183v1</link><description>The development of foundation vision models has pushed the general visualrecognition to a high level, but cannot well address the fine-grainedrecognition in specialized domain such as invasive species classification.Identifying and managing invasive species has strong social and ecologicalvalue. Currently, most invasive species datasets are limited in scale and covera narrow range of species, which restricts the development of deep-learningbased invasion biometrics systems. To fill the gap of this area, we introducedSpecies196, a large-scale semi-supervised dataset of 196-category invasivespecies. It collects over 19K images with expert-level accurate annotationsSpecies196-L, and 1.2M unlabeled images of invasive species Species196-U. Thedataset provides four experimental settings for benchmarking the existingmodels and algorithms, namely, supervised learning, semi-supervised learning,self-supervised pretraining and zero-shot inference ability of largemulti-modal models. To facilitate future research on these four learningparadigms, we conduct an empirical study of the representative methods on theintroduced dataset. The dataset is publicly available athttps://species-dataset.github.io/.</description><author>Wei He, Kai Han, Ying Nie, Chengcheng Wang, Yunhe Wang</author><pubDate>Mon, 25 Sep 2023 15:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14183v1</guid></item><item><title>BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional Feature Fusion for Brain Tumor Detection</title><link>http://arxiv.org/abs/2309.12585v2</link><description>You Only Look Once (YOLO)-based object detectors have shown remarkableaccuracy for automated brain tumor detection. In this paper, we develop a novelBGF-YOLO architecture by incorporating Bi-level Routing Attention (BRA),Generalized feature pyramid networks (GFPN), and Fourth detecting head intoYOLOv8. BGF-YOLO contains an attention mechanism to focus more on importantfeatures, and feature pyramid networks to enrich feature representation bymerging high-level semantic features with spatial details. Furthermore, weinvestigate the effect of different attention mechanisms and feature fusions,detection head architectures on brain tumor detection accuracy. Experimentalresults show that BGF-YOLO gives a 4.7% absolute increase of mAP$_{50}$compared to YOLOv8x, and achieves state-of-the-art on the brain tumor detectiondataset Br35H. The code is available at https://github.com/mkang315/BGF-YOLO.</description><author>Ming Kang, Chee-Ming Ting, Fung Fung Ting, Raphaël C. -W. Phan</author><pubDate>Mon, 25 Sep 2023 15:44:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12585v2</guid></item><item><title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</title><link>http://arxiv.org/abs/2309.14181v1</link><description>The rapid evolution of Multi-modality Large Language Models (MLLMs) hascatalyzed a shift in computer vision from specialized models to general-purposefoundation models. Nevertheless, there is still an inadequacy in assessing theabilities of MLLMs on low-level visual perception and understanding. To addressthis gap, we present Q-Bench, a holistic benchmark crafted to systematicallyevaluate potential abilities of MLLMs on three realms: low-level visualperception, low-level visual description, and overall visual qualityassessment. a) To evaluate the low-level perception ability, we construct theLLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equippedwith a human-asked question focusing on its low-level attributes. We thenmeasure the correctness of MLLMs on answering these questions. b) To examinethe description ability of MLLMs on low-level information, we propose theLLDescribe dataset consisting of long expert-labelled golden low-level textdescriptions on 499 images, and a GPT-involved comparison pipeline betweenoutputs of MLLMs and the golden descriptions. c) Besides these two tasks, wefurther measure their visual quality assessment ability to align with humanopinion scores. Specifically, we design a softmax-based strategy that enablesMLLMs to predict quantifiable quality scores, and evaluate them on variousexisting image quality assessment (IQA) datasets. Our evaluation across thethree abilities confirms that MLLMs possess fundamental low-level visualskills. However, these skills are still unstable and relatively imprecise,indicating the need for specific enhancements on MLLMs towards these abilities.We hope that our benchmark can encourage the research community to delve deeperto discover and enhance these untapped potentials of MLLMs.</description><author>Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin</author><pubDate>Mon, 25 Sep 2023 15:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14181v1</guid></item><item><title>Deformable Neural Radiance Fields using RGB and Event Cameras</title><link>http://arxiv.org/abs/2309.08416v2</link><description>Modeling Neural Radiance Fields for fast-moving deformable objects fromvisual data alone is a challenging problem. A major issue arises due to thehigh deformation and low acquisition rates. To address this problem, we proposeto use event cameras that offer very fast acquisition of visual change in anasynchronous manner. In this work, we develop a novel method to model thedeformable neural radiance fields using RGB and event cameras. The proposedmethod uses the asynchronous stream of events and calibrated sparse RGB frames.In our setup, the camera pose at the individual events required to integratethem into the radiance fields remains unknown. Our method jointly optimizesthese poses and the radiance field. This happens efficiently by leveraging thecollection of events at once and actively sampling the events during learning.Experiments conducted on both realistically rendered graphics and real-worlddatasets demonstrate a significant benefit of the proposed method over thestate-of-the-art and the compared baseline. This shows a promising direction for modeling deformable neural radiancefields in real-world dynamic scenes.</description><author>Qi Ma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool</author><pubDate>Mon, 25 Sep 2023 15:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08416v2</guid></item><item><title>Federated Learning Under Restricted User Availability</title><link>http://arxiv.org/abs/2309.14176v1</link><description>Federated Learning (FL) is a decentralized machine learning framework thatenables collaborative model training while respecting data privacy. In variousapplications, non-uniform availability or participation of users is unavoidabledue to an adverse or stochastic environment, the latter often beinguncontrollable during learning. Here, we posit a generic user selectionmechanism implementing a possibly randomized, stationary selection policy,suggestively termed as a Random Access Model (RAM). We propose a newformulation of the FL problem which effectively captures and mitigates limitedparticipation of data originating from infrequent, or restricted users, at thepresence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the(unknown) RAM distribution, we extend the expected loss FL objective to arisk-aware objective, enabling the design of an efficient training algorithmthat is completely oblivious to the RAM, and with essentially identicalcomplexity as FedAvg. Our experiments on synthetic and benchmark datasets showthat the proposed approach achieves significantly improved performance ascompared with standard FL, under a variety of setups.</description><author>Periklis Theodoropoulos, Konstantinos E. Nikolakakis, Dionysis Kalogerias</author><pubDate>Mon, 25 Sep 2023 15:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14176v1</guid></item><item><title>Only 5\% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation</title><link>http://arxiv.org/abs/2309.14174v1</link><description>Document-level Neural Machine Translation (DocNMT) has been proven crucialfor handling discourse phenomena by introducing document-level contextinformation. One of the most important directions is to input the wholedocument directly to the standard Transformer model. In this case, efficiencybecomes a critical concern due to the quadratic complexity of the attentionmodule. Existing studies either focus on the encoder part, which cannot bedeployed on sequence-to-sequence generation tasks, e.g., Machine Translation(MT), or suffer from a significant performance drop. In this work, we keep thetranslation performance while gaining 20\% speed up by introducing extraselection layer based on lightweight attention that selects a small portion oftokens to be attended. It takes advantage of the original attention to ensureperformance and dimension reduction to accelerate inference. Experimentalresults show that our method could achieve up to 95\% sparsity (only 5\% tokensattended) approximately, and save 93\% computation cost on the attention modulecompared with the original Transformer, while maintaining the performance.</description><author>Zihan Liu, Zewei Sun, Shanbo Cheng, Shujian Huang, Mingxuan Wang</author><pubDate>Mon, 25 Sep 2023 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14174v1</guid></item><item><title>Targeted demand response for flexible energy communities using clustering techniques</title><link>http://arxiv.org/abs/2303.00186v3</link><description>The present study proposes clustering techniques for designing demandresponse (DR) programs for commercial and residential prosumers. The goal is toalter the consumption behavior of the prosumers within a distributed energycommunity in Italy. This aggregation aims to: a) minimize the reverse powerflow at the primary substation, occuring when generation from solar panels inthe local grid exceeds consumption, and b) shift the system wide peak demand,that typically occurs during late afternoon. Regarding the clustering stage, weconsider daily prosumer load profiles and divide them across the extractedclusters. Three popular machine learning algorithms are employed, namelyk-means, k-medoids and agglomerative clustering. We evaluate the methods usingmultiple metrics including a novel metric proposed within this study, namelypeak performance score (PPS). The k-means algorithm with dynamic time warpingdistance considering 14 clusters exhibits the highest performance with a PPS of0.689. Subsequently, we analyze each extracted cluster with respect to loadshape, entropy, and load types. These characteristics are used to distinguishthe clusters that have the potential to serve the optimization objectives bymatching them to proper DR schemes including time of use, critical peakpricing, and real-time pricing. Our results confirm the effectiveness of theproposed clustering algorithm in generating meaningful flexibility clusters,while the derived DR pricing policy encourages consumption during off-peakhours. The developed methodology is robust to the low availability and qualityof training datasets and can be used by aggregator companies for segmentingenergy communities and developing personalized DR policies.</description><author>Sotiris Pelekis, Angelos Pipergias, Evangelos Karakolis, Spiros Mouzakitis, Francesca Santori, Mohammad Ghoreishi, Dimitris Askounis</author><pubDate>Mon, 25 Sep 2023 15:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00186v3</guid></item><item><title>Towards Comprehensive Monocular Depth Estimation: Multiple Heads Are Better Than One</title><link>http://arxiv.org/abs/2111.08313v2</link><description>Depth estimation attracts widespread attention in the computer visioncommunity. However, it is still quite difficult to recover an accurate depthmap using only one RGB image. We observe a phenomenon that existing methodstend to fail in different cases, caused by differences in network architecture,loss function and so on. In this work, we investigate into the phenomenon andpropose to integrate the strengths of multiple weak depth predictor to build acomprehensive and accurate depth predictor, which is critical for manyreal-world applications, e.g., 3D reconstruction. Specifically, we constructmultiple base (weak) depth predictors by utilizing different Transformer-basedand convolutional neural network (CNN)-based architectures. Transformerestablishes long-range correlation while CNN preserves local informationignored by Transformer due to the spatial inductive bias. Therefore, thecoupling of Transformer and CNN contributes to the generation of complementarydepth estimates, which are essential to achieve a comprehensive depthpredictor. Then, we design mixers to learn from multiple weak predictions andadaptively fuse them into a strong depth estimate. The resultant model, whichwe refer to as Transformer-assisted depth ensembles (TEDepth). On the standardNYU-Depth-v2 and KITTI datasets, we thoroughly explore how the neural ensemblesaffect the depth estimation and demonstrate that our TEDepth achieves betterresults than previous state-of-the-art approaches. To validate thegeneralizability across cameras, we directly apply the models trained onNYU-Depth-v2 to the SUN RGB-D dataset without any fine-tuning, and the superiorresults emphasize its strong generalizability.</description><author>Shuwei Shao, Ran Li, Zhongcai Pei, Zhong Liu, Weihai Chen, Wentao Zhu, Xingming Wu, Baochang Zhang</author><pubDate>Mon, 25 Sep 2023 15:29:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08313v2</guid></item><item><title>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</title><link>http://arxiv.org/abs/2307.12032v2</link><description>Air transport poses significant environmental challenges, particularlyregarding the role of flight contrails in climate change due to their potentialglobal warming impact. Traditional computer vision techniques struggle undervarying remote sensing image conditions, and conventional machine learningapproaches using convolutional neural networks are limited by the scarcity ofhand-labeled contrail datasets. To address these issues, we employ few-shottransfer learning to introduce an innovative approach for accurate contrailsegmentation with minimal labeled data. Our methodology leverages backbonesegmentation models pre-trained on extensive image datasets and fine-tunedusing an augmented contrail-specific dataset. We also introduce a novel lossfunction, termed SR Loss, which enhances contrail line detection bytransforming the image space into Hough space. This transformation results in asignificant performance improvement over generic image segmentation lossfunctions. Our approach offers a robust solution to the challenges posed bylimited labeled data and significantly advances the state of contrail detectionmodels.</description><author>Junzi Sun, Esther Roosenbrand</author><pubDate>Mon, 25 Sep 2023 15:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12032v2</guid></item><item><title>GECCO: Geometrically-Conditioned Point Diffusion Models</title><link>http://arxiv.org/abs/2303.05916v2</link><description>Diffusion models generating images conditionally on text, such as Dall-E 2and Stable Diffusion, have recently made a splash far beyond the computervision community. Here, we tackle the related problem of generating pointclouds, both unconditionally, and conditionally with images. For the latter, weintroduce a novel geometrically-motivated conditioning scheme based onprojecting sparse image features into the point cloud and attaching them toeach individual point, at every step in the denoising process. This approachimproves geometric consistency and yields greater fidelity than current methodsrelying on unstructured, global latent codes. Additionally, we show how toapply recent continuous-time diffusion schemes. Our method performs on par orabove the state of art on conditional and unconditional experiments onsynthetic data, while being faster, lighter, and delivering tractablelikelihoods. We show it can also scale to diverse indoors scenes.</description><author>Michał J. Tyszkiewicz, Pascal Fua, Eduard Trulls</author><pubDate>Mon, 25 Sep 2023 15:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05916v2</guid></item><item><title>OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios</title><link>http://arxiv.org/abs/2309.11858v2</link><description>Recently, linear computed tomography (LCT) systems have actively attractedattention. To weaken projection truncation and image the region of interest(ROI) for LCT, the backprojection filtration (BPF) algorithm is an effectivesolution. However, in BPF for LCT, it is difficult to achieve stable interiorreconstruction, and for differentiated backprojection (DBP) images of LCT,multiple rotation-finite inversion of Hilbert transform (Hilbertfiltering)-inverse rotation operations will blur the image. To satisfy multiplereconstruction scenarios for LCT, including interior ROI, complete object, andexterior region beyond field-of-view (FOV), and avoid the rotation operationsof Hilbert filtering, we propose two types of reconstruction architectures. Thefirst overlays multiple DBP images to obtain a complete DBP image, then uses anetwork to learn the overlying Hilbert filtering function, referred to as theOverlay-Single Network (OSNet). The second uses multiple networks to traindifferent directional Hilbert filtering models for DBP images of multiplelinear scannings, respectively, and then overlays the reconstructed results,i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introducea Swin Transformer (ST) block to the generator of pix2pixGAN to extract bothlocal and global features from DBP images at the same time. We investigate twoarchitectures from different networks, FOV sizes, pixel sizes, number ofprojections, geometric magnification, and processing time. Experimental resultsshow that two architectures can both recover images. OSNet outperforms BPF invarious scenarios. For the different networks, ST-pix2pixGAN is superior topix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differencesamong the multiple models, but any one of its models is suitable for imagingthe exterior edge in a certain direction.</description><author>Zhisheng Wang, Zihan Deng, Fenglin Liu, Yixing Huang, Haijun Yu, Junning Cui</author><pubDate>Mon, 25 Sep 2023 15:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11858v2</guid></item><item><title>Where Am I Now? Dynamically Finding Optimal Sensor States to Minimize Localization Uncertainty for a Perception-Denied Rover</title><link>http://arxiv.org/abs/2211.16721v2</link><description>We present DyFOS, an active perception method that dynamically finds optimalstates to minimize localization uncertainty while avoiding obstacles andocclusions. We consider the scenario where a perception-denied rover relies onposition and uncertainty measurements from a viewer robot to localize itselfalong an obstacle-filled path. The position uncertainty from the viewer'ssensor is a function of the states of the sensor itself, the rover, and thesurrounding environment. To find an optimal sensor state that minimizes therover's localization uncertainty, DyFOS uses a localization uncertaintyprediction pipeline in an optimization search. Given numerous samples of thestates mentioned above, the pipeline predicts the rover's localizationuncertainty with the help of a trained, complex state-dependent sensormeasurement model (a probabilistic neural network). Our pipeline also predictsocclusion and obstacle collision to remove undesirable viewer states and reduceunnecessary computations. We evaluate the proposed method numerically and insimulation. Our results show that DyFOS is faster than brute force yet performson par. DyFOS also yielded lower localization uncertainties than faster randomand heuristic-based searches.</description><author>Troi Williams, Po-Lun Chen, Sparsh Bhogavilli, Vaibhav Sanjay, Pratap Tokekar</author><pubDate>Mon, 25 Sep 2023 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16721v2</guid></item><item><title>Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices</title><link>http://arxiv.org/abs/2309.14165v1</link><description>Semantic parsing of user-generated instructional text, in the way of enablingend-users to program the Internet of Things (IoT), is an underexplored area. Inthis study, we provide a unique annotated corpus which aims to support thetransformation of cooking recipe instructions to machine-understandablecommands for IoT devices in the kitchen. Each of these commands is a tuplecapturing the semantics of an instruction involving a kitchen device in termsof "What", "Where", "Why" and "How". Based on this corpus, we developed machinelearning-based sequence labelling methods, namely conditional random fields(CRF) and a neural network model, in order to parse recipe instructions andextract our tuples of interest from them. Our results show that while it isfeasible to train semantic parsers based on our annotations, mostnatural-language instructions are incomplete, and thus transforming them intoformal meaning representation, is not straightforward.</description><author>Filippos Ventirozos, Sarah Clinch, Riza Batista-Navarro</author><pubDate>Mon, 25 Sep 2023 15:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14165v1</guid></item><item><title>Data Upcycling Knowledge Distillation for Image Super-Resolution</title><link>http://arxiv.org/abs/2309.14162v1</link><description>Knowledge distillation (KD) emerges as a challenging yet promising techniquefor compressing deep learning models, characterized by the transmission ofextensive learning representations from proficient and computationallyintensive teacher models to compact student models. However, only a handful ofstudies have endeavored to compress the models for single imagesuper-resolution (SISR) through KD, with their effects on student modelenhancement remaining marginal. In this paper, we put forth an approach fromthe perspective of efficient data utilization, namely, the Data UpcyclingKnowledge Distillation (DUKD) which facilitates the student model by the priorknowledge teacher provided via upcycled in-domain data derived from theirinputs. This upcycling process is realized through two efficient image zoomingoperations and invertible data augmentations which introduce the labelconsistency regularization to the field of KD for SISR and substantially boostsstudent model's generalization. The DUKD, due to its versatility, can beapplied across a broad spectrum of teacher-student architectures. Comprehensiveexperiments across diverse benchmarks demonstrate that our proposed DUKD methodsignificantly outperforms previous art, exemplified by an increase of up to0.5dB in PSNR over baselines methods, and a 67% parameters reduced RCAN model'sperformance remaining on par with that of the RCAN teacher model.</description><author>Yun Zhang, Wei Li, Simiao Li, Jie Hu, Hanting Chen, Hailing Wang, Zhijun Tu, Wenjia Wang, Bingyi Jing, Yunhe Wang</author><pubDate>Mon, 25 Sep 2023 15:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14162v1</guid></item><item><title>Onion Universe Algorithm: Applications in Weakly Supervised Learning</title><link>http://arxiv.org/abs/2307.04870v2</link><description>We introduce Onion Universe Algorithm (OUA), a novel classification method inensemble learning. In particular, we show its applicability as a label modelfor weakly supervised learning. OUA offers simplicity in implementation withminimal assumptions on the data or weak signals. The model is well suited forscenarios where fully labeled data is not available. Our method is built upongeometrical interpretation of the space spanned by weak signals. Our analysisof the high dimensional convex hull structure underlying general set of weaksignals bridges geometry with machine learning. Empirical results alsodemonstrate that OUA works well in practice and compares favorably to bestexisting label models for weakly supervised learning.</description><author>Woojoo Na</author><pubDate>Mon, 25 Sep 2023 15:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04870v2</guid></item><item><title>LAPP: Layer Adaptive Progressive Pruning for Compressing CNNs from Scratch</title><link>http://arxiv.org/abs/2309.14157v1</link><description>Structured pruning is a commonly used convolutional neural network (CNN)compression approach. Pruning rate setting is a fundamental problem instructured pruning. Most existing works introduce too many additional learnableparameters to assign different pruning rates across different layers in CNN orcannot control the compression rate explicitly. Since too narrow network blocksinformation flow for training, automatic pruning rate setting cannot explore ahigh pruning rate for a specific layer. To overcome these limitations, wepropose a novel framework named Layer Adaptive Progressive Pruning (LAPP),which gradually compresses the network during initial training of a few epochsfrom scratch. In particular, LAPP designs an effective and efficient pruningstrategy that introduces a learnable threshold for each layer and FLOPsconstraints for network. Guided by both task loss and FLOPs constraints, thelearnable thresholds are dynamically and gradually updated to accommodatechanges of importance scores during training. Therefore the pruning strategycan gradually prune the network and automatically determine the appropriatepruning rates for each layer. What's more, in order to maintain the expressivepower of the pruned layer, before training starts, we introduce an additionallightweight bypass for each convolutional layer to be pruned, which only addsrelatively few additional burdens. Our method demonstrates superior performancegains over previous compression methods on various datasets and backbonearchitectures. For example, on CIFAR-10, our method compresses ResNet-20 to40.3% without accuracy drop. 55.6% of FLOPs of ResNet-18 are reduced with 0.21%top-1 accuracy increase and 0.40% top-5 accuracy increase on ImageNet.</description><author>Pucheng Zhai, Kailing Guo, Fang Liu, Xiaofen Xing, Xiangmin Xu</author><pubDate>Mon, 25 Sep 2023 15:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14157v1</guid></item><item><title>Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials</title><link>http://arxiv.org/abs/2309.14156v1</link><description>Personalized adaptive interventions offer the opportunity to increase patientbenefits, however, there are challenges in their planning and implementation.Once implemented, it is an important question whether personalized adaptiveinterventions are indeed clinically more effective compared to a fixed goldstandard intervention. In this paper, we present an innovative N-of-1 trialstudy design testing whether implementing a personalized intervention by anonline reinforcement learning agent is feasible and effective. Throughout, weuse a new study on physical exercise recommendations to reduce pain inendometriosis for illustration. We describe the design of a contextual banditrecommendation agent and evaluate the agent in simulation studies. The resultsshow that adaptive interventions add complexity to the design andimplementation process, but have the potential to improve patients' benefitseven if only few observations are available. In order to quantify the expectedbenefit, data from previous interventional studies is required. We expect ourapproach to be transferable to other interventions and clinical interventions.</description><author>Dominik Meier, Ipek Ensari, Stefan Konigorski</author><pubDate>Mon, 25 Sep 2023 15:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14156v1</guid></item><item><title>Extragradient Type Methods for Riemannian Variational Inequality Problems</title><link>http://arxiv.org/abs/2309.14155v1</link><description>Riemannian convex optimization and minimax optimization have recently drawnconsiderable attention. Their appeal lies in their capacity to adeptly managethe non-convexity of the objective function as well as constraints inherent inthe feasible set in the Euclidean sense. In this work, we delve into monotoneRiemannian Variational Inequality Problems (RVIPs), which encompass bothRiemannian convex optimization and minimax optimization as particular cases. Inthe context of Euclidean space, it is established that the last-iterates ofboth the extragradient (EG) and past extragradient (PEG) methods converge tothe solution of monotone variational inequality problems at a rate of$O\left(\frac{1}{\sqrt{T}}\right)$ (Cai et al., 2022). However, analogousbehavior on Riemannian manifolds remains an open question. To bridge this gap,we introduce the Riemannian extragradient (REG) and Riemannian pastextragradient (RPEG) methods. We demonstrate that both exhibit$O\left(\frac{1}{\sqrt{T}}\right)$ last-iterate convergence. Additionally, weshow that the average-iterate convergence of both REG and RPEG is$O\left(\frac{1}{{T}}\right)$, aligning with observations in the Euclidean case(Mokhtari et al., 2020). These results are enabled by judiciously addressingthe holonomy effect so that additional complications in Riemannian cases can bereduced and the Euclidean proof inspired by the performance estimation problem(PEP) technique or the sum-of-squares (SOS) technique can be applied again.</description><author>Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob Abernethy, Molei Tao</author><pubDate>Mon, 25 Sep 2023 15:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14155v1</guid></item><item><title>Dynamic Implicit Image Function for Efficient Arbitrary-Scale Image Representation</title><link>http://arxiv.org/abs/2306.12321v2</link><description>Recent years have witnessed the remarkable success of implicit neuralrepresentation methods. The recent work Local Implicit Image Function (LIIF)has achieved satisfactory performance for continuous image representation,where pixel values are inferred from a neural network in a continuous spatialdomain. However, the computational cost of such implicit arbitrary-scalesuper-resolution (SR) methods increases rapidly as the scale factor increases,which makes arbitrary-scale SR time-consuming. In this paper, we proposeDynamic Implicit Image Function (DIIF), which is a fast and efficient method torepresent images with arbitrary resolution. Instead of taking an imagecoordinate and the nearest 2D deep features as inputs to predict its pixelvalue, we propose a coordinate grouping and slicing strategy, which enables theneural network to perform decoding from coordinate slices to pixel valueslices. We further propose a Coarse-to-Fine Multilayer Perceptron (C2F-MLP) toperform decoding with dynamic coordinate slicing, where the number ofcoordinates in each slice varies as the scale factor varies. With dynamiccoordinate slicing, DIIF significantly reduces the computational cost whenencountering arbitrary-scale SR. Experimental results demonstrate that DIIF canbe integrated with implicit arbitrary-scale SR methods and achieves SOTA SRperformance with significantly superior computational efficiency, therebyopening a path for real-time arbitrary-scale image representation. Our code canbe found at https://github.com/HeZongyao/DIIF.</description><author>Zongyao He, Zhi Jin</author><pubDate>Mon, 25 Sep 2023 15:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12321v2</guid></item><item><title>SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture</title><link>http://arxiv.org/abs/2309.14148v1</link><description>The advent of serverless computing has ushered in notable advancements indistributed machine learning, particularly within parameter server-basedarchitectures. Yet, the integration of serverless features within peer-to-peer(P2P) distributed networks remains largely uncharted. In this paper, weintroduce SPIRT, a fault-tolerant, reliable, and secure serverless P2P MLtraining architecture. designed to bridge this existing gap. Capitalizing on the inherent robustness and reliability innate to P2Psystems, SPIRT employs RedisAI for in-database operations, leading to an 82\%reduction in the time required for model updates and gradient averaging acrossa variety of models and batch sizes. This architecture showcases resilienceagainst peer failures and adeptly manages the integration of new peers, therebyhighlighting its fault-tolerant characteristics and scalability. Furthermore,SPIRT ensures secure communication between peers, enhancing the reliability ofdistributed machine learning tasks. Even in the face of Byzantine attacks, thesystem's robust aggregation algorithms maintain high levels of accuracy. Thesefindings illuminate the promising potential of serverless architectures in P2Pdistributed machine learning, offering a significant stride towards thedevelopment of more efficient, scalable, and resilient applications.</description><author>Amine Barrak, Mayssa Jaziri, Ranim Trabelsi, Fehmi Jaafar, Fabio Petrillo</author><pubDate>Mon, 25 Sep 2023 15:01:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14148v1</guid></item><item><title>Examining Temporal Bias in Abusive Language Detection</title><link>http://arxiv.org/abs/2309.14146v1</link><description>The use of abusive language online has become an increasingly pervasiveproblem that damages both individuals and society, with effects ranging frompsychological harm right through to escalation to real-life violence and evendeath. Machine learning models have been developed to automatically detectabusive language, but these models can suffer from temporal bias, thephenomenon in which topics, language use or social norms change over time. Thisstudy aims to investigate the nature and impact of temporal bias in abusivelanguage detection across various languages and explore mitigation methods. Weevaluate the performance of models on abusive data sets from different timeperiods. Our results demonstrate that temporal bias is a significant challengefor abusive language detection, with models trained on historical data showinga significant drop in performance over time. We also present an extensivelinguistic analysis of these abusive data sets from a diachronic perspective,aiming to explore the reasons for language evolution and performance decline.This study sheds light on the pervasive issue of temporal bias in abusivelanguage detection across languages, offering crucial insights into languageevolution and temporal bias mitigation.</description><author>Mali Jin, Yida Mu, Diana Maynard, Kalina Bontcheva</author><pubDate>Mon, 25 Sep 2023 14:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14146v1</guid></item><item><title>A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance</title><link>http://arxiv.org/abs/2301.03283v3</link><description>Model transparency, label correlation learning and the robust-ness to labelnoise are crucial for multilabel learning. However, few existing methods studythese three characteristics simultaneously. To address this challenge, wepropose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) withthree mechanisms. First, we design a soft label learning mechanism to reducethe effect of label noise by explicitly measuring the interactions betweenlabels, which is also the basis of the other two mechanisms. Second, therule-based TSK FS is used as the base model to efficiently model the inferencerelationship be-tween features and soft labels in a more transparent way thanmany existing multilabel models. Third, to further improve the performance ofmultilabel learning, we build a correlation enhancement learning mechanismbased on the soft label space and the fuzzy feature space. Extensiveexperiments are conducted to demonstrate the superiority of the proposedmethod.</description><author>Qiongdan Lou, Zhaohong Deng, Kup-Sze Choi, Shitong Wang</author><pubDate>Mon, 25 Sep 2023 14:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03283v3</guid></item><item><title>A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers</title><link>http://arxiv.org/abs/2302.12168v2</link><description>Short-term load forecasting (STLF) is vital for the effective and economicoperation of power grids and energy markets. However, the non-linearity andnon-stationarity of electricity demand as well as its dependency on variousexternal factors renders STLF a challenging task. To that end, several deeplearning models have been proposed in the literature for STLF, reportingpromising results. In order to evaluate the accuracy of said models inday-ahead forecasting settings, in this paper we focus on the national netaggregated STLF of Portugal and conduct a comparative study considering a setof indicative, well-established deep autoregressive models, namely multi-layerperceptrons (MLP), long short-term memory networks (LSTM), neural basisexpansion coefficient analysis (N-BEATS), temporal convolutional networks(TCN), and temporal fusion transformers (TFT). Moreover, we identify factorsthat significantly affect the demand and investigate their impact on theaccuracy of each model. Our results suggest that N-BEATS consistentlyoutperforms the rest of the examined models. MLP follows, providing furtherevidence towards the use of feed-forward networks over relatively moresophisticated architectures. Finally, certain calendar and weather featureslike the hour of the day and the temperature are identified as key accuracydrivers, providing insights regarding the forecasting approach that should beused per case.</description><author>Sotiris Pelekis, Ioannis-Konstantinos Seisopoulos, Evangelos Spiliotis, Theodosios Pountridis, Evangelos Karakolis, Spiros Mouzakitis, Dimitris Askounis</author><pubDate>Mon, 25 Sep 2023 14:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12168v2</guid></item><item><title>Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning</title><link>http://arxiv.org/abs/2309.14139v1</link><description>The increasing demand for computational power in big data and machinelearning has driven the development of distributed training methodologies.Among these, peer-to-peer (P2P) networks provide advantages such as enhancedscalability and fault tolerance. However, they also encounter challengesrelated to resource consumption, costs, and communication overhead as thenumber of participating peers grows. In this paper, we introduce a novelarchitecture that combines serverless computing with P2P networks fordistributed training and present a method for efficient parallel gradientcomputation under resource constraints. Our findings show a significant enhancement in gradient computation time,with up to a 97.34\% improvement compared to conventional P2P distributedtraining methods. As for costs, our examination confirmed that the serverlessarchitecture could incur higher expenses, reaching up to 5.4 times more thaninstance-based architectures. It is essential to consider that these highercosts are associated with marked improvements in computation time, particularlyunder resource-constrained scenarios. Despite the cost-time trade-off, theserverless approach still holds promise due to its pay-as-you-go model.Utilizing dynamic resource allocation, it enables faster training times andoptimized resource utilization, making it a promising candidate for a widerange of machine learning applications.</description><author>Amine Barral, Ranim Trabelsi, Fehmi Jaafar, Fabio Petrillo</author><pubDate>Mon, 25 Sep 2023 14:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14139v1</guid></item></channel></rss>