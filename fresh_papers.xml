<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 19 May 2024 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Toon3D: Seeing Cartoons from a New Perspective</title><link>http://arxiv.org/abs/2405.10320v1</link><description>In this work, we recover the underlying 3D structure of non-geometricallyconsistent scenes. We focus our analysis on hand-drawn images from cartoons andanime. Many cartoons are created by artists without a 3D rendering engine,which means that any new image of a scene is hand-drawn. The hand-drawn imagesare usually faithful representations of the world, but only in a qualitativesense, since it is difficult for humans to draw multiple perspectives of anobject or scene 3D consistently. Nevertheless, people can easily perceive 3Dscenes from inconsistent inputs! In this work, we correct for 2D drawinginconsistencies to recover a plausible 3D structure such that the newly warpeddrawings are consistent with each other. Our pipeline consists of auser-friendly annotation tool, camera pose estimation, and image deformation torecover a dense structure. Our method warps images to obey a perspective cameramodel, enabling our aligned results to be plugged into novel-view synthesisreconstruction methods to experience cartoons from viewpoints never drawnbefore. Our project page is https://toon3d.studio/.</description><author>Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Angjoo Kanazawa</author><pubDate>Thu, 16 May 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10320v1</guid></item><item><title>Text-to-Vector Generation with Neural Path Representation</title><link>http://arxiv.org/abs/2405.10317v1</link><description>Vector graphics are widely used in digital art and highly favored bydesigners due to their scalability and layer-wise properties. However, theprocess of creating and editing vector graphics requires creativity and designexpertise, making it a time-consuming task. Recent advancements intext-to-vector (T2V) generation have aimed to make this process moreaccessible. However, existing T2V methods directly optimize control points ofvector graphics paths, often resulting in intersecting or jagged paths due tothe lack of geometry constraints. To overcome these limitations, we propose anovel neural path representation by designing a dual-branch VariationalAutoencoder (VAE) that learns the path latent space from both sequence andimage modalities. By optimizing the combination of neural paths, we canincorporate geometric constraints while preserving expressivity in generatedSVGs. Furthermore, we introduce a two-stage path optimization method to improvethe visual and topological quality of generated SVGs. In the first stage, apre-trained text-to-image diffusion model guides the initial generation ofcomplex vector graphics through the Variational Score Distillation (VSD)process. In the second stage, we refine the graphics using a layer-wise imagevectorization strategy to achieve clearer elements and structure. Wedemonstrate the effectiveness of our method through extensive experiments andshowcase various applications. The project page ishttps://intchous.github.io/T2V-NPR.</description><author>Peiying Zhang, Nanxuan Zhao, Jing Liao</author><pubDate>Thu, 16 May 2024 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10317v1</guid></item><item><title>Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model</title><link>http://arxiv.org/abs/2405.10316v1</link><description>Visual In-Context Learning (ICL) has emerged as a promising research area dueto its capability to accomplish various tasks with limited example pairsthrough analogical reasoning. However, training-based visual ICL haslimitations in its ability to generalize to unseen tasks and requires thecollection of a diverse task dataset. On the other hand, existing methods inthe inference-based visual ICL category solely rely on textual prompts, whichfail to capture fine-grained contextual information from given examples and canbe time-consuming when converting from images to text prompts. To address thesechallenges, we propose Analogist, a novel inference-based visual ICL approachthat exploits both visual and textual prompting techniques using atext-to-image diffusion model pretrained for image inpainting. For visualprompting, we propose a self-attention cloning (SAC) method to guide thefine-grained structural-level analogy between image examples. For textualprompting, we leverage GPT-4V's visual reasoning capability to efficientlygenerate text prompts and introduce a cross-attention masking (CAM) operationto enhance the accuracy of semantic-level analogy guided by text prompts. Ourmethod is out-of-the-box and does not require fine-tuning or optimization. Itis also generic and flexible, enabling a wide range of visual tasks to beperformed in an in-context manner. Extensive experiments demonstrate thesuperiority of our method over existing approaches, both qualitatively andquantitatively.</description><author>Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao</author><pubDate>Thu, 16 May 2024 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10316v1</guid></item><item><title>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</title><link>http://arxiv.org/abs/2405.10315v1</link><description>Learning in simulation and transferring the learned policy to the real worldhas the potential to enable generalist robots. The key challenge of thisapproach is to address simulation-to-reality (sim-to-real) gaps. Previousmethods often require domain-specific knowledge a priori. We argue that astraightforward way to obtain such knowledge is by asking humans to observe andassist robot policy execution in the real world. The robots can then learn fromhumans to close various sim-to-real gaps. We propose TRANSIC, a data-drivenapproach to enable successful sim-to-real transfer based on a human-in-the-loopframework. TRANSIC allows humans to augment simulation policies to overcomevarious unmodeled sim-to-real gaps holistically through intervention and onlinecorrection. Residual policies can be learned from human corrections andintegrated with simulation policies for autonomous execution. We show that ourapproach can achieve successful sim-to-real transfer in complex andcontact-rich manipulation tasks such as furniture assembly. Through synergisticintegration of policies learned in simulation and from humans, TRANSIC iseffective as a holistic approach to addressing various, often coexistingsim-to-real gaps. It displays attractive properties such as scaling with humaneffort. Videos and code are available at https://transic-robot.github.io/</description><author>Yunfan Jiang, Chen Wang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei</author><pubDate>Thu, 16 May 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10315v1</guid></item><item><title>CAT3D: Create Anything in 3D with Multi-View Diffusion Models</title><link>http://arxiv.org/abs/2405.10314v1</link><description>Advances in 3D reconstruction have enabled high-quality 3D capture, butrequire a user to collect hundreds to thousands of images to create a 3D scene.We present CAT3D, a method for creating anything in 3D by simulating thisreal-world capture process with a multi-view diffusion model. Given any numberof input images and a set of target novel viewpoints, our model generateshighly consistent novel views of a scene. These generated views can be used asinput to robust 3D reconstruction techniques to produce 3D representations thatcan be rendered from any viewpoint in real-time. CAT3D can create entire 3Dscenes in as little as one minute, and outperforms existing methods for singleimage and few-view 3D scene creation. See our project page for results andinteractive demos at https://cat3d.github.io .</description><author>Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron, Ben Poole</author><pubDate>Thu, 16 May 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10314v1</guid></item><item><title>How Far Are We From AGI</title><link>http://arxiv.org/abs/2405.10313v1</link><description>The evolution of artificial intelligence (AI) has profoundly impacted humansociety, driving significant advancements in multiple sectors. Yet, theescalating demands on AI have highlighted the limitations of AI's currentofferings, catalyzing a movement towards Artificial General Intelligence (AGI).AGI, distinguished by its ability to execute diverse real-world tasks withefficiency and effectiveness comparable to human intelligence, reflects aparamount milestone in AI evolution. While existing works have summarizedspecific recent advancements of AI, they lack a comprehensive discussion ofAGI's definitions, goals, and developmental trajectories. Different fromexisting survey papers, this paper delves into the pivotal questions of ourproximity to AGI and the strategies necessary for its realization throughextensive surveys, discussions, and original perspectives. We start byarticulating the requisite capability frameworks for AGI, integrating theinternal, interface, and system dimensions. As the realization of AGI requiresmore advanced capabilities and adherence to stringent constraints, we furtherdiscuss necessary AGI alignment technologies to harmonize these factors.Notably, we emphasize the importance of approaching AGI responsibly by firstdefining the key levels of AGI progression, followed by the evaluationframework that situates the status-quo, and finally giving our roadmap of howto reach the pinnacle of AGI. Moreover, to give tangible insights into theubiquitous impact of the integration of AI, we outline existing challenges andpotential pathways toward AGI in multiple domains. In sum, serving as apioneering exploration into the current state and future trajectory of AGI,this paper aims to foster a collective comprehension and catalyze broaderpublic discussions among researchers and practitioners on AGI.</description><author>Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, Jiaxuan You</author><pubDate>Thu, 16 May 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10313v1</guid></item><item><title>Stochastic Q-learning for Large Discrete Action Spaces</title><link>http://arxiv.org/abs/2405.10310v1</link><description>In complex environments with large discrete action spaces, effectivedecision-making is critical in reinforcement learning (RL). Despite thewidespread use of value-based RL approaches like Q-learning, they come with acomputational burden, necessitating the maximization of a value function overall actions in each iteration. This burden becomes particularly challengingwhen addressing large-scale problems and using deep neural networks as functionapproximators. In this paper, we present stochastic value-based RL approacheswhich, in each iteration, as opposed to optimizing over the entire set of $n$actions, only consider a variable stochastic set of a sublinear number ofactions, possibly as small as $\mathcal{O}(\log(n))$. The presented stochasticvalue-based RL methods include, among others, Stochastic Q-learning, StochDQN,and StochDDQN, all of which integrate this stochastic approach for bothvalue-function updates and action selection. The theoretical convergence ofStochastic Q-learning is established, while an analysis of stochasticmaximization is provided. Moreover, through empirical validation, we illustratethat the various proposed approaches outperform the baseline methods acrossdiverse environments, including different control problems, achievingnear-optimal average returns in significantly reduced time.</description><author>Fares Fourati, Vaneet Aggarwal, Mohamed-Slim Alouini</author><pubDate>Thu, 16 May 2024 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10310v1</guid></item><item><title>4D Panoptic Scene Graph Generation</title><link>http://arxiv.org/abs/2405.10305v1</link><description>We are living in a three-dimensional space while moving forward through afourth dimension: time. To allow artificial intelligence to develop acomprehensive understanding of such a 4D environment, we introduce 4D PanopticScene Graph (PSG-4D), a new representation that bridges the raw visual dataperceived in a dynamic 4D world and high-level visual understanding.Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represententities with precise location and status information, and edges, which capturethe temporal relations. To facilitate research in this new area, we build arichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of1M frames, each of which is labeled with 4D panoptic segmentation masks as wellas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,a Transformer-based model that can predict panoptic segmentation masks, trackmasks along the time axis, and generate the corresponding scene graphs via arelation component. Extensive experiments on the new dataset show that ourmethod can serve as a strong baseline for future research on PSG-4D. In theend, we provide a real-world application example to demonstrate how we canachieve dynamic scene understanding by integrating a large language model intoour PSG-4D system.</description><author>Jingkang Yang, Jun Cen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, Ziwei Liu</author><pubDate>Thu, 16 May 2024 18:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10305v1</guid></item><item><title>Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title><link>http://arxiv.org/abs/2405.10302v1</link><description>As machine learning models are increasingly deployed in dynamic environments,it becomes paramount to assess and quantify uncertainties associated withdistribution shifts. A distribution shift occurs when the underlyingdata-generating process changes, leading to a deviation in the model'sperformance. The prediction interval, which captures the range of likelyoutcomes for a given prediction, serves as a crucial tool for characterizinguncertainties induced by their underlying distribution. In this paper, wepropose methodologies for aggregating prediction intervals to obtain one withminimal width and adequate coverage on the target domain under unsuperviseddomain shift, under which we have labeled samples from a related source domainand unlabeled covariates from the target domain. Our analysis encompassesscenarios where the source and the target domain are related via i) a boundeddensity ratio, and ii) a measure-preserving transformation. Our proposedmethodologies are computationally efficient and easy to implement. Beyondillustrating the performance of our method through a real-world dataset, wealso delve into the theoretical details. This includes establishing rigoroustheoretical guarantees, coupled with finite sample bounds, regarding thecoverage and width of our prediction intervals. Our approach excels inpractical applications and is underpinned by a solid theoretical framework,ensuring its reliability and effectiveness across diverse contexts.</description><author>Jiawei Ge, Debarghya Mukherjee, Jianqing Fan</author><pubDate>Thu, 16 May 2024 18:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10302v1</guid></item><item><title>Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees</title><link>http://arxiv.org/abs/2405.10301v1</link><description>Before deploying outputs from foundation models in high-stakes tasks, it isimperative to ensure that they align with human values. For instance, inradiology report generation, reports generated by a vision-language model mustalign with human evaluations before their use in medical decision-making. Thispaper presents Conformal Alignment, a general framework for identifying unitswhose outputs meet a user-specified alignment criterion. It is guaranteed thaton average, a prescribed fraction of selected units indeed meet the alignmentcriterion, regardless of the foundation model or the data distribution. Givenany pre-trained model and new units with model-generated outputs, ConformalAlignment leverages a set of reference data with ground-truth alignment statusto train an alignment predictor. It then selects new units whose predictedalignment scores surpass a data-dependent threshold, certifying theircorresponding outputs as trustworthy. Through applications to questionanswering and radiology report generation, we demonstrate that our method isable to accurately identify units with trustworthy outputs via lightweighttraining over a moderate amount of reference data. En route, we investigate theinformativeness of various features in alignment prediction and combine themwith standard models to construct the alignment predictor.</description><author>Yu Gui, Ying Jin, Zhimei Ren</author><pubDate>Thu, 16 May 2024 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10301v1</guid></item><item><title>Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection</title><link>http://arxiv.org/abs/2405.10300v1</link><description>This paper introduces Grounding DINO 1.5, a suite of advanced open-set objectdetection models developed by IDEA Research, which aims to advance the "Edge"of open-set object detection. The suite encompasses two models: Grounding DINO1.5 Pro, a high-performance model designed for stronger generalizationcapability across a wide range of scenarios, and Grounding DINO 1.5 Edge, anefficient model optimized for faster speed demanded in many applicationsrequiring edge deployment. The Grounding DINO 1.5 Pro model advances itspredecessor by scaling up the model architecture, integrating an enhancedvision backbone, and expanding the training dataset to over 20 million imageswith grounding annotations, thereby achieving a richer semantic understanding.The Grounding DINO 1.5 Edge model, while designed for efficiency with reducedfeature scales, maintains robust detection capabilities by being trained on thesame comprehensive dataset. Empirical results demonstrate the effectiveness ofGrounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 APon the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shottransfer benchmark, setting new records for open-set object detection.Furthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT,achieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 APon the LVIS-minival benchmark, making it more suitable for edge computingscenarios. Model examples and demos with API will be released athttps://github.com/IDEA-Research/Grounding-DINO-1.5-API</description><author>Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, Lei Zhang</author><pubDate>Thu, 16 May 2024 18:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10300v1</guid></item><item><title>HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models</title><link>http://arxiv.org/abs/2405.10299v1</link><description>The expanding size of language models has created the necessity for acomprehensive examination across various dimensions that reflect the desideratawith respect to the tradeoffs between various hardware metrics, such aslatency, energy consumption, GPU memory usage, and performance. There is agrowing interest in establishing Pareto frontiers for different language modelconfigurations to identify optimal models with specified hardware constraints.Notably, architectures that excel in latency on one device may not performoptimally on another. However, exhaustive training and evaluation of numerousarchitectures across diverse hardware configurations is computationallyprohibitive. To this end, we propose HW-GPT-Bench, a hardware-aware languagemodel surrogate benchmark, where we leverage weight-sharing techniques fromNeural Architecture Search (NAS) to efficiently train a supernet proxy,encompassing language models of varying scales in a single model. We conductprofiling of these models across 13 devices, considering 5 hardware metrics and3 distinct model scales. Finally, we showcase the usability of HW-GPT-Benchusing 8 different multi-objective NAS algorithms and evaluate the quality ofthe resultant Pareto fronts. Through this benchmark, our objective is to propeland expedite research in the advancement of multi-objective methods for NAS andstructural pruning in large language models.</description><author>Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Jorg K. H. Franke, Frank Hutter</author><pubDate>Thu, 16 May 2024 18:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10299v1</guid></item><item><title>Societal Adaptation to Advanced AI</title><link>http://arxiv.org/abs/2405.10295v1</link><description>Existing strategies for managing risks from advanced AI systems often focuson affecting what AI systems are developed and how they diffuse. However, thisapproach becomes less feasible as the number of developers of advanced AIgrows, and impedes beneficial use-cases as well as harmful ones. In response,we urge a complementary approach: increasing societal adaptation to advancedAI, that is, reducing the expected negative impacts from a given level ofdiffusion of a given AI capability. We introduce a conceptual framework whichhelps identify adaptive interventions that avoid, defend against and remedypotentially harmful uses of AI systems, illustrated with examples in electionmanipulation, cyberterrorism, and loss of control to AI decision-makers. Wediscuss a three-step cycle that society can implement to adapt to AI.Increasing society's ability to implement this cycle builds its resilience toadvanced AI. We conclude with concrete recommendations for governments,industry, and third-parties.</description><author>Jamie Bernardi, Gabriel Mukobi, Hilary Greaves, Lennart Heim, Markus Anderljung</author><pubDate>Thu, 16 May 2024 18:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10295v1</guid></item><item><title>Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning</title><link>http://arxiv.org/abs/2405.10292v1</link><description>Large vision-language models (VLMs) fine-tuned on specialized visualinstruction-following data have exhibited impressive language reasoningcapabilities across various scenarios. However, this fine-tuning paradigm maynot be able to efficiently learn optimal decision-making agents in multi-stepgoal-directed tasks from interactive environments. To address this challenge,we propose an algorithmic framework that fine-tunes VLMs with reinforcementlearning (RL). Specifically, our framework provides a task description and thenprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLMto efficiently explore intermediate reasoning steps that lead to the finaltext-based action. Next, the open-ended text output is parsed into anexecutable action to interact with the environment to obtain goal-directed taskrewards. Finally, our framework uses these task rewards to fine-tune the entireVLM with RL. Empirically, we demonstrate that our proposed framework enhancesthe decision-making capabilities of VLM agents across various tasks, enabling7b models to outperform commercial models such as GPT4-V or Gemini.Furthermore, we find that CoT reasoning is a crucial component for performanceimprovement, as removing the CoT reasoning results in a significant decrease inthe overall performance of our method.</description><author>Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine</author><pubDate>Thu, 16 May 2024 18:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10292v1</guid></item><item><title>Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees</title><link>http://arxiv.org/abs/2405.10289v1</link><description>In nonsmooth, nonconvex stochastic optimization, understanding the uniformconvergence of subdifferential mappings is crucial for analyzing stationarypoints of sample average approximations of risk as they approach the populationrisk. Yet, characterizing this convergence remains a fundamental challenge. This work introduces a novel perspective by connecting the uniformconvergence of subdifferential mappings to that of subgradient mappings asempirical risk converges to the population risk. We prove that, for stochasticweakly-convex objectives, and within any open set, a uniform bound on theconvergence of subgradients -- chosen arbitrarily from the correspondingsubdifferential sets -- translates to a uniform bound on the convergence of thesubdifferential sets itself, measured by the Hausdorff metric. Using this technique, we derive uniform convergence rates for subdifferentialsets of stochastic convex-composite objectives. Our results do not rely on keydistributional assumptions in the literature, which require the population andfinite sample subdifferentials to be continuous in the Hausdorff metric, yetstill provide tight convergence rates. These guarantees lead to new insightsinto the nonsmooth landscapes of such objectives within finite samples.</description><author>Feng Ruan</author><pubDate>Thu, 16 May 2024 18:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10289v1</guid></item><item><title>Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction</title><link>http://arxiv.org/abs/2405.10288v1</link><description>Facts extraction is pivotal for constructing knowledge graphs. Recently, theincreasing demand for temporal facts in downstream tasks has led to theemergence of the task of temporal fact extraction. In this paper, wespecifically address the extraction of temporal facts from natural languagetext. Previous studies fail to handle the challenge of establishingtime-to-fact correspondences in complex sentences. To overcome this hurdle, wepropose a timeline-based sentence decomposition strategy using large languagemodels (LLMs) with in-context learning, ensuring a fine-grained understandingof the timeline associated with various facts. In addition, we evaluate theperformance of LLMs for direct temporal fact extraction and get unsatisfactoryresults. To this end, we introduce TSDRE, a method that incorporates thedecomposition capabilities of LLMs into the traditional fine-tuning of smallerpre-trained language models (PLMs). To support the evaluation, we constructComplexTRED, a complex temporal fact extraction dataset. Our experiments showthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal andComplexTRED datasets.</description><author>Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, Yuzhong Qu</author><pubDate>Thu, 16 May 2024 18:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10288v1</guid></item><item><title>FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models</title><link>http://arxiv.org/abs/2405.10286v1</link><description>Despite noise and caption quality having been acknowledged as importantfactors impacting vision-language contrastive pre-training, in this paper, weshow that the full potential of improving the training process by addressingsuch issues is yet to be realized. Specifically, we firstly study and analyzetwo issues affecting training: incorrect assignment of negative pairs, and lowcaption quality and diversity. Then, we devise effective solutions foraddressing both problems, which essentially require training with multiple truepositive pairs. Finally, we propose training with sigmoid loss to address sucha requirement. We show very large gains over the current state-of-the-art forboth image recognition ($\sim +6\%$ on average over 11 datasets) and imageretrieval ($\sim +19\%$ on Flickr30k and $\sim +15\%$ on MSCOCO).</description><author>Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos</author><pubDate>Thu, 16 May 2024 18:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10286v1</guid></item><item><title>Quantum Vision Transformers for Quark-Gluon Classification</title><link>http://arxiv.org/abs/2405.10284v1</link><description>We introduce a hybrid quantum-classical vision transformer architecture,notable for its integration of variational quantum circuits within both theattention mechanism and the multi-layer perceptrons. The research addresses thecritical challenge of computational efficiency and resource constraints inanalyzing data from the upcoming High Luminosity Large Hadron Collider,presenting the architecture as a potential solution. In particular, we evaluateour method by applying the model to multi-detector jet images from CMS OpenData. The goal is to distinguish quark-initiated from gluon-initiated jets. Wesuccessfully train the quantum model and evaluate it via numerical simulations.Using this approach, we achieve classification performance almost on par withthe one obtained with the completely classical architecture, considering asimilar number of parameters.</description><author>Marçal Comajoan Cara, Gopal Ramesh Dahale, Zhongtian Dong, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu</author><pubDate>Thu, 16 May 2024 18:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10284v1</guid></item><item><title>OpenLLM-Ro -- Technical Report on Open-source Romanian LLMs</title><link>http://arxiv.org/abs/2405.07703v4</link><description>In recent years, Large Language Models (LLMs) have achieved almost human-likeperformance on various tasks. While some LLMs have been trained on multilingualdata, most of the training data is in English. Hence, their performance inEnglish greatly exceeds their performance in other languages. This documentpresents our approach to training and evaluating the first foundational andchat LLM specialized for Romanian.</description><author>Mihai Masala, Denis C. Ilie-Ablachim, Dragos Corlatescu, Miruna Zavelca, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea</author><pubDate>Thu, 16 May 2024 18:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07703v4</guid></item><item><title>VREN: Volleyball Rally Dataset with Expression Notation Language</title><link>http://arxiv.org/abs/2209.13846v2</link><description>This research is intended to accomplish two goals: The first goal is tocurate a large and information rich dataset that contains crucial and succinctsummaries on the players' actions and positions and the back-and-forth travelpatterns of the volleyball in professional and NCAA Div-I indoor volleyballgames. While several prior studies have aimed to create similar datasets forother sports (e.g. badminton and soccer), creating such a dataset for indoorvolleyball is not yet realized. The second goal is to introduce a volleyballdescriptive language to fully describe the rally processes in the games andapply the language to our dataset. Based on the curated dataset and ourdescriptive sports language, we introduce three tasks for automated volleyballaction and tactic analysis using our dataset: (1) Volleyball Rally Prediction,aimed at predicting the outcome of a rally and helping players and coachesimprove decision-making in practice, (2) Setting Type and Hitting TypePrediction, to help coaches and players prepare more effectively for the game,and (3) Volleyball Tactics and Attacking Zone Statistics, to provide advancedvolleyball statistics and help coaches understand the game and opponent'stactics better. We conducted case studies to show how experimental results canprovide insights to the volleyball analysis community. Furthermore,experimental evaluation based on real-world data establishes a baseline forfuture studies and applications of our dataset and language. This study bridgesthe gap between the indoor volleyball field and computer science. The datasetis available at: https://github.com/haotianxia/VREN.</description><author>Haotian Xia, Rhys Tracy, Yun Zhao, Erwan Fraisse, Yuan-Fang Wang, Linda Petzold</author><pubDate>Thu, 16 May 2024 18:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13846v2</guid></item><item><title>Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers</title><link>http://arxiv.org/abs/2405.10276v1</link><description>Numerous recent works aim to enhance the efficacy of Large Language Models(LLMs) through strategic prompting. In particular, the Optimization byPROmpting (OPRO) approach provides state-of-the-art performance by leveragingLLMs as optimizers where the optimization task is to find instructions thatmaximize the task accuracy. In this paper, we revisit OPRO for automatedprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral7B. Our investigation reveals that OPRO shows limited effectiveness insmall-scale LLMs, with limited inference capabilities constraining optimizationability. We suggest future automatic prompting engineering to consider bothmodel capabilities and computational costs. Additionally, for small-scale LLMs,we recommend direct instructions that clearly outline objectives andmethodologies as robust prompt baselines, ensuring efficient and effectiveprompt engineering in ongoing research.</description><author>Tuo Zhang, Jinyue Yuan, Salman Avestimehr</author><pubDate>Thu, 16 May 2024 18:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10276v1</guid></item><item><title>Faces that Speak: Jointly Synthesising Talking Face and Speech from Text</title><link>http://arxiv.org/abs/2405.10272v1</link><description>The goal of this work is to simultaneously generate natural talking faces andspeech outputs from text. We achieve this by integrating Talking FaceGeneration (TFG) and Text-to-Speech (TTS) systems into a unified framework. Weaddress the main challenges of each task: (1) generating a range of head posesrepresentative of real-world scenarios, and (2) ensuring voice consistencydespite variations in facial motion for the same identity. To tackle theseissues, we introduce a motion sampler based on conditional flow matching, whichis capable of high-quality motion code generation in an efficient way.Moreover, we introduce a novel conditioning method for the TTS system, whichutilises motion-removed features from the TFG model to yield uniform speechoutputs. Our extensive experiments demonstrate that our method effectivelycreates natural-looking talking faces and speech that accurately match theinput text. To our knowledge, this is the first effort to build a multimodalsynthesis system that can generalise to unseen identities.</description><author>Youngjoon Jang, Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung</author><pubDate>Thu, 16 May 2024 18:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10272v1</guid></item><item><title>Automated Federated Learning via Informed Pruning</title><link>http://arxiv.org/abs/2405.10271v1</link><description>Federated learning (FL) represents a pivotal shift in machine learning (ML)as it enables collaborative training of local ML models coordinated by acentral aggregator, all without the need to exchange local data. However, itsapplication on edge devices is hindered by limited computational capabilitiesand data communication challenges, compounded by the inherent complexity ofDeep Learning (DL) models. Model pruning is identified as a key technique forcompressing DL models on devices with limited resources. Nonetheless,conventional pruning techniques typically rely on manually crafted heuristicsand demand human expertise to achieve a balance between model size, speed, andaccuracy, often resulting in sub-optimal solutions. In this study, we introduce an automated federated learning approachutilizing informed pruning, called AutoFLIP, which dynamically prunes andcompresses DL models within both the local clients and the global server. Itleverages a federated loss exploration phase to investigate model gradientbehavior across diverse datasets and losses, providing insights into parametersignificance. Our experiments showcase notable enhancements in scenarios withstrong non-IID data, underscoring AutoFLIP's capacity to tackle computationalconstraints and achieve superior global convergence.</description><author>Christian Internò, Elena Raponi, Niki van Stein, Thomas Bäck, Markus Olhofer, Yaochu Jin, Barbara Hammer</author><pubDate>Thu, 16 May 2024 18:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10271v1</guid></item><item><title>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title><link>http://arxiv.org/abs/2405.04434v3</link><description>We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language modelcharacterized by economical training and efficient inference. It comprises 236Btotal parameters, of which 21B are activated for each token, and supports acontext length of 128K tokens. DeepSeek-V2 adopts innovative architecturesincluding Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guaranteesefficient inference through significantly compressing the Key-Value (KV) cacheinto a latent vector, while DeepSeekMoE enables training strong models at aneconomical cost through sparse computation. Compared with DeepSeek 67B,DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximumgeneration throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-qualityand multi-source corpus consisting of 8.1T tokens, and further performSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlockits potential. Evaluation results show that, even with only 21B activatedparameters, DeepSeek-V2 and its chat versions still achieve top-tierperformance among open-source models.</description><author>DeepSeek-AI</author><pubDate>Thu, 16 May 2024 18:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04434v3</guid></item><item><title>PhilHumans: Benchmarking Machine Learning for Personal Health</title><link>http://arxiv.org/abs/2405.02770v2</link><description>The use of machine learning in Healthcare has the potential to improvepatient outcomes as well as broaden the reach and affordability of Healthcare.The history of other application areas indicates that strong benchmarks areessential for the development of intelligent systems. We present PersonalHealth Interfaces Leveraging HUman-MAchine Natural interactions (PhilHumans), aholistic suite of benchmarks for machine learning across different Healthcaresettings - talk therapy, diet coaching, emergency care, intensive care,obstetric sonography - as well as different learning settings, such as actionanticipation, timeseries modeling, insight mining, language modeling, computervision, reinforcement learning and program synthesis</description><author>Vadim Liventsev, Vivek Kumar, Allmin Pradhap Singh Susaiyah, Zixiu Wu, Ivan Rodin, Asfand Yaar, Simone Balloccu, Marharyta Beraziuk, Sebastiano Battiato, Giovanni Maria Farinella, Aki Härmä, Rim Helaoui, Milan Petkovic, Diego Reforgiato Recupero, Ehud Reiter, Daniele Riboni, Raymond Sterling</author><pubDate>Thu, 16 May 2024 18:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02770v2</guid></item><item><title>An invitation to the sample complexity of quantum hypothesis testing</title><link>http://arxiv.org/abs/2403.17868v3</link><description>Quantum hypothesis testing (QHT) has been traditionally studied from theinformation-theoretic perspective, wherein one is interested in the optimaldecay rate of error probabilities as a function of the number of samples of anunknown state. In this paper, we study the sample complexity of QHT, whereinthe goal is to determine the minimum number of samples needed to reach adesired error probability. By making use of the wealth of knowledge thatalready exists in the literature on QHT, we characterize the sample complexityof binary QHT in the symmetric and asymmetric settings, and we provide boundson the sample complexity of multiple QHT. In more detail, we prove that thesample complexity of symmetric binary QHT depends logarithmically on theinverse error probability and inversely on the negative logarithm of thefidelity. As a counterpart of the quantum Stein's lemma, we also find that thesample complexity of asymmetric binary QHT depends logarithmically on theinverse type II error probability and inversely on the quantum relativeentropy, provided that the type II error probability is sufficiently small. Wethen provide lower and upper bounds on the sample complexity of multiple QHT,with it remaining an intriguing open question to improve these bounds. Thefinal part of our paper outlines and reviews how sample complexity of QHT isrelevant to a broad swathe of research areas and can enhance understanding ofmany fundamental concepts, including quantum algorithms for simulation andsearch, quantum learning and classification, and foundations of quantummechanics. As such, we view our paper as an invitation to researchers comingfrom different communities to study and contribute to the problem of samplecomplexity of QHT, and we outline a number of open directions for futureresearch.</description><author>Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde</author><pubDate>Thu, 16 May 2024 18:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17868v3</guid></item><item><title>Sharpness-Aware Minimization in Genetic Programming</title><link>http://arxiv.org/abs/2405.10267v1</link><description>Sharpness-Aware Minimization (SAM) was recently introduced as aregularization procedure for training deep neural networks. It simultaneouslyminimizes the fitness (or loss) function and the so-called fitness sharpness.The latter serves as a %connection between the geometry of the fitnesslandscape measure of the nonlinear behavior of a solution %and generalizationand does so by finding solutions that lie in neighborhoods having uniformlysimilar loss values across all fitness cases. In this contribution, we adaptSAM for tree Genetic Programming (TGP) by exploring the semantic neighborhoodsof solutions using two simple approaches By capitalizing upon perturbing inputand output of program trees, sharpness can be estimated and used as a secondoptimization criterion during the evolution. To better understand the impact ofthis variant of SAM on TGP, we collect numerous indicators of the evolutionaryprocess, including generalization ability, complexity, diversity, and arecently proposed genotype-phenotype mapping to study the amount of redundancyin trees. The experimental results demonstrate that using any of the twoproposed SAM adaptations in TGP allows (i) a significant reduction of treesizes in the population and (ii) a decrease in redundancy of the trees. Whenassessed on real-world benchmarks, the generalization ability of the elitesolutions does not deteriorate.</description><author>Illya Bakurov, Nathan Haut, Wolfgang Banzhaf</author><pubDate>Thu, 16 May 2024 18:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10267v1</guid></item><item><title>A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision</title><link>http://arxiv.org/abs/2405.10266v1</link><description>In this work, our goals are two fold: large-vocabulary continuous signlanguage recognition (CSLR), and sign language retrieval. To this end, weintroduce a multi-task Transformer model, CSLR2, that is able to ingest asigning sequence and output in a joint embedding space between signed languageand spoken language text. To enable CSLR evaluation in the large-vocabularysetting, we introduce new dataset annotations that have been manuallycollected. These provide continuous sign-level annotations for six hours oftest videos, and will be made publicly available. We demonstrate that by acareful choice of loss functions, training the model for both the CSLR andretrieval tasks is mutually beneficial in terms of performance -- retrievalimproves CSLR performance by providing context, while CSLR improves retrievalwith more fine-grained supervision. We further show the benefits of leveragingweak and noisy supervision from large-vocabulary datasets such as BOBSL, namelysign-level pseudo-labels, and English subtitles. Our model significantlyoutperforms the previous state of the art on both tasks.</description><author>Charles Raude, K R Prajwal, Liliane Momeni, Hannah Bull, Samuel Albanie, Andrew Zisserman, Gül Varol</author><pubDate>Thu, 16 May 2024 18:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10266v1</guid></item><item><title>Architectures and random properties of symplectic quantum circuits</title><link>http://arxiv.org/abs/2405.10264v1</link><description>Parametrized and random unitary (or orthogonal) $n$-qubit circuits play acentral role in quantum information. As such, one could naturally assume thatcircuits implementing symplectic transformation would attract similarattention. However, this is not the case, as $\mathbb{SP}(d/2)$ -- the group of$d\times d$ unitary symplectic matrices -- has thus far been overlooked. Inthis work, we aim at starting to right this wrong. We begin by presenting auniversal set of generators $\mathcal{G}$ for the symplectic algebra$i\mathfrak{sp}(d/2)$, consisting of one- and two-qubit Pauli operators actingon neighboring sites in a one-dimensional lattice. Here, we uncover twocritical differences between such set, and equivalent ones for unitary andorthogonal circuits. Namely, we find that the operators in $\mathcal{G}$ cannotgenerate arbitrary local symplectic unitaries and that they are nottranslationally invariant. We then review the Schur-Weyl duality between thesymplectic group and the Brauer algebra, and use tools from Weingarten calculusto prove that Pauli measurements at the output of Haar random symplecticcircuits can converge to Gaussian processes. As a by-product, such analysisprovides us with concentration bounds for Pauli measurements in circuits thatform $t$-designs over $\mathbb{SP}(d/2)$. To finish, we present tensor-networktools to analyze shallow random symplectic circuits, and we use these tonumerically show that computational-basis measurements anti-concentrate atlogarithmic depth.</description><author>Diego García-Martín, Paolo Braccia, M. Cerezo</author><pubDate>Thu, 16 May 2024 18:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10264v1</guid></item><item><title>On Partially Unitary Learning</title><link>http://arxiv.org/abs/2405.10263v1</link><description>The problem of an optimal mapping between Hilbert spaces $IN$ of$\left|\psi\right\rangle$ and $OUT$ of $\left|\phi\right\rangle$ based on a setof wavefunction measurements (within a phase) $\psi_l \to \phi_l$, $l=1\dotsM$, is formulated as an optimization problem maximizing the total fidelity$\sum_{l=1}^{M} \omega^{(l)}\left|\langle\phi_l|\mathcal{U}|\psi_l\rangle\right|^2$ subject to probabilitypreservation constraints on $\mathcal{U}$ (partial unitarity). Constructedoperator $\mathcal{U}$ can be considered as a $IN$ to $OUT$ quantum channel; itis a partially unitary rectangular matrix of the dimension $\dim(OUT) \times\dim(IN)$ transforming operators as $A^{OUT}=\mathcal{U} A^{IN}\mathcal{U}^{\dagger}$. An iteration algorithm finding the global maximum ofthis optimization problem is developed and it's application to a number ofproblems is demonstrated. A software product implementing the algorithm isavailable from the authors.</description><author>Mikhail Gennadievich Belov, Vladislav Gennadievich Malyshkin</author><pubDate>Thu, 16 May 2024 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10263v1</guid></item><item><title>Two-Phase Dynamics of Interactions Explains the Starting Point of a DNN Learning Over-Fitted Features</title><link>http://arxiv.org/abs/2405.10262v1</link><description>This paper investigates the dynamics of a deep neural network (DNN) learninginteractions. Previous studies have discovered and mathematically proven thatgiven each input sample, a well-trained DNN usually only encodes a small numberof interactions (non-linear relationships) between input variables in thesample. A series of theorems have been derived to prove that we can considerthe DNN's inference equivalent to using these interactions as primitivepatterns for inference. In this paper, we discover the DNN learns interactionsin two phases. The first phase mainly penalizes interactions of medium and highorders, and the second phase mainly learns interactions of gradually increasingorders. We can consider the two-phase phenomenon as the starting point of a DNNlearning over-fitted features. Such a phenomenon has been widely shared by DNNswith various architectures trained for different tasks. Therefore, thediscovery of the two-phase dynamics provides a detailed mechanism for how a DNNgradually learns different inference patterns (interactions). In particular, wehave also verified the claim that high-order interactions have weakergeneralization power than low-order interactions. Thus, the discoveredtwo-phase dynamics also explains how the generalization power of a DNN changesduring the training process.</description><author>Junpeng Zhang, Qing Li, Liang Lin, Quanshi Zhang</author><pubDate>Thu, 16 May 2024 18:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10262v1</guid></item><item><title>Keep It Private: Unsupervised Privatization of Online Text</title><link>http://arxiv.org/abs/2405.10260v1</link><description>Authorship obfuscation techniques hold the promise of helping people protecttheir privacy in online communications by automatically rewriting text to hidethe identity of the original author. However, obfuscation has been evaluated innarrow settings in the NLP literature and has primarily been addressed withsuperficial edit operations that can lead to unnatural outputs. In this work,we introduce an automatic text privatization framework that fine-tunes a largelanguage model via reinforcement learning to produce rewrites that balancesoundness, sense, and privacy. We evaluate it extensively on a large-scale testset of English Reddit posts by 68k authors composed of short-medium lengthtexts. We study how the performance changes among evaluative conditionsincluding authorial profile length and authorship detection strategy. Ourmethod maintains high text quality according to both automated metrics andhuman evaluation, and successfully evades several automated authorship attacks.</description><author>Calvin Bao, Marine Carpuat</author><pubDate>Thu, 16 May 2024 18:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10260v1</guid></item><item><title>Goal-conditioned Offline Reinforcement Learning through State Space Partitioning</title><link>http://arxiv.org/abs/2303.09367v2</link><description>Offline reinforcement learning (RL) aims to infer sequential decisionpolicies using only offline datasets. This is a particularly difficult setup,especially when learning to achieve multiple different goals or outcomes undera given scenario with only sparse rewards. For offline learning ofgoal-conditioned policies via supervised learning, previous work has shown thatan advantage weighted log-likelihood loss guarantees monotonic policyimprovement. In this work we argue that, despite its benefits, this approach isstill insufficient to fully address the distribution shift and multi-modalityproblems. The latter is particularly severe in long-horizon tasks where findinga unique and optimal policy that goes from a state to the desired goal ischallenging as there may be multiple and potentially conflicting solutions. Totackle these challenges, we propose a complementary advantage-based weightingscheme that introduces an additional source of inductive bias: given avalue-based partitioning of the state space, the contribution of actionsexpected to lead to target regions that are easier to reach, compared to thefinal goal, is further increased. Empirically, we demonstrate that the proposedapproach, Dual-Advantage Weighted Offline Goal-conditioned RL (DAWOG),outperforms several competing offline algorithms in commonly used benchmarks.Analytically, we offer a guarantee that the learnt policy is never worse thanthe underlying behaviour policy.</description><author>Mianchu Wang, Yue Jin, Giovanni Montana</author><pubDate>Thu, 16 May 2024 18:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09367v2</guid></item><item><title>Biasing &amp; Debiasing based Approach Towards Fair Knowledge Transfer for Equitable Skin Analysis</title><link>http://arxiv.org/abs/2405.10256v1</link><description>Deep learning models, particularly Convolutional Neural Networks (CNNs), havedemonstrated exceptional performance in diagnosing skin diseases, oftenoutperforming dermatologists. However, they have also unveiled biases linked tospecific demographic traits, notably concerning diverse skin tones or gender,prompting concerns regarding fairness and limiting their widespread deployment.Researchers are actively working to ensure fairness in AI-based solutions, butexisting methods incur an accuracy loss when striving for fairness. To solvethis issue, we propose a `two-biased teachers' (i.e., biased on differentsensitive attributes) based approach to transfer fair knowledge into thestudent network. Our approach mitigates biases present in the student networkwithout harming its predictive accuracy. In fact, in most cases, our approachimproves the accuracy of the baseline model. To achieve this goal, we developeda weighted loss function comprising biasing and debiasing loss terms. Wesurpassed available state-of-the-art approaches to attain fairness and alsoimproved the accuracy at the same time. The proposed approach has beenevaluated and validated on two dermatology datasets using standard accuracy andfairness evaluation measures. We will make source code publicly available tofoster reproducibility and future research.</description><author>Anshul Pundhir, Balasubramanian Raman, Pravendra Singh</author><pubDate>Thu, 16 May 2024 18:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10256v1</guid></item><item><title>Global-Local Image Perceptual Score (GLIPS): Evaluating Photorealistic Quality of AI-Generated Images</title><link>http://arxiv.org/abs/2405.09426v2</link><description>This paper introduces the Global-Local Image Perceptual Score (GLIPS), animage metric designed to assess the photorealistic image quality ofAI-generated images with a high degree of alignment to human visual perception.Traditional metrics such as FID and KID scores do not align closely with humanevaluations. The proposed metric incorporates advanced transformer-basedattention mechanisms to assess local similarity and Maximum Mean Discrepancy(MMD) to evaluate global distributional similarity. To evaluate the performanceof GLIPS, we conducted a human study on photorealistic image quality.Comprehensive tests across various generative models demonstrate that GLIPSconsistently outperforms existing metrics like FID, SSIM, and MS-SSIM in termsof correlation with human scores. Additionally, we introduce the InterpolativeBinning Scale (IBS), a refined scaling method that enhances theinterpretability of metric scores by aligning them more closely with humanevaluative standards. The proposed metric and scaling approach not onlyprovides more reliable assessments of AI-generated images but also suggestpathways for future enhancements in image generation technologies.</description><author>Memoona Aziz, Umair Rehman, Muhammad Umair Danish, Katarina Grolinger</author><pubDate>Thu, 16 May 2024 18:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09426v2</guid></item><item><title>When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2405.10255v1</link><description>As large language models (LLMs) evolve, their integration with 3D spatialdata (3D-LLMs) has seen rapid progress, offering unprecedented capabilities forunderstanding and interacting with physical spaces. This survey provides acomprehensive overview of the methodologies enabling LLMs to process,understand, and generate 3D data. Highlighting the unique advantages of LLMs,such as in-context learning, step-by-step reasoning, open-vocabularycapabilities, and extensive world knowledge, we underscore their potential tosignificantly advance spatial comprehension and interaction within embodiedArtificial Intelligence (AI) systems. Our investigation spans various 3D datarepresentations, from point clouds to Neural Radiance Fields (NeRFs). Itexamines their integration with LLMs for tasks such as 3D scene understanding,captioning, question-answering, and dialogue, as well as LLM-based agents forspatial reasoning, planning, and navigation. The paper also includes a briefreview of other methods that integrate 3D and language. The meta-analysispresented in this paper reveals significant progress yet underscores thenecessity for novel approaches to harness the full potential of 3D-LLMs. Hence,with this paper, we aim to chart a course for future research that explores andexpands the capabilities of 3D-LLMs in understanding and interacting with thecomplex 3D world. To support this survey, we have established a project pagewhere papers related to our topic are organized and listed:https://github.com/ActiveVisionLab/Awesome-LLM-3D.</description><author>Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H Torr, Marc Pollefeys, Matthias Nießner, Ian D Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu</author><pubDate>Thu, 16 May 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10255v1</guid></item><item><title>PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology</title><link>http://arxiv.org/abs/2405.10254v1</link><description>Foundation models in computational pathology promise to unlock thedevelopment of new clinical decision support systems and models for precisionmedicine. However, there is a mismatch between most clinical analysis, which isdefined at the level of one or more whole slide images, and foundation modelsto date, which process the thousands of image tiles contained in a whole slideimage separately. The requirement to train a network to aggregate informationacross a large number of tiles in multiple whole slide images limits thesemodels' impact. In this work, we present a slide-level foundation model forH&amp;E-stained histopathology, PRISM, that builds on Virchow tile embeddings andleverages clinical report text for pre-training. Using the tile embeddings,PRISM produces slide-level embeddings with the ability to generate clinicalreports, resulting in several modes of use. Using text prompts, PRISM achieveszero-shot cancer detection and sub-typing performance approaching andsurpassing that of a supervised aggregator model. Using the slide embeddingswith linear classifiers, PRISM surpasses supervised aggregator models.Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yieldslabel-efficient training for biomarker prediction, a task that typicallysuffers from low availability of training data; an aggregator initialized withPRISM and trained on as little as 10% of the training data can outperform asupervised baseline that uses all of the data.</description><author>George Shaikovski, Adam Casson, Kristen Severson, Eric Zimmermann, Yi Kan Wang, Jeremy D. Kunz, Juan A. Retamero, Gerard Oakley, David Klimstra, Christopher Kanan, Matthew Hanna, Michal Zelechowski, Julian Viret, Neil Tenenholtz, James Hall, Nicolo Fusi, Razik Yousfi, Peter Hamilton, William A. Moye, Eugene Vorontsov, Siqi Liu, Thomas J. Fuchs</author><pubDate>Thu, 16 May 2024 17:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10254v1</guid></item><item><title>MMFusion: Multi-modality Diffusion Model for Lymph Node Metastasis Diagnosis in Esophageal Cancer</title><link>http://arxiv.org/abs/2405.09539v2</link><description>Esophageal cancer is one of the most common types of cancer worldwide andranks sixth in cancer-related mortality. Accurate computer-assisted diagnosisof cancer progression can help physicians effectively customize personalizedtreatment plans. Currently, CT-based cancer diagnosis methods have receivedmuch attention for their comprehensive ability to examine patients' conditions.However, multi-modal based methods may likely introduce information redundancy,leading to underperformance. In addition, efficient and effective interactionsbetween multi-modal representations need to be further explored, lackinginsightful exploration of prognostic correlation in multi-modality features. Inthis work, we introduce a multi-modal heterogeneous graph-based conditionalfeature-guided diffusion model for lymph node metastasis diagnosis based on CTimages as well as clinical measurements and radiomics data. To explore theintricate relationships between multi-modal features, we construct aheterogeneous graph. Following this, a conditional feature-guided diffusionapproach is applied to eliminate information redundancy. Moreover, we propose amasked relational representation learning strategy, aiming to uncover thelatent prognostic correlations and priorities of primary tumor and lymph nodeimage representations. Various experimental results validate the effectivenessof our proposed method. The code is available athttps://github.com/wuchengyu123/MMFusion.</description><author>Chengyu Wu, Chengkai Wang, Yaqi Wang, Huiyu Zhou, Yatao Zhang, Qifeng Wang, Shuai Wang</author><pubDate>Thu, 16 May 2024 17:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09539v2</guid></item><item><title>A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks</title><link>http://arxiv.org/abs/2405.10251v1</link><description>Recent efforts have evaluated large language models (LLMs) in areas such ascommonsense reasoning, mathematical reasoning, and code generation. However, tothe best of our knowledge, no work has specifically investigated theperformance of LLMs in natural language generation (NLG) tasks, a pivotalcriterion for determining model excellence. Thus, this paper conducts acomprehensive evaluation of well-known and high-performing LLMs, namelyChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models,in the context of NLG tasks. We select English and Chinese datasetsencompassing Dialogue Generation and Text Summarization. Moreover, we propose acommon evaluation setting that incorporates input templates and post-processingstrategies. Our study reports both automatic results, accompanied by a detailedanalysis.</description><author>Xuanfan Ni, Piji Li</author><pubDate>Thu, 16 May 2024 17:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10251v1</guid></item><item><title>A Foundation Model for Brain Lesion Segmentation with Mixture of Modality Experts</title><link>http://arxiv.org/abs/2405.10246v1</link><description>Brain lesion segmentation plays an essential role in neurological researchand diagnosis. As brain lesions can be caused by various pathologicalalterations, different types of brain lesions tend to manifest with differentcharacteristics on different imaging modalities. Due to this complexity, brainlesion segmentation methods are often developed in a task-specific manner. Aspecific segmentation model is developed for a particular lesion type andimaging modality. However, the use of task-specific models requirespredetermination of the lesion type and imaging modality, which complicatestheir deployment in real-world scenarios. In this work, we propose a universalfoundation model for 3D brain lesion segmentation, which can automaticallysegment different types of brain lesions for input data of various imagingmodalities. We formulate a novel Mixture of Modality Experts (MoME) frameworkwith multiple expert networks attending to different imaging modalities. Ahierarchical gating network combines the expert predictions and fostersexpertise collaboration. Furthermore, we introduce a curriculum learningstrategy during training to avoid the degeneration of each expert network andpreserve their specialization. We evaluated the proposed method on nine brainlesion datasets, encompassing five imaging modalities and eight lesion types.The results show that our model outperforms state-of-the-art universal modelsand provides promising generalization to unseen datasets.</description><author>Xinru Zhang, Ni Ou, Berke Doga Basaran, Marco Visentin, Mengyun Qiao, Renyang Gu, Cheng Ouyang, Yaou Liu, Paul M. Matthew, Chuyang Ye, Wenjia Bai</author><pubDate>Thu, 16 May 2024 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10246v1</guid></item><item><title>Towards Task-Compatible Compressible Representations</title><link>http://arxiv.org/abs/2405.10244v1</link><description>We identify an issue in multi-task learnable compression, in which arepresentation learned for one task does not positively contribute to therate-distortion performance of a different task as much as expected, given theestimated amount of information available in it. We interpret this issue usingthe predictive $\mathcal{V}$-information framework. In learnable scalablecoding, previous work increased the utilization of side-information for inputreconstruction by also rewarding input reconstruction when learning this sharedrepresentation. We evaluate the impact of this idea in the context of inputreconstruction more rigorously and extended it to other computer vision tasks.We perform experiments using representations trained for object detection onCOCO 2017 and depth estimation on the Cityscapes dataset, and use them toassist in image reconstruction and semantic segmentation tasks. The resultsshow considerable improvements in the rate-distortion performance of theassisted tasks. Moreover, using the proposed representations, the performanceof the base tasks are also improved. Results suggest that the proposed methodinduces simpler representations that are more compatible with downstreamprocesses.</description><author>Anderson de Andrade, Ivan Bajić</author><pubDate>Thu, 16 May 2024 17:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10244v1</guid></item><item><title>DocuMint: Docstring Generation for Python using Small Language Models</title><link>http://arxiv.org/abs/2405.10243v1</link><description>Effective communication, specifically through documentation, is the beatingheart of collaboration among contributors in software development. Recentadvancements in language models (LMs) have enabled the introduction of a newtype of actor in that ecosystem: LM-powered assistants capable of codegeneration, optimization, and maintenance. Our study investigates the efficacyof small language models (SLMs) for generating high-quality docstrings byassessing accuracy, conciseness, and clarity, benchmarking performancequantitatively through mathematical formulas and qualitatively through humanevaluation using Likert scale. Further, we introduce DocuMint, as a large-scalesupervised fine-tuning dataset with 100,000 samples. In quantitativeexperiments, Llama 3 8B achieved the best performance across all metrics, withconciseness and clarity scores of 0.605 and 64.88, respectively. However, underhuman evaluation, CodeGemma 7B achieved the highest overall score with anaverage of 8.3 out of 10 across all metrics. Fine-tuning the CodeGemma 2B modelusing the DocuMint dataset led to significant improvements in performanceacross all metrics, with gains of up to 22.5% in conciseness. The fine-tunedmodel and the dataset can be found in HuggingFace and the code can be found inthe repository.</description><author>Bibek Poudel, Adam Cook, Sekou Traore, Shelah Ameli</author><pubDate>Thu, 16 May 2024 17:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10243v1</guid></item><item><title>Lookbehind-SAM: k steps back, 1 step forward</title><link>http://arxiv.org/abs/2307.16704v3</link><description>Sharpness-aware minimization (SAM) methods have gained increasing popularityby formulating the problem of minimizing both loss value and loss sharpness asa minimax objective. In this work, we increase the efficiency of themaximization and minimization parts of SAM's objective to achieve a betterloss-sharpness trade-off. By taking inspiration from the Lookahead optimizer,which uses multiple descent steps ahead, we propose Lookbehind, which performsmultiple ascent steps behind to enhance the maximization step of SAM and find aworst-case perturbation with higher loss. Then, to mitigate the variance in thedescent step arising from the gathered gradients across the multiple ascentsteps, we employ linear interpolation to refine the minimization step.Lookbehind leads to a myriad of benefits across a variety of tasks.Particularly, we show increased generalization performance, greater robustnessagainst noisy weights, as well as improved learning and less catastrophicforgetting in lifelong learning settings. Our code is available athttps://github.com/chandar-lab/Lookbehind-SAM.</description><author>Gonçalo Mordido, Pranshu Malviya, Aristide Baratin, Sarath Chandar</author><pubDate>Thu, 16 May 2024 17:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16704v3</guid></item><item><title>Invariant Risk Minimization Is A Total Variation Model</title><link>http://arxiv.org/abs/2405.01389v4</link><description>Invariant risk minimization (IRM) is an arising approach to generalizeinvariant features to different environments in machine learning. While mostrelated works focus on new IRM settings or new application scenarios, themathematical essence of IRM remains to be properly explained. We verify thatIRM is essentially a total variation based on $L^2$ norm (TV-$\ell_2$) of thelearning risk with respect to the classifier variable. Moreover, we propose anovel IRM framework based on the TV-$\ell_1$ model. It not only expands theclasses of functions that can be used as the learning risk, but also has robustperformance in denoising and invariant feature preservation based on the coareaformula. We also illustrate some requirements for IRM-TV-$\ell_1$ to achieveout-of-distribution generalization. Experimental results show that the proposedframework achieves competitive performance in several benchmark machinelearning scenarios.</description><author>Zhao-Rong Lai, Weiwen Wang</author><pubDate>Thu, 16 May 2024 17:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01389v4</guid></item><item><title>Results about sets of desirable gamble sets</title><link>http://arxiv.org/abs/2404.17924v2</link><description>Coherent sets of desirable gamble sets is used as a model for representing anagents opinions and choice preferences under uncertainty. In this paper weprovide some results about the axioms required for coherence and the naturalextension of a given set of desirable gamble sets. We also show that coherentsets of desirable gamble sets can be represented by a proper filter of coherentsets of desirable gambles.</description><author>Catrin Campbell-Moore</author><pubDate>Thu, 16 May 2024 17:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17924v2</guid></item><item><title>Influencer Cartels</title><link>http://arxiv.org/abs/2405.10231v1</link><description>Social media influencers account for a growing share of marketing worldwide.We demonstrate the existence of a novel form of market failure in thisadvertising market: influencer cartels, where groups of influencers collude toincrease their advertising revenue by inflating their engagement. Ourtheoretical model shows that influencer cartels can improve consumer welfare ifthey expand social media engagement to the target audience, or reduce welfareif they divert engagement to less relevant audiences. We validate the modelempirically using novel data on influencer cartels combined with machinelearning tools, and derive policy implications for how to maximize consumerwelfare.</description><author>Marit Hinnosaar, Toomas Hinnosaar</author><pubDate>Thu, 16 May 2024 17:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10231v1</guid></item><item><title>Random ReLU Neural Networks as Non-Gaussian Processes</title><link>http://arxiv.org/abs/2405.10229v1</link><description>We consider a large class of shallow neural networks with randomlyinitialized parameters and rectified linear unit activation functions. We provethat these random neural networks are well-defined non-Gaussian processes. As aby-product, we demonstrate that these networks are solutions to stochasticdifferential equations driven by impulsive white noise (combinations of randomDirac measures). These processes are parameterized by the law of the weightsand biases as well as the density of activation thresholds in each boundedregion of the input domain. We prove that these processes are isotropic andwide-sense self-similar with Hurst exponent $3/2$. We also derive a remarkablysimple closed-form expression for their autocovariance function. Our resultsare fundamentally different from prior work in that we consider anon-asymptotic viewpoint: The number of neurons in each bounded region of theinput domain (i.e., the width) is itself a random variable with a Poisson lawwith mean proportional to the density parameter. Finally, we show that, undersuitable hypotheses, as the expected width tends to infinity, these processescan converge in law not only to Gaussian processes, but also to non-Gaussianprocesses depending on the law of the weights. Our asymptotic results provide anew take on several classical results (wide networks converge to Gaussianprocesses) as well as some new ones (wide networks can converge to non-Gaussianprocesses).</description><author>Rahul Parhi, Pakshal Bohra, Ayoub El Biari, Mehrsa Pourya, Michael Unser</author><pubDate>Thu, 16 May 2024 17:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10229v1</guid></item><item><title>LLM-Assisted Rule Based Machine Translation for Low/No-Resource Languages</title><link>http://arxiv.org/abs/2405.08997v2</link><description>We propose a new paradigm for machine translation that is particularly usefulfor no-resource languages (those without any publicly available bilingual ormonolingual corpora): LLM-RBMT (LLM-Assisted Rule Based Machine Translation).Using the LLM-RBMT paradigm, we design the first languageeducation/revitalization-oriented machine translator for Owens Valley Paiute(OVP), a critically endangered Indigenous American language for which there isvirtually no publicly available data. We present a detailed evaluation of thetranslator's components: a rule-based sentence builder, an OVP to Englishtranslator, and an English to OVP translator. We also discuss the potential ofthe paradigm, its limitations, and the many avenues for future research that itopens up.</description><author>Jared Coleman, Bhaskar Krishnamachari, Khalil Iskarous, Ruben Rosales</author><pubDate>Thu, 16 May 2024 17:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08997v2</guid></item><item><title>Scalarisation-based risk concepts for robust multi-objective optimisation</title><link>http://arxiv.org/abs/2405.10221v1</link><description>Robust optimisation is a well-established framework for optimising functionsin the presence of uncertainty. The inherent goal of this problem is toidentify a collection of inputs whose outputs are both desirable for thedecision maker, whilst also being robust to the underlying uncertainties in theproblem. In this work, we study the multi-objective extension of this problemfrom a computational standpoint. We identify that the majority of all robustmulti-objective algorithms rely on two key operations: robustification andscalarisation. Robustification refers to the strategy that is used tomarginalise over the uncertainty in the problem. Whilst scalarisation refers tothe procedure that is used to encode the relative importance of each objective.As these operations are not necessarily commutative, the order that they areperformed in has an impact on the resulting solutions that are identified andthe final decisions that are made. This work aims to give an exposition on thephilosophical differences between these two operations and highlight when oneshould opt for one ordering over the other. As part of our analysis, weshowcase how many existing risk concepts can be easily integrated into thespecification and solution of a robust multi-objective optimisation problem.Besides this, we also demonstrate how one can principally define the notion ofa robust Pareto front and a robust performance metric based on our robustifyand scalarise methodology. To illustrate the efficacy of these new ideas, wepresent two insightful numerical case studies which are based on real-worlddata sets.</description><author>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</author><pubDate>Thu, 16 May 2024 17:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10221v1</guid></item><item><title>ENADPool: The Edge-Node Attention-based Differentiable Pooling for Graph Neural Networks</title><link>http://arxiv.org/abs/2405.10218v1</link><description>Graph Neural Networks (GNNs) are powerful tools for graph classification. Oneimportant operation for GNNs is the downsampling or pooling that can learneffective embeddings from the node representations. In this paper, we propose anew hierarchical pooling operation, namely the Edge-Node Attention-basedDifferentiable Pooling (ENADPool), for GNNs to learn effective graphrepresentations. Unlike the classical hierarchical pooling operation that isbased on the unclear node assignment and simply computes the averaged featureover the nodes of each cluster, the proposed ENADPool not only employs a hardclustering strategy to assign each node into an unique cluster, but alsocompress the node features as well as their edge connectivity strengths intothe resulting hierarchical structure based on the attention mechanism aftereach pooling step. As a result, the proposed ENADPool simultaneously identifiesthe importance of different nodes within each separated cluster and edgesbetween corresponding clusters, that significantly addresses the shortcomingsof the uniform edge-node based structure information aggregation arising in theclassical hierarchical pooling operation. Moreover, to mitigate theover-smoothing problem arising in existing GNNs, we propose a Multi-distanceGNN (MD-GNN) model associated with the proposed ENADPool operation, allowingthe nodes to actively and directly receive the feature information fromneighbors at different random walk steps. Experiments demonstrate theeffectiveness of the MD-GNN associated with the proposed ENADPool.</description><author>Zhehan Zhao, Lu Bai, Lixin Cui, Ming Li, Yue Wang, Lixiang Xu, Edwin R. Hancock</author><pubDate>Thu, 16 May 2024 17:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10218v1</guid></item><item><title>A Framework for Improving the Reliability of Black-box Variational Inference</title><link>http://arxiv.org/abs/2203.15945v2</link><description>Black-box variational inference (BBVI) now sees widespread use in machinelearning and statistics as a fast yet flexible alternative to Markov chainMonte Carlo methods for approximate Bayesian inference. However, stochasticoptimization methods for BBVI remain unreliable and require substantialexpertise and hand-tuning to apply effectively. In this paper, we proposeRobust and Automated Black-box VI (RABVI), a framework for improving thereliability of BBVI optimization. RABVI is based on rigorously justifiedautomation techniques, includes just a small number of intuitive tuningparameters, and detects inaccurate estimates of the optimal variationalapproximation. RABVI adaptively decreases the learning rate by detectingconvergence of the fixed--learning-rate iterates, then estimates thesymmetrized Kullback--Leibler (KL) divergence between the current variationalapproximation and the optimal one. It also employs a novel optimizationtermination criterion that enables the user to balance desired accuracy againstcomputational cost by comparing (i) the predicted relative decrease in thesymmetrized KL divergence if a smaller learning were used and (ii) thepredicted computation required to converge with the smaller learning rate. Wevalidate the robustness and accuracy of RABVI through carefully designedsimulation studies and on a diverse set of real-world model and data examples.</description><author>Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins</author><pubDate>Thu, 16 May 2024 17:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.15945v2</guid></item><item><title>Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting</title><link>http://arxiv.org/abs/2405.10216v1</link><description>Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning largepre-trained or foundational models across different modalities and tasks.However, its application to time series data, particularly within foundationalmodels, remains underexplored. This paper examines the impact of LoRA oncontemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos.We demonstrate LoRA's fine-tuning potential for forecasting the vital signs ofsepsis patients in intensive care units (ICUs), emphasizing the models'adaptability to previously unseen, out-of-domain modalities. Integrating LoRAaims to enhance forecasting performance while reducing inefficienciesassociated with fine-tuning large models on limited domain-specific data. Ourexperiments show that LoRA fine-tuning of time series foundational modelssignificantly improves forecasting, achieving results comparable tostate-of-the-art models trained from scratch on similar modalities. We conductcomprehensive ablation studies to demonstrate the trade-offs between the numberof tunable parameters and forecasting performance and assess the impact ofvarying LoRA matrix ranks on model performance.</description><author>Divij Gupta, Anubhav Bhatti, Suraj Parmar, Chen Dan, Yuwei Liu, Bingjie Shen, San Lee</author><pubDate>Thu, 16 May 2024 17:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10216v1</guid></item><item><title>SMLP: Symbolic Machine Learning Prover (User Manual)</title><link>http://arxiv.org/abs/2405.10215v1</link><description>SMLP: Symbolic Machine Learning Prover an open source tool for explorationand optimization of systems represented by machine learning models. SMLP usessymbolic reasoning for ML model exploration and optimization under verificationand stability constraints, based on SMT, constraint and NN solvers. In additionits exploration methods are guided by probabilistic and statistical methods.SMLP is a general purpose tool that requires only data suitable for MLmodelling in the csv format (usually samples of the system's input/output).SMLP has been applied at Intel for analyzing and optimizing hardware designs atthe analog level. Currently SMLP supports NNs, polynomial and tree models, anduses SMT solvers for reasoning and optimization at the backend, integration ofspecialized NN solvers is in progress.</description><author>Franz Brauße, Zurab Khasidashvili, Konstantin Korovin</author><pubDate>Thu, 16 May 2024 17:05:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10215v1</guid></item><item><title>Words as Trigger Points in Social Media Discussions</title><link>http://arxiv.org/abs/2405.10213v1</link><description>Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) tostudy qualitative focus group interviews and understand polarisation inGermany. When people communicate, trigger points represent moments whenindividuals feel that their understanding of what is fair, normal, orappropriate in society is questioned. In the original studies, individualsreact affectively to such triggers and show strong and negative emotionalresponses. In this paper, we introduce the first systematic study of thelarge-scale effect of individual words as trigger points by analysing a largeamount of social media posts. We examine online deliberations on Reddit between2020 and 2022 and collect &gt;100 million posts from subreddits related to a setof words identified as trigger points in UK politics. We find that such triggerwords affect user engagement and have noticeable consequences on animosity inonline discussions. We share empirical evidence of trigger words causinganimosity, and how they provide incentives for hate speech, adversarialdebates, and disagreements. Our work is the first to introduce trigger pointsto computational studies of online communication. Our findings are relevant toresearchers interested in online harms and who examine how citizens debatepolitics and society in light of affective polarisation.</description><author>Dimosthenis Antypas, Christian Arnold, Jose Camacho-Collados, Nedjma Ousidhoum, Carla Perez Almendros</author><pubDate>Thu, 16 May 2024 17:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10213v1</guid></item><item><title>CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations</title><link>http://arxiv.org/abs/2405.10212v1</link><description>In this paper, we introduce a novel psychological benchmark, CPsyExam,constructed from questions sourced from Chinese language examinations. CPsyExamis designed to prioritize psychological knowledge and case analysis separately,recognizing the significance of applying psychological knowledge to real-worldscenarios. From the pool of 22k questions, we utilize 4k to create thebenchmark that offers balanced coverage of subjects and incorporates a diverserange of case analysis techniques.Furthermore, we evaluate a range of existinglarge language models~(LLMs), spanning from open-sourced to API-based models.Our experiments and analysis demonstrate that CPsyExam serves as an effectivebenchmark for enhancing the understanding of psychology within LLMs and enablesthe comparison of LLMs across various granularities.</description><author>Jiahao Zhao, Jingwei Zhu, Minghuan Tan, Min Yang, Di Yang, Chenhao Zhang, Guancheng Ye, Chengming Li, Xiping Hu</author><pubDate>Thu, 16 May 2024 17:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10212v1</guid></item><item><title>Building a Luganda Text-to-Speech Model From Crowdsourced Data</title><link>http://arxiv.org/abs/2405.10211v1</link><description>Text-to-speech (TTS) development for African languages such as Luganda isstill limited, primarily due to the scarcity of high-quality, single-speakerrecordings essential for training TTS models. Prior work has focused onutilizing the Luganda Common Voice recordings of multiple speakers aged between20-49. Although the generated speech is intelligible, it is still of lowerquality than the model trained on studio-grade recordings. This is due to theinsufficient data preprocessing methods applied to improve the quality of theCommon Voice recordings. Furthermore, speech convergence is more difficult toachieve due to varying intonations, as well as background noise. In this paper,we show that the quality of Luganda TTS from Common Voice can improve bytraining on multiple speakers of close intonation in addition to furtherpreprocessing of the training data. Specifically, we selected six femalespeakers with close intonation determined by subjectively listening andcomparing their voice recordings. In addition to trimming out silent portionsfrom the beginning and end of the recordings, we applied a pre-trained speechenhancement model to reduce background noise and enhance audio quality. We alsoutilized a pre-trained, non-intrusive, self-supervised Mean Opinion Score (MOS)estimation model to filter recordings with an estimated MOS over 3.5,indicating high perceived quality. Subjective MOS evaluations from nine nativeLuganda speakers demonstrate that our TTS model achieves a significantly betterMOS of 3.55 compared to the reported 2.5 MOS of the existing model. Moreover,for a fair comparison, our model trained on six speakers outperforms modelstrained on a single-speaker (3.13 MOS) or two speakers (3.22 MOS). Thisshowcases the effectiveness of compensating for the lack of data from onespeaker with data from multiple speakers of close intonation to improve TTSquality.</description><author>Sulaiman Kagumire, Andrew Katumba, Joyce Nakatumba-Nabende, John Quinn</author><pubDate>Thu, 16 May 2024 17:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10211v1</guid></item><item><title>GPT Store Mining and Analysis</title><link>http://arxiv.org/abs/2405.10210v1</link><description>As a pivotal extension of the renowned ChatGPT, the GPT Store serves as adynamic marketplace for various Generative Pre-trained Transformer (GPT)models, shaping the frontier of conversational AI. This paper presents anin-depth measurement study of the GPT Store, with a focus on the categorizationof GPTs by topic, factors influencing GPT popularity, and the potentialsecurity risks. Our investigation starts with assessing the categorization ofGPTs in the GPT Store, analyzing how they are organized by topics, andevaluating the effectiveness of the classification system. We then examine thefactors that affect the popularity of specific GPTs, looking into userpreferences, algorithmic influences, and market trends. Finally, the studydelves into the security risks of the GPT Store, identifying potential threatsand evaluating the robustness of existing security measures. This study offersa detailed overview of the GPT Store's current state, shedding light on itsoperational dynamics and user interaction patterns. Our findings aim to enhanceunderstanding of the GPT ecosystem, providing valuable insights for futureresearch, development, and policy-making in generative AI.</description><author>Dongxun Su, Yanjie Zhao, Xinyi Hou, Shenao Wang, Haoyu Wang</author><pubDate>Thu, 16 May 2024 17:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10210v1</guid></item><item><title>Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level</title><link>http://arxiv.org/abs/2405.10202v1</link><description>Scientific document summarization has been a challenging task due to the longstructure of the input text. The long input hinders the simultaneous effectivemodeling of both global high-order relations between sentences and localintra-sentence relations which is the most critical step in extractivesummarization. However, existing methods mostly focus on one type of relation,neglecting the simultaneous effective modeling of both relations, which canlead to insufficient learning of semantic representations. In this paper, wepropose HAESum, a novel approach utilizing graph neural networks to locally andglobally model documents based on their hierarchical discourse structure.First, intra-sentence relations are learned using a local heterogeneous graph.Subsequently, a novel hypergraph self-attention layer is introduced to furtherenhance the characterization of high-order inter-sentence relations. Wevalidate our approach on two benchmark datasets, and the experimental resultsdemonstrate the effectiveness of HAESum and the importance of consideringhierarchical structures in modeling long scientific documents. Our code will beavailable at \url{https://github.com/MoLICHENXI/HAESum}</description><author>Chenlong Zhao, Xiwen Zhou, Xiaopeng Xie, Yong Zhang</author><pubDate>Thu, 16 May 2024 16:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10202v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v3</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PREFS (Precision and Recall Evaluationof Summary FactS), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset Papalampidi and Lapata (2023), our method produces higherquality summaries than comparison models, as measured with ROUGE and our newfact-based metric.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Thu, 16 May 2024 16:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v3</guid></item><item><title>Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets</title><link>http://arxiv.org/abs/2310.17882v2</link><description>Amid the increasing interest in the deployment of Distributed EnergyResources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal toolfor aggregating diverse DERs and facilitating their participation in wholesaleenergy markets. These VPP deployments have been fueled by the Federal EnergyRegulatory Commission's Order 2222, which makes DERs and VPPs competitiveacross market segments. However, the diversity and decentralized nature of DERspresent significant challenges to the scalable coordination of VPP assets. Toaddress efficiency and speed bottlenecks, this paper presents a novel machinelearning-assisted distributed optimization to coordinate VPP assets. Ourmethod, named LOOP-MAC(Learning to Optimize the Optimization Process forMulti-agent Coordination), adopts a multi-agent coordination perspective whereeach VPP agent manages multiple DERs and utilizes neural network approximatorsto expedite the solution search. The LOOP-MAC method employs a gauge map toguarantee strict compliance with local constraints, effectively reducing theneed for additional post-processing steps. Our results highlight the advantagesof LOOP-MAC, showcasing accelerated solution times per iteration andsignificantly reduced convergence times. The LOOP-MAC method outperformsconventional centralized and distributed optimization methods in optimizationtasks that require repetitive and sequential execution.</description><author>Meiyi Li, Javad Mohammadi</author><pubDate>Thu, 16 May 2024 16:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17882v2</guid></item><item><title>Fast Estimations of Hitting Time of Elitist Evolutionary Algorithms from Fitness Levels</title><link>http://arxiv.org/abs/2311.10502v2</link><description>The fitness level method is an easy-to-use tool for estimating the hittingtime of elitist evolutionary algorithms. Recently, linear lower and upperbounds by fitness levels have been constructed. But these bounds requirerecursive computation, which makes them difficult to use in practice. Weaddress this shortcoming with a new directed graph (digraph) method that doesnot require recursive computation and significantly simplifies the calculationof coefficients in the lower bound. In the method, we select a sub-digraph anddivide it into fitness levels, then construct an explicit formula for computingthe linear lower bound coefficients using transition probabilities restrictedto the subdigraph. A major advantage of the new method is the derivation oftight lower bounds on fitness functions with shortcuts, which are difficult toachieve using previous fitness methods. We use three examples (FullyDeceptive,TwoMax1 and Deceptive) to demonstrate that each new lower bound is tight, butprevious lower bounds are not. Our work significantly extends the fitness levelmethod from addressing simple fitness functions without shortcuts to morecomplex functions with shortcuts.</description><author>Jun He, Siang Yew Chong, Xin Yao</author><pubDate>Thu, 16 May 2024 16:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10502v2</guid></item><item><title>ShennongAlpha: an AI-driven sharing and collaboration platform for intelligent curation, acquisition, and translation of natural medicinal material knowledge</title><link>http://arxiv.org/abs/2401.00020v2</link><description>Natural Medicinal Materials (NMMs) have a long history of global clinicalapplications and a wealth of records and knowledge. Although NMMs are a majorsource for drug discovery and clinical application, the utilization and sharingof NMM knowledge face crucial challenges, including the standardizeddescription of critical information, efficient curation and acquisition, andlanguage barriers. To address these, we developed ShennongAlpha, an AI-drivensharing and collaboration platform for intelligent knowledge curation,acquisition, and translation. For standardized knowledge curation, the platformintroduced a Systematic Nomenclature to enable accurate differentiation andidentification of NMMs. More than fourteen thousand Chinese NMMs have beencurated into the platform along with their knowledge. Furthermore, the platformpioneered chat-based knowledge acquisition, standardized machine translation,and collaborative knowledge updating. Together, our study represents the firstmajor advance in leveraging AI to empower NMM knowledge sharing, which not onlymarks a novel application of AI for Science, but also will significantlybenefit the global biomedical, pharmaceutical, physician, and patientcommunities.</description><author>Zijie Yang, Yongjing Yin, Chaojun Kong, Tiange Chi, Wufan Tao, Yue Zhang, Tian Xu</author><pubDate>Thu, 16 May 2024 16:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00020v2</guid></item><item><title>Influence Maximization in Hypergraphs using Multi-Objective Evolutionary Algorithms</title><link>http://arxiv.org/abs/2405.10187v1</link><description>The Influence Maximization (IM) problem is a well-known NP-hard combinatorialproblem over graphs whose goal is to find the set of nodes in a network thatspreads influence at most. Among the various methods for solving the IMproblem, evolutionary algorithms (EAs) have been shown to be particularlyeffective. While the literature on the topic is particularly ample, only a fewattempts have been made at solving the IM problem over higher-order networks,namely extensions of standard graphs that can capture interactions that involvemore than two nodes. Hypergraphs are a valuable tool for modeling complexinteraction networks in various domains; however, they require rethinking ofseveral graph-based problems, including IM. In this work, we propose amulti-objective EA for the IM problem over hypergraphs that leverages smartinitialization and hypergraph-aware mutation. While the existing methods relyon greedy or heuristic methods, to our best knowledge this is the first attemptat applying EAs to this problem. Our results over nine real-world datasets andthree propagation models, compared with five baseline algorithms, reveal thatour method achieves in most cases state-of-the-art results in terms ofhypervolume and solution diversity.</description><author>Stefano Genetti, Eros Ribaga, Elia Cunegatti, Quintino Francesco Lotito, Giovanni Iacca</author><pubDate>Thu, 16 May 2024 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10187v1</guid></item><item><title>DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data</title><link>http://arxiv.org/abs/2405.10185v1</link><description>Instance segmentation is data-hungry, and as model capacity increases, datascale becomes crucial for improving the accuracy. Most instance segmentationdatasets today require costly manual annotation, limiting their data scale.Models trained on such data are prone to overfitting on the training set,especially for those rare categories. While recent works have delved intoexploiting generative models to create synthetic datasets for dataaugmentation, these approaches do not efficiently harness the full potential ofgenerative models. To address these issues, we introduce a more efficient strategy to constructgenerative datasets for data augmentation, termed DiverGen. Firstly, we providean explanation of the role of generative data from the perspective ofdistribution discrepancy. We investigate the impact of different data on thedistribution learned by the model. We argue that generative data can expand thedata distribution that the model can learn, thus mitigating overfitting.Additionally, we find that the diversity of generative data is crucial forimproving model performance and enhance it through various strategies,including category diversity, prompt diversity, and generative model diversity.With these strategies, we can scale the data to millions while maintaining thetrend of model performance improvement. On the LVIS dataset, DiverGensignificantly outperforms the strong model X-Paste, achieving +1.1 box AP and+1.1 mask AP across all categories, and +1.9 box AP and +2.5 mask AP for rarecategories.</description><author>Chengxiang Fan, Muzhi Zhu, Hao Chen, Yang Liu, Weijia Wu, Huaqi Zhang, Chunhua Shen</author><pubDate>Thu, 16 May 2024 16:30:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10185v1</guid></item><item><title>A Guide to Tracking Phylogenies in Parallel and Distributed Agent-based Evolution Models</title><link>http://arxiv.org/abs/2405.10183v1</link><description>Computer simulations are an important tool for studying the mechanics ofbiological evolution. In particular, in silico work with agent-based modelsprovides an opportunity to collect high-quality records of ancestryrelationships among simulated agents. Such phylogenies can provide insight intoevolutionary dynamics within these simulations. Existing work generally trackslineages directly, yielding an exact phylogenetic record of evolutionaryhistory. However, direct tracking can be inefficient for large-scale,many-processor evolutionary simulations. An alternate approach to extractingphylogenetic information from simulation that scales more favorably is post hocestimation, akin to how bioinformaticians build phylogenies by assessinggenetic similarities between organisms. Recently introduced ``hereditarystratigraphy'' algorithms provide means for efficient inference of phylogenetichistory from non-coding annotations on simulated organisms' genomes. A numberof options exist in configuring hereditary stratigraphy methodology, but nowork has yet tested how they impact reconstruction quality. To address thisquestion, we surveyed reconstruction accuracy under alternate configurationsacross a matrix of evolutionary conditions varying in selection pressure,spatial structure, and ecological dynamics. We synthesize results from theseexperiments to suggest a prescriptive system of best practices for work withhereditary stratigraphy, ultimately guiding researchers in choosing appropriateinstrumentation for large-scale simulation studies.</description><author>Matthew Andres Moreno, Anika Ranjan, Emily Dolson, Luis Zaman</author><pubDate>Thu, 16 May 2024 16:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10183v1</guid></item><item><title>Filling Missing Values Matters for Range Image-Based Point Cloud Segmentation</title><link>http://arxiv.org/abs/2405.10175v1</link><description>Point cloud segmentation (PCS) plays an essential role in robot perceptionand navigation tasks. To efficiently understand large-scale outdoor pointclouds, their range image representation is commonly adopted. This image-likerepresentation is compact and structured, making range image-based PCS modelspractical. However, undesirable missing values in the range images damage theshapes and patterns of objects. This problem creates difficulty for the modelsin learning coherent and complete geometric information from the objects.Consequently, the PCS models only achieve inferior performance. Delving deeplyinto this issue, we find that the use of unreasonable projection approaches anddeskewing scans mainly leads to unwanted missing values in the range images.Besides, almost all previous works fail to consider filling in the unexpectedmissing values in the PCS task. To alleviate this problem, we first propose anew projection method, namely scan unfolding++ (SU++), to avoid massive missingvalues in the generated range images. Then, we introduce a simple yet effectiveapproach, namely range-dependent $K$-nearest neighbor interpolation ($K$NNI),to further fill in missing values. Finally, we introduce the Filling MissingValues Network (FMVNet) and Fast FMVNet. Extensive experimental results onSemanticKITTI, SemanticPOSS, and nuScenes datasets demonstrate that byemploying the proposed SU++ and $K$NNI, existing range image-based PCS modelsconsistently achieve better performance than the baseline models. Besides, bothFMVNet and Fast FMVNet achieve state-of-the-art performance in terms of thespeed-accuracy trade-off. The proposed methods can be applied to other rangeimage-based tasks and practical applications.</description><author>Bike Chen, Chen Gong, Juha Röning</author><pubDate>Thu, 16 May 2024 16:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10175v1</guid></item><item><title>The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows</title><link>http://arxiv.org/abs/2309.09743v3</link><description>We propose the NFLikelihood, an unsupervised version, based on NormalizingFlows, of the DNNLikelihood proposed in Ref.[1]. We show, through realisticexamples, how Autoregressive Flows, based on affine and rational quadraticspline bijectors, are able to learn complicated high-dimensional Likelihoodsarising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysisexample already considered in the literature and on two Effective Field Theoryfits of flavor and electroweak observables, whose samples have been obtainedthrought the HEPFit code. We discuss advantages and disadvantages of theunsupervised approach with respect to the supervised one and discuss possibleinterplays of the two.</description><author>Humberto Reyes-Gonzalez, Riccardo Torre</author><pubDate>Thu, 16 May 2024 16:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09743v3</guid></item><item><title>LFED: A Literary Fiction Evaluation Dataset for Large Language Models</title><link>http://arxiv.org/abs/2405.10166v1</link><description>The rapid evolution of large language models (LLMs) has ushered in the needfor comprehensive assessments of their performance across various dimensions.In this paper, we propose LFED, a Literary Fiction Evaluation Dataset, whichaims to evaluate the capability of LLMs on the long fiction comprehension andreasoning. We collect 95 literary fictions that are either originally writtenin Chinese or translated into Chinese, covering a wide range of topics acrossseveral centuries. We define a question taxonomy with 8 question categories toguide the creation of 1,304 questions. Additionally, we conduct an in-depthanalysis to ascertain how specific attributes of literary fictions (e.g., noveltypes, character numbers, the year of publication) impact LLM performance inevaluations. Through a series of experiments with various state-of-the-artLLMs, we demonstrate that these models face considerable challenges ineffectively addressing questions related to literary fictions, with ChatGPTreaching only 57.08% under the zero-shot setting. The dataset will be publiclyavailable at https://github.com/tjunlp-lab/LFED.git</description><author>Linhao Yu, Qun Liu, Deyi Xiong</author><pubDate>Thu, 16 May 2024 16:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10166v1</guid></item><item><title>Building Knowledge-Grounded Dialogue Systems with Graph-Based Semantic Modeling</title><link>http://arxiv.org/abs/2204.12681v2</link><description>The knowledge-grounded dialogue task aims to generate responses that conveyinformation from given knowledge documents. However, it is a challenge for thecurrent sequence-based model to acquire knowledge from complex documents andintegrate it to perform correct responses without the aid of an explicitsemantic structure. To address these issues, we propose a novel graphstructure, Grounded Graph ($G^2$), that models the semantic structure of bothdialogue and knowledge to facilitate knowledge selection and integration forknowledge-grounded dialogue generation. We also propose a Grounded Graph AwareTransformer ($G^2AT$) model that fuses multi-forms knowledge (both sequentialand graphic) to enhance knowledge-grounded response generation. Our experimentsresults show that our proposed model outperforms the previous state-of-the-artmethods with more than 10\% gains in response generation and nearly 20\%improvement in factual consistency. Further, our model reveals goodgeneralization ability and robustness. By incorporating semantic structures asprior knowledge in deep neural networks, our model provides an effective way toaid language generation.</description><author>Yizhe Yang, Heyan Huang, Yang Gao, Jiawei Li and</author><pubDate>Thu, 16 May 2024 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.12681v2</guid></item><item><title>The WHY in Business Processes: Discovery of Causal Execution Dependencies</title><link>http://arxiv.org/abs/2310.14975v2</link><description>Unraveling the causal relationships among the execution of process activitiesis a crucial element in predicting the consequences of process interventionsand making informed decisions regarding process improvements. Process discoveryalgorithms exploit time precedence as their main source of model derivation.Hence, a causal view can supplement process discovery, being a new perspectivein which relations reflect genuine cause-effect dependencies among the tasks.This calls for faithful new techniques to discover the causal executiondependencies among the tasks in the process. To this end, our work offers asystematic approach to the unveiling of the causal business process byleveraging an existing causal discovery algorithm over activity timing. Inaddition, this work delves into a set of conditions under which process miningdiscovery algorithms generate a model that is incongruent with the causalbusiness process model, and shows how the latter model can be methodologicallyemployed for a sound analysis of the process. Our methodology searches for suchdiscrepancies between the two models in the context of three causal patterns,and derives a new view in which these inconsistencies are annotated over themined process model. We demonstrate our methodology employing two open processmining algorithms, the IBM Process Mining tool, and the LiNGAM causal discoverytechnique. We apply it on a synthesized dataset and on two open benchmark datasets.</description><author>Fabiana Fournier, Lior Limonad, Inna Skarbovsky, Yuval David</author><pubDate>Thu, 16 May 2024 15:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14975v2</guid></item><item><title>EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time</title><link>http://arxiv.org/abs/2405.01762v2</link><description>Understanding and explaining the predictions of Graph Neural Networks (GNNs),is crucial for enhancing their safety and trustworthiness. Subgraph-levelexplanations are gaining attention for their intuitive appeal. However, mostexisting subgraph-level explainers face efficiency challenges in explainingGNNs due to complex search processes. The key challenge is to find a balancebetween intuitiveness and efficiency while ensuring transparency. Additionally,these explainers usually induce subgraphs by nodes, which may introduceless-intuitive disconnected nodes in the subgraph-level explanations or omitmany important subgraph structures. In this paper, we reveal that inducingsubgraph explanations by edges is more comprehensive than other subgraphinducing techniques. We also emphasize the need of determining the subgraphexplanation size for each data instance, as different data instances mayinvolve different important substructures. Building upon these considerations,we introduce a training-free approach, named EiG-Search. We employ an efficientlinear-time search algorithm over the edge-induced subgraphs, where the edgesare ranked by an enhanced gradient-based importance. We conduct extensiveexperiments on a total of seven datasets, demonstrating its superiorperformance and efficiency both quantitatively and qualitatively over theleading baselines.</description><author>Shengyao Lu, Bang Liu, Keith G. Mills, Jiao He, Di Niu</author><pubDate>Thu, 16 May 2024 15:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01762v2</guid></item><item><title>PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning</title><link>http://arxiv.org/abs/2405.10160v1</link><description>Remote sensing image-text retrieval constitutes a foundational aspect ofremote sensing interpretation tasks, facilitating the alignment of vision andlanguage representations. This paper introduces a prior instructionrepresentation (PIR) learning paradigm that draws on prior knowledge toinstruct adaptive learning of vision and text representations. Based on PIR, adomain-adapted remote sensing image-text retrieval framework PIR-ITR isdesigned to address semantic noise issues in vision-language understandingtasks. However, with massive additional data for pre-training thevision-language foundation model, remote sensing image-text retrieval isfurther developed into an open-domain retrieval task. Continuing with theabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remotesensing image-text retrieval, to address semantic noise in remote sensingvision-language representations and further improve open-domain retrievalperformance. In vision representation, Vision Instruction Representation (VIR)based on Spatial-PAE utilizes the prior-guided knowledge of the remote sensingscene recognition by building a belief matrix to select key features forreducing the impact of semantic noise. In text representation, Language CycleAttention (LCA) based on Temporal-PAE uses the previous time step to cyclicallyactivate the current time step to enhance text representation capability. Acluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classesand to reduce the semantic confusion zones in the common subspace.Comprehensive experiments demonstrate that PIR could enhance vision and textrepresentations and outperform the state-of-the-art methods of closed-domainand open-domain retrieval on two benchmark datasets, RSICD and RSITMD.</description><author>Jiancheng Pan, Muyuan Ma, Qing Ma, Cong Bai, Shengyong Chen</author><pubDate>Thu, 16 May 2024 15:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10160v1</guid></item><item><title>$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy</title><link>http://arxiv.org/abs/2401.01268v2</link><description>In deep learning, classification tasks are formalized as optimizationproblems often solved via the minimization of the cross-entropy. However,recent advancements in the design of objective functions allow the usage of the$f$-divergence to generalize the formulation of the optimization problem forclassification. We adopt a Bayesian perspective and formulate theclassification task as a maximum a posteriori probability problem. We propose aclass of objective functions based on the variational representation of the$f$-divergence. Furthermore, driven by the challenge of improving thestate-of-the-art approach, we propose a bottom-up method that leads us to theformulation of an objective function corresponding to a novel $f$-divergencereferred to as shifted log (SL). We theoretically analyze the objectivefunctions proposed and numerically test them in three application scenarios:toy examples, image datasets, and signal detection/decoding problems. Theanalyzed scenarios demonstrate the effectiveness of the proposed approach andthat the SL divergence achieves the highest classification accuracy in almostall the considered cases.</description><author>Nicola Novello, Andrea M. Tonello</author><pubDate>Thu, 16 May 2024 15:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01268v2</guid></item><item><title>Speaker Verification in Agent-Generated Conversations</title><link>http://arxiv.org/abs/2405.10150v1</link><description>The recent success of large language models (LLMs) has attracted widespreadinterest to develop role-playing conversational agents personalized to thecharacteristics and styles of different speakers to enhance their abilities toperform both general and special purpose dialogue tasks. However, the abilityto personalize the generated utterances to speakers, whether conducted by humanor LLM, has not been well studied. To bridge this gap, our study introduces anovel evaluation challenge: speaker verification in agent-generatedconversations, which aimed to verify whether two sets of utterances originatefrom the same speaker. To this end, we assemble a large dataset collectionencompassing thousands of speakers and their utterances. We also develop andevaluate speaker verification models under experiment setups. We furtherutilize the speaker verification models to evaluate the personalizationabilities of LLM-based role-playing models. Comprehensive experiments suggestthat the current role-playing models fail in accurately mimicking speakers,primarily due to their inherent linguistic characteristics.</description><author>Yizhe Yang, Heyan Huang, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim</author><pubDate>Thu, 16 May 2024 15:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10150v1</guid></item><item><title>Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey</title><link>http://arxiv.org/abs/2302.07200v3</link><description>Neurosymbolic AI is an increasingly active area of research that combinessymbolic reasoning methods with deep learning to leverage their complementarybenefits. As knowledge graphs are becoming a popular way to representheterogeneous and multi-relational data, methods for reasoning on graphstructures have attempted to follow this neurosymbolic paradigm. Traditionally,such approaches have utilized either rule-based inference or generatedrepresentative numerical embeddings from which patterns could be extracted.However, several recent studies have attempted to bridge this dichotomy togenerate models that facilitate interpretability, maintain competitiveperformance, and integrate expert knowledge. Therefore, we survey methods thatperform neurosymbolic reasoning tasks on knowledge graphs and propose a noveltaxonomy by which we can classify them. Specifically, we propose three majorcategories: (1) logically-informed embedding approaches, (2) embeddingapproaches with logical constraints, and (3) rule learning approaches.Alongside the taxonomy, we provide a tabular overview of the approaches andlinks to their source code, if available, for more direct comparison. Finally,we discuss the unique characteristics and limitations of these methods, thenpropose several prospective directions toward which this field of researchcould evolve.</description><author>Lauren Nicole DeLong, Ramon Fernández Mir, Jacques D. Fleuriot</author><pubDate>Thu, 16 May 2024 15:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07200v3</guid></item><item><title>SpecDETR: A Transformer-based Hyperspectral Point Object Detection Network</title><link>http://arxiv.org/abs/2405.10148v1</link><description>Hyperspectral target detection (HTD) aims to identify specific materialsbased on spectral information in hyperspectral imagery and can detect pointtargets, some of which occupy a smaller than one-pixel area. However, existingHTD methods are developed based on per-pixel binary classification, whichlimits the feature representation capability for point targets. In this paper,we rethink the hyperspectral point target detection from the object detectionperspective, and focus more on the object-level prediction capability ratherthan the pixel classification capability. Inspired by the token-basedprocessing flow of Detection Transformer (DETR), we propose the firstspecialized network for hyperspectral multi-class point object detection,SpecDETR. Without the backbone part of the current object detection framework,SpecDETR treats the spectral features of each pixel in hyperspectral images asa token and utilizes a multi-layer Transformer encoder with local and globalcoordination attention modules to extract deep spatial-spectral joint features.SpecDETR regards point object detection as a one-to-many set predictionproblem, thereby achieving a concise and efficient DETR decoder that surpassesthe current state-of-the-art DETR decoder in terms of parameters and accuracyin point object detection. We develop a simulated hyperSpectral Point ObjectDetection benchmark termed SPOD, and for the first time, evaluate and comparethe performance of current object detection networks and HTD methods onhyperspectral multi-class point object detection. SpecDETR demonstratessuperior performance as compared to current object detection networks and HTDmethods on the SPOD dataset. Additionally, we validate on a public HTD datasetthat by using data simulation instead of manual annotation, SpecDETR can detectreal-world single-spectral point objects directly.</description><author>Zhaoxu Li, Wei An, Gaowei Guo, Longguang Wang, Yingqian Wang, Zaiping Lin</author><pubDate>Thu, 16 May 2024 15:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10148v1</guid></item><item><title>Common Corruptions for Enhancing and Evaluating Robustness in Air-to-Air Visual Object Detection</title><link>http://arxiv.org/abs/2405.06765v2</link><description>The main barrier to achieving fully autonomous flights lies in autonomousaircraft navigation. Managing non-cooperative traffic presents the mostimportant challenge in this problem. The most efficient strategy for handlingnon-cooperative traffic is based on monocular video processing through deeplearning models. This study contributes to the vision-based deep learningaircraft detection and tracking literature by investigating the impact of datacorruption arising from environmental and hardware conditions on theeffectiveness of these methods. More specifically, we designed $7$ types ofcommon corruptions for camera inputs taking into account real-world flightconditions. By applying these corruptions to the Airborne Object Tracking (AOT)dataset we constructed the first robustness benchmark dataset named AOT-C forair-to-air aerial object detection. The corruptions included in this datasetcover a wide range of challenging conditions such as adverse weather and sensornoise. The second main contribution of this letter is to present an extensiveexperimental evaluation involving $8$ diverse object detectors to explore thedegradation in the performance under escalating levels of corruptions (domainshifts). Based on the evaluation results, the key observations that emerge arethe following: 1) One-stage detectors of the YOLO family demonstrate betterrobustness, 2) Transformer-based and multi-stage detectors like Faster R-CNNare extremely vulnerable to corruptions, 3) Robustness against corruptions isrelated to the generalization ability of models. The third main contribution isto present that finetuning on our augmented synthetic data results inimprovements in the generalisation ability of the object detector in real-worldflight experiments.</description><author>Anastasios Arsenos, Vasileios Karampinis, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos</author><pubDate>Thu, 16 May 2024 15:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06765v2</guid></item><item><title>Relational DNN Verification With Cross Executional Bound Refinement</title><link>http://arxiv.org/abs/2405.10143v1</link><description>We focus on verifying relational properties defined over deep neural networks(DNNs) such as robustness against universal adversarial perturbations (UAP),certified worst-case hamming distance for binary string classifications, etc.Precise verification of these properties requires reasoning about multipleexecutions of the same DNN. However, most of the existing works in DNNverification only handle properties defined over single executions and as aresult, are imprecise for relational properties. Though few recent works forrelational DNN verification, capture linear dependencies between the inputs ofmultiple executions, they do not leverage dependencies between the outputs ofhidden layers producing imprecise results. We develop a scalable relationalverifier RACoon that utilizes cross-execution dependencies at all layers of theDNN gaining substantial precision over SOTA baselines on a wide range ofdatasets, networks, and relational properties.</description><author>Debangshu Banerjee, Gagandeep Singh</author><pubDate>Thu, 16 May 2024 15:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10143v1</guid></item><item><title>TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa</title><link>http://arxiv.org/abs/2404.00297v2</link><description>Sentiment analysis is crucial for understanding public opinion and consumerbehavior. Existing models face challenges with linguistic diversity,generalizability, and explainability. We propose TRABSA, a hybrid frameworkintegrating transformer-based architectures, attention mechanisms, and BiLSTMnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridgegaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.Augmenting datasets with tweets from 32 countries and US states, we compare sixword-embedding techniques and three lexicon-based labeling techniques,selecting the best for optimal sentiment analysis. TRABSA outperformstraditional ML and deep learning models with 94% accuracy and significantprecision, recall, and F1-score gains. Evaluation across diverse datasetsdemonstrates consistent superiority and generalizability. SHAP and LIMEanalyses enhance interpretability, improving confidence in predictions. Ourstudy facilitates pandemic resource management, aiding resource planning,policy formation, and vaccination tactics.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, M. F. Mridha, Md Rashedul Islam, Yutaka Watanobe</author><pubDate>Thu, 16 May 2024 15:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00297v2</guid></item><item><title>Libra: Building Decoupled Vision System on Large Language Models</title><link>http://arxiv.org/abs/2405.10140v1</link><description>In this work, we introduce Libra, a prototype model with a decoupled visionsystem on a large language model (LLM). The decoupled vision system decouplesinner-modal modeling and cross-modal interaction, yielding unique visualinformation modeling and effective cross-modal comprehension. Libra is trainedthrough discrete auto-regressive modeling on both vision and language inputs.Specifically, we incorporate a routed visual expert with a cross-modal bridgemodule into a pretrained LLM to route the vision and language flows duringattention computing to enable different attention patterns in inner-modalmodeling and cross-modal interaction scenarios. Experimental resultsdemonstrate that the dedicated design of Libra achieves a strong MLLM baselinethat rivals existing works in the image-to-text scenario with merely 50 milliontraining data, providing a new perspective for future multimodal foundationmodels. Code is available at https://github.com/YifanXu74/Libra.</description><author>Yifan Xu, Xiaoshan Yang, Yaguang Song, Changsheng Xu</author><pubDate>Thu, 16 May 2024 15:34:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10140v1</guid></item><item><title>PL-MTEB: Polish Massive Text Embedding Benchmark</title><link>http://arxiv.org/abs/2405.10138v1</link><description>In this paper, we introduce the Polish Massive Text Embedding Benchmark(PL-MTEB), a comprehensive benchmark for text embeddings in Polish. The PL-MTEBconsists of 28 diverse NLP tasks from 5 task types. We adapted the tasks basedon previously used datasets by the Polish NLP community. In addition, wecreated a new PLSC (Polish Library of Science Corpus) dataset consisting oftitles and abstracts of scientific publications in Polish, which was used asthe basis for two novel clustering tasks. We evaluated 15 publicly availablemodels for text embedding, including Polish and multilingual ones, andcollected detailed results for individual tasks and aggregated results for eachtask type and the entire benchmark. PL-MTEB comes with open-source code athttps://github.com/rafalposwiata/pl-mteb.</description><author>Rafał Poświata, Sławomir Dadas, Michał Perełkiewicz</author><pubDate>Thu, 16 May 2024 15:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10138v1</guid></item><item><title>Scaling the weight parameters in Markov logic networks and relational logistic regression models</title><link>http://arxiv.org/abs/2103.15140v3</link><description>We consider Markov logic networks and relational logistic regression as twofundamental representation formalisms in statistical relational artificialintelligence that use weighted formulas in their specification. However, Markovlogic networks are based on undirected graphs, while relational logisticregression is based on directed acyclic graphs. We show that when scaling theweight parameters with the domain size, the asymptotic behaviour of arelational logistic regression model is transparently controlled by theparameters, and we supply an algorithm to compute asymptotic probabilities. Wealso show using two examples that this is not true for Markov logic networks.We also discuss using several examples, mainly from the literature, how theapplication context can help the user to decide when such scaling isappropriate and when using the raw unscaled parameters might be preferable. Wehighlight random sampling as a particularly promising area of application forscaled models and expound possible avenues for further research.</description><author>Felix Weitkämper</author><pubDate>Thu, 16 May 2024 15:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.15140v3</guid></item><item><title>Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention</title><link>http://arxiv.org/abs/2405.10134v1</link><description>In autonomous driving, accurately interpreting the movements of other roadusers and leveraging this knowledge to forecast future trajectories is crucial.This is typically achieved through the integration of map data and trackedtrajectories of various agents. Numerous methodologies combine this informationinto a singular embedding for each agent, which is then utilized to predictfuture behavior. However, these approaches have a notable drawback in that theymay lose exact location information during the encoding process. The encodingstill includes general map information. However, the generation of valid andconsistent trajectories is not guaranteed. This can cause the predictedtrajectories to stray from the actual lanes. This paper introduces a newrefinement module designed to project the predicted trajectories back onto theactual map, rectifying these discrepancies and leading towards more consistentpredictions. This versatile module can be readily incorporated into a widerange of architectures. Additionally, we propose a novel scene encoder thathandles all relations between agents and their environment in a single unifiedheterogeneous graph attention network. By analyzing the attention values on thedifferent edges in this graph, we can gain unique insights into the neuralnetwork's inner workings leading towards a more explainable prediction.</description><author>Tobias Demmler, Andreas Tamke, Thao Dang, Karsten Haug, Lars Mikelsons</author><pubDate>Thu, 16 May 2024 15:31:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10134v1</guid></item><item><title>Turkronicles: Diachronic Resources for the Fast Evolving Turkish Language</title><link>http://arxiv.org/abs/2405.10133v1</link><description>Over the past century, the Turkish language has undergone substantialchanges, primarily driven by governmental interventions. In this work, our goalis to investigate the evolution of the Turkish language since the establishmentof T\"urkiye in 1923. Thus, we first introduce Turkronicles which is adiachronic corpus for Turkish derived from the Official Gazette of T\"urkiye.Turkronicles contains 45,375 documents, detailing governmental actions, makingit a pivotal resource for analyzing the linguistic evolution influenced by thestate policies. In addition, we expand an existing diachronic Turkish corpuswhich consists of the records of the Grand National Assembly of T\"urkiye bycovering additional years. Next, combining these two diachronic corpora, weseek answers for two main research questions: How have the Turkish vocabularyand the writing conventions changed since the 1920s? Our analysis reveals thatthe vocabularies of two different time periods diverge more as the time betweenthem increases, and newly coined Turkish words take the place of their oldcounterparts. We also observe changes in writing conventions. In particular,the use of circumflex noticeably decreases and words ending with the letters"-b" and "-d" are successively replaced with "-p" and "-t" letters,respectively. Overall, this study quantitatively highlights the dramaticchanges in Turkish from various aspects of the language in a diachronicperspective.</description><author>Togay Yazar, Mucahid Kutlu, İsa Kerem Bayırlı</author><pubDate>Thu, 16 May 2024 15:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10133v1</guid></item><item><title>Cooperative Visual-LiDAR Extrinsic Calibration Technology for Intersection Vehicle-Infrastructure: A review</title><link>http://arxiv.org/abs/2405.10132v1</link><description>In the typical urban intersection scenario, both vehicles and infrastructuresare equipped with visual and LiDAR sensors. By successfully integrating thedata from vehicle-side and road monitoring devices, a more comprehensive andaccurate environmental perception and information acquisition can be achieved.The Calibration of sensors, as an essential component of autonomous drivingtechnology, has consistently drawn significant attention. Particularly inscenarios involving multiple sensors collaboratively perceiving and addressinglocalization challenges, the requirement for inter-sensor calibration becomescrucial. Recent years have witnessed the emergence of the concept of multi-endcooperation, where infrastructure captures and transmits surroundingenvironment information to vehicles, bolstering their perception capabilitieswhile mitigating costs. However, this also poses technical complexities,underscoring the pressing need for diverse end calibration. Camera and LiDAR,the bedrock sensors in autonomous driving, exhibit expansive applicability.This paper comprehensively examines and analyzes the calibration of multi-endcamera-LiDAR setups from vehicle, roadside, and vehicle-road cooperationperspectives, outlining their relevant applications and profound significance.Concluding with a summary, we present our future-oriented ideas and hypotheses.</description><author>Xinyu Zhang, Yijin Xiong, Qianxin Qu, Renjie Wang, Xin Gao, Jing Liu, Shichun Guo, Jun Li</author><pubDate>Thu, 16 May 2024 15:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10132v1</guid></item><item><title>StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis</title><link>http://arxiv.org/abs/2405.10129v1</link><description>The emergence of large language models (LLMs) capable of generating realistictexts and images has sparked ethical concerns across various sectors. Inresponse, researchers in academia and industry are actively exploring methodsto distinguish AI-generated content from human-authored material. However, acrucial question remains: What are the unique characteristics of AI-generatedtext? Addressing this gap, this study proposes StyloAI, a data-driven modelthat uses 31 stylometric features to identify AI-generated texts by applying aRandom Forest classifier on two multi-domain datasets. StyloAI achievesaccuracy rates of 81% and 98% on the test set of the AuTextification datasetand the Education dataset, respectively. This approach surpasses theperformance of existing state-of-the-art models and provides valuable insightsinto the differences between AI-generated and human-authored texts.</description><author>Chidimma Opara</author><pubDate>Thu, 16 May 2024 15:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10129v1</guid></item><item><title>Red Teaming Language Models for Contradictory Dialogues</title><link>http://arxiv.org/abs/2405.10128v1</link><description>Most language models currently available are prone to self-contradictionduring dialogues. To mitigate this issue, this study explores a novelcontradictory dialogue processing task that aims to detect and modifycontradictory statements in a conversation. This task is inspired by researchon context faithfulness and dialogue comprehension, which have demonstratedthat the detection and understanding of contradictions often necessitatedetailed explanations. We develop a dataset comprising contradictory dialogues,in which one side of the conversation contradicts itself. Each dialogue isaccompanied by an explanatory label that highlights the location and details ofthe contradiction. With this dataset, we present a Red Teaming framework forcontradictory dialogue processing. The framework detects and attempts toexplain the dialogue, then modifies the existing contradictory content usingthe explanation. Our experiments demonstrate that the framework improves theability to detect contradictory dialogues and provides valid explanations.Additionally, it showcases distinct capabilities for modifying such dialogues.Our study highlights the importance of the logical inconsistency problem inconversational AI.</description><author>Xiaofei Wen, Bangzheng Li, Tenghao Huang, Muhao Chen</author><pubDate>Thu, 16 May 2024 15:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10128v1</guid></item><item><title>Estimating a Function and Its Derivatives Under a Smoothness Condition</title><link>http://arxiv.org/abs/2405.10126v1</link><description>We consider the problem of estimating an unknown function f* and its partialderivatives from a noisy data set of n observations, where we make noassumptions about f* except that it is smooth in the sense that it has squareintegrable partial derivatives of order m. A natural candidate for theestimator of f* in such a case is the best fit to the data set that satisfies acertain smoothness condition. This estimator can be seen as a least squaresestimator subject to an upper bound on some measure of smoothness. Anotheruseful estimator is the one that minimizes the degree of smoothness subject toan upper bound on the average of squared errors. We prove that these twoestimators are computable as solutions to quadratic programs, establish theconsistency of these estimators and their partial derivatives, and study theconvergence rate as n increases to infinity. The effectiveness of theestimators is illustrated numerically in a setting where the value of a stockoption and its second derivative are estimated as functions of the underlyingstock price.</description><author>Eunji Lim</author><pubDate>Thu, 16 May 2024 15:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10126v1</guid></item><item><title>Ensuring UAV Safety: A Vision-only and Real-time Framework for Collision Avoidance Through Object Detection, Tracking, and Distance Estimation</title><link>http://arxiv.org/abs/2405.06749v2</link><description>In the last twenty years, unmanned aerial vehicles (UAVs) have garneredgrowing interest due to their expanding applications in both military andcivilian domains. Detecting non-cooperative aerial vehicles with efficiency andestimating collisions accurately are pivotal for achieving fully autonomousaircraft and facilitating Advanced Air Mobility (AAM). This paper presents adeep-learning framework that utilizes optical sensors for the detection,tracking, and distance estimation of non-cooperative aerial vehicles. Inimplementing this comprehensive sensing framework, the availability of depthinformation is essential for enabling autonomous aerial vehicles to perceiveand navigate around obstacles. In this work, we propose a method for estimatingthe distance information of a detected aerial object in real time using onlythe input of a monocular camera. In order to train our deep learning componentsfor the object detection, tracking and depth estimation tasks we utilize theAmazon Airborne Object Tracking (AOT) Dataset. In contrast to previousapproaches that integrate the depth estimation module into the object detector,our method formulates the problem as image-to-image translation. We employ aseparate lightweight encoder-decoder network for efficient and robust depthestimation. In a nutshell, the object detection module identifies and localizesobstacles, conveying this information to both the tracking module formonitoring obstacle movement and the depth estimation module for calculatingdistances. Our approach is evaluated on the Airborne Object Tracking (AOT)dataset which is the largest (to the best of our knowledge) air-to-air airborneobject dataset.</description><author>Vasileios Karampinis, Anastasios Arsenos, Orfeas Filippopoulos, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos</author><pubDate>Thu, 16 May 2024 15:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06749v2</guid></item><item><title>Bridging the Gap: Protocol Towards Fair and Consistent Affect Analysis</title><link>http://arxiv.org/abs/2405.06841v2</link><description>The increasing integration of machine learning algorithms in daily lifeunderscores the critical need for fairness and equity in their deployment. Asthese technologies play a pivotal role in decision-making, addressing biasesacross diverse subpopulation groups, including age, gender, and race, becomesparamount. Automatic affect analysis, at the intersection of physiology,psychology, and machine learning, has seen significant development. However,existing databases and methodologies lack uniformity, leading to biasedevaluations. This work addresses these issues by analyzing six affectivedatabases, annotating demographic attributes, and proposing a common protocolfor database partitioning. Emphasis is placed on fairness in evaluations.Extensive experiments with baseline and state-of-the-art methods demonstratethe impact of these changes, revealing the inadequacy of prior assessments. Thefindings underscore the importance of considering demographic attributes inaffect analysis research and provide a foundation for more equitablemethodologies. Our annotations, code and pre-trained models are available at:https://github.com/dkollias/Fair-Consistent-Affect-Analysis</description><author>Guanyu Hu, Eleni Papadopoulou, Dimitrios Kollias, Paraskevi Tzouveli, Jie Wei, Xinyu Yang</author><pubDate>Thu, 16 May 2024 15:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06841v2</guid></item><item><title>Asynchronous Federated Stochastic Optimization with Exact Averaging for Heterogeneous Local Objectives</title><link>http://arxiv.org/abs/2405.10123v1</link><description>Federated learning (FL) was recently proposed to securely train models withdata held over multiple locations ("clients") under the coordination of acentral server. Two major challenges hindering the performance of FL algorithmsare long training times caused by straggling clients and a decrease in trainingaccuracy induced by non-iid local distributions ("client drift"). In this workwe propose and analyze AREA, a new stochastic (sub)gradient algorithm that isrobust to client drift and utilizes asynchronous communication to speed upconvergence in the presence of stragglers. Moreover, AREA is, to the best ofour knowledge, the first method that is both guaranteed to converge underarbitrarily long delays, and converges to an error neighborhood whose sizedepends only on the variance of the stochastic (sub)gradients used and thus isindependent of both the heterogeneity between the local datasets and the lengthof client delays, without the use of delay-adaptive stepsizes. Our numericalresults confirm our theoretical analysis and suggest that AREA outperformsstate-of-the-art methods when local data are highly non-iid.</description><author>Charikleia Iakovidou, Kibaek Kim</author><pubDate>Thu, 16 May 2024 15:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10123v1</guid></item><item><title>The generalised distribution semantics and projective families of distributions</title><link>http://arxiv.org/abs/2211.06751v2</link><description>We generalise the distribution semantics underpinning probabilistic logicprogramming by distilling its essential concept, the separation of a freerandom component and a deterministic part. This abstracts the core ideas beyondlogic programming as such to encompass frameworks from probabilistic databases,probabilistic finite model theory and discrete lifted Bayesian networks. Todemonstrate the usefulness of such a general approach, we completelycharacterise the projective families of distributions representable in thegeneralised distribution semantics and we demonstrate both that large classesof interesting projective families cannot be represented in a generaliseddistribution semantics and that already a very limited fragment of logicprogramming (acyclic determinate logic programs) in the determinsitic partsuffices to represent all those projective families that are representable inthe generalised distribution semantics at all.</description><author>Felix Weitkämper</author><pubDate>Thu, 16 May 2024 15:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06751v2</guid></item><item><title>Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks</title><link>http://arxiv.org/abs/2405.10122v1</link><description>Multistep instructions, such as recipes and how-to guides, greatly benefitfrom visual aids, such as a series of images that accompany the instructionsteps. While Large Language Models (LLMs) have become adept at generatingcoherent textual steps, Large Vision/Language Models (LVLMs) are less capableof generating accompanying image sequences. The most challenging aspect is thateach generated image needs to adhere to the relevant textual step instruction,as well as be visually consistent with earlier images in the sequence. Toaddress this problem, we propose an approach for generating consistent imagesequences, which integrates a Latent Diffusion Model (LDM) with an LLM totransform the sequence into a caption to maintain the semantic coherence of thesequence. In addition, to maintain the visual coherence of the image sequence,we introduce a copy mechanism to initialise reverse diffusion processes with alatent vector iteration from a previously generated image from a relevant step.Both strategies will condition the reverse diffusion process on the sequence ofinstruction steps and tie the contents of the current image to previousinstruction steps and corresponding images. Experiments show that the proposedapproach is preferred by humans in 46.6% of the cases against 26.6% for thesecond best method. In addition, automatic metrics showed that the proposedmethod maintains semantic coherence and visual consistency across steps in bothdomains.</description><author>João Bordalo, Vasco Ramos, Rodrigo Valério, Diogo Glória-Silva, Yonatan Bitton, Michal Yarom, Idan Szpektor, Joao Magalhaes</author><pubDate>Thu, 16 May 2024 15:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10122v1</guid></item><item><title>Probabilities of the third type: Statistical Relational Learning and Reasoning with Relative Frequencies</title><link>http://arxiv.org/abs/2202.10367v3</link><description>Dependencies on the relative frequency of a state in the domain are commonwhen modelling probabilistic dependencies on relational data. For instance, thelikelihood of a school closure during an epidemic might depend on theproportion of infected pupils exceeding a threshold. Often, rather thandepending on discrete thresholds, dependencies are continuous: for instance,the likelihood of any one mosquito bite transmitting an illness depends on theproportion of carrier mosquitoes. Current approaches usually only considerprobabilities over possible worlds rather than over domain elements themselves.An exception are the recently introduced Lifted Bayesian Networks forConditional Probability Logic, which express discrete dependencies onprobabilistic data. We introduce functional lifted Bayesian networks, aformalism that explicitly incorporates continuous dependencies on relativefrequencies into statistical relational artificial intelligence. and compareand contrast them with ifted Bayesian Networks for Conditional ProbabilityLogic. Incorporating relative frequencies is not only beneficial to modelling;it also provides a more rigorous approach to learning problems where trainingand test or application domains have different sizes. To this end, we provide arepresentation of the asymptotic probability distributions induced byfunctional lifted Bayesian networks on domains of increasing sizes. Since thatrepresentation has well-understood scaling behaviour across domain sizes, itcan be used to estimate parameters for a large domain consistently fromrandomly sampled subpopulations. Furthermore, we show that in parametricfamilies of FLBN, convergence is uniform in the parameters, which ensures ameaningful dependence of the asymptotic probabilities on the parameters of themodel.</description><author>Felix Weitkämper</author><pubDate>Thu, 16 May 2024 15:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.10367v3</guid></item><item><title>Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation</title><link>http://arxiv.org/abs/2405.10121v1</link><description>Integrating multimodal knowledge into large language models (LLMs) representsa significant advancement in dialogue generation capabilities. However, theeffective incorporation of such knowledge in zero-resource scenarios remains asubstantial challenge due to the scarcity of diverse, high-quality dialoguedatasets. To address this, we propose the Visual Implicit KnowledgeDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMsfor enriched dialogue generation in zero-resource contexts by leveragingimplicit multimodal knowledge. VIKDF comprises two main stages: knowledgedistillation, using an Implicit Query Transformer to extract and encode visualimplicit knowledge from image-text pairs into knowledge vectors; and knowledgeintegration, employing a novel Bidirectional Variational Information Fusiontechnique to seamlessly integrate these distilled vectors into LLMs. Thisenables the LLMs to generate dialogues that are not only coherent and engagingbut also exhibit a deep understanding of the context through implicitmultimodal cues, effectively overcoming the limitations of zero-resourcescenarios. Our extensive experimentation across two dialogue datasets showsthat VIKDF outperforms existing state-of-the-art models in generatinghigh-quality dialogues. The code will be publicly available followingacceptance.</description><author>Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin</author><pubDate>Thu, 16 May 2024 15:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10121v1</guid></item><item><title>NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments</title><link>http://arxiv.org/abs/2401.01189v2</link><description>Neural implicit representations have been explored to enhance visual SLAMalgorithms, especially in providing high-fidelity dense map. Existing methodsoperate robustly in static scenes but struggle with the disruption caused bymoving objects. In this paper we present NID-SLAM, which significantly improvesthe performance of neural SLAM in dynamic environments. We propose a newapproach to enhance inaccurate regions in semantic masks, particularly inmarginal areas. Utilizing the geometric information present in depth images,this method enables accurate removal of dynamic objects, thereby reducing theprobability of camera drift. Additionally, we introduce a keyframe selectionstrategy for dynamic scenes, which enhances camera tracking robustness againstlarge-scale objects and improves the efficiency of mapping. Experiments onpublicly available RGB-D datasets demonstrate that our method outperformscompetitive neural SLAM approaches in tracking accuracy and mapping quality indynamic environments.</description><author>Ziheng Xu, Jianwei Niu, Qingfeng Li, Tao Ren, Chen Chen</author><pubDate>Thu, 16 May 2024 15:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01189v2</guid></item><item><title>Semi-supervised Anomaly Detection via Adaptive Reinforcement Learning-Enabled Method with Causal Inference for Sensor Signals</title><link>http://arxiv.org/abs/2405.06925v2</link><description>Semi-supervised anomaly detection for sensor signals is critical in ensuringsystem reliability in smart manufacturing. However, existing methods relyheavily on data correlation, neglecting causality and leading to potentialmisinterpretations due to confounding factors. Moreover, while currentreinforcement learning-based methods can effectively identify known and unknownanomalies with limited labeled samples, these methods still face severalchallenges, such as under-utilization of priori knowledge, lack of modelflexibility, and deficient reward feedback during environmental interactions.To address the above problems, this paper innovatively constructs acounterfactual causal reinforcement learning model, termed Triple-AssistedCausal Reinforcement Learning Anomaly Detector (Tri-CRLAD). The model leveragescausal inference to extract the intrinsic causal feature in data, enhancing theagent's utilization of prior knowledge and improving its generalizationcapability. In addition, Tri-CRLAD features a triple decision supportmechanism, including a sampling strategy based on historical similarity, anadaptive threshold smoothing adjustment strategy, and an adaptive decisionreward mechanism. These mechanisms further enhance the flexibility andgeneralization ability of the model, enabling it to effectively respond tovarious complex and dynamically changing environments. Experimental resultsacross seven diverse sensor signal datasets demonstrate that Tri-CRLADoutperforms nine state-of-the-art baseline methods. Notably, Tri-CRLAD achievesup to a 23\% improvement in anomaly detection stability with minimal knownanomaly samples, highlighting its potential in semi-supervised anomalydetection scenarios. Our code is available athttps://github.com/Aoudsung/Tri-CRLAD.</description><author>Xiangwei Chen, Ruliang Xiaoa, Zhixia Zeng, Zhipeng Qiu, Shi Zhang, Xin Du</author><pubDate>Thu, 16 May 2024 15:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06925v2</guid></item><item><title>Disguised Copyright Infringement of Latent Diffusion Models</title><link>http://arxiv.org/abs/2404.06737v3</link><description>Copyright infringement may occur when a generative model produces samplessubstantially similar to some copyrighted data that it had access to during thetraining phase. The notion of access usually refers to including copyrightedsamples directly in the training dataset, which one may inspect to identify aninfringement. We argue that such visual auditing largely overlooks a concealedcopyright infringement, where one constructs a disguise that looks drasticallydifferent from the copyrighted sample yet still induces the effect of trainingLatent Diffusion Models on it. Such disguises only require indirect access tothe copyrighted material and cannot be visually distinguished, thus easilycircumventing the current auditing tools. In this paper, we provide a betterunderstanding of such disguised copyright infringement by uncovering thedisguises generation algorithm, the revelation of the disguises, andimportantly, how to detect them to augment the existing toolbox. Additionally,we introduce a broader notion of acknowledgment for comprehending such indirectaccess.</description><author>Yiwei Lu, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu</author><pubDate>Thu, 16 May 2024 15:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06737v3</guid></item><item><title>UCB-driven Utility Function Search for Multi-objective Reinforcement Learning</title><link>http://arxiv.org/abs/2405.00410v2</link><description>In Multi-objective Reinforcement Learning (MORL) agents are tasked withoptimising decision-making behaviours that trade-off between multiple, possiblyconflicting, objectives. MORL based on decomposition is a family of solutionmethods that employ a number of utility functions to decompose themulti-objective problem into individual single-objective problems solvedsimultaneously in order to approximate a Pareto front of policies. We focus onthe case of linear utility functions parameterised by weight vectors w. Weintroduce a method based on Upper Confidence Bound to efficiently search forthe most promising weight vectors during different stages of the learningprocess, with the aim of maximising the hypervolume of the resulting Paretofront. The proposed method is shown to outperform various MORL baselines onMujoco benchmark problems across different random seeds. The code is online at:https://github.com/SYCAMORE-1/ucb-MOPPO.</description><author>Yucheng Shi, Alexandros Agapitos, David Lynch, Giorgio Cruciata, Cengis Hasan, Hao Wang, Yayu Yao, Aleksandar Milenovic</author><pubDate>Thu, 16 May 2024 15:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00410v2</guid></item><item><title>MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</title><link>http://arxiv.org/abs/2404.13923v3</link><description>Driven by powerful image diffusion models, recent research has achieved theautomatic creation of 3D objects from textual or visual guidance. By performingscore distillation sampling (SDS) iteratively across different views, thesemethods succeed in lifting 2D generative prior to the 3D space. However, such a2D generative image prior bakes the effect of illumination and shadow into thetexture. As a result, material maps optimized by SDS inevitably involvespurious correlated components. The absence of precise material definitionmakes it infeasible to relight the generated assets reasonably in novel scenes,which limits their application in downstream scenarios. In contrast, humans caneffortlessly circumvent this ambiguity by deducing the material of the objectfrom its appearance and semantics. Motivated by this insight, we proposeMaterialSeg3D, a 3D asset material generation framework to infer underlyingmaterial from the 2D semantic prior. Based on such a prior model, we devise amechanism to parse material in 3D space. We maintain a UV stack, each map ofwhich is unprojected from a specific viewpoint. After traversing allviewpoints, we fuse the stack through a weighted voting scheme and then employregion unification to ensure the coherence of the object parts. To fuel thelearning of semantics prior, we collect a material dataset, named MaterializedIndividual Objects (MIO), which features abundant images, diverse categories,and accurate annotations. Extensive quantitative and qualitative experimentsdemonstrate the effectiveness of our method.</description><author>Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, Jiaheng Liu, Ziwei Zhu Man Zhang, Qing Li, Xucheng Yin, Zhaoxiang Zhang, Junran Peng</author><pubDate>Thu, 16 May 2024 15:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13923v3</guid></item></channel></rss>