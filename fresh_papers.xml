<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 13 Nov 2024 13:00:06 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Material Transforms from Disentangled NeRF Representations</title><link>http://arxiv.org/abs/2411.08037v1</link><description>In this paper, we first propose a novel method for transferring materialtransformations across different scenes. Building on disentangled NeuralRadiance Field (NeRF) representations, our approach learns to map BidirectionalReflectance Distribution Functions (BRDF) from pairs of scenes observed invarying conditions, such as dry and wet. The learned transformations can thenbe applied to unseen scenes with similar materials, therefore effectivelyrendering the transformation learned with an arbitrary level of intensity.Extensive experiments on synthetic scenes and real-world objects validate theeffectiveness of our approach, showing that it can learn varioustransformations such as wetness, painting, coating, etc. Our results highlightnot only the versatility of our method but also its potential for practicalapplications in computer graphics. We publish our method implementation, alongwith our synthetic/real datasets onhttps://github.com/astra-vision/BRDFTransform</description><author>Ivan Lopes, Jean-Fran√ßois Lalonde, Raoul de Charette</author><pubDate>Tue, 12 Nov 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08037v1</guid></item><item><title>UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts</title><link>http://arxiv.org/abs/2411.07240v1</link><description>The evaluation of mathematical reasoning capabilities is essential foradvancing Artificial General Intelligence (AGI). While Large Language Models(LLMs) have shown impressive performance in solving mathematical problems,existing benchmarks such as GSM8K and MATH present limitations, includingnarrow problem definitions with specific numbers and reliance on predeterminedrules that hinder accurate assessments of reasoning and adaptability. Thispaper introduces the UTMath Benchmark, which robustly evaluates the modelsthrough extensive unit tests. It consists of 1,053 problems across 9mathematical domains, with over 68 test cases per problem.We propose aninnovative evaluation framework inspired by unit testing in softwaredevelopment, focusing on both accuracy and reliability of results. Furthermore,we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, whichencourages LLMs to perform explicit reasoning before generating code, leadingto generating more advanced solution and improved performance. Furthermore, weare releasing not only the UTMath benchmark but also the UTMath-Train trainingdataset (more than 70k samples), to support the community in further exploringmathematical reasoning.</description><author>Bo Yang, Qingping Yang, Runtao Liu</author><pubDate>Mon, 11 Nov 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07240v1</guid></item><item><title>DeepONet as a Multi-Operator Extrapolation Model: Distributed Pretraining with Physics-Informed Fine-Tuning</title><link>http://arxiv.org/abs/2411.07239v1</link><description>We propose a novel fine-tuning method to achieve multi-operator learningthrough training a distributed neural operator with diverse function data andthen zero-shot fine-tuning the neural network using physics-informed losses fordownstream tasks. Operator learning effectively approximates solution operatorsfor PDEs and various PDE-related problems, yet it often struggles to generalizeto new tasks. To address this, we investigate fine-tuning a pretrained model,while carefully selecting an initialization that enables rapid adaptation tonew tasks with minimal data. Our approach combines distributed learning tointegrate data from various operators in pre-training, while physics-informedmethods enable zero-shot fine-tuning, minimizing the reliance on downstreamdata. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,applying both to train complex nonlinear target operators that are difficult tolearn only using random initialization. Through comprehensive numericalexamples, we demonstrate the advantages of our approach, showcasing significantimprovements in accuracy. Our findings provide a robust framework for advancingmulti-operator learning and highlight the potential of transfer learningtechniques in this domain.</description><author>Zecheng Zhang, Christian Moya, Lu Lu, Guang Lin, Hayden Schaeffer</author><pubDate>Mon, 11 Nov 2024 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07239v1</guid></item><item><title>OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model</title><link>http://arxiv.org/abs/2411.07238v1</link><description>OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,finetuned on over 2,000,000 Thai instruction pairs. This report provides anengineering perspective on the model's development, capabilities, andperformance. We discuss the model's architecture, training process, and keyfeatures, including multi-turn conversation support, Retrieval AugmentedGeneration (RAG) compatibility, and tool-calling functionality. Benchmarkresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on variousThai language tasks, outperforming other open-source Thai language models. Wealso address practical considerations such as GPU memory requirements anddeployment strategies.</description><author>Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima</author><pubDate>Mon, 11 Nov 2024 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07238v1</guid></item><item><title>Contextualized Evaluations: Taking the Guesswork Out of Language Model Evaluations</title><link>http://arxiv.org/abs/2411.07237v1</link><description>Language model users often issue queries that lack specification, where thecontext under which a query was issued -- such as the user's identity, thequery's intent, and the criteria for a response to be useful -- is notexplicit. For instance, a good response to a subjective query like "What bookshould I read next?" would depend on the user's preferences, and a goodresponse to an open-ended query like "How do antibiotics work againstbacteria?" would depend on the user's expertise. This makes evaluation ofresponses to such queries an ill-posed task, as evaluators may make arbitraryjudgments about the response quality. To remedy this, we present contextualizedevaluations, a protocol that synthetically constructs context surrounding anunderspecified query and provides it during evaluation. We find that thepresence of context can 1) alter conclusions drawn from evaluation, evenflipping win rates between model pairs, 2) nudge evaluators to make fewerjudgments based on surface-level criteria, like style, and 3) provide newinsights about model behavior across diverse contexts. Specifically, ourprocedure uncovers an implicit bias towards WEIRD contexts in models' "default"responses and we find that models are not equally sensitive to followingdifferent contexts, even when they are provided in prompts.</description><author>Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo</author><pubDate>Mon, 11 Nov 2024 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07237v1</guid></item><item><title>Score-based generative diffusion with "active" correlated noise sources</title><link>http://arxiv.org/abs/2411.07233v1</link><description>Diffusion models exhibit robust generative properties by approximating theunderlying distribution of a dataset and synthesizing data by sampling from theapproximated distribution. In this work, we explore how the generativeperformance may be be modulated if noise sources with temporal correlations --akin to those used in the field of active matter -- are used for thedestruction of the data in the forward process. Our numerical and analyticalexperiments suggest that the corresponding reverse process may exhibit improvedgenerative properties.</description><author>Alexandra Lamtyugina, Agnish Kumar Behera, Aditya Nandy, Carlos Floyd, Suriyanarayanan Vaikuntanathan</author><pubDate>Mon, 11 Nov 2024 18:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07233v1</guid></item><item><title>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</title><link>http://arxiv.org/abs/2411.07232v1</link><description>Adding Object into images based on text instructions is a challenging task insemantic image editing, requiring a balance between preserving the originalscene and seamlessly integrating the new object in a fitting location. Despiteextensive efforts, existing models often struggle with this balance,particularly with finding a natural location for adding an object in complexscenes. We introduce Add-it, a training-free approach that extends diffusionmodels' attention mechanisms to incorporate information from three key sources:the scene image, the text prompt, and the generated image itself. Our weightedextended-attention mechanism maintains structural consistency and fine detailswhile ensuring natural object placement. Without task-specific fine-tuning,Add-it achieves state-of-the-art results on both real and generated imageinsertion benchmarks, including our newly constructed "Additing AffordanceBenchmark" for evaluating object placement plausibility, outperformingsupervised methods. Human evaluations show that Add-it is preferred in over 80%of cases, and it also demonstrates improvements in various automated metrics.</description><author>Yoad Tewel, Rinon Gal, Dvir Samuel Yuval Atzmon, Lior Wolf, Gal Chechik</author><pubDate>Mon, 11 Nov 2024 18:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07232v1</guid></item><item><title>Watermark Anything with Localized Messages</title><link>http://arxiv.org/abs/2411.07231v1</link><description>Image watermarking methods are not tailored to handle small watermarkedareas. This restricts applications in real-world scenarios where parts of theimage may come from different sources or have been edited. We introduce adeep-learning model for localized image watermarking, dubbed the WatermarkAnything Model (WAM). The WAM embedder imperceptibly modifies the input image,while the extractor segments the received image into watermarked andnon-watermarked areas and recovers one or several hidden messages from theareas found to be watermarked. The models are jointly trained at low resolutionand without perceptual constraints, then post-trained for imperceptibility andmultiple watermarks. Experiments show that WAM is competitive with state-of-theart methods in terms of imperceptibility and robustness, especially againstinpainting and splicing, even on high-resolution images. Moreover, it offersnew capabilities: WAM can locate watermarked areas in spliced images andextract distinct 32-bit messages with less than 1 bit error from multiple smallregions - no larger than 10% of the image surface - even for small $256\times256$ images.</description><author>Tom Sander, Pierre Fernandez, Alain Durmus, Teddy Furon, Matthijs Douze</author><pubDate>Mon, 11 Nov 2024 18:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07231v1</guid></item><item><title>INQUIRE: A Natural World Text-to-Image Retrieval Benchmark</title><link>http://arxiv.org/abs/2411.02537v3</link><description>We introduce INQUIRE, a text-to-image retrieval benchmark designed tochallenge multimodal vision-language models on expert-level queries. INQUIREincludes iNaturalist 2024 (iNat24), a new dataset of five million natural worldimages, along with 250 expert-level retrieval queries. These queries are pairedwith all relevant images comprehensively labeled within iNat24, comprising33,000 total matches. Queries span categories such as species identification,context, behavior, and appearance, emphasizing tasks that require nuanced imageunderstanding and domain expertise. Our benchmark evaluates two core retrievaltasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailedevaluation of a range of recent multimodal models demonstrates that INQUIREposes a significant challenge, with the best models failing to achieve anmAP@50 above 50%. In addition, we show that reranking with more powerfulmultimodal models can enhance retrieval performance, yet there remains asignificant margin for improvement. By focusing on scientifically-motivatedecological challenges, INQUIRE aims to bridge the gap between AI capabilitiesand the needs of real-world scientific inquiry, encouraging the development ofretrieval systems that can assist with accelerating ecological and biodiversityresearch. Our dataset and code are available athttps://inquire-benchmark.github.io</description><author>Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn</author><pubDate>Mon, 11 Nov 2024 18:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02537v3</guid></item><item><title>Learning from Limited and Imperfect Data</title><link>http://arxiv.org/abs/2411.07229v1</link><description>The datasets used for Deep Neural Network training (e.g., ImageNet, MSCOCO,etc.) are often manually balanced across categories (classes) to facilitatelearning of all the categories. This curation process is often expensive andrequires throwing away precious annotated data to balance the frequency acrossclasses. This is because the distribution of data in the world (e.g., internet,etc.) significantly differs from the well-curated datasets and is oftenover-populated with samples from common categories. The algorithms designed forwell-curated datasets perform suboptimally when used to learn from imperfectdatasets with long-tailed imbalances and distribution shifts. For deep modelsto be widely used, getting away with the costly curation process by developingrobust algorithms that can learn from real-world data distribution isnecessary. Toward this goal, we develop practical algorithms for Deep NeuralNetworks that can learn from limited and imperfect data present in the realworld. These works are divided into four segments, each covering a scenario oflearning from limited or imperfect data. The first part of the works focuses onLearning Generative Models for Long-Tail Data, where we mitigate themode-collapse for tail (minority) classes and enable diverse aesthetic imagegenerations as head (majority) classes. In the second part, we enable effectivegeneralization on tail classes through Inductive Regularization schemes, whichallow tail classes to generalize as the head classes without enforcing explicitgeneration of images. In the third part, we develop algorithms for OptimizingRelevant Metrics compared to the average accuracy for learning from long-taileddata with limited annotation (semi-supervised), followed by the fourth part,which focuses on the effective domain adaptation of the model to variousdomains with zero to very few labeled samples.</description><author>Harsh Rangwani</author><pubDate>Mon, 11 Nov 2024 18:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07229v1</guid></item><item><title>Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving</title><link>http://arxiv.org/abs/2411.07228v1</link><description>To enhance large language models (LLMs) for chemistry problem solving,several LLM-based agents augmented with tools have been proposed, such asChemCrow and Coscientist. However, their evaluations are narrow in scope,leaving a large gap in understanding the benefits of tools across diversechemistry tasks. To bridge this gap, we develop ChemAgent, an enhancedchemistry agent over ChemCrow, and conduct a comprehensive evaluation of itsperformance on both specialized chemistry tasks and general chemistryquestions. Surprisingly, ChemAgent does not consistently outperform its baseLLMs without tools. Our error analysis with a chemistry expert suggests that:For specialized chemistry tasks, such as synthesis prediction, we shouldaugment agents with specialized tools; however, for general chemistry questionslike those in exams, agents' ability to reason correctly with chemistryknowledge matters more, and tool augmentation does not always help.</description><author>Botao Yu, Frazier N. Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, Huan Sun</author><pubDate>Mon, 11 Nov 2024 18:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07228v1</guid></item><item><title>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling</title><link>http://arxiv.org/abs/2410.24210v2</link><description>Deep learning architectures for supervised learning on tabular data rangefrom simple multilayer perceptrons (MLP) to sophisticated Transformers andretrieval-augmented methods. This study highlights a major, yet so faroverlooked opportunity for substantially improving tabular MLPs: namely,parameter-efficient ensembling -- a paradigm for implementing an ensemble ofmodels as one model producing multiple predictions. We start by developing TabM-- a simple model based on MLP and our variations of BatchEnsemble (an existingtechnique). Then, we perform a large-scale evaluation of tabular DLarchitectures on public benchmarks in terms of both task performance andefficiency, which renders the landscape of tabular DL in a new light.Generally, we show that MLPs, including TabM, form a line of stronger and morepractical models compared to attention- and retrieval-based architectures. Inparticular, we find that TabM demonstrates the best performance among tabularDL models. Lastly, we conduct an empirical analysis on the ensemble-like natureof TabM. For example, we observe that the multiple predictions of TabM are weakindividually, but powerful collectively. Overall, our work brings an impactfultechnique to tabular DL, analyses its behaviour, and advances theperformance-efficiency trade-off with TabM -- a simple and powerful baselinefor researchers and practitioners.</description><author>Yury Gorishniy, Akim Kotelnikov, Artem Babenko</author><pubDate>Mon, 11 Nov 2024 18:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24210v2</guid></item><item><title>TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models</title><link>http://arxiv.org/abs/2411.07224v1</link><description>With the widespread of digital environments, reliable authentication andcontinuous access control has become crucial. It can minimize cyber attacks andprevent frauds, specially those associated with identity theft. A particularinterest lies on keystroke dynamics (KD), which refers to the task ofrecognizing individuals' identity based on their unique typing style. In thiswork, we propose the use of pre-trained language models (PLMs) to recognizesuch patterns. Although PLMs have shown high performance on multiple NLPbenchmarks, the use of these models on specific tasks requires customization.BERT and RoBERTa, for instance, rely on subword tokenization, and they cannotbe directly applied to KD, which requires temporal-character information torecognize users. Recent character-aware PLMs are able to process both subwordsand character-level information and can be an alternative solution.Notwithstanding, they are still not suitable to be directly fine-tuned for KDas they are not optimized to account for user's temporal typing information(e.g., hold time and flight time). To overcome this limitation, we proposeTempCharBERT, an architecture that incorporates temporal-character informationin the embedding layer of CharBERT. This allows modeling keystroke dynamics forthe purpose of user identification and authentication. Our results show asignificant improvement with this customization. We also showed the feasibilityof training TempCharBERT on a federated learning settings in order to fosterdata privacy.</description><author>Matheus Sim√£o, Fabiano Prado, Omar Abdul Wahab, Anderson Avila</author><pubDate>Mon, 11 Nov 2024 18:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07224v1</guid></item><item><title>Grounding Video Models to Actions through Goal Conditioned Exploration</title><link>http://arxiv.org/abs/2411.07223v1</link><description>Large video models, pretrained on massive amounts of Internet video, providea rich source of physical knowledge about the dynamics and motions of objectsand tasks. However, video models are not grounded in the embodiment of anagent, and do not describe how to actuate the world to reach the visual statesdepicted in a video. To tackle this problem, current methods use a separatevision-based inverse dynamic model trained on embodiment-specific data to mapimage states to actions. Gathering data to train such a model is oftenexpensive and challenging, and this model is limited to visual settings similarto the ones in which data are available. In this paper, we investigate how todirectly ground video models to continuous actions through self-exploration inthe embodied environment -- using generated video states as visual goals forexploration. We propose a framework that uses trajectory level actiongeneration in combination with video guidance to enable an agent to solvecomplex tasks without any external supervision, e.g., rewards, action labels,or segmentation masks. We validate the proposed approach on 8 tasks in Libero,6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor VisualNavigation. We show how our approach is on par with or even surpasses multiplebehavior cloning baselines trained on expert demonstrations while withoutrequiring any action annotations.</description><author>Yunhao Luo, Yilun Du</author><pubDate>Mon, 11 Nov 2024 18:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07223v1</guid></item><item><title>Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries</title><link>http://arxiv.org/abs/2409.12197v3</link><description>Artificial Intelligence (AI) for health has the potential to significantlychange and improve healthcare. However in most African countries, identifyingculturally and contextually attuned approaches for deploying these solutions isnot well understood. To bridge this gap, we conduct a qualitative study toinvestigate the best practices, fairness indicators, and potential biases tomitigate when deploying AI for health in African countries, as well as exploreopportunities where artificial intelligence could make a positive impact inhealth. We used a mixed methods approach combining in-depth interviews (IDIs)and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy,and AI across 17 countries, and through an inductive approach we conduct aqualitative thematic analysis on expert IDI responses. We administer a blinded30-minute survey with case studies to 672 general population participantsacross 5 countries in Africa and analyze responses on quantitative scales,statistically comparing responses by country, age, gender, and level offamiliarity with AI. We thematically summarize open-ended responses fromsurveys. Our results find generally positive attitudes, high levels of trust,accompanied by moderate levels of concern among general population participantsfor AI usage for health in Africa. This contrasts with expert responses, wheremajor themes revolved around trust/mistrust, ethical concerns, and systemicbarriers to integration, among others. This work presents the first-of-its-kindqualitative research study of the potential of AI for health in Africa from analgorithmic fairness angle, with perspectives from both experts and the generalpopulation. We hope that this work guides policymakers and drives home the needfor further research and the inclusion of general population perspectives indecision-making around AI usage.</description><author>Mercy Nyamewaa Asiedu, Iskandar Haykel, Awa Dieng, Kerrie Kauer, Tousif Ahmed, Florence Ofori, Charisma Chan, Stephen Pfohl, Negar Rostamzadeh, Katherine Heller</author><pubDate>Mon, 11 Nov 2024 18:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12197v3</guid></item><item><title>Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions</title><link>http://arxiv.org/abs/2411.03755v2</link><description>Understanding identifiability of latent content and style variables fromunaligned multi-domain data is essential for tasks such as domain translationand data generation. Existing works on content-style identification were oftendeveloped under somewhat stringent conditions, e.g., that all latent componentsare mutually independent and that the dimensions of the content and stylevariables are known. We introduce a new analytical framework via cross-domain\textit{latent distribution matching} (LDM), which establishes content-styleidentifiability under substantially more relaxed conditions. Specifically, weshow that restrictive assumptions such as component-wise independence of thelatent variables can be removed. Most notably, we prove that prior knowledge ofthe content and style dimensions is not necessary for ensuring identifiability,if sparsity constraints are properly imposed onto the learned latentrepresentations. Bypassing the knowledge of the exact latent dimension has beena longstanding aspiration in unsupervised representation learning -- ouranalysis is the first to underpin its theoretical and practical viability. Onthe implementation side, we recast the LDM formulation into a regularizedmulti-domain GAN loss with coupled latent variables. We show that thereformulation is equivalent to LDM under mild conditions -- yet requiringconsiderably less computational resource. Experiments corroborate with ourtheoretical claims.</description><author>Sagar Shrestha, Xiao Fu</author><pubDate>Mon, 11 Nov 2024 18:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03755v2</guid></item><item><title>TreeCoders: Trees of Transformers</title><link>http://arxiv.org/abs/2411.07218v1</link><description>In this paper, we introduce TreeCoders, a novel family of transformer trees.We moved away from traditional linear transformers to complete k-ary trees.Transformer blocks serve as nodes, and generic classifiers learn to select thebest child and route the sequence of tokens to a specific leaf. The selectors,moved outside the transformer blocks, allow for the use of a variety ofarchitecture without further modifications. Furthermore, our proposedarchitecture supports sparse node activation due to the logarithmic complexityof a tree search. We validate our idea by testing a series of decoder-only treetransformers, achieving competitive results across a diverse range of languagedatasets. Our study demonstrates that the proposed tree transformer modeloutperforms a size-equivalent linear transformer model 76\% of the time over awide range of tree architectures. Furthermore, our proposed model naturallylends itself to distributed implementation.</description><author>Pierre Colonna D'Istria, Abdulrahman Altahhan</author><pubDate>Mon, 11 Nov 2024 18:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07218v1</guid></item><item><title>An information field theory approach to Bayesian state and parameter estimation in dynamical systems</title><link>http://arxiv.org/abs/2306.02150v2</link><description>Dynamical system state estimation and parameter calibration problems areubiquitous across science and engineering. Bayesian approaches to the problemare the gold standard as they allow for the quantification of uncertainties andenable the seamless fusion of different experimental modalities. When thedynamics are discrete and stochastic, one may employ powerful techniques suchas Kalman, particle, or variational filters. Practitioners commonly apply thesemethods to continuous-time, deterministic dynamical systems after discretizingthe dynamics and introducing fictitious transition probabilities. However,approaches based on time-discretization suffer from the curse of dimensionalitysince the number of random variables grows linearly with the number oftime-steps. Furthermore, the introduction of fictitious transitionprobabilities is an unsatisfactory solution because it increases the number ofmodel parameters and may lead to inference bias. To address these drawbacks,the objective of this paper is to develop a scalable Bayesian approach to stateand parameter estimation suitable for continuous-time, deterministic dynamicalsystems. Our methodology builds upon information field theory. Specifically, weconstruct a physics-informed prior probability measure on the function space ofsystem responses so that functions that satisfy the physics are more likely.This prior allows us to quantify model form errors. We connect the system'sresponse to observations through a probabilistic model of the measurementprocess. The joint posterior over the system responses and all parameters isgiven by Bayes' rule. To approximate the intractable posterior, we develop astochastic variational inference algorithm. In summary, the developedmethodology offers a powerful framework for Bayesian estimation in dynamicalsystems.</description><author>Kairui Hao, Ilias Bilionis</author><pubDate>Mon, 11 Nov 2024 18:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02150v2</guid></item><item><title>Feature Selection Based on Wasserstein Distance</title><link>http://arxiv.org/abs/2411.07217v1</link><description>In this paper, we present a novel feature selection method based on theWasserstein distance. Feature selection plays a critical role in reducing thedimensionality of input data, thereby improving machine learning efficiency andgeneralization performance. Unlike traditional feature selection approachesthat rely on criteria such as correlation or KL divergence, our methodleverages the Wasserstein distance to measure the similarity betweendistributions of selected features and original features. This approachinherently accounts for similarities between classes, making it robust inscenarios involving noisy labels. Experimental results demonstrate that ourmethod outperforms traditional approaches, particularly in challenging settingsinvolving noisy labeled data.</description><author>Fuwei Li</author><pubDate>Mon, 11 Nov 2024 18:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07217v1</guid></item><item><title>Stronger Random Baselines for In-Context Learning</title><link>http://arxiv.org/abs/2404.13020v2</link><description>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes, extensive prompt-selectionusing the validation set, and intentionally difficult tasks that lead tonear-random performance. The standard random baseline--the expected accuracy ofguessing labels uniformly at random--is stable when the evaluation set is usedonly once or when the dataset is large. We account for the common practice ofvalidation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks, more than 20% of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available, this stronger baseline is alsoa better predictor of held-out performance than the standard baseline, avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</description><author>Gregory Yauney, David Mimno</author><pubDate>Mon, 11 Nov 2024 18:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13020v2</guid></item><item><title>Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks</title><link>http://arxiv.org/abs/2411.07213v1</link><description>A key objective of interpretability research on large language models (LLMs)is to develop methods for robustly steering models toward desired behaviors. Tothis end, two distinct approaches to interpretability -- ``bottom-up" and``top-down" -- have been presented, but there has been little quantitativecomparison between them. We present a case study comparing the effectiveness ofrepresentative vector steering methods from each branch: function vectors (FV;arXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;arXiv:2311.06668) as a top-down method. While both aim to capture compactrepresentations of broad in-context learning tasks, we find they are effectiveonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,whereas FVs excel in tasks requiring more precision. We discuss theimplications for future evaluations of steering methods and for furtherresearch into top-down and bottom-up steering given these findings.</description><author>Madeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, Usman Anwar</author><pubDate>Mon, 11 Nov 2024 18:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07213v1</guid></item><item><title>General Geospatial Inference with a Population Dynamics Foundation Model</title><link>http://arxiv.org/abs/2411.07207v1</link><description>Supporting the health and well-being of dynamic populations around the worldrequires governmental agencies, organizations and researchers to understand andreason over complex relationships between human behavior and local contexts inorder to identify high-risk groups and strategically allocate limitedresources. Traditional approaches to these classes of problems often entaildeveloping manually curated, task-specific features and models to representhuman behavior and the natural and built environment, which can be challengingto adapt to new, or even, related tasks. To address this, we introduce aPopulation Dynamics Foundation Model (PDFM) that aims to capture therelationships between diverse data modalities and is applicable to a broadrange of geospatial tasks. We first construct a geo-indexed dataset for postalcodes and counties across the United States, capturing rich aggregatedinformation on human behavior from maps, busyness, and aggregated searchtrends, and environmental factors such as weather and air quality. We thenmodel this data and the complex relationships between locations using a graphneural network, producing embeddings that can be adapted to a wide range ofdownstream tasks using relatively simple models. We evaluate the effectivenessof our approach by benchmarking it on 27 downstream tasks spanning threedistinct domains: health indicators, socioeconomic factors, and environmentalmeasurements. The approach achieves state-of-the-art performance on all 27geospatial interpolation tasks, and on 25 out of the 27 extrapolation andsuper-resolution tasks. We combined the PDFM with a state-of-the-artforecasting foundation model, TimesFM, to predict unemployment and poverty,achieving performance that surpasses fully supervised forecasting. The full setof embeddings and sample code are publicly available for researchers.</description><author>Mohit Agarwal, Mimi Sun, Chaitanya Kamath, Arbaaz Muslim, Prithul Sarker, Joydeep Paul, Hector Yee, Marcin Sieniek, Kim Jablonski, Yael Mayer, David Fork, Sheila de Guia, Jamie McPike, Adam Boulanger, Tomer Shekel, David Schottlander, Yao Xiao, Manjit Chakravarthy Manukonda, Yun Liu, Neslihan Bulut, Sami Abu-el-haija, Arno Eigenwillig, Parth Kothari, Bryan Perozzi, Monica Bharel, Von Nguyen, Luke Barrington, Niv Efron, Yossi Matias, Greg Corrado, Krish Eswaran, Shruthi Prabhakara, Shravya Shetty, Gautam Prasad</author><pubDate>Mon, 11 Nov 2024 18:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07207v1</guid></item><item><title>Scaling Law Hypothesis for Multimodal Model</title><link>http://arxiv.org/abs/2409.06754v4</link><description>We propose a scaling law hypothesis for multimodal models processing text,audio, images, and video within a shared token and embedding space. Ourframework predicts model performance based on modality-specific compression andtokenization efficiency, extending established scaling laws from text-baseddecoder models to mixed-modality systems. We explore whether leveraging moretraining data in multiple modalities can reduce the size of the multimodalmodel, enabling efficient deployment on resource-constrained devices.</description><author>Qingyun Sun, Zhen Guo, PIN AI Team</author><pubDate>Mon, 11 Nov 2024 18:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06754v4</guid></item><item><title>DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID</title><link>http://arxiv.org/abs/2411.07205v1</link><description>With the recent exhibited strength of generative diffusion models, an openresearch question is \textit{if images generated by these models can be used tolearn better visual representations}. While this generative data expansion maysuffice for easier visual tasks, we explore its efficacy on a more difficultdiscriminative task: clothes-changing person re-identification (CC-ReID).CC-ReID aims to match people appearing in non-overlapping cameras, even whenthey change their clothes across cameras. Not only are current CC-ReID modelsconstrained by the limited diversity of clothing in current CC-ReID datasets,but generating additional data that retains important personal features foraccurate identification is a current challenge. To address this issue wepropose DLCR, a novel data expansion framework that leverages pre-traineddiffusion and large language models (LLMs) to accurately generate diverseimages of individuals in varied attire. We generate additional data for fivebenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\textbf{increase their clothing diversity by \boldmath{$10$}x, totaling over\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guidedinpainting, conditioned on clothing prompts constructed using LLMs, to generatesynthetic data that only modifies a subject's clothes while preserving theirpersonally identifiable features. With this massive increase in data, weintroduce two novel strategies - progressive learning and test-time predictionrefinement - that respectively reduce training time and further boosts CC-ReIDperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvementof $11.3\%$ by training CAL, a previous state of the art (SOTA) method, withDLCR-generated data. We publicly release our code and generated data for eachdataset here: \url{https://github.com/CroitoruAlin/dlcr}.</description><author>Nyle Siddiqui, Florinel Alin Croitoru, Gaurav Kumar Nayak, Radu Tudor Ionescu, Mubarak Shah</author><pubDate>Mon, 11 Nov 2024 18:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07205v1</guid></item><item><title>Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media</title><link>http://arxiv.org/abs/2403.00037v2</link><description>With the rapid development of social media, the wide dissemination of fakenews on social media is increasingly threatening both individuals and society.One of the unique challenges for fake news detection on social media is how todetect fake news on future events. Recently, numerous fake news detectionmodels that utilize textual information and the propagation structure of postshave been proposed. Unfortunately, most of the existing approaches can hardlyhandle this challenge since they rely heavily on event-specific features forprediction and cannot generalize to unseen events. To address this, weintroduce \textbf{F}uture \textbf{AD}aptive \textbf{E}vent-based Fake newsDetection (FADE) framework. Specifically, we train a target predictor throughan adaptive augmentation strategy and graph contrastive learning to obtainhigher-quality features and make more accurate overall predictions.Simultaneously, we independently train an event-only predictor to obtain biasedpredictions. We further mitigate event bias by subtracting the event-onlypredictor's output from the target predictor's output to obtain the finalprediction. Encouraging results from experiments designed to emulate real-worldsocial media conditions validate the effectiveness of our method in comparisonto existing state-of-the-art approaches.</description><author>Jiajun Zhang, Zhixun Li, Qiang Liu, Shu Wu, Liang Wang</author><pubDate>Mon, 11 Nov 2024 18:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00037v2</guid></item><item><title>Programming Distributed Collective Processes in the eXchange Calculus</title><link>http://arxiv.org/abs/2401.11212v2</link><description>Recent trends like the Internet of Things (IoT) suggest a vision of dense andmulti-scale deployments of computing devices in nearly all kinds ofenvironments. A prominent engineering challenge revolves around programming thecollective adaptive behaviour of such computational ecosystems. This requiresabstractions able to capture concepts like ensembles (dynamic groups ofcooperating devices) and collective tasks (joint activities carried out byensembles). In this work, we consider collections of devices interacting withneighbours and that execute in nearly-synchronised sense-compute-interactrounds, where the computation is given by a single program mapping sensingvalues and incoming messages to output and outcoming messages. To supportprogramming whole computational collectives, we propose the abstraction of adistributed collective process, which can be used to define at once theensemble formation logic and its collective task. We formalise the abstractionin the eXchange Calculus (XC), a core functional language based on neighbouringvalues (maps from neighbours to values) where state and interaction is handledthrough a single primitive, exchange, and provide a correspondingimplementation in the FCPP language. Then, we exercise distributed collectiveprocesses using two case studies: multi-hop message propagation and distributedmonitoring of spatial properties. Finally, we discuss the features of theabstraction and its suitability for different kinds of distributed computingapplications.</description><author>Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Gianluca Torta, Mirko Viroli</author><pubDate>Mon, 11 Nov 2024 18:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11212v2</guid></item><item><title>'Explaining RL Decisions with Trajectories': A Reproducibility Study</title><link>http://arxiv.org/abs/2411.07200v1</link><description>This work investigates the reproducibility of the paper 'Explaining RLdecisions with trajectories'. The original paper introduces a novel approach inexplainable reinforcement learning based on the attribution decisions of anagent to specific clusters of trajectories encountered during training. Weverify the main claims from the paper, which state that (i) training on lesstrajectories induces a lower initial state value, (ii) trajectories in acluster present similar high-level patterns, (iii) distant trajectoriesinfluence the decision of an agent, and (iv) humans correctly identify theattributed trajectories to the decision of the agent. We recover theenvironments used by the authors based on the partial original code theyprovided for one of the environments (Grid-World), and implemented theremaining from scratch (Seaquest, HalfCheetah, Breakout and Q*Bert). While weconfirm that (i), (ii), and (iii) partially hold, we extend on the largelyqualitative experiments from the authors by introducing a quantitative metricto further support (iii), and new experiments and visual results for (i).Moreover, we investigate the use of different clustering algorithms and encoderarchitectures to further support (ii). We could not support (iv), given thelimited extent of the original experiments. We conclude that, while some of theclaims can be supported, further investigations and experiments could be ofinterest. We recognise the novelty of the work from the authors and hope thatour work paves the way for clearer and more transparent approaches.</description><author>Karim Abdel Sadek, Matteo Nulli, Joan Velja, Jort Vincenti</author><pubDate>Mon, 11 Nov 2024 18:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07200v1</guid></item><item><title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title><link>http://arxiv.org/abs/2411.07199v1</link><description>Instruction-guided image editing methods have demonstrated significantpotential by training diffusion models on automatically synthesized or manuallyannotated image editing pairs. However, these methods remain far frompractical, real-life applications. We identify three primary challengescontributing to this gap. Firstly, existing models have limited editing skillsdue to the biased synthesis process. Secondly, these methods are trained withdatasets with a high volume of noise and artifacts. This is due to theapplication of simple filtering methods like CLIP-score. Thirdly, all thesedatasets are restricted to a single low resolution and fixed aspect ratio,limiting the versatility to handle real-world use cases. In this paper, wepresent \omniedit, which is an omnipotent editor to handle seven differentimage editing tasks with any aspect ratio seamlessly. Our contribution is infour folds: (1) \omniedit is trained by utilizing the supervision from sevendifferent specialist models to ensure task coverage. (2) we utilize importancesampling based on the scores provided by large multimodal models (like GPT-4o)instead of CLIP-score to improve the data quality. (3) we propose a new editingarchitecture called EditNet to greatly boost the editing success rate, (4) weprovide images with different aspect ratios to ensure that our model can handleany image in the wild. We have curated a test set containing images ofdifferent aspect ratios, accompanied by diverse instructions to cover differenttasks. Both automatic evaluation and human evaluations demonstrate that\omniedit can significantly outperform all the existing models. Our code,dataset and model will be available at\url{https://tiger-ai-lab.github.io/OmniEdit/}</description><author>Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, Wenhu Chen</author><pubDate>Mon, 11 Nov 2024 18:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07199v1</guid></item><item><title>Data-Driven Predictive Control of Nonholonomic Robots Based on a Bilinear Koopman Realization: Data Does Not Replace Geometry</title><link>http://arxiv.org/abs/2411.07192v1</link><description>Advances in machine learning and the growing trend towards effortless datageneration in real-world systems has led to an increasing interest fordata-inferred models and data-based control in robotics. It seems appealing togovern robots solely based on data, bypassing the traditional, more elaboratepipeline of system modeling through first-principles and subsequent controllerdesign. One promising data-driven approach is the Extended Dynamic ModeDecomposition (EDMD) for control-affine systems, a system class which containsmany vehicles and machines of immense practical importance including, e.g.,typical wheeled mobile robots. EDMD can be highly data-efficient,computationally inexpensive, can deal with nonlinear dynamics as prevalent inrobotics and mechanics, and has a sound theoretical foundation rooted inKoopman theory. On this background, this present paper examines how EDMD modelscan be integrated into predictive controllers for nonholonomic mobile robots.In addition to the conventional kinematic mobile robot, we also cover thecomplete data-driven control pipeline - from data acquisition to control design- when the robot is not treated in terms of first-order kinematics but in asecond-order manner, allowing to account for actuator dynamics. Using onlyreal-world measurement data, it is shown in both simulations and hardwareexperiments that the surrogate models enable high-precision predictivecontrollers in the studied cases. However, the findings raise significantconcerns about purely data-centric approaches that overlook the underlyinggeometry of nonholonomic systems, showing that, for nonholonomic systems, somegeometric insight seems necessary and cannot be easily compensated for withlarge amounts of data.</description><author>Mario Rosenfelder, Lea Bold, Hannes Eschmann, Peter Eberhard, Karl Worthmann, Henrik Ebel</author><pubDate>Mon, 11 Nov 2024 18:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07192v1</guid></item><item><title>Diffusion Models for Audio Restoration</title><link>http://arxiv.org/abs/2402.09821v3</link><description>With the development of audio playback devices and fast data transmission,the demand for high sound quality is rising for both entertainment andcommunications. In this quest for better sound quality, challenges emerge fromdistortions and interferences originating at the recording side or caused by animperfect transmission pipeline. To address this problem, audio restorationmethods aim to recover clean sound signals from the corrupted input data. Wepresent here audio restoration algorithms based on diffusion models, with afocus on speech enhancement and music restoration tasks. Traditionalapproaches, often grounded in handcrafted rules and statistical heuristics,have shaped our understanding of audio signals. In the past decades, there hasbeen a notable shift towards data-driven methods that exploit the modelingcapabilities of DNNs. Deep generative models, and among them diffusion models,have emerged as powerful techniques for learning complex data distributions.However, relying solely on DNN-based learning approaches carries the risk ofreducing interpretability, particularly when employing end-to-end models.Nonetheless, data-driven approaches allow more flexibility in comparison tostatistical model-based frameworks, whose performance depends on distributionaland statistical assumptions that can be difficult to guarantee. Here, we aim toshow that diffusion models can combine the best of both worlds and offer theopportunity to design audio restoration algorithms with a good degree ofinterpretability and a remarkable performance in terms of sound quality. Weexplain the diffusion formalism and its application to the conditionalgeneration of clean audio signals. We believe that diffusion models open anexciting field of research with the potential to spawn new audio restorationalgorithms that are natural-sounding and remain robust in difficult acousticsituations.</description><author>Jean-Marie Lemercier, Julius Richter, Simon Welker, Eloi Moliner, Vesa V√§lim√§ki, Timo Gerkmann</author><pubDate>Mon, 11 Nov 2024 18:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09821v3</guid></item><item><title>The Super Weight in Large Language Models</title><link>http://arxiv.org/abs/2411.07191v1</link><description>Recent works have shown a surprising result: a small fraction of LargeLanguage Model (LLM) parameter outliers are disproportionately important to thequality of the model. LLMs contain billions of parameters, so these smallfractions, such as 0.01%, translate to hundreds of thousands of parameters. Inthis work, we present an even more surprising finding: Pruning as few as asingle parameter can destroy an LLM's ability to generate text -- increasingperplexity by 3 orders of magnitude and reducing zero-shot accuracy toguessing. We propose a data-free method for identifying such parameters, termedsuper weights, using a single forward pass through the model. We additionallyfind that these super weights induce correspondingly rare and large activationoutliers, termed super activations. When preserved with high precision, superactivations can improve simple round-to-nearest quantization to becomecompetitive with state-of-the-art methods. For weight quantization, wesimilarly find that by preserving the super weight and clipping other weightoutliers, round-to-nearest quantization can scale to much larger block sizesthan previously considered. To facilitate further research into super weights,we provide an index of super weight coordinates for common, openly availableLLMs.</description><author>Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan</author><pubDate>Mon, 11 Nov 2024 18:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07191v1</guid></item><item><title>NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics</title><link>http://arxiv.org/abs/2411.07186v1</link><description>Large language models (LLMs) prompted with text and audio represent the stateof the art in various auditory tasks, including speech, music, and generalaudio, showing emergent abilities on unseen tasks. However, these capabilitieshave yet to be fully demonstrated in bioacoustics tasks, such as detectinganimal vocalizations in large recordings, classifying rare and endangeredspecies, and labeling context and behavior - tasks that are crucial forconservation, biodiversity monitoring, and the study of animal behavior. Inthis work, we present NatureLM-audio, the first audio-language foundation modelspecifically designed for bioacoustics. Our carefully curated training datasetcomprises text-audio pairs spanning a diverse range of bioacoustics, speech,and music data, designed to address the challenges posed by limited annotateddatasets in the field. We demonstrate successful transfer of learnedrepresentations from music and speech to bioacoustics, and our model showspromising generalization to unseen taxa and tasks. Importantly, we testNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state ofthe art (SotA) on several bioacoustics tasks, including zero-shotclassification of unseen species. To advance bioacoustics research, we alsoopen-source the code for generating training and benchmark data, as well as fortraining the model.</description><author>David Robinson, Marius Miron, Masato Hagiwara, Olivier Pietquin</author><pubDate>Mon, 11 Nov 2024 18:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07186v1</guid></item><item><title>Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags</title><link>http://arxiv.org/abs/2406.10839v2</link><description>Despite recent advances in the general visual instruction-following abilityof Multimodal Large Language Models (MLLMs), they still struggle with criticalproblems when required to provide a precise and detailed response to a visualinstruction: (1) failure to identify novel objects or entities, (2) mention ofnon-existent objects, and (3) neglect of object's attributed details. Intuitivesolutions include improving the size and quality of data or using largerfoundation models. They show effectiveness in mitigating these issues, but atan expensive cost of collecting a vast amount of new data and introducing asignificantly larger model. Standing at the intersection of these approaches,we examine the three object-oriented problems from the perspective of theimage-to-text mapping process by the multimodal connector. In this paper, wefirst identify the limitations of multimodal connectors stemming frominsufficient training data. Driven by this, we propose to enhance the mappingwith retrieval-augmented tag tokens, which contain rich object-awareinformation such as object names and attributes. With our Tag-grounded visualinstruction tuning with retrieval Augmentation (TUNA), we outperform baselinesthat share the same language model and training data on 12 benchmarks.Furthermore, we show the zero-shot capability of TUNA when provided withspecific datastores.</description><author>Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li</author><pubDate>Mon, 11 Nov 2024 17:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10839v2</guid></item><item><title>Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2411.07185v1</link><description>Multi-source unsupervised domain adaptation aims to leverage labeled datafrom multiple source domains for training a machine learning model togeneralize well on a target domain without labels. Source domain selectionplays a crucial role in determining the model's performance. It relies on thesimilarities amongst source and target domains. Nonetheless, existing work forsource domain selection often involves heavyweight computational procedures,especially when dealing with numerous source domains and the need to identifythe best ones from them. In this paper, we introduce a framework for gradualfine tuning (GFT) of machine learning models on multiple source domains. Werepresent multiple source domains as an undirected weighted graph. We then givea new generalization error bound for GFT along any path within the graph, whichis used to determine the optimal path corresponding to the optimal trainingorder. With this formulation, we introduce three lightweight graph-routingstrategies which tend to minimize the error bound. Our best strategy improves$2.3\%$ of accuracy over the state-of-the-art on Natural Language Inference(NLI) task and achieves competitive performance on Sentiment Analysis (SA)task, especially a $3.9\%$ improvement on a more diverse subset of data we usefor SA.</description><author>Yao Ma, Samuel Louvan, Zhunxuan Wang</author><pubDate>Mon, 11 Nov 2024 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07185v1</guid></item><item><title>SAMPart3D: Segment Any Part in 3D Objects</title><link>http://arxiv.org/abs/2411.07184v1</link><description>3D part segmentation is a crucial and challenging task in 3D perception,playing a vital role in applications such as robotics, 3D generation, and 3Dediting. Recent methods harness the powerful Vision Language Models (VLMs) for2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation.However, these methods are limited by their reliance on text prompts, whichrestricts the scalability to large-scale unlabeled datasets and the flexibilityin handling part ambiguities. In this work, we introduce SAMPart3D, a scalablezero-shot 3D part segmentation framework that segments any 3D object intosemantic parts at multiple granularities, without requiring predefined partlabel sets as text prompts. For scalability, we use text-agnostic visionfoundation models to distill a 3D feature extraction backbone, allowing scalingto large unlabeled 3D datasets to learn rich 3D priors. For flexibility, wedistill scale-conditioned part-aware 3D features for 3D part segmentation atmultiple granularities. Once the segmented parts are obtained from thescale-conditioned part-aware 3D features, we use VLMs to assign semantic labelsto each part based on the multi-view renderings. Compared to previous methods,our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverseand handle complex, non-ordinary objects. Additionally, we contribute a new 3Dpart segmentation benchmark to address the lack of diversity and complexity ofobjects and parts in existing benchmarks. Experiments show that our SAMPart3Dsignificantly outperforms existing zero-shot 3D part segmentation methods, andcan facilitate various applications such as part-level editing and interactivesegmentation.</description><author>Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Y. Lam, Yan-Pei Cao, Xihui Liu</author><pubDate>Mon, 11 Nov 2024 17:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07184v1</guid></item><item><title>Revisiting Ensembling in One-Shot Federated Learning</title><link>http://arxiv.org/abs/2411.07182v1</link><description>Federated learning (FL) is an appealing approach to training machine learningmodels without sharing raw data. However, standard FL algorithms are iterativeand thus induce a significant communication cost. One-shot federated learning(OFL) trades the iterative exchange of models between clients and the serverwith a single round of communication, thereby saving substantially oncommunication costs. Not surprisingly, OFL exhibits a performance gap in termsof accuracy with respect to FL, especially under high data heterogeneity. Weintroduce FENS, a novel federated ensembling scheme that approaches theaccuracy of FL with the communication efficiency of OFL. Learning in FENSproceeds in two phases: first, clients train models locally and send them tothe server, similar to OFL; second, clients collaboratively train a lightweightprediction aggregator model using FL. We showcase the effectiveness of FENSthrough exhaustive experiments spanning several datasets and heterogeneitylevels. In the particular case of heterogeneously distributed CIFAR-10 dataset,FENS achieves up to a 26.9% higher accuracy over state-of-the-art (SOTA) OFL,being only 3.1% lower than FL. At the same time, FENS incurs at most 4.3x morecommunication than OFL, whereas FL is at least 10.9x morecommunication-intensive than FENS.</description><author>Youssef Allouah, Akash Dhasade, Rachid Guerraoui, Nirupam Gupta, Anne-Marie Kermarrec, Rafael Pinot, Rafael Pires, Rishi Sharma</author><pubDate>Mon, 11 Nov 2024 17:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07182v1</guid></item><item><title>Counterfactual Generation from Language Models</title><link>http://arxiv.org/abs/2411.07180v1</link><description>Understanding and manipulating the causal generation mechanisms in languagemodels is essential for controlling their behavior. Previous work has primarilyrelied on techniques such as representation surgery -- e.g., model ablations ormanipulation of linear subspaces tied to specific concepts -- to intervene onthese models. To understand the impact of interventions precisely, it is usefulto examine counterfactuals -- e.g., how a given sentence would have appearedhad it been generated by the model following a specific intervention. Wehighlight that counterfactual reasoning is conceptually distinct frominterventions, as articulated in Pearl's causal hierarchy. Based on thisobservation, we propose a framework for generating true string counterfactualsby reformulating language models as Generalized Structural-equation. Modelsusing the Gumbel-max trick. This allows us to model the joint distribution overoriginal strings and their counterfactuals resulting from the sameinstantiation of the sampling noise. We develop an algorithm based on hindsightGumbel sampling that allows us to infer the latent noise variables and generatecounterfactuals of observed strings. Our experiments demonstrate that theapproach produces meaningful counterfactuals while at the same time showingthat commonly used intervention techniques have considerable undesired sideeffects.</description><author>Shauli Ravfogel, Anej Svete, V√©steinn Sn√¶bjarnarson, Ryan Cotterell</author><pubDate>Mon, 11 Nov 2024 17:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07180v1</guid></item><item><title>Joint Age-State Belief is All You Need: Minimizing AoII via Pull-Based Remote Estimation</title><link>http://arxiv.org/abs/2411.07179v1</link><description>Age of incorrect information (AoII) is a recently proposed freshness andmismatch metric that penalizes an incorrect estimation along with its duration.Therefore, keeping track of AoII requires the knowledge of both the source andestimation processes. In this paper, we consider a time-slotted pull-basedremote estimation system under a sampling rate constraint where the informationsource is a general discrete-time Markov chain (DTMC) process. Moreover, packettransmission times from the source to the monitor are non-zero which disallowsthe monitor to have perfect information on the actual AoII process at any time.Hence, for this pull-based system, we propose the monitor to maintain asufficient statistic called {\em belief} which stands for the jointdistribution of the age and source processes to be obtained from the history ofall observations. Using belief, we first propose a maximum a posteriori (MAP)estimator to be used at the monitor as opposed to existing martingaleestimators in the literature. Second, we obtain the optimality equations fromthe belief-MDP (Markov decision process) formulation. Finally, we propose twobelief-dependent policies one of which is based on deep reinforcement learning,and the other one is a threshold-based policy based on the instantaneousexpected AoII.</description><author>Ismail Cosandal, Sennur Ulukus, Nail Akar</author><pubDate>Mon, 11 Nov 2024 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07179v1</guid></item><item><title>Understanding Long Videos with Multimodal Language Models</title><link>http://arxiv.org/abs/2403.16998v2</link><description>Large Language Models (LLMs) have allowed recent LLM-based approaches toachieve excellent performance on long-video understanding benchmarks. Weinvestigate how extensive world knowledge and strong reasoning skills ofunderlying LLMs influence this strong performance. Surprisingly, we discoverthat LLM-based approaches can yield surprisingly good accuracy on long-videotasks with limited video information, sometimes even with no video specificinformation. Building on this, we exploring injecting video-specificinformation into an LLM-based framework. We utilize off-the-shelf vision toolsto extract three object-centric information modalities from videos and thenleverage natural language as a medium for fusing this information. Ourresulting Multimodal Video Understanding (MVU) framework demonstratesstate-of-the-art performance across multiple video understanding benchmarks.Strong performance also on robotics domain tasks establish its stronggenerality. Our code will be released publicly.</description><author>Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo</author><pubDate>Mon, 11 Nov 2024 17:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16998v2</guid></item><item><title>More Expressive Attention with Negative Weights</title><link>http://arxiv.org/abs/2411.07176v1</link><description>We propose a novel attention mechanism, named Cog Attention, that enablesattention weights to be negative for enhanced expressiveness, which stems fromtwo key factors: (1) Cog Attention can shift the token deletion and copyingfunction from a static OV matrix to dynamic QK inner products, with the OVmatrix now focusing more on refinement or modification. The attention head cansimultaneously delete, copy, or retain tokens by assigning them negative,positive, or minimal attention weights, respectively. As a result, a singleattention head becomes more flexible and expressive. (2) Cog Attention improvesthe model's robustness against representational collapse, which can occur whenearlier tokens are over-squashed into later positions, leading to homogeneousrepresentations. Negative weights reduce effective information paths fromearlier to later tokens, helping to mitigate this issue. We developTransformer-like models which use Cog Attention as attention modules, includingdecoder-only models for language modeling and U-ViT diffusion models for imagegeneration. Experiments show that models using Cog Attention exhibit superiorperformance compared to those employing traditional softmax attention modules.Our approach suggests a promising research direction for rethinking andbreaking the entrenched constraints of traditional softmax attention, such asthe requirement for non-negative weights.</description><author>Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Rui Yan</author><pubDate>Mon, 11 Nov 2024 17:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07176v1</guid></item><item><title>Continual Memorization of Factoids in Large Language Models</title><link>http://arxiv.org/abs/2411.07175v1</link><description>Large language models can absorb a massive amount of knowledge throughpretraining, but pretraining is inefficient for acquiring long-tailed orspecialized facts. Therefore, fine-tuning on specialized or new knowledge thatreflects changes in the world has become popular, though it risks disruptingthe model's original capabilities. We study this fragility in the context ofcontinual memorization, where the model is trained on a small set of long-tailfactoids (factual associations) and must retain these factoids after multiplestages of subsequent training on other datasets. Through extensive experiments,we show that LLMs suffer from forgetting across a wide range of subsequenttasks, and simple replay techniques do not fully prevent forgetting, especiallywhen the factoid datasets are trained in the later stages. We posit that thereare two ways to alleviate forgetting: 1) protect the memorization process asthe model learns the factoids, or 2) reduce interference from training in laterstages. With this insight, we develop an effective mitigation strategy: REMIX(Random and Generic Data Mixing). REMIX prevents forgetting by mixing genericdata sampled from pretraining corpora or even randomly generated word sequencesduring each stage, despite being unrelated to the memorized factoids in thefirst stage. REMIX can recover performance from severe forgetting, oftenoutperforming replay-based methods that have access to the factoids from thefirst stage. We then analyze how REMIX alters the learning process and findthat successful forgetting prevention is associated with a pattern: the modelstores factoids in earlier layers than usual and diversifies the set of layersthat store these factoids. The efficacy of REMIX invites further investigationinto the underlying dynamics of memorization and forgetting, opening excitingpossibilities for future research.</description><author>Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen</author><pubDate>Mon, 11 Nov 2024 17:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07175v1</guid></item><item><title>Qwen2.5-Coder Technical Report</title><link>http://arxiv.org/abs/2409.12186v2</link><description>In this report, we introduce the Qwen2.5-Coder series, a significant upgradefrom its predecessor, CodeQwen1.5. This series includes six models:Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrainedon a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coderdemonstrates impressive code generation capabilities while retaining generaland math skills. These models have been evaluated on a wide range ofcode-related tasks, achieving state-of-the-art (SOTA) performance across morethan 10 benchmarks, including code generation, completion, reasoning, andrepair, consistently outperforming larger models of the same model size. Webelieve that the release of the Qwen2.5-Coder series will advance research incode intelligence and, with its permissive licensing, support wider adoption bydevelopers in real-world applications.</description><author>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin</author><pubDate>Mon, 11 Nov 2024 17:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12186v2</guid></item><item><title>FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2406.10740v3</link><description>Human motion synthesis is a fundamental task in computer animation. Despiterecent progress in this field utilizing deep learning and motion capture data,existing methods are always limited to specific motion categories,environments, and styles. This poor generalizability can be partiallyattributed to the difficulty and expense of collecting large-scale andhigh-quality motion data. At the same time, foundation models trained withinternet-scale image and text data have demonstrated surprising world knowledgeand reasoning ability for various downstream tasks. Utilizing these foundationmodels may help with human motion synthesis, which some recent works havesuperficially explored. However, these methods didn't fully unveil thefoundation models' potential for this task and only support several simpleactions and environments. In this paper, we for the first time, without anymotion data, explore open-set human motion synthesis using natural languageinstructions as user control signals based on MLLMs across any motion task andenvironment. Our framework can be split into two stages: 1) sequential keyframegeneration by utilizing MLLMs as a keyframe designer and animator; 2) motionfilling between keyframes through interpolation and motion tracking. Our methodcan achieve general human motion synthesis for many downstream tasks. Thepromising results demonstrate the worth of mocap-free human motion synthesisaided by MLLMs and pave the way for future research.</description><author>Zhikai Zhang, Yitang Li, Haofeng Huang, Mingxian Lin, Li Yi</author><pubDate>Mon, 11 Nov 2024 17:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10740v3</guid></item><item><title>Mutual Information Estimation via $f$-Divergence and Data Derangements</title><link>http://arxiv.org/abs/2305.20025v2</link><description>Estimating mutual information accurately is pivotal across diverseapplications, from machine learning to communications and biology, enabling usto gain insights into the inner mechanisms of complex systems. Yet, dealingwith high-dimensional data presents a formidable challenge, due to its size andthe presence of intricate relationships. Recently proposed neural methodsemploying variational lower bounds on the mutual information have gainedprominence. However, these approaches suffer from either high bias or highvariance, as the sample size and the structure of the loss function directlyinfluence the training process. In this paper, we propose a novel class ofdiscriminative mutual information estimators based on the variationalrepresentation of the $f$-divergence. We investigate the impact of thepermutation function used to obtain the marginal training samples and present anovel architectural solution based on derangements. The proposed estimator isflexible since it exhibits an excellent bias/variance trade-off. The comparisonwith state-of-the-art neural estimators, through extensive experimentationwithin established reference scenarios, shows that our approach offers higheraccuracy and lower complexity.</description><author>Nunzio A. Letizia, Nicola Novello, Andrea M. Tonello</author><pubDate>Mon, 11 Nov 2024 17:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20025v2</guid></item><item><title>Anytime Sequential Halving in Monte-Carlo Tree Search</title><link>http://arxiv.org/abs/2411.07171v1</link><description>Monte-Carlo Tree Search (MCTS) typically uses multi-armed bandit (MAB)strategies designed to minimize cumulative regret, such as UCB1, as itsselection strategy. However, in the root node of the search tree, it is moresensible to minimize simple regret. Previous work has proposed using SequentialHalving as selection strategy in the root node, as, in theory, it performsbetter with respect to simple regret. However, Sequential Halving requires abudget of iterations to be predetermined, which is often impractical. Thispaper proposes an anytime version of the algorithm, which can be halted at anyarbitrary time and still return a satisfactory result, while being designedsuch that it approximates the behavior of Sequential Halving. Empirical resultsin synthetic MAB problems and ten different board games demonstrate that thealgorithm's performance is competitive with Sequential Halving and UCB1 (andtheir analogues in MCTS).</description><author>Dominic Sagers, Mark H. M. Winands, Dennis J. N. J. Soemers</author><pubDate>Mon, 11 Nov 2024 17:49:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07171v1</guid></item><item><title>Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network</title><link>http://arxiv.org/abs/2411.07168v1</link><description>Mining machinery operating in variable environments faces high wear andunpredictable stress, challenging Predictive Maintenance (PdM). This paperintroduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), ahierarchical inference framework across edge devices, gateways, and cloudservices for real-time condition monitoring. The system dynamically adjustsinference locations--on-device, on-gateway, or on-cloud--based on trade-offsamong accuracy, latency, and battery life, leveraging Tiny Machine Learning(TinyML) techniques for model optimization on resource-constrained devices.Performance evaluations showed that on-sensor and on-gateway inference modesachieved over 90\% classification accuracy, while cloud-based inference reached99\%. On-sensor inference reduced power consumption by approximately 44\%,enabling up to 104 hours of operation. Latency was lowest for on-deviceinference (3.33 ms), increasing when offloading to the gateway (146.67 ms) orcloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solutionfor reliable anomaly detection and PdM, crucial for maintaining machineryuptime in remote environments. By balancing accuracy, latency, and energyconsumption, this approach advances PdM frameworks for industrial applications.</description><author>Ra√∫l de la Fuente, Luciano Radrigan, Anibal S Morales</author><pubDate>Mon, 11 Nov 2024 17:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07168v1</guid></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2410.10733v2</link><description>We present Deep Compression Autoencoder (DC-AE), a new family of autoencodermodels for accelerating high-resolution diffusion models. Existing autoencodermodels have demonstrated impressive results at a moderate spatial compressionratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy forhigh spatial compression ratios (e.g., 64x). We address this challenge byintroducing two key techniques: (1) Residual Autoencoding, where we design ourmodels to learn residuals based on the space-to-channel transformed features toalleviate the optimization difficulty of high spatial-compression autoencoders;(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phasestraining strategy for mitigating the generalization penalty of highspatial-compression autoencoders. With these designs, we improve theautoencoder's spatial compression ratio up to 128 while maintaining thereconstruction quality. Applying our DC-AE to latent diffusion models, weachieve significant speedup without accuracy drop. For example, on ImageNet512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedupon H100 GPU for UViT-H while achieving a better FID, compared with the widelyused SD-VAE-f8 autoencoder. Our code is available athttps://github.com/mit-han-lab/efficientvit.</description><author>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</author><pubDate>Mon, 11 Nov 2024 17:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10733v2</guid></item><item><title>A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19</title><link>http://arxiv.org/abs/2411.07163v1</link><description>Monitoring public sentiment via social media is potentially helpful duringhealth crises such as the COVID-19 pandemic. However, traditionalfrequency-based, data-driven neural network-based approaches can miss newlyrelevant content due to the evolving nature of language in a dynamicallyevolving environment. Human-curated symbolic knowledge sources, such aslexicons for standard language and slang terms, can potentially elevate socialmedia signals in evolving language. We introduce a neurosymbolic method thatintegrates neural networks with symbolic knowledge sources, enhancing thedetection and interpretation of mental health-related tweets relevant toCOVID-19. Our method was evaluated using a corpus of large datasets(approximately 12 billion tweets, 2.5 million subreddit data, and 700k newsarticles) and multiple knowledge graphs. This method dynamically adapts toevolving language, outperforming purely data-driven models with an F1 scoreexceeding 92\%. This approach also showed faster adaptation to new data andlower computational demands than fine-tuning pre-trained large language models(LLMs). This study demonstrates the benefit of neurosymbolic methods ininterpreting text in a dynamic environment for tasks such as healthsurveillance.</description><author>Vedant Khandelwal, Manas Gaur, Ugur Kursuncu, Valerie Shalin, Amit Sheth</author><pubDate>Mon, 11 Nov 2024 17:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07163v1</guid></item><item><title>RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2411.07161v1</link><description>This study investigates the efficacy of Multi-Agent Systems in elicitingcross-agent communication and enhancing collective intelligence through groupdecision-making in a decentralized setting. Unlike centralized mechanisms,where a fixed hierarchy governs social choice, decentralized groupdecision-making allows agents to engage in joint deliberation. Our researchfocuses on the dynamics of communication and decision-making within varioussocial choice methods. By applying different voting rules in variousenvironments, we find that moderate decision flexibility yields betteroutcomes. Additionally, exploring the linguistic features of agent-to-agentconversations reveals indicators of effective collaboration, offering insightsinto communication patterns that facilitate or hinder collaboration. Finally,we propose various methods for determining the optimal stopping point inmulti-agent collaborations based on linguistic cues. Our findings contribute toa deeper understanding of how decentralized decision-making and groupconversation shape multi-agent collaboration, with implications for the designof more effective MAS environments.</description><author>Young-Min Cho, Raphael Shu, Nilaksh Das, Tamer Alkhouli, Yi-An Lai, Jason Cai, Monica Sunkara, Yi Zhang</author><pubDate>Mon, 11 Nov 2024 17:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07161v1</guid></item><item><title>CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference Annotation</title><link>http://arxiv.org/abs/2411.02481v2</link><description>Preference tuning of large language models (LLMs) relies on high-qualityhuman preference data, which is often expensive and time-consuming to gather.While existing methods can use trained reward models or proprietary model asjudges for preference annotation, they have notable drawbacks: training rewardmodels remain dependent on initial human data, and using proprietary modelimposes license restrictions that inhibits commercial usage. In this paper, weintroduce customized density ratio (CDR), a training-free and highly effectivemethod that leverages off-the-shelf LLMs for preference data annotation. Ourapproach uses the log-density ratio between a better-aligned LLM and a lessaligned LLM as a reward signal. We explores 221 different LLMs pairs andempirically demonstrate that increasing the performance gap between paired LLMscorrelates with better reward generalization. Furthermore, we show thattailoring the density ratio reward function with specific criteria andpreference exemplars enhances performance across domains and within targetareas. In our experiment using density ratio from a pair of Mistral-7B models, CDRachieves a RewardBench score of 82.6, outperforming the best trained rewardfunctions from same model class and demonstrating competitive performanceagainst SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDRto annotate an on-policy preference dataset with which we preference tuneLlama-3-8B-Instruct with SimPO. Using reward signals from two relatively weakmodels, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate onArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,along with a score of 8.0 on MT-Bench.</description><author>Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava</author><pubDate>Mon, 11 Nov 2024 17:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02481v2</guid></item><item><title>A Primer on Word Embeddings: AI Techniques for Text Analysis in Social Work</title><link>http://arxiv.org/abs/2411.07156v1</link><description>Word embeddings represent a transformative technology for analyzing text datain social work research, offering sophisticated tools for understanding casenotes, policy documents, research literature, and other text-based materials.This methodological paper introduces word embeddings to social workresearchers, explaining how these mathematical representations capture meaningand relationships in text data more effectively than traditional keyword-basedapproaches. We discuss fundamental concepts, technical foundations, andpractical applications, including semantic search, clustering, and retrievalaugmented generation. The paper demonstrates how embeddings can enhanceresearch workflows through concrete examples from social work practice, such asanalyzing case notes for housing instability patterns and comparing social worklicensing examinations across languages. While highlighting the potential ofembeddings for advancing social work research, we acknowledge limitationsincluding information loss, training data constraints, and potential biases. Weconclude that successfully implementing embedding technologies in social workrequires developing domain-specific models, creating accessible tools, andestablishing best practices aligned with social work's ethical principles. Thisintegration can enhance our ability to analyze complex patterns in text datawhile supporting more effective services and interventions.</description><author>Brian E. Perron, Kelley A. Rivenburgh, Bryan G. Victor, Zia Qi, Hui Luan</author><pubDate>Mon, 11 Nov 2024 17:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07156v1</guid></item><item><title>Conditional simulation via entropic optimal transport: Toward non-parametric estimation of conditional Brenier maps</title><link>http://arxiv.org/abs/2411.07154v1</link><description>Conditional simulation is a fundamental task in statistical modeling:Generate samples from the conditionals given finitely many data points from ajoint distribution. One promising approach is to construct conditional Breniermaps, where the components of the map pushforward a reference distribution toconditionals of the target. While many estimators exist, few, if any, come withstatistical or algorithmic guarantees. To this end, we propose a non-parametricestimator for conditional Brenier maps based on the computational scalabilityof \emph{entropic} optimal transport. Our estimator leverages a result ofCarlier et al. (2010), which shows that optimal transport maps under a rescaledquadratic cost asymptotically converge to conditional Brenier maps; ourestimator is precisely the entropic analogues of these converging maps. Weprovide heuristic justifications for choosing the scaling parameter in the costas a function of the number of samples by fully characterizing the Gaussiansetting. We conclude by comparing the performance of the estimator to othermachine learning and non-parametric approaches on benchmark datasets andBayesian inference problems.</description><author>Ricardo Baptista, Aram-Alexandre Pooladian, Michael Brennan, Youssef Marzouk, Jonathan Niles-Weed</author><pubDate>Mon, 11 Nov 2024 17:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07154v1</guid></item><item><title>Using Large Language Models for Hyperparameter Optimization</title><link>http://arxiv.org/abs/2312.04528v2</link><description>This paper explores the use of foundational large language models (LLMs) inhyperparameter optimization (HPO). Hyperparameters are critical in determiningthe effectiveness of machine learning models, yet their optimization oftenrelies on manual approaches in limited-budget settings. By prompting LLMs withdataset and model descriptions, we develop a methodology where LLMs suggesthyperparameter configurations, which are iteratively refined based on modelperformance. Our empirical evaluations on standard benchmarks reveal thatwithin constrained search budgets, LLMs can match or outperform traditional HPOmethods like Bayesian optimization across different models on standardbenchmarks. Furthermore, we propose to treat the code specifying our model as ahyperparameter, which the LLM outputs and affords greater flexibility thanexisting HPO approaches.</description><author>Michael R. Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, Jimmy Ba</author><pubDate>Mon, 11 Nov 2024 17:30:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04528v2</guid></item><item><title>HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals</title><link>http://arxiv.org/abs/2411.07152v1</link><description>Task-Oriented Dialogue (TOD) systems assist users in completing tasks throughnatural language interactions, often relying on a single-layered workflowstructure for slot-filling in public tasks, such as hotel bookings. However, inenterprise environments, which involve rich domain-specific knowledge, TODsystems face challenges due to task complexity and the lack of standardizeddocumentation. In this work, we introduce HierTOD, an enterprise TOD systemdriven by hierarchical goals and can support composite workflows. By focusingon goal-driven interactions, our system serves a more proactive role,facilitating mixed-initiative dialogue and improving task completion. Equippedwith components for natural language understanding, composite goal retriever,dialogue management, and response generation, backed by a well-organized dataservice with domain knowledge base and retrieval engine, HierTOD deliversefficient task assistance. Furthermore, our system implementation unifies twoTOD paradigms: slot-filling for information collection and step-by-stepguidance for task execution. Our human study demonstrates the effectiveness andhelpfulness of HierTOD in performing both paradigms.</description><author>Lingbo Mo, Shun Jiang, Akash Maharaj, Bernard Hishamunda, Yunyao Li</author><pubDate>Mon, 11 Nov 2024 17:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07152v1</guid></item><item><title>VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding</title><link>http://arxiv.org/abs/2406.12384v2</link><description>We introduce a new benchmark designed to advance the development ofgeneral-purpose, large-scale vision-language models for remote sensing images.Although several vision-language datasets in remote sensing have been proposedto pursue this goal, existing datasets are typically tailored to single tasks,lack detailed object information, or suffer from inadequate quality control.Exploring these improvement opportunities, we present a Versatilevision-language Benchmark for Remote Sensing image understanding, termedVRSBench. This benchmark comprises 29,614 images, with 29,614 human-verifieddetailed captions, 52,472 object references, and 123,221 question-answer pairs.It facilitates the training and evaluation of vision-language models across abroad spectrum of remote sensing image understanding tasks. We furtherevaluated state-of-the-art models on this benchmark for three vision-languagetasks: image captioning, visual grounding, and visual question answering. Ourwork aims to significantly contribute to the development of advancedvision-language models in the field of remote sensing. The data and code can beaccessed at https://github.com/lx709/VRSBench.</description><author>Xiang Li, Jian Ding, Mohamed Elhoseiny</author><pubDate>Mon, 11 Nov 2024 17:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12384v2</guid></item><item><title>Knowledge Transfer in Deep Reinforcement Learning via an RL-Specific GAN-Based Correspondence Function</title><link>http://arxiv.org/abs/2209.06604v2</link><description>Deep reinforcement learning has demonstrated superhuman performance incomplex decision-making tasks, but it struggles with generalization andknowledge reuse - key aspects of true intelligence. This article introduces anovel approach that modifies Cycle Generative Adversarial Networks specificallyfor reinforcement learning, enabling effective one-to-one knowledge transferbetween two tasks. Our method enhances the loss function with two newcomponents: model loss, which captures dynamic relationships between source andtarget tasks, and Q-loss, which identifies states significantly influencing thetarget decision policy. Tested on the 2-D Atari game Pong, our method achieved100% knowledge transfer in identical tasks and either 100% knowledge transferor a 30% reduction in training time for a rotated task, depending on thenetwork architecture. In contrast, using standard Generative AdversarialNetworks or Cycle Generative Adversarial Networks led to worse performance thantraining from scratch in the majority of cases. The results demonstrate thatthe proposed method ensured enhanced knowledge generalization in deepreinforcement learning.</description><author>Marko Ruman, Tatiana V. Guy</author><pubDate>Mon, 11 Nov 2024 17:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06604v2</guid></item><item><title>Variational Graph Contrastive Learning</title><link>http://arxiv.org/abs/2411.07150v1</link><description>Graph representation learning (GRL) is a fundamental task in machinelearning, aiming to encode high-dimensional graph-structured data intolow-dimensional vectors. Self-supervised learning (SSL) methods are widely usedin GRL because they can avoid expensive human annotation. In this work, wepropose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Ourapproach introduces a subgraph Gaussian embedding module, which adaptively mapssubgraphs to a structured Gaussian space, ensuring the preservation of graphcharacteristics while controlling the distribution of generated subgraphs. Weemploy optimal transport distances, including Wasserstein andGromov-Wasserstein distances, to effectively measure the similarity betweensubgraphs, enhancing the robustness of the contrastive learning process.Extensive experiments across multiple benchmarks demonstrate that SGECoutperforms or presents competitive performance against state-of-the-artapproaches. Our findings provide insights into the design of SSL methods forGRL, emphasizing the importance of the distribution of the generatedcontrastive pairs.</description><author>Shifeng Xie, Jhony H. Giraldo</author><pubDate>Mon, 11 Nov 2024 17:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07150v1</guid></item><item><title>Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization</title><link>http://arxiv.org/abs/2406.02016v2</link><description>We propose adaptive, line search-free second-order methods with optimal rateof convergence for solving convex-concave min-max problems. By means of anadaptive step size, our algorithms feature a simple update rule that requiressolving only one linear system per iteration, eliminating the need for linesearch or backtracking mechanisms. Specifically, we base our algorithms on theoptimistic method and appropriately combine it with second-order information.Moreover, distinct from common adaptive schemes, we define the step sizerecursively as a function of the gradient norm and the prediction error in theoptimistic update. We first analyze a variant where the step size requiresknowledge of the Lipschitz constant of the Hessian. Under the additionalassumption of Lipschitz continuous gradients, we further design aparameter-free version by tracking the Hessian Lipschitz constant locally andensuring the iterates remain bounded. We also evaluate the practicalperformance of our algorithm by comparing it to existing second-orderalgorithms for minimax optimization.</description><author>Ruichen Jiang, Ali Kavis, Qiujiang Jin, Sujay Sanghavi, Aryan Mokhtari</author><pubDate>Mon, 11 Nov 2024 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02016v2</guid></item><item><title>Lost in Tracking Translation: A Comprehensive Analysis of Visual SLAM in Human-Centered XR and IoT Ecosystems</title><link>http://arxiv.org/abs/2411.07146v1</link><description>Advancements in tracking algorithms have empowered nascent applicationsacross various domains, from steering autonomous vehicles to guiding robots toenhancing augmented reality experiences for users. However, these algorithmsare application-specific and do not work across applications with differenttypes of motion; even a tracking algorithm designed for a given applicationdoes not work in scenarios deviating from highly standard conditions. Forexample, a tracking algorithm designed for robot navigation inside a buildingwill not work for tracking the same robot in an outdoor environment. Todemonstrate this problem, we evaluate the performance of the state-of-the-arttracking methods across various applications and scenarios. To inform ouranalysis, we first categorize algorithmic, environmental, andlocomotion-related challenges faced by tracking algorithms. We quantitativelyevaluate the performance using multiple tracking algorithms and representativedatasets for a wide range of Internet of Things (IoT) and Extended Reality (XR)applications, including autonomous vehicles, drones, and humans. Our analysisshows that no tracking algorithm works across different applications andscenarios within applications. Ultimately, using the insights generated fromour analysis, we discuss multiple approaches to improving the trackingperformance using input data characterization, leveraging intermediateinformation, and output evaluation.</description><author>Yasra Chandio, Khotso Selialia, Joseph DeGol, Luis Garcia, Fatima M. Anwar</author><pubDate>Mon, 11 Nov 2024 17:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07146v1</guid></item><item><title>Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt</title><link>http://arxiv.org/abs/2411.07142v1</link><description>Financial documents are filled with specialized terminology, arcane jargon,and curious acronyms that pose challenges for general-purpose text embeddings.Yet, few text embeddings specialized for finance have been reported in theliterature, perhaps in part due to a lack of public datasets and benchmarks. Wepresent BAM embeddings, a set of text embeddings finetuned on a carefullyconstructed dataset of 14.3M query-passage pairs. Demonstrating the benefits ofdomain-specific training, BAM embeddings achieve Recall@1 of 62.8% on aheld-out test set, vs. only 39.2% for the best general-purpose text embeddingfrom OpenAI. Further, BAM embeddings increase question answering accuracy by 8%on FinanceBench and show increased sensitivity to the finance-specific elementsthat are found in detailed, forward-looking and company and date-specificqueries. To support further research we describe our approach in detail,quantify the importance of hard negative mining and dataset scale.</description><author>Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, Charlie Flanagan</author><pubDate>Mon, 11 Nov 2024 17:13:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07142v1</guid></item><item><title>Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2411.07140v1</link><description>New LLM evaluation benchmarks are important to align with the rapiddevelopment of Large Language Models (LLMs). In this work, we present ChineseSimpleQA, the first comprehensive Chinese benchmark to evaluate the factualityability of language models to answer short questions, and Chinese SimpleQAmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6major topics with 99 diverse subtopics. Second, we conduct a comprehensivequality control process to achieve high-quality questions and answers, wherethe reference answers are static and cannot be changed over time. Third,following SimpleQA, the questions and answers are very short, and the gradingprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, weperform a comprehensive evaluation on the factuality abilities of existingLLMs. Finally, we hope that Chinese SimpleQA could guide the developers tobetter understand the Chinese factuality abilities of their models andfacilitate the growth of foundation models.</description><author>Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng</author><pubDate>Mon, 11 Nov 2024 17:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07140v1</guid></item><item><title>Nuremberg Letterbooks: A Multi-Transcriptional Dataset of Early 15th Century Manuscripts for Document Analysis</title><link>http://arxiv.org/abs/2411.07138v1</link><description>Most datasets in the field of document analysis utilize highly standardizedlabels, which, while simplifying specific tasks, often produce outputs that arenot directly applicable to humanities research. In contrast, the NurembergLetterbooks dataset, which comprises historical documents from the early 15thcentury, addresses this gap by providing multiple types of transcriptions andaccompanying metadata. This approach allows for developing methods that aremore closely aligned with the needs of the humanities. The dataset includes 4books containing 1711 labeled pages written by 10 scribes. Three types oftranscriptions are provided for handwritten text recognition: Basic,diplomatic, and regularized. For the latter two, versions with and withoutexpanded abbreviations are also available. A combination of letter ID andwriter ID supports writer identification due to changing writers within pages.In the technical validation, we established baselines for various tasks,demonstrating data consistency and providing benchmarks for future research tobuild upon.</description><author>Martin Mayr, Julian Krenz, Katharina Neumeier, Anna Bub, Simon B√ºrcky, Nina Brolich, Klaus Herbers, Mechthild Habermann, Peter Fleischmann, Andreas Maier, Vincent Christlein</author><pubDate>Mon, 11 Nov 2024 17:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07138v1</guid></item><item><title>Edify 3D: Scalable High-Quality 3D Asset Generation</title><link>http://arxiv.org/abs/2411.07135v1</link><description>We introduce Edify 3D, an advanced solution designed for high-quality 3Dasset generation. Our method first synthesizes RGB and surface normal images ofthe described object at multiple viewpoints using a diffusion model. Themulti-view observations are then used to reconstruct the shape, texture, andPBR materials of the object. Our method can generate high-quality 3D assetswith detailed geometry, clean shape topologies, high-resolution textures, andmaterials within 2 minutes of runtime.</description><author>NVIDIA, :, Maciej Bala, Yin Cui, Yifan Ding, Yunhao Ge, Zekun Hao, Jon Hasselgren, Jacob Huffman, Jingyi Jin, J. P. Lewis, Zhaoshuo Li, Chen-Hsuan Lin, Yen-Chen Lin, Tsung-Yi Lin, Ming-Yu Liu, Alice Luo, Qianli Ma, Jacob Munkberg, Stella Shi, Fangyin Wei, Donglai Xiang, Jiashu Xu, Xiaohui Zeng, Qinsheng Zhang</author><pubDate>Mon, 11 Nov 2024 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07135v1</guid></item><item><title>Stronger Models are NOT Stronger Teachers for Instruction Tuning</title><link>http://arxiv.org/abs/2411.07133v1</link><description>Instruction tuning has been widely adopted to ensure large language models(LLMs) follow user instructions effectively. The resultinginstruction-following capabilities of LLMs heavily rely on the instructiondatasets used for tuning. Recently, synthetic instruction datasets have emergedas an economically viable solution to provide LLMs diverse and high-qualityinstructions. However, existing approaches typically assume that larger orstronger models are stronger teachers for instruction tuning, and hence simplyadopt these models as response generators to the synthetic instructions. Inthis paper, we challenge this commonly-adopted assumption. Our extensiveexperiments across five base models and twenty response generators reveal thatlarger and stronger models are not necessarily stronger teachers of smallermodels. We refer to this phenomenon as the Larger Models' Paradox. We observethat existing metrics cannot precisely predict the effectiveness of responsegenerators since they ignore the compatibility between teachers and base modelsbeing fine-tuned. We thus develop a novel metric, named asCompatibility-Adjusted Reward (CAR) to measure the effectiveness of responsegenerators. Our experiments across five base models demonstrate that CARoutperforms almost all baselines.</description><author>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</author><pubDate>Mon, 11 Nov 2024 17:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07133v1</guid></item><item><title>ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition</title><link>http://arxiv.org/abs/2410.05774v4</link><description>Our world is full of varied actions and moves across specialized domains thatwe, as humans, strive to identify and understand. Within any single domain,actions can often appear quite similar, making it challenging for deep modelsto distinguish them accurately. To evaluate the effectiveness of multimodalfoundation models in helping us recognize such actions, we present ActionAtlasv1.0, a multiple-choice video question answering benchmark featuring shortvideos across various sports. Each video in the dataset is paired with aquestion and four or five choices. The question pinpoints specific individuals,asking which choice "best" describes their action within a certain temporalcontext. Overall, the dataset includes 934 videos showcasing 580 unique actionsacross 56 sports, with a total of 1896 actions within choices. Unlike mostexisting video question answering benchmarks that only cover simplisticactions, often identifiable from a single frame, ActionAtlas focuses onintricate movements and rigorously tests the model's capability to discernsubtle differences between moves that look similar within each domain. Weevaluate open and proprietary foundation models on this benchmark, finding thatthe best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile,Non-expert crowd workers, provided with action description for each choice,achieve 61.64% accuracy, where random chance is approximately 21%. Our findingswith state-of-the-art models indicate that having a high frame sampling rate isimportant for accurately recognizing actions in ActionAtlas, a feature thatsome leading proprietary video models, such as Gemini, do not include in theirdefault configuration.</description><author>Mohammadreza Salehi, Jae Sung Park, Tanush Yadav, Aditya Kusupati, Ranjay Krishna, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi</author><pubDate>Mon, 11 Nov 2024 17:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05774v4</guid></item><item><title>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2411.07132v1</link><description>Although text-to-image (T2I) models exhibit remarkable generationcapabilities, they frequently fail to accurately bind semantically relatedobjects or attributes in the input prompts; a challenge termed semanticbinding. Previous approaches either involve intensive fine-tuning of the entireT2I model or require users or large language models to specify generationlayouts, adding complexity. In this paper, we define semantic binding as thetask of associating a given object with its attribute, termed attributebinding, or linking it to other related sub-objects, referred to as objectbinding. We introduce a novel method called Token Merging (ToMe), whichenhances semantic binding by aggregating relevant tokens into a singlecomposite token. This ensures that the object, its attributes and sub-objectsall share the same cross-attention map. Additionally, to address potentialconfusion among main objects with complex textual prompts, we propose end tokensubstitution as a complementary strategy. To further refine our approach in theinitial stages of T2I generation, where layouts are determined, we incorporatetwo auxiliary losses, an entropy loss and a semantic binding loss, toiteratively update the composite token to improve the generation integrity. Weconducted extensive experiments to validate the effectiveness of ToMe,comparing it against various existing methods on the T2I-CompBench and ourproposed GPT-4o object binding benchmark. Our method is particularly effectivein complex scenarios that involve multiple objects and attributes, whichprevious methods often fail to address. The code will be publicly available at\url{https://github.com/hutaihang/ToMe}.</description><author>Taihang Hu, Linxuan Li, Joost van de Weijer, Hongcheng Gao, Fahad Shahbaz Khan, Jian Yang, Ming-Ming Cheng, Kai Wang, Yaxing Wang</author><pubDate>Mon, 11 Nov 2024 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07132v1</guid></item><item><title>Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation</title><link>http://arxiv.org/abs/2411.07130v1</link><description>Language models (LMs) have demonstrated an improved capacity to handlelong-context information, yet existing long-context benchmarks primarilymeasure LMs' retrieval abilities with extended inputs, e.g., pinpointing ashort phrase from long-form text. Therefore, they may fall short whenevaluating models' global context understanding capacity, such as synthesizingand reasoning over content across input to generate the response. In thispaper, we study long-context language model (LCLM) evaluation through many-shotin-context learning (ICL). Concretely, we identify the skills each ICL taskrequires, and examine models' long-context capabilities on them. We first ask:What types of ICL tasks benefit from additional demonstrations, and are thesetasks effective at evaluating LCLMs? We find that classification andsummarization tasks show notable performance improvements with additionaldemonstrations, while translation and reasoning tasks do not exhibit cleartrends. This suggests the classification tasks predominantly test models'retrieval skills. Next, we ask: To what extent does each task require retrievalskills versus global context understanding from LCLMs? We develop metrics tocategorize ICL tasks into two groups: (i) retrieval tasks that require strongretrieval ability to pinpoint relevant examples, and (ii) global contextunderstanding tasks that necessitate a deeper comprehension of the full input.We find that not all datasets can effectively evaluate these long-contextcapabilities. To address this gap, we introduce a new many-shot ICL benchmark,MANYICLBENCH, designed to characterize LCLMs' retrieval and global contextunderstanding capabilities separately. Benchmarking 11 open-weight LCLMs withMANYICLBENCH, we find that while state-of-the-art models perform well inretrieval tasks up to 64k tokens, many show significant drops in global contexttasks at just 16k tokens.</description><author>Kaijian Zou, Muhammad Khalifa, Lu Wang</author><pubDate>Mon, 11 Nov 2024 17:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07130v1</guid></item><item><title>Adam-mini: Use Fewer Learning Rates To Gain More</title><link>http://arxiv.org/abs/2406.16793v6</link><description>We propose Adam-mini, an optimizer that achieves on par or better performancethan AdamW with 50% less memory footprint. Adam-mini reduces memory by cuttingdown the learning rate resources in Adam (i.e., $1/\sqrt{v}$). By investigatingthe Hessian structure of neural nets, we find Adam's $v$ might not function atits full potential as effectively as we expected. We find that $\geq$ 99.9% ofthese learning rates in $v$ could be harmlessly removed if we (1) carefullypartition the parameters into blocks following our new principle on Hessianstructure; (2) assign a single but good learning rate to each parameter block.We then provide one simple way to find good learning rates and proposeAdam-mini. Empirically, we verify that Adam-mini performs on par or better thanAdamW on various language models sized from 39M to 13B for pre-training,supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-minialso alleviates communication overheads among GPUs, thereby increasingthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamWwhen pre-training Llama 2-7B on $2\times$ A800-80GB GPUs, which saves 33%wall-clock time for pre-training.</description><author>Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P. Kingma, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun</author><pubDate>Mon, 11 Nov 2024 16:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16793v6</guid></item><item><title>Enhancing learning in spiking neural networks through neuronal heterogeneity and neuromodulatory signaling</title><link>http://arxiv.org/abs/2407.04525v4</link><description>Recent progress in artificial intelligence (AI) has been driven by insightsfrom neuroscience, particularly with the development of artificial neuralnetworks (ANNs). This has significantly enhanced the replication of complexcognitive tasks such as vision and natural language processing. Despite theseadvances, ANNs struggle with continual learning, adaptable knowledge transfer,robustness, and resource efficiency - capabilities that biological systemshandle seamlessly. Specifically, ANNs often overlook the functional andmorphological diversity of the brain, hindering their computationalcapabilities. Furthermore, incorporating cell-type specific neuromodulatoryeffects into ANNs with neuronal heterogeneity could enable learning at twospatial scales: spiking behavior at the neuronal level, and synaptic plasticityat the circuit level, thereby potentially enhancing their learning abilities.In this article, we summarize recent bio-inspired models, learning rules andarchitectures and propose a biologically-informed framework for enhancing ANNs.Our proposed dual-framework approach highlights the potential of spiking neuralnetworks (SNNs) for emulating diverse spiking behaviors and dendriticcompartments to simulate morphological and functional diversity of neuronalcomputations. Finally, we outline how the proposed approach integratesbrain-inspired compartmental models and task-driven SNNs, balancesbioinspiration and complexity, and provides scalable solutions for pressing AIchallenges, such as continual learning, adaptability, robustness, andresource-efficiency.</description><author>Alejandro Rodriguez-Garcia, Jie Mei, Srikanth Ramaswamy</author><pubDate>Mon, 11 Nov 2024 16:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04525v4</guid></item><item><title>Benchmarking LLMs' Judgments with No Gold Standard</title><link>http://arxiv.org/abs/2411.07127v1</link><description>We introduce the GEM (Generative Estimator for Mutual Information), anevaluation metric for assessing language generation by Large Language Models(LLMs), particularly in generating informative judgments, without the need fora gold standard reference. GEM broadens the scenarios where we can benchmarkLLM generation performance-from traditional ones, like machine translation andsummarization, where gold standard references are readily available, tosubjective tasks without clear gold standards, such as academic peer review. GEM uses a generative model to estimate mutual information between candidateand reference responses, without requiring the reference to be a gold standard.In experiments on a human-annotated dataset, GEM demonstrates competitivecorrelations with human scores compared to the state-of-the-art GPT-4oExaminer, and outperforms all other baselines. Additionally, GEM is more robustagainst strategic manipulations, such as rephrasing or elongation, which canartificially inflate scores under a GPT-4o Examiner. We also present GRE-bench (Generating Review Evaluation Benchmark) whichevaluates LLMs based on how well they can generate high-quality peer reviewsfor academic research papers. Because GRE-bench is based upon GEM, it inheritsits robustness properties. Additionally, GRE-bench circumvents datacontamination problems (or data leakage) by using the continuous influx of newopen-access research papers and peer reviews each year. We show GRE-benchresults of various popular LLMs on their peer review capabilities using theICLR2023 dataset.</description><author>Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong</author><pubDate>Mon, 11 Nov 2024 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07127v1</guid></item><item><title>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</title><link>http://arxiv.org/abs/2411.07126v1</link><description>We introduce Edify Image, a family of diffusion models capable of generatingphotorealistic image content with pixel-perfect accuracy. Edify Image utilizescascaded pixel-space diffusion models trained using a novel Laplacian diffusionprocess, in which image signals at different frequency bands are attenuated atvarying rates. Edify Image supports a wide range of applications, includingtext-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panoramageneration, and finetuning for image customization.</description><author>NVIDIA, :, Yuval Atzmon, Maciej Bala, Yogesh Balaji, Tiffany Cai, Yin Cui, Jiaojiao Fan, Yunhao Ge, Siddharth Gururani, Jacob Huffman, Ronald Isaac, Pooya Jannaty, Tero Karras, Grace Lam, J. P. Lewis, Aaron Licata, Yen-Chen Lin, Ming-Yu Liu, Qianli Ma, Arun Mallya, Ashlee Martino-Tarr, Doug Mendez, Seungjun Nah, Chris Pruett, Fitsum Reda, Jiaming Song, Ting-Chun Wang, Fangyin Wei, Xiaohui Zeng, Yu Zeng, Qinsheng Zhang</author><pubDate>Mon, 11 Nov 2024 16:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07126v1</guid></item><item><title>Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes</title><link>http://arxiv.org/abs/2405.06687v2</link><description>With the impressive performance in various downstream tasks, large languagemodels (LLMs) have been widely integrated into production pipelines, likerecruitment and recommendation systems. A known issue of models trained onnatural language data is the presence of human biases, which can impact thefairness of the system. This paper investigates LLMs' behavior with respect togender stereotypes, in the context of occupation decision making. Our frameworkis designed to investigate and quantify the presence of gender stereotypes inLLMs' behavior via multi-round question answering. Inspired by prior works, weconstruct a dataset by leveraging a standard occupation classificationknowledge base released by authoritative agencies. We tested three LLMs(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all modelsexhibit gender stereotypes analogous to human biases, but with differentpreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat mayimply the current alignment methods are insufficient for debiasing and couldintroduce new biases contradicting the traditional gender stereotypes.</description><author>Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz</author><pubDate>Mon, 11 Nov 2024 16:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06687v2</guid></item><item><title>Fast and Robust Contextual Node Representation Learning over Dynamic Graphs</title><link>http://arxiv.org/abs/2411.07123v1</link><description>Real-world graphs grow rapidly with edge and vertex insertions over time,motivating the problem of efficiently maintaining robust node representationover evolving graphs. Recent efficient GNNs are designed to decouple recursivemessage passing from the learning process, and favor Personalized PageRank(PPR) as the underlying feature propagation mechanism. However, most PPR-basedGNNs are designed for static graphs, and efficient PPR maintenance remains asan open problem. Further, there is surprisingly little theoreticaljustification for the choice of PPR, despite its impressive empiricalperformance. In this paper, we are inspired by the recent PPR formulation as an explicit$\ell_1$-regularized optimization problem and propose a unified dynamic graphlearning framework based on sparse node-wise attention. We also present a setof desired properties to justify the choice of PPR in STOA GNNs, and serves asthe guideline for future node attention designs. Meanwhile, we take advantageof the PPR-equivalent optimization formulation and employ the proximal gradientmethod (ISTA) to improve the efficiency of PPR-based GNNs upto 6 times.Finally, we instantiate a simple-yet-effective model (\textsc{GoPPE}) withrobust positional encodings by maximizing PPR previously used as attention. Themodel performs comparably to or better than the STOA baselines and greatlyoutperforms when the initial node attributes are noisy during graph evolution,demonstrating the effectiveness and robustness of \textsc{GoPPE}.</description><author>Xingzhi Guo, Silong Wang, Baojian Zhou, Yanghua Xiao, Steven Skiena</author><pubDate>Mon, 11 Nov 2024 16:51:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07123v1</guid></item><item><title>SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs</title><link>http://arxiv.org/abs/2411.07122v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities ingenerating human-like text, but their output may not be aligned with the useror even produce harmful content. This paper presents a novel approach to detectand steer concepts such as toxicity before generation. We introduce the SparseConditioned Autoencoder (SCAR), a single trained module that extends theotherwise untouched LLM. SCAR ensures full steerability, towards and away fromconcepts (e.g., toxic content), without compromising the quality of the model'stext generation on standard evaluation benchmarks. We demonstrate the effectiveapplication of our approach through a variety of concepts, including toxicity,safety, and writing style alignment. As such, this work establishes a robustframework for controlling LLM generations, ensuring their ethical and safedeployment in real-world applications.</description><author>Ruben H√§rle, Felix Friedrich, Manuel Brack, Bj√∂rn Deiseroth, Patrick Schramowski, Kristian Kersting</author><pubDate>Mon, 11 Nov 2024 16:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07122v1</guid></item><item><title>Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models</title><link>http://arxiv.org/abs/2411.07121v1</link><description>Neural decoding, the process of understanding how brain activity correspondsto different stimuli, has been a primary objective in cognitive sciences. Overthe past three decades, advancements in functional Magnetic Resonance Imagingand machine learning have greatly improved our ability to map visual stimuli tobrain activity, especially in the visual cortex. Concurrently, research hasexpanded into decoding more complex processes like language and memory acrossthe whole brain, utilizing techniques to handle greater variability and improvesignal accuracy. We argue that "seeing" involves more than just mapping visualstimuli onto the visual cortex; it engages the entire brain, as variousemotions and cognitive states can emerge from observing different scenes. Inthis paper, we develop algorithms to enhance our understanding of visualprocesses by incorporating whole-brain activation maps while individuals areexposed to visual stimuli. We utilize large-scale fMRI encoders and Imagegenerative models pre-trained on large public datasets, which are thenfine-tuned through Image-fMRI contrastive learning. Our models hence can decodevisual experience across the entire cerebral cortex, surpassing the traditionalconfines of the visual cortex. We first compare our method withstate-of-the-art approaches to decoding visual processing and show improvedpredictive semantic accuracy by 43%. A network ablation analysis suggests thatbeyond the visual cortex, the default mode network contributes most to decodingstimuli, in line with the proposed role of this network in sense-making andsemantic processing. Additionally, we implemented zero-shot imaginationdecoding on an extra validation dataset, achieving a p-value of 0.0206 formapping the reconstructed images and ground-truth text stimuli, whichsubstantiates the model's capability to capture semantic meanings acrossvarious scenarios.</description><author>Yanchen Wang, Adam Turnbull, Tiange Xiang, Yunlong Xu, Sa Zhou, Adnan Masoud, Shekoofeh Azizi, Feng Vankee Lin, Ehsan Adeli</author><pubDate>Mon, 11 Nov 2024 16:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07121v1</guid></item><item><title>Tighter Confidence Bounds for Sequential Kernel Regression</title><link>http://arxiv.org/abs/2403.12732v2</link><description>Confidence bounds are an essential tool for rigorously quantifying theuncertainty of predictions. They are a core component in many sequentiallearning and decision-making algorithms, with tighter confidence bounds givingrise to algorithms with better empirical performance and better performanceguarantees. In this work, we use martingale tail inequalities to establish newconfidence bounds for sequential kernel regression. Our confidence bounds canbe computed by solving a conic program, although this bare version quicklybecomes impractical, because the number of variables grows with the samplesize. However, we show that the dual of this conic program allows us toefficiently compute tight confidence bounds. We prove that our new confidencebounds are always tighter than existing ones in this setting. We apply ourconfidence bounds to kernel bandit problems, and we find that when ourconfidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm hasbetter empirical performance, a matching worst-case performance guarantee andcomparable computational cost.</description><author>Hamish Flynn, David Reeb</author><pubDate>Mon, 11 Nov 2024 16:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12732v2</guid></item><item><title>Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products</title><link>http://arxiv.org/abs/2401.10216v2</link><description>Developing equivariant neural networks for the E(3) group plays an importantrole in modeling 3D data across real-world applications. Enforcing thisequivariance primarily involves the tensor products of irreduciblerepresentations (irreps). However, the computational complexity of suchoperations increases significantly as higher-order tensors are used. In thiswork, we propose a systematic approach to substantially accelerate thecomputation of the tensor products of irreps. We mathematically connect thecommonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which areintegrals of products of three spherical harmonics. Through Gaunt coefficients,the tensor product of irreps becomes equivalent to the multiplication betweenspherical functions represented by spherical harmonics. This perspectivefurther allows us to change the basis for the equivariant operations fromspherical harmonics to a 2D Fourier basis. Consequently, the multiplicationbetween spherical functions represented by a 2D Fourier basis can beefficiently computed via the convolution theorem and Fast Fourier Transforms.This transformation reduces the complexity of full tensor products of irrepsfrom $\mathcal{O}(L^6)$ to $\mathcal{O}(L^3)$, where $L$ is the max degree ofirreps. Leveraging this approach, we introduce the Gaunt Tensor Product, whichserves as a new method to construct efficient equivariant operations acrossdifferent model architectures. Our experiments on the Open Catalyst Project and3BPA datasets demonstrate both the increased efficiency and improvedperformance of our approach.</description><author>Shengjie Luo, Tianlang Chen, Aditi S. Krishnapriyan</author><pubDate>Mon, 11 Nov 2024 16:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10216v2</guid></item><item><title>Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis</title><link>http://arxiv.org/abs/2410.23131v3</link><description>In federated learning, it is common to assume that clients are alwaysavailable to participate in training, which may not be feasible with userdevices in practice. Recent works analyze federated learning under morerealistic participation patterns, such as cyclic client availability orarbitrary participation. However, all such works either require strongassumptions (e.g., all clients participate almost surely within a boundedwindow), do not achieve linear speedup and reduced communication rounds, or arenot applicable in the general non-convex setting. In this work, we focus onnonconvex optimization and consider participation patterns in which the chanceof participation over a fixed window of rounds is equal among all clients,which includes cyclic client availability as a special case. Under thissetting, we propose a new algorithm, named Amplified SCAFFOLD, and prove thatit achieves linear speedup, reduced communication, and resilience to dataheterogeneity simultaneously. In particular, for cyclic participation, ouralgorithm is proved to enjoy $\mathcal{O}(\epsilon^{-2})$ communication roundsto find an $\epsilon$-stationary point in the non-convex stochastic setting. Incontrast, the prior work under the same setting requires $\mathcal{O}(\kappa^2\epsilon^{-4})$ communication rounds, where $\kappa$ denotes the dataheterogeneity. Therefore, our algorithm significantly reduces communicationrounds due to better dependency in terms of $\epsilon$ and $\kappa$. Ouranalysis relies on a fine-grained treatment of the nested dependence betweenclient participation and errors in the control variates, which results intighter guarantees than previous work. We also provide experimental resultswith (1) synthetic data and (2) real-world data with a large number of clients$(N = 250)$, demonstrating the effectiveness of our algorithm under periodicclient participation.</description><author>Michael Crawshaw, Mingrui Liu</author><pubDate>Mon, 11 Nov 2024 16:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23131v3</guid></item><item><title>Efficient Adaptive Optimization via Subset-Norm and Subspace-Momentum: Fast, Memory-Reduced Training with Convergence Guarantees</title><link>http://arxiv.org/abs/2411.07120v1</link><description>We introduce two complementary techniques for efficient adaptive optimizationthat reduce memory requirements while accelerating training of large-scaleneural networks. The first technique, Subset-Norm adaptive step size,generalizes AdaGrad-Norm and AdaGrad(-Coordinate) by reducing the second momentterm's memory footprint from $O(d)$ to $O(\sqrt{d})$ through step-size sharing,where $d$ is the model size. For non-convex smooth objectives undercoordinate-wise sub-gaussian gradient noise, we prove a noise-adaptedhigh-probability convergence guarantee showing improved dimensional dependenceover existing methods. Our second technique, Subspace-Momentum, reduces themomentum state's memory footprint by operating in a low-dimensional subspacewhile applying standard SGD in the orthogonal complement. We establishhigh-probability convergence rates under similar relaxed assumptions. Empiricalevaluation on LLaMA models from 60M to 1B parameters demonstrates theeffectiveness of our methods, where combining subset-norm withsubspace-momentum achieves Adam's validation perplexity in approximately halfthe training tokens (6.8B vs 13.1B) while using only 20% of the Adam'soptimizer-states memory footprint and requiring minimal additionalhyperparameter tuning.</description><author>Thien Hang Nguyen, Huy Le Nguyen</author><pubDate>Mon, 11 Nov 2024 16:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07120v1</guid></item><item><title>Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering</title><link>http://arxiv.org/abs/2411.02926v2</link><description>Combating money laundering has become increasingly complex with the rise ofcybercrime and digitalization of financial transactions. Graph-based machinelearning techniques have emerged as promising tools for Anti-Money Laundering(AML) detection, capturing intricate relationships within money launderingnetworks. However, the effectiveness of AML solutions is hindered by data siloswithin financial institutions, limiting collaboration and overall efficacy.This research presents a novel privacy-preserving approach for collaborativeAML machine learning, facilitating secure data sharing across institutions andborders while preserving privacy and regulatory compliance. Leveraging FullyHomomorphic Encryption (FHE), computations are directly performed on encrypteddata, ensuring the confidentiality of financial data. Notably, FHE over theTorus (TFHE) was integrated with graph-based machine learning using ZamaConcrete ML. The research contributes two key privacy-preserving pipelines.First, the development of a privacy-preserving Graph Neural Network (GNN)pipeline was explored. Optimization techniques like quantization and pruningwere used to render the GNN FHE-compatible. Second, a privacy-preservinggraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) wassuccessfully developed. Experiments demonstrated strong predictive performance,with the XGBoost model consistently achieving over 99% accuracy, F1-score,precision, and recall on the balanced AML dataset in both unencrypted andFHE-encrypted inference settings. On the imbalanced dataset, the incorporationof graph-based features improved the F1-score by 8%. The research highlightsthe need to balance the trade-off between privacy and computational efficiency.</description><author>Fabrianne Effendi, Anupam Chattopadhyay</author><pubDate>Mon, 11 Nov 2024 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02926v2</guid></item><item><title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title><link>http://arxiv.org/abs/2411.07118v1</link><description>Transformer models have demonstrated remarkable success in many domains suchas natural language processing (NLP) and computer vision. With the growinginterest in transformer-based architectures, they are now utilized for gesturerecognition. So, we also explore and devise a novel ConvMixFormer architecturefor dynamic hand gestures. The transformers use quadratic scaling of theattention features with the sequential data, due to which these models arecomputationally complex and heavy. We have considered this drawback of thetransformer and designed a resource-efficient model that replaces theself-attention in the transformer with the simple convolutional layer-basedtoken mixer. The computational cost and the parameters used for theconvolution-based mixer are comparatively less than the quadraticself-attention. Convolution-mixer helps the model capture the local spatialfeatures that self-attention struggles to capture due to their sequentialprocessing nature. Further, an efficient gate mechanism is employed instead ofa conventional feed-forward network in the transformer to help the modelcontrol the flow of features within different stages of the proposed model.This design uses fewer learnable parameters which is nearly half the vanillatransformer that helps in fast and efficient training. The proposed method isevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model hasachieved state-of-the-art results on single and multimodal inputs. We have alsoshown the parameter efficiency of the proposed ConvMixFormer model compared toother methods. The source code is available athttps://github.com/mallikagarg/ConvMixFormer.</description><author>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</author><pubDate>Mon, 11 Nov 2024 16:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07118v1</guid></item><item><title>Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency</title><link>http://arxiv.org/abs/2410.08129v2</link><description>3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, bothfor inverse rendering as well as real-time exploration of scenes. In theseapplications, coherence across camera frames and multiple views is crucial, beit for robust convergence of a scene reconstruction or for artifact-freefly-throughs. Recent work started mitigating artifacts that break multi-viewcoherence, including popping artifacts due to inconsistent transparency sortingand perspective-correct outlines of (2D) splats. At the same time, real-timerequirements forced such implementations to accept compromises in howtransparency of large assemblies of 3D Gaussians is resolved, in turn breakingcoherence in other ways. In our work, we aim at achieving maximum coherence, byrendering fully perspective-correct 3D Gaussians while using a high-qualityapproximation of accurate blending, hybrid transparency, on a per-pixel level,in order to retain real-time frame rates. Our fast and perspectively accurateapproach for evaluation of 3D Gaussians does not require matrix inversions,thereby ensuring numerical stability and eliminating the need for specialhandling of degenerate splats, and the hybrid transparency formulation forblending maintains similar quality as fully resolved per-pixel transparenciesat a fraction of the rendering costs. We further show that each of these twocomponents can be independently integrated into Gaussian splatting systems. Incombination, they achieve up to 2$\times$ higher frame rates, 2$\times$ fasteroptimization, and equal or better image quality with fewer rendering artifactscompared to traditional 3DGS on common benchmarks.</description><author>Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor</author><pubDate>Mon, 11 Nov 2024 16:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08129v2</guid></item><item><title>TinyML Security: Exploring Vulnerabilities in Resource-Constrained Machine Learning Systems</title><link>http://arxiv.org/abs/2411.07114v1</link><description>Tiny Machine Learning (TinyML) systems, which enable machine learninginference on highly resource-constrained devices, are transforming edgecomputing but encounter unique security challenges. These devices, restrictedby RAM and CPU capabilities two to three orders of magnitude smaller thanconventional systems, make traditional software and hardware security solutionsimpractical. The physical accessibility of these devices exacerbates theirsusceptibility to side-channel attacks and information leakage. Additionally,TinyML models pose security risks, with weights potentially encoding sensitivedata and query interfaces that can be exploited. This paper offers the firstthorough survey of TinyML security threats. We present a device taxonomy thatdifferentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilitiesunique to TinyML. We list various attack vectors, assess their threat levelsusing the Common Vulnerability Scoring System, and evaluate both existing andpossible defenses. Our analysis identifies where traditional security measuresare adequate and where solutions tailored to TinyML are essential. Our resultsunderscore the pressing need for specialized security solutions in TinyML toensure robust and secure edge computing applications. We aim to inform theresearch community and inspire innovative approaches to protecting this rapidlyevolving and critical field.</description><author>Jacob Huckelberry, Yuke Zhang, Allison Sansone, James Mickens, Peter A. Beerel, Vijay Janapa Reddi</author><pubDate>Mon, 11 Nov 2024 16:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07114v1</guid></item><item><title>AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics</title><link>http://arxiv.org/abs/2409.17852v3</link><description>All-atom molecular simulations offer detailed insights into macromolecularphenomena, but their substantial computational cost hinders the exploration ofcomplex biological processes. We introduce Advanced Machine-learning AtomicRepresentation Omni-force-field (AMARO), a new neural network potential (NNP)that combines an O(3)-equivariant message-passing neural network architecture,TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMAROdemonstrates the feasibility of training coarser NNP, without prior energyterms, to run stable protein dynamics with scalability and generalizationcapabilities.</description><author>Antonio Mirarchi, Raul P. Pelaez, Guillem Simeon, Gianni De Fabritiis</author><pubDate>Mon, 11 Nov 2024 16:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17852v3</guid></item><item><title>Learning Dynamics from Multicellular Graphs with Deep Neural Networks</title><link>http://arxiv.org/abs/2401.12196v3</link><description>Multicellular self-assembly into functional structures is a dynamic processthat is critical in the development and diseases, including embryo development,organ formation, tumor invasion, and others. Being able to infer collectivecell migratory dynamics from their static configuration is valuable for bothunderstanding and predicting these complex processes. However, theidentification of structural features that can indicate multicellular motionhas been difficult, and existing metrics largely rely on physical instincts.Here we show that using a graph neural network (GNN), the motion ofmulticellular collectives can be inferred from a static snapshot of cellpositions, in both experimental and synthetic datasets.</description><author>Haiqian Yang, Florian Meyer, Shaoxun Huang, Liu Yang, Cristiana Lungu, Monilola A. Olayioye, Markus J. Buehler, Ming Guo</author><pubDate>Mon, 11 Nov 2024 16:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12196v3</guid></item><item><title>Building a Taiwanese Mandarin Spoken Language Model: A First Attempt</title><link>http://arxiv.org/abs/2411.07111v1</link><description>This technical report presents our initial attempt to build a spoken largelanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enablereal-time, speech-to-speech interaction in multi-turn conversations. Ourend-to-end model incorporates a decoder-only transformer architecture and aimsto achieve seamless interaction while preserving the conversational flow,including full-duplex capabilities allowing simultaneous speaking andlistening. The paper also details the training process, including datapreparation with synthesized dialogues and adjustments for real-timeinteraction. We also developed a platform to evaluate conversational fluencyand response coherence in multi-turn dialogues. We hope the release of thereport can contribute to the future development of spoken LLMs in TaiwaneseMandarin.</description><author>Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee</author><pubDate>Mon, 11 Nov 2024 16:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07111v1</guid></item><item><title>Stochastic Newton Proximal Extragradient Method</title><link>http://arxiv.org/abs/2406.01478v2</link><description>Stochastic second-order methods achieve fast local convergence in stronglyconvex optimization by using noisy Hessian estimates to precondition thegradient. However, these methods typically reach superlinear convergence onlywhen the stochastic Hessian noise diminishes, increasing per-iteration costsover time. Recent work in [arXiv:2204.09266] addressed this with a Hessianaveraging scheme that achieves superlinear convergence without higherper-iteration costs. Nonetheless, the method has slow global convergence,requiring up to $\tilde{O}(\kappa^2)$ iterations to reach the superlinear rateof $\tilde{O}((1/t)^{t/2})$, where $\kappa$ is the problem's condition number.In this paper, we propose a novel stochastic Newton proximal extragradientmethod that improves these bounds, achieving a faster global linear rate andreaching the same fast superlinear rate in $\tilde{O}(\kappa)$ iterations. Weaccomplish this by extending the Hybrid Proximal Extragradient (HPE) framework,achieving fast global and local convergence rates for strongly convex functionswith access to a noisy Hessian oracle.</description><author>Ruichen Jiang, Micha≈Ç Derezi≈Ñski, Aryan Mokhtari</author><pubDate>Mon, 11 Nov 2024 16:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01478v2</guid></item><item><title>Training Neural Networks as Recognizers of Formal Languages</title><link>http://arxiv.org/abs/2411.07107v1</link><description>Characterizing the computational power of neural network architectures interms of formal language theory remains a crucial line of research, as itdescribes lower and upper bounds on the reasoning capabilities of modern AI.However, when empirically testing these bounds, existing work often leaves adiscrepancy between experiments and the formal claims they are meant tosupport. The problem is that formal language theory pertains specifically torecognizers: machines that receive a string as input and classify whether itbelongs to a language. On the other hand, it is common to instead use proxytasks that are similar in only an informal sense, such as language modeling orsequence-to-sequence transduction. We correct this mismatch by training andevaluating neural networks directly as binary classifiers of strings, using ageneral method that can be applied to a wide variety of languages. As part ofthis, we extend an algorithm recently proposed by Sn{\ae}bjarnarson et al.(2024) to do length-controlled sampling of strings from regular languages, withmuch better asymptotic time complexity than previous methods. We provideresults on a variety of languages across the Chomsky hierarchy for three neuralarchitectures: a simple RNN, an LSTM, and a causally-masked transformer. Wefind that the RNN and LSTM often outperform the transformer, and that auxiliarytraining objectives such as language modeling can help, although no singleobjective uniformly improves performance across languages and architectures.Our contributions will facilitate theoretically sound empirical testing oflanguage recognition claims in future work. We have released our datasets as abenchmark called FLaRe (Formal Language Recognition), along with our code.</description><author>Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell</author><pubDate>Mon, 11 Nov 2024 16:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07107v1</guid></item><item><title>Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to Address Shortcut Shifts in Natural Language Understanding</title><link>http://arxiv.org/abs/2406.12060v2</link><description>Recent models for natural language understanding are inclined to exploitsimple patterns in datasets, commonly known as shortcuts. These shortcuts hingeon spurious correlations between labels and latent features existing in thetraining data. At inference time, shortcut-dependent models are likely togenerate erroneous predictions under distribution shifts, particularly whensome latent features are no longer correlated with the labels. To avoid this,previous studies have trained models to eliminate the reliance on shortcuts. Inthis study, we explore a different direction: pessimistically aggregating thepredictions of a mixture-of-experts, assuming each expert captures relativelydifferent latent features. The experimental results demonstrate that ourpost-hoc control over the experts significantly enhances the model's robustnessto the distribution shift in shortcuts. Besides, we show that our approach hassome practical advantages. We also analyze our model and provide results tosupport the assumption.</description><author>Ukyo Honda, Tatsushi Oka, Peinan Zhang, Masato Mita</author><pubDate>Mon, 11 Nov 2024 16:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12060v2</guid></item><item><title>HandCraft: Anatomically Correct Restoration of Malformed Hands in Diffusion Generated Images</title><link>http://arxiv.org/abs/2411.04332v2</link><description>Generative text-to-image models, such as Stable Diffusion, have demonstrateda remarkable ability to generate diverse, high-quality images. However, theyare surprisingly inept when it comes to rendering human hands, which are oftenanatomically incorrect or reside in the "uncanny valley". In this paper, wepropose a method HandCraft for restoring such malformed hands. This is achievedby automatically constructing masks and depth images for hands as conditioningsignals using a parametric model, allowing a diffusion-based image editor tofix the hand's anatomy and adjust its pose while seamlessly integrating thechanges into the original image, preserving pose, color, and style. Ourplug-and-play hand restoration solution is compatible with existing pretraineddiffusion models, and the restoration process facilitates adoption by eschewingany fine-tuning or training requirements for the diffusion models. We alsocontribute MalHand datasets that contain generated images with a wide varietyof malformed hands in several styles for hand detector training and handrestoration benchmarking, and demonstrate through qualitative and quantitativeevaluation that HandCraft not only restores anatomical correctness but alsomaintains the integrity of the overall image.</description><author>Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell</author><pubDate>Mon, 11 Nov 2024 16:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04332v2</guid></item><item><title>Learning Multi-Agent Collaborative Manipulation for Long-Horizon Quadrupedal Pushing</title><link>http://arxiv.org/abs/2411.07104v1</link><description>Recently, quadrupedal locomotion has achieved significant success, but theirmanipulation capabilities, particularly in handling large objects, remainlimited, restricting their usefulness in demanding real-world applications suchas search and rescue, construction, industrial automation, and roomorganization. This paper tackles the task of obstacle-aware, long-horizonpushing by multiple quadrupedal robots. We propose a hierarchical multi-agentreinforcement learning framework with three levels of control. The high-levelcontroller integrates an RRT planner and a centralized adaptive policy togenerate subgoals, while the mid-level controller uses a decentralizedgoal-conditioned policy to guide the robots toward these sub-goals. Apre-trained low-level locomotion policy executes the movement commands. Weevaluate our method against several baselines in simulation, demonstratingsignificant improvements over baseline approaches, with 36.0% higher successrates and 24.5% reduction in completion time than the best baseline. Ourframework successfully enables long-horizon, obstacle-aware manipulation taskslike Push-Cuboid and Push-T on Go1 robots in the real world.</description><author>Chuye Hong, Yuming Feng, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao</author><pubDate>Mon, 11 Nov 2024 16:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07104v1</guid></item><item><title>Effectively Leveraging Momentum Terms in Stochastic Line Search Frameworks for Fast Optimization of Finite-Sum Problems</title><link>http://arxiv.org/abs/2411.07102v1</link><description>In this work, we address unconstrained finite-sum optimization problems, withparticular focus on instances originating in large scale deep learningscenarios. Our main interest lies in the exploration of the relationshipbetween recent line search approaches for stochastic optimization in theoverparametrized regime and momentum directions. First, we point out thatcombining these two elements with computational benefits is notstraightforward. To this aim, we propose a solution based on mini-batchpersistency. We then introduce an algorithmic framework that exploits a mix ofdata persistency, conjugate-gradient type rules for the definition of themomentum parameter and stochastic line searches. The resulting algorithm isempirically shown to outperform other popular methods from the literature,obtaining state-of-the-art results in both convex and nonconvex large scaletraining problems.</description><author>Matteo Lapucci, Davide Pucci</author><pubDate>Mon, 11 Nov 2024 16:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07102v1</guid></item><item><title>Bounded Rationality Equilibrium Learning in Mean Field Games</title><link>http://arxiv.org/abs/2411.07099v1</link><description>Mean field games (MFGs) tractably model behavior in large agent populations.The literature on learning MFG equilibria typically focuses on finding Nashequilibria (NE), which assume perfectly rational agents and are henceimplausible in many realistic situations. To overcome these limitations, weincorporate bounded rationality into MFGs by leveraging the well-known conceptof quantal response equilibria (QRE). Two novel types of MFG QRE enable themodeling of large agent populations where individuals only noisily estimate thetrue objective. We also introduce a second source of bounded rationality toMFGs by restricting agents' planning horizon. The resulting novel recedinghorizon (RH) MFGs are combined with QRE and existing approaches to modeldifferent aspects of bounded rationality in MFGs. We formally define MFG QREand RH MFGs and compare them to existing equilibrium concepts such asentropy-regularized NE. Subsequently, we design generalized fixed pointiteration and fictitious play algorithms to learn QRE and RH equilibria. Aftera theoretical analysis, we give different examples to evaluate the capabilitiesof our learning algorithms and outline practical differences between theequilibrium concepts.</description><author>Yannick Eich, Christian Fabian, Kai Cui, Heinz Koeppl</author><pubDate>Mon, 11 Nov 2024 16:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07099v1</guid></item><item><title>A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs</title><link>http://arxiv.org/abs/2411.07098v1</link><description>As modern web services increasingly rely on REST APIs, their thorough testinghas become crucial. Furthermore, the advent of REST API specifications such asthe OpenAPI Specification has led to the emergence of many black-box REST APItesting tools. However, these tools often focus on individual test elements inisolation (e.g., APIs, parameters, values), resulting in lower coverage andless effectiveness in detecting faults (i.e., 500 response codes). To addressthese limitations, we present AutoRestTest, the first black-box framework toadopt a dependency-embedded multi-agent approach for REST API testing,integrating Multi-Agent Reinforcement Learning (MARL) with a Semantic PropertyDependency Graph (SPDG) and Large Language Models (LLMs). Our approach treatsREST API testing as a separable problem, where four agents -- API, dependency,parameter, and value -- collaborate to optimize API exploration. LLMs handledomain-specific value restrictions, the SPDG model simplifies the search spacefor dependencies using a similarity score between API operations, and MARLdynamically optimizes the agents' behavior. Evaluated on 12 real-world RESTservices, AutoRestTest outperforms the four leading black-box REST API testingtools, including those assisted by RESTGPT (which augments realistic testinputs using LLMs), in terms of code coverage, operation coverage, and faultdetection. Notably, AutoRestTest is the only tool able to identify an internalserver error in Spotify. Our ablation study underscores the significantcontributions of the agent learning, SPDG, and LLM components.</description><author>Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso</author><pubDate>Mon, 11 Nov 2024 16:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07098v1</guid></item><item><title>Arctique: An artificial histopathological dataset unifying realism and controllability for uncertainty quantification</title><link>http://arxiv.org/abs/2411.07097v1</link><description>Uncertainty Quantification (UQ) is crucial for reliable image segmentation.Yet, while the field sees continual development of novel methods, a lack ofagreed-upon benchmarks limits their systematic comparison and evaluation:Current UQ methods are typically tested either on overly simplistic toydatasets or on complex real-world datasets that do not allow to discern trueuncertainty. To unify both controllability and complexity, we introduceArctique, a procedurally generated dataset modeled after histopathologicalcolon images. We chose histopathological images for two reasons: 1) theircomplexity in terms of intricate object structures and highly variableappearance, which yields challenging segmentation problems, and 2) their broadprevalence for medical diagnosis and respective relevance of high-quality UQ.To generate Arctique, we established a Blender-based framework for 3D scenecreation with intrinsic noise manipulation. Arctique contains 50,000 renderedimages with precise masks as well as noisy label simulations. We show that byindependently controlling the uncertainty in both images and labels, we caneffectively study the performance of several commonly used UQ methods. Hence,Arctique serves as a critical resource for benchmarking and advancing UQtechniques and other methodologies in complex, multi-object environments,bridging the gap between realism and controllability. All code is publiclyavailable, allowing re-creation and controlled manipulations of our shippedimages as well as creation and rendering of new scenes.</description><author>Jannik Franzen, Claudia Winklmayr, Vanessa E. Guarino, Christoph Karg, Xiaoyan Yu, Nora Koreuber, Jan P. Albrecht, Philip Bischoff, Dagmar Kainmueller</author><pubDate>Mon, 11 Nov 2024 16:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07097v1</guid></item><item><title>Extreme Rotation Estimation in the Wild</title><link>http://arxiv.org/abs/2411.07096v1</link><description>We present a technique and benchmark dataset for estimating the relative 3Dorientation between a pair of Internet images captured in an extreme setting,where the images have limited or non-overlapping field of views. Prior worktargeting extreme rotation estimation assume constrained 3D environments andemulate perspective images by cropping regions from panoramic views. However,real images captured in the wild are highly diverse, exhibiting variation inboth appearance and camera intrinsics. In this work, we propose aTransformer-based method for estimating relative rotations in extremereal-world settings, and contribute the ExtremeLandmarkPairs dataset, assembledfrom scene-level Internet photo collections. Our evaluation demonstrates thatour approach succeeds in estimating the relative rotations in a wide variety ofextremeview Internet image pairs, outperforming various baselines, includingdedicated rotation estimation techniques and contemporary 3D reconstructionmethods.</description><author>Hana Bezalel, Dotan Ankri, Ruojin Cai, Hadar Averbuch-Elor</author><pubDate>Mon, 11 Nov 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07096v1</guid></item><item><title>Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization</title><link>http://arxiv.org/abs/2410.21764v2</link><description>The goal of multi-objective optimization (MOO) is to learn under multiple,potentially conflicting, objectives. One widely used technique to tackle MOO isthrough linear scalarization, where one fixed preference vector is used tocombine the objectives into a single scalar value for optimization. However,recent work (Hu et al., 2024) has shown linear scalarization often fails tocapture the non-convex regions of the Pareto Front, failing to recover thecomplete set of Pareto optimal solutions. In light of the above limitations,this paper focuses on Tchebycheff scalarization that optimizes for theworst-case objective. In particular, we propose an online mirror descentalgorithm for Tchebycheff scalarization, which we call OMD-TCH. We show thatOMD-TCH enjoys a convergence rate of $O(\sqrt{\log m/T})$ where $m$ is thenumber of objectives and $T$ is the number of iteration rounds. We also proposea novel adaptive online-to-batch conversion scheme that significantly improvesthe practical performance of OMD-TCH while maintaining the same convergenceguarantees. We demonstrate the effectiveness of OMD-TCH and the adaptiveconversion scheme on both synthetic problems and federated learning tasks underfairness constraints, showing state-of-the-art performance.</description><author>Meitong Liu, Xiaoyuan Zhang, Chulin Xie, Kate Donahue, Han Zhao</author><pubDate>Mon, 11 Nov 2024 16:17:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21764v2</guid></item><item><title>Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs</title><link>http://arxiv.org/abs/2310.02195v3</link><description>Automated guided vehicles (AGVs) are widely used in various industries, andscheduling and routing them in a conflict-free manner is crucial to theirefficient operation. We propose a loop-based algorithm that solves the online,conflict-free scheduling and routing problem for AGVs with any capacity andordered jobs in loop-based graphs. The proposed algorithm is compared againstan exact method, a greedy heuristic and a metaheuristic. We experimentallyshow, using theoretical and real instances on a model representing a realmanufacturing plant, that this algorithm either outperforms the otheralgorithms or gets an equally good solution in less computing time.</description><author>Louis Stubbe, Jens Goemaere, Jan Goedgebeur</author><pubDate>Mon, 11 Nov 2024 16:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02195v3</guid></item><item><title>Differentially-Private Collaborative Online Personalized Mean Estimation</title><link>http://arxiv.org/abs/2411.07094v1</link><description>We consider the problem of collaborative personalized mean estimation under aprivacy constraint in an environment of several agents continuously receivingdata according to arbitrary unknown agent-specific distributions. Inparticular, we provide a method based on hypothesis testing coupled withdifferential privacy and data variance estimation. Two privacy mechanisms andtwo data variance estimation schemes are proposed, and we provide a theoreticalconvergence analysis of the proposed algorithm for any bounded unknowndistributions on the agents' data, showing that collaboration provides fasterconvergence than a fully local approach where agents do not share data.Moreover, we provide analytical performance curves for the case with an oracleclass estimator, i.e., the class structure of the agents, where agentsreceiving data from distributions with the same mean are considered to be inthe same class, is known. The theoretical faster-than-local convergenceguarantee is backed up by extensive numerical results showing that for aconsidered scenario the proposed approach indeed converges much faster than afully local approach, and performs comparably to ideal performance where alldata is public. This illustrates the benefit of private collaboration in anonline setting.</description><author>Yauhen Yakimenka, Chung-Wei Weng, Hsuan-Yin Lin, Eirik Rosnes, J√∂rg Kliewer</author><pubDate>Mon, 11 Nov 2024 16:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07094v1</guid></item><item><title>Towards Characterizing Cyber Networks with Large Language Models</title><link>http://arxiv.org/abs/2411.07089v1</link><description>Threat hunting analyzes large, noisy, high-dimensional data to find sparseadversarial behavior. We believe adversarial activities, however they aredisguised, are extremely difficult to completely obscure in high dimensionalspace. In this paper, we employ these latent features of cyber data to findanomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEMwas trained on Zeek network traffic logs from both a real-world productionnetwork and an from Internet of Things (IoT) cybersecurity testbed. The modelis deliberately overtrained on a sliding window of data to characterize eachwindow closely. We use the Adjusted Rand Index (ARI) to comparing the k-meansclustering of CLEM output to expert labeling of the embeddings. Our approachdemonstrates that there is promise in using natural language modeling tounderstand cyber data.</description><author>Alaric Hartsock, Luiz Manella Pereira, Glenn Fink</author><pubDate>Mon, 11 Nov 2024 16:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07089v1</guid></item></channel></rss>