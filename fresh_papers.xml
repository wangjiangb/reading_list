<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 31 May 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Concise Answers to Complex Questions: Summarization of Long-form Answers</title><link>http://arxiv.org/abs/2305.19271v1</link><description>Long-form question answering systems provide rich information by presentingparagraph-level answers, often containing optional background or auxiliaryinformation. While such comprehensive answers are helpful, not all informationis required to answer the question (e.g. users with domain knowledge do notneed an explanation of background). Can we provide a concise version of theanswer by summarizing it, while still addressing the question? We conduct auser study on summarized answers generated from state-of-the-art models and ournewly proposed extract-and-decontextualize approach. We find a large proportionof long-form answers (over 90%) in the ELI5 domain can be adequately summarizedby at least one system, while complex and implicit answers are challenging tocompress. We observe that decontextualization improves the quality of theextractive summary, exemplifying its potential in the summarization task. Topromote future work, we provide an extractive summarization dataset covering 1Klong-form answers and our user study annotations. Together, we present thefirst study on summarizing long-form answers, taking a step forward for QAagents that can provide answers at multiple granularities.</description><author>Abhilash Potluri, Fangyuan Xu, Eunsol Choi</author><pubDate>Tue, 30 May 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19271v1</guid></item><item><title>Learning without Forgetting for Vision-Language Models</title><link>http://arxiv.org/abs/2305.19270v1</link><description>Class-Incremental Learning (CIL) or continual learning is a desiredcapability in the real world, which requires a learning system to adapt to newtasks without forgetting former ones. While traditional CIL methods focus onvisual information to grasp core features, recent advances in Vision-LanguageModels (VLM) have shown promising capabilities in learning generalizablerepresentations with the aid of textual information. However, when continuallytrained with new classes, VLMs often suffer from catastrophic forgetting offormer knowledge. Applying VLMs to CIL poses two major challenges: 1) how toadapt the model without forgetting; and 2) how to make full use of themulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) thatenables VLMs to learn without forgetting. To handle the first challenge, wepropose training task-specific projections based on the frozen image/textencoders. When facing new tasks, new projections are expanded and formerprojections are fixed, alleviating the forgetting of old concepts. For thesecond challenge, we propose the fusion module to better utilize thecross-modality information. By jointly adjusting visual and textual features,the model can capture semantic information with stronger representationability. Extensive experiments on nine benchmark datasets validate PROOFachieves state-of-the-art performance.</description><author>Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu</author><pubDate>Tue, 30 May 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19270v1</guid></item><item><title>Make-A-Voice: Unified Voice Synthesis With Discrete Representation</title><link>http://arxiv.org/abs/2305.19269v1</link><description>Various applications of voice synthesis have been developed independentlydespite the fact that they generate "voice" as output in common. In addition,the majority of voice synthesis models currently rely on annotated audio data,but it is crucial to scale them to self-supervised datasets in order toeffectively capture the wide range of acoustic variations present in humanvoice, including speaker identity, emotion, and prosody. In this work, wepropose Make-A-Voice, a unified framework for synthesizing and manipulatingvoice signals from discrete representations. Make-A-Voice leverages a"coarse-to-fine" approach to model the human voice, which involves threestages: 1) semantic stage: model high-level transformation between linguisticcontent and self-supervised semantic tokens, 2) acoustic stage: introducevarying control signals as acoustic conditions for semantic-to-acousticmodeling, and 3) generation stage: synthesize high-fidelity waveforms fromacoustic tokens. Make-A-Voice offers notable benefits as a unified voicesynthesis framework: 1) Data scalability: the major backbone (i.e., acousticand generation stage) does not require any annotations, and thus the trainingdata could be scaled up. 2) Controllability and conditioning flexibility: weinvestigate different conditioning mechanisms and effectively handle threevoice synthesis applications, including text-to-speech (TTS), voice conversion(VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voicerepresentations with prompt guidance. Experimental results demonstrate thatMake-A-Voice exhibits superior audio quality and style similarity compared withcompetitive baseline models. Audio samples are available athttps://Make-A-Voice.github.io</description><author>Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue Jiang, Chao Weng, Zhou Zhao, Dong Yu</author><pubDate>Tue, 30 May 2023 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19269v1</guid></item><item><title>Intriguing Properties of Quantization at Scale</title><link>http://arxiv.org/abs/2305.19268v1</link><description>Emergent properties have been widely adopted as a term to describe behaviornot present in smaller models but observed in larger models. Recent worksuggests that the trade-off incurred by quantization is also an emergentproperty, with sharp drops in performance in models over 6B parameters. In thiswork, we ask "are quantization cliffs in performance solely a factor of scale?"Against a backdrop of increased research focus on why certain emergentproperties surface at scale, this work provides a useful counter-example. Weposit that it is possible to optimize for a quantization friendly trainingrecipe that suppresses large activation magnitude outliers. Here, we find thatoutlier dimensions are not an inherent product of scale, but rather sensitiveto the optimization conditions present during pre-training. This both opens updirections for more efficient quantization, and poses the question of whetherother emergent properties are inherent or can be altered and conditioned byoptimization and architecture design choices. We successfully quantize modelsranging in size from 410M to 52B with minimal degradation in performance.</description><author>Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Stephen Gou, Phil Blunsom, Ahmet Üstün, Sara Hooker</author><pubDate>Tue, 30 May 2023 18:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19268v1</guid></item><item><title>GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children</title><link>http://arxiv.org/abs/2305.16809v2</link><description>When caregivers ask open--ended questions to motivate dialogue with children,it facilitates the child's reading comprehension skills.Although there is scopefor use of technological tools, referred here as "intelligent tutoringsystems", to scaffold this process, it is currently unclear whether existingintelligent systems that generate human--language like questions is beneficial.Additionally, training data used in the development of these automated questiongeneration systems is typically sourced without attention to demographics, butpeople with different cultural backgrounds may ask different questions. As apart of a broader project to design an intelligent reading support app forLatinx children, we crowdsourced questions from Latinx caregivers andnoncaregivers as well as caregivers and noncaregivers from other demographics.We examine variations in question--asking within this dataset mediated byindividual, cultural, and contextual factors. We then design a system thatautomatically extracts templates from this data to generate open--endedquestions that are representative of those asked by Latinx caregivers.</description><author>Arun Balajiee Lekshmi Narayanan, Ligia E. Gomez, Martha Michelle Soto Fernandez, Tri Nguyen, Chris Blais, M. Adelaida Restrepo, Art Glenberg</author><pubDate>Tue, 30 May 2023 18:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16809v2</guid></item><item><title>Parallelized Acquisition for Active Learning using Monte Carlo Sampling</title><link>http://arxiv.org/abs/2305.19267v1</link><description>Bayesian inference remains one of the most important tool-kits for anyscientist, but increasingly expensive likelihood functions are required forever-more complex experiments, raising the cost of generating a Monte Carlosample of the posterior. Recent attention has been directed towards the use ofemulators of the posterior based on Gaussian Process (GP) regression combinedwith active sampling to achieve comparable precision with far fewer costlylikelihood evaluations. Key to this approach is the batched acquisition ofproposals, so that the true posterior can be evaluated in parallel. This isusually achieved via sequential maximization of the highly multimodalacquisition function. Unfortunately, this approach parallelizes poorly and isprone to getting stuck in local maxima. Our approach addresses this issue bygenerating nearly-optimal batches of candidates using an almost-embarrassinglyparallel Nested Sampler on the mean prediction of the GP. The resultingnearly-sorted Monte Carlo sample is used to generate a batch of candidatesranked according to their sequentially conditioned acquisition function valuesat little cost. The final sample can also be used for inferring marginalquantities. Our proposed implementation (NORA) demonstrates comparable accuracyto sequential conditioned acquisition optimization and efficientparallelization in various synthetic and cosmological inference problems.</description><author>Jesús Torrado, Nils Schöneberg, Jonas El Gammal</author><pubDate>Tue, 30 May 2023 18:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19267v1</guid></item><item><title>Probabilistic Computation with Emerging Covariance: towards efficient uncertainty quantification</title><link>http://arxiv.org/abs/2305.19265v1</link><description>Building robust, interpretable, and secure artificial intelligence systemrequires some degree of quantifying and representing uncertainty via aprobabilistic perspective, as it allows to mimic human cognitive abilities.However, probabilistic computation presents significant challenges due to itsinherent complexity. In this paper, we develop an efficient and interpretableprobabilistic computation framework by truncating the probabilisticrepresentation up to its first two moments, i.e., mean and covariance. Weinstantiate the framework by training a deterministic surrogate of a stochasticnetwork that learns the complex probabilistic representation via combinationsof simple activations, encapsulating the non-linearities coupling of the meanand covariance. We show that when the mean is supervised for optimizing thetask objective, the unsupervised covariance spontaneously emerging from thenon-linear coupling with the mean faithfully captures the uncertaintyassociated with model predictions. Our research highlights the inherentcomputability and simplicity of probabilistic computation, enabling its widerapplication in large-scale settings.</description><author>Hengyuan Ma, Yang Qi, Li Zhang, Wenlian Lu, Jianfeng Feng</author><pubDate>Tue, 30 May 2023 18:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19265v1</guid></item><item><title>Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning</title><link>http://arxiv.org/abs/2305.19264v1</link><description>Efficient finetuning of pretrained language transformers is becomingincreasingly prevalent for solving natural language processing tasks. Whileeffective, it can still require a large number of tunable parameters. This canbe a drawback for low-resource applications and training withdifferential-privacy constraints, where excessive noise may be introducedduring finetuning. To this end, we propose a novel language transformerfinetuning strategy that introduces task-specific parameters in multipletransformer layers. These parameters are derived from fixed random projectionsof a single trainable vector, enabling finetuning with significantly fewerparameters while maintaining performance. We achieve within 5% of fullfinetuning performance on GLUE tasks with as few as 4,100 parameters per task,outperforming other parameter-efficient finetuning approaches that use asimilar number of per-task parameters. Besides, the random projections can beprecomputed at inference, avoiding additional computational latency. All thesemake our method particularly appealing for low-resource applications. Finally,our method achieves the best or comparable utility compared to several recentfinetuning methods when training with the same privacy constraints,underscoring its effectiveness and potential real-world impact.</description><author>Umang Gupta, Aram Galstyan, Greg Ver Steeg</author><pubDate>Tue, 30 May 2023 18:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19264v1</guid></item><item><title>Scaling Data-Constrained Language Models</title><link>http://arxiv.org/abs/2305.16264v2</link><description>The current trend of scaling language models involves increasing bothparameter count and training dataset size. Extrapolating this trend suggeststhat training dataset size may soon be limited by the amount of text dataavailable on the internet. Motivated by this limit, we investigate scalinglanguage models in data-constrained regimes. Specifically, we run a large setof experiments varying the extent of data repetition and compute budget,ranging up to 900 billion training tokens and 9 billion parameter models. Wefind that with constrained data for a fixed compute budget, training with up to4 epochs of repeated data yields negligible changes to loss compared to havingunique data. However, with more repetition, the value of adding computeeventually decays to zero. We propose and empirically validate a scaling lawfor compute optimality that accounts for the decreasing value of repeatedtokens and excess parameters. Finally, we experiment with approaches mitigatingdata scarcity, including augmenting the training dataset with code data orremoving commonly used filters. Models and datasets from our 400 training runsare freely available at https://github.com/huggingface/datablations.</description><author>Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, Colin Raffel</author><pubDate>Tue, 30 May 2023 18:51:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16264v2</guid></item><item><title>Semantically-informed Hierarchical Event Modeling</title><link>http://arxiv.org/abs/2212.10547v2</link><description>Prior work has shown that coupling sequential latent variable models withsemantic ontological knowledge can improve the representational capabilities ofevent modeling approaches. In this work, we present a novel, doublyhierarchical, semi-supervised event modeling framework that provides structuralhierarchy while also accounting for ontological hierarchy. Our approachconsists of multiple layers of structured latent variables, where eachsuccessive layer compresses and abstracts the previous layers. We guide thiscompression through the injection of structured ontological knowledge that isdefined at the type level of events: importantly, our model allows for partialinjection of semantic knowledge and it does not depend on observing instancesat any particular level of the semantic ontology. Across two different datasetsand four different evaluation metrics, we demonstrate that our approach is ableto out-perform the previous state-of-the-art approaches by up to 8.5%,demonstrating the benefits of structured and semantic hierarchical knowledgefor event modeling.</description><author>Shubhashis Roy Dipta, Mehdi Rezaee, Francis Ferraro</author><pubDate>Tue, 30 May 2023 18:47:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10547v2</guid></item><item><title>Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders</title><link>http://arxiv.org/abs/2305.19259v1</link><description>Stochastic Gradient Descent (SGD) algorithms are widely used in optimizingneural networks, with Random Reshuffling (RR) and Single Shuffle (SS) beingpopular choices for cycling through random or single permutations of thetraining data. However, the convergence properties of these algorithms in thenon-convex case are not fully understood. Existing results suggest that, inrealistic training scenarios where the number of epochs is smaller than thetraining set size, RR may perform worse than SGD. In this paper, we analyze a general SGD algorithm that allows for arbitrarydata orderings and show improved convergence rates for non-convex functions.Specifically, our analysis reveals that SGD with random and single shuffling isalways faster or at least as good as classical SGD with replacement, regardlessof the number of iterations. Overall, our study highlights the benefits ofusing SGD with random/single shuffling and provides new insights into itsconvergence properties for non-convex optimization.</description><author>Anastasia Koloskova, Nikita Doikov, Sebastian U. Stich, Martin Jaggi</author><pubDate>Tue, 30 May 2023 18:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19259v1</guid></item><item><title>Ambient Diffusion: Learning Clean Distributions from Corrupted Data</title><link>http://arxiv.org/abs/2305.19256v1</link><description>We present the first diffusion-based framework that can learn an unknowndistribution using only highly-corrupted samples. This problem arises inscientific applications where access to uncorrupted samples is impossible orexpensive to acquire. Another benefit of our approach is the ability to traingenerative models that are less likely to memorize individual training samplessince they never observe clean training data. Our main idea is to introduceadditional measurement distortion during the diffusion process and require themodel to predict the original corrupted image from the further corrupted image.We prove that our method leads to models that learn the conditional expectationof the full uncorrupted image given this additional measurement corruption.This holds for any corruption process that satisfies some technical conditions(and in particular includes inpainting and compressed sensing). We train modelson standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learnthe distribution even when all the training samples have $90\%$ of their pixelsmissing. We also show that we can finetune foundation models on small corrupteddatasets (e.g. MRI scans with block corruptions) and learn the cleandistribution without memorizing the training set.</description><author>Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros G. Dimakis, Adam Klivans</author><pubDate>Tue, 30 May 2023 18:43:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19256v1</guid></item><item><title>One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</title><link>http://arxiv.org/abs/2303.06555v2</link><description>This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fitall distributions relevant to a set of multi-modal data in one model. Our keyinsight is -- learning diffusion models for marginal, conditional, and jointdistributions can be unified as predicting the noise in the perturbed data,where the perturbation levels (i.e. timesteps) can be different for differentmodalities. Inspired by the unified view, UniDiffuser learns all distributionssimultaneously with a minimal modification to the original diffusion model --perturbs data in all modalities instead of a single modality, inputs individualtimesteps in different modalities, and predicts the noise of all modalitiesinstead of a single modality. UniDiffuser is parameterized by a transformer fordiffusion models to handle input types of different modalities. Implemented onlarge-scale paired image-text data, UniDiffuser is able to perform image, text,text-to-image, image-to-text, and image-text pair generation by setting propertimesteps without additional overhead. In particular, UniDiffuser is able toproduce perceptually realistic samples in all tasks and its quantitativeresults (e.g., the FID and CLIP score) are not only superior to existinggeneral-purpose models but also comparable to the bespoken models (e.g., StableDiffusion and DALL-E 2) in representative tasks (e.g., text-to-imagegeneration).</description><author>Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu</author><pubDate>Tue, 30 May 2023 18:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06555v2</guid></item><item><title>A Stutter Seldom Comes Alone -- Cross-Corpus Stuttering Detection as a Multi-label Problem</title><link>http://arxiv.org/abs/2305.19255v1</link><description>Most stuttering detection and classification research has viewed stutteringas a multi-class classification problem or a binary detection task for eachdysfluency type; however, this does not match the nature of stuttering, inwhich one dysfluency seldom comes alone but rather co-occurs with others. Thispaper explores multi-language and cross-corpus end-to-end stuttering detectionas a multi-label problem using a modified wav2vec 2.0 system with anattention-based classification head and multi-task learning. We evaluate themethod using combinations of three datasets containing English and Germanstuttered speech, one containing speech modified by fluency shaping. Theexperimental results and an error analysis show that multi-label stutteringdetection systems trained on cross-corpus and multi-language data achievecompetitive results but performance on samples with multiple labels stays belowover-all detection results.</description><author>Sebastian P. Bayerl, Dominik Wagner, Ilja Baumann, Florian Hönig, Tobias Bocklet, Elmar Nöth, Korbinian Riedhammer</author><pubDate>Tue, 30 May 2023 18:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19255v1</guid></item><item><title>What Can We Learn from Unlearnable Datasets?</title><link>http://arxiv.org/abs/2305.19254v1</link><description>In an era of widespread web scraping, unlearnable dataset methods have thepotential to protect data privacy by preventing deep neural networks fromgeneralizing. But in addition to a number of practical limitations that maketheir use unlikely, we make a number of findings that call into question theirability to safeguard data. First, it is widely believed that neural networkstrained on unlearnable datasets only learn shortcuts, simpler rules that arenot useful for generalization. In contrast, we find that networks actually canlearn useful features that can be reweighed for high test performance,suggesting that image privacy is not preserved. Unlearnable datasets are alsobelieved to induce learning shortcuts through linear separability of addedperturbations. We provide a counterexample, demonstrating that linearseparability of perturbations is not a necessary condition. To emphasize whylinearly separable perturbations should not be relied upon, we propose anorthogonal projection attack which allows learning from unlearnable datasetspublished in ICML 2021 and ICLR 2023. Our proposed attack is significantly lesscomplex than recently proposed techniques.</description><author>Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein</author><pubDate>Tue, 30 May 2023 18:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19254v1</guid></item><item><title>Video Prediction Models as Rewards for Reinforcement Learning</title><link>http://arxiv.org/abs/2305.14343v2</link><description>Specifying reward signals that allow agents to learn complex behaviors is along-standing challenge in reinforcement learning. A promising approach is toextract preferences for behaviors from unlabeled videos, which are widelyavailable on the internet. We present Video Prediction Rewards (VIPER), analgorithm that leverages pretrained video prediction models as action-freereward signals for reinforcement learning. Specifically, we first train anautoregressive transformer on expert videos and then use the video predictionlikelihoods as reward signals for a reinforcement learning agent. VIPER enablesexpert-level control without programmatic task rewards across a wide range ofDMC, Atari, and RLBench tasks. Moreover, generalization of the video predictionmodel allows us to derive rewards for an out-of-distribution environment whereno expert data is available, enabling cross-embodiment generalization fortabletop manipulation. We see our work as starting point for scalable rewardspecification from unlabeled videos that will benefit from the rapid advancesin generative modeling. Source code and datasets are available on the projectwebsite: https://escontrela.me/viper</description><author>Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, Pieter Abbeel</author><pubDate>Tue, 30 May 2023 18:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14343v2</guid></item><item><title>Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models</title><link>http://arxiv.org/abs/2305.19249v1</link><description>Large pre-trained language models (PLMs) have demonstrated strong performanceon natural language understanding (NLU) tasks through fine-tuning. However,fine-tuned models still suffer from overconfident predictions, especially inout-of-domain settings. In this paper, we tackle the problem of calibratingfine-tuned language models. We demonstrate that the PLMs are well-calibrated onthe masked language modeling task with robust predictive confidence underdomain shift, yet the fine-tuned models fail to retain such property due tocatastrophic forgetting, which impacts the calibration on the downstreamclassification task. In light of these observations, we evaluate thecalibration of several methods that preserve pre-trained features and show thatpreserving pre-trained features can improve the calibration of fine-tunedlanguage models. Among these methods, our proposed method that encourages thefine-tuned model to learn generative representations with auxiliary languagemodeling objective achieves competitive accuracy and the lowest expectedcalibration error compared to several strong baselines under both in-domain andout-of-domain settings on three downstream NLU tasks.</description><author>Guande He, Jianfei Chen, Jun Zhu</author><pubDate>Tue, 30 May 2023 18:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19249v1</guid></item><item><title>Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones</title><link>http://arxiv.org/abs/2207.13700v2</link><description>Medication for neurological diseases such as the Parkinson's disease usuallyhappens remotely away from hospitals. Such out-of-lab environments posechallenges in collecting timely and accurate health status data. Individualdifferences in behavioral signals collected from wearable sensors also lead todifficulties in adopting current general machine learning analysis pipelines.To address these challenges, we present a method for predicting the medicationstatus of Parkinson's disease patients using the public mPower dataset, whichcontains 62,182 remote multi-modal test records collected on smartphones from487 patients. The proposed method shows promising results in predicting threemedication statuses objectively: Before Medication (AUC=0.95), After Medication(AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historicalrecords with the attention weights learned through a Transformer model. Ourmethod provides an innovative way for personalized remote health sensing in atimely and objective fashion which could benefit a broad range of similarapplications.</description><author>Weijian Li, Wei Zhu, E. Ray Dorsey, Jiebo Luo</author><pubDate>Tue, 30 May 2023 18:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13700v2</guid></item><item><title>Convolutional Neural Operators for robust and accurate learning of PDEs</title><link>http://arxiv.org/abs/2302.01178v2</link><description>Although very successfully used in conventional machine learning, convolutionbased neural network architectures -- believed to be inconsistent in functionspace -- have been largely ignored in the context of learning solutionoperators of PDEs. Here, we present novel adaptations for convolutional neuralnetworks to demonstrate that they are indeed able to process functions asinputs and outputs. The resulting architecture, termed as convolutional neuraloperators (CNOs), is designed specifically to preserve its underlyingcontinuous nature, even when implemented in a discretized form on a computer.We prove a universality theorem to show that CNOs can approximate operatorsarising in PDEs to desired accuracy. CNOs are tested on a novel suite ofbenchmarks, encompassing a diverse set of PDEs with possibly multi-scalesolutions and are observed to significantly outperform baselines, paving theway for an alternative framework for robust and accurate operator learning.</description><author>Bogdan Raonić, Roberto Molinaro, Tim De Ryck, Tobias Rohner, Francesca Bartolucci, Rima Alaifari, Siddhartha Mishra, Emmanuel de Bézenac</author><pubDate>Tue, 30 May 2023 18:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01178v2</guid></item><item><title>AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation</title><link>http://arxiv.org/abs/2305.19245v1</link><description>This paper presents a method that can quickly adapt dynamic 3D avatars toarbitrary text descriptions of novel styles. Among existing approaches foravatar stylization, direct optimization methods can produce excellent resultsfor arbitrary styles but they are unpleasantly slow. Furthermore, they requireredoing the optimization process from scratch for every new input. Fastapproximation methods using feed-forward networks trained on a large dataset ofstyle images can generate results for new inputs quickly, but tend not togeneralize well to novel styles and fall short in quality. We thereforeinvestigate a new approach, AlteredAvatar, that combines those two approachesusing the meta-learning framework. In the inner loop, the model learns tooptimize to match a single target style well; while in the outer loop, themodel learns to stylize efficiently across many styles. After training,AlteredAvatar learns an initialization that can quickly adapt within a smallnumber of update steps to a novel style, which can be given using texts, areference image, or a combination of both. We show that AlteredAvatar canachieve a good balance between speed, flexibility and quality, whilemaintaining consistency across a wide range of novel views and facialexpressions.</description><author>Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen Lombardi, Lei Xiao</author><pubDate>Tue, 30 May 2023 18:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19245v1</guid></item><item><title>Testing for the Markov Property in Time Series via Deep Conditional Generative Learning</title><link>http://arxiv.org/abs/2305.19244v1</link><description>The Markov property is widely imposed in analysis of time series data.Correspondingly, testing the Markov property, and relatedly, inferring theorder of a Markov model, are of paramount importance. In this article, wepropose a nonparametric test for the Markov property in high-dimensional timeseries via deep conditional generative learning. We also apply the testsequentially to determine the order of the Markov model. We show that the testcontrols the type-I error asymptotically, and has the power approaching one.Our proposal makes novel contributions in several ways. We utilize and extendstate-of-the-art deep generative learning to estimate the conditional densityfunctions, and establish a sharp upper bound on the approximation error of theestimators. We derive a doubly robust test statistic, which employs anonparametric estimation but achieves a parametric convergence rate. We furtheradopt sample splitting and cross-fitting to minimize the conditions required toensure the consistency of the test. We demonstrate the efficacy of the testthrough both simulations and the three data applications.</description><author>Yunzhe Zhou, Chengchun Shi, Lexin Li, Qiwei Yao</author><pubDate>Tue, 30 May 2023 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19244v1</guid></item><item><title>Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks</title><link>http://arxiv.org/abs/2305.19243v1</link><description>It is widely recognized that the generalization ability of neural networkscan be greatly enhanced through carefully designing the training procedure. Thecurrent state-of-the-art training approach involves utilizing stochasticgradient descent (SGD) or Adam optimization algorithms along with a combinationof additional regularization techniques such as weight decay, dropout, or noiseinjection. Optimal generalization can only be achieved by tuning a multitude ofhyperparameters through grid search, which can be time-consuming andnecessitates additional validation datasets. To address this issue, weintroduce a practical PAC-Bayes training framework that is nearly tuning-freeand requires no additional regularization while achieving comparable testingperformance to that of SGD/Adam after a complete grid search and with extraregularizations. Our proposed algorithm demonstrates the remarkable potentialof PAC training to achieve state-of-the-art performance on deep neural networkswith enhanced robustness and interpretability.</description><author>Xitong Zhang, Avrajit Ghosh, Guangliang Liu, Rongrong Wang</author><pubDate>Tue, 30 May 2023 18:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19243v1</guid></item><item><title>NetHack is Hard to Hack</title><link>http://arxiv.org/abs/2305.19240v1</link><description>Neural policy learning methods have achieved remarkable results in variouscontrol problems, ranging from Atari games to simulated locomotion. However,these methods struggle in long-horizon tasks, especially in open-endedenvironments with multi-modal observations, such as the popular dungeon-crawlergame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed thatsymbolic agents outperformed neural approaches by over four times in mediangame score. In this paper, we delve into the reasons behind this performancegap and present an extensive study on neural policy learning for NetHack. Toconduct this study, we analyze the winning symbolic agent, extending itscodebase to track internal strategy selection in order to generate one of thelargest available demonstration datasets. Utilizing this dataset, we examine(i) the advantages of an action hierarchy; (ii) enhancements in neuralarchitecture; and (iii) the integration of reinforcement learning withimitation learning. Our investigations produce a state-of-the-art neural agentthat surpasses previous fully neural policies by 127% in offline settings and25% in online settings on median game score. However, we also demonstrate thatmere scaling is insufficient to bridge the performance gap with the bestsymbolic models or even the top human players.</description><author>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus</author><pubDate>Tue, 30 May 2023 18:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19240v1</guid></item><item><title>Differentiable Clustering with Perturbed Spanning Forests</title><link>http://arxiv.org/abs/2305.16358v2</link><description>We introduce a differentiable clustering method based on minimum-weightspanning forests, a variant of spanning trees with several connectedcomponents. Our method relies on stochastic perturbations of solutions oflinear programs, for smoothing and efficient gradient computations. This allowsus to include clustering in end-to-end trainable pipelines. We show that ourmethod performs well even in difficult settings, such as datasets with highnoise and challenging geometries. We also formulate an ad hoc loss toefficiently learn from partial clustering data using this operation. Wedemonstrate its performance on several real world datasets for supervised andsemi-supervised tasks.</description><author>Lawrence Stewart, Francis S Bach, Felipe Llinares López, Quentin Berthet</author><pubDate>Tue, 30 May 2023 18:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16358v2</guid></item><item><title>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories</title><link>http://arxiv.org/abs/2212.10511v3</link><description>Despite their impressive performance on diverse tasks, large language models(LMs) still struggle with tasks requiring rich world knowledge, implying thelimitations of relying solely on their parameters to encode a wealth of worldknowledge. This paper aims to understand LMs' strengths and limitations inmemorizing factual knowledge, by conducting large-scale knowledge probingexperiments of 10 models and 4 augmentation methods on PopQA, our newopen-domain QA dataset with 14k questions. We find that LMs struggle with lesspopular factual knowledge, and that scaling fails to appreciably improvememorization of factual knowledge in the long tail. We then show thatretrieval-augmented LMs largely outperform orders of magnitude larger LMs,while unassisted LMs remain competitive in questions about high-popularityentities. Based on those findings, we devise a simple, yet effective, methodfor powerful and efficient retrieval-augmented LMs, which retrievesnon-parametric memories only when necessary. Experimental results show thatthis significantly improves models' performance while reducing the inferencecosts.</description><author>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi</author><pubDate>Tue, 30 May 2023 18:27:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10511v3</guid></item><item><title>Grammar Prompting for Domain-Specific Language Generation with Large Language Models</title><link>http://arxiv.org/abs/2305.19234v1</link><description>Large language models (LLMs) can learn to perform a wide range of naturallanguage tasks from just a handful of in-context examples. However, forgenerating strings from highly structured languages (e.g., semantic parsing tocomplex domain-specific languages), it is challenging for the LLM to generalizefrom just a few exemplars. We explore $\textbf{grammar prompting}$ as a simpleapproach for enabling LLMs to use external knowledge and domain-specificconstraints, expressed through a grammar expressed in Backus--Naur Form (BNF),during in-context learning. Grammar prompting augments each demonstrationexample with a specialized grammar that is minimally sufficient for generatingthe particular output example, where the specialized grammar is a subset of thefull DSL grammar. For inference, the LLM first predicts a BNF grammar given atest input, and then generates the output according to the rules of thegrammar. Experiments demonstrate that grammar prompting can enable LLMs toperform competitively on a diverse set of DSL generation tasks, includingsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and evenmolecule generation (SMILES).</description><author>Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, Yoon Kim</author><pubDate>Tue, 30 May 2023 18:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19234v1</guid></item><item><title>On the Stepwise Nature of Self-Supervised Learning</title><link>http://arxiv.org/abs/2303.15438v2</link><description>We present a simple picture of the training process of joint embeddingself-supervised learning methods. We find that these methods learn theirhigh-dimensional embeddings one dimension at a time in a sequence of discrete,well-separated steps. We arrive at this conclusion via the study of alinearized model of Barlow Twins applicable to the case in which the trainednetwork is infinitely wide. We solve the training dynamics of this model fromsmall initialization, finding that the model learns the top eigenmodes of acertain contrastive kernel in a stepwise fashion, and obtain a closed-formexpression for the final learned representations. Remarkably, we then see thesame stepwise learning phenomenon when training deep ResNets using the BarlowTwins, SimCLR, and VICReg losses. Our theory suggests that, just as kernelregression can be thought of as a model of supervised learning, kernel PCA mayserve as a useful model of self-supervised learning.</description><author>James B. Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J. Fetterman, Joshua Albrecht</author><pubDate>Tue, 30 May 2023 18:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15438v2</guid></item><item><title>Controlled Text Generation with Hidden Representation Transformations</title><link>http://arxiv.org/abs/2305.19230v1</link><description>We propose CHRT (Control Hidden Representation Transformation) - a controlledlanguage generation framework that steers large language models to generatetext pertaining to certain attributes (such as toxicity). CHRT gains attributecontrol by modifying the hidden representation of the base model throughlearned transformations. We employ a contrastive-learning framework to learnthese transformations that can be combined to gain multi-attribute control. Theeffectiveness of CHRT is experimentally shown by comparing it with sevenbaselines over three attributes. CHRT outperforms all the baselines in the taskof detoxification, positive sentiment steering, and text simplification whileminimizing the loss in linguistic qualities. Further, our approach has thelowest inference latency of only 0.01 seconds more than the base model, makingit the most suitable for high-performance production environments. Weopen-source our code and release two novel datasets to further propelcontrolled language generation research.</description><author>Vaibhav Kumar, Hana Koorehdavoudi, Masud Moshtaghi, Amita Misra, Ankit Chadha, Emilio Ferrara</author><pubDate>Tue, 30 May 2023 18:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19230v1</guid></item><item><title>FedDisco: Federated Learning with Discrepancy-Aware Collaboration</title><link>http://arxiv.org/abs/2305.19229v1</link><description>This work considers the category distribution heterogeneity in federatedlearning. This issue is due to biased labeling preferences at multiple clientsand is a typical setting of data heterogeneity. To alleviate this issue, mostprevious works consider either regularizing local models or fine-tuning theglobal model, while they ignore the adjustment of aggregation weights andsimply assign weights based on the dataset size. However, based on ourempirical observations and theoretical analysis, we find that the dataset sizeis not optimal and the discrepancy between local and global categorydistributions could be a beneficial and complementary indicator for determiningaggregation weights. We thus propose a novel aggregation method, FederatedLearning with Discrepancy-aware Collaboration (FedDisco), whose aggregationweights not only involve both the dataset size and the discrepancy value, butalso contribute to a tighter theoretical upper bound of the optimization error.FedDisco also promotes privacy-preservation, communication and computationefficiency, as well as modularity. Extensive experiments show that our FedDiscooutperforms several state-of-the-art methods and can be easily incorporatedwith many existing methods to further enhance the performance. Our code will beavailable at https://github.com/MediaBrain-SJTU/FedDisco.</description><author>Rui Ye, Mingkai Xu, Jianyu Wang, Chenxin Xu, Siheng Chen, Yanfeng Wang</author><pubDate>Tue, 30 May 2023 18:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19229v1</guid></item><item><title>Unsupervised Melody-to-Lyric Generation</title><link>http://arxiv.org/abs/2305.19228v1</link><description>Automatic melody-to-lyric generation is a task in which song lyrics aregenerated to go with a given melody. It is of significant practical interestand more challenging than unconstrained lyric generation as the music imposesadditional constraints onto the lyrics. The training data is limited as mostsongs are copyrighted, resulting in models that underfit the complicatedcross-modal relationship between melody and lyrics. In this work, we propose amethod for generating high-quality lyrics without training on any alignedmelody-lyric data. Specifically, we design a hierarchical lyric generationframework that first generates a song outline and second the complete lyrics.The framework enables disentanglement of training (based purely on text) frominference (melody-guided text generation) to circumvent the shortage ofparallel data. We leverage the segmentation and rhythm alignment between melody and lyricsto compile the given melody into decoding constraints as guidance duringinference. The two-step hierarchical design also enables content control viathe lyric outline, a much-desired feature for democratizing collaborative songcreation. Experimental results show that our model can generate high-qualitylyrics that are more on-topic, singable, intelligible, and coherent than strongbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,with a 24% relative overall quality improvement based on human ratings. O</description><author>Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang, Nanyun Peng</author><pubDate>Tue, 30 May 2023 18:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19228v1</guid></item><item><title>Incentivizing honest performative predictions with proper scoring rules</title><link>http://arxiv.org/abs/2305.17601v2</link><description>Proper scoring rules incentivize experts to accurately report beliefs,assuming predictions cannot influence outcomes. We relax this assumption andinvestigate incentives when predictions are performative, i.e., when they caninfluence the outcome of the prediction, such as when making public predictionsabout the stock market. We say a prediction is a fixed point if it accuratelyreflects the expert's beliefs after that prediction has been made. We show thatin this setting, reports maximizing expected score generally do not reflect anexpert's beliefs, and we give bounds on the inaccuracy of such reports. We showthat, for binary predictions, if the influence of the expert's prediction onoutcomes is bounded, it is possible to define scoring rules under which optimalreports are arbitrarily close to fixed points. However, this is impossible forpredictions over more than two outcomes. We also perform numerical simulationsin a toy setting, showing that our bounds are tight in some situations and thatprediction error is often substantial (greater than 5-10%). Lastly, we discussalternative notions of optimality, including performative stability, and showthat they incentivize reporting fixed points.</description><author>Caspar Oesterheld, Johannes Treutlein, Emery Cooper, Rubi Hudson</author><pubDate>Tue, 30 May 2023 18:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17601v2</guid></item><item><title>Why Target Networks Stabilise Temporal Difference Methods</title><link>http://arxiv.org/abs/2302.12537v2</link><description>Integral to recent successes in deep reinforcement learning has been a classof temporal difference methods that use infrequently updated target values forpolicy evaluation in a Markov Decision Process. Yet a complete theoreticalexplanation for the effectiveness of target networks remains elusive. In thiswork, we provide an analysis of this popular class of algorithms, to finallyanswer the question: `why do target networks stabilise TD learning'? To do so,we formalise the notion of a partially fitted policy evaluation method, whichdescribes the use of target networks and bridges the gap between fitted methodsand semigradient temporal difference algorithms. Using this framework we areable to uniquely characterise the so-called deadly triad - the use of TDupdates with (nonlinear) function approximation and off-policy data - whichoften leads to nonconvergent algorithms. This insight leads us to conclude thatthe use of target networks can mitigate the effects of poor conditioning in theJacobian of the TD update. Instead, we show that under mild regularityconditions and a well tuned target network update frequency, convergence can beguaranteed even in the extremely challenging off-policy sampling and nonlinearfunction approximation setting.</description><author>Mattie Fellows, Matthew J. A. Smith, Shimon Whiteson</author><pubDate>Tue, 30 May 2023 18:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12537v2</guid></item><item><title>Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safety</title><link>http://arxiv.org/abs/2305.19223v1</link><description>The rapid advancement of artificial intelligence (AI) systems suggests thatartificial general intelligence (AGI) systems may soon arrive. Many researchersare concerned that AIs and AGIs will harm humans via intentional misuse(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,there is an increasing effort focused on developing algorithms and paradigmsthat ensure AI systems are aligned to what humans intend, e.g. AI systems thatyield actions or recommendations that humans might judge as consistent withtheir intentions and goals. Here we argue that alignment to human intent isinsufficient for safe AI systems and that preservation of long-term agency ofhumans may be a more robust standard, and one that needs to be separatedexplicitly and a priori during optimization. We argue that AI systems canreshape human intention and discuss the lack of biological and psychologicalmechanisms that protect humans from loss of agency. We provide the first formaldefinition of agency-preserving AI-human interactions which focuses onforward-looking agency evaluations and argue that AI systems - not humans -must be increasingly tasked with making these evaluations. We show how agencyloss can occur in simple environments containing embedded agents that usetemporal-difference learning to make action recommendations. Finally, wepropose a new area of research called "agency foundations" and pose fourinitial topics designed to improve our understanding of agency in AI-humaninteractions: benevolent game theory, algorithmic foundations of human rights,mechanistic interpretability of agency representation in neural-networks andreinforcement learning from internal states.</description><author>Catalin Mitelut, Ben Smith, Peter Vamplew</author><pubDate>Tue, 30 May 2023 18:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19223v1</guid></item><item><title>VoxDet: Voxel Learning for Novel Instance Detection</title><link>http://arxiv.org/abs/2305.17220v2</link><description>Detecting unseen instances based on multi-view templates is a challengingproblem due to its open-world nature. Traditional methodologies, whichprimarily rely on 2D representations and matching techniques, are ofteninadequate in handling pose variations and occlusions. To solve this, weintroduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes thestrong 3D voxel representation and reliable voxel matching mechanism. VoxDetfirst ingeniously proposes template voxel aggregation (TVA) module, effectivelytransforming multi-view 2D images into 3D voxel features. By leveragingassociated camera poses, these features are aggregated into a compact 3Dtemplate voxel. In novel instance detection, this voxel representationdemonstrates heightened resilience to occlusion and pose variations. We alsodiscover that a 3D reconstruction objective helps to pre-train the 2D-3Dmapping in TVA. Second, to quickly align with the template voxel, VoxDetincorporates a Query Voxel Matching (QVM) module. The 2D queries are firstconverted into their voxel representation with the learned 2D-3D mapping. Wefind that since the 3D voxel representations encode the geometry, we can firstestimate the relative rotation and then compare the aligned voxels, leading toimproved accuracy and efficiency. Exhaustive experiments are conducted on thedemanding LineMod-Occlusion, YCB-video, and the newly built RoboToolsbenchmarks, where VoxDet outperforms various 2D baselines remarkably with 20%higher recall and faster speed. To the best of our knowledge, VoxDet is thefirst to incorporate implicit 3D knowledge for 2D tasks.</description><author>Bowen Li, Jiashun Wang, Yaoyu Hu, Chen Wang, Sebastian Scherer</author><pubDate>Tue, 30 May 2023 18:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17220v2</guid></item><item><title>Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions</title><link>http://arxiv.org/abs/2206.03369v2</link><description>This paper is concerned with online filtering of discretely observednonlinear diffusion processes. Our approach is based on the fully adaptedauxiliary particle filter, which involves Doob's $h$-transforms that aretypically intractable. We propose a computational framework to approximatethese $h$-transforms by solving the underlying backward Kolmogorov equationsusing nonlinear Feynman-Kac formulas and neural networks. The methodologyallows one to train a locally optimal particle filter prior to thedata-assimilation procedure. Numerical experiments illustrate that the proposedapproach can be orders of magnitude more efficient than state-of-the-artparticle filters in the regime of highly informative observations, when theobservations are extreme under the model, or if the state dimension is large.</description><author>Nicolas Chopin, Andras Fulop, Jeremy Heng, Alexandre H. Thiery</author><pubDate>Tue, 30 May 2023 18:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03369v2</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v5</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Tue, 30 May 2023 18:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v5</guid></item><item><title>Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting</title><link>http://arxiv.org/abs/2209.01470v2</link><description>In this paper, we introduce a neural rendering pipeline for transferring thefacial expressions, head pose, and body movements of one person in a sourcevideo to another in a target video. We apply our method to the challenging caseof Sign Language videos: given a source video of a sign language user, we canfaithfully transfer the performed manual (e.g., handshape, palm orientation,movement, location) and non-manual (e.g., eye gaze, facial expressions, mouthpatterns, head, and body movements) signs to a target video in aphoto-realistic manner. Our method can be used for Sign Language Anonymization,Sign Language Production (synthesis module), as well as for reenacting othertypes of full body activities (dancing, acting performance, exercising, etc.).We conduct detailed qualitative and quantitative evaluations and comparisons,which demonstrate the particularly promising and realistic results that weobtain and the advantages of our method over existing approaches.</description><author>Christina O. Tze, Panagiotis P. Filntisis, Athanasia-Lida Dimou, Anastasios Roussos, Petros Maragos</author><pubDate>Tue, 30 May 2023 18:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01470v2</guid></item><item><title>Adversarial Attacks on Online Learning to Rank with Stochastic Click Models</title><link>http://arxiv.org/abs/2305.19218v1</link><description>We propose the first study of adversarial attacks on online learning to rank.The goal of the adversary is to misguide the online learning to rank algorithmto place the target item on top of the ranking list linear times to timehorizon $T$ with a sublinear attack cost. We propose generalized list poisoningattacks that perturb the ranking list presented to the user. This strategy canefficiently attack any no-regret ranker in general stochastic click models.Furthermore, we propose a click poisoning-based strategy named attack-then-quitthat can efficiently attack two representative OLTR algorithms for stochasticclick models. We theoretically analyze the success and cost upper bound of thetwo proposed methods. Experimental results based on synthetic and real-worlddata further validate the effectiveness and cost-efficiency of the proposedattack strategies.</description><author>Zichen Wang, Rishab Balasubramanian, Hui Yuan, Chenyu Song, Mengdi Wang, Huazheng Wang</author><pubDate>Tue, 30 May 2023 18:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19218v1</guid></item><item><title>Translation-Enhanced Multilingual Text-to-Image Generation</title><link>http://arxiv.org/abs/2305.19216v1</link><description>Research on text-to-image generation (TTI) still predominantly focuses on theEnglish language due to the lack of annotated image-caption data in otherlanguages; in the long run, this might widen inequitable access to TTItechnology. In this work, we thus investigate multilingual TTI (termed mTTI)and the current potential of neural machine translation (NMT) to bootstrap mTTIsystems. We provide two key contributions. 1) Relying on a multilingualmulti-modal encoder, we provide a systematic empirical study of standardmethods used in cross-lingual NLP when applied to mTTI: Translate Train,Translate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),a novel parameter-efficient approach that learns to weigh and consolidate themultilingual text knowledge within the mTTI framework, mitigating the languagegap and thus improving mTTI performance. Our evaluations on standard mTTIdatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential oftranslation-enhanced mTTI systems and also validate the benefits of theproposed EnsAd which derives consistent gains across all datasets. Furtherinvestigations on model variants, ablation studies, and qualitative analysesprovide additional insights on the inner workings of the proposed mTTIapproaches.</description><author>Yaoyiran Li, Ching-Yun Chang, Stephen Rawls, Ivan Vulić, Anna Korhonen</author><pubDate>Tue, 30 May 2023 18:03:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19216v1</guid></item><item><title>dotears: Scalable, consistent DAG estimation using observational and interventional data</title><link>http://arxiv.org/abs/2305.19215v1</link><description>Learning causal directed acyclic graphs (DAGs) from data is complicated by alack of identifiability and the combinatorial space of solutions. Recent workhas improved tractability of score-based structure learning of DAGs inobservational data, but is sensitive to the structure of the exogenous errorvariances. On the other hand, learning exogenous variance structure fromobservational data requires prior knowledge of structure. Motivated by newbiological technologies that link highly parallel gene interventions to ahigh-dimensional observation, we present $\texttt{dotears}$ [doo-tairs], ascalable structure learning framework which leverages observational andinterventional data to infer a single causal structure through continuousoptimization. $\texttt{dotears}$ exploits predictable structural consequencesof interventions to directly estimate the exogenous error structure, bypassingthe circular estimation problem. We extend previous work to show, bothempirically and analytically, that the inferences of previous methods aredriven by exogenous variance structure, but $\texttt{dotears}$ is robust toexogenous variance structure. Across varied simulations of large random DAGs,$\texttt{dotears}$ outperforms state-of-the-art methods in structureestimation. Finally, we show that $\texttt{dotears}$ is a provably consistentestimator of the true DAG under mild assumptions.</description><author>Albert Xue, Jingyou Rao, Sriram Sankararaman, Harold Pimentel</author><pubDate>Tue, 30 May 2023 18:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19215v1</guid></item><item><title>Design and implementation of intelligent packet filtering in IoT microcontroller-based devices</title><link>http://arxiv.org/abs/2305.19214v1</link><description>Internet of Things (IoT) devices are increasingly pervasive and essentialcomponents in enabling new applications and services. However, their widespreaduse also exposes them to exploitable vulnerabilities and flaws that can lead tosignificant losses. In this context, ensuring robust cybersecurity measures isessential to protect IoT devices from malicious attacks. However, the currentsolutions that provide flexible policy specifications and higher securitylevels for IoT devices are scarce. To address this gap, we introduce T800, alow-resource packet filter that utilizes machine learning (ML) algorithms toclassify packets in IoT devices. We present a detailed performance benchmarkingframework and demonstrate T800's effectiveness on the ESP32 system-on-chipmicrocontroller and ESP-IDF framework. Our evaluation shows that T800 is anefficient solution that increases device computational capacity by excludingunsolicited malicious traffic from the processing pipeline. Additionally, T800is adaptable to different systems and provides a well-documented performanceevaluation strategy for security ML-based mechanisms on ESP32-based IoTsystems. Our research contributes to improving the cybersecurity ofresource-constrained IoT devices and provides a scalable, efficient solutionthat can be used to enhance the security of IoT systems.</description><author>Gustavo de Carvalho Bertoli, Gabriel Victor C. Fernandes, Pedro H. Borges Monici, César H. de Araujo Guibo, Lourenço Alves Pereira Jr., Aldri Santos</author><pubDate>Tue, 30 May 2023 18:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19214v1</guid></item><item><title>The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code</title><link>http://arxiv.org/abs/2305.19213v1</link><description>Causal reasoning, the ability to identify cause-and-effect relationship, iscrucial in human thinking. Although large language models (LLMs) succeed inmany NLP tasks, it is still challenging for them to conduct complex causalreasoning like abductive reasoning and counterfactual reasoning. Given the factthat programming code may express causal relations more often and explicitlywith conditional statements like ``if``, we want to explore whether Code-LLMsacquire better causal reasoning abilities. Our experiments show that comparedto text-only LLMs, Code-LLMs with code prompts are significantly better incausal reasoning. We further intervene on the prompts from different aspects,and discover that the programming structure is crucial in code prompt design,while Code-LLMs are robust towards format perturbations.</description><author>Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao</author><pubDate>Tue, 30 May 2023 18:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19213v1</guid></item><item><title>Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling</title><link>http://arxiv.org/abs/2304.05365v5</link><description>There is a growing interest in using reinforcement learning (RL) topersonalize sequences of treatments in digital health to support users inadopting healthier behaviors. Such sequential decision-making problems involvedecisions about when to treat and how to treat based on the user's context(e.g., prior activity level, location, etc.). Online RL is a promisingdata-driven approach for this problem as it learns based on each user'shistorical responses and uses that knowledge to personalize these decisions.However, to decide whether the RL algorithm should be included in an``optimized'' intervention for real-world deployment, we must assess the dataevidence indicating that the RL algorithm is actually personalizing thetreatments to its users. Due to the stochasticity in the RL algorithm, one mayget a false impression that it is learning in certain states and using thislearning to provide specific treatments. We use a working definition ofpersonalization and introduce a resampling-based methodology for investigatingwhether the personalization exhibited by the RL algorithm is an artifact of theRL algorithm stochasticity. We illustrate our methodology with a case study byanalyzing the data from a physical activity clinical trial called HeartSteps,which included the use of an online RL algorithm. We demonstrate how ourapproach enhances data-driven truth-in-advertising of algorithm personalizationboth across all users as well as within specific users in the study.</description><author>Susobhan Ghosh, Raphael Kim, Prasidh Chhabria, Raaz Dwivedi, Predrag Klasnja, Peng Liao, Kelly Zhang, Susan Murphy</author><pubDate>Tue, 30 May 2023 18:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05365v5</guid></item><item><title>Solving Projected Model Counting by Utilizing Treewidth and its Limits</title><link>http://arxiv.org/abs/2305.19212v1</link><description>In this paper, we introduce a novel algorithm to solve projected modelcounting (PMC). PMC asks to count solutions of a Boolean formula with respectto a given set of projection variables, where multiple solutions that areidentical when restricted to the projection variables count as only onesolution. Inspired by the observation that the so-called "treewidth" is one ofthe most prominent structural parameters, our algorithm utilizes smalltreewidth of the primal graph of the input instance. More precisely, it runs intime O(2^2k+4n2) where k is the treewidth and n is the input size of theinstance. In other words, we obtain that the problem PMC is fixed-parametertractable when parameterized by treewidth. Further, we take the exponentialtime hypothesis (ETH) into consideration and establish lower bounds of boundedtreewidth algorithms for PMC, yielding asymptotically tight runtime bounds ofour algorithm. While the algorithm above serves as a first theoretical upperbound and although it might be quite appealing for small values of k,unsurprisingly a naive implementation adhering to this runtime bound suffersalready from instances of relatively small width. Therefore, we turn ourattention to several measures in order to resolve this issue towards exploitingtreewidth in practice: We present a technique called nested dynamicprogramming, where different levels of abstractions of the primal graph areused to (recursively) compute and refine tree decompositions of a giveninstance. Finally, we provide a nested dynamic programming algorithm and animplementation that relies on database technology for PMC and a prominentspecial case of PMC, namely model counting (#Sat). Experiments indicate thatthe advancements are promising, allowing us to solve instances of treewidthupper bounds beyond 200.</description><author>Johannes K. Fichte, Markus Hecher, Michael Morak, Patrick Thier, Stefan Woltran</author><pubDate>Tue, 30 May 2023 18:02:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19212v1</guid></item><item><title>COVID-19 Detection from Mass Spectra of Exhaled Breath</title><link>http://arxiv.org/abs/2305.19211v1</link><description>According to the World Health Organization, the SARS-CoV-2 virus generated aglobal emergency between 2020 and 2023 resulting in about 7 million deaths outof more than 750 million individuals diagnosed with COVID-19. During theseyears, polymerase-chain-reaction and antigen testing played a prominent role indisease control. In this study, we propose a fast and non-invasive detectionsystem exploiting a proprietary mass spectrometer to measure ions in exhaledbreath. We demonstrated that infected individuals, even if asymptomatic,exhibit characteristics in the air expelled from the lungs that can be detectedby a nanotech-based technology and then recognized by soft-computingalgorithms. A clinical trial was ran on about 300 patients: the mass spectra inthe 10-351 mass-to-charge range were measured, suitably pre-processed, andanalyzed by different classification models; eventually, the system shown anaccuracy of 95% and a recall of 94% in identifying cases of COVID-19. Withperformances comparable to traditional methodologies, the proposed system couldplay a significant role in both routine examination for common diseases andemergency response for new epidemics.</description><author>Nicolò Bellarmino, Giorgio Bozzini, Riccardo Cantoro, Francesco Castelletti, Michele Castelluzzo, Carla Ciricugno, Raffaele Correale, Daniela Dalla Gasperina, Francesco Dentali, Giovanni Poggialini, Piergiorgio Salerno, Giovanni Squillero, Stefano Taborelli</author><pubDate>Tue, 30 May 2023 18:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19211v1</guid></item><item><title>Group Invariant Global Pooling</title><link>http://arxiv.org/abs/2305.19207v1</link><description>Much work has been devoted to devising architectures that buildgroup-equivariant representations, while invariance is often induced usingsimple global pooling mechanisms. Little work has been done on creatingexpressive layers that are invariant to given symmetries, despite the successof permutation invariant pooling in various molecular tasks. In this work, wepresent Group Invariant Global Pooling (GIGP), an invariant pooling layer thatis provably sufficiently expressive to represent a large class of invariantfunctions. We validate GIGP on rotated MNIST and QM9, showing improvements forthe latter while attaining identical results for the former. By making thepooling process group orbit-aware, this invariant aggregation method leads toimproved performance, while performing well-principled group aggregation.</description><author>Kamil Bujel, Yonatan Gideoni, Chaitanya K. Joshi, Pietro Liò</author><pubDate>Tue, 30 May 2023 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19207v1</guid></item><item><title>Fast global convergence of gradient descent for low-rank matrix approximation</title><link>http://arxiv.org/abs/2305.19206v1</link><description>This paper investigates gradient descent for solving low-rank matrixapproximation problems. We begin by establishing the local linear convergenceof gradient descent for symmetric matrix approximation. Building on thisresult, we prove the rapid global convergence of gradient descent, particularlywhen initialized with small random values. Remarkably, we show that even withmoderate random initialization, which includes small random initialization as aspecial case, gradient descent achieves fast global convergence in scenarioswhere the top eigenvalues are identical. Furthermore, we extend our analysis toaddress asymmetric matrix approximation problems and investigate theeffectiveness of a retraction-free eigenspace computation method. Numericalexperiments strongly support our theory. In particular, the retraction-freealgorithm outperforms the corresponding Riemannian gradient descent method,resulting in a significant 29\% reduction in runtime.</description><author>Hengchao Chen, Xin Chen, Mohamad Elmasri, Qiang Sun</author><pubDate>Tue, 30 May 2023 17:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19206v1</guid></item><item><title>AMatFormer: Efficient Feature Matching via Anchor Matching Transformer</title><link>http://arxiv.org/abs/2305.19205v1</link><description>Learning based feature matching methods have been commonly studied in recentyears. The core issue for learning feature matching is to how to learn (1)discriminative representations for feature points (or regions) within eachintra-image and (2) consensus representations for feature points acrossinter-images. Recently, self- and cross-attention models have been exploited toaddress this issue. However, in many scenes, features are coming withlarge-scale, redundant and outliers contaminated. Previousself-/cross-attention models generally conduct message passing on all primalfeatures which thus lead to redundant learning and high computational cost. Tomitigate limitations, inspired by recent seed matching methods, in this paper,we propose a novel efficient Anchor Matching Transformer (AMatFormer) for thefeature matching problem. AMatFormer has two main aspects: First, it mainlyconducts self-/cross-attention on some anchor features and leverages theseanchor features as message bottleneck to learn the representations for allprimal features. Thus, it can be implemented efficiently and compactly. Second,AMatFormer adopts a shared FFN module to further embed the features of twoimages into the common domain and thus learn the consensus featurerepresentations for the matching problem. Experiments on several benchmarksdemonstrate the effectiveness and efficiency of the proposed AMatFormermatching approach.</description><author>Bo Jiang, Shuxian Luo, Xiao Wang, Chuanfu Li, Jin Tang</author><pubDate>Tue, 30 May 2023 17:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19205v1</guid></item><item><title>SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages</title><link>http://arxiv.org/abs/2305.19204v1</link><description>Text simplification research has mostly focused on sentence-levelsimplification, even though many desirable edits - such as adding relevantbackground information or reordering content - may require document-levelcontext. Prior work has also predominantly framed simplification as asingle-step, input-to-output task, only implicitly modeling the fine-grained,span-level edits that elucidate the simplification process. To address bothgaps, we introduce the SWiPE dataset, which reconstructs the document-levelediting process from English Wikipedia (EW) articles to paired Simple Wikipedia(SEW) articles. In contrast to prior work, SWiPE leverages the entire revisionhistory when pairing pages in order to better identify simplification edits. Wework with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labelingmore than 40,000 edits with proposed 19 categories. To scale our efforts, wepropose several models to automatically label edits, achieving an F-1 score ofup to 70.6, indicating that this is a tractable but challenging NLU task.Finally, we categorize the edits produced by several simplification models andfind that SWiPE-trained models generate more complex edits while reducingunwanted edits.</description><author>Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu</author><pubDate>Tue, 30 May 2023 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19204v1</guid></item><item><title>DäRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth Adaptation</title><link>http://arxiv.org/abs/2305.19201v1</link><description>Neural radiance fields (NeRF) shows powerful performance in novel viewsynthesis and 3D geometry reconstruction, but it suffers from criticalperformance degradation when the number of known viewpoints is drasticallyreduced. Existing works attempt to overcome this problem by employing externalpriors, but their success is limited to certain types of scenes or datasets.Employing monocular depth estimation (MDE) networks, pretrained on large-scaleRGB-D datasets, with powerful generalization capability would be a key tosolving this problem: however, using MDE in conjunction with NeRF comes with anew set of challenges due to various ambiguity problems exhibited by monoculardepths. In this light, we propose a novel framework, dubbed D\"aRF, thatachieves robust NeRF reconstruction with a handful of real-world images bycombining the strengths of NeRF and monocular depth estimation through onlinecomplementary training. Our framework imposes the MDE network's powerfulgeometry prior to NeRF representation at both seen and unseen viewpoints toenhance its robustness and coherence. In addition, we overcome the ambiguityproblems of monocular depths through patch-wise scale-shift fitting andgeometry distillation, which adapts the MDE network to produce depths alignedaccurately with NeRF geometry. Experiments show our framework achievesstate-of-the-art results both quantitatively and qualitatively, demonstratingconsistent and reliable performance in both indoor and outdoor real-worlddatasets. Project page is available at https://ku-cvlab.github.io/DaRF/.</description><author>Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim</author><pubDate>Tue, 30 May 2023 17:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19201v1</guid></item><item><title>Likelihood-Based Diffusion Language Models</title><link>http://arxiv.org/abs/2305.18619v1</link><description>Despite a growing interest in diffusion-based language models, existing workhas not shown that these models can attain nontrivial likelihoods on standardlanguage modeling benchmarks. In this work, we take the first steps towardsclosing the likelihood gap between autoregressive and diffusion-based languagemodels, with the goal of building and releasing a diffusion model whichoutperforms a small but widely-known autoregressive model. We pursue this goalthrough algorithmic improvements, scaling laws, and increased compute. On thealgorithmic front, we introduce several methodological improvements for themaximum-likelihood training of diffusion language models. We then study scalinglaws for our diffusion models and find compute-optimal training regimes whichdiffer substantially from autoregressive models. Using our methods and scalinganalysis, we train and release Plaid 1B, a large diffusion language model whichoutperforms GPT-2 124M in likelihood on benchmark datasets and generates fluentsamples in unconditional and zero-shot control settings.</description><author>Ishaan Gulrajani, Tatsunori B. Hashimoto</author><pubDate>Tue, 30 May 2023 17:43:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18619v1</guid></item><item><title>Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling</title><link>http://arxiv.org/abs/2305.10769v3</link><description>Diffusion Probability Models (DPMs) have made impressive advancements invarious machine learning domains. However, achieving high-quality syntheticsamples typically involves performing a large number of sampling steps, whichimpedes the possibility of real-time sample synthesis. Traditional acceleratedsampling algorithms via knowledge distillation rely on pre-trained modelweights and discrete time step scenarios, necessitating additional trainingsessions to achieve their goals. To address these issues, we propose theCatch-Up Distillation (CUD), which encourages the current moment output of thevelocity estimation model ``catch up'' with its previous moment output.Specifically, CUD adjusts the original Ordinary Differential Equation (ODE)training objective to align the current moment output with both the groundtruth label and the previous moment output, utilizing Runge-Kutta-basedmulti-step alignment distillation for precise ODE estimation while preventingasynchronous updates. Furthermore, we investigate the design space for CUDsunder continuous time-step scenarios and analyze how to determine the suitablestrategies. To demonstrate CUD's effectiveness, we conduct thorough ablationand comparison experiments on CIFAR-10, MNIST, and ImageNet-64. On CIFAR-10, weobtain a FID of 2.80 by sampling in 15 steps under one-session training and thenew state-of-the-art FID of 3.37 by sampling in one step with additionaltraining. This latter result necessitated only 620k iterations with a batchsize of 128, in contrast to Consistency Distillation, which demanded 2100kiterations with a larger batch size of 256. Our code is released athttps://anonymous.4open.science/r/Catch-Up-Distillation-E31F.</description><author>Shitong Shao, Xu Dai, Shouyi Yin, Lujun Li, Huanran Chen, Yang Hu</author><pubDate>Tue, 30 May 2023 17:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10769v3</guid></item><item><title>PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2305.19195v1</link><description>Vision-and-Language Navigation (VLN) requires the agent to follow languageinstructions to navigate through 3D environments. One main challenge in VLN isthe limited availability of photorealistic training environments, which makesit hard to generalize to new and unseen environments. To address this problem,we propose PanoGen, a generation method that can potentially create an infinitenumber of diverse panoramic environments conditioned on text. Specifically, wecollect room descriptions by captioning the room images in existingMatterport3D environments, and leverage a state-of-the-art text-to-imagediffusion model to generate the new panoramic environments. We use recursiveoutpainting over the generated images to create consistent 360-degree panoramaviews. Our new panoramic environments share similar semantic information withthe original environments by conditioning on text descriptions, which ensuresthe co-occurrence of objects in the panorama follows human intuition, andcreates enough diversity in room appearance and layout with image outpainting.Lastly, we explore two ways of utilizing PanoGen in VLN pre-training andfine-tuning. We generate instructions for paths in our PanoGen environmentswith a speaker built on a pre-trained vision-and-language model for VLNpre-training, and augment the visual observation with our panoramicenvironments during agents' fine-tuning to avoid overfitting to seenenvironments. Empirically, learning with our PanoGen environments achieves thenew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.Pre-training with our PanoGen speaker data is especially effective for CVDN,which has under-specified instructions and needs commonsense knowledge. Lastly,we show that the agent can benefit from training with more generated panoramicenvironments, suggesting promising results for scaling up the PanoGenenvironments.</description><author>Jialu Li, Mohit Bansal</author><pubDate>Tue, 30 May 2023 17:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19195v1</guid></item><item><title>FakeSwarm: Improving Fake News Detection with Swarming Characteristics</title><link>http://arxiv.org/abs/2305.19194v1</link><description>The proliferation of fake news poses a serious threat to society, as it canmisinform and manipulate the public, erode trust in institutions, and underminedemocratic processes. To address this issue, we present FakeSwarm, a fake newsidentification system that leverages the swarming characteristics of fake news.To extract the swarm behavior, we propose a novel concept of fake news swarmingcharacteristics and design three types of swarm features, including principalcomponent analysis, metric representation, and position encoding. We evaluateour system on a public dataset and demonstrate the effectiveness ofincorporating swarm features in fake news identification, achieving an f1-scoreand accuracy of over 97% by combining all three types of swarm features.Furthermore, we design an online learning pipeline based on the hypothesis ofthe temporal distribution pattern of fake news emergence, validated on a topicwith early emerging fake news and a shortage of text samples, showing thatswarm features can significantly improve recall rates in such cases. Our workprovides a new perspective and approach to fake news detection and highlightsthe importance of considering swarming characteristics in detecting fake news.</description><author>Jun Wu, Xuesong Ye</author><pubDate>Tue, 30 May 2023 17:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19194v1</guid></item><item><title>Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models</title><link>http://arxiv.org/abs/2305.19193v1</link><description>In this study, we present an efficient and effective approach for achievingtemporally consistent synthetic-to-real video translation in videos of varyinglengths. Our method leverages off-the-shelf conditional image diffusion models,allowing us to perform multiple synthetic-to-real image generations inparallel. By utilizing the available optical flow information from thesynthetic videos, our approach seamlessly enforces temporal consistency amongcorresponding pixels across frames. This is achieved through joint noiseoptimization, effectively minimizing spatial and temporal discrepancies. To thebest of our knowledge, our proposed method is the first to accomplish diverseand temporally consistent synthetic-to-real video translation using conditionalimage diffusion models. Furthermore, our approach does not require any trainingor fine-tuning of the diffusion models. Extensive experiments conducted onvarious benchmarks for synthetic-to-real video translation demonstrate theeffectiveness of our approach, both quantitatively and qualitatively. Finally,we show that our method outperforms other baseline methods in terms of bothtemporal consistency and visual quality.</description><author>Ernie Chu, Shuo-Yen Lin, Jun-Cheng Chen</author><pubDate>Tue, 30 May 2023 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19193v1</guid></item><item><title>Inverse Approximation Theory for Nonlinear Recurrent Neural Networks</title><link>http://arxiv.org/abs/2305.19190v1</link><description>We prove an inverse approximation theorem for the approximation of nonlinearsequence-to-sequence relationships using RNNs. This is a so-calledBernstein-type result in approximation theory, which deduces properties of atarget function under the assumption that it can be effectively approximated bya hypothesis space. In particular, we show that nonlinear sequencerelationships, viewed as functional sequences, that can be stably approximatedby RNNs with hardtanh/tanh activations must have an exponential decaying memorystructure -- a notion that can be made precise. This extends the previouslyidentified curse of memory in linear RNNs into the general nonlinear setting,and quantifies the essential limitations of the RNN architecture for learningsequential relationships with long-term memory. Based on the analysis, wepropose a principled reparameterization method to overcome the limitations. Ourtheoretical results are confirmed by numerical experiments.</description><author>Shida Wang, Zhong Li, Qianxiao Li</author><pubDate>Tue, 30 May 2023 17:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19190v1</guid></item><item><title>Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models</title><link>http://arxiv.org/abs/2305.19187v1</link><description>Large language models (LLMs) specializing in natural language generation(NLG) have recently started exhibiting promising capabilities across a varietyof domains. However, gauging the trustworthiness of responses generated by LLMsremains an open challenge, with limited research on uncertainty quantificationfor NLG. Furthermore, existing literature typically assumes white-box access tolanguage models, which is becoming unrealistic either due to the closed-sourcenature of the latest LLMs or due to computational constraints. In this work, weinvestigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. Wefirst differentiate two closely-related notions: $\textit{uncertainty}$, whichdepends only on the input, and $\textit{confidence}$, which additionallydepends on the generated response. We then propose and compare severalconfidence/uncertainty metrics, applying them to $\textit{selective NLG}$,where unreliable results could either be ignored or yielded for furtherassessment. Our findings on several popular LLMs and datasets reveal that asimple yet effective metric for the average semantic dispersion can be areliable predictor of the quality of LLM responses. This study can providevaluable insights for practitioners on uncertainty management when adoptingLLMs. The code to replicate all our experiments is available athttps://github.com/zlin7/UQ-NLG.</description><author>Zhen Lin, Shubhendu Trivedi, Jimeng Sun</author><pubDate>Tue, 30 May 2023 17:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19187v1</guid></item><item><title>Compression with Bayesian Implicit Neural Representations</title><link>http://arxiv.org/abs/2305.19185v1</link><description>Many common types of data can be represented as functions that mapcoordinates to signal values, such as pixel locations to RGB values in the caseof an image. Based on this view, data can be compressed by overfitting acompact neural network to its functional representation and then encoding thenetwork weights. However, most current solutions for this are inefficient, asquantization to low-bit precision substantially degrades the reconstructionquality. To address this issue, we propose overfitting variational Bayesianneural networks to the data and compressing an approximate posterior weightsample using relative entropy coding instead of quantizing and entropy codingit. This strategy enables direct optimization of the rate-distortionperformance by minimizing the $\beta$-ELBO, and target differentrate-distortion trade-offs for a given network architecture by adjusting$\beta$. Moreover, we introduce an iterative algorithm for learning priorweight distributions and employ a progressive refinement process for thevariational posterior that significantly enhances performance. Experiments showthat our method achieves strong performance on image and audio compressionwhile retaining simplicity.</description><author>Zongyu Guo, Gergely Flamich, Jiajun He, Zhibo Chen, José Miguel Hernández-Lobato</author><pubDate>Tue, 30 May 2023 17:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19185v1</guid></item><item><title>Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models</title><link>http://arxiv.org/abs/2305.19184v1</link><description>In large part due to their implicit semantic modeling, self-supervisedlearning (SSL) methods have significantly increased the performance of valencerecognition in speech emotion recognition (SER) systems. Yet, their large sizemay often hinder practical implementations. In this work, we take HuBERT as anexample of an SSL model and analyze the relevance of each of its layers forSER. We show that shallow layers are more important for arousal recognitionwhile deeper layers are more important for valence. This observation motivatesthe importance of additional textual information for accurate valencerecognition, as the distilled framework lacks the depth of its large-scale SSLteacher. Thus, we propose an audio-textual distilled SSL framework that, whilehaving only ~20% of the trainable parameters of a large SSL model, achieves onpar performance across the three emotion dimensions (arousal, valence,dominance) on the MSP-Podcast v1.10 dataset.</description><author>Danilo de Oliveira, Navin Raj Prabhu, Timo Gerkmann</author><pubDate>Tue, 30 May 2023 17:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19184v1</guid></item><item><title>Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting</title><link>http://arxiv.org/abs/2305.19183v1</link><description>Existing relationships among time series can be exploited as inductive biasesin learning effective forecasting models. In hierarchical time series,relationships among subsets of sequences induce hard constraints (hierarchicalinductive biases) on the predicted values. In this paper, we propose agraph-based methodology to unify relational and hierarchical inductive biasesin the context of deep learning for time series forecasting. In particular, wemodel both types of relationships as dependencies in a pyramidal graphstructure, with each pyramidal layer corresponding to a level of the hierarchy.By exploiting modern - trainable - graph pooling operators we show that thehierarchical structure, if not available as a prior, can be learned directlyfrom data, thus obtaining cluster assignments aligned with the forecastingobjective. A differentiable reconciliation stage is incorporated into theprocessing architecture, allowing hierarchical constraints to act both as anarchitectural bias as well as a regularization element for predictions.Simulation results on representative datasets show that the proposed methodcompares favorably against the state of the art.</description><author>Andrea Cini, Danilo Mandic, Cesare Alippi</author><pubDate>Tue, 30 May 2023 17:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19183v1</guid></item><item><title>Table Detection for Visually Rich Document Images</title><link>http://arxiv.org/abs/2305.19181v1</link><description>Table Detection (TD) is a fundamental task towards visually rich documentunderstanding. Current studies usually formulate the TD problem as an objectdetection problem, then leverage Intersection over Union (IoU) based metrics toevaluate the model performance and IoU-based loss functions to optimize themodel. TD applications usually require the prediction results to cover all thetable contents and avoid information loss. However, IoU and IoU-based lossfunctions cannot directly reflect the degree of information loss for theprediction results. Therefore, we propose to decouple IoU into a ground truthcoverage term and a prediction coverage term, in which the former can be usedto measure the information loss of the prediction results. Besides, tables in the documents are usually large, sparsely distributed, andhave no overlaps because they are designed to summarize essential informationto make it easy to read and interpret for human readers. Therefore, in thisstudy, we use SparseR-CNN as the base model, and further improve the model byusing Gaussian Noise Augmented Image Size region proposals and many-to-onelabel assignments. To demonstrate the effectiveness of proposed method and compare withstate-of-the-art methods fairly, we conduct experiments and use IoU-basedevaluation metrics to evaluate the model performance. The experimental resultsshow that the proposed method can consistently outperform state-of-the-artmethods under different IoU-based metric on a variety of datasets. We conductfurther experiments to show the superiority of the proposed decoupled IoU forthe TD applications by replacing the IoU-based loss functions and evaluationmetrics with proposed decoupled IoU counterparts. The experimental results showthat our proposed decoupled IoU loss can encourage the model to alleviateinformation loss.</description><author>Bin Xiao, Murat Simsek, Burak Kantarci, Ala Abu Alkheir</author><pubDate>Tue, 30 May 2023 17:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19181v1</guid></item><item><title>Expand-and-Cluster: Exact Parameter Recovery of Neural Networks</title><link>http://arxiv.org/abs/2304.12794v2</link><description>Can we recover the hidden parameters of an Artificial Neural Network (ANN) byprobing its input-output mapping? We propose a systematic method, called`Expand-and-Cluster' that needs only the number of hidden layers and theactivation function of the probed ANN to identify all network parameters. Inthe expansion phase, we train a series of networks of increasing size using theprobed data of the ANN as a teacher. Expansion stops when a minimal loss isconsistently reached in networks of a given size. In the clustering phase,weight vectors of the expanded students are clustered, which allows structuredpruning of superfluous neurons in a principled way. We find that anoverparameterization of a factor four is sufficient to reliably identify theminimal number of neurons and to retrieve the original network parameters in$80\%$ of tasks across a family of 150 toy problems of variable difficulty.Furthermore, shallow and deep teacher networks trained on MNIST data can beidentified with less than $5\%$ overhead in the neuron number. Thus, whiledirect training of a student network with a size identical to that of theteacher is practically impossible because of the highly non-convex lossfunction, training with mild overparameterization followed by clustering andstructured pruning correctly identifies the target network.</description><author>Flavio Martinelli, Berfin Simsek, Johanni Brea, Wulfram Gerstner</author><pubDate>Tue, 30 May 2023 17:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12794v2</guid></item><item><title>LNO: Laplace Neural Operator for Solving Differential Equations</title><link>http://arxiv.org/abs/2303.10528v2</link><description>We introduce the Laplace neural operator (LNO), which leverages the Laplacetransform to decompose the input space. Unlike the Fourier Neural Operator(FNO), LNO can handle non-periodic signals, account for transient responses,and exhibit exponential convergence. LNO incorporates the pole-residuerelationship between the input and the output space, enabling greaterinterpretability and improved generalization ability. Herein, we demonstratethe superior approximation accuracy of a single Laplace layer in LNO over fourFourier modules in FNO in approximating the solutions of three ODEs (Duffingoscillator, driven gravity pendulum, and Lorenz system) and three PDEs(Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system).Notably, LNO outperforms FNO in capturing transient responses in undampedscenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO'sexact representation of the pole-residue formulation yields significantlybetter results than FNO. For the nonlinear reaction-diffusion system, LNO'serrors are smaller than those of FNO, demonstrating the effectiveness of usingsystem poles and residues as network parameters for operator learning. Overall,our results suggest that LNO represents a promising new approach for learningneural operators that map functions between infinite-dimensional spaces.</description><author>Qianying Cao, Somdatta Goswami, George Em Karniadakis</author><pubDate>Tue, 30 May 2023 17:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10528v2</guid></item><item><title>Forward-Forward Training of an Optical Neural Network</title><link>http://arxiv.org/abs/2305.19170v1</link><description>Neural networks (NN) have demonstrated remarkable capabilities in varioustasks, but their computation-intensive nature demands faster and moreenergy-efficient hardware implementations. Optics-based platforms, usingtechnologies such as silicon photonics and spatial light modulators, offerpromising avenues for achieving this goal. However, training multiple trainablelayers in tandem with these physical systems poses challenges, as they aredifficult to fully characterize and describe with differentiable functions,hindering the use of error backpropagation algorithm. The recently introducedForward-Forward Algorithm (FFA) eliminates the need for perfectcharacterization of the learning system and shows promise for efficienttraining with large numbers of programmable parameters. The FFA does notrequire backpropagating an error signal to update the weights, rather theweights are updated by only sending information in one direction. The localloss function for each set of trainable weights enables low-power analoghardware implementations without resorting to metaheuristic algorithms orreinforcement learning. In this paper, we present an experiment utilizingmultimode nonlinear wave propagation in an optical fiber demonstrating thefeasibility of the FFA approach using an optical system. The results show thatincorporating optical transforms in multilayer NN architectures trained withthe FFA, can lead to performance improvements, even with a relatively smallnumber of trainable weights. The proposed method offers a new path to thechallenge of training optical NNs and provides insights into leveragingphysical transformations for enhancing NN performance.</description><author>Ilker Oguz, Junjie Ke, Qifei Wang, Feng Yang, Mustafa Yildirim, Niyazi Ulas Dinc, Jih-Liang Hsieh, Christophe Moser, Demetri Psaltis</author><pubDate>Tue, 30 May 2023 17:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19170v1</guid></item><item><title>Reduced Precision Floating-Point Optimization for Deep Neural Network On-Device Learning on MicroControllers</title><link>http://arxiv.org/abs/2305.19167v1</link><description>Enabling On-Device Learning (ODL) for Ultra-Low-Power Micro-Controller Units(MCUs) is a key step for post-deployment adaptation and fine-tuning of DeepNeural Network (DNN) models in future TinyML applications. This paper tacklesthis challenge by introducing a novel reduced precision optimization techniquefor ODL primitives on MCU-class devices, leveraging the State-of-Artadvancements in RISC-V RV32 architectures with support for vectorized 16-bitfloating-point (FP16) Single-Instruction Multiple-Data (SIMD) operations. Ourapproach for the Forward and Backward steps of the Back-Propagation trainingalgorithm is composed of specialized shape transform operators and MatrixMultiplication (MM) kernels, accelerated with parallelization and loopunrolling. When evaluated on a single training step of a 2D Convolution layer,the SIMD-optimized FP16 primitives result up to 1.72$\times$ faster than theFP32 baseline on a RISC-V-based 8+1-core MCU. An average computing efficiencyof 3.11 Multiply and Accumulate operations per clock cycle (MAC/clk) and 0.81MAC/clk is measured for the end-to-end training tasks of a ResNet8 and a DS-CNNfor Image Classification and Keyword Spotting, respectively -- requiring 17.1ms and 6.4 ms on the target platform to compute a training step on a singlesample. Overall, our approach results more than two orders of magnitude fasterthan existing ODL software frameworks for single-core MCUs and outperforms by1.6 $\times$ previous FP32 parallel implementations on a Continual Learningsetup.</description><author>Davide Nadalini, Manuele Rusci, Luca Benini, Francesco Conti</author><pubDate>Tue, 30 May 2023 17:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19167v1</guid></item><item><title>Analogy-Forming Transformers for Few-Shot 3D Parsing</title><link>http://arxiv.org/abs/2304.14382v2</link><description>We present Analogical Networks, a model that encodes domain knowledgeexplicitly, in a collection of structured labelled 3D scenes, in addition toimplicitly, as model parameters, and segments 3D object scenes with analogicalreasoning: instead of mapping a scene to part segments directly, our modelfirst retrieves related scenes from memory and their corresponding partstructures, and then predicts analogous part structures for the input scene,via an end-to-end learnable modulation mechanism. By conditioning on more thanone retrieved memories, compositions of structures are predicted, that mix andmatch parts across the retrieved memories. One-shot, few-shot or many-shotlearning are treated uniformly in Analogical Networks, by conditioning on theappropriate set of memories, whether taken from a single, few or many memoryexemplars, and inferring analogous parses. We show Analogical Networks arecompetitive with state-of-the-art 3D segmentation transformers in many-shotsettings, and outperform them, as well as existing paradigms of meta-learningand few-shot learning, in few-shot settings. Analogical Networks successfullysegment instances of novel object categories simply by expanding their memory,without any weight updates. Our code and models are publicly available in theproject webpage: http://analogicalnets.github.io/.</description><author>Nikolaos Gkanatsios, Mayank Singh, Zhaoyuan Fang, Shubham Tulsiani, Katerina Fragkiadaki</author><pubDate>Tue, 30 May 2023 17:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14382v2</guid></item><item><title>Strategic Reasoning with Language Models</title><link>http://arxiv.org/abs/2305.19165v1</link><description>Strategic reasoning enables agents to cooperate, communicate, and competewith other agents in diverse situations. Existing approaches to solvingstrategic games rely on extensive training, yielding strategies that do notgeneralize to new scenarios or games without retraining. Large Language Models(LLMs), with their ability to comprehend and generate complex, context-richlanguage, could prove powerful as tools for strategic gameplay. This paperintroduces an approach that uses pretrained LLMs with few-shot chain-of-thoughtexamples to enable strategic reasoning for AI agents. Our approach usessystematically generated demonstrations of reasoning about states, values, andbeliefs to prompt the model. Using extensive variations of simple matrix games,we show that strategies that are derived based on systematically generatedprompts generalize almost perfectly to new game structures, alternateobjectives, and hidden information. Additionally, we demonstrate our approachcan lead to human-like negotiation strategies in realistic scenarios withoutany extra training or fine-tuning. Our results highlight the ability of LLMs,guided by systematic reasoning demonstrations, to adapt and excel in diversestrategic scenarios.</description><author>Kanishk Gandhi, Dorsa Sadigh, Noah D. Goodman</author><pubDate>Tue, 30 May 2023 17:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19165v1</guid></item><item><title>LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images</title><link>http://arxiv.org/abs/2305.19164v1</link><description>We propose an automated algorithm to stress-test a trained visual model bygenerating language-guided counterfactual test images (LANCE). Our methodleverages recent progress in large language modeling and text-based imageediting to augment an IID test set with a suite of diverse, realistic, andchallenging test images without altering model weights. We benchmark theperformance of a diverse set of pretrained models on our generated data andobserve significant and consistent performance drops. We further analyze modelsensitivity across different types of edits, and demonstrate its applicabilityat surfacing previously unknown class-level model biases in ImageNet.</description><author>Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, Judy Hoffman</author><pubDate>Tue, 30 May 2023 17:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19164v1</guid></item><item><title>NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</title><link>http://arxiv.org/abs/2304.09116v3</link><description>Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wilddatasets is important to capture the diversity in human speech such as speakeridentities, prosodies, and styles (e.g., singing). Current large TTS systemsusually quantize speech into discrete tokens and use language models togenerate these tokens one by one, which suffer from unstable prosody, wordskipping/repeating issue, and poor voice quality. In this paper, we developNaturalSpeech 2, a TTS system that leverages a neural audio codec with residualvector quantizers to get the quantized latent vectors and uses a diffusionmodel to generate these latent vectors conditioned on text input. To enhancethe zero-shot capability that is important to achieve diverse speech synthesis,we design a speech prompting mechanism to facilitate in-context learning in thediffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 tolarge-scale datasets with 44K hours of speech and singing data and evaluate itsvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTSsystems by a large margin in terms of prosody/timbre similarity, robustness,and voice quality in a zero-shot setting, and performs novel zero-shot singingsynthesis with only a speech prompt. Audio samples are available athttps://speechresearch.github.io/naturalspeech2.</description><author>Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian</author><pubDate>Tue, 30 May 2023 17:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09116v3</guid></item><item><title>Deep Operator Learning Lessens the Curse of Dimensionality for PDEs</title><link>http://arxiv.org/abs/2301.12227v2</link><description>Deep neural networks (DNNs) have achieved remarkable success in numerousdomains, and their application to PDE-related problems has been rapidlyadvancing. This paper provides an estimate for the generalization error oflearning Lipschitz operators over Banach spaces using DNNs with applications tovarious PDE solution operators. The goal is to specify DNN width, depth, andthe number of training samples needed to guarantee a certain testing error.Under mild assumptions on data distributions or operator structures, ouranalysis shows that deep operator learning can have a relaxed dependence on thediscretization resolution of PDEs and, hence, lessen the curse ofdimensionality in many PDE-related problems including elliptic equations,parabolic equations, and Burgers equations. Our results are also applied togive insights about discretization-invariant in operator learning.</description><author>Ke Chen, Chunmei Wang, Haizhao Yang</author><pubDate>Tue, 30 May 2023 17:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12227v2</guid></item><item><title>An AMR-based Link Prediction Approach for Document-level Event Argument Extraction</title><link>http://arxiv.org/abs/2305.19162v1</link><description>Recent works have introduced Abstract Meaning Representation (AMR) forDocument-level Event Argument Extraction (Doc-level EAE), since AMR provides auseful interpretation of complex semantic structures and helps to capturelong-distance dependency. However, in these works AMR is used only implicitly,for instance, as additional features or training signals. Motivated by the factthat all event structures can be inferred from AMR, this work reformulates EAEas a link prediction problem on AMR graphs. Since AMR is a generic structureand does not perfectly suit EAE, we propose a novel graph structure, TailoredAMR Graph (TAG), which compresses less informative subgraphs and edge types,integrates span information, and highlights surrounding events in the samedocument. With TAG, we further propose a novel method using graph neuralnetworks as a link prediction model to find event arguments. Our extensiveexperiments on WikiEvents and RAMS show that this simpler approach outperformsthe state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do sowith reduced 56% inference time. The code is availabel athttps://github.com/ayyyq/TARA.</description><author>Yuqing Yang, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang</author><pubDate>Tue, 30 May 2023 17:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19162v1</guid></item><item><title>Cooperative Thresholded Lasso for Sparse Linear Bandit</title><link>http://arxiv.org/abs/2305.19161v1</link><description>We present a novel approach to address the multi-agent sparse contextuallinear bandit problem, in which the feature vectors have a high dimension $d$whereas the reward function depends on only a limited set of features -precisely $s_0 \ll d$. Furthermore, the learning follows underinformation-sharing constraints. The proposed method employs Lasso regressionfor dimension reduction, allowing each agent to independently estimate anapproximate set of main dimensions and share that information with othersdepending on the network's structure. The information is then aggregatedthrough a specific process and shared with all agents. Each agent then resolvesthe problem with ridge regression focusing solely on the extracted dimensions.We represent algorithms for both a star-shaped network and a peer-to-peernetwork. The approaches effectively reduce communication costs while ensuringminimal cumulative regret per agent. Theoretically, we show that our proposedmethods have a regret bound of order $\mathcal{O}(s_0 \log d + s_0 \sqrt{T})$with high probability, where $T$ is the time horizon. To our best knowledge, itis the first algorithm that tackles row-wise distributed data in sparse linearbandits, achieving comparable performance compared to the state-of-the-artsingle and multi-agent methods. Besides, it is widely applicable tohigh-dimensional multi-agent problems where efficient feature extraction iscritical for minimizing regret. To validate the effectiveness of our approach,we present experimental results on both synthetic and real-world datasets.</description><author>Haniyeh Barghi, Xiaotong Cheng, Setareh Maghsudi</author><pubDate>Tue, 30 May 2023 17:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19161v1</guid></item><item><title>Learning Control by Iterative Inversion</title><link>http://arxiv.org/abs/2211.01724v2</link><description>We propose $\textit{iterative inversion}$ -- an algorithm for learning aninverse function without input-output pairs, but only with samples from thedesired output distribution and access to the forward function. The keychallenge is a $\textit{distribution shift}$ between the desired outputs andthe outputs of an initial random guess, and we prove that iterative inversioncan steer the learning correctly, under rather strict conditions on thefunction. We apply iterative inversion to learn control. Our input is a set ofdemonstrations of desired behavior, given as video embeddings of trajectories(without actions), and our method iteratively learns to imitate trajectoriesgenerated by the current policy, perturbed by random exploration noise. Ourapproach does not require rewards, and only employs supervised learning, whichcan be easily scaled to use state-of-the-art trajectory embedding techniquesand policy representations. Indeed, with a VQ-VAE embedding, and atransformer-based policy, we demonstrate non-trivial continuous control onseveral tasks. Further, we report an improved performance on imitating diversebehaviors compared to reward based methods.</description><author>Gal Leibovich, Guy Jacob, Or Avner, Gal Novik, Aviv Tamar</author><pubDate>Tue, 30 May 2023 17:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01724v2</guid></item><item><title>Recognizing People by Body Shape Using Deep Networks of Images and Words</title><link>http://arxiv.org/abs/2305.19160v1</link><description>Common and important applications of person identification occur at distancesand viewpoints in which the face is not visible or is not sufficiently resolvedto be useful. We examine body shape as a biometric across distance andviewpoint variation. We propose an approach that combines standard objectclassification networks with representations based on linguistic (word-based)descriptions of bodies. Algorithms with and without linguistic training werecompared on their ability to identify people from body shape in images capturedacross a large range of distances/views (close-range, 100m, 200m, 270m, 300m,370m, 400m, 490m, 500m, 600m, and at elevated pitch in images taken by anunmanned aerial vehicle [UAV]). Accuracy, as measured by identity-match rankingand false accept errors in an open-set test, was surprisingly good. Foridentity-ranking, linguistic models were more accurate for close-range images,whereas non-linguistic models fared better at intermediary distances. Fusion ofthe linguistic and non-linguistic embeddings improved performance at all, butthe farthest distance. Although the non-linguistic model yielded fewer falseaccepts at all distances, fusion of the linguistic and non-linguistic modelsdecreased false accepts for all, but the UAV images. We conclude thatlinguistic and non-linguistic representations of body shape can offercomplementary identity information for bodies that can improve identificationin applications of interest.</description><author>Blake A. Myers, Lucas Jaggernauth, Thomas M. Metz, Matthew Q. Hill, Veda Nandan Gandi, Carlos D. Castillo, Alice J. O'Toole</author><pubDate>Tue, 30 May 2023 17:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19160v1</guid></item><item><title>Diversity Enhanced Table-to-Text Generation via Type Control</title><link>http://arxiv.org/abs/2205.10938v2</link><description>Generating natural language statements to convey logical inferences fromtabular data (i.e., Logical NLG) is a process with one input and a variety ofvalid outputs. This characteristic underscores the need for a method to producea diverse set of valid outputs, presenting different perspectives of the inputdata. We propose a simple yet effective diversity-enhancing scheme that buildsupon an inherent property of the statements, their logic-types, by using atype-controlled table-to-text generation model. We demonstrate, throughextensive automatic and human evaluations over the two publicly availableLogical NLG datasets, that our proposed method both facilitates the ability toeffectively control the generated statement type, and produces results superiorto the strongest baselines in terms of quality and factuality-diversitytrade-off.</description><author>Yotam Perlitz, Liat Ein-Dor, Dafna Sheinwald, Noam Slonim, Michal Shmueli-Scheuer</author><pubDate>Tue, 30 May 2023 17:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10938v2</guid></item><item><title>Competing for Shareable Arms in Multi-Player Multi-Armed Bandits</title><link>http://arxiv.org/abs/2305.19158v1</link><description>Competitions for shareable and limited resources have long been studied withstrategic agents. In reality, agents often have to learn and maximize therewards of the resources at the same time. To design an individualizedcompeting policy, we model the competition between agents in a novelmulti-player multi-armed bandit (MPMAB) setting where players are selfish andaim to maximize their own rewards. In addition, when several players pull thesame arm, we assume that these players averagely share the arms' rewards byexpectation. Under this setting, we first analyze the Nash equilibrium whenarms' rewards are known. Subsequently, we propose a novel SelfishMPMAB withAveraging Allocation (SMAA) approach based on the equilibrium. We theoreticallydemonstrate that SMAA could achieve a good regret guarantee for each playerwhen all players follow the algorithm. Additionally, we establish that nosingle selfish player can significantly increase their rewards throughdeviation, nor can they detrimentally affect other players' rewards withoutincurring substantial losses for themselves. We finally validate theeffectiveness of the method in extensive synthetic experiments.</description><author>Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui</author><pubDate>Tue, 30 May 2023 16:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19158v1</guid></item><item><title>V1T: large-scale mouse V1 response prediction using a Vision Transformer</title><link>http://arxiv.org/abs/2302.03023v3</link><description>Accurate predictive models of the visual cortex neural response to naturalvisual stimuli remain a challenge in computational neuroscience. In this work,we introduce V1T, a novel Vision Transformer based architecture that learns ashared visual and behavioral representation across animals. We evaluate ourmodel on two large datasets recorded from mouse primary visual cortex andoutperform previous convolution-based models by more than 12.7% in predictionperformance. Moreover, we show that the self-attention weights learned by theTransformer correlate with the population receptive fields. Our model thus setsa new benchmark for neural response prediction and can be used jointly withbehavioral and neural recordings to reveal meaningful characteristic featuresof the visual cortex.</description><author>Bryan M. Li, Isabel M. Cornacchia, Nathalie L. Rochefort, Arno Onken</author><pubDate>Tue, 30 May 2023 16:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03023v3</guid></item><item><title>FERN: Leveraging Graph Attention Networks for Failure Evaluation and Robust Network Design</title><link>http://arxiv.org/abs/2305.19153v1</link><description>Robust network design, which aims to guarantee network availability undervarious failure scenarios while optimizing performance/cost objectives, hasreceived significant attention. Existing approaches often rely on model-basedmixed-integer optimization that is hard to scale or employ deep learning tosolve specific engineering problems yet with limited generalizability. In thispaper, we show that failure evaluation provides a common kernel to improve thetractability and scalability of existing solutions. By providing a neuralnetwork function approximation of this common kernel using graph attentionnetworks, we develop a unified learning-based framework, FERN, for scalableFailure Evaluation and Robust Network design. FERN represents rich probleminputs as a graph and captures both local and global views by attentivelyperforming feature extraction from the graph. It enables a broad range ofrobust network design problems, including robust network validation, networkupgrade optimization, and fault-tolerant traffic engineering that are discussedin this paper, to be recasted with respect to the common kernel and thuscomputed efficiently using neural networks and over a small set of criticalfailure scenarios. Extensive experiments on real-world network topologies showthat FERN can efficiently and accurately identify key failure scenarios forboth OSPF and optimal routing scheme, and generalizes well to differenttopologies and input traffic patterns. It can speed up multiple robust networkdesign problems by more than 80x, 200x, 10x, respectively with negligibleperformance gap.</description><author>Chenyi Liu, Vaneet Aggarwal, Tian Lan, Nan Geng, Yuan Yang, Mingwei Xu, Qing Li</author><pubDate>Tue, 30 May 2023 16:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19153v1</guid></item><item><title>BLEU Meets COMET: Combining Lexical and Neural Metrics Towards Robust Machine Translation Evaluation</title><link>http://arxiv.org/abs/2305.19144v1</link><description>Although neural-based machine translation evaluation metrics, such as COMETor BLEURT, have achieved strong correlations with human judgements, they aresometimes unreliable in detecting certain phenomena that can be considered ascritical errors, such as deviations in entities and numbers. In contrast,traditional evaluation metrics, such as BLEU or chrF, which measure lexical orcharacter overlap between translation hypotheses and human references, havelower correlations with human judgements but are sensitive to such deviations.In this paper, we investigate several ways of combining the two approaches inorder to increase robustness of state-of-the-art evaluation methods totranslations with critical errors. We show that by using additional informationduring training, such as sentence-level features and word-level tags, thetrained metrics improve their capability to penalize translations with specifictroublesome phenomena, which leads to gains in correlation with human judgmentsand on recent challenge sets on several language pairs.</description><author>Taisiya Glushkova, Chrysoula Zerva, André F. T. Martins</author><pubDate>Tue, 30 May 2023 16:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19144v1</guid></item><item><title>A Tale of Two Laws of Semantic Change: Predicting Synonym Changes with Distributional Semantic Models</title><link>http://arxiv.org/abs/2305.19143v1</link><description>Lexical Semantic Change is the study of how the meaning of words evolvesthrough time. Another related question is whether and how lexical relationsover pairs of words, such as synonymy, change over time. There are currentlytwo competing, apparently opposite hypotheses in the historical linguisticliterature regarding how synonymous words evolve: the Law of Differentiation(LD) argues that synonyms tend to take on different meanings over time, whereasthe Law of Parallel Change (LPC) claims that synonyms tend to undergo the samesemantic change and therefore remain synonyms. So far, there has been littleresearch using distributional models to assess to what extent these laws applyon historical corpora. In this work, we take a first step toward detectingwhether LD or LPC operates for given word pairs. After recasting the probleminto a more tractable task, we combine two linguistic resources to propose thefirst complete evaluation framework on this problem and provide empiricalevidence in favor of a dominance of LD. We then propose various computationalapproaches to the problem using Distributional Semantic Models and grounded inrecent literature on Lexical Semantic Change detection. Our best approachesachieve a balanced accuracy above 0.6 on our dataset. We discuss challengesstill faced by these approaches, such as polysemy or the potential confusionbetween synonymy and hypernymy.</description><author>Bastien Liétard, Mikaela Keller, Pascal Denis</author><pubDate>Tue, 30 May 2023 16:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19143v1</guid></item><item><title>Taylorformer: Probabilistic Predictions for Time Series and other Processes</title><link>http://arxiv.org/abs/2305.19141v1</link><description>We propose the Taylorformer for time series and other random processes. Itstwo key components are: 1) the LocalTaylor wrapper to learn how and when to useTaylor series-based approximations for predictions, and 2) the MHA-X attentionblock which makes predictions in a way inspired by how Gaussian Processes' meanpredictions are linear smoothings of contextual data. Taylorformer outperformsthe state-of-the-art on several forecasting datasets, including electricity,oil temperatures and exchange rates with at least 14% improvement in MSE on alltasks, and better likelihood on 5/6 classic Neural Process tasks such asmeta-learning 1D functions. Taylorformer combines desirable features from theNeural Process (uncertainty-aware predictions and consistency) and forecasting(predictive accuracy) literature, two previously distinct bodies.</description><author>Omer Nivron, Raghul Parthipan, Damon J. Wischik</author><pubDate>Tue, 30 May 2023 16:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19141v1</guid></item><item><title>Learning Robust Kernel Ensembles with Kernel Average Pooling</title><link>http://arxiv.org/abs/2210.00062v2</link><description>Model ensembles have long been used in machine learning to reduce thevariance in individual model predictions, making them more robust to inputperturbations. Pseudo-ensemble methods like dropout have also been commonlyused in deep learning models to improve generalization. However, theapplication of these techniques to improve neural networks' robustness againstinput perturbations remains underexplored. We introduce Kernel Average Pooling(KAP), a neural network building block that applies the mean filter along thekernel dimension of the layer activation tensor. We show that ensembles ofkernels with similar functionality naturally emerge in convolutional neuralnetworks equipped with KAP and trained with backpropagation. Moreover, we showthat when trained on inputs perturbed with additive Gaussian noise, KAP modelsare remarkably robust against various forms of adversarial attacks. Empiricalevaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets showsubstantial improvements in robustness against strong adversarial attacks suchas AutoAttack without training on any adversarial examples.</description><author>Pouya Bashivan, Adam Ibrahim, Amirozhan Dehghani, Yifei Ren</author><pubDate>Tue, 30 May 2023 16:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00062v2</guid></item><item><title>Context-Preserving Two-Stage Video Domain Translation for Portrait Stylization</title><link>http://arxiv.org/abs/2305.19135v1</link><description>Portrait stylization, which translates a real human face image into anartistically stylized image, has attracted considerable interest and many priorworks have shown impressive quality in recent years. However, despite theirremarkable performances in the image-level translation tasks, prior methodsshow unsatisfactory results when they are applied to the video domain. Toaddress the issue, we propose a novel two-stage video translation frameworkwith an objective function which enforces a model to generate a temporallycoherent stylized video while preserving context in the source video.Furthermore, our model runs in real-time with the latency of 0.011 seconds perframe and requires only 5.6M parameters, and thus is widely applicable topractical real-world applications.</description><author>Doyeon Kim, Eunji Ko, Hyunsu Kim, Yunji Kim, Junho Kim, Dongchan Min, Junmo Kim, Sung Ju Hwang</author><pubDate>Tue, 30 May 2023 16:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19135v1</guid></item><item><title>Runtime Analyses of Multi-Objective Evolutionary Algorithms in the Presence of Noise</title><link>http://arxiv.org/abs/2305.10259v3</link><description>In single-objective optimization, it is well known that evolutionaryalgorithms also without further adjustments can tolerate a certain amount ofnoise in the evaluation of the objective function. In contrast, this questionis not at all understood for multi-objective optimization. In this work, we conduct the first mathematical runtime analysis of a simplemulti-objective evolutionary algorithm (MOEA) on a classic benchmark in thepresence of noise in the objective functions. We prove that when bit-wise priornoise with rate $p \le \alpha/n$, $\alpha$ a suitable constant, is present, the\emph{simple evolutionary multi-objective optimizer} (SEMO) without anyadjustments to cope with noise finds the Pareto front of the OneMinMaxbenchmark in time $O(n^2\log n)$, just as in the case without noise. Given thatthe problem here is to arrive at a population consisting of $n+1$ individualswitnessing the Pareto front, this is a surprisingly strong robustness to noise(comparably simple evolutionary algorithms cannot optimize the single-objectiveOneMax problem in polynomial time when $p = \omega(\log(n)/n)$). Our proofssuggest that the strong robustness of the MOEA stems from its implicitdiversity mechanism designed to enable it to compute a population covering thewhole Pareto front. Interestingly this result only holds when the objective value of a solutionis determined only once and the algorithm from that point on works with this,possibly noisy, objective value. We prove that when all solutions arereevaluated in each iteration, then any noise rate $p = \omega(\log(n)/n^2)$leads to a super-polynomial runtime. This is very different fromsingle-objective optimization, where it is generally preferred to reevaluatesolutions whenever their fitness is important and where examples are known suchthat not reevaluating solutions can lead to catastrophic performance losses.</description><author>Matthieu Dinot, Benjamin Doerr, Ulysse Hennebelle, Sebastian Will</author><pubDate>Tue, 30 May 2023 16:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10259v3</guid></item><item><title>Learning from Children: Improving Image-Caption Pretraining via Curriculum</title><link>http://arxiv.org/abs/2305.17540v2</link><description>Image-caption pretraining has been quite successfully used for downstreamvision tasks like zero-shot image classification and object detection. However,image-caption pretraining is still a hard problem -- it requires multipleconcepts (nouns) from captions to be aligned to several objects in images. Totackle this problem, we go to the roots -- the best learner, children. We takeinspiration from cognitive science studies dealing with children's languagelearning to propose a curriculum learning framework. The learning begins witheasy-to-align image caption pairs containing one concept per caption. Thedifficulty is progressively increased with each new phase by adding one moreconcept per caption. Correspondingly, the knowledge acquired in each learningphase is utilized in subsequent phases to effectively constrain the learningproblem to aligning one new concept-object pair in each phase. We show thatthis learning strategy improves over vanilla image-caption training in varioussettings -- pretraining from scratch, using a pretrained image or/andpretrained text encoder, low data regime etc.</description><author>Hammad A. Ayyubi, Rahul Lokesh, Alireza Zareian, Bo Wu, Shih-Fu Chang</author><pubDate>Tue, 30 May 2023 16:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17540v2</guid></item><item><title>Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks</title><link>http://arxiv.org/abs/2305.19130v1</link><description>Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)are now able to synthesize intelligible speech from articulatory movement dataunder certain conditions. However, the resulting models are ratherspeaker-specific, making a quick switch between users troublesome. Even for thesame speaker, these models perform poorly cross-session, i.e. after dismountingand re-mounting the recording equipment. To aid quick speaker and sessionadaptation of ultrasound tongue imaging-based SSI models, we extend our deepnetworks with a spatial transformer network (STN) module, capable of performingan affine transformation on the input images. Although the STN part takes uponly about 10\% of the network, our experiments show that adapting just the STNmodule might allow to reduce MSE by 88\% on the average, compared to retrainingthe whole network. The improvement is even larger (around 92\%) when adaptingthe network to different recording sessions from the same speaker.</description><author>László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Csapó Tamás Gábor</author><pubDate>Tue, 30 May 2023 16:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19130v1</guid></item><item><title>Men Also Do Laundry: Multi-Attribute Bias Amplification</title><link>http://arxiv.org/abs/2210.11924v3</link><description>As computer vision systems become more widely deployed, there is increasingconcern from both the research community and the public that these systems arenot only reproducing but amplifying harmful social biases. The phenomenon ofbias amplification, which is the focus of this work, refers to modelsamplifying inherent training set biases at test time. Existing metrics measurebias amplification with respect to single annotated attributes (e.g.,$\texttt{computer}$). However, several visual datasets consist of images withmultiple attribute annotations. We show models can learn to exploitcorrelations with respect to multiple attributes (e.g., {$\texttt{computer}$,$\texttt{keyboard}$}), which are not accounted for by current metrics. Inaddition, we show current metrics can give the erroneous impression thatminimal or no bias amplification has occurred as they involve aggregating overpositive and negative values. Further, these metrics lack a clear desiredvalue, making them difficult to interpret. To address these shortcomings, wepropose a new metric: Multi-Attribute Bias Amplification. We validate ourproposed metric through an analysis of gender bias amplification on the COCOand imSitu datasets. Finally, we benchmark bias mitigation methods using ourproposed metric, suggesting possible avenues for future bias mitigation</description><author>Dora Zhao, Jerone T. A. Andrews, Alice Xiang</author><pubDate>Tue, 30 May 2023 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11924v3</guid></item><item><title>Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model</title><link>http://arxiv.org/abs/2305.17116v2</link><description>Large language models (LLMs) have made significant advancements in naturallanguage processing (NLP). Broad corpora capture diverse patterns but canintroduce irrelevance, while focused corpora enhance reliability by reducingmisleading information. Training LLMs on focused corpora poses computationalchallenges. An alternative approach is to use a retrieval-augmentation (RetA)method tested in a specific domain. To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and acustom RetA model were compared using 19 questions on diffuse large B-celllymphoma (DLBCL) disease. Eight independent reviewers assessed responses basedon accuracy, relevance, and readability (rated 1-3). The RetA model performed best in accuracy (12/19 3-point scores, total=47)and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4received the highest readability scores (17/19, 55), followed by GPT-3 (15/19,53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34),relevance (32), and readability (38). Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses comparedto the RetA model and Prometheus. Hallucinations were mostly associated withnon-existent references or fabricated efficacy data. These findings suggest that RetA models, supplemented with domain-specificcorpora, may outperform general-purpose LLMs in accuracy and relevance withinspecific domains. However, this evaluation was limited to specific questionsand metrics and may not capture challenges in semantic search and other NLPtasks. Further research will explore different LLM architectures, RetAmethodologies, and evaluation methods to assess strengths and limitations morecomprehensively.</description><author>David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline Costa Sá, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh, Brandon W Higgs</author><pubDate>Tue, 30 May 2023 16:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17116v2</guid></item><item><title>Hierarchical Graph Generation with $K^2$-trees</title><link>http://arxiv.org/abs/2305.19125v1</link><description>Generating graphs from a target distribution is a significant challengeacross many domains, including drug discovery and social network analysis. Inthis work, we introduce a novel graph generation method leveraging $K^2$-treerepresentation which was originally designed for lossless graph compression.Our motivation stems from the ability of the $K^2$-trees to enable compactgeneration while concurrently capturing the inherent hierarchical structure ofa graph. In addition, we make further contributions by (1) presenting asequential $K^2$-tree representation that incorporates pruning, flattening, andtokenization processes and (2) introducing a Transformer-based architecturedesigned to generate the sequence by incorporating a specialized treepositional encoding scheme. Finally, we extensively evaluate our algorithm onfour general and two molecular graph datasets to confirm its superiority forgraph generation.</description><author>Yunhui Jang, Dongwoo Kim, Sungsoo Ahn</author><pubDate>Tue, 30 May 2023 16:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19125v1</guid></item><item><title>Tighter Bounds on the Expressivity of Transformer Encoders</title><link>http://arxiv.org/abs/2301.10743v2</link><description>Characterizing neural networks in terms of better-understood formal systemshas the potential to yield new insights into the power and limitations of thesenetworks. Doing so for transformers remains an active area of research.Bhattamishra and others have shown that transformer encoders are at least asexpressive as a certain kind of counter machine, while Merrill and Sabharwalhave shown that fixed-precision transformer encoders recognize only languagesin uniform $TC^0$. We connect and strengthen these results by identifying avariant of first-order logic with counting quantifiers that is simultaneouslyan upper bound for fixed-precision transformer encoders and a lower bound fortransformer encoders. This brings us much closer than before to an exactcharacterization of the languages that transformer encoders recognize.</description><author>David Chiang, Peter Cholak, Anand Pillay</author><pubDate>Tue, 30 May 2023 16:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10743v2</guid></item><item><title>Calliffusion: Chinese Calligraphy Generation and Style Transfer with Diffusion Modeling</title><link>http://arxiv.org/abs/2305.19124v1</link><description>In this paper, we propose Calliffusion, a system for generating high-qualityChinese calligraphy using diffusion models. Our model architecture is based onDDPM (Denoising Diffusion Probabilistic Models), and it is capable ofgenerating common characters in five different scripts and mimicking the stylesof famous calligraphers. Experiments demonstrate that our model can generatecalligraphy that is difficult to distinguish from real artworks and that ourcontrols for characters, scripts, and styles are effective. Moreover, wedemonstrate one-shot transfer learning, using LoRA (Low-Rank Adaptation) totransfer Chinese calligraphy art styles to unseen characters and evenout-of-domain symbols such as English letters and digits.</description><author>Qisheng Liao, Gus Xia, Zhinuo Wang</author><pubDate>Tue, 30 May 2023 16:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19124v1</guid></item><item><title>Zero-Shot Batch-Level Anomaly Detection</title><link>http://arxiv.org/abs/2302.07849v3</link><description>Anomaly detection (AD) plays a crucial role in many safety-criticalapplication domains. The challenge of adapting an anomaly detector to drift inthe normal data distribution, especially when no training data is available forthe "new normal", has led to the development of zero-shot AD techniques. Inthis paper, we propose a simple yet effective method called Adaptive CenteredRepresentations (ACR) for zero-shot batch-level AD. Our approach trainsoff-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set ofinter-related training data distributions in combination with batchnormalization, enabling automatic zero-shot generalization for unseen AD tasks.This simple recipe, batch normalization plus meta-training, is a highlyeffective and versatile tool. Our results demonstrate the first zero-shot ADresults for tabular data and outperform existing methods in zero-shot anomalydetection and segmentation on image data from specialized domains.</description><author>Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt</author><pubDate>Tue, 30 May 2023 16:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07849v3</guid></item><item><title>ELSA: Efficient Label Shift Adaptation through the Lens of Semiparametric Models</title><link>http://arxiv.org/abs/2305.19123v1</link><description>We study the domain adaptation problem with label shift in this work. Underthe label shift context, the marginal distribution of the label varies acrossthe training and testing datasets, while the conditional distribution offeatures given the label is the same. Traditional label shift adaptationmethods either suffer from large estimation errors or require cumbersomepost-prediction calibrations. To address these issues, we first propose amoment-matching framework for adapting the label shift based on the geometry ofthe influence function. Under such a framework, we propose a novel method named\underline{E}fficient \underline{L}abel \underline{S}hift\underline{A}daptation (ELSA), in which the adaptation weights can be estimatedby solving linear systems. Theoretically, the ELSA estimator is$\sqrt{n}$-consistent ($n$ is the sample size of the source data) andasymptotically normal. Empirically, we show that ELSA can achievestate-of-the-art estimation performances without post-prediction calibrations,thus, gaining computational efficiency.</description><author>Qinglong Tian, Xin Zhang, Jiwei Zhao</author><pubDate>Tue, 30 May 2023 16:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19123v1</guid></item><item><title>Comparing and combining some popular NER approaches on Biomedical tasks</title><link>http://arxiv.org/abs/2305.19120v1</link><description>We compare three simple and popular approaches for NER: 1) SEQ(sequence-labeling with a linear token classifier) 2) SeqCRF (sequence-labelingwith Conditional Random Fields), and 3) SpanPred (span-prediction with boundarytoken embeddings). We compare the approaches on 4 biomedical NER tasks: GENIA,NCBI-Disease, LivingNER (Spanish), and SocialDisNER (Spanish). The SpanPredmodel demonstrates state-of-the-art performance on LivingNER and SocialDisNER,improving F1 by 1.3 and 0.6 F1 respectively. The SeqCRF model also demonstratesstate-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 0.2F1 and 0.7 respectively. The SEQ model is competitive with the state-of-the-arton the LivingNER dataset. We explore some simple ways of combining the threeapproaches. We find that majority voting consistently gives high precision andhigh F1 across all 4 datasets. Lastly, we implement a system that learns tocombine the predictions of SEQ and SpanPred, generating systems thatconsistently give high recall and high F1 across all 4 datasets. On the GENIAdataset, we find that our learned combiner system significantly boosts F1(+1.2)and recall(+2.1) over the systems being combined. We release all thewell-documented code necessary to reproduce all systems athttps://github.com/flyingmothman/bionlp.</description><author>Harsh Verma, Sabine Bergler, Narjesossadat Tahaei</author><pubDate>Tue, 30 May 2023 16:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19120v1</guid></item><item><title>Trade-off Between Efficiency and Consistency for Removal-based Explanations</title><link>http://arxiv.org/abs/2210.17426v2</link><description>In the current landscape of explanation methodologies, most predominantapproaches, such as SHAP and LIME, employ removal-based techniques to evaluatethe impact of individual features by simulating various scenarios with specificfeatures omitted. Nonetheless, these methods primarily emphasize efficiency inthe original context, often resulting in general inconsistencies. In thispaper, we demonstrate that such inconsistency is an inherent aspect of theseapproaches by establishing the Impossible Trinity Theorem, which posits thatinterpretability, efficiency and consistency cannot hold simultaneously.Recognizing that the attainment of an ideal explanation remains elusive, wepropose the utilization of interpretation error as a metric to gaugeinconsistencies and inefficiencies. To this end, we present two novelalgorithms founded on the standard polynomial basis, aimed at minimizinginterpretation error. Our empirical findings indicate that the proposed methodsachieve a substantial reduction in interpretation error, up to 31.8 times lowerwhen compared to alternative techniques.</description><author>Yifan Zhang, Haowei He, Zhiquan Tan, Yang Yuan</author><pubDate>Tue, 30 May 2023 16:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17426v2</guid></item><item><title>Learning Instance-Specific Augmentations by Capturing Local Invariances</title><link>http://arxiv.org/abs/2206.00051v3</link><description>We introduce InstaAug, a method for automatically learning input-specificaugmentations from data. Previous methods for learning augmentations havetypically assumed independence between the original input and thetransformation applied to that input. This can be highly restrictive, as theinvariances we hope our augmentation will capture are themselves often highlyinput dependent. InstaAug instead introduces a learnable invariance module thatmaps from inputs to tailored transformation parameters, allowing localinvariances to be captured. This can be simultaneously trained alongside thedownstream model in a fully end-to-end manner, or separately learned for apre-trained model. We empirically demonstrate that InstaAug learns meaningfulinput-dependent augmentations for a wide range of transformation classes, whichin turn provides better performance on both supervised and self-supervisedtasks.</description><author>Ning Miao, Tom Rainforth, Emile Mathieu, Yann Dubois, Yee Whye Teh, Adam Foster, Hyunjik Kim</author><pubDate>Tue, 30 May 2023 16:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00051v3</guid></item><item><title>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</title><link>http://arxiv.org/abs/2305.19118v1</link><description>Modern large language models (LLMs) like ChatGPT have shown remarkableperformance on general language tasks but still struggle on complex reasoningtasks, which drives the research on cognitive behaviors of LLMs to explorehuman-like problem-solving strategies. Along this direction, one representativestrategy is self-reflection, which asks an LLM to refine the solution with thefeedback generated by itself iteratively. However, our study shows that suchreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:once the LLM has established confidence in its solutions, it is unable togenerate novel thoughts later through reflection even if its initial stance isincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)framework, in which multiple agents express their arguments in the state of"tit for tat" and a judge manages the debate process to obtain a finalsolution. Clearly, our MAD framework encourages divergent thinking in LLMswhich would be helpful for tasks that require deep levels of contemplation.Experiment results on two challenging datasets, commonsense machine translationand counter-intuitive arithmetic reasoning, demonstrate the effectiveness ofour MAD framework. Extensive analyses suggest that the adaptive break of debateand the modest level of "tit for tat" state are required for MAD to obtain goodperformance. Moreover, we find that LLMs might not be a fair judge if differentLLMs are used for agents. Codes:https://github.com/Skytliang/Multi-Agents-Debate</description><author>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi</author><pubDate>Tue, 30 May 2023 16:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19118v1</guid></item><item><title>Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information</title><link>http://arxiv.org/abs/2305.16967v2</link><description>The long-standing one-to-many issue of the open-domain dialogues posessignificant challenges for automatic evaluation methods, i.e., there may bemultiple suitable responses which differ in semantics for a givenconversational context. To tackle this challenge, we propose a novellearning-based automatic evaluation metric (CMN), which can robustly evaluateopen-domain dialogues by augmenting Conditional Variational Autoencoders(CVAEs) with a Next Sentence Prediction (NSP) objective and employing MutualInformation (MI) to model the semantic similarity of text in the latent space.Experimental results on two open-domain dialogue datasets demonstrate thesuperiority of our method compared with a wide range of baselines, especiallyin handling responses which are distant to the golden reference responses insemantics.</description><author>Kun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio, Xiaohui Cui</author><pubDate>Tue, 30 May 2023 16:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16967v2</guid></item><item><title>One Embedder, Any Task: Instruction-Finetuned Text Embeddings</title><link>http://arxiv.org/abs/2212.09741v3</link><description>We introduce INSTRUCTOR, a new method for computing text embeddings giventask instructions: every text input is embedded together with instructionsexplaining the use case (e.g., task and domain descriptions). Unlike encodersfrom prior work that are more specialized, INSTRUCTOR is a single embedder thatcan generate text embeddings tailored to different downstream tasks anddomains, without any further training. We first annotate instructions for 330diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastiveloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which areunseen during training), ranging from classification and information retrievalto semantic textual similarity and text generation evaluation. INSTRUCTOR,while having an order of magnitude fewer parameters than the previous bestmodel, achieves state-of-the-art performance, with an average improvement of3.4% compared to the previous best results on the 70 diverse datasets. Ouranalysis suggests that INSTRUCTOR is robust to changes in instructions, andthat instruction finetuning mitigates the challenge of training a single modelon diverse datasets. Our model, code, and data are available athttps://instructor-embedding.github.io.</description><author>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu</author><pubDate>Tue, 30 May 2023 16:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09741v3</guid></item><item><title>Multi-Source Diffusion Models for Simultaneous Music Generation and Separation</title><link>http://arxiv.org/abs/2302.02257v3</link><description>In this work, we define a diffusion-based generative model capable of bothmusic synthesis and source separation by learning the score of the jointprobability density of sources sharing a context. Alongside the classic totalinference tasks (i.e., generating a mixture, separating the sources), we alsointroduce and experiment on the partial generation task of source imputation,where we generate a subset of the sources given the others (e.g., play a pianotrack that goes well with the drums). Additionally, we introduce a novelinference method for the separation task based on Dirac likelihood functions.We train our model on Slakh2100, a standard dataset for musical sourceseparation, provide qualitative results in the generation settings, andshowcase competitive quantitative results in the source separation setting. Ourmethod is the first example of a single model that can handle both generationand separation tasks, thus representing a step toward general audio models.</description><author>Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà</author><pubDate>Tue, 30 May 2023 16:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02257v3</guid></item></channel></rss>