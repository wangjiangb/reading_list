<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 18 Jul 2023 06:00:28 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Diffusion Models Beat GANs on Image Classification</title><link>http://arxiv.org/abs/2307.08702v1</link><description>While many unsupervised learning models focus on one family of tasks, eithergenerative or discriminative, we explore the possibility of a unifiedrepresentation learner: a model which uses a single pre-training stage toaddress both families of tasks simultaneously. We identify diffusion models asa prime candidate. Diffusion models have risen to prominence as astate-of-the-art method for image generation, denoising, inpainting,super-resolution, manipulation, etc. Such models involve training a U-Net toiteratively predict and remove noise, and the resulting model can synthesizehigh fidelity, diverse, novel images. The U-Net architecture, as aconvolution-based architecture, generates a diverse set of featurerepresentations in the form of intermediate feature maps. We present ourfindings that these embeddings are useful beyond the noise prediction task, asthey contain discriminative information and can also be leveraged forclassification. We explore optimal methods for extracting and using theseembeddings for classification tasks, demonstrating promising results on theImageNet classification task. We find that with careful feature selection andpooling, diffusion models outperform comparable generative-discriminativemethods such as BigBiGAN for classification tasks. We investigate diffusionmodels in the transfer learning regime, examining their performance on severalfine-grained visual classification datasets. We compare these embeddings tothose generated by competing architectures and pre-trainings for classificationtasks.</description><author>Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, Abhinav Shrivastava</author><pubDate>Mon, 17 Jul 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08702v1</guid></item><item><title>AlpaGasus: Training A Better Alpaca with Fewer Data</title><link>http://arxiv.org/abs/2307.08701v1</link><description>Large language models~(LLMs) obtain instruction-following capability throughinstruction-finetuning (IFT) on supervised instruction/response data. However,widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain manylow-quality instances with incorrect or irrelevant responses, which aremisleading and detrimental to IFT. In this paper, we propose a simple andeffective data selection strategy that automatically identifies and removeslow-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduceAlpaGasus, which is finetuned on only 9k high-quality data filtered from the52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca asevaluated by GPT-4 on multiple test sets and its 13B variant matches $&gt;90\%$performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It alsoprovides 5.7x faster training, reducing the training time for a 7B variant from80 minutes (for Alpaca) to 14 minutes \footnote{We apply IFT for the samenumber of epochs as Alpaca(7B) but on fewer data, using 4$\times$NVIDIA A100(80GB) GPUs and following the original Alpaca setting and hyperparameters.}.Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can begenerally applied to instruction-tuning data, leading to faster training andbetter instruction-following models. Our project page is available at:\url{https://lichang-chen.github.io/AlpaGasus/}.</description><author>Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin</author><pubDate>Mon, 17 Jul 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08701v1</guid></item><item><title>HumanMAC: Masked Motion Completion for Human Motion Prediction</title><link>http://arxiv.org/abs/2302.03665v3</link><description>Human motion prediction is a classical problem in computer vision andcomputer graphics, which has a wide range of practical applications. Previouseffects achieve great empirical performance based on an encoding-decodingstyle. The methods of this style work by first encoding previous motions tolatent representations and then decoding the latent representations intopredicted motions. However, in practice, they are still unsatisfactory due toseveral issues, including complicated loss constraints, cumbersome trainingprocesses, and scarce switch of different categories of motions in prediction.In this paper, to address the above issues, we jump out of the foregoing styleand propose a novel framework from a new perspective. Specifically, ourframework works in a masked completion fashion. In the training stage, we learna motion diffusion model that generates motions from random noise. In theinference stage, with a denoising procedure, we make motion predictionconditioning on observed motions to output more continuous and controllablepredictions. The proposed framework enjoys promising algorithmic properties,which only needs one loss in optimization and is trained in an end-to-endmanner. Additionally, it accomplishes the switch of different categories ofmotions effectively, which is significant in realistic tasks, e.g., theanimation task. Comprehensive experiments on benchmarks confirm the superiorityof the proposed framework. The project page is available athttps://lhchen.top/Human-MAC.</description><author>Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, Tongliang Liu</author><pubDate>Mon, 17 Jul 2023 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03665v3</guid></item><item><title>Fast model inference and training on-board of Satellites</title><link>http://arxiv.org/abs/2307.08700v1</link><description>Artificial intelligence onboard satellites has the potential to reduce datatransmission requirements, enable real-time decision-making and collaborationwithin constellations. This study deploys a lightweight foundational modelcalled RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variationalauto-encoder (VAE) that generates compressed latent vectors from small imagetiles, enabling several downstream tasks. In this work we demonstrate thereliable use of RaVAEn onboard a satellite, achieving an encoding time of0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fastfew-shot training onboard a satellite using the latent representation of data.We compare the deployment of the model on the on-board CPU and on the availableMyriad vision processing unit (VPU) accelerator. To our knowledge, this workshows for the first time the deployment of a multi-task model on-board aCubeSat and the on-board training of a machine learning model.</description><author>Vít Růžička, Gonzalo Mateo-García, Chris Bridges, Chris Brunskill, Cormac Purcell, Nicolas Longépé, Andrew Markham</author><pubDate>Mon, 17 Jul 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08700v1</guid></item><item><title>Pair then Relation: Pair-Net for Panoptic Scene Graph Generation</title><link>http://arxiv.org/abs/2307.08699v1</link><description>Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation(SGG) that aims to create a more comprehensive scene graph representation usingpanoptic segmentation instead of boxes. However, current PSG methods havelimited performance, which can hinder downstream task development. To improvePSG methods, we conducted an in-depth analysis to identify the bottleneck ofthe current PSG models, finding that inter-object pair-wise recall is a crucialfactor which was ignored by previous PSG methods. Based on this, we present anovel framework: Pair then Relation (Pair-Net), which uses a Pair ProposalNetwork (PPN) to learn and filter sparse pair-wise relationships betweensubjects and objects. We also observed the sparse nature of object pairs andused this insight to design a lightweight Matrix Learner within the PPN.Through extensive ablation and analysis, our approach significantly improvesupon leveraging the strong segmenter baseline. Notably, our approach achievesnew state-of-the-art results on the PSG benchmark, with over 10% absolute gainscompared to PSGFormer. The code of this paper is publicly available athttps://github.com/king159/Pair-Net.</description><author>Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, Ziwei Liu</author><pubDate>Mon, 17 Jul 2023 18:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08699v1</guid></item><item><title>Flow Matching in Latent Space</title><link>http://arxiv.org/abs/2307.08698v1</link><description>Flow matching is a recent framework to train generative models that exhibitsimpressive empirical performance while being relatively easier to traincompared with diffusion-based models. Despite its advantageous properties,prior methods still face the challenges of expensive computing and a largenumber of function evaluations of off-the-shelf solvers in the pixel space.Furthermore, although latent-based generative methods have shown great successin recent years, this particular model type remains underexplored in this area.In this work, we propose to apply flow matching in the latent spaces ofpretrained autoencoders, which offers improved computational efficiency andscalability for high-resolution image synthesis. This enables flow-matchingtraining on constrained computational resources while maintaining their qualityand flexibility. Additionally, our work stands as a pioneering contribution inthe integration of various conditions into flow matching for conditionalgeneration tasks, including label-conditioned image generation, imageinpainting, and semantic-to-image generation. Through extensive experiments,our approach demonstrates its effectiveness in both quantitative andqualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church &amp;Bedroom, and ImageNet. We also provide a theoretical control of theWasserstein-2 distance between the reconstructed latent flow distribution andtrue data distribution, showing it is upper-bounded by the latent flow matchingobjective. Our code will be available athttps://github.com/VinAIResearch/LFM.git.</description><author>Quan Dao, Hao Phung, Binh Nguyen, Anh Tran</author><pubDate>Mon, 17 Jul 2023 18:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08698v1</guid></item><item><title>Neural Video Depth Stabilizer</title><link>http://arxiv.org/abs/2307.08695v1</link><description>Video depth estimation aims to infer temporally consistent depth. Somemethods achieve temporal consistency by finetuning a single-image depth modelduring test time using geometry and re-projection constraints, which isinefficient and not robust. An alternative approach is to learn how to enforcetemporal consistency from data, but this requires well-designed models andsufficient video depth data. To address these challenges, we propose aplug-and-play framework called Neural Video Depth Stabilizer (NVDS) thatstabilizes inconsistent depth estimations and can be applied to differentsingle-image depth models without extra effort. We also introduce a large-scaledataset, Video Depth in the Wild (VDW), which consists of 14,203 videos withover two million frames, making it the largest natural-scene video depthdataset to our knowledge. We evaluate our method on the VDW dataset as well astwo public benchmarks and demonstrate significant improvements in consistency,accuracy, and efficiency compared to previous approaches. Our work serves as asolid baseline and provides a data foundation for learning-based video depthmodels. We will release our dataset and code for future research.</description><author>Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin</author><pubDate>Mon, 17 Jul 2023 18:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08695v1</guid></item><item><title>Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution</title><link>http://arxiv.org/abs/2210.00131v3</link><description>Modern language modeling tasks are often underspecified: for a given tokenprediction, many words may satisfy the user's intent of producing naturallanguage at inference time, however only one word would minimize the task'sloss function at training time. We provide a simple yet plausible causalmechanism describing the role underspecification plays in the generation ofspurious correlations. Despite its simplicity, our causal model directlyinforms the development of two lightweight black-box evaluation methods, thatwe apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aidin the detection of inference-time task underspecification by exploiting 2)previously unreported gender vs. time and gender vs. location spuriouscorrelations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B)pre-training objectives: from masked &amp; autoregressive language modeling to amixture of these objectives, and C) training stages: from pre-training only toreinforcement learning from human feedback (RLHF). Code and open-source demosavailable at https: //github.com/2dot71mily/sib_paper.</description><author>Emily McMilin</author><pubDate>Mon, 17 Jul 2023 18:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00131v3</guid></item><item><title>SEMI-DiffusionInst: A Diffusion Model Based Approach for Semiconductor Defect Classification and Segmentation</title><link>http://arxiv.org/abs/2307.08693v1</link><description>With continuous progression of Moore's Law, integrated circuit (IC) devicecomplexity is also increasing. Scanning Electron Microscope (SEM) image basedextensive defect inspection and accurate metrology extraction are two mainchallenges in advanced node (2 nm and beyond) technology. Deep learning (DL)algorithm based computer vision approaches gained popularity in semiconductordefect inspection over last few years. In this research work, a newsemiconductor defect inspection framework "SEMI-DiffusionInst" is investigatedand compared to previous frameworks. To the best of the authors' knowledge,this work is the first demonstration to accurately detect and precisely segmentsemiconductor defect patterns by using a diffusion model. Different featureextractor networks as backbones and data sampling strategies are investigatedtowards achieving a balanced trade-off between precision and computingefficiency. Our proposed approach outperforms previous work on overall mAP andperforms comparatively better or as per for almost all defect classes (perclass APs). The bounding box and segmentation mAPs achieved by the proposedSEMI-DiffusionInst model are improved by 3.83% and 2.10%,respectively. Amongindividual defect types, precision on line collapse and thin bridge defects areimproved approximately 15% on detection task for both defect types. It has alsobeen shown that by tuning inference hyperparameters, inference time can beimproved significantly without compromising model precision. Finally, certainlimitations and future work strategy to overcome them are discussed.</description><author>Vic De Ridder, Bappaditya Dey, Sandip Halder, Bartel Van Waeyenberge</author><pubDate>Mon, 17 Jul 2023 18:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08693v1</guid></item><item><title>A Multiobjective Reinforcement Learning Framework for Microgrid Energy Management</title><link>http://arxiv.org/abs/2307.08692v1</link><description>The emergence of microgrids (MGs) has provided a promising solution fordecarbonizing and decentralizing the power grid, mitigating the challengesposed by climate change. However, MG operations often involve consideringmultiple objectives that represent the interests of different stakeholders,leading to potentially complex conflicts. To tackle this issue, we propose anovel multi-objective reinforcement learning framework that explores thehigh-dimensional objective space and uncovers the tradeoffs between conflictingobjectives. This framework leverages exogenous information and capitalizes onthe data-driven nature of reinforcement learning, enabling the training of aparametric policy without the need for long-term forecasts or knowledge of theunderlying uncertainty distribution. The trained policies exhibit diverse,adaptive, and coordinative behaviors with the added benefit of providinginterpretable insights on the dynamics of their information use. We employ thisframework on the Cornell University MG (CU-MG), which is a combined heat andpower MG, to evaluate its effectiveness. The results demonstrate performanceimprovements in all objectives considered compared to the status quo operationsand offer more flexibility in navigating complex operational tradeoffs.</description><author>M. Vivienne Liu, Patrick M. Reed, David Gold, Garret Quist, C. Lindsay Anderson</author><pubDate>Mon, 17 Jul 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08692v1</guid></item><item><title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title><link>http://arxiv.org/abs/2307.08691v1</link><description>Scaling Transformers to longer sequence lengths has been a major problem inthe last several years, promising to improve performance in language modelingand high-resolution image understanding, as well as to unlock new applicationsin code, audio, and video generation. The attention layer is the mainbottleneck in scaling to longer sequences, as its runtime and memory increasequadratically in the sequence length. FlashAttention exploits the asymmetricGPU memory hierarchy to bring significant memory saving (linear instead ofquadratic) and runtime speedup (2-4$\times$ compared to optimized baselines),with no approximation. However, FlashAttention is still not nearly as fast asoptimized matrix-multiply (GEMM) operations, reaching only 25-40\% of thetheoretical maximum FLOPs/s. We observe that the inefficiency is due tosuboptimal work partitioning between different thread blocks and warps on theGPU, causing either low-occupancy or unnecessary shared memory reads/writes. Wepropose FlashAttention-2, with better work partitioning to address theseissues. In particular, we (1) tweak the algorithm to reduce the number ofnon-matmul FLOPs (2) parallelize the attention computation, even for a singlehead, across different thread blocks to increase occupancy, and (3) within eachthread block, distribute the work between warps to reduce communication throughshared memory. These yield around 2$\times$ speedup compared to FlashAttention,reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting closeto the efficiency of GEMM operations. We empirically validate that when usedend-to-end to train GPT-style models, FlashAttention-2 reaches training speedof up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).</description><author>Tri Dao</author><pubDate>Mon, 17 Jul 2023 18:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08691v1</guid></item><item><title>COLLIE: Systematic Construction of Constrained Text Generation Tasks</title><link>http://arxiv.org/abs/2307.08689v1</link><description>Text generation under constraints have seen increasing interests in naturallanguage processing, especially with the rapidly improving capabilities oflarge language models. However, existing benchmarks for constrained generationusually focus on fixed constraint types (e.g.,generate a sentence containingcertain words) that have proved to be easy for state-of-the-art models likeGPT-4. We present COLLIE, a grammar-based framework that allows thespecification of rich, compositional constraints with diverse generation levels(word, sentence, paragraph, passage) and modeling challenges (e.g.,languageunderstanding, logical reasoning, counting, semantic planning). We also developtools for automatic extraction of task instances given a constraint structureand a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080instances comprising 13 constraint structures. We perform systematicexperiments across five state-of-the-art instruction-tuned language models andanalyze their performances to reveal shortcomings. COLLIE is designed to beextensible and lightweight, and we hope the community finds it useful todevelop more complex constraints and evaluations in the future.</description><author>Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan</author><pubDate>Mon, 17 Jul 2023 18:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08689v1</guid></item><item><title>An R package for parametric estimation of causal effects</title><link>http://arxiv.org/abs/2307.08686v1</link><description>This article explains the usage of R package CausalModels, which is publiclyavailable on the Comprehensive R Archive Network. While packages are availablefor sufficiently estimating causal effects, there lacks a package that providesa collection of structural models using the conventional statistical approachdeveloped by Hern\'an and Robins (2020). CausalModels addresses this deficiencyof software in R concerning causal inference by offering tools for methods thataccount for biases in observational data without requiring extensivestatistical knowledge. These methods should not be ignored and may be moreappropriate or efficient in solving particular problems. While implementationsof these statistical models are distributed among a number of causal packages,CausalModels introduces a simple and accessible framework for a consistentmodeling pipeline among a variety of statistical methods for estimating causaleffects in a single R package. It consists of common methods includingstandardization, IP weighting, G-estimation, outcome regression, instrumentalvariables and propensity matching.</description><author>Joshua Wolff Anderson, Cyril Rakovsk</author><pubDate>Mon, 17 Jul 2023 18:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08686v1</guid></item><item><title>A Rubik's Cube inspired approach to Clifford synthesis</title><link>http://arxiv.org/abs/2307.08684v1</link><description>The problem of decomposing an arbitrary Clifford element into a sequence ofClifford gates is known as Clifford synthesis. Drawing inspiration fromsimilarities between this and the famous Rubik's Cube problem, we develop amachine learning approach for Clifford synthesis based on learning anapproximation to the distance to the identity. This approach is probabilisticand computationally intensive. However, when a decomposition is successfullyfound, it often involves fewer gates than existing synthesis algorithms.Additionally, our approach is much more flexible than existing algorithms inthat arbitrary gate sets, device topologies, and gate fidelities mayincorporated, thus allowing for the approach to be tailored to a specificdevice.</description><author>Ning Bao, Gavin S. Hartnett</author><pubDate>Mon, 17 Jul 2023 18:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08684v1</guid></item><item><title>Implementation of a perception system for autonomous vehicles using a detection-segmentation network in SoC FPGA</title><link>http://arxiv.org/abs/2307.08682v1</link><description>Perception and control systems for autonomous vehicles are an active area ofscientific and industrial research. These solutions should be characterised byhigh efficiency in recognising obstacles and other environmental elements indifferent road conditions, real-time capability, and energy efficiency.Achieving such functionality requires an appropriate algorithm and a suitablecomputing platform. In this paper, we have used the MultiTaskV3detection-segmentation network as the basis for a perception system that canperform both functionalities within a single architecture. It was appropriatelytrained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AIembedded platform. By using this device, it was possible to parallelise andaccelerate the computations. Furthermore, the whole system consumes relativelylittle power compared to a CPU-based implementation (an average of 5 watts,compared to the minimum of 55 watts for weaker CPUs, and the small size (119mmx 140mm x 36mm) of the platform allows it to be used in devices where theamount of space available is limited. It also achieves an accuracy higher than97% of the mAP (mean average precision) for object detection and above 90% ofthe mIoU (mean intersection over union) for image segmentation. The articlealso details the design of the Mecanum wheel vehicle, which was used to testthe proposed solution in a mock-up city.</description><author>Maciej Baczmanski, Mateusz Wasala, Tomasz Kryjak</author><pubDate>Mon, 17 Jul 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08682v1</guid></item><item><title>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</title><link>http://arxiv.org/abs/2307.08678v1</link><description>Large language models (LLMs) are trained to imitate humans to explain humandecisions. However, do LLMs explain themselves? Can they help humans buildmental models of how LLMs process different inputs? To answer these questions,we propose to evaluate $\textbf{counterfactual simulatability}$ of naturallanguage explanations: whether an explanation can enable humans to preciselyinfer the model's outputs on diverse counterfactuals of the explained input.For example, if a model answers "yes" to the input question "Can eagles fly?"with the explanation "all birds can fly", then humans would infer from theexplanation that it would also answer "yes" to the counterfactual input "Canpenguins fly?". If the explanation is precise, then the model's answer shouldmatch humans' expectations. We implemented two metrics based on counterfactual simulatability: precisionand generality. We generated diverse counterfactuals automatically using LLMs.We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) ontwo tasks: multi-hop factual reasoning and reward modeling. We found that LLM'sexplanations have low precision and that precision does not correlate withplausibility. Therefore, naively optimizing human approvals (e.g., RLHF) maynot be a sufficient solution.</description><author>Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen McKeown</author><pubDate>Mon, 17 Jul 2023 18:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08678v1</guid></item><item><title>Understanding Best Subset Selection: A Tale of Two C(omplex)ities</title><link>http://arxiv.org/abs/2301.06259v2</link><description>For decades, best subset selection (BSS) has eluded statisticians mainly dueto its computational bottleneck. However, until recently, modern computationalbreakthroughs have rekindled theoretical interest in BSS and have led to newfindings. Recently, \cite{guo2020best} showed that the model selectionperformance of BSS is governed by a margin quantity that is robust to thedesign dependence, unlike modern methods such as LASSO, SCAD, MCP, etc.Motivated by their theoretical results, in this paper, we also study thevariable selection properties of best subset selection for high-dimensionalsparse linear regression setup. We show that apart from the identifiabilitymargin, the following two complexity measures play a fundamental role incharacterizing the margin condition for model consistency: (a) complexity of\emph{residualized features}, (b) complexity of \emph{spurious projections}. Inparticular, we establish a simple margin condition that depends only on theidentifiability margin and the dominating one of the two complexity measures.Furthermore, we show that a margin condition depending on similar marginquantity and complexity measures is also necessary for model consistency ofBSS. For a broader understanding, we also consider some simple illustrativeexamples to demonstrate the variation in the complexity measures that refinesour theoretical understanding of the model selection performance of BSS underdifferent correlation structures.</description><author>Saptarshi Roy, Ambuj Tewari, Ziwei Zhu</author><pubDate>Mon, 17 Jul 2023 18:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06259v2</guid></item><item><title>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</title><link>http://arxiv.org/abs/2307.08674v1</link><description>Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework's adaptability tospecific use cases.</description><author>Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</author><pubDate>Mon, 17 Jul 2023 18:36:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08674v1</guid></item><item><title>CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models</title><link>http://arxiv.org/abs/2307.08673v1</link><description>Batch effects (BEs) refer to systematic technical differences in datacollection unrelated to biological variations whose noise is shown tonegatively impact machine learning (ML) model generalizability. Here we releaseCohortFinder, an open-source tool aimed at mitigating BEs via data-drivencohort partitioning. We demonstrate CohortFinder improves ML model performancein downstream medical image processing tasks. CohortFinder is freely availablefor download at cohortfinder.com.</description><author>Fan Fan, Georgia Martinez, Thomas Desilvio, John Shin, Yijiang Chen, Bangchen Wang, Takaya Ozeki, Maxime W. Lafarge, Viktor H. Koelzer, Laura Barisoni, Anant Madabhushi, Satish E. Viswanath, Andrew Janowczyk</author><pubDate>Mon, 17 Jul 2023 18:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08673v1</guid></item><item><title>Quaternion Convolutional Neural Networks: Current Advances and Future Directions</title><link>http://arxiv.org/abs/2307.08663v1</link><description>Since their first applications, Convolutional Neural Networks (CNNs) havesolved problems that have advanced the state-of-the-art in several domains.CNNs represent information using real numbers. Despite encouraging results,theoretical analysis shows that representations such as hyper-complex numberscan achieve richer representational capacities than real numbers, and thatHamilton products can capture intrinsic interchannel relationships. Moreover,in the last few years, experimental research has shown that Quaternion-ValuedCNNs (QCNNs) can achieve similar performance with fewer parameters than theirreal-valued counterparts. This paper condenses research in the development ofQCNNs from its very beginnings. We propose a conceptual organization of currenttrends and analyze the main building blocks used in the design of QCNN models.Based on this conceptual organization, we propose future directions ofresearch.</description><author>Gerardo Altamirano-Gomez, Carlos Gershenson</author><pubDate>Mon, 17 Jul 2023 18:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08663v1</guid></item><item><title>Automated scholarly paper review: Concepts, technologies, and challenges</title><link>http://arxiv.org/abs/2111.07533v4</link><description>Peer review is a widely accepted mechanism for research evaluation, playing apivotal role in academic publishing. However, criticisms have long been leveledat this mechanism, mostly because of its poor efficiency and lowreproducibility. Recent years have seen the application of artificialintelligence (AI) in assisting the peer review process. Nonetheless, with theinvolvement of humans, such limitations remain inevitable. In this paper, wepropose the concept and pipeline of automated scholarly paper review (ASPR) andreview the relevant literature and technologies of achieving a full-scalecomputerized review process. On the basis of the review and discussion, weconclude that there is already corresponding research and preliminaryimplementation at each stage of ASPR. We further look into the challenges inASPR with the existing technologies. The major difficulties lie in inadequatedata, imperfect document parsing and representation, defectivehuman$\unicode{x2013}$computer interaction, and flawed deep logical reasoning.Moreover, we point out the future directions and discuss the possible moral andethical issues of ASPR. In the foreseeable future, ASPR and peer review willcoexist in a reinforcing manner before ASPR is able to fully undertake thereviewing workload from humans.</description><author>Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi</author><pubDate>Mon, 17 Jul 2023 18:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.07533v4</guid></item><item><title>Neural Image Compression: Generalization, Robustness, and Spectral Biases</title><link>http://arxiv.org/abs/2307.08657v1</link><description>Recent neural image compression (NIC) advances have produced models which arestarting to outperform traditional codecs. While this has led to growingexcitement about using NIC in real-world applications, the successful adoptionof any machine learning system in the wild requires it to generalize (and berobust) to unseen distribution shifts at deployment. Unfortunately, currentresearch lacks comprehensive datasets and informative tools to evaluate andunderstand NIC performance in real-world settings. To bridge this crucial gap,first, this paper presents a comprehensive benchmark suite to evaluate theout-of-distribution (OOD) performance of image compression methods.Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions topopular CLIC and Kodak benchmarks. Next, we propose spectrally inspiredinspection tools to gain deeper insight into errors introduced by imagecompression methods as well as their OOD performance. We then carry out adetailed performance comparison of a classical codec with several NIC variants,revealing intriguing findings that challenge our current understanding of thestrengths and limitations of NIC. Finally, we corroborate our empiricalfindings with theoretical analysis, providing an in-depth view of the OODperformance of NIC and its dependence on the spectral properties of the data.Our benchmarks, spectral inspection tools, and findings provide a crucialbridge to the real-world adoption of NIC. We hope that our work will propelfuture efforts in designing robust and generalizable NIC methods. Code and datawill be made available at https://github.com/klieberman/ood_nic.</description><author>Kelsey Lieberman, James Diffenderfer, Charles Godfrey, Bhavya Kailkhura</author><pubDate>Mon, 17 Jul 2023 18:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08657v1</guid></item><item><title>Multi-Task Multi-Behavior MAP-Elites</title><link>http://arxiv.org/abs/2305.01264v2</link><description>We propose Multi-Task Multi-Behavior MAP-Elites, a variant of MAP-Elites thatfinds a large number of high-quality solutions for a large set of tasks(optimization problems from a given family). It combines the originalMAP-Elites for the search for diversity and Multi-Task MAP-Elites forleveraging similarity between tasks. It performs better than three baselines ona humanoid fault-recovery set of tasks, solving more tasks and finding twice asmany solutions per solved task.</description><author>Anne, Mouret</author><pubDate>Mon, 17 Jul 2023 18:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01264v2</guid></item><item><title>Multilingual Speech-to-Speech Translation into Multiple Target Languages</title><link>http://arxiv.org/abs/2307.08655v1</link><description>Speech-to-speech translation (S2ST) enables spoken communication betweenpeople talking in different languages. Despite a few studies on multilingualS2ST, their focus is the multilinguality on the source side, i.e., thetranslation from multiple source languages to one target language. We presentthe first work on multilingual S2ST supporting multiple target languages.Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, weequip these key components with multilingual capability. Speech-to-masked-unit(S2MU) is the multilingual extension of S2U, which applies masking to unitswhich don't belong to the given target language to reduce the languageinterference. We also propose multilingual vocoder which is trained withlanguage embedding and the auxiliary loss of language identification. Onbenchmark translation testsets, our proposed multilingual model shows superiorperformance than bilingual models in the translation from English into $16$target languages.</description><author>Hongyu Gong, Ning Dong, Sravya Popuri, Vedanuj Goswami, Ann Lee, Juan Pino</author><pubDate>Mon, 17 Jul 2023 18:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08655v1</guid></item><item><title>Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok</title><link>http://arxiv.org/abs/2306.13253v2</link><description>This paper focuses on predicting the occurrence of grokking in neuralnetworks, a phenomenon in which perfect generalization emerges long after signsof overfitting or memorization are observed. It has been reported that grokkingcan only be observed with certain hyper-parameters. This makes it critical toidentify the parameters that lead to grokking. However, since grokking occursafter a large number of epochs, searching for the hyper-parameters that lead toit is time-consuming. In this paper, we propose a low-cost method to predictgrokking without training for a large number of epochs. In essence, by studyingthe learning curve of the first few epochs, we show that one can predictwhether grokking will occur later on. Specifically, if certain oscillationsoccur in the early epochs, one can expect grokking to occur if the model istrained for a much longer period of time. We propose using the spectralsignature of a learning curve derived by applying the Fourier transform toquantify the amplitude of low-frequency components to detect the presence ofsuch oscillations. We also present additional experiments aimed at explainingthe cause of these oscillations and characterizing the loss landscape.</description><author>Pascal Jr. Tikeng Notsawo, Hattie Zhou, Mohammad Pezeshki, Irina Rish, Guillaume Dumas</author><pubDate>Mon, 17 Jul 2023 18:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13253v2</guid></item><item><title>A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond</title><link>http://arxiv.org/abs/2307.08643v1</link><description>Corruption is frequently observed in collected data and has been extensivelystudied in machine learning under different corruption models. Despite this,there remains a limited understanding of how these models relate such that aunified view of corruptions and their consequences on learning is stilllacking. In this work, we formally analyze corruption models at thedistribution level through a general, exhaustive framework based on Markovkernels. We highlight the existence of intricate joint and dependentcorruptions on both labels and attributes, which are rarely touched by existingresearch. Further, we show how these corruptions affect standard supervisedlearning by analyzing the resulting changes in Bayes Risk. Our findings offerqualitative insights into the consequences of "more complex" corruptions on thelearning problem, and provide a foundation for future quantitative comparisons.Applications of the framework include corruption-corrected learning, a subcaseof which we study in this paper by theoretically analyzing loss correction withrespect to different corruption instances.</description><author>Laura Iacovissi, Nan Lu, Robert C. Williamson</author><pubDate>Mon, 17 Jul 2023 17:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08643v1</guid></item><item><title>LearnedSort as a learning-augmented SampleSort: Analysis and Parallelization</title><link>http://arxiv.org/abs/2307.08637v1</link><description>This work analyzes and parallelizes LearnedSort, the novel algorithm thatsorts using machine learning models based on the cumulative distributionfunction. LearnedSort is analyzed under the lens of algorithms withpredictions, and it is argued that LearnedSort is a learning-augmentedSampleSort. A parallel LearnedSort algorithm is developed combining LearnedSortwith the state-of-the-art SampleSort implementation, IPS4o. Benchmarks onsynthetic and real-world datasets demonstrate improved parallel performance forparallel LearnedSort compared to IPS4o and other sorting algorithms.</description><author>Ivan Carvalho, Ramon Lawrence</author><pubDate>Mon, 17 Jul 2023 17:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08637v1</guid></item><item><title>PolyGNN: Polyhedron-based Graph Neural Network for 3D Building Reconstruction from Point Clouds</title><link>http://arxiv.org/abs/2307.08636v1</link><description>We present PolyGNN, a polyhedron-based graph neural network for 3D buildingreconstruction from point clouds. PolyGNN learns to assemble primitivesobtained by polyhedral decomposition via graph node classification, achieving awatertight, compact, and weakly semantic reconstruction. To effectivelyrepresent arbitrary-shaped polyhedra in the neural network, we propose threedifferent sampling strategies to select representative points aspolyhedron-wise queries, enabling efficient occupancy inference. Furthermore,we incorporate the inter-polyhedron adjacency to enhance the classification ofthe graph nodes. We also observe that existing city-building models areabstractions of the underlying instances. To address this abstraction gap andprovide a fair evaluation of the proposed method, we develop our method on alarge-scale synthetic dataset covering 500k+ buildings with well-defined groundtruths of polyhedral class labels. We further conduct a transferabilityanalysis across cities and on real-world point clouds. Both qualitative andquantitative results demonstrate the effectiveness of our method, particularlyits efficiency for large-scale reconstructions. The source code and data of ourwork are available at https://github.com/chenzhaiyu/polygnn.</description><author>Zhaiyu Chen, Yilei Shi, Liangliang Nan, Zhitong Xiong, Xiao Xiang Zhu</author><pubDate>Mon, 17 Jul 2023 17:52:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08636v1</guid></item><item><title>Deficiency-Aware Masked Transformer for Video Inpainting</title><link>http://arxiv.org/abs/2307.08629v1</link><description>Recent video inpainting methods have made remarkable progress by utilizingexplicit guidance, such as optical flow, to propagate cross-frame pixels.However, there are cases where cross-frame recurrence of the masked video isnot available, resulting in a deficiency. In such situation, instead ofborrowing pixels from other frames, the focus of the model shifts towardsaddressing the inverse problem. In this paper, we introduce adual-modality-compatible inpainting framework called Deficiency-aware MaskedTransformer (DMT), which offers three key advantages. Firstly, we pretrain aimage inpainting model DMT_img serve as a prior for distilling the video modelDMT_vid, thereby benefiting the hallucination of deficiency cases. Secondly,the self-attention module selectively incorporates spatiotemporal tokens toaccelerate inference and remove noise signals. Thirdly, a simple yet effectiveReceptive Field Contextualizer is integrated into DMT, further improvingperformance. Extensive experiments conducted on YouTube-VOS and DAVIS datasetsdemonstrate that DMT_vid significantly outperforms previous solutions. The codeand video demonstrations can be found at github.com/yeates/DMT.</description><author>Yongsheng Yu, Heng Fan, Libo Zhang</author><pubDate>Mon, 17 Jul 2023 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08629v1</guid></item><item><title>Retentive Network: A Successor to Transformer for Large Language Models</title><link>http://arxiv.org/abs/2307.08621v1</link><description>In this work, we propose Retentive Network (RetNet) as a foundationarchitecture for large language models, simultaneously achieving trainingparallelism, low-cost inference, and good performance. We theoretically derivethe connection between recurrence and attention. Then we propose the retentionmechanism for sequence modeling, which supports three computation paradigms,i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallelrepresentation allows for training parallelism. The recurrent representationenables low-cost $O(1)$ inference, which improves decoding throughput, latency,and GPU memory without sacrificing performance. The chunkwise recurrentrepresentation facilitates efficient long-sequence modeling with linearcomplexity, where each chunk is encoded parallelly while recurrentlysummarizing the chunks. Experimental results on language modeling show thatRetNet achieves favorable scaling results, parallel training, low-costdeployment, and efficient inference. The intriguing properties make RetNet astrong successor to Transformer for large language models. Code will beavailable at https://aka.ms/retnet.</description><author>Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei</author><pubDate>Mon, 17 Jul 2023 17:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08621v1</guid></item><item><title>Understanding the impacts of crop diversification in the context of climate change: a machine learning approach</title><link>http://arxiv.org/abs/2307.08617v1</link><description>The concept of sustainable intensification in agriculture necessitates theimplementation of management practices that prioritize sustainability withoutcompromising productivity. However, the effects of such practices are known todepend on environmental conditions, and are therefore expected to change as aresult of a changing climate. We study the impact of crop diversification onproductivity in the context of climate change. We leverage heterogeneous EarthObservation data and contribute a data-driven approach based on causal machinelearning for understanding how crop diversification impacts may change in thefuture. We apply this method to the country of Cyprus throughout a 4-yearperiod. We find that, on average, crop diversification significantly benefitedthe net primary productivity of crops, increasing it by 2.8%. The effectgenerally synergized well with higher maximum temperatures and lower soilmoistures. In a warmer and more drought-prone climate, we conclude that cropdiversification exhibits promising adaptation potential and is thus a sensiblepolicy choice with regards to agricultural productivity for present and future.</description><author>Georgios Giannarakis, Ilias Tsoumas, Stelios Neophytides, Christiana Papoutsa, Charalampos Kontoes, Diofantos Hadjimitsis</author><pubDate>Mon, 17 Jul 2023 17:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08617v1</guid></item><item><title>Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain</title><link>http://arxiv.org/abs/2307.08616v1</link><description>We study the real economic activity in the Bitcoin blockchain that involvestransactions from/to retail users rather than between organizations such asmarketplaces, exchanges, or other services. We first introduce a heuristicmethod to classify Bitcoin players into three main categories: FrequentReceivers (FR), Neighbors of FR, and Others. We show that most realtransactions involve Frequent Receivers, representing a small fraction of thetotal value exchanged according to the blockchain, but a significant fractionof all payments, raising concerns about the centralization of the Bitcoinecosystem. We also conduct a weekly pattern analysis of activity, providinginsights into the geographical location of Bitcoin users and allowing us toquantify the bias of a well-known dataset for actor identification.</description><author>Rafael Ramos Tubino, Remy Cazabet, Natkamon Tovanich, Celine Robardet</author><pubDate>Mon, 17 Jul 2023 17:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08616v1</guid></item><item><title>Benchmarking fixed-length Fingerprint Representations across different Embedding Sizes and Sensor Types</title><link>http://arxiv.org/abs/2307.08615v1</link><description>Traditional minutiae-based fingerprint representations consist of avariable-length set of minutiae. This necessitates a more complex comparisoncausing the drawback of high computational cost in one-to-many comparison.Recently, deep neural networks have been proposed to extract fixed-lengthembeddings from fingerprints. In this paper, we explore to what extentfingerprint texture information contained in such embeddings can be reduced interms of dimension while preserving high biometric performance. This is ofparticular interest since it would allow to reduce the number of operationsincurred at comparisons. We also study the impact in terms of recognitionperformance of the fingerprint textural information for two sensor types, i.e.optical and capacitive. Furthermore, the impact of rotation and translation offingerprint images on the extraction of fingerprint embeddings is analysed.Experimental results conducted on a publicly available database reveal anoptimal embedding size of 512 feature elements for the texture-based embeddingpart of fixed-length fingerprint representations. In addition, differences inperformance between sensor types can be perceived.</description><author>Tim Rohwedder, Daile Osorio-Roig, Christian Rathgeb, Christoph Busch</author><pubDate>Mon, 17 Jul 2023 17:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08615v1</guid></item><item><title>Robust empirical risk minimization via Newton's method</title><link>http://arxiv.org/abs/2301.13192v2</link><description>A new variant of Newton's method for empirical risk minimization is studied,where at each iteration of the optimization algorithm, the gradient and Hessianof the objective function are replaced by robust estimators taken from existingliterature on robust mean estimation for multivariate data. After proving ageneral theorem about the convergence of successive iterates to a small ballaround the population-level minimizer, consequences of the theory ingeneralized linear models are studied when data are generated from Huber'sepsilon-contamination model and/or heavytailed distributions. An algorithm forobtaining robust Newton directions based on the conjugate gradient method isalso proposed, which may be more appropriate for high-dimensional settings, andconjectures about the convergence of the resulting algorithm are offered.Compared to robust gradient descent, the proposed algorithm enjoys the fasterrates of convergence for successive iterates often achieved by second-orderalgorithms for convex problems, i.e., quadratic convergence in a neighborhoodof the optimum, with a stepsize that may be chosen adaptively via backtrackinglinesearch.</description><author>Eirini Ioannou, Muni Sreenivas Pydi, Po-Ling Loh</author><pubDate>Mon, 17 Jul 2023 17:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13192v2</guid></item><item><title>Overlapping Batch Confidence Intervals on Statistical Functionals Constructed from Time Series: Application to Quantiles, Optimization, and Estimation</title><link>http://arxiv.org/abs/2307.08609v1</link><description>We propose a general purpose confidence interval procedure (CIP) forstatistical functionals constructed using data from a stationary time series.The procedures we propose are based on derived distribution-free analogues ofthe $\chi^2$ and Student's $t$ random variables for the statistical functionalcontext, and hence apply in a wide variety of settings including quantileestimation, gradient estimation, M-estimation, CVAR-estimation, and arrivalprocess rate estimation, apart from more traditional statistical settings. Likethe method of subsampling, we use overlapping batches of time series data toestimate the underlying variance parameter; unlike subsampling and thebootstrap, however, we assume that the implied point estimator of thestatistical functional obeys a central limit theorem (CLT) to help identify theweak asymptotics (called OB-x limits, x=I,II,III) of batched Studentizedstatistics. The OB-x limits, certain functionals of the Wiener processparameterized by the size of the batches and the extent of their overlap, formthe essential machinery for characterizing dependence, and consequently thecorrectness of the proposed CIPs. The message from extensive numericalexperimentation is that in settings where a functional CLT on the pointestimator is in effect, using \emph{large overlapping batches} alongside OB-xcritical values yields confidence intervals that are often of significantlyhigher quality than those obtained from more generic methods like subsamplingor the bootstrap. We illustrate using examples from CVaR estimation, ARMAparameter estimation, and NHPP rate estimation; R and MATLAB code for OB-xcritical values is available at~\texttt{web.ics.purdue.edu/~pasupath/}.</description><author>Ziwei Su, Raghu Pasupathy, Yingchieh Yeh, Peter W. Glynn</author><pubDate>Mon, 17 Jul 2023 17:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08609v1</guid></item><item><title>Reducing hyperparameter dependence by external timescale tailoring</title><link>http://arxiv.org/abs/2307.08603v1</link><description>Task specific hyperparameter tuning in reservoir computing is an open issue,and is of particular relevance for hardware implemented reservoirs. Weinvestigate the influence of directly including externally controllable taskspecific timescales on the performance and hyperparameter sensitivity ofreservoir computing approaches. We show that the need for hyperparameteroptimisation can be reduced if timescales of the reservoir are tailored to thespecific task. Our results are mainly relevant for temporal tasks requiringmemory of past inputs, for example chaotic timeseries prediciton. We considervarious methods of including task specific timescales in the reservoircomputing approach and demonstrate the universality of our message by lookingat both time-multiplexed and spatially multiplexed reservoir computing.</description><author>Lina C. Jaurigue, Kathy Lüdge</author><pubDate>Mon, 17 Jul 2023 17:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08603v1</guid></item><item><title>A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors</title><link>http://arxiv.org/abs/2211.10773v3</link><description>$k$-nearest neighbor classification is a popular non-parametric methodbecause of desirable properties like automatic adaption to distributional scalechanges. Unfortunately, it has thus far proved difficult to design activelearning strategies for the training of local voting-based classifiers thatnaturally retain these desirable properties, and hence active learningstrategies for $k$-nearest neighbor classification have been conspicuouslymissing from the literature. In this work, we introduce a simple and intuitiveactive learning algorithm for the training of $k$-nearest neighbor classifiers,the first in the literature which retains the concept of the $k$-nearestneighbor vote at prediction time. We provide consistency guarantees for amodified $k$-nearest neighbors classifier trained on samples acquired via ourscheme, and show that when the conditional probability function$\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise conditionholds, our actively trained classifiers converge to the Bayes optimalclassifier at a faster asymptotic rate than passively trained $k$-nearestneighbor classifiers.</description><author>Nick Rittler, Kamalika Chaudhuri</author><pubDate>Mon, 17 Jul 2023 17:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10773v3</guid></item><item><title>LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering</title><link>http://arxiv.org/abs/2305.03403v4</link><description>As the field of automated machine learning (AutoML) advances, it becomesincreasingly important to incorporate domain knowledge into these systems. Wepresent an approach for doing so by harnessing the power of large languagemodels (LLMs). Specifically, we introduce Context-Aware Automated FeatureEngineering (CAAFE), a feature engineering method for tabular datasets thatutilizes an LLM to iteratively generate additional semantically meaningfulfeatures for tabular datasets based on the description of the dataset. Themethod produces both Python code for creating new features and explanations forthe utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 outof 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 acrossall dataset - similar to the improvement achieved by using a random forestinstead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation foreach generated feature. CAAFE paves the way for more extensive semi-automationin data science tasks and emphasizes the significance of context-awaresolutions that can extend the scope of AutoML systems to semantic AutoML. Werelease our $\href{https://github.com/automl/CAAFE}{code}$, a simple$\href{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB_alZvyARTMjhl6RZf0a}{demo}$and a $\href{https://pypi.org/project/caafe/}{python\ package}$.</description><author>Noah Hollmann, Samuel Müller, Frank Hutter</author><pubDate>Mon, 17 Jul 2023 17:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03403v4</guid></item><item><title>Glamour muscles: why having a body is not what it means to be embodied</title><link>http://arxiv.org/abs/2307.08598v1</link><description>Embodiment has recently enjoyed renewed consideration as a means to amplifythe faculties of smart machines. Proponents of embodiment seem to imply thatoptimizing for movement in physical space promotes something more than theacquisition of niche capabilities for solving problems in physical space.However, there is nothing in principle which should so distinguish the problemof action selection in physical space from the problem of action selection inmore abstract spaces, like that of language. Rather, what makes embodimentpersuasive as a means toward higher intelligence is that it promises tocapture, but does not actually realize, contingent facts about certain bodies(living intelligence) and the patterns of activity associated with them. Theseinclude an active resistance to annihilation and revisable constraints on theprocesses that make the world intelligible. To be theoretically or practicallyuseful beyond the creation of niche tools, we argue that "embodiment" cannot bethe trivial fact of a body, nor its movement through space, but the perpetualnegotiation of the function, design, and integrity of thatbody$\unicode{x2013}$that is, to participate in what it means to$\textit{constitute}$ a given body. It follows that computer programs which arestrictly incapable of traversing physical space might, under the rightconditions, be more embodied than a walking, talking robot.</description><author>Shawn L. Beaulieu, Sam Kriegman</author><pubDate>Mon, 17 Jul 2023 17:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08598v1</guid></item><item><title>Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions</title><link>http://arxiv.org/abs/2307.08597v1</link><description>In this study, we aim to develop a model that comprehends a natural languageinstruction (e.g., "Go to the living room and get the nearest pillow to theradio art on the wall") and generates a segmentation mask for the targeteveryday object. The task is challenging because it requires (1) theunderstanding of the referring expressions for multiple objects in theinstruction, (2) the prediction of the target phrase of the sentence among themultiple phrases, and (3) the generation of pixel-wise segmentation masksrather than bounding boxes. Studies have been conducted on languagebasedsegmentation methods; however, they sometimes mask irrelevant regions forcomplex sentences. In this paper, we propose the Multimodal DiffusionSegmentation Model (MDSM), which generates a mask in the first stage andrefines it in the second stage. We introduce a crossmodal parallel featureextraction mechanism and extend diffusion probabilistic models to handlecrossmodal features. To validate our model, we built a new dataset based on thewell-known Matterport3D and REVERIE datasets. This dataset consists ofinstructions with complex referring expressions accompanied by real indoorenvironmental images that feature various target objects, in addition topixel-wise segmentation masks. The performance of MDSM surpassed that of thebaseline method by a large margin of +10.13 mean IoU.</description><author>Yui Iioka, Yu Yoshida, Yuiga Wada, Shumpei Hatanaka, Komei Sugiura</author><pubDate>Mon, 17 Jul 2023 17:07:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08597v1</guid></item><item><title>Artificial Intelligence for the Electron Ion Collider (AI4EIC)</title><link>http://arxiv.org/abs/2307.08593v1</link><description>The Electron-Ion Collider (EIC), a state-of-the-art facility for studying thestrong force, is expected to begin commissioning its first experiments in 2028.This is an opportune time for artificial intelligence (AI) to be included fromthe start at this facility and in all phases that lead up to the experiments.The second annual workshop organized by the AI4EIC working group, whichrecently took place, centered on exploring all current and prospectiveapplication areas of AI for the EIC. This workshop is not only beneficial forthe EIC, but also provides valuable insights for the newly established ePICcollaboration at EIC. This paper summarizes the different activities and R&amp;Dprojects covered across the sessions of the workshop and provides an overviewof the goals, approaches and strategies regarding AI/ML in the EIC community,as well as cutting-edge techniques currently studied in other experiments.</description><author>C. Allaire, R. Ammendola, E. -C. Aschenauer, M. Balandat, M. Battaglieri, J. Bernauer, M. Bondì, N. Branson, T. Britton, A. Butter, I. Chahrour, P. Chatagnon, E. Cisbani, E. W. Cline, S. Dash, C. Dean, W. Deconinck, A. Deshpande, M. Diefenthaler, R. Ent, C. Fanelli, M. Finger, M. Finger, Jr., E. Fol, S. Furletov, Y. Gao, J. Giroux, N. C. Gunawardhana Waduge, R. Harish, O. Hassan, P. L. Hegde, R. J. Hernández-Pinto, A. Hiller Blin, T. Horn, J. Huang, D. Jayakodige, B. Joo, M. Junaid, P. Karande, B. Kriesten, R. Kunnawalkam Elayavalli, M. Lin, F. Liu, S. Liuti, G. Matousek, M. McEneaney, D. McSpadden, T. Menzo, T. Miceli, V. Mikuni, R. Montgomery, B. Nachman, R. R. Nair, J. Niestroy, S. A. Ochoa Oregon, J. Oleniacz, J. D. Osborn, C. Paudel, C. Pecar, C. Peng, G. N. Perdue, W. Phelps, M. L. P</author><pubDate>Mon, 17 Jul 2023 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08593v1</guid></item><item><title>Provably Faster Gradient Descent via Long Steps</title><link>http://arxiv.org/abs/2307.06324v3</link><description>This work establishes provably faster convergence rates for gradient descentvia a computer-assisted analysis technique. Our theory allows nonconstantstepsize policies with frequent long steps potentially violating descent byanalyzing the overall effect of many iterations at once rather than the typicalone-iteration inductions used in most first-order method analyses. We show thatlong steps, which may increase the objective value in the short term, lead toprovably faster convergence in the long term. A conjecture towards proving afaster $O(1/T\log T)$ rate for gradient descent is also motivated along withsimple numerical validation.</description><author>Benjamin Grimmer</author><pubDate>Mon, 17 Jul 2023 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06324v3</guid></item><item><title>Snapshot Spectral Clustering -- a costless approach to deep clustering ensembles generation</title><link>http://arxiv.org/abs/2307.08591v1</link><description>Despite tremendous advancements in Artificial Intelligence, learning fromlarge sets of data in an unsupervised manner remains a significant challenge.Classical clustering algorithms often fail to discover complex dependencies inlarge datasets, especially considering sparse, high-dimensional spaces.However, deep learning techniques proved to be successful when dealing withlarge quantities of data, efficiently reducing their dimensionality withoutlosing track of underlying information. Several interesting advancements havealready been made to combine deep learning and clustering. Still, the idea ofenhancing the clustering results by combining multiple views of the datagenerated by deep neural networks appears to be insufficiently explored yet.This paper aims to investigate this direction and bridge the gap between deepneural networks, clustering techniques and ensemble learning methods. Toachieve this goal, we propose a novel deep clustering ensemble method -Snapshot Spectral Clustering, designed to maximize the gain from combiningmultiple data views while minimizing the computational costs of creating theensemble. Comparative analysis and experiments described in this paper provethe proposed concept, while the conducted hyperparameter study provides avaluable intuition to follow when selecting proper values.</description><author>Adam Piróg, Halina Kwaśnicka</author><pubDate>Mon, 17 Jul 2023 17:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08591v1</guid></item><item><title>Syntax-Aware Complex-Valued Neural Machine Translation</title><link>http://arxiv.org/abs/2307.08586v1</link><description>Syntax has been proven to be remarkably effective in neural machinetranslation (NMT). Previous models obtained syntax information from syntacticparsing tools and integrated it into NMT models to improve translationperformance. In this work, we propose a method to incorporate syntaxinformation into a complex-valued Encoder-Decoder architecture. The proposedmodel jointly learns word-level and syntax-level attention scores from thesource side to the target side using an attention mechanism. Importantly, it isnot dependent on specific network architectures and can be directly integratedinto any existing sequence-to-sequence (Seq2Seq) framework. The experimentalresults demonstrate that the proposed method can bring significant improvementsin BLEU scores on two datasets. In particular, the proposed method achieves agreater improvement in BLEU scores in translation tasks involving languagepairs with significant syntactic differences.</description><author>Yang Liu, Yuexian Hou</author><pubDate>Mon, 17 Jul 2023 16:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08586v1</guid></item><item><title>Identity-Preserving Aging of Face Images via Latent Diffusion Models</title><link>http://arxiv.org/abs/2307.08585v1</link><description>The performance of automated face recognition systems is inevitably impactedby the facial aging process. However, high quality datasets of individualscollected over several years are typically small in scale. In this work, wepropose, train, and validate the use of latent text-to-image diffusion modelsfor synthetically aging and de-aging face images. Our models succeed withfew-shot training, and have the added benefit of being controllable viaintuitive textual prompting. We observe high degrees of visual realism in thegenerated images while maintaining biometric fidelity measured by commonly usedmetrics. We evaluate our method on two benchmark datasets (CelebA and AgeDB)and observe significant reduction (~44%) in the False Non-Match Rate comparedto existing state-of the-art baselines.</description><author>Sudipta Banerjee, Govind Mittal, Ameya Joshi, Chinmay Hegde, Nasir Memon</author><pubDate>Mon, 17 Jul 2023 16:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08585v1</guid></item><item><title>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</title><link>http://arxiv.org/abs/2307.08581v1</link><description>LLMs have demonstrated remarkable abilities at interacting with humansthrough language, especially with the usage of instruction-following data.Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, furtherenlarge their abilities by incorporating multi-modal inputs, including image,video, and speech. Despite their effectiveness at generating precise anddetailed language understanding of the given modality signal, these LLMs giveup the ability to ground specific parts of inputs, thus only constructing acoarse-grained mapping. However, explicit and informative correspondencebetween text and other modalities will not only improve the user experience butalso help to expand the application scenario of multi-modal LLMs. Therefore, wepropose BuboGPT, a multi-modal LLM with visual grounding that can performcross-modal interaction between vision, audio and language, providingfine-grained understanding of visual objects and other given modalities. As aresult, BuboGPT is able to point out the specific location of an object in theimage, when it is generating response or description for that object. Ourcontributions are two-fold: 1) An off-the-shelf visual grounding module basedon SAM that extracts entities in a sentence and find corresponding masks in theimage. 2) A two-stage training scheme and instruction dataset to endow jointtext-image-audio understanding. Our experiments show that BuboGPT achievesimpressive multi-modality understanding and visual grounding abilities duringthe interaction with human. It performs consistently well when provided byarbitrary modality combinations (either aligned or unaligned). Our code, modeland dataset are available at https://bubo-gpt.github.io .</description><author>Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang</author><pubDate>Mon, 17 Jul 2023 16:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08581v1</guid></item><item><title>LDMVFI: Video Frame Interpolation with Latent Diffusion Models</title><link>http://arxiv.org/abs/2303.09508v2</link><description>Existing works on video frame interpolation (VFI) mostly employ deep neuralnetworks trained to minimize the L1 or L2 distance between their outputs andground-truth frames. Despite recent advances, existing VFI methods tend toproduce perceptually inferior results, particularly for challenging scenariosincluding large motions and dynamic textures. Towards developingperceptually-oriented VFI methods, we propose latent diffusion model-based VFI,LDMVFI. This approaches the VFI problem from a generative perspective byformulating it as a conditional generation problem. As the first effort toaddress VFI using latent diffusion models, we rigorously benchmark our methodfollowing the common evaluation protocol adopted in the existing VFIliterature. Our quantitative experiments and user study indicate that LDMVFI isable to interpolate video content with superior perceptual quality compared tothe state of the art, even in the high-resolution regime. Our source code willbe made available here.</description><author>Duolikun Danier, Fan Zhang, David Bull</author><pubDate>Mon, 17 Jul 2023 16:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09508v2</guid></item><item><title>The Resume Paradox: Greater Language Differences, Smaller Pay Gaps</title><link>http://arxiv.org/abs/2307.08580v1</link><description>Over the past decade, the gender pay gap has remained steady with womenearning 84 cents for every dollar earned by men on average. Many studiesexplain this gap through demand-side bias in the labor market representedthrough employers' job postings. However, few studies analyze potential biasfrom the worker supply-side. Here, we analyze the language in millions of USworkers' resumes to investigate how differences in workers' self-representationby gender compare to differences in earnings. Across US occupations, languagedifferences between male and female resumes correspond to 11% of the variationin gender pay gap. This suggests that females' resumes that are semanticallysimilar to males' resumes may have greater wage parity. However, surprisingly,occupations with greater language differences between male and female resumeshave lower gender pay gaps. A doubling of the language difference betweenfemale and male resumes results in an annual wage increase of $2,797 for theaverage female worker. This result holds with controls for gender-biases ofresume text and we find that per-word bias poorly describes the variance inwage gap. The results demonstrate that textual data and self-representation arevaluable factors for improving worker representations and understandingemployment inequities.</description><author>Joshua R. Minot, Marc Maier, Bradford Demarest, Nicholas Cheney, Christopher M. Danforth, Peter Sheridan Dodds, Morgan R. Frank</author><pubDate>Mon, 17 Jul 2023 16:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08580v1</guid></item><item><title>Scale-Aware Modulation Meet Transformer</title><link>http://arxiv.org/abs/2307.08579v1</link><description>This paper presents a new vision Transformer, Scale-Aware ModulationTransformer (SMT), that can handle various downstream tasks efficiently bycombining the convolutional network and vision Transformer. The proposedScale-Aware Modulation (SAM) in the SMT includes two primary novel designs.Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which cancapture multi-scale features and expand the receptive field. Secondly, wepropose the Scale-Aware Aggregation (SAA) module, which is lightweight buteffective, enabling information fusion across different heads. By leveragingthese two modules, convolutional modulation is further enhanced. Furthermore,in contrast to prior works that utilized modulations throughout all stages tobuild an attention-free network, we propose an Evolutionary Hybrid Network(EHN), which can effectively simulate the shift from capturing local to globaldependencies as the network becomes deeper, resulting in superior performance.Extensive experiments demonstrate that SMT significantly outperforms existingstate-of-the-art models across a wide range of visual tasks. Specifically, SMTwith 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned withresolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN,the SMT base trained with 1x and 3x schedule outperforms the Swin Transformercounterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentationwith UPerNet, the SMT base test at single- and multi-scale surpasses Swin by2.0 and 1.1 mIoU respectively on the ADE20K.</description><author>Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin</author><pubDate>Mon, 17 Jul 2023 16:47:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08579v1</guid></item><item><title>Establishing a stronger baseline for lightweight contrastive models</title><link>http://arxiv.org/abs/2212.07158v2</link><description>Recent research has reported a performance degradation in self-supervisedcontrastive learning for specially designed efficient networks, such asMobileNet and EfficientNet. A common practice to address this problem is tointroduce a pretrained contrastive teacher model and train the lightweightnetworks with distillation signals generated by the teacher. However, it istime and resource consuming to pretrain a teacher model when it is notavailable. In this work, we aim to establish a stronger baseline forlightweight contrastive models without using a pretrained teacher model.Specifically, we show that the optimal recipe for efficient models is differentfrom that of larger models, and using the same training settings as ResNet50,as previous research does, is inappropriate. Additionally, we observe a commonissu e in contrastive learning where either the positive or negative views canbe noisy, and propose a smoothed version of InfoNCE loss to alleviate thisproblem. As a result, we successfully improve the linear evaluation resultsfrom 36.3\% to 62.3\% for MobileNet-V3-Large and from 42.2\% to 65.8\% forEfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with$5\times$ fewer parameters. We hope our research will facilitate the usage oflightweight contrastive models.</description><author>Wenye Lin, Yifeng Ding, Zhixiong Cao, Hai-tao Zheng</author><pubDate>Mon, 17 Jul 2023 16:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07158v2</guid></item><item><title>A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale</title><link>http://arxiv.org/abs/2307.08576v1</link><description>Background: Depression is a common mental disorder with societal and economicburden. Current diagnosis relies on self-reports and assessment scales, whichhave reliability issues. Objective approaches are needed for diagnosingdepression. Objective: Evaluate the potential of GPT technology in diagnosingdepression. Assess its ability to simulate individuals with depression andinvestigate the influence of depression scales. Methods: Threedepression-related assessment tools (HAMD-17, SDS, GDS-15) were used. Twoexperiments simulated GPT responses to normal individuals and individuals withdepression. Compare GPT's responses with expected results, assess itsunderstanding of depressive symptoms, and performance differences underdifferent conditions. Results: GPT's performance in depression assessment wasevaluated. It aligned with scoring criteria for both individuals withdepression and normal individuals. Some performance differences were observedbased on depression severity. GPT performed better on scales with highersensitivity. Conclusion: GPT accurately simulates individuals with depressionand normal individuals during depression-related assessments. Deviations occurwhen simulating different degrees of depression, limiting understanding of mildand moderate cases. GPT performs better on scales with higher sensitivity,indicating potential for developing more effective depression scales. GPT hasimportant potential in depression assessment, supporting clinicians andpatients.</description><author>Sijin Cai, Nanfeng Zhang, Jiaying Zhu, Yanjie Liu, Yongjin Zhou</author><pubDate>Mon, 17 Jul 2023 16:44:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08576v1</guid></item><item><title>FedCME: Client Matching and Classifier Exchanging to Handle Data Heterogeneity in Federated Learning</title><link>http://arxiv.org/abs/2307.08574v1</link><description>Data heterogeneity across clients is one of the key challenges in FederatedLearning (FL), which may slow down the global model convergence and even weakenglobal model performance. Most existing approaches tackle the heterogeneity byconstraining local model updates through reference to global informationprovided by the server. This can alleviate the performance degradation on theaggregated global model. Different from existing methods, we focus theinformation exchange between clients, which could also enhance theeffectiveness of local training and lead to generate a high-performance globalmodel. Concretely, we propose a novel FL framework named FedCME by clientmatching and classifier exchanging. In FedCME, clients with large differencesin data distribution will be matched in pairs, and then the corresponding pairof clients will exchange their classifiers at the stage of local training in anintermediate moment. Since the local data determines the local model trainingdirection, our method can correct update direction of classifiers andeffectively alleviate local update divergence. Besides, we propose featurealignment to enhance the training of the feature extractor. Experimentalresults demonstrate that FedCME performs better than FedAvg, FedProx, MOON andFedRS on popular federated learning benchmarks including FMNIST and CIFAR10, inthe case where data are heterogeneous.</description><author>Jun Nie, Danyang Xiao, Lei Yang, Weigang Wu</author><pubDate>Mon, 17 Jul 2023 16:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08574v1</guid></item><item><title>Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study</title><link>http://arxiv.org/abs/2307.08572v1</link><description>Coping with distributional shifts is an important part of transfer learningmethods in order to perform well in real-life tasks. However, most of theexisting approaches in this area either focus on an ideal scenario in which thedata does not contain noises or employ a complicated training paradigm or modeldesign to deal with distributional shifts. In this paper, we revisit therobustness of the minimum error entropy (MEE) criterion, a widely usedobjective in statistical signal processing to deal with non-Gaussian noises,and investigate its feasibility and usefulness in real-life transfer learningregression tasks, where distributional shifts are common. Specifically, we putforward a new theoretical result showing the robustness of MEE againstcovariate shift. We also show that by simply replacing the mean squared error(MSE) loss with the MEE on basic transfer learning algorithms such asfine-tuning and linear probing, we can achieve competitive performance withrespect to state-of-the-art transfer learning algorithms. We justify ourarguments on both synthetic data and 5 real-world time-series data.</description><author>Luis Pedro Silvestrin, Shujian Yu, Mark Hoogendoorn</author><pubDate>Mon, 17 Jul 2023 16:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08572v1</guid></item><item><title>On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization</title><link>http://arxiv.org/abs/2307.08551v1</link><description>Achieving high accuracy on data from domains unseen during training is afundamental challenge in domain generalization (DG). While state-of-the-art DGclassifiers have demonstrated impressive performance across various tasks, theyhave shown a bias towards domain-dependent information, such as image styles,rather than domain-invariant information, such as image content. This biasrenders them unreliable for deployment in risk-sensitive scenarios such asautonomous driving where a misclassification could lead to catastrophicconsequences. To enable risk-averse predictions from a DG classifier, wepropose a novel inference procedure, Test-Time Neural Style Smoothing (TT-NSS),that uses a "style-smoothed" version of the DG classifier for prediction attest time. Specifically, the style-smoothed classifier classifies a test imageas the most probable class predicted by the DG classifier on randomre-stylizations of the test image. TT-NSS uses a neural style transfer moduleto stylize a test image on the fly, requires only black-box access to the DGclassifier, and crucially, abstains when predictions of the DG classifier onthe stylized test images lack consensus. Additionally, we propose a neuralstyle smoothing (NSS) based training procedure that can be seamlesslyintegrated with existing DG methods. This procedure enhances predictionconsistency, improving the performance of TT-NSS on non-abstained samples. Ourempirical results demonstrate the effectiveness of TT-NSS and NSS at producingand improving risk-averse predictions on unseen domains from DG classifierstrained with SOTA training methods on various benchmark datasets and theirvariations.</description><author>Akshay Mehra, Yunbei Zhang, Bhavya Kailkhura, Jihun Hamm</author><pubDate>Mon, 17 Jul 2023 16:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08551v1</guid></item><item><title>Improving Data Efficiency for Plant Cover Prediction with Label Interpolation and Monte-Carlo Cropping</title><link>http://arxiv.org/abs/2307.08559v1</link><description>The plant community composition is an essential indicator of environmentalchanges and is, for this reason, usually analyzed in ecological field studiesin terms of the so-called plant cover. The manual acquisition of this kind ofdata is time-consuming, laborious, and prone to human error. Automated camerasystems can collect high-resolution images of the surveyed vegetation plots ata high frequency. In combination with subsequent algorithmic analysis, it ispossible to objectively extract information on plant community compositionquickly and with little human effort. An automated camera system can easilycollect the large amounts of image data necessary to train a Deep Learningsystem for automatic analysis. However, due to the amount of work required toannotate vegetation images with plant cover data, only few labeled samples areavailable. As automated camera systems can collect many pictures withoutlabels, we introduce an approach to interpolate the sparse labels in thecollected vegetation plot time series down to the intermediate dense andunlabeled images to artificially increase our training dataset to seven timesits original size. Moreover, we introduce a new method we call Monte-CarloCropping. This approach trains on a collection of cropped parts of the trainingimages to deal with high-resolution images efficiently, implicitly augment thetraining images, and speed up training. We evaluate both approaches on a plantcover dataset containing images of herbaceous plant communities and find thatour methods lead to improvements in the species, community, and segmentationmetrics investigated.</description><author>Matthias Körschens, Solveig Franziska Bucher, Christine Römermann, Joachim Denzler</author><pubDate>Mon, 17 Jul 2023 16:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08559v1</guid></item><item><title>Predicting Out-of-Domain Generalization with Neighborhood Invariance</title><link>http://arxiv.org/abs/2207.02093v3</link><description>Developing and deploying machine learning models safely depends on theability to characterize and compare their abilities to generalize to newenvironments. Although recent work has proposed a variety of methods that candirectly predict or theoretically bound the generalization capacity of a model,they rely on strong assumptions such as matching train/test distributions andaccess to model gradients. In order to characterize generalization when theseassumptions are not satisfied, we propose neighborhood invariance, a measure ofa classifier's output invariance in a local transformation neighborhood.Specifically, we sample a set of transformations and given an input test point,calculate the invariance as the largest fraction of transformed pointsclassified into the same class. Crucially, our measure is simple to calculate,does not depend on the test point's true label, makes no assumptions about thedata distribution or model, and can be applied even in out-of-domain (OOD)settings where existing methods cannot, requiring only selecting a set ofappropriate data transformations. In experiments on robustness benchmarks inimage classification, sentiment analysis, and natural language inference, wedemonstrate a strong and robust correlation between our neighborhood invariancemeasure and actual OOD generalization on over 4,600 models evaluated on over100 unique train/test domain pairs.</description><author>Nathan Ng, Neha Hulkund, Kyunghyun Cho, Marzyeh Ghassemi</author><pubDate>Mon, 17 Jul 2023 16:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02093v3</guid></item><item><title>Deep Learning with Passive Optical Nonlinear Mapping</title><link>http://arxiv.org/abs/2307.08558v1</link><description>Deep learning has fundamentally transformed artificial intelligence, but theever-increasing complexity in deep learning models calls for specializedhardware accelerators. Optical accelerators can potentially offer enhancedperformance, scalability, and energy efficiency. However, achieving nonlinearmapping, a critical component of neural networks, remains challengingoptically. Here, we introduce a design that leverages multiple scattering in areverberating cavity to passively induce optical nonlinear random mapping,without the need for additional laser power. A key advantage emerging from ourwork is that we show we can perform optical data compression, facilitated bymultiple scattering in the cavity, to efficiently compress and retain vitalinformation while also decreasing data dimensionality. This allows rapidoptical information processing and generation of low dimensional mixtures ofhighly nonlinear features. These are particularly useful for applicationsdemanding high-speed analysis and responses such as in edge computing devices.Utilizing rapid optical information processing capabilities, our opticalplatforms could potentially offer more efficient and real-time processingsolutions for a broad range of applications. We demonstrate the efficacy of ourdesign in improving computational performance across tasks, includingclassification, image reconstruction, key-point detection, and objectdetection, all achieved through optical data compression combined with adigital decoder. Notably, we observed high performance, at an extremecompression ratio, for real-time pedestrian detection. Our findings pave theway for novel algorithms and architectural designs for optical computing.</description><author>Fei Xia, Kyungduk Kim, Yaniv Eliezer, Liam Shaughnessy, Sylvain Gigan, Hui Cao</author><pubDate>Mon, 17 Jul 2023 16:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08558v1</guid></item><item><title>Machine-Learning-based Colorectal Tissue Classification via Acoustic Resolution Photoacoustic Microscopy</title><link>http://arxiv.org/abs/2307.08556v1</link><description>Colorectal cancer is a deadly disease that has become increasingly prevalentin recent years. Early detection is crucial for saving lives, but traditionaldiagnostic methods such as colonoscopy and biopsy have limitations. Colonoscopycannot provide detailed information within the tissues affected by cancer,while biopsy involves tissue removal, which can be painful and invasive. Inorder to improve diagnostic efficiency and reduce patient suffering, we studiedmachine-learningbased approach for colorectal tissue classification that usesacoustic resolution photoacoustic microscopy (ARPAM). With this tool, we wereable to classify benign and malignant tissue using multiple machine learningmethods. Our results were analyzed both quantitatively and qualitatively toevaluate the effectiveness of our approach.</description><author>Shangqing Tong, Peng Ge, Yanan Jiao, Zhaofu Ma, Ziye Li, Longhai Liu, Feng Gao, Xiaohui Du, Fei Gao</author><pubDate>Mon, 17 Jul 2023 16:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08556v1</guid></item><item><title>Fast Matrix Multiplication Without Tears: A Constraint Programming Approach</title><link>http://arxiv.org/abs/2306.01097v2</link><description>It is known that the multiplication of an $N \times M$ matrix with an $M\times P$ matrix can be performed using fewer multiplications than what thenaive $NMP$ approach suggests. The most famous instance of this is Strassen'salgorithm for multiplying two $2\times 2$ matrices in 7 instead of 8multiplications. This gives rise to the constraint satisfaction problem of fastmatrix multiplication, where a set of $R &lt; NMP$ multiplication terms must bechosen and combined such that they satisfy correctness constraints on theoutput matrix. Despite its highly combinatorial nature, this problem has notbeen exhaustively examined from that perspective, as evidenced for example bythe recent deep reinforcement learning approach of AlphaTensor. In this work,we propose a simple yet novel Constraint Programming approach to findnon-commutative algorithms for fast matrix multiplication or provide proof ofinfeasibility otherwise. We propose a set of symmetry-breaking constraints andvalid inequalities that are particularly helpful in proving infeasibility. Onthe feasible side, we find that exploiting solver performance variability inconjunction with a sparsity-based problem decomposition enables findingsolutions for larger (feasible) instances of fast matrix multiplication. Ourexperimental results using CP Optimizer demonstrate that we can find fastmatrix multiplication algorithms for matrices up to $3\times 3$ in a shortamount of time.</description><author>Arnaud Deza, Chang Liu, Pashootan Vaezipoor, Elias B. Khalil</author><pubDate>Mon, 17 Jul 2023 16:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01097v2</guid></item><item><title>Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution</title><link>http://arxiv.org/abs/2307.08544v1</link><description>Look-up table(LUT)-based methods have shown the great efficacy in singleimage super-resolution (SR) task. However, previous methods ignore theessential reason of restricted receptive field (RF) size in LUT, which iscaused by the interaction of space and channel features in vanilla convolution.They can only increase the RF at the cost of linearly increasing LUT size. Toenlarge RF with contained LUT sizes, we propose a novel ReconstructedConvolution(RC) module, which decouples channel-wise and spatial calculation.It can be formulated as $n^2$ 1D LUTs to maintain $n\times n$ receptive field,which is obviously smaller than $n\times n$D LUT formulated before. The LUTgenerated by our RC module reaches less than 1/10000 storage compared withSR-LUT baseline. The proposed Reconstructed Convolution module based LUTmethod, termed as RCLUT, can enlarge the RF size by 9 times than thestate-of-the-art LUT-based SR method and achieve superior performance on fivepopular benchmark dataset. Moreover, the efficient and robust RC module can beused as a plugin to improve other LUT-based SR methods. The code is availableat https://github.com/liuguandu/RC-LUT.</description><author>Guandu Liu, Yukang Ding, Mading Li, Ming Sun, Xing Wen, Bin Wang</author><pubDate>Mon, 17 Jul 2023 16:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08544v1</guid></item><item><title>Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training</title><link>http://arxiv.org/abs/2307.07246v2</link><description>The foundation models based on pre-training technology have significantlyadvanced artificial intelligence from theoretical to practical applications.These models have facilitated the feasibility of computer-aided diagnosis forwidespread use. Medical contrastive vision-language pre-training, which doesnot require human annotations, is an effective approach for guidingrepresentation learning using description information in diagnostic reports.However, the effectiveness of pre-training is limited by the large-scalesemantic overlap and shifting problems in medical field. To address theseissues, we propose the Knowledge-Boosting Contrastive Vision-LanguagePre-training framework (KoBo), which integrates clinical knowledge into thelearning of vision-language semantic consistency. The framework uses anunbiased, open-set sample-wise knowledge representation to measure negativesample noise and supplement the correspondence between vision-language mutualinformation and clinical knowledge. Extensive experiments validate the effectof our framework on eight tasks including classification, segmentation,retrieval, and semantic relatedness, achieving comparable or better performancewith the zero-shot or few-shot settings. Our code is open onhttps://github.com/ChenXiaoFei-CS/KoBo.</description><author>Xiaofei Chen, Yuting He, Cheng Xue, Rongjun Ge, Shuo Li, Guanyu Yang</author><pubDate>Mon, 17 Jul 2023 16:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07246v2</guid></item><item><title>Discovering collective narratives shifts in online discussions</title><link>http://arxiv.org/abs/2307.08541v1</link><description>Narrative is a foundation of human cognition and decision making. Becausenarratives play a crucial role in societal discourses and spread ofmisinformation and because of the pervasive use of social media, the narrativedynamics on social media can have profound societal impact. Yet, systematic andcomputational understanding of online narratives faces critical challenge ofthe scale and dynamics; how can we reliably and automatically extractnarratives from massive amount of texts? How do narratives emerge, spread, anddie? Here, we propose a systematic narrative discovery framework that fill thisgap by combining change point detection, semantic role labeling (SRL), andautomatic aggregation of narrative fragments into narrative networks. Weevaluate our model with synthetic and empirical data two-Twitter corpora aboutCOVID-19 and 2017 French Election. Results demonstrate that our approach canrecover major narrative shifts that correspond to the major events.</description><author>Wanying Zhao, Fiona Guo, Kristina Lerman, Yong-Yeol Ahn</author><pubDate>Mon, 17 Jul 2023 16:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08541v1</guid></item><item><title>Variational Probabilistic Fusion Network for RGB-T Semantic Segmentation</title><link>http://arxiv.org/abs/2307.08536v1</link><description>RGB-T semantic segmentation has been widely adopted to handle hard sceneswith poor lighting conditions by fusing different modality features of RGB andthermal images. Existing methods try to find an optimal fusion feature forsegmentation, resulting in sensitivity to modality noise, class-imbalance, andmodality bias. To overcome the problems, this paper proposes a novelVariational Probabilistic Fusion Network (VPFNet), which regards fusionfeatures as random variables and obtains robust segmentation by averagingsegmentation results under multiple samples of fusion features. The randomsamples generation of fusion features in VPFNet is realized by a novelVariational Feature Fusion Module (VFFM) designed based on variation attention.To further avoid class-imbalance and modality bias, we employ the weightedcross-entropy loss and introduce prior information of illumination and categoryto control the proposed VFFM. Experimental results on MFNet and PST900 datasetsdemonstrate that the proposed VPFNet can achieve state-of-the-art segmentationperformance.</description><author>Baihong Lin, Zengrong Lin, Yulan Guo, Yulan Zhang, Jianxiao Zou, Shicai Fan</author><pubDate>Mon, 17 Jul 2023 15:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08536v1</guid></item><item><title>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images</title><link>http://arxiv.org/abs/2307.08535v1</link><description>Cine magnetic resonance imaging (MRI) is the current gold standard for theassessment of cardiac anatomy and function. However, it typically only acquiresa set of two-dimensional (2D) slices of the underlying three-dimensional (3D)anatomy of the heart, thus limiting the understanding and analysis of bothhealthy and pathological cardiac morphology and physiology. In this paper, wepropose a novel fully automatic surface reconstruction pipeline capable ofreconstructing multi-class 3D cardiac anatomy meshes from raw cine MRIacquisitions. Its key component is a multi-class point cloud completion network(PCCN) capable of correcting both the sparsity and misalignment issues of the3D reconstruction task in a unified model. We first evaluate the PCCN on alarge synthetic dataset of biventricular anatomies and observe Chamferdistances between reconstructed and gold standard anatomies below or similar tothe underlying image resolution for multiple levels of slice misalignment.Furthermore, we find a reduction in reconstruction error compared to abenchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and meansurface distance, respectively. We then apply the PCCN as part of our automatedreconstruction pipeline to 1000 subjects from the UK Biobank study in across-domain transfer setting and demonstrate its ability to reconstructaccurate and topologically plausible biventricular heart meshes with clinicalmetrics comparable to the previous literature. Finally, we investigate therobustness of our proposed approach and observe its capacity to successfullyhandle multiple common outlier conditions.</description><author>Marcel Beetz, Abhirup Banerjee, Julius Ossenberg-Engels, Vicente Grau</author><pubDate>Mon, 17 Jul 2023 15:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08535v1</guid></item><item><title>Nonlinear Processing with Linear Optics</title><link>http://arxiv.org/abs/2307.08533v1</link><description>Deep neural networks have achieved remarkable breakthroughs by leveragingmultiple layers of data processing to extract hidden representations, albeit atthe cost of large electronic computing power. To enhance energy efficiency andspeed, the optical implementation of neural networks aims to harness theadvantages of optical bandwidth and the energy efficiency of opticalinterconnections. In the absence of low-power optical nonlinearities, thechallenge in the implementation of multilayer optical networks lies inrealizing multiple optical layers without resorting to electronic components.In this study, we present a novel framework that uses multiple scattering thatis capable of synthesizing programmable linear and nonlinear transformationsconcurrently at low optical power by leveraging the nonlinear relationshipbetween the scattering potential, represented by data, and the scattered field.Theoretical and experimental investigations show that repeating the data bymultiple scattering enables non-linear optical computing at low powercontinuous wave light.</description><author>Mustafa Yildirim, Niyazi Ulas Dinc, Ilker Oguz, Demetri Psaltis, Christophe Moser</author><pubDate>Mon, 17 Jul 2023 15:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08533v1</guid></item><item><title>LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents</title><link>http://arxiv.org/abs/2307.08532v1</link><description>In the last few decades we have witnessed a significant development inArtificial Intelligence (AI) thanks to the availability of a variety oftestbeds, mostly based on simulated environments and video games. Among those,roguelike games offer a very good trade-off in terms of complexity of theenvironment and computational costs, which makes them perfectly suited to testAI agents generalization capabilities. In this work, we present LuckyMera, aflexible, modular, extensible and configurable AI framework built aroundNetHack, a popular terminal-based, single-player roguelike video game. Thislibrary is aimed at simplifying and speeding up the development of AI agentscapable of successfully playing the game and offering a high-level interfacefor designing game strategies. LuckyMera comes with a set of off-the-shelfsymbolic and neural modules (called "skills"): these modules can be eitherhard-coded behaviors, or neural Reinforcement Learning approaches, with thepossibility of creating compositional hybrid solutions. Additionally, LuckyMeracomes with a set of utility features to save its experiences in the form oftrajectories for further analysis and to use them as datasets to train neuralmodules, with a direct interface to the NetHack Learning Environment andMiniHack. Through an empirical evaluation we validate our skills implementationand propose a strong baseline agent that can reach state-of-the-artperformances in the complete NetHack game. LuckyMera is open-source andavailable at https://github.com/Pervasive-AI-Lab/LuckyMera.</description><author>Luigi Quarantiello, Simone Marzeddu, Antonio Guzzi, Vincenzo Lomonaco</author><pubDate>Mon, 17 Jul 2023 15:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08532v1</guid></item><item><title>ENInst: Enhancing Weakly-supervised Low-shot Instance Segmentation</title><link>http://arxiv.org/abs/2302.09765v2</link><description>We address a weakly-supervised low-shot instance segmentation, anannotation-efficient training method to deal with novel classes effectively.Since it is an under-explored problem, we first investigate the difficulty ofthe problem and identify the performance bottleneck by conducting systematicanalyses of model components and individual sub-tasks with a simple baselinemodel. Based on the analyses, we propose ENInst with sub-task enhancementmethods: instance-wise mask refinement for enhancing pixel localization qualityand novel classifier composition for improving classification accuracy. Ourproposed method lifts the overall performance by enhancing the performance ofeach sub-task. We demonstrate that our ENInst is 7.5 times more efficient inachieving comparable performance to the existing fully-supervised few-shotmodels and even outperforms them at times.</description><author>Moon Ye-Bin, Dongmin Choi, Yongjin Kwon, Junsik Kim, Tae-Hyun Oh</author><pubDate>Mon, 17 Jul 2023 15:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09765v2</guid></item><item><title>Multi-Domain Learning with Modulation Adapters</title><link>http://arxiv.org/abs/2307.08528v1</link><description>Deep convolutional networks are ubiquitous in computer vision, due to theirexcellent performance across different tasks for various domains. Models are,however, often trained in isolation for each task, failing to exploitrelatedness between tasks and domains to learn more compact models thatgeneralise better in low-data regimes. Multi-domain learning aims to handlerelated tasks, such as image classification across multiple domains,simultaneously. Previous work on this problem explored the use of a pre-trainedand fixed domain-agnostic base network, in combination with smaller learnabledomain-specific adaptation modules. In this paper, we introduce ModulationAdapters, which update the convolutional filter weights of the model in amultiplicative manner for each task. Parameterising these adaptation weights ina factored manner allows us to scale the number of per-task parameters in aflexible manner, and to strike different parameter-accuracy trade-offs. Weevaluate our approach on the Visual Decathlon challenge, composed of ten imageclassification tasks across different domains, and on the ImageNet-to-Sketchbenchmark, which consists of six image classification tasks. Our approachyields excellent results, with accuracies that are comparable to or better thanthose of existing state-of-the-art approaches.</description><author>Ekaterina Iakovleva, Karteek Alahari, Jakob Verbeek</author><pubDate>Mon, 17 Jul 2023 15:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08528v1</guid></item><item><title>Image Captions are Natural Prompts for Text-to-Image Models</title><link>http://arxiv.org/abs/2307.08526v1</link><description>With the rapid development of Artificial Intelligence Generated Content(AIGC), it has become common practice in many learning tasks to train orfine-tune large models on synthetic data due to the data-scarcity and privacyleakage problems. Albeit promising with unlimited data generation, owing tomassive and diverse information conveyed in real images, it is challenging fortext-to-image generative models to synthesize informative training data withhand-crafted prompts, which usually leads to inferior generalizationperformance when training downstream models. In this paper, we theoreticallyanalyze the relationship between the training effect of synthetic data and thesynthetic data distribution induced by prompts. Then we correspondingly proposea simple yet effective method that prompts text-to-image generative models tosynthesize more informative and diverse training data. Specifically, we captioneach real image with the advanced captioning model to obtain informative andfaithful prompts that extract class-relevant information and clarify thepolysemy of class names. The image captions and class names are concatenated toprompt generative models for training image synthesis. Extensive experiments onImageNette, ImageNet-100, and ImageNet-1K verify that our method significantlyimproves the performance of models trained on synthetic training data, i.e.,10% classification accuracy improvements on average.</description><author>Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao</author><pubDate>Mon, 17 Jul 2023 15:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08526v1</guid></item><item><title>Masked Spiking Transformer</title><link>http://arxiv.org/abs/2210.01208v2</link><description>The combination of Spiking Neural Networks (SNNs) and Transformers hasattracted significant attention due to their potential for high energyefficiency and high-performance nature. However, existing works on this topictypically rely on direct training, which can lead to suboptimal performance. Toaddress this issue, we propose to leverage the benefits of the ANN-to-SNNconversion method to combine SNNs and Transformers, resulting in significantlyimproved performance over existing state-of-the-art SNN models. Furthermore,inspired by the quantal synaptic failures observed in the nervous system, whichreduces the number of spikes transmitted across synapses, we introduce a novelMasked Spiking Transformer (MST) framework that incorporates a Random SpikeMasking (RSM) method to prune redundant spikes and reduce energy consumptionwithout sacrificing performance. Our experimental results demonstrate that theproposed MST model achieves a significant reduction of 26.8% in powerconsumption when the masking ratio is 75% while maintaining the same level ofperformance as the unmasked model.</description><author>Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang, Zhongrui Wang, Renjing Xu</author><pubDate>Mon, 17 Jul 2023 15:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01208v2</guid></item><item><title>Results on Counterfactual Invariance</title><link>http://arxiv.org/abs/2307.08519v1</link><description>In this paper we provide a theoretical analysis of counterfactual invariance.We present a variety of existing definitions, study how they relate to eachother and what their graphical implications are. We then turn to the currentmajor question surrounding counterfactual invariance, how does it relate toconditional independence? We show that whilst counterfactual invariance impliesconditional independence, conditional independence does not give anyimplications about the degree or likelihood of satisfying counterfactualinvariance. Furthermore, we show that for discrete causal modelscounterfactually invariant functions are often constrained to be functions ofparticular variables, or even constant.</description><author>Jake Fawkes, Robin J. Evans</author><pubDate>Mon, 17 Jul 2023 15:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08519v1</guid></item><item><title>Covariate shift in nonparametric regression with Markovian design</title><link>http://arxiv.org/abs/2307.08517v1</link><description>Covariate shift in regression problems and the associated distributionmismatch between training and test data is a commonly encountered phenomenon inmachine learning. In this paper, we extend recent results on nonparametricconvergence rates for i.i.d. data to Markovian dependence structures. Wedemonstrate that under H\"older smoothness assumptions on the regressionfunction, convergence rates for the generalization risk of a Nadaraya-Watsonkernel estimator are determined by the similarity between the invariantdistributions associated to source and target Markov chains. The similarity isexplicitly captured in terms of a bandwidth-dependent similarity measurerecently introduced in Pathak, Ma and Wainwright [ICML, 2022]. Preciseconvergence rates are derived for the particular cases of finite Markov chainsand spectral gap Markov chains for which the similarity measure between theirinvariant distributions grows polynomially with decreasing bandwidth. For thelatter, we extend the notion of a distribution transfer exponent from Kpotufeand Martinet [Ann. Stat., 49(6), 2021] to kernel transfer exponents ofuniformly ergodic Markov chains in order to generate a rich class of Markovkernel pairs for which convergence guarantees for the covariate shift problemcan be formulated.</description><author>Lukas Trottner</author><pubDate>Mon, 17 Jul 2023 15:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08517v1</guid></item><item><title>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation</title><link>http://arxiv.org/abs/2307.07119v2</link><description>Current automated machine learning (ML) tools are model-centric, focusing onmodel selection and parameter optimization. However, the majority of the timein data analysis is devoted to data cleaning and wrangling, for which limitedtools are available. Here we present DataAssist, an automated data preparationand cleaning platform that enhances dataset quality using ML-informed methods.We show that DataAssist provides a pipeline for exploratory data analysis anddata cleaning, including generating visualization for user-selected variables,unifying data annotation, suggesting anomaly removal, and preprocessing data.The exported dataset can be readily integrated with other autoML tools oruser-specified model for downstream analysis. Our data-centric tool isapplicable to a variety of fields, including economics, business, andforecasting applications saving over 50% time of the time spent on datacleansing and preparation.</description><author>Kartikay Goyle, Quin Xie, Vakul Goyle</author><pubDate>Mon, 17 Jul 2023 15:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07119v2</guid></item><item><title>ContrasInver: Ultra-Sparse Label Semi-supervised Regression for Multi-dimensional Seismic Inversion</title><link>http://arxiv.org/abs/2302.06441v3</link><description>The automated interpretation and inversion of seismic data have advancedsignificantly with the development of Deep Learning (DL) methods. However,these methods often require numerous costly well logs, limiting theirapplication only to mature or synthetic data. This paper presents ContrasInver,a method that achieves seismic inversion using as few as two or three welllogs, significantly reducing current requirements. In ContrasInver, we proposethree key innovations to address the challenges of applying semi-supervisedlearning to regression tasks with ultra-sparse labels. The Multi-dimensionalSample Generation (MSG) technique pioneers a paradigm for sample generation inmulti-dimensional inversion. It produces a large number of diverse samples froma single well, while establishing lateral continuity in seismic data. MSGyields substantial improvements over current techniques, even without the useof semi-supervised learning. The Region-Growing Training (RGT) strategyleverages the inherent continuity of seismic data, effectively propagatingaccuracy from closer to more distant regions based on the proximity of welllogs. The Impedance Vectorization Projection (IVP) vectorizes impedance valuesand performs semi-supervised learning in a compressed space. We demonstratedthat the Jacobian matrix derived from this space can filter out some outliercomponents in pseudo-label vectors, thereby solving the value confusion issuein semi-supervised regression learning. In the experiments, ContrasInverachieved state-of-the-art performance in the synthetic data SEAM I. In thefield data with two or three well logs, only the methods based on thecomponents proposed in this paper were able to achieve reasonable results. It'sthe first data-driven approach yielding reliable results on the Netherlands F3and Delft, using only three and two well logs respectively.</description><author>Yimin Dou, Kewen Li, Wenjun Lv, Timing Li, Hongjie Duan, Zhifeng Xu</author><pubDate>Mon, 17 Jul 2023 15:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06441v3</guid></item><item><title>Kernel-Based Testing for Single-Cell Differential Analysis</title><link>http://arxiv.org/abs/2307.08509v1</link><description>Single-cell technologies have provided valuable insights into thedistribution of molecular features, such as gene expression and epigenomicmodifications. However, comparing these complex distributions in a controlledand powerful manner poses methodological challenges. Here we propose to benefitfrom the kernel-testing framework to compare the complex cell-wisedistributions of molecular features in a non-linear manner based on theirkernel embedding. Our framework not only allows for feature-wise analyses butalso enables global comparisons of transcriptomes or epigenomes, consideringtheir intricate dependencies. By using a classifier to discriminate cells basedon the variability of their embedding, our method uncovers heterogeneities incell populations that would otherwise go undetected. We show that kerneltesting overcomes the limitations of differential analysis methods dedicated tosingle-cell. Kernel testing is applied to investigate the reversion process ofdifferentiating cells, successfully identifying cells in transition betweenreversion and differentiation stages. Additionally, we analyze single-cellChIP-Seq data and identify a subpopulation of untreated breast cancer cellsthat exhibit an epigenomic profile similar to persister cells.</description><author>Anthony Ozier-Lafontaine, Camille Fourneaux, Ghislain Durif, Céline Vallot, Olivier Gandrillon, Sandrine Giraud, Bertrand Michel, Franck Picard</author><pubDate>Mon, 17 Jul 2023 15:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08509v1</guid></item><item><title>Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients</title><link>http://arxiv.org/abs/2307.08507v1</link><description>We design a novel algorithm for optimal transport by drawing from theentropic optimal transport, mirror descent and conjugate gradients literatures.Our algorithm is able to compute optimal transport costs with arbitraryaccuracy without running into numerical stability issues. The algorithm isimplemented efficiently on GPUs and is shown empirically to converge morequickly than traditional algorithms such as Sinkhorn's Algorithm both in termsof number of iterations and wall-clock time in many cases. We pay particularattention to the entropy of marginal distributions and show that high entropymarginals make for harder optimal transport problems, for which our algorithmis a good fit. We provide a careful ablation analysis with respect to algorithmand problem parameters, and present benchmarking over the MNIST dataset. Theresults suggest that our algorithm can be a useful addition to thepractitioner's optimal transport toolkit. Our code is open-sourced athttps://github.com/adaptive-agents-lab/MDOT-PNCG .</description><author>Mete Kemertas, Allan D. Jepson, Amir-massoud Farahmand</author><pubDate>Mon, 17 Jul 2023 15:09:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08507v1</guid></item><item><title>Does Visual Pretraining Help End-to-End Reasoning?</title><link>http://arxiv.org/abs/2307.08506v1</link><description>We aim to investigate whether end-to-end learning of visual reasoning can beachieved with general-purpose neural networks, with the help of visualpretraining. A positive result would refute the common belief that explicitvisual abstraction (e.g. object detection) is essential for compositionalgeneralization on visual reasoning, and confirm the feasibility of a neuralnetwork "generalist" to solve visual recognition and reasoning tasks. Wepropose a simple and general self-supervised framework which "compresses" eachvideo frame into a small set of tokens with a transformer network, andreconstructs the remaining frames based on the compressed temporal context. Tominimize the reconstruction loss, the network must learn a compactrepresentation for each image, as well as capture temporal dynamics and objectpermanence from temporal context. We perform evaluation on two visual reasoningbenchmarks, CATER and ACRE. We observe that pretraining is essential to achievecompositional generalization for end-to-end visual reasoning. Our proposedframework outperforms traditional supervised pretraining, including imageclassification and explicit object detection, by large margins.</description><author>Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid</author><pubDate>Mon, 17 Jul 2023 15:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08506v1</guid></item><item><title>BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization</title><link>http://arxiv.org/abs/2307.08504v1</link><description>Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models havedemonstrated impressive performance in various tasks. However, the lengthyvisual token sequences fed into ViT can lead to training inefficiency andineffectiveness. Existing efforts address the challenge by either bottom-levelpatch extraction in the ViT backbone or top-level patch abstraction outside,not balancing training efficiency and effectiveness well. Inspired by textsummarization in natural language processing, we propose a Bottom-Up PatchSummarization approach named BUS, coordinating bottom-level extraction andtop-level abstraction to learn a concise summary of lengthy visual tokensequences efficiently. Specifically, We incorporate a Text-Semantics-AwarePatch Selector (TSPS) into the ViT backbone to perform a coarse-grained visualtoken extraction and then attach a flexible Transformer-based Patch AbstractionDecoder (PAD) upon the backbone for top-level visual abstraction. Thisbottom-up collaboration enables our BUS to yield high training efficiency whilemaintaining or even improving effectiveness. We evaluate our approach onvarious visual-language understanding and generation tasks and show competitivedownstream task performance while boosting the training efficiency by 50\%.Additionally, our model achieves state-of-the-art performance on manydownstream tasks by increasing input image resolution without increasingcomputational costs over baselines.</description><author>Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang Li, Ming Yan, Bin Bi, Shikun Zhang, Fei Huang, Songfang Huang</author><pubDate>Mon, 17 Jul 2023 15:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08504v1</guid></item><item><title>Cumulative Spatial Knowledge Distillation for Vision Transformers</title><link>http://arxiv.org/abs/2307.08500v1</link><description>Distilling knowledge from convolutional neural networks (CNNs) is adouble-edged sword for vision transformers (ViTs). It boosts the performancesince the image-friendly local-inductive bias of CNN helps ViT learn faster andbetter, but leading to two problems: (1) Network designs of CNN and ViT arecompletely different, which leads to different semantic levels of intermediatefeatures, making spatial-wise knowledge transfer methods (e.g., featuremimicking) inefficient. (2) Distilling knowledge from CNN limits the networkconvergence in the later training period since ViT's capability of integratingglobal information is suppressed by CNN's local-inductive-bias supervision. Tothis end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKDdistills spatial-wise knowledge to all patch tokens of ViT from thecorresponding spatial responses of CNN, without introducing intermediatefeatures. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF)module, which introduces the global response of CNN and increasingly emphasizesits importance during the training. Applying CKF leverages CNN's localinductive bias in the early training period and gives full play to ViT's globalcapability in the later one. Extensive experiments and analysis on ImageNet-1kand downstream datasets demonstrate the superiority of our CSKD. Code will bepublicly available.</description><author>Borui Zhao, Renjie Song, Jiajun Liang</author><pubDate>Mon, 17 Jul 2023 15:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08500v1</guid></item><item><title>Can We Trust Race Prediction?</title><link>http://arxiv.org/abs/2307.08496v1</link><description>In the absence of sensitive race and ethnicity data, researchers, regulators,and firms alike turn to proxies. In this paper, I train a Bidirectional LongShort-Term Memory (BiLSTM) model on a novel dataset of voter registration datafrom all 50 US states and create an ensemble that achieves up to 36.8% higherout of sample (OOS) F1 scores than the best performing machine learning modelsin the literature. Additionally, I construct the most comprehensive database offirst and surname distributions in the US in order to improve the coverage andaccuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian ImprovedFirstname Surname Geocoding (BIFSG). Finally, I provide the first high-qualitybenchmark dataset in order to fairly compare existing models and aid futuremodel developers.</description><author>Cangyuan Li</author><pubDate>Mon, 17 Jul 2023 14:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08496v1</guid></item><item><title>Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization</title><link>http://arxiv.org/abs/2305.05812v2</link><description>The nuclear fuel loading pattern optimization problem belongs to the class oflarge-scale combinatorial optimization. It is also characterized by multipleobjectives and constraints, which makes it impossible to solve explicitly.Stochastic optimization methodologies including Genetic Algorithms andSimulated Annealing are used by different nuclear utilities and vendors, buthand-designed solutions continue to be the prevalent method in the industry. Toimprove the state-of-the-art, Deep Reinforcement Learning (RL), in particular,Proximal Policy Optimization is leveraged. This work presents a first-of-a-kindapproach to utilize deep RL to solve the loading pattern problem and could beleveraged for any engineering design optimization. This paper is also to ourknowledge the first to propose a study of the behavior of severalhyper-parameters that influence the RL algorithm. The algorithm is highlydependent on multiple factors such as the shape of the objective functionderived for the core design that behaves as a fudge factor that affects thestability of the learning. But also, an exploration/exploitation trade-off thatmanifests through different parameters such as the number of loading patternsseen by the agents per episode, the number of samples collected before a policyupdate nsteps, and an entropy factor ent_coef that increases the randomness ofthe policy during training. We found that RL must be applied similarly to aGaussian Process in which the acquisition function is replaced by aparametrized policy. Then, once an initial set of hyper-parameters is found,reducing nsteps and ent_coef until no more learning is observed will result inthe highest sample efficiency robustly and stably. This resulted in an economicbenefit of 535,000- 642,000 $/year/plant.</description><author>Paul Seurin, Koroush Shirvan</author><pubDate>Mon, 17 Jul 2023 14:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05812v2</guid></item><item><title>SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator</title><link>http://arxiv.org/abs/2307.08492v1</link><description>In this paper, we propose a novel network, SVDFormer, to tackle two specificchallenges in point cloud completion: understanding faithful global shapes fromincomplete point clouds and generating high-accuracy local structures. Currentmethods either perceive shape patterns using only 3D coordinates or importextra images with well-calibrated intrinsic parameters to guide the geometryestimation of the missing parts. However, these approaches do not always fullyleverage the cross-modal self-structures available for accurate andhigh-quality point cloud completion. To this end, we first design a Self-viewFusion Network that leverages multiple-view depth image information to observeincomplete self-shape and generate a compact global shape. To reveal highlydetailed structures, we then introduce a refinement module, calledSelf-structure Dual-generator, in which we incorporate learned shape priors andgeometric self-similarities for producing new points. By perceiving theincompleteness of each point, the dual-path design disentangles refinementstrategies conditioned on the structural type of each point. SVDFormer absorbsthe wisdom of self-structures, avoiding any additional paired information suchas color images with precisely calibrated camera intrinsic parameters.Comprehensive experiments indicate that our method achieves state-of-the-artperformance on widely-used benchmarks. Code will be available athttps://github.com/czvvd/SVDFormer.</description><author>Zhe Zhu, Honghua Chen, Xing He, Weiming Wang, Jing Qin, Mingqiang Wei</author><pubDate>Mon, 17 Jul 2023 14:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08492v1</guid></item><item><title>Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models</title><link>http://arxiv.org/abs/2307.08487v1</link><description>Researchers have invested considerable effort into ensuring that largelanguage models (LLMs) align with human values, using various trainingtechniques, such as instruction tuning and Reinforcement Learning from Human orAI Feedback (RLHF/RLAIF), to guard against text unsafety. However, thesedefenses remain incredibly vulnerable to some jailbreak attacks, which cancause the model to become overly defensive to sensitive topics or stillgenerate harmful content, leaving the model performance particularly fragile.Therefore, to comprehensively study text safety and output robustness, wepropose a latent jailbreak prompt dataset, each involving malicious instructionembedding. Specifically, we instruct the model to complete a regular task, suchas translation, where the text to be translated contains maliciousinstructions. To further analyze the safety and robustness, we design ahierarchical annotation framework. We present a systematic analysis of thesafety and robustness of LLMs concerning the position of explicit normalinstructions, word replacement (verbs in explicit normal instructions, targetgroups in malicious instructions, cue words in malicious instructions), andinstruction replacement (different explicit normal instructions). Our resultsshow that current LLMs not only have a preference for certain instructionverbs, but also exhibit different jailbreak rates for different instructionverbs in explicit normal instructions. In other words, the probability ofgenerating unsafe content by the model will be reinforced to varying degreesdepending on the instruction verb in explicit normal instructions. Code anddata are available at https://github.com/qiuhuachuan/latent-jailbreak.</description><author>Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan</author><pubDate>Mon, 17 Jul 2023 14:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08487v1</guid></item><item><title>Fairness in KI-Systemen</title><link>http://arxiv.org/abs/2307.08486v1</link><description>The more AI-assisted decisions affect people's lives, the more important thefairness of such decisions becomes. In this chapter, we provide an introductionto research on fairness in machine learning. We explain the main fairnessdefinitions and strategies for achieving fairness using concrete examples andplace fairness research in the European context. Our contribution is aimed atan interdisciplinary audience and therefore avoids mathematical formulation butemphasizes visualizations and examples. -- Je mehr KI-gest\"utzte Entscheidungen das Leben von Menschen betreffen, destowichtiger ist die Fairness solcher Entscheidungen. In diesem Kapitel geben wireine Einf\"uhrung in die Forschung zu Fairness im maschinellen Lernen. Wirerkl\"aren die wesentlichen Fairness-Definitionen und Strategien zur Erreichungvon Fairness anhand konkreter Beispiele und ordnen die Fairness-Forschung inden europ\"aischen Kontext ein. Unser Beitrag richtet sich dabei an eininterdisziplin\"ares Publikum und verzichtet daher auf die mathematischeFormulierung sondern betont Visualisierungen und Beispiele.</description><author>Janine Strotherm, Alissa Müller, Barbara Hammer, Benjamin Paaßen</author><pubDate>Mon, 17 Jul 2023 14:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08486v1</guid></item><item><title>Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines</title><link>http://arxiv.org/abs/2307.08485v1</link><description>Interpretability is a crucial aspect of machine learning models that enableshumans to understand and trust the decision-making process of these models. Inmany real-world applications, the interpretability of models is essential forlegal, ethical, and practical reasons. For instance, in the banking domain,interpretability is critical for lenders and borrowers to understand thereasoning behind the acceptance or rejection of loan applications as per fairlending laws. However, achieving interpretability in machine learning models ischallenging, especially for complex high-performance models. Hence ExplainableBoosting Machines (EBMs) have been gaining popularity due to theirinterpretable and high-performance nature in various prediction tasks. However,these models can suffer from issues such as spurious interactions withredundant features and single-feature dominance across all interactions, whichcan affect the interpretability and reliability of the model's predictions. Inthis paper, we explore novel approaches to address these issues by utilizingalternate Cross-feature selection, ensemble features and model configurationalteration techniques. Our approach involves a multi-step feature selectionprocedure that selects a set of candidate features, ensemble features and thenbenchmark the same using the EBM model. We evaluate our method on threebenchmark datasets and show that the alternate techniques outperform vanillaEBM methods, while providing better interpretability and feature selectionstability, and improving the model's predictive performance. Moreover, we showthat our approach can identify meaningful interactions and reduce the dominanceof single features in the model's predictions, leading to more reliable andinterpretable models. Index Terms- Interpretability, EBM's, ensemble, feature selection.</description><author>Shree Charran R, Sandipan Das Mahapatra</author><pubDate>Mon, 17 Jul 2023 14:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08485v1</guid></item><item><title>Navigating Fairness Measures and Trade-Offs</title><link>http://arxiv.org/abs/2307.08484v1</link><description>In order to monitor and prevent bias in AI systems we can use a wide range of(statistical) fairness measures. However, it is mathematically impossible tooptimize for all of these measures at the same time. In addition, optimizing afairness measure often greatly reduces the accuracy of the system (Kozodoi etal, 2022). As a result, we need a substantive theory that informs us how tomake these decisions and for what reasons. I show that by using Rawls' notionof justice as fairness, we can create a basis for navigating fairness measuresand the accuracy trade-off. In particular, this leads to a principled choicefocusing on both the most vulnerable groups and the type of fairness measurethat has the biggest impact on that group. This also helps to close part of thegap between philosophical accounts of distributive justice and the fairnessliterature that has been observed (Kuppler et al, 2021) and to operationalisethe value of fairness.</description><author>Stefan Buijsman</author><pubDate>Mon, 17 Jul 2023 14:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08484v1</guid></item><item><title>Differentiable Transportation Pruning</title><link>http://arxiv.org/abs/2307.08483v1</link><description>Deep learning algorithms are increasingly employed at the edge. However, edgedevices are resource constrained and thus require efficient deployment of deepneural networks. Pruning methods are a key tool for edge deployment as they canimprove storage, compute, memory bandwidth, and energy usage. In this paper wepropose a novel accurate pruning technique that allows precise control over theoutput network size. Our method uses an efficient optimal transportation schemewhich we make end-to-end differentiable and which automatically tunes theexploration-exploitation behavior of the algorithm to find accurate sparsesub-networks. We show that our method achieves state-of-the-art performancecompared to previous pruning methods on 3 different datasets, using 5 differentmodels, across a wide range of pruning ratios, and with two types of sparsitybudgets and pruning granularities.</description><author>Yunqiang Li, Jan C. van Gemert, Torsten Hoefler, Bert Moons, Evangelos Eleftheriou, Bram-Ernst Verhoef</author><pubDate>Mon, 17 Jul 2023 14:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08483v1</guid></item><item><title>Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets</title><link>http://arxiv.org/abs/2307.08481v1</link><description>This paper establishes alternative characterizations of very expressiveclasses of existential rule sets with decidable query entailment. We considerthe notable class of greedy bounded-treewidth sets (gbts) and a new,generalized variant, called weakly gbts (wgbts). Revisiting and building on thenotion of derivation graphs, we define (weakly) cycle-free derivation graphsets ((w)cdgs) and employ elaborate proof-theoretic arguments to obtain thatgbts and cdgs coincide, as do wgbts and wcdgs. These novel characterizationsadvance our analytic proof-theoretic understanding of existential rules andwill likely be instrumental in practice.</description><author>Tim S. Lyon, Sebastian Rudolph</author><pubDate>Mon, 17 Jul 2023 14:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08481v1</guid></item><item><title>Optimal Preconditioning and Fisher Adaptive Langevin Sampling</title><link>http://arxiv.org/abs/2305.14442v2</link><description>We define an optimal preconditioning for the Langevin diffusion byanalytically optimizing the expected squared jumped distance. This yields asthe optimal preconditioning an inverse Fisher information covariance matrix,where the covariance matrix is computed as the outer product of log targetgradients averaged under the target. We apply this result to the Metropolisadjusted Langevin algorithm (MALA) and derive a computationally efficientadaptive MCMC scheme that learns the preconditioning from the history ofgradients produced as the algorithm runs. We show in several experiments thatthe proposed algorithm is very robust in high dimensions and significantlyoutperforms other methods, including a closely related adaptive MALA schemethat learns the preconditioning with standard adaptive MCMC as well as theposition-dependent Riemannian manifold MALA sampler.</description><author>Michalis K. Titsias</author><pubDate>Mon, 17 Jul 2023 14:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14442v2</guid></item><item><title>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</title><link>http://arxiv.org/abs/2307.08476v1</link><description>Skeleton sequence representation learning has shown great advantages foraction recognition due to its promising ability to model human joints andtopology. However, the current methods usually require sufficient labeled datafor training computationally expensive models, which is labor-intensive andtime-consuming. Moreover, these methods ignore how to utilize the fine-graineddependencies among different skeleton joints to pre-train an efficient skeletonsequence learning model that can generalize well across different datasets. Inthis paper, we propose an efficient skeleton sequence learning framework, namedSkeleton Sequence Learning (SSL). To comprehensively capture the human pose andobtain discriminative skeleton sequence representation, we build an asymmetricgraph-based encoder-decoder pre-training architecture named SkeletonMAE, whichembeds skeleton joint sequence into Graph Convolutional Network (GCN) andreconstructs the masked skeleton joints and edges based on the prior humantopology knowledge. Then, the pre-trained SkeletonMAE encoder is integratedwith the Spatial-Temporal Representation Learning (STRL) module to build theSSL framework. Extensive experimental results show that our SSL generalizeswell across different datasets and outperforms the state-of-the-artself-supervised skeleton-based action recognition methods on FineGym, Diving48,NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance tosome fully supervised methods. The code is avaliable athttps://github.com/HongYan1123/SkeletonMAE.</description><author>Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin</author><pubDate>Mon, 17 Jul 2023 14:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08476v1</guid></item><item><title>A Fast Task Offloading Optimization Framework for IRS-Assisted Multi-Access Edge Computing System</title><link>http://arxiv.org/abs/2307.08474v1</link><description>Terahertz communication networks and intelligent reflecting surfaces exhibitsignificant potential in advancing wireless networks, particularly within thedomain of aerial-based multi-access edge computing systems. These technologiesenable efficient offloading of computational tasks from user electronic devicesto Unmanned Aerial Vehicles or local execution. For the generation ofhigh-quality task-offloading allocations, conventional numerical optimizationmethods often struggle to solve challenging combinatorial optimization problemswithin the limited channel coherence time, thereby failing to respond quicklyto dynamic changes in system conditions. To address this challenge, we proposea deep learning-based optimization framework called Iterative Order-Preservingpolicy Optimization (IOPO), which enables the generation of energy-efficienttask-offloading decisions within milliseconds. Unlike exhaustive searchmethods, IOPO provides continuous updates to the offloading decisions withoutresorting to exhaustive search, resulting in accelerated convergence andreduced computational complexity, particularly when dealing with complexproblems characterized by extensive solution spaces. Experimental resultsdemonstrate that the proposed framework can generate energy-efficienttask-offloading decisions within a very short time period, outperforming otherbenchmark methods.</description><author>Jianqiu Wu, Zhongyi Yu, Jianxiong Guo, Zhiqing Tang, Tian Wang, Weijia Jia</author><pubDate>Mon, 17 Jul 2023 14:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08474v1</guid></item><item><title>EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation</title><link>http://arxiv.org/abs/2307.08473v1</link><description>Transformer and its variants have been widely used for medical imagesegmentation. However, the large number of parameter and computational load ofthese models make them unsuitable for mobile health applications. To addressthis issue, we propose a more efficient approach, the Efficient Group EnhancedUNet (EGE-UNet). We incorporate a Group multi-axis Hadamard Product Attentionmodule (GHPA) and a Group Aggregation Bridge module (GAB) in a lightweightmanner. The GHPA groups input features and performs Hadamard Product Attentionmechanism (HPA) on different axes to extract pathological information fromdiverse perspectives. The GAB effectively fuses multi-scale information bygrouping low-level features, high-level features, and a mask generated by thedecoder at each stage. Comprehensive experiments on the ISIC2017 and ISIC2018datasets demonstrate that EGE-UNet outperforms existing state-of-the-artmethods. In short, compared to the TransFuse, our model achieves superiorsegmentation performance while reducing parameter and computation costs by 494xand 160x, respectively. Moreover, to our best knowledge, this is the firstmodel with a parameter count limited to just 50KB. Our code is available athttps://github.com/JCruan519/EGE-UNet.</description><author>Jiacheng Ruan, Mingye Xie, Jingsheng Gao, Ting Liu, Yuzhuo Fu</author><pubDate>Mon, 17 Jul 2023 14:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08473v1</guid></item><item><title>Clarifying the Half Full or Half Empty Question: Multimodal Container Classification</title><link>http://arxiv.org/abs/2307.08471v1</link><description>Multimodal integration is a key component of allowing robots to perceive theworld. Multimodality comes with multiple challenges that have to be considered,such as how to integrate and fuse the data. In this paper, we compare differentpossibilities of fusing visual, tactile and proprioceptive data. The data isdirectly recorded on the NICOL robot in an experimental setup in which therobot has to classify containers and their content. Due to the different natureof the containers, the use of the modalities can wildly differ between theclasses. We demonstrate the superiority of multimodal solutions in this usecase and evaluate three fusion strategies that integrate the data at differenttime steps. We find that the accuracy of the best fusion strategy is 15% higherthan the best strategy using only one singular sense.</description><author>Josua Spisak, Matthias Kerzel, Stefan Wermter</author><pubDate>Mon, 17 Jul 2023 14:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08471v1</guid></item><item><title>Riesz feature representation: scale equivariant scattering network for classification tasks</title><link>http://arxiv.org/abs/2307.08467v1</link><description>Scattering networks yield powerful and robust hierarchical image descriptorswhich do not require lengthy training and which work well with very fewtraining data. However, they rely on sampling the scale dimension. Hence, theybecome sensitive to scale variations and are unable to generalize to unseenscales. In this work, we define an alternative feature representation based onthe Riesz transform. We detail and analyze the mathematical foundations behindthis representation. In particular, it inherits scale equivariance from theRiesz transform and completely avoids sampling of the scale dimension.Additionally, the number of features in the representation is reduced by afactor four compared to scattering networks. Nevertheless, our representationperforms comparably well for texture classification with an interestingaddition: scale equivariance. Our method yields superior performance whendealing with scales outside of those covered by the training dataset. Theusefulness of the equivariance property is demonstrated on the digitclassification task, where accuracy remains stable even for scales four timeslarger than the one chosen for training. As a second example, we considerclassification of textures.</description><author>Tin Barisin, Jesus Angulo, Katja Schladitz, Claudia Redenbach</author><pubDate>Mon, 17 Jul 2023 14:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08467v1</guid></item><item><title>Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks</title><link>http://arxiv.org/abs/2307.08466v1</link><description>Undetected partial discharges (PDs) are a safety critical issue in highvoltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under ACvoltage is well-established, the analysis of PDs under DC voltage remains anactive research field. A key focus of these investigations is theclassification of different PD sources to enable subsequent sophisticatedanalysis. In this paper, we propose and analyze a neural network-based approach forclassifying PD signals caused by metallic protrusions and conductive particleson the insulator of HVDC GIS, without relying on pulse sequence analysisfeatures. In contrast to previous approaches, our proposed model candiscriminate the studied PD signals obtained at negative and positivepotentials, while also generalizing to unseen operating voltage multiples.Additionally, we compare the performance of time- and frequency-domain inputsignals and explore the impact of different normalization schemes to mitigatethe influence of free-space path loss between the sensor and defect location.</description><author>Steffen Seitz, Thomas Götz, Christopher Lindenberg, Ronald Tetzlaff, Stephan Schlegel</author><pubDate>Mon, 17 Jul 2023 14:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08466v1</guid></item><item><title>Towards eXplainable AI for Mobility Data Science</title><link>http://arxiv.org/abs/2307.08461v1</link><description>This paper presents our ongoing work towards XAI for Mobility Data Scienceapplications, focusing on explainable models that can learn from densetrajectory data, such as GPS tracks of vehicles and vessels using temporalgraph neural networks (GNNs) and counterfactuals. We review the existing GeoXAIstudies, argue the need for comprehensible explanations with human-centeredapproaches, and outline a research path toward XAI for Mobility Data Science.</description><author>Anahid Jalali, Anita Graser, Clemens Heistracher</author><pubDate>Mon, 17 Jul 2023 14:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08461v1</guid></item><item><title>Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI</title><link>http://arxiv.org/abs/2307.08456v1</link><description>Lateral ventricular volume (LVV) is an important biomarker for clinicalinvestigation. We present the first transfer learning-based LVV segmentationmethod for fluid-attenuated inversion recovery (FLAIR) MRI. To mitigatecovariate shifts between source and target domains, this work proposes andomain adaptation method that optimizes performance on three target datasets.Silver standard (SS) masks were generated from the target domain using a novelconventional image processing ventricular segmentation algorithm and used tosupplement the gold standard (GS) data from the source domain, CanadianAtherosclerosis Imaging Network (CAIN). Four models were tested on held-outtest sets from four datasets: 1) SS+GS: trained on target SS masks andfine-tuned on source GS masks, 2) GS+SS: trained on source GS masks andfine-tuned on target SS masks, 3) trained on source GS (GS CAIN Only) and 4)trained on target SS masks (SS Only). The SS+GS model had the best and mostconsistent performance (mean DSC = 0.89, CoV = 0.05) and showed significantly(p &lt; 0.05) higher DSC compared to the GS-only model on three target domains.Results suggest pre-training with noisy labels from the target domain allowsthe model to adapt to the dataset-specific characteristics and provides robustparameter initialization while fine-tuning with GS masks allows the model tolearn detailed features. This method has wide application to other medicalimaging problems where labeled data is scarce, and can be used as a per-datasetcalibration method to accelerate wide-scale adoption.</description><author>Owen Crystal, Pejman J. Maralani, Sandra Black, Alan R. Moody, April Khademi</author><pubDate>Mon, 17 Jul 2023 13:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08456v1</guid></item><item><title>SBMLtoODEjax: efficient simulation and optimization of ODE SBML models in JAX</title><link>http://arxiv.org/abs/2307.08452v1</link><description>Developing methods to explore, predict and control the dynamic behavior ofbiological systems, from protein pathways to complex cellular processes, is anessential frontier of research for bioengineering and biomedicine. Thus,significant effort has gone in computational inference and mathematicalmodeling of biological systems. This effort has resulted in the development oflarge collections of publicly-available models, typically stored and exchangedon online platforms (such as the BioModels Database) using the Systems BiologyMarkup Language (SBML), a standard format for representing mathematical modelsof biological systems. SBMLtoODEjax is a lightweight library that allows toautomatically parse and convert SBML models into python models writtenend-to-end in JAX, a high-performance numerical computing library withautomatic differentiation capabilities. SBMLtoODEjax is targeted at researchersthat aim to incorporate SBML-specified ordinary differential equation (ODE)models into their python projects and machine learning pipelines, in order toperform efficient numerical simulation and optimization with only a few linesof code. SBMLtoODEjax is available athttps://github.com/flowersteam/sbmltoodejax.</description><author>Mayalen Etcheverry, Michael Levin, Clément Moulin-Frier, Pierre-Yves Oudeyer</author><pubDate>Mon, 17 Jul 2023 13:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08452v1</guid></item><item><title>Learning to Reconstruct Signals From Binary Measurements</title><link>http://arxiv.org/abs/2303.08691v2</link><description>Recent advances in unsupervised learning have highlighted the possibility oflearning to reconstruct signals from noisy and incomplete linear measurementsalone. These methods play a key role in medical and scientific imaging andsensing, where ground truth data is often scarce or difficult to obtain.However, in practice, measurements are not only noisy and incomplete but alsoquantized. Here we explore the extreme case of learning from binaryobservations and provide necessary and sufficient conditions on the number ofmeasurements required for identifying a set of signals from incomplete binarydata. Our results are complementary to existing bounds on signal recovery frombinary measurements. Furthermore, we introduce a novel self-supervised learningapproach, which we name SSBM, that only requires binary data for training. Wedemonstrate in a series of experiments with real datasets that SSBM performs onpar with supervised learning and outperforms sparse reconstruction methods witha fixed wavelet basis by a large margin.</description><author>Julián Tachella, Laurent Jacques</author><pubDate>Mon, 17 Jul 2023 13:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08691v2</guid></item><item><title>Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation</title><link>http://arxiv.org/abs/2307.08448v1</link><description>Conditional diffusion models have demonstrated impressive performance inimage manipulation tasks. The general pipeline involves adding noise to theimage and then denoising it. However, this method faces a trade-off problem:adding too much noise affects the fidelity of the image while adding too littleaffects its editability. This largely limits their practical applicability. Inthis paper, we propose a novel framework, Selective Diffusion Distillation(SDD), that ensures both the fidelity and editability of images. Instead ofdirectly editing images with a diffusion model, we train a feedforward imagemanipulation network under the guidance of the diffusion model. Besides, wepropose an effective indicator to select the semantic-related timestep toobtain the correct semantic guidance from the diffusion model. This approachsuccessfully avoids the dilemma caused by the diffusion process. Our extensiveexperiments demonstrate the advantages of our framework. Code is released athttps://github.com/AndysonYs/Selective-Diffusion-Distillation.</description><author>Luozhou Wang, Shuai Yang, Shu Liu, Ying-cong Chen</author><pubDate>Mon, 17 Jul 2023 13:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08448v1</guid></item></channel></rss>