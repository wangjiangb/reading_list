<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 28 Aug 2023 06:00:33 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A System-Level View on Out-of-Distribution Data in Robotics</title><link>http://arxiv.org/abs/2212.14020v2</link><description>When testing conditions differ from those represented in training data,so-called out-of-distribution (OOD) inputs can mar the reliability of learnedcomponents in the modern robot autonomy stack. Therefore, coping with OOD datais an important challenge on the path towards trustworthy learning-enabledopen-world autonomy. In this paper, we aim to demystify the topic of OOD dataand its associated challenges in the context of data-driven robotic systems,drawing connections to emerging paradigms in the ML community that study theeffect of OOD data on learned models in isolation. We argue that asroboticists, we should reason about the overall \textit{system-level}competence of a robot as it operates in OOD conditions. We highlight keyresearch questions around this system-level view of OOD problems to guidefuture research toward safe and reliable learning-enabled autonomy.</description><author>Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel Luo, Spencer M. Richards, Yixiao Sun, Edward Schmerling, Marco Pavone</author><pubDate>Fri, 25 Aug 2023 18:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14020v2</guid></item><item><title>ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection</title><link>http://arxiv.org/abs/2308.13517v1</link><description>Open intent detection, a crucial aspect of natural language understanding,involves the identification of previously unseen intents in user-generatedtext. Despite the progress made in this field, challenges persist in handlingnew combinations of language components, which is essential for compositionalgeneralization. In this paper, we present a case study exploring the use ofChatGPT as a data augmentation technique to enhance compositionalgeneralization in open intent detection tasks. We begin by discussing thelimitations of existing benchmarks in evaluating this problem, highlighting theneed for constructing datasets for addressing compositional generalization inopen intent detection tasks. By incorporating synthetic data generated byChatGPT into the training process, we demonstrate that our approach caneffectively improve model performance. Rigorous evaluation of multiplebenchmarks reveals that our method outperforms existing techniques andsignificantly enhances open intent detection capabilities. Our findingsunderscore the potential of large language models like ChatGPT for dataaugmentation in natural language understanding tasks.</description><author>Yihao Fang, Xianzhi Li, Stephen W. Thomas, Xiaodan Zhu</author><pubDate>Fri, 25 Aug 2023 18:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13517v1</guid></item><item><title>Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs</title><link>http://arxiv.org/abs/2308.13513v1</link><description>Graph Neural Networks (GNNs) are powerful tools for learning representationson graphs, such as social networks. However, their vulnerability to privacyinference attacks restricts their practicality, especially in high-stakedomains. To address this issue, privacy-preserving GNNs have been proposed,focusing on preserving node and/or link privacy. This work takes a step backand investigates how GNNs contribute to privacy leakage. Through theoreticalanalysis and simulations, we identify message passing under structural bias asthe core component that allows GNNs to \textit{propagate} and \textit{amplify}privacy leakage. Building upon these findings, we propose a principledprivacy-preserving GNN framework that effectively safeguards both node and linkprivacy, referred to as dual-privacy preservation. The framework comprisesthree major modules: a Sensitive Information Obfuscation Module that removessensitive information from node embeddings, a Dynamic Structure DebiasingModule that dynamically corrects the structural bias, and an AdversarialLearning Module that optimizes the privacy-utility trade-off. Experimentalresults on four benchmark datasets validate the effectiveness of the proposedmodel in protecting both node and link privacy while preserving high utilityfor downstream tasks, such as node classification.</description><author>Tianyi Zhao, Hui Hu, Lu Cheng</author><pubDate>Fri, 25 Aug 2023 18:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13513v1</guid></item><item><title>Uncertainty Estimation using the Local Lipschitz for Deep Learning Image Reconstruction Models</title><link>http://arxiv.org/abs/2305.07618v2</link><description>The use of supervised deep neural network approaches has been investigated tosolve inverse problems in all domains, especially radiology where imagingtechnologies are at the heart of diagnostics. However, in deployment, thesemodels are exposed to input distributions that are widely shifted from trainingdata, due in part to data biases or drifts. It becomes crucial to know whethera given input lies outside the training data distribution before relying on thereconstruction for diagnosis. The goal of this work is three-fold: (i)demonstrate use of the local Lipshitz value as an uncertainty estimationthreshold for determining suitable performance, (ii) provide method foridentifying out-of-distribution (OOD) images where the model may not havegeneralized, and (iii) use the local Lipschitz values to guide proper dataaugmentation through identifying false positives and decrease epistemicuncertainty. We provide results for both MRI reconstruction and CT sparse viewto full view reconstruction using AUTOMAP and UNET architectures due to itbeing pertinent in the medical domain that reconstructed images remaindiagnostically accurate.</description><author>Danyal F. Bhutto, Bo Zhu, Jeremiah Z. Liu, Neha Koonjoo, Bruce R. Rosen, Matthew S. Rosen</author><pubDate>Fri, 25 Aug 2023 18:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07618v2</guid></item><item><title>On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls</title><link>http://arxiv.org/abs/2010.14449v5</link><description>We analyze principal component regression (PCR) in a high-dimensionalerror-in-variables setting with fixed design. Under suitable conditions, weshow that PCR consistently identifies the unique model with minimum$\ell_2$-norm. These results enable us to establish non-asymptoticout-of-sample prediction guarantees that improve upon the best known rates. Inthe course of our analysis, we introduce a natural linear algebraic conditionbetween the in- and out-of-sample covariates, which allows us to avoiddistributional assumptions for out-of-sample predictions. Our simulationsillustrate the importance of this condition for generalization, even undercovariate shifts. Accordingly, we construct a hypothesis test to check whenthis conditions holds in practice. As a byproduct, our results also lead tonovel results for the synthetic controls literature, a leading approach forpolicy evaluation. To the best of our knowledge, our prediction guarantees forthe fixed design setting have been elusive in both the high-dimensionalerror-in-variables and synthetic controls literatures.</description><author>Anish Agarwal, Devavrat Shah, Dennis Shen</author><pubDate>Fri, 25 Aug 2023 18:33:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.14449v5</guid></item><item><title>Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models</title><link>http://arxiv.org/abs/2308.13507v1</link><description>Large language models (LLMs) have significantly improved the ability toperform tasks in the field of code generation. However, there is still a gapbetween LLMs being capable coders and being top-tier software engineers. Basedon the observation that top-level software engineers often ask clarifyingquestions to reduce ambiguity in both requirements and coding solutions, weargue that the same should be applied to LLMs for code generation tasks. Byasking probing questions in various topics before generating the final code,the challenges of programming with LLMs, such as unclear intent specification,lack of computational thinking, and undesired code quality, may be alleviated.This, in turn, increases confidence in the generated code. In this work, weexplore how to leverage better communication skills to achieve greaterconfidence in generated code. We propose a communication-centered process thatuses an LLM-generated communicator to identify issues with high ambiguity orlow confidence in problem descriptions and generated code. We then askclarifying questions to obtain responses from users for refining the code.</description><author>Jie JW Wu</author><pubDate>Fri, 25 Aug 2023 18:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13507v1</guid></item><item><title>Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level</title><link>http://arxiv.org/abs/2308.13506v1</link><description>As research on machine translation moves to translating text beyond thesentence level, it remains unclear how effective automatic evaluation metricsare at scoring longer translations. In this work, we first propose a method forcreating paragraph-level data for training and meta-evaluating metrics fromexisting sentence-level data. Then, we use these new datasets to benchmarkexisting sentence-level metrics as well as train learned metrics at theparagraph level. Interestingly, our experimental results demonstrate that usingsentence-level metrics to score entire paragraphs is equally as effective asusing a metric designed to work at the paragraph level. We speculate thisresult can be attributed to properties of the task of reference-basedevaluation as well as limitations of our datasets with respect to capturing alltypes of phenomena that occur in paragraph-level translations.</description><author>Daniel Deutsch, Juraj Juraska, Mara Finkelstein, and Markus Freitag</author><pubDate>Fri, 25 Aug 2023 18:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13506v1</guid></item><item><title>Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation</title><link>http://arxiv.org/abs/2308.13505v1</link><description>Current prevailing Video Object Segmentation (VOS) methods usually performdense matching between the current and reference frames after extracting theirfeatures. One on hand, the decoupled modeling restricts the targets informationpropagation only at high-level feature space. On the other hand, the pixel-wisematching leads to a lack of holistic understanding of the targets. To overcomethese issues, we propose a unified VOS framework, coined as JointFormer, forjoint modeling the three elements of feature, correspondence, and a compressedmemory. The core design is the Joint Block, utilizing the flexibility ofattention to simultaneously extract feature and propagate the targetsinformation to the current tokens and the compressed memory token. This schemeallows to perform extensive information propagation and discriminative featurelearning. To incorporate the long-term temporal targets information, we alsodevise a customized online updating mechanism for the compressed memory token,which can prompt the information flow along the temporal dimension and thusimprove the global modeling capability. Under the design, our method achieves anew state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) andYouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existingworks by a large margin.</description><author>Jiaming Zhang, Yutao Cui, Gangshan Wu, Limin Wang</author><pubDate>Fri, 25 Aug 2023 18:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13505v1</guid></item><item><title>A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance</title><link>http://arxiv.org/abs/2308.13504v1</link><description>We present accumulator-aware quantization (A2Q), a novel weight quantizationmethod designed to train quantized neural networks (QNNs) to avoid overflowwhen using low-precision accumulators during inference. A2Q introduces a uniqueformulation inspired by weight normalization that constrains the L1-norm ofmodel weights according to accumulator bit width bounds that we derive. Thus,in training QNNs for low-precision accumulation, A2Q also inherently promotesunstructured weight sparsity to guarantee overflow avoidance. We apply ourmethod to deep learning-based computer vision tasks to show that A2Q can trainQNNs for low-precision accumulators while maintaining model accuracycompetitive with a floating-point baseline. In our evaluations, we consider theimpact of A2Q on both general-purpose platforms and programmable hardware.However, we primarily target model deployment on FPGAs because they can beprogrammed to fully exploit custom accumulator bit widths. Our experimentationshows accumulator bit width significantly impacts the resource efficiency ofFPGA-based accelerators. On average across our benchmarks, A2Q offers up to a2.3x reduction in resource utilization over 32-bit accumulator counterpartswith 99.2% of the floating-point model accuracy.</description><author>Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig</author><pubDate>Fri, 25 Aug 2023 18:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13504v1</guid></item><item><title>Kernel Density Matrices for Probabilistic Deep Learning</title><link>http://arxiv.org/abs/2305.18204v2</link><description>This paper introduces a novel approach to probabilistic deep learning, kerneldensity matrices, which provide a simpler yet effective mechanism forrepresenting joint probability distributions of both continuous and discreterandom variables. In quantum mechanics, a density matrix is the most generalway to describe the state of a quantum system. This work extends the concept ofdensity matrices by allowing them to be defined in a reproducing kernel Hilbertspace. This abstraction allows the construction of differentiable models fordensity estimation, inference, and sampling, and enables their integration intoend-to-end deep neural models. In doing so, we provide a versatilerepresentation of marginal and joint probability distributions that allows usto develop a differentiable, compositional, and reversible inference procedurethat covers a wide range of machine learning tasks, including densityestimation, discriminative learning, and generative modeling. The broadapplicability of the framework is illustrated by two examples: an imageclassification model that can be naturally transformed into a conditionalgenerative model, and a model for learning with label proportions thatdemonstrates the framework's ability to deal with uncertainty in the trainingsamples.</description><author>Fabio A. González, Raúl Ramos-Pollán, Joseph A. Gallego-Mejia</author><pubDate>Fri, 25 Aug 2023 18:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18204v2</guid></item><item><title>Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning</title><link>http://arxiv.org/abs/2308.13503v1</link><description>This work explores various ways of exploring multi-task learning (MTL)techniques aimed at classifying videos as original or manipulated incross-manipulation scenario to attend generalizability in deep fake scenario.The dataset used in our evaluation is FaceForensics++, which features 1000original videos manipulated by four different techniques, with a total of 5000videos. We conduct extensive experiments on multi-task learning and contrastivetechniques, which are well studied in literature for their generalizationbenefits. It can be concluded that the proposed detection model is quitegeneralized, i.e., accurately detects manipulation methods not encounteredduring training as compared to the state-of-the-art.</description><author>Pranav Balaji, Abhijit Das, Srijan Das, Antitza Dantcheva</author><pubDate>Fri, 25 Aug 2023 18:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13503v1</guid></item><item><title>Federated Object Detection for Quality Inspection in Shared Production</title><link>http://arxiv.org/abs/2306.17645v2</link><description>Federated learning (FL) has emerged as a promising approach for trainingmachine learning models on decentralized data without compromising dataprivacy. In this paper, we propose a FL algorithm for object detection inquality inspection tasks using YOLOv5 as the object detection algorithm andFederated Averaging (FedAvg) as the FL algorithm. We apply this approach to amanufacturing use-case where multiple factories/clients contribute data fortraining a global object detection model while preserving data privacy on anon-IID dataset. Our experiments demonstrate that our FL approach achievesbetter generalization performance on the overall clients' test dataset andgenerates improved bounding boxes around the objects compared to models trainedusing local clients' datasets. This work showcases the potential of FL forquality inspection tasks in the manufacturing industry and provides valuableinsights into the performance and feasibility of utilizing YOLOv5 and FedAvgfor federated object detection.</description><author>Vinit Hegiste, Tatjana Legler, Martin Ruskowski</author><pubDate>Fri, 25 Aug 2023 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17645v2</guid></item><item><title>Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators</title><link>http://arxiv.org/abs/2308.13498v1</link><description>This work introduces a novel approach for epistemic uncertainty estimationfor ensemble models using pairwise-distance estimators (PaiDEs). Theseestimators utilize the pairwise-distance between model components to establishbounds on entropy and uses said bounds as estimates for information-basedcriterion. Unlike recent deep learning methods for epistemic uncertaintyestimation, which rely on sample-based Monte Carlo estimators, PaiDEs are ableto estimate epistemic uncertainty up to 100$\times$ faster, over a larger space(up to 100$\times$) and perform more accurately in higher dimensions. Tovalidate our approach, we conducted a series of experiments commonly used toevaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0,Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an ActiveLearning framework was applied to demonstrate the advantages of PaiDEs forepistemic uncertainty estimation.</description><author>Lucas Berry, David Meger</author><pubDate>Fri, 25 Aug 2023 18:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13498v1</guid></item><item><title>Ngambay-French Neural Machine Translation (sba-Fr)</title><link>http://arxiv.org/abs/2308.13497v1</link><description>In Africa, and the world at large, there is an increasing focus on developingNeural Machine Translation (NMT) systems to overcome language barriers. NMT forLow-resource language is particularly compelling as it involves learning withlimited labelled data. However, obtaining a well-aligned parallel corpus forlow-resource languages can be challenging. The disparity between thetechnological advancement of a few global languages and the lack of research onNMT for local languages in Chad is striking. End-to-end NMT trials onlow-resource Chad languages have not been attempted. Additionally, there is adearth of online and well-structured data gathering for research in NaturalLanguage Processing, unlike some African languages. However, a guided approachfor data gathering can produce bitext data for many Chadian languagetranslation pairs with well-known languages that have ample data. In thisproject, we created the first sba-Fr Dataset, which is a corpus ofNgambay-to-French translations, and fine-tuned three pre-trained models usingthis dataset. Our experiments show that the M2M100 model outperforms othermodels with high BLEU scores on both original and original+synthetic data. Thepublicly available bitext dataset can be used for research purposes.</description><author>Sakayo Toadoum Sari, Angela Fan, Lema Logamou Seknewna</author><pubDate>Fri, 25 Aug 2023 18:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13497v1</guid></item><item><title>Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper</title><link>http://arxiv.org/abs/2308.13495v1</link><description>Eye tracking has been a pivotal tool in diverse fields such as visionresearch, language analysis, and usability assessment. The majority of priorinvestigations, however, have concentrated on expansive desktop displaysemploying specialized, costly eye tracking hardware that lacks scalability.Remarkably little insight exists into ocular movement patterns on smartphones,despite their widespread adoption and significant usage. In this manuscript, wepresent an open-source implementation of a smartphone-based gaze tracker thatemulates the methodology proposed by a GooglePaper (whose source code remainsproprietary). Our focus is on attaining accuracy comparable to that attainedthrough the GooglePaper's methodology, without the necessity for supplementaryhardware. Through the integration of machine learning techniques, we unveil anaccurate eye tracking solution that is native to smartphones. Our approachdemonstrates precision akin to the state-of-the-art mobile eye trackers, whichare characterized by a cost that is two orders of magnitude higher. Leveragingthe vast MIT GazeCapture dataset, which is available through registration onthe dataset's website, we successfully replicate crucial findings from previousstudies concerning ocular motion behavior in oculomotor tasks and saliencyanalyses during natural image observation. Furthermore, we emphasize theapplicability of smartphone-based gaze tracking in discerning readingcomprehension challenges. Our findings exhibit the inherent potential toamplify eye movement research by significant proportions, accommodatingparticipation from thousands of subjects with explicit consent. Thisscalability not only fosters advancements in vision research, but also extendsits benefits to domains such as accessibility enhancement and healthcareapplications.</description><author>Sushmanth reddy Mereddy, Jyothi Swaroop Reddy, Somnath Sharma</author><pubDate>Fri, 25 Aug 2023 18:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13495v1</guid></item><item><title>Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers</title><link>http://arxiv.org/abs/2308.13494v1</link><description>Vision Transformers achieve impressive accuracy across a range of visualrecognition tasks. Unfortunately, their accuracy frequently comes with highcomputational costs. This is a particular issue in video recognition, wheremodels are often applied repeatedly across frames or temporal chunks. In thiswork, we exploit temporal redundancy between subsequent inputs to reduce thecost of Transformers for video processing. We describe a method for identifyingand re-processing only those tokens that have changed significantly over time.Our proposed family of models, Eventful Transformers, can be converted fromexisting Transformers (often without any re-training) and give adaptive controlover the compute cost at runtime. We evaluate our method on large-scaledatasets for video object detection (ImageNet VID) and action recognition(EPIC-Kitchens 100). Our approach leads to significant computational savings(on the order of 2-4x) with only minor reductions in accuracy.</description><author>Matthew Dutson, Yin Li, Mohit Gupta</author><pubDate>Fri, 25 Aug 2023 18:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13494v1</guid></item><item><title>Federated Ensemble YOLOv5 -- A Better Generalized Object Detection Algorithm</title><link>http://arxiv.org/abs/2306.17829v2</link><description>Federated learning (FL) has gained significant traction as aprivacy-preserving algorithm, but the underlying resemblances of federatedlearning algorithms like Federated averaging (FedAvg) or Federated SGD (FedSGD) to ensemble learning algorithms have not been fully explored. The purposeof this paper is to examine the application of FL to object detection as amethod to enhance generalizability, and to compare its performance against acentralized training approach for an object detection algorithm. Specifically,we investigate the performance of a YOLOv5 model trained using FL acrossmultiple clients and employ a random sampling strategy without replacement, soeach client holds a portion of the same dataset used for centralized training.Our experimental results showcase the superior efficiency of the FL objectdetector's global model in generating accurate bounding boxes for unseenobjects, with the test set being a mixture of objects from two distinct clientsnot represented in the training dataset. These findings suggest that FL can beviewed from an ensemble algorithm perspective, akin to a synergistic blend ofBagging and Boosting techniques. As a result, FL can be seen not only as amethod to enhance privacy, but also as a method to enhance the performance of amachine learning model.</description><author>Vinit Hegiste, Tatjana Legler, Martin Ruskowski</author><pubDate>Fri, 25 Aug 2023 18:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17829v2</guid></item><item><title>Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere</title><link>http://arxiv.org/abs/2308.13492v1</link><description>Due to the lack of more efficient diagnostic tools for monkeypox, its spreadremains unchecked, presenting a formidable challenge to global health. Whilethe high efficacy of deep learning models for monkeypox diagnosis has beendemonstrated in related studies, the overlook of inference speed, the parametersize and diagnosis performance for early-stage monkeypox renders the modelsinapplicable in real-world settings. To address these challenges, we proposedan ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possessesonly 0.27M parameters and can process input images at 68 frames per second(FPS) on the CPU. To counteract the diagnostic performance limitation broughtabout by the small model capacity, it integrates the attention-based featurefusion module and the multiple auxiliary losses enhancement strategy for betterdetecting subtle image changes and optimizing weights. Using transfer learningand five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy on theMpox dataset. Notably, its recall for early-stage monkeypox achieves 93.65%. Byadopting data augmentation, our model's Accuracy rises to 98.40% and attains aPracticality Score (A new metric for measuring model practicality in real-timediagnosis application) of 0.80. We also developed an application system namedMpox-AISM V2 for both personal computers and mobile phones. Mpox-AISM V2features ultrafast responses, offline functionality, and easy deployment,enabling accurate and real-time diagnosis for both the public and individualsin various real-world settings, especially in populous settings during theoutbreak. Our work could potentially mitigate future monkeypox outbreak andilluminate a fresh paradigm for developing real-time diagnostic tools in thehealthcare field.</description><author>Yubiao Yue, Xiaoqiang Shi, Li Qin, Xinyue Zhang, Yanmei Chen, Jialong Xu, Zipei Zheng, Yujun Cao, Di Liu, Zhenzhang Li, Yang Li</author><pubDate>Fri, 25 Aug 2023 18:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13492v1</guid></item><item><title>Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning</title><link>http://arxiv.org/abs/2308.13491v1</link><description>Head-to-head autonomous racing is a challenging problem, as the vehicle needsto operate at the friction or handling limits in order to achieve minimum laptimes while also actively looking for strategies to overtake/stay ahead of theopponent. In this work we propose a head-to-head racing environment forreinforcement learning which accurately models vehicle dynamics. Some previousworks have tried learning a policy directly in the complex vehicle dynamicsenvironment but have failed to learn an optimal policy. In this work, wepropose a curriculum learning-based framework by transitioning from a simplervehicle model to a more complex real environment to teach the reinforcementlearning agent a policy closer to the optimal policy. We also propose a controlbarrier function-based safe reinforcement learning algorithm to enforce thesafety of the agent in a more effective way while not compromising onoptimality.</description><author>Dvij Kalaria, Qin Lin, John M. Dolan</author><pubDate>Fri, 25 Aug 2023 18:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13491v1</guid></item><item><title>TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</title><link>http://arxiv.org/abs/2308.13490v1</link><description>Precise hardware performance models play a crucial role in codeoptimizations. They can assist compilers in making heuristic decisions or aidautotuners in identifying the optimal configuration for a given program. Forexample, the autotuner for XLA, a machine learning compiler, discovered 10-20%speedup on state-of-the-art models serving substantial production traffic atGoogle. Although there exist a few datasets for program performance prediction,they target small sub-programs such as basic blocks or kernels. This paperintroduces TpuGraphs, a performance prediction dataset on full tensor programs,represented as computational graphs, running on Tensor Processing Units (TPUs).Each graph in the dataset represents the main computation of a machine learningworkload, e.g., a training epoch or an inference step. Each data samplecontains a computational graph, a compilation configuration, and the executiontime of the graph when compiled with the configuration. The graphs in thedataset are collected from open-source machine learning programs, featuringpopular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, andTransformer. TpuGraphs provides 25x more graphs than the largest graph propertyprediction dataset (with comparable graph sizes), and 770x larger graphs onaverage compared to existing performance prediction datasets on machinelearning programs. This graph-level prediction task on large graphs introducesnew challenges in learning, ranging from scalability, training efficiency, tomodel quality.</description><author>Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Charith Mendis, Bryan Perozzi</author><pubDate>Fri, 25 Aug 2023 18:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13490v1</guid></item><item><title>Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets</title><link>http://arxiv.org/abs/2308.13488v1</link><description>Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) isa widely used modality for diagnosing myocardial blood flow (perfusion)abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300time-resolved images of myocardial perfusion are acquired at various contrast"wash in/out" phases. Manual segmentation of myocardial contours in eachtime-frame of a DCE image series can be tedious and time-consuming,particularly when non-rigid motion correction has failed or is unavailable.While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRIdatasets, a "dynamic quality control" (dQC) technique for reliably detectingfailed segmentations is lacking. Here we propose a new space-time uncertaintymetric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRIdatasets by validating the proposed metric on an external dataset andestablishing a human-in-the-loop framework to improve the segmentation results.In the proposed approach, we referred the top 10% most uncertain segmentationsas detected by our dQC tool to the human expert for refinement. This approachresulted in a significant increase in the Dice score (p&lt;0.001) and a notabledecrease in the number of images with failed segmentation (16.2% to 11.3%)whereas the alternative approach of randomly selecting the same number ofsegmentations for human referral did not achieve any significant improvement.Our results suggest that the proposed dQC framework has the potential toaccurately identify poor-quality segmentations and may enable efficientDNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinicalinterpretation and reporting of dynamic CMRI datasets.</description><author>Dilek M. Yalcinkaya, Khalid Youssef, Bobak Heydari, Orlando Simonetti, Rohan Dharmakumar, Subha Raman, Behzad Sharif</author><pubDate>Fri, 25 Aug 2023 17:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13488v1</guid></item><item><title>Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework</title><link>http://arxiv.org/abs/2303.01693v3</link><description>Recently, data-driven models such as deep neural networks have shown to bepromising tools for modelling and state inference in soft robots. However,voluminous amounts of data are necessary for deep models to performeffectively, which requires exhaustive and quality data collection,particularly of state labels. Consequently, obtaining labelled state data forsoft robotic systems is challenged for various reasons, including difficulty inthe sensorization of soft robots and the inconvenience of collecting data inunstructured environments. To address this challenge, in this paper, we proposea semi-supervised sequential variational Bayes (DSVB) framework for transferlearning and state inference in soft robots with missing state labels oncertain robot configurations. Considering that soft robots may exhibit distinctdynamics under different robot configurations, a feature space transferstrategy is also incorporated to promote the adaptation of latent featuresacross multiple configurations. Unlike existing transfer learning approaches,our proposed DSVB employs a recurrent neural network to model the nonlineardynamics and temporal coherence in soft robot data. The proposed framework isvalidated on multiple setup configurations of a pneumatic-based soft robotfinger. Experimental results on four transfer scenarios demonstrate that DSVBperforms effective transfer learning and accurate state inference amidstmissing state labels. The data and code are available athttps://github.com/shageenderan/DSVB.</description><author>Shageenderan Sapai, Junn Yong Loo, Ze Yang Ding, Chee Pin Tan, Raphael CW Phan, Vishnu Monn Baskaran, Surya Girinatha Nurzaman</author><pubDate>Fri, 25 Aug 2023 17:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01693v3</guid></item><item><title>Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages</title><link>http://arxiv.org/abs/2308.13479v1</link><description>Large language models (LLMs) are increasingly capable and prevalent, and canbe used to produce creative content. The quality of content is influenced bythe prompt used, with more specific prompts that incorporate examples generallyproducing better results. On from this, it could be seen that usinginstructions written for crowdsourcing tasks (that are specific and includeexamples to guide workers) could prove effective LLM prompts. To explore this,we used a previous crowdsourcing pipeline that gave examples to people to helpthem generate a collectively diverse corpus of motivational messages. We thenused this same pipeline to generate messages using GPT-4, and compared thecollective diversity of messages from: (1) crowd-writers, (2) GPT-4 using thepipeline, and (3 &amp; 4) two baseline GPT-4 prompts. We found that the LLM promptsusing the crowdsourcing pipeline caused GPT-4 to produce more diverse messagesthan the two baseline prompts. We also discuss implications from messagesgenerated by both human writers and LLMs.</description><author>Samuel Rhys Cox, Ashraf Abdul, Wei Tsang Ooi</author><pubDate>Fri, 25 Aug 2023 17:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13479v1</guid></item><item><title>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</title><link>http://arxiv.org/abs/2308.12219v2</link><description>The recent surge of generative AI has been fueled by the generative power ofdiffusion probabilistic models and the scalable capabilities of large languagemodels. Despite their potential, it remains elusive whether diffusion languagemodels can solve general language tasks comparable to their autoregressivecounterparts. This paper demonstrates that scaling diffusion models w.r.t.data, sizes, and tasks can effectively make them strong language learners. Webuild competent diffusion language models at scale by first acquiring knowledgefrom massive data via masked language modeling pretraining thanks to theirintrinsic connections. We then reprogram pretrained masked language models intodiffusion language models via diffusive adaptation, wherein task-specificfinetuning and instruction finetuning are explored to unlock their versatilityin solving general language tasks. Experiments show that scaling diffusionlanguage models consistently improves performance across downstream languagetasks. We further discover that instruction finetuning can elicit zero-shot andfew-shot in-context learning abilities that help tackle many unseen tasks byfollowing natural language instructions, and show promise in advanced andchallenging abilities such as reasoning.</description><author>Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</author><pubDate>Fri, 25 Aug 2023 17:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12219v2</guid></item><item><title>Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning</title><link>http://arxiv.org/abs/2308.12843v2</link><description>In this paper, we investigate the operation of an aerial manipulator system,namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm withtwo degrees of freedom to carry out actuation tasks on the fly. Our solution isbased on employing a Q-learning method to control the trajectory of the tip ofthe arm, also called end-effector. More specifically, we develop a motionplanning model based on Time To Collision (TTC), which enables a quadrotor UAVto navigate around obstacles while ensuring the manipulator's reachability.Additionally, we utilize a model-based Q-learning model to independently trackand control the desired trajectory of the manipulator's end-effector, given anarbitrary baseline trajectory for the UAV platform. Such a combination enablesa variety of actuation tasks such as high-altitude welding, structuralmonitoring and repair, battery replacement, gutter cleaning, skyscrappercleaning, and power line maintenance in hard-to-reach and risky environmentswhile retaining compatibility with flight control firmware. Our RL-basedcontrol mechanism results in a robust control strategy that can handleuncertainties in the motion of the UAV, offering promising performance.Specifically, our method achieves 92% accuracy in terms of average displacementerror (i.e. the mean distance between the target and obtained trajectorypoints) using Q-learning with 15,000 episodes</description><author>Hazim Alzorgan, Abolfazl Razi, Ata Jahangir Moshayedi</author><pubDate>Fri, 25 Aug 2023 17:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12843v2</guid></item><item><title>The Curse of Unrolling: Rate of Differentiating Through Optimization</title><link>http://arxiv.org/abs/2209.13271v3</link><description>Computing the Jacobian of the solution of an optimization problem is acentral problem in machine learning, with applications in hyperparameteroptimization, meta-learning, optimization as a layer, and dataset distillation,to name a few. Unrolled differentiation is a popular heuristic thatapproximates the solution using an iterative solver and differentiates itthrough the computational path. This work provides a non-asymptoticconvergence-rate analysis of this approach on quadratic objectives for gradientdescent and the Chebyshev method. We show that to ensure convergence of theJacobian, we can either 1) choose a large learning rate leading to a fastasymptotic convergence but accept that the algorithm may have an arbitrarilylong burn-in phase or 2) choose a smaller learning rate leading to an immediatebut slower convergence. We refer to this phenomenon as the curse of unrolling.Finally, we discuss open problems relative to this approach, such as deriving apractical update rule for the optimal unrolling strategy and making novelconnections with the field of Sobolev orthogonal polynomials.</description><author>Damien Scieur, Quentin Bertrand, Gauthier Gidel, Fabian Pedregosa</author><pubDate>Fri, 25 Aug 2023 17:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13271v3</guid></item><item><title>Unlocking the Performance of Proximity Sensors by Utilizing Transient Histograms</title><link>http://arxiv.org/abs/2308.13473v1</link><description>We provide methods which recover planar scene geometry by utilizing thetransient histograms captured by a class of close-range time-of-flight (ToF)distance sensor. A transient histogram is a one dimensional temporal waveformwhich encodes the arrival time of photons incident on the ToF sensor.Typically, a sensor processes the transient histogram using a proprietaryalgorithm to produce distance estimates, which are commonly used in severalrobotics applications. Our methods utilize the transient histogram directly toenable recovery of planar geometry more accurately than is possible using onlyproprietary distance estimates, and consistent recovery of the albedo of theplanar surface, which is not possible with proprietary distance estimatesalone. This is accomplished via a differentiable rendering pipeline, whichsimulates the transient imaging process, allowing direct optimization of scenegeometry to match observations. To validate our methods, we capture 3,800measurements of eight planar surfaces from a wide range of viewpoints, and showthat our method outperforms the proprietary-distance-estimate baseline by anorder of magnitude in most scenarios. We demonstrate a simple roboticsapplication which uses our method to sense the distance to and slope of aplanar surface from a sensor mounted on the end effector of a robot arm.</description><author>Carter Sifferman, Yeping Wang, Mohit Gupta, Michael Gleicher</author><pubDate>Fri, 25 Aug 2023 17:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13473v1</guid></item><item><title>Editing Implicit Assumptions in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2303.08084v2</link><description>Text-to-image diffusion models often make implicit assumptions about theworld when generating images. While some assumptions are useful (e.g., the skyis blue), they can also be outdated, incorrect, or reflective of social biasespresent in the training data. Thus, there is a need to control theseassumptions without requiring explicit user input or costly re-training. Inthis work, we aim to edit a given implicit assumption in a pre-traineddiffusion model. Our Text-to-Image Model Editing method, TIME for short,receives a pair of inputs: a "source" under-specified prompt for which themodel makes an implicit assumption (e.g., "a pack of roses"), and a"destination" prompt that describes the same setting, but with a specifieddesired attribute (e.g., "a pack of blue roses"). TIME then updates the model'scross-attention layers, as these layers assign visual meaning to textualtokens. We edit the projection matrices in these layers such that the sourceprompt is projected close to the destination prompt. Our method is highlyefficient, as it modifies a mere 2.2% of the model's parameters in under onesecond. To evaluate model editing approaches, we introduce TIMED (TIMEDataset), containing 147 source and destination prompt pairs from variousdomains. Our experiments (using Stable Diffusion) show that TIME is successfulin model editing, generalizes well for related prompts unseen during editing,and imposes minimal effect on unrelated generations.</description><author>Hadas Orgad, Bahjat Kawar, Yonatan Belinkov</author><pubDate>Fri, 25 Aug 2023 17:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08084v2</guid></item><item><title>A Fast Minimization Algorithm for the Euler Elastica Model Based on a Bilinear Decomposition</title><link>http://arxiv.org/abs/2308.13471v1</link><description>The Euler Elastica (EE) model with surface curvature can generateartifact-free results compared with the traditional total variationregularization model in image processing. However, strong nonlinearity andsingularity due to the curvature term in the EE model pose a great challengefor one to design fast and stable algorithms for the EE model. In this paper,we propose a new, fast, hybrid alternating minimization (HALM) algorithm forthe EE model based on a bilinear decomposition of the gradient of theunderlying image and prove the global convergence of the minimizing sequencegenerated by the algorithm under mild conditions. The HALM algorithm comprisesthree sub-minimization problems and each is either solved in the closed form orapproximated by fast solvers making the new algorithm highly accurate andefficient. We also discuss the extension of the HALM strategy to deal withgeneral curvature-based variational models, especially with a Lipschitz smoothfunctional of the curvature. A host of numerical experiments are conducted toshow that the new algorithm produces good results with much-improved efficiencycompared to other state-of-the-art algorithms for the EE model. As one of thebenchmarks, we show that the average running time of the HALM algorithm is atmost one-quarter of that of the fast operator-splitting-basedDeng-Glowinski-Tai algorithm.</description><author>Zhifang Liu, Baochen Sun, Xue-Cheng Tai, Qi Wang, Huibin Chang</author><pubDate>Fri, 25 Aug 2023 17:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13471v1</guid></item><item><title>RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network</title><link>http://arxiv.org/abs/2308.13469v1</link><description>Cross-domain few-shot segmentation (CD-FSS) aims to achieve semanticsegmentation in previously unseen domains with a limited number of annotatedsamples. Although existing CD-FSS models focus on cross-domain featuretransformation, relying exclusively on inter-domain knowledge transfer may leadto the loss of critical intra-domain information. To this end, we propose anovel residual transformation network (RestNet) that facilitates knowledgetransfer while retaining the intra-domain support-query feature information.Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) modulethat maps features to a stable domain-agnostic space using advanced semantics.Additionally, an Intra-domain Residual Enhancement (IRE) module is designed tomaintain the intra-domain representation of the original discriminant space inthe new space. We also propose a mask prediction strategy based on prototypefusion to help the model gradually learn how to segment. Our RestNet cantransfer cross-domain knowledge from both inter-domain and intra-domain withoutrequiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray,and FSS-1000 show that our RestNet achieves state-of-the-art performance. Ourcode will be available soon.</description><author>Xinyang Huang, Chuang Zhu, Wenkai Chen</author><pubDate>Fri, 25 Aug 2023 17:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13469v1</guid></item><item><title>StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables</title><link>http://arxiv.org/abs/2304.03853v3</link><description>StepMix is an open-source Python package for the pseudo-likelihood estimation(one-, two- and three-step approaches) of generalized finite mixture models(latent profile and latent class analysis) with external variables (covariatesand distal outcomes). In many applications in social sciences, the mainobjective is not only to cluster individuals into latent classes, but also touse these classes to develop more complex statistical models. These modelsgenerally divide into a measurement model that relates the latent classes toobserved indicators, and a structural model that relates covariates and outcomevariables to the latent classes. The measurement and structural models can beestimated jointly using the so-called one-step approach or sequentially usingstepwise methods, which present significant advantages for practitionersregarding the interpretability of the estimated latent classes. In addition tothe one-step approach, StepMix implements the most important stepwiseestimation methods from the literature, including the bias-adjusted three-stepmethods with BCH and ML corrections and the more recent two-step approach.These pseudo-likelihood estimators are presented in this paper under a unifiedframework as specific expectation-maximization subroutines. To facilitate andpromote their adoption among the data science community, StepMix follows theobject-oriented design of the scikit-learn library and provides an additional Rwrapper.</description><author>Sacha Morin, Robin Legault, Félix Laliberté, Zsuzsa Bakk, Charles-Édouard Giguère, Roxane de la Sablonnière, Éric Lacourse</author><pubDate>Fri, 25 Aug 2023 17:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03853v3</guid></item><item><title>Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models</title><link>http://arxiv.org/abs/2308.13467v1</link><description>The Natural Language Processing(NLP) community has been using crowd sourcingtechniques to create benchmark datasets such as General Language Understandingand Evaluation(GLUE) for training modern Language Models such as BERT. GLUEtasks measure the reliability scores using inter annotator metrics i.e. CohensKappa. However, the reliability aspect of LMs has often been overlooked. Tocounter this problem, we explore a knowledge-guided LM ensembling approach thatleverages reinforcement learning to integrate knowledge from ConceptNet andWikipedia as knowledge graph embeddings. This approach mimics human annotatorsresorting to external knowledge to compensate for information deficits in thedatasets. Across nine GLUE datasets, our research shows that ensemblingstrengthens reliability and accuracy scores, outperforming state of the art.</description><author>Nancy Tyagi, Surjodeep Sarkar, Manas Gaur</author><pubDate>Fri, 25 Aug 2023 17:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13467v1</guid></item><item><title>Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network</title><link>http://arxiv.org/abs/2304.01568v2</link><description>Reasonably and effectively monitoring arrhythmias through ECG signals hassignificant implications for human health. With the development of deeplearning, numerous ECG classification algorithms based on deep learning haveemerged. However, most existing algorithms trade off high accuracy for complexmodels, resulting in high storage usage and power consumption. This alsoinevitably increases the difficulty of implementation on wearable ArtificialIntelligence-of-Things (AIoT) devices with limited resources. In this study, weproposed a universally applicable ultra-lightweight binary neural network(BNN)that is capable of 5-class and 17-class arrhythmia classification based on ECGsignals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (fullprecision 98.00%) accuracy for 5-class and 17-class classification,respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB).Compared to other binarization works, our approach excels in supporting twomulti-classification modes while achieving the smallest known storage space.Moreover, our model achieves optimal accuracy in 17-class classification andboasts an elegantly simple network architecture. The algorithm we use isoptimized specifically for hardware implementation. Our research showcases thepotential of lightweight deep learning models in the healthcare industry,specifically in wearable medical devices, which hold great promise forimproving patient outcomes and quality of life. Code is available on:https://github.com/xpww/ECG_BNN_Net</description><author>Ninghao Pu, Zhongxing Wu, Ao Wang, Hanshi Sun, Zijin Liu, Hao Liu</author><pubDate>Fri, 25 Aug 2023 17:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01568v2</guid></item><item><title>Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction</title><link>http://arxiv.org/abs/2308.13466v1</link><description>Despite the recent success of Graph Neural Networks (GNNs), it remainschallenging to train GNNs on large-scale graphs due to neighbor explosions. Asa remedy, distributed computing becomes a promising solution by leveragingabundant computing resources (e.g., GPU). However, the node dependency of graphdata increases the difficulty of achieving high concurrency in distributed GNNtraining, which suffers from the massive communication overhead. To address it,Historical value approximation is deemed a promising class of distributedtraining techniques. It utilizes an offline memory to cache historicalinformation (e.g., node embedding) as an affordable approximation of the exactvalue and achieves high concurrency. However, such benefits come at the cost ofinvolving dated training information, leading to staleness, imprecision, andconvergence issues. To overcome these challenges, this paper proposes SAT(Staleness-Alleviated Training), a novel and scalable distributed GNN trainingframework that reduces the embedding staleness adaptively. The key idea of SATis to model the GNN's embedding evolution as a temporal graph and build a modelupon it to predict future embedding, which effectively alleviates the stalenessof the cached historical embedding. We propose an online algorithm to train theembedding predictor and the distributed GNN alternatively and further provide aconvergence analysis. Empirically, we demonstrate that SAT can effectivelyreduce embedding staleness and thus achieve better performance and convergencespeed on multiple large-scale graph datasets.</description><author>Guangji Bai, Ziyang Yu, Zheng Chai, Yue Cheng, Liang Zhao</author><pubDate>Fri, 25 Aug 2023 17:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13466v1</guid></item><item><title>SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts</title><link>http://arxiv.org/abs/2306.02207v3</link><description>Large language models (LLMs) have gained considerable attention forArtificial Intelligence Generated Content (AIGC), particularly with theemergence of ChatGPT. However, the direct adaptation of continuous speech toLLMs that process discrete tokens remains an unsolved challenge, hindering theapplication of LLMs for speech generation. The advanced speech LMs are in thecorner, as that speech signals encapsulate a wealth of information, includingspeaker and emotion, beyond textual data alone. Prompt tuning has demonstratednotable gains in parameter efficiency and competitive performance on somespeech classification tasks. However, the extent to which prompts caneffectively elicit generation tasks from speech LMs remains an open question.In this paper, we present pioneering research that explores the application ofprompt tuning to stimulate speech LMs for various generation tasks, within aunified framework called SpeechGen, with around 10M trainable parameters. Theproposed unified framework holds great promise for efficiency andeffectiveness, particularly with the imminent arrival of advanced speech LMs,which will significantly enhance the capabilities of the framework. The codeand demos of SpeechGen will be available on the project website:\url{https://ga642381.github.io/SpeechPrompt/speechgen}</description><author>Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, Hung-yi Lee</author><pubDate>Fri, 25 Aug 2023 17:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02207v3</guid></item><item><title>Symmetry-Preserving Program Representations for Learning Code Semantics</title><link>http://arxiv.org/abs/2308.03312v2</link><description>Large Language Models (LLMs) have shown promise in automated programreasoning, a crucial aspect of many security tasks. However, existing LLMarchitectures for code are often borrowed from other domains like naturallanguage processing, raising concerns about their generalization and robustnessto unseen code. A key generalization challenge is to incorporate the knowledgeof code semantics, including control and data flow, into the LLM architectures. Drawing inspiration from examples of convolution layers exploitingtranslation symmetry, we explore how code symmetries can enhance LLMarchitectures for program analysis and modeling. We present a rigorousgroup-theoretic framework that formally defines code symmetries assemantics-preserving transformations and provides techniques for preciselyreasoning about symmetry preservation within LLM architectures. Using thisframework, we introduce a novel variant of self-attention that preservesprogram symmetries, demonstrating its effectiveness in generalization androbustness through detailed experimental evaluations across different binaryand source code analysis tasks. Overall, our code symmetry framework offersrigorous and powerful reasoning techniques that can guide the futuredevelopment of specialized LLMs for code and advance LLM-guided programreasoning tasks.</description><author>Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</author><pubDate>Fri, 25 Aug 2023 17:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03312v2</guid></item><item><title>ARTIST: ARTificial Intelligence for Simplified Text</title><link>http://arxiv.org/abs/2308.13458v1</link><description>Complex text is a major barrier for many citizens when accessing publicinformation and knowledge. While often done manually, Text Simplification is akey Natural Language Processing task that aims for reducing the linguisticcomplexity of a text while preserving the original meaning. Recent advances inGenerative Artificial Intelligence (AI) have enabled automatic textsimplification both on the lexical and syntactical levels. However, asapplications often focus on English, little is understood about theeffectiveness of Generative AI techniques on low-resource languages such asDutch. For this reason, we carry out empirical studies to understand thebenefits and limitations of applying generative technologies for textsimplification and provide the following outcomes: 1) the design andimplementation for a configurable text simplification pipeline thatorchestrates state-of-the-art generative text simplification models, domain andreader adaptation, and visualisation modules; 2) insights and lessons learned,showing the strengths of automatic text simplification while exposing thechallenges in handling cultural and commonsense knowledge. These outcomesrepresent a first step in the exploration of Dutch text simplification and shedlight on future endeavours both for research and practice.</description><author>Lorenzo Corti, Jie Yang</author><pubDate>Fri, 25 Aug 2023 17:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13458v1</guid></item><item><title>Overcoming Adversarial Attacks for Human-in-the-Loop Applications</title><link>http://arxiv.org/abs/2306.05952v2</link><description>Including human analysis has the potential to positively affect therobustness of Deep Neural Networks and is relatively unexplored in theAdversarial Machine Learning literature. Neural network visual explanation mapshave been shown to be prone to adversarial attacks. Further research is neededin order to select robust visualizations of explanations for the image analystto evaluate a given model. These factors greatly impact Human-In-The-Loop(HITL) evaluation tools due to their reliance on adversarial images, includingexplanation maps and measurements of robustness. We believe models of humanvisual attention may improve interpretability and robustness of human-machineimagery analysis systems. Our challenge remains, how can HITL evaluation berobust in this adversarial landscape?</description><author>Ryan McCoppin, Marla Kennedy, Platon Lukyanenko, Sean Kennedy</author><pubDate>Fri, 25 Aug 2023 16:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05952v2</guid></item><item><title>360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View</title><link>http://arxiv.org/abs/2303.11910v3</link><description>Seeing only a tiny part of the whole is not knowing the full circumstance.Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps fromegocentric views, is restricted when using a narrow Field of View (FoV) alone.In this work, mapping from 360{\deg} panoramas to BEV semantics, the 360BEVtask, is established for the first time to achieve holistic representations ofindoor scenes in a top-down view. Instead of relying on narrow-FoV imagesequences, a panoramic image with depth information is sufficient to generate aholistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets,360BEV-Matterport and 360BEV-Stanford, both of which include egocentricpanoramic images and semantic segmentation labels, as well as allocentricsemantic maps. Besides delving deep into different mapping paradigms, wepropose a dedicated solution for panoramic semantic mapping, namely 360Mapper.Through extensive experiments, our methods achieve 44.32% and 45.78% in mIoU onboth datasets respectively, surpassing previous counterparts with gains of+7.60% and +9.70% in mIoU. Code and datasets are available at the project page:https://jamycheung.github.io/360BEV.html.</description><author>Zhifeng Teng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Hao Shi, Simon Reiß, Ke Cao, Rainer Stiefelhagen</author><pubDate>Fri, 25 Aug 2023 16:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11910v3</guid></item><item><title>Learning to Intervene on Concept Bottlenecks</title><link>http://arxiv.org/abs/2308.13453v1</link><description>While traditional deep learning models often lack interpretability, conceptbottleneck models (CBMs) provide inherent explanations via their conceptrepresentations. Specifically, they allow users to perform interventionalinteractions on these concepts by updating the concept values and thuscorrecting the predictive output of the model. Traditionally, however, theseinterventions are applied to the model only once and discarded afterward. Torectify this, we present concept bottleneck memory models (CB2M), an extensionto CBMs. Specifically, a CB2M learns to generalize interventions to appropriatenovel situations via a two-fold memory with which it can learn to detectmistakes and to reapply previous interventions. In this way, a CB2M learns toautomatically improve model performance from a few initially obtainedinterventions. If no prior human interventions are available, a CB2M can detectpotential mistakes of the CBM bottleneck and request targeted interventions. Inour experimental evaluations on challenging scenarios like handlingdistribution shifts and confounded training data, we illustrate that CB2M areable to successfully generalize interventions to unseen data and can indeedidentify wrongly inferred concepts. Overall, our results show that CB2M is agreat tool for users to provide interactive feedback on CBMs, e.g., by guidinga user's interaction and requiring fewer interventions.</description><author>David Steinmann, Wolfgang Stammer, Felix Friedrich, Kristian Kersting</author><pubDate>Fri, 25 Aug 2023 16:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13453v1</guid></item><item><title>Gotta match 'em all: Solution diversification in graph matching matched filters</title><link>http://arxiv.org/abs/2308.13451v1</link><description>We present a novel approach for finding multiple noisily embedded templategraphs in a very large background graph. Our method builds upon thegraph-matching-matched-filter technique proposed in Sussman et al., with thediscovery of multiple diverse matchings being achieved by iterativelypenalizing a suitable node-pair similarity matrix in the matched filteralgorithm. In addition, we propose algorithmic speed-ups that greatly enhancethe scalability of our matched-filter approach. We present theoreticaljustification of our methodology in the setting of correlated Erdos-Renyigraphs, showing its ability to sequentially discover multiple templates undermild model conditions. We additionally demonstrate our method's utility viaextensive experiments both using simulated models and real-world dataset,include human brain connectomes and a large transactional knowledge base.</description><author>Zhirui Li, Ben Johnson, Daniel L. Sussman, Carey E. Priebe, Vince Lyzinski</author><pubDate>Fri, 25 Aug 2023 16:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13451v1</guid></item><item><title>The Poison of Alignment</title><link>http://arxiv.org/abs/2308.13449v1</link><description>From the perspective of content safety issues, alignment has shown to limitlarge language models' (LLMs) harmful content generation. This intentionalmethod of reinforcing models to not respond to certain user inputs seem to bepresent in many modern open-source instruction tuning datasets such asOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tunedmodel's performance affected by the presence of alignment in supervisedfine-tuning dataset. To be specific, we noticed that alignment acts as if it ispoisoning the instruction dataset. Experimentally, we demonstrate that alignedanswers significantly worsen the performance of the resulting fine-tunedmodel's on various reasoning benchmarks such as Big Bench (BBH), MassiveMultitask Language Understanding (MMLU), Human Eval, and Discrete ReasoningOver Paragraphs (DROP), performing worse than the counterpart tuned withoutalignment by 4-33%.</description><author>Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, James Yamazaki</author><pubDate>Fri, 25 Aug 2023 16:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13449v1</guid></item><item><title>Unlocking Fine-Grained Details with Wavelet-based High-Frequency Enhancement in Transformers</title><link>http://arxiv.org/abs/2308.13442v1</link><description>Medical image segmentation is a critical task that plays a vital role indiagnosis, treatment planning, and disease monitoring. Accurate segmentation ofanatomical structures and abnormalities from medical images can aid in theearly detection and treatment of various diseases. In this paper, we addressthe local feature deficiency of the Transformer model by carefully re-designingthe self-attention map to produce accurate dense prediction in medical images.To this end, we first apply the wavelet transformation to decompose the inputfeature map into low-frequency (LF) and high-frequency (HF) subbands. The LFsegment is associated with coarse-grained features while the HF componentspreserve fine-grained features such as texture and edge information. Next, wereformulate the self-attention operation using the efficient Transformer toperform both spatial and context attention on top of the frequencyrepresentation. Furthermore, to intensify the importance of the boundaryinformation, we impose an additional attention map by creating a Gaussianpyramid on top of the HF components. Moreover, we propose a multi-scale contextenhancement block within skip connections to adaptively model inter-scaledependencies to overcome the semantic gap among stages of the encoder anddecoder modules. Throughout comprehensive experiments, we demonstrate theeffectiveness of our strategy on multi-organ and skin lesion segmentationbenchmarks. The implementation code will be available upon acceptance.\href{https://github.com/mindflow-institue/WaveFormer}{GitHub}.</description><author>Reza Azad, Amirhossein Kazerouni, Alaa Sulaiman, Afshin Bozorgpour, Ehsan Khodapanah Aghdam, Abin Jose, Dorit Merhof</author><pubDate>Fri, 25 Aug 2023 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13442v1</guid></item><item><title>Mesh-Wise Prediction of Demographic Composition from Satellite Images Using Multi-Head Convolutional Neural Network</title><link>http://arxiv.org/abs/2308.13441v1</link><description>Population aging is one of the most serious problems in certain countries. Inorder to implement its countermeasures, understanding its rapid progress is ofurgency with a granular resolution. However, a detailed and rigorous surveywith high frequency is not feasible due to the constraints of financial andhuman resources. Nowadays, Deep Learning is prevalent for pattern recognitionwith significant accuracy, with its application to remote sensing. This paperproposes a multi-head Convolutional Neural Network model with transfer learningfrom pre-trained ResNet50 for estimating mesh-wise demographics of Japan as oneof the most aged countries in the world, with satellite images fromLandsat-8/OLI and Suomi NPP/VIIRS-DNS as inputs and census demographics aslabels. The trained model was performed on a testing dataset with a test scoreof at least 0.8914 in $\text{R}^2$ for all the demographic composition groups,and the estimated demographic composition was generated and visualised for 2022as a non-census year.</description><author>Yuta Sato</author><pubDate>Fri, 25 Aug 2023 16:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13441v1</guid></item><item><title>Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2308.13437v1</link><description>Recently, Multimodal Large Language Models (MLLMs) that enable Large LanguageModels (LLMs) to interpret images through visual instruction tuning haveachieved significant success. However, existing visual instruction tuningmethods only utilize image-language instruction data to align the language andimage modalities, lacking a more fine-grained cross-modal alignment. In thispaper, we propose Position-enhanced Visual Instruction Tuning (PVIT), whichextends the functionality of MLLMs by integrating an additional region-levelvision encoder. This integration promotes a more detailed comprehension ofimages for the MLLM. In addition, to efficiently achieve a fine-grainedalignment between the vision modules and the LLM, we design multiple datageneration strategies to construct an image-region-language instructiondataset. Finally, we present both quantitative experiments and qualitativeanalysis that demonstrate the superiority of the proposed model. Code and datawill be released at https://github.com/THUNLP-MT/PVIT.</description><author>Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang Liu</author><pubDate>Fri, 25 Aug 2023 16:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13437v1</guid></item><item><title>Representing Timed Automata and Timing Anomalies of Cyber-Physical Production Systems in Knowledge Graphs</title><link>http://arxiv.org/abs/2308.13433v1</link><description>Model-Based Anomaly Detection has been a successful approach to identifydeviations from the expected behavior of Cyber-Physical Production Systems.Since manual creation of these models is a time-consuming process, it isadvantageous to learn them from data and represent them in a generic formalismlike timed automata. However, these models - and by extension, the detectedanomalies - can be challenging to interpret due to a lack of additionalinformation about the system. This paper aims to improve model-based anomalydetection in CPPS by combining the learned timed automaton with a formalknowledge graph about the system. Both the model and the detected anomalies aredescribed in the knowledge graph in order to allow operators an easierinterpretation of the model and the detected anomalies. The authorsadditionally propose an ontology of the necessary concepts. The approach wasvalidated on a five-tank mixing CPPS and was able to formally define bothautomata model as well as timing anomalies in automata execution.</description><author>Tom Westermann, Milapji Singh Gill, Alexander Fay</author><pubDate>Fri, 25 Aug 2023 16:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13433v1</guid></item><item><title>On the lifting and reconstruction of nonlinear systems with multiple attractors</title><link>http://arxiv.org/abs/2304.11860v2</link><description>The Koopman operator provides a linear perspective on non-linear dynamics byfocusing on the evolution of observables in an invariant subspace. Observablesof interest are typically linearly reconstructed from the Koopmaneigenfunctions. Despite the broad use of Koopman operators over the past fewyears, there exist some misconceptions about the applicability of Koopmanoperators to dynamical systems with more than one fixed point. In this work, anexplanation is provided for the mechanism of lifting for the Koopman operatorof nonlinear systems with multiple attractors. Considering the example of theDuffing oscillator, we show that by exploiting the inherent symmetry betweenthe basins of attraction, a linear reconstruction with three degrees of freedomin the Koopman observable space is sufficient to globally linearize the system.</description><author>Shaowu Pan, Karthik Duraisamy</author><pubDate>Fri, 25 Aug 2023 16:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11860v2</guid></item><item><title>Six Lectures on Linearized Neural Networks</title><link>http://arxiv.org/abs/2308.13431v1</link><description>In these six lectures, we examine what can be learnt about the behavior ofmulti-layer neural networks from the analysis of linear models. We first recallthe correspondence between neural networks and linear models via the so-calledlazy regime. We then review four models for linearized neural networks: linearregression with concentrated features, kernel ridge regression, random featuremodel and neural tangent model. Finally, we highlight the limitations of thelinear theory and discuss how other approaches can overcome them.</description><author>Theodor Misiakiewicz, Andrea Montanari</author><pubDate>Fri, 25 Aug 2023 16:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13431v1</guid></item><item><title>Vectorized Scenario Description and Motion Prediction for Scenario-Based Testing</title><link>http://arxiv.org/abs/2302.01161v2</link><description>Automated vehicles (AVs) are tested in diverse scenarios, typically specifiedby parameters such as velocities, distances, or curve radii. To describescenarios uniformly independent of such parameters, this paper proposes avectorized scenario description defined by the road geometry and vehicles'trajectories. Data of this form are generated for three scenarios, merged, andused to train the motion prediction model VectorNet, allowing to predict anAV's trajectory for unseen scenarios. Predicting scenario evaluation metrics,VectorNet partially achieves lower errors than regression models thatseparately process the three scenarios' data. However, for comprehensivegeneralization, sufficient variance in the training data must be ensured. Thus,contrary to existing methods, our proposed method can merge diverse scenarios'data and exploit spatial and temporal nuances in the vectorized scenariodescription. As a result, data from specified test scenarios and real-worldscenarios can be compared and combined for (predictive) analyses and scenarioselection.</description><author>Max Winkelmann, Constantin Vasconi, Steffen Müller</author><pubDate>Fri, 25 Aug 2023 16:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01161v2</guid></item><item><title>On marginal feature attributions of tree-based models</title><link>http://arxiv.org/abs/2302.08434v2</link><description>Due to their power and ease of use, tree-based machine learning models, suchas random forests and gradient-boosted tree ensembles, have become verypopular. To interpret them, local feature attributions based on marginalexpectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values,may be employed. Such methods are true to the model and implementationinvariant, i.e. dependent only on the input-output function of the model. Wecontrast this with the popular TreeSHAP algorithm by presenting two(statistically similar) decision trees that compute the exact same function forwhich the "path-dependent" TreeSHAP yields different rankings of features,whereas the marginal Shapley values coincide. Furthermore, we discuss how theinternal structure of tree-based models may be leveraged to help with computingtheir marginal feature attributions according to a linear game value. Oneimportant observation is that these are simple (piecewise-constant) functionswith respect to a certain grid partition of the input space determined by thetrained model. Another crucial observation, showcased by experiments withXGBoost, LightGBM and CatBoost libraries, is that only a portion of allfeatures appears in a tree from the ensemble. Thus, the complexity of computingmarginal Shapley (or Owen or Banzhaf) feature attributions may be reduced. Thisremains valid for a broader class of game values which we shall axiomaticallycharacterize. A prime example is the case of CatBoost models where the treesare oblivious (symmetric) and the number of features in each of them is nolarger than the depth. We exploit the symmetry to derive an explicit formula,with improved complexity and only in terms of the internal model parameters,for marginal Shapley (and Banzhaf and Owen) values of CatBoost models. Thisresults in a fast, accurate algorithm for estimating these featureattributions.</description><author>Khashayar Filom, Alexey Miroshnikov, Konstandinos Kotsiopoulos, Arjun Ravi Kannan</author><pubDate>Fri, 25 Aug 2023 16:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08434v2</guid></item><item><title>Non-exemplar Class-incremental Learning by Random Auxiliary Classes Augmentation and Mixed Features</title><link>http://arxiv.org/abs/2304.07707v2</link><description>Non-exemplar class-incremental learning refers to classifying new and oldclasses without storing samples of old classes. Since only new class samplesare available for optimization, it often occurs catastrophic forgetting of oldknowledge. To alleviate this problem, many new methods are proposed such asmodel distillation, class augmentation. In this paper, we propose an effectivenon-exemplar method called RAMF consisting of Random Auxiliary classesaugmentation and Mixed Feature. On the one hand, we design a novel randomauxiliary classes augmentation method, where one augmentation is randomlyselected from three augmentations and applied on the input to generateaugmented samples and extra class labels. By extending data and label space, itallows the model to learn more diverse representations, which can prevent themodel from being biased towards learning task-specific features. When learningnew tasks, it will reduce the change of feature space and improve modelgeneralization. On the other hand, we employ mixed feature to replace the newfeatures since only using new feature to optimize the model will affect therepresentation that was previously embedded in the feature space. Instead, bymixing new and old features, old knowledge can be retained without increasingthe computational complexity. Extensive experiments on three benchmarksdemonstrate the superiority of our approach, which outperforms thestate-of-the-art non-exemplar methods and is comparable to high-performancereplay-based methods.</description><author>Ke Song, Quan Xia, Guoqiang Liang, Zhaojie Chen, Yanning Zhang</author><pubDate>Fri, 25 Aug 2023 16:17:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07707v2</guid></item><item><title>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</title><link>http://arxiv.org/abs/2308.01423v2</link><description>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built topredict and generate metal-organic frameworks (MOFs). By leveraging alarge-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts keydetails from textual inputs and delivers appropriate responses, thuseliminating the necessity for rigid structured queries. The system is comprisedof three core components (i.e. an agent, a toolkit, and an evaluator) and itforms a robust pipeline that manages a variety of tasks, including dataretrieval, property prediction, and structure generations. The study furtherexplores the merits and constraints of using large language models (LLMs) AIsystem in material sciences using and showcases its transformative potentialfor future advancements.</description><author>Yeonghun Kang, Jihan Kim</author><pubDate>Fri, 25 Aug 2023 16:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01423v2</guid></item><item><title>QKSAN: A Quantum Kernel Self-Attention Network</title><link>http://arxiv.org/abs/2308.13422v1</link><description>Self-Attention Mechanism (SAM) is skilled at extracting important informationfrom the interior of data to improve the computational efficiency of models.Nevertheless, many Quantum Machine Learning (QML) models lack the ability todistinguish the intrinsic connections of information like SAM, which limitstheir effectiveness on massive high-dimensional quantum data. To address thisissue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced, whichcombines the data representation benefit of Quantum Kernel Methods (QKM) withthe efficient information extraction capability of SAM. A Quantum KernelSelf-Attention Network (QKSAN) framework is built based on QKSAM, with DeferredMeasurement Principle (DMP) and conditional measurement techniques, whichreleases half of the quantum resources with probabilistic measurements duringcomputation. The Quantum Kernel Self-Attention Score (QKSAS) determines themeasurement conditions and reflects the probabilistic nature of quantumsystems. Finally, four QKSAN models are deployed on the Pennylane platform toperform binary classification on MNIST images. The best-performing among thefour models is assessed for noise immunity and learning ability. Remarkably,the potential learning benefit of partial QKSAN models over classical deeplearning is that they require few parameters for a high return of 98\% $\pm$1\% test and train accuracy, even with highly compressed images. QKSAN lays thefoundation for future quantum computers to perform machine learning on massiveamounts of data, while driving advances in areas such as quantum NaturalLanguage Processing (NLP).</description><author>Ren-Xin Zhao, Jinjing Shi, Xuelong Li</author><pubDate>Fri, 25 Aug 2023 16:08:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13422v1</guid></item><item><title>Exploiting Diverse Feature for Multimodal Sentiment Analysis</title><link>http://arxiv.org/abs/2308.13421v1</link><description>In this paper, we present our solution to the MuSe-Personalisationsub-challenge in the MuSe 2023 Multimodal Sentiment Analysis Challenge. Thetask of MuSe-Personalisation aims to predict the continuous arousal and valencevalues of a participant based on their audio-visual, language, andphysiological signal modalities data. Considering different people havepersonal characteristics, the main challenge of this task is how to buildrobustness feature presentation for sentiment prediction. To address thisissue, we propose exploiting diverse features. Specifically, we proposed aseries of feature extraction methods to build a robust representation and modelensemble. We empirically evaluate the performance of the utilized method on theofficially provided dataset. \textbf{As a result, we achieved 3rd place in theMuSe-Personalisation sub-challenge.} Specifically, we achieve the results of0.8492 and 0.8439 for MuSe-Personalisation in terms of arousal and valence CCC.</description><author>Jia Li, Wei Qian, Kun Li, Qi Li, Dan Guo, Meng Wang</author><pubDate>Fri, 25 Aug 2023 16:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13421v1</guid></item><item><title>Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities</title><link>http://arxiv.org/abs/2308.13420v1</link><description>Evolutionary algorithms (EA), a class of stochastic search algorithms basedon the principles of natural evolution, have received widespread acclaim fortheir exceptional performance in various optimization problems. Whileresearchers worldwide have proposed a wide variety of EAs, certain limitationsremain, such as slow convergence speed and poor generalization capabilities.Consequently, numerous scholars are actively exploring improvements toalgorithmic structures, operators, search patterns, etc., to enhance theiroptimization performance. Reinforcement learning (RL) integrated as a componentin the EA framework has demonstrated superior performance in recent years. Thispaper presents a comprehensive survey on the integration of reinforcementlearning into the evolutionary algorithm, referred to as reinforcementlearning-assisted evolutionary algorithm (RL-EA). Firstly, we introducereinforcement learning and the evolutionary algorithm. We then provide ataxonomy of RL-EA. We then discuss the RL-EA integration method, theRL-assisted strategy adopted by RL-EA, and its applications according to theexisting literature. The RL-assisted strategy is divided according to theimplemented functions including the solution generation, learnable objectivefunction, algorithm/operator/sub-population selection, parameter adaptation,and other strategies. Subsequently, other attribute settings of RL in RL-EA arediscussed. Finally, we analyze potential directions for future research. Thispaper serves as a comprehensive resource for researchers who are interested inRL-EA as it provides an overview of the current state-of-the-art and highlightsthe associated challenges. By leveraging this survey, readers can swiftly gaininsights into RL-EA to develop efficient algorithms, thereby fostering furtheradvancements in this emerging field.</description><author>Yanjie Song, Yutong Wu, Yangyang Guo, Ran Yan, P. N. Suganthan, Yue Zhang, Witold Pedrycz, Yingwu Chen, Swagatam Das, Rammohan Mallipeddi, Oladayo Solomon Ajani</author><pubDate>Fri, 25 Aug 2023 16:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13420v1</guid></item><item><title>Nougat: Neural Optical Understanding for Academic Documents</title><link>http://arxiv.org/abs/2308.13418v1</link><description>Scientific knowledge is predominantly stored in books and scientificjournals, often in the form of PDFs. However, the PDF format leads to a loss ofsemantic information, particularly for mathematical expressions. We proposeNougat (Neural Optical Understanding for Academic Documents), a VisualTransformer model that performs an Optical Character Recognition (OCR) task forprocessing scientific documents into a markup language, and demonstrate theeffectiveness of our model on a new dataset of scientific documents. Theproposed approach offers a promising solution to enhance the accessibility ofscientific knowledge in the digital age, by bridging the gap betweenhuman-readable documents and machine-readable text. We release the models andcode to accelerate future work on scientific text recognition.</description><author>Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic</author><pubDate>Fri, 25 Aug 2023 16:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13418v1</guid></item><item><title>LATFormer: Locality-Aware Point-View Fusion Transformer for 3D Shape Recognition</title><link>http://arxiv.org/abs/2109.01291v2</link><description>Recently, 3D shape understanding has achieved significant progress due to theadvances of deep learning models on various data formats like images, voxels,and point clouds. Among them, point clouds and multi-view images are twocomplementary modalities of 3D objects and learning representations by fusingboth of them has been proven to be fairly effective. While prior workstypically focus on exploiting global features of the two modalities, herein weargue that more discriminative features can be derived by modeling ``where tofuse''. To investigate this, we propose a novel Locality-Aware Point-ViewFusion Transformer (LATFormer) for 3D shape retrieval and classification. Thecore component of LATFormer is a module named Locality-Aware Fusion (LAF) whichintegrates the local features of correlated regions across the two modalitiesbased on the co-occurrence scores. We further propose to filter out scores withlow values to obtain salient local co-occurring regions, which reducesredundancy for the fusion process. In our LATFormer, we utilize the LAF moduleto fuse the multi-scale features of the two modalities both bidirectionally andhierarchically to obtain more informative features. Comprehensive experimentson four popular 3D shape benchmarks covering 3D object retrieval andclassification validate its effectiveness.</description><author>Xinwei He, Silin Cheng, Dingkang Liang, Song Bai, Xi Wang, Yingying Zhu</author><pubDate>Fri, 25 Aug 2023 16:02:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.01291v2</guid></item><item><title>Pathology Steered Stratification Network for Subtype Identification in Alzheimer's Disease</title><link>http://arxiv.org/abs/2210.05880v2</link><description>Alzheimer's disease (AD) is a heterogeneous, multifactorial neurodegenerativedisorder characterized by beta-amyloid, pathologic tau, and neurodegeneration.There are no effective treatments for Alzheimer's disease at a late stage,urging for early intervention. However, existing statistical inferenceapproaches of AD subtype identification ignore the pathological domainknowledge, which could lead to ill-posed results that are sometimesinconsistent with the essential neurological principles. Integrating systemsbiology modeling with machine learning, we propose a novel pathology steeredstratification network (PSSN) that incorporates established domain knowledge inAD pathology through a reaction-diffusion model, where we consider non-linearinteractions between major biomarkers and diffusion along brain structuralnetwork. Trained on longitudinal multimodal neuroimaging data, the biologicalmodel predicts long-term trajectories that capture individual progressionpattern, filling in the gaps between sparse imaging data available. A deeppredictive neural network is then built to exploit spatiotemporal dynamics,link neurological examinations with clinical profiles, and generate subtypeassignment probability on an individual basis. We further identify anevolutionary disease graph to quantify subtype transition probabilities throughextensive simulations. Our stratification achieves superior performance in bothinter-cluster heterogeneity and intra-cluster homogeneity of various clinicalscores. Applying our approach to enriched samples of aging populations, weidentify six subtypes spanning AD spectrum, where each subtype exhibits adistinctive biomarker pattern that is consistent with its clinical outcome.PSSN provides insights into pre-symptomatic diagnosis and practical guidance onclinical treatments, which may be further generalized to otherneurodegenerative diseases.</description><author>Enze Xu, Jingwen Zhang, Jiadi Li, Qianqian Song, Defu Yang, Guorong Wu, Minghan Chen</author><pubDate>Fri, 25 Aug 2023 15:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05880v2</guid></item><item><title>What's the Difference? The potential for Convolutional Neural Networks for transient detection without template subtraction</title><link>http://arxiv.org/abs/2203.07390v3</link><description>We present a study of the potential for Convolutional Neural Networks (CNNs)to enable separation of astrophysical transients from image artifacts, a taskknown as "real-bogus" classification without requiring a template subtracted(or difference) image which requires a computationally expensive process togenerate, involving image matching on small spatial scales in large volumes ofdata. Using data from the Dark Energy Survey, we explore the use of CNNs to (1)automate the "real-bogus" classification, (2) reduce the computational costs oftransient discovery. We compare the efficiency of two CNNs with similararchitectures, one that uses "image triplets" (templates, search, anddifference image) and one that takes as input the template and search only. Wemeasure the decrease in efficiency associated with the loss of information ininput finding that the testing accuracy is reduced from 96% to 91.1%. Wefurther investigate how the latter model learns the required information fromthe template and search by exploring the saliency maps. Our work (1) confirmsthat CNNs are excellent models for "real-bogus" classification that relyexclusively on the imaging data and require no feature engineering task; (2)demonstrates that high-accuracy (&gt; 90%) models can be built without the need toconstruct difference images, but some accuracy is lost. Since once trained,neural networks can generate predictions at minimal computational costs, weargue that future implementations of this methodology could dramatically reducethe computational costs in the detection of transients in synoptic surveys likeRubin Observatory's Legacy Survey of Space and Time by bypassing the DifferenceImage Analysis entirely.</description><author>Tatiana Acero-Cuellar, Federica Bianco, Gregory Dobler, Masao Sako, Helen Qu, The LSST Dark Energy Science Collaboration</author><pubDate>Fri, 25 Aug 2023 15:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07390v3</guid></item><item><title>SoTaNa: The Open-Source Software Development Assistant</title><link>http://arxiv.org/abs/2308.13416v1</link><description>Software development plays a crucial role in driving innovation andefficiency across modern societies. To meet the demands of this dynamic field,there is a growing need for an effective software development assistant.However, existing large language models represented by ChatGPT suffer fromlimited accessibility, including training data and model weights. Althoughother large open-source models like LLaMA have shown promise, they stillstruggle with understanding human intent. In this paper, we present SoTaNa, anopen-source software development assistant. SoTaNa utilizes ChatGPT to generatehigh-quality instruction-based data for the domain of software engineering andemploys a parameter-efficient fine-tuning approach to enhance the open-sourcefoundation model, LLaMA. We evaluate the effectiveness of \our{} in answeringStack Overflow questions and demonstrate its capabilities. Additionally, wediscuss its capabilities in code summarization and generation, as well as theimpact of varying the volume of generated data on model performance. Notably,SoTaNa can run on a single GPU, making it accessible to a broader range ofresearchers. Our code, model weights, and data are public at\url{https://github.com/DeepSoftwareAnalytics/SoTaNa}.</description><author>Ensheng Shi, Fengji Zhang, Yanlin Wang, Bei Chen, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun</author><pubDate>Fri, 25 Aug 2023 15:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13416v1</guid></item><item><title>An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation</title><link>http://arxiv.org/abs/2308.13415v1</link><description>In medical imaging, artificial intelligence (AI) is increasingly being usedto automate routine tasks. However, these algorithms can exhibit and exacerbatebiases which lead to disparate performances between protected groups. Weinvestigate the impact of model choice on how imbalances in subject sex andrace in training datasets affect AI-based cine cardiac magnetic resonance imagesegmentation. We evaluate three convolutional neural network-based models andone vision transformer model. We find significant sex bias in three of the fourmodels and racial bias in all of the models. However, the severity and natureof the bias varies between the models, highlighting the importance of modelchoice when attempting to train fair AI-based segmentation models for medicalimaging tasks.</description><author>Tiarna Lee, Esther Puyol-Antón, Bram Ruijsink, Keana Aitcheson, Miaojing Shi, Andrew P. King</author><pubDate>Fri, 25 Aug 2023 15:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13415v1</guid></item><item><title>Resource-Adaptive Newton's Method for Distributed Learning</title><link>http://arxiv.org/abs/2308.10154v2</link><description>Distributed stochastic optimization methods based on Newton's method offersignificant advantages over first-order methods by leveraging curvatureinformation for improved performance. However, the practical applicability ofNewton's method is hindered in large-scale and heterogeneous learningenvironments due to challenges such as high computation and communication costsassociated with the Hessian matrix, sub-model diversity, staleness in training,and data heterogeneity. To address these challenges, this paper introduces anovel and efficient algorithm called RANL, which overcomes the limitations ofNewton's method by employing a simple Hessian initialization and adaptiveassignments of training regions. The algorithm demonstrates impressiveconvergence properties, which are rigorously analyzed under standardassumptions in stochastic optimization. The theoretical analysis establishesthat RANL achieves a linear convergence rate while effectively adapting toavailable resources and maintaining high efficiency. Unlike traditionalfirst-order methods, RANL exhibits remarkable independence from the conditionnumber of the problem and eliminates the need for complex parameter tuning.These advantages make RANL a promising approach for distributed stochasticoptimization in practical scenarios.</description><author>Shuzhen Chen, Yuan Yuan, Youming Tao, Zhipeng Cai, Dongxiao Yu</author><pubDate>Fri, 25 Aug 2023 15:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10154v2</guid></item><item><title>LExecutor: Learning-Guided Execution</title><link>http://arxiv.org/abs/2302.02343v3</link><description>Executing code is essential for various program analysis tasks, e.g., todetect bugs that manifest through exceptions or to obtain execution traces forfurther dynamic analysis. However, executing an arbitrary piece of code isoften difficult in practice, e.g., because of missing variable definitions,missing user inputs, and missing third-party dependencies. This paper presentsLExecutor, a learning-guided approach for executing arbitrary code snippets inan underconstrained way. The key idea is to let a neural model predict missingvalues that otherwise would cause the program to get stuck, and to inject thesevalues into the execution. For example, LExecutor injects likely values forotherwise undefined variables and likely return values of calls to otherwisemissing functions. We evaluate the approach on Python code from popularopen-source projects and on code snippets extracted from Stack Overflow. Theneural model predicts realistic values with an accuracy between 79.5% and98.2%, allowing LExecutor to closely mimic real executions. As a result, theapproach successfully executes significantly more code than any availabletechnique, such as simply executing the code as-is. For example, executing theopen-source code snippets as-is covers only 4.1% of all lines, because the codecrashes early on, whereas LExecutor achieves a coverage of 51.6%.</description><author>Beatriz Souza, Michael Pradel</author><pubDate>Fri, 25 Aug 2023 15:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02343v3</guid></item><item><title>Characteristics of networks generated by kernel growing neural gas</title><link>http://arxiv.org/abs/2308.08163v2</link><description>This research aims to develop kernel GNG, a kernelized version of the growingneural gas (GNG) algorithm, and to investigate the features of the networksgenerated by the kernel GNG. The GNG is an unsupervised artificial neuralnetwork that can transform a dataset into an undirected graph, therebyextracting the features of the dataset as a graph. The GNG is widely used invector quantization, clustering, and 3D graphics. Kernel methods are often usedto map a dataset to feature space, with support vector machines being the mostprominent application. This paper introduces the kernel GNG approach andexplores the characteristics of the networks generated by kernel GNG. Fivekernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and logkernels, are used in this study. The results of this study show that theaverage degree and the average clustering coefficient decrease as the kernelparameter increases for Gaussian, Laplacian, Cauchy, and IMQ kernels. If weavoid more edges and a higher clustering coefficient (or more triangles), thekernel GNG with a larger value of the parameter will be more appropriate.</description><author>Kazuhisa Fujita</author><pubDate>Fri, 25 Aug 2023 15:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08163v2</guid></item><item><title>Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning</title><link>http://arxiv.org/abs/2308.13411v1</link><description>Glaucoma is the number one cause of irreversible blindness globally. A majorchallenge for accurate glaucoma detection and progression forecasting is thebottleneck of limited labeled patients with the state-of-the-art (SOTA) 3Dretinal imaging data of optical coherence tomography (OCT). To address the datascarcity issue, this paper proposes two solutions. First, we develop a novelgeneralization-reinforced semi-supervised learning (SSL) model called pseudosupervisor to optimally utilize unlabeled data. Compared with SOTA models, theproposed pseudo supervisor optimizes the policy of predicting pseudo labelswith unlabeled samples to improve empirical generalization. Our pseudosupervisor model is evaluated with two clinical tasks consisting of glaucomadetection and progression forecasting. The progression forecasting task isevaluated both unimodally and multimodally. Our pseudo supervisor modeldemonstrates superior performance than SOTA SSL comparison models. Moreover,our model also achieves the best results on the publicly available LAG fundusdataset. Second, we introduce the Harvard Glaucoma Detection and Progression(Harvard-GDP) Dataset, a multimodal multitask dataset that includes data from1,000 patients with OCT imaging data, as well as labels for glaucoma detectionand progression. This is the largest glaucoma detection dataset with 3D OCTimaging data and the first glaucoma progression forecasting dataset that ispublicly available. Detailed sex and racial analysis are provided, which can beused by interested researchers for fairness learning studies. Our releaseddataset is benchmarked with several SOTA supervised CNN and transformer deeplearning models. The dataset and code are made publicly available via\url{https://ophai.hms.harvard.edu/datasets/harvard-gdp1000}.</description><author>Yan Luo, Min Shi, Yu Tian, Tobias Elze, Mengyu Wang</author><pubDate>Fri, 25 Aug 2023 15:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13411v1</guid></item><item><title>Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction</title><link>http://arxiv.org/abs/2304.06714v4</link><description>3D-aware image synthesis encompasses a variety of tasks, such as scenegeneration and novel view synthesis from images. Despite numerous task-specificmethods, developing a comprehensive model remains challenging. In this paper,we present SSDNeRF, a unified approach that employs an expressive diffusionmodel to learn a generalizable prior of neural radiance fields (NeRF) frommulti-view images of diverse objects. Previous studies have used two-stageapproaches that rely on pretrained NeRFs as real data to train diffusionmodels. In contrast, we propose a new single-stage training paradigm with anend-to-end objective that jointly optimizes a NeRF auto-decoder and a latentdiffusion model, enabling simultaneous 3D reconstruction and prior learning,even from sparsely available views. At test time, we can directly sample thediffusion prior for unconditional generation, or combine it with arbitraryobservations of unseen objects for NeRF reconstruction. SSDNeRF demonstratesrobust results comparable to or better than leading task-specific methods inunconditional generation and single/sparse-view 3D reconstruction.</description><author>Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su</author><pubDate>Fri, 25 Aug 2023 15:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06714v4</guid></item><item><title>Using Visual and Vehicular Sensors for Driver Behavior Analysis: A Survey</title><link>http://arxiv.org/abs/2308.13406v1</link><description>Risky drivers account for 70% of fatal accidents in the United States. Withrecent advances in sensors and intelligent vehicular systems, there has beensignificant research on assessing driver behavior to improve drivingexperiences and road safety. This paper examines the various techniques used toanalyze driver behavior using visual and vehicular data, providing an overviewof the latest research in this field. The paper also discusses the challengesand open problems in the field and offers potential recommendations for futureresearch. The survey concludes that integrating vision and vehicularinformation can significantly enhance the accuracy and effectiveness of driverbehavior analysis, leading to improved safety measures and reduced trafficaccidents.</description><author>Bikram Adhikari</author><pubDate>Fri, 25 Aug 2023 15:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13406v1</guid></item><item><title>Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding</title><link>http://arxiv.org/abs/2304.00058v2</link><description>Contrastive learning has shown promising potential for learning robustrepresentations by utilizing unlabeled data. However, constructing effectivepositive-negative pairs for contrastive learning on facial behavior datasetsremains challenging. This is because such pairs inevitably encode thesubject-ID information, and the randomly constructed pairs may push similarfacial images away due to the limited number of subjects in facial behaviordatasets. To address this issue, we propose to utilize activity descriptions,coarse-grained information provided in some datasets, which can providehigh-level semantic information about the image sequences but is oftenneglected in previous studies. More specifically, we introduce a two-stageContrastive Learning with Text-Embeded framework for Facial behaviorunderstanding (CLEF). The first stage is a weakly-supervised contrastivelearning method that learns representations from positive-negative pairsconstructed using coarse-grained activity information. The second stage aims totrain the recognition of facial expressions or facial action units bymaximizing the similarity between image and the corresponding text label names.The proposed CLEF achieves state-of-the-art performance on three in-the-labdatasets for AU recognition and three in-the-wild datasets for facialexpression recognition.</description><author>Xiang Zhang, Taoyue Wang, Xiaotian Li, Huiyuan Yang, Lijun Yin</author><pubDate>Fri, 25 Aug 2023 15:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00058v2</guid></item><item><title>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</title><link>http://arxiv.org/abs/2308.06093v2</link><description>Structural re-parameterization is a general training scheme for ConvolutionalNeural Networks (CNNs), which achieves performance improvement withoutincreasing inference cost. As Vision Transformers (ViTs) are graduallysurpassing CNNs in various visual tasks, one may question: if a training schemespecifically for ViTs exists that can also achieve performance improvementwithout increasing inference cost? Recently, Mixture-of-Experts (MoE) hasattracted increasing attention, as it can efficiently scale up the capacity ofTransformers at a fixed cost through sparsely activated experts. Consideringthat MoE can also be viewed as a multi-branch structure, can we utilize MoE toimplement a ViT training scheme similar to structural re-parameterization? Inthis paper, we affirmatively answer these questions, with a new generaltraining strategy for ViTs. Specifically, we decouple the training andinference phases of ViTs. During training, we replace some Feed-ForwardNetworks (FFNs) of the ViT with specially designed, more efficient MoEs thatassign tokens to experts by random uniform partition, and perform ExpertsWeights Averaging (EWA) on these MoEs at the end of each iteration. Aftertraining, we convert each MoE into an FFN by averaging the experts,transforming the model back into original ViT for inference. We further providea theoretical analysis to show why and how it works. Comprehensive experimentsacross various 2D and 3D visual tasks, ViT architectures, and datasets validatethe effectiveness and generalizability of the proposed training scheme.Besides, our training scheme can also be applied to improve performance whenfine-tuning ViTs. Lastly, but equally important, the proposed EWA technique cansignificantly improve the effectiveness of naive MoE in various 2D visual smalldatasets and 3D visual tasks.</description><author>Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Tong He, Wanli Ouyang</author><pubDate>Fri, 25 Aug 2023 15:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06093v2</guid></item><item><title>Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2302.14833v2</link><description>Autonomous Mobility-on-Demand (AMoD) systems are an evolving mode oftransportation in which a centrally coordinated fleet of self-driving vehiclesdynamically serves travel requests. The control of these systems is typicallyformulated as a large network optimization problem, and reinforcement learning(RL) has recently emerged as a promising approach to solve the open challengesin this space. Recent centralized RL approaches focus on learning from onlinedata, ignoring the per-sample-cost of interactions within real-worldtransportation systems. To address these limitations, we propose to formalizethe control of AMoD systems through the lens of offline reinforcement learningand learn effective control strategies using solely offline data, which isreadily available to current mobility operators. We further investigate designdecisions and provide empirical evidence based on data from real-world mobilitysystems showing how offline learning allows to recover AMoD control policiesthat (i) exhibit performance on par with online methods, (ii) allow forsample-efficient online fine-tuning and (iii) eliminate the need for complexsimulation environments. Crucially, this paper demonstrates that offline RL isa promising paradigm for the application of RL-based solutions withineconomically-critical systems, such as mobility systems.</description><author>Carolin Schmidt, Daniele Gammelli, Francisco Camara Pereira, Filipe Rodrigues</author><pubDate>Fri, 25 Aug 2023 15:28:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14833v2</guid></item><item><title>Relighting Neural Radiance Fields with Shadow and Highlight Hints</title><link>http://arxiv.org/abs/2308.13404v1</link><description>This paper presents a novel neural implicit radiance representation for freeviewpoint relighting from a small set of unstructured photographs of an objectlit by a moving point light source different from the view position. We expressthe shape as a signed distance function modeled by a multi layer perceptron. Incontrast to prior relightable implicit neural representations, we do notdisentangle the different reflectance components, but model both the local andglobal reflectance at each point by a second multi layer perceptron that, inaddition, to density features, the current position, the normal (from thesigned distace function), view direction, and light position, also takes shadowand highlight hints to aid the network in modeling the corresponding highfrequency light transport effects. These hints are provided as a suggestion,and we leave it up to the network to decide how to incorporate these in thefinal relit result. We demonstrate and validate our neural implicitrepresentation on synthetic and real scenes exhibiting a wide variety ofshapes, material properties, and global illumination light transport.</description><author>Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong</author><pubDate>Fri, 25 Aug 2023 15:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13404v1</guid></item><item><title>EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression</title><link>http://arxiv.org/abs/2308.13399v1</link><description>We propose an unsupervised method to extract keywords and keyphrases fromtexts based on a pre-trained language model (LM) and Shannon's informationmaximization. Specifically, our method extracts phrases having the highestconditional entropy under the LM. The resulting set of keyphrases turns out tosolve a relevant information-theoretic problem: if provided as sideinformation, it leads to the expected minimal binary code length in compressingthe text using the LM and an entropy encoder. Alternately, the resulting set isan approximation via a causal LM to the set of phrases that minimize theentropy of the text when conditioned upon it. Empirically, the method providesresults comparable to the most commonly used methods in various keyphraseextraction benchmark challenges.</description><author>Alexander Tsvetkov. Alon Kipnis</author><pubDate>Fri, 25 Aug 2023 15:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13399v1</guid></item><item><title>Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales</title><link>http://arxiv.org/abs/2302.08961v2</link><description>The quality of text-to-image generation is continuously improving, yet theboundaries of its applicability are still unclear. In particular, refinement ofthe text input with the objective of achieving better results - commonly calledprompt engineering - so far seems to have not been geared towards work withpre-existing texts. We investigate whether text-to-image generation and promptengineering could be used to generate basic illustrations of popularfairytales. Using Midjourney v4, we engage in action research with a dual aim:to attempt to generate 5 believable illustrations for each of 5 popularfairytales, and to define a prompt engineering process that starts from apre-existing text and arrives at an illustration of it. We arrive at atentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)style refinement, and iv) variation selection. We also discuss three reasonswhy the generation model struggles with certain illustrations: difficultieswith counts, bias from stereotypical configurations and inability to depictoverly fantastic situations. Our findings are not limited to the specificgeneration model and are intended to be generalisable to future ones.</description><author>Martin Ruskov</author><pubDate>Fri, 25 Aug 2023 15:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08961v2</guid></item><item><title>PMC-LLaMA: Towards Building Open-source Language Models for Medicine</title><link>http://arxiv.org/abs/2304.14454v3</link><description>Recently, Large Language Models (LLMs) have showcased remarkable capabilitiesin natural language understanding. While demonstrating proficiency in everydayconversations and question-answering situations, these models frequentlystruggle in domains that require precision, such as medical applications, dueto their lack of domain-specific knowledge. In this paper, we describe theprocedure for building a powerful, open-source language model specificallydesigned for medicine applications, termed as PMC-LLaMA. Our contributions arethreefold: (i) we systematically investigate the process of adapting ageneral-purpose foundation language model towards medical domain, this involvesdata-centric knowledge injection through the integration of 4.8M biomedicalacademic papers and 30K medical textbooks, as well as comprehensive fine-tuningfor alignment with domain-specific instructions; (ii) we contribute alarge-scale, comprehensive dataset for instruction tuning. This datasetencompasses medical question-answering (QA), rationale for reasoning, andconversational dialogues, comprising a total of 202M tokens; (iii) we conductthorough ablation studies to demonstrate the effectiveness of each proposedcomponent. While evaluating on various public medical question-answeringbenchmarks, our lightweight PMCLLaMA, which consists of only 13 billionparameters, exhibits superior performance, even surpassing ChatGPT. All models,codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.</description><author>Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Fri, 25 Aug 2023 15:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14454v3</guid></item><item><title>Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features</title><link>http://arxiv.org/abs/2308.13392v1</link><description>Whilst contrastive learning yields powerful representations by matchingdifferent augmented views of the same instance, it lacks the ability to capturethe similarities between different instances. One popular way to address thislimitation is by learning global features (after the global pooling) to captureinter-instance relationships based on knowledge distillation, where the globalfeatures of the teacher are used to guide the learning of the global featuresof the student. Inspired by cross-modality learning, we extend this existingframework that only learns from global features by encouraging the globalfeatures and intermediate layer features to learn from each other. This leadsto our novel self-supervised framework: cross-context learning between globaland hypercolumn features (CGH), that enforces the consistency of instancerelations between low- and high-level semantics. Specifically, we stack theintermediate feature maps to construct a hypercolumn representation so that wecan measure instance relations using two contexts (hypercolumn and globalfeature) separately, and then use the relations of one context to guide thelearning of the other. This cross-context learning allows the model to learnfrom the differences between the two contexts. The experimental results onlinear classification and downstream tasks show that our method outperforms thestate-of-the-art methods.</description><author>Zheng Gao, Chen Feng, Ioannis Patras</author><pubDate>Fri, 25 Aug 2023 15:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13392v1</guid></item><item><title>Direction-aware Video Demoireing with Temporal-guided Bilateral Learning</title><link>http://arxiv.org/abs/2308.13388v1</link><description>Moire patterns occur when capturing images or videos on screens, severelydegrading the quality of the captured images or videos. Despite the recentprogresses, existing video demoireing methods neglect the physicalcharacteristics and formation process of moire patterns, significantly limitingthe effectiveness of video recovery. This paper presents a unified framework,DTNet, a direction-aware and temporal-guided bilateral learning network forvideo demoireing. DTNet effectively incorporates the process of moire patternremoval, alignment, color correction, and detail refinement. Our proposed DTNetcomprises two primary stages: Frame-level Direction-aware Demoireing andAlignment (FDDA) and Tone and Detail Refinement (TDR). In FDDA, we employmultiple directional DCT modes to perform the moire pattern removal process inthe frequency domain, effectively detecting the prominent moire edges. Then,the coarse and fine-grained alignment is applied on the demoired features forfacilitating the utilization of neighboring information. In TDR, we propose atemporal-guided bilateral learning pipeline to mitigate the degradation ofcolor and details caused by the moire patterns while preserving the restoredfrequency information in FDDA. Guided by the aligned temporal features fromFDDA, the affine transformations for the recovery of the ultimate clean framesare learned in TDR. Extensive experiments demonstrate that our video demoireingmethod outperforms state-of-the-art approaches by 2.3 dB in PSNR, and alsodelivers a superior visual experience.</description><author>Shuning Xu, Binbin Song, Xiangyu Chen, Jiantao Zhou</author><pubDate>Fri, 25 Aug 2023 15:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13388v1</guid></item><item><title>Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs</title><link>http://arxiv.org/abs/2308.13387v1</link><description>With the rapid evolution of large language models (LLMs), new andhard-to-predict harmful capabilities are emerging. This requires developers tobe able to identify risks through the evaluation of "dangerous capabilities" inorder to responsibly deploy LLMs. In this work, we collect the firstopen-source dataset to evaluate safeguards in LLMs, and deploy saferopen-source LLMs at a low cost. Our dataset is curated and filtered to consistonly of instructions that responsible language models should not follow. Weannotate and assess the responses of six popular LLMs to these instructions.Based on our annotation, we proceed to train several BERT-like classifiers, andfind that these small classifiers can achieve results that are comparable withGPT-4 on automatic safety evaluation. Warning: this paper contains example datathat may be offensive, harmful, or biased.</description><author>Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin</author><pubDate>Fri, 25 Aug 2023 15:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13387v1</guid></item><item><title>TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting</title><link>http://arxiv.org/abs/2308.13386v1</link><description>Long-term time series forecasting is a vital task and has a wide range ofreal applications. Recent methods focus on capturing the underlying patternsfrom one single domain (e.g. the time domain or the frequency domain), and havenot taken a holistic view to process long-term time series from thetime-frequency domains. In this paper, we propose a Time-Frequency EnhancedDecomposed Network (TFDNet) to capture both the long-term underlying patternsand temporal periodicity from the time-frequency domain. In TFDNet, we devise amulti-scale time-frequency enhanced encoder backbone and develop two separatetrend and seasonal time-frequency blocks to capture the distinct patternswithin the decomposed trend and seasonal components in multi-resolutions.Diverse kernel learning strategies of the kernel operations in time-frequencyblocks have been explored, by investigating and incorporating the potentialdifferent channel-wise correlation patterns of multivariate time series.Experimental evaluation of eight datasets from five benchmark domainsdemonstrated that TFDNet is superior to state-of-the-art approaches in botheffectiveness and efficiency.</description><author>Yuxiao Luo, Ziyu Lyu, Xingyu Huang</author><pubDate>Fri, 25 Aug 2023 15:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13386v1</guid></item><item><title>Modern Constraint Programming Education: Lessons for the Future</title><link>http://arxiv.org/abs/2306.13676v2</link><description>This paper details an outlook on modern constraint programming (CP) educationthrough the lens of a CP instructor. A general overview of current CP coursesand instructional methods is presented, with a focus on online andvirtually-delivered courses. This is followed by a discussion of the novelapproach taken to introductory CP education for engineering students at largescale at the Georgia Institute of Technology (Georgia Tech) in Atlanta, GA,USA. The paper summarizes important takeaways from the Georgia Tech CP courseand ends with a discussion on the future of CP education. Some ideas forinstructional methods, promotional methods, and organizational changes areproposed to aid in the long-term growth of CP education.</description><author>Tejas Santanam, Pascal Van Hentenryck</author><pubDate>Fri, 25 Aug 2023 15:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13676v2</guid></item><item><title>Class-relation Knowledge Distillation for Novel Class Discovery</title><link>http://arxiv.org/abs/2307.09158v3</link><description>We tackle the problem of novel class discovery, which aims to learn novelclasses without supervision based on labeled data from known classes. A keychallenge lies in transferring the knowledge in the known-class data to thelearning of novel classes. Previous methods mainly focus on building a sharedrepresentation space for knowledge transfer and often ignore modeling classrelations. To address this, we introduce a class relation representation forthe novel classes based on the predicted class distribution of a model trainedon known classes. Empirically, we find that such class relation becomes lessinformative during typical discovery training. To prevent such informationloss, we propose a novel knowledge distillation framework, which utilizes ourclass-relation representation to regularize the learning of novel classes. Inaddition, to enable a flexible knowledge distillation scheme for each datapoint in novel classes, we develop a learnable weighting function for theregularization, which adaptively promotes knowledge transfer based on thesemantic similarity between the novel and known classes. To validate theeffectiveness and generalization of our method, we conduct extensiveexperiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, andFGVC-Aircraft datasets. Our results demonstrate that the proposed methodoutperforms the previous state-of-the-art methods by a significant margin onalmost all benchmarks. Code is available at\href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.</description><author>Peiyan Gu, Chuyu Zhang, Ruijie Xu, Xuming He</author><pubDate>Fri, 25 Aug 2023 14:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09158v3</guid></item><item><title>ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task</title><link>http://arxiv.org/abs/2307.06954v2</link><description>Conspiracy Theory Identication task is a new shared task proposed for thefirst time at the Evalita 2023. The ACTI challenge, based exclusively oncomments published on conspiratorial channels of telegram, is divided into twosubtasks: (i) Conspiratorial Content Classification: identifying conspiratorialcontent and (ii) Conspiratorial Category Classification about specificconspiracy theory classification. A total of fifteen teams participated in thetask for a total of 81 submissions. We illustrate the best performingapproaches were based on the utilization of large language models. We finallydraw conclusions about the utilization of these models for counteracting thespreading of misinformation in online platforms.</description><author>Giuseppe Russo, Niklas Stoehr, Manoel Horta Ribeiro</author><pubDate>Fri, 25 Aug 2023 14:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06954v2</guid></item><item><title>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation</title><link>http://arxiv.org/abs/2308.11561v3</link><description>This report details the methods of the winning entry of the AVDN Challenge inICCV CLVL 2023. The competition addresses the Aerial Navigation from DialogHistory (ANDH) task, which requires a drone agent to associate dialog historywith aerial observations to reach the destination. For better cross-modalgrounding abilities of the drone agent, we propose a Target-GroundedGraph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leveragesa graph-aware transformer to capture spatiotemporal dependency, which benefitsnavigation state tracking and robust action planning. In addition,an auxiliaryvisual grounding task is devised to boost the agent's awareness of referredlandmarks. Moreover, a hybrid augmentation strategy based on large languagemodels is utilized to mitigate data scarcity limitations. Our TG-GAT frameworkwon the AVDN Challenge, with 2.2% and 3.0% absolute improvements over thebaseline on SPL and SR metrics, respectively. The code is available athttps://github.com/yifeisu/TG-GAT.</description><author>Yifei Su, Dong An, Yuan Xu, Kehan Chen, Yan Huang</author><pubDate>Fri, 25 Aug 2023 14:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11561v3</guid></item><item><title>Novel Class Discovery for Long-tailed Recognition</title><link>http://arxiv.org/abs/2308.02989v3</link><description>While the novel class discovery has recently made great progress, existingmethods typically focus on improving algorithms on class-balanced benchmarks.However, in real-world recognition tasks, the class distributions of theircorresponding datasets are often imbalanced, which leads to serious performancedegeneration of those methods. In this paper, we consider a more realisticsetting for novel class discovery where the distributions of novel and knownclasses are long-tailed. One main challenge of this new problem is to discoverimbalanced novel classes with the help of long-tailed known classes. To tacklethis problem, we propose an adaptive self-labeling strategy based on anequiangular prototype representation of classes. Our method infers high-qualitypseudo-labels for the novel classes by solving a relaxed optimal transportproblem and effectively mitigates the class biases in learning the known andnovel classes. We perform extensive experiments on CIFAR100, ImageNet100,Herbarium19 and large-scale iNaturalist18 datasets, and the results demonstratethe superiority of our method. Our code is available athttps://github.com/kleinzcy/NCDLR.</description><author>Chuyu Zhang, Ruijie Xu, Xuming He</author><pubDate>Fri, 25 Aug 2023 14:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02989v3</guid></item><item><title>Assessing Keyness using Permutation Tests</title><link>http://arxiv.org/abs/2308.13383v1</link><description>We propose a resampling-based approach for assessing keyness in corpuslinguistics based on suggestions by Gries (2006, 2022). Traditional approachesbased on hypothesis tests (e.g. Likelihood Ratio) model the copora asindependent identically distributed samples of tokens. This model does notaccount for the often observed uneven distribution of occurences of a wordacross a corpus. When occurences of a word are concentrated in few documents,large values of LLR and similar scores are in fact much more likely thanaccounted for by the token-by-token sampling model, leading to false positives. We replace the token-by-token sampling model by a model where corpora aresamples of documents rather than tokens, which is much closer to the waycorpora are actually assembled. We then use a permutation approach toapproximate the distribution of a given keyness score under the null hypothesisof equal frequencies and obtain p-values for assessing significance. We do notneed any assumption on how the tokens are organized within or across documents,and the approach works with basically *any* keyness score. Hence, appart fromobtaining more accurate p-values for scores like LLR, we can also assesssignificance for e.g. the logratio which has been proposed as a measure ofeffect size. An efficient implementation of the proposed approach is provided in the `R`package `keyperm` available from github.</description><author>Thoralf Mildenberger</author><pubDate>Fri, 25 Aug 2023 14:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13383v1</guid></item><item><title>Prompting Visual-Language Models for Dynamic Facial Expression Recognition</title><link>http://arxiv.org/abs/2308.13382v1</link><description>This paper presents a novel visual-language model called DFER-CLIP, which isbased on the CLIP model and designed for in-the-wild Dynamic Facial ExpressionRecognition (DFER). Specifically, the proposed DFER-CLIP consists of a visualpart and a textual part. For the visual part, based on the CLIP image encoder,a temporal model consisting of several Transformer encoders is introduced forextracting temporal facial expression features, and the final feature embeddingis obtained as a learnable "class" token. For the textual part, we use asinputs textual descriptions of the facial behaviour that is related to theclasses (facial expressions) that we are interested in recognising -- thosedescriptions are generated using large language models, like ChatGPT. This, incontrast to works that use only the class names and more accurately capturesthe relationship between them. Alongside the textual description, we introducea learnable token which helps the model learn relevant context information foreach expression during training. Extensive experiments demonstrate theeffectiveness of the proposed method and show that our DFER-CLIP also achievesstate-of-the-art results compared with the current supervised DFER methods onthe DFEW, FERV39k, and MAFW benchmarks. Code is publicly available athttps://github.com/zengqunzhao/DFER-CLIP.</description><author>Zengqun Zhao, Ioannis Patras</author><pubDate>Fri, 25 Aug 2023 14:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13382v1</guid></item><item><title>Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</title><link>http://arxiv.org/abs/2308.06689v2</link><description>Binarization of neural networks is a dominant paradigm in neural networkscompression. The pioneering work BinaryConnect uses Straight Through Estimator(STE) to mimic the gradients of the sign function, but it also causes thecrucial inconsistency problem. Most of the previous methods design differentestimators instead of STE to mitigate it. However, they ignore the fact thatwhen reducing the estimating error, the gradient stability will decreaseconcomitantly. These highly divergent gradients will harm the model trainingand increase the risk of gradient vanishing and gradient exploding. To fullytake the gradient stability into consideration, we present a new perspective tothe BNNs training, regarding it as the equilibrium between the estimating errorand the gradient stability. In this view, we firstly design two indicators toquantitatively demonstrate the equilibrium phenomenon. In addition, in order tobalance the estimating error and the gradient stability well, we revise theoriginal straight through estimator and propose a power function basedestimator, Rectified Straight Through Estimator (ReSTE for short). Comparing toother estimators, ReSTE is rational and capable of flexibly balancing theestimating error with the gradient stability. Extensive experiments on CIFAR-10and ImageNet datasets show that ReSTE has excellent performance and surpassesthe state-of-the-art methods without any auxiliary modules or losses.</description><author>Xiao-Ming Wu, Dian Zheng, Zuhao Liu, Wei-Shi Zheng</author><pubDate>Fri, 25 Aug 2023 14:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06689v2</guid></item><item><title>How to Estimate Model Transferability of Pre-Trained Speech Models?</title><link>http://arxiv.org/abs/2306.01015v2</link><description>In this work, we introduce a "score-based assessment" framework forestimating the transferability of pre-trained speech models (PSMs) forfine-tuning target tasks. We leverage upon two representation theories,Bayesian likelihood estimation and optimal transport, to generate rank scoresfor the PSM candidates using the extracted representations. Our frameworkefficiently computes transferability scores without actual fine-tuning ofcandidate models or layers by making a temporal independent hypothesis. Weevaluate some popular supervised speech models (e.g., Conformer RNN-Transducer)and self-supervised speech models (e.g., HuBERT) in cross-layer and cross-modelsettings using public data. Experimental results show a high Spearman's rankcorrelation and low $p$-value between our estimation framework and fine-tuningground truth. Our proposed transferability framework requires lesscomputational time and resources, making it a resource-saving andtime-efficient approach for tuning speech foundation models.</description><author>Zih-Ching Chen, Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Shou-Yiin Chang, Rohit Prabhavalkar, Hung-yi Lee, Tara N. Sainath</author><pubDate>Fri, 25 Aug 2023 14:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01015v2</guid></item><item><title>In-context learning for model-free system identification</title><link>http://arxiv.org/abs/2308.13380v1</link><description>In traditional system identification, we estimate a model of an unknowndynamical system based on given input/output sequences and available physicalknowledge. Yet, is it also possible to understand the intricacies of dynamicalsystems not solely from their input/output patterns, but by observing thebehavior of other systems within the same class? This central question drivesthe study presented in this paper. In response to this query, we introduce a novel paradigm for systemidentification, addressing two primary tasks: one-step-ahead prediction andmulti-step simulation. Unlike conventional methods, we do not directly estimatea model for the specific system. Instead, we pretrain a meta model thatrepresents a class of dynamical systems. This meta model is trained from apotentially infinite stream of synthetic data, generated by systems randomlyextracted from a certain distribution. At its core, the meta model serves as animplicit representation of the main characteristics of a class of dynamicalsystems. When provided with a brief context from a new system - specifically, ashort input/output sequence - the meta model implicitly discerns its dynamics,enabling predictions of its behavior. The proposed approach harnesses the power of Transformer architectures,renowned for their in-context learning capabilities in Natural LanguageProcessing tasks. For one-step prediction, a GPT-like decoder-only architectureis utilized, whereas the simulation problem employs an encoder-decoderstructure. Initial experimental results affirmatively answer our foundational question,opening doors to fresh research avenues in system identification.</description><author>Marco Forgione, Filippo Pura, Dario Piga</author><pubDate>Fri, 25 Aug 2023 14:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13380v1</guid></item><item><title>Early Stopping for Deep Image Prior</title><link>http://arxiv.org/abs/2112.06074v3</link><description>Deep image prior (DIP) and its variants have showed remarkable potential forsolving inverse problems in computer vision, without any extra training data.Practical DIP models are often substantially overparameterized. During thefitting process, these models learn mostly the desired visual content first,and then pick up the potential modeling and observational noise, i.e.,overfitting. Thus, the practicality of DIP often depends critically on goodearly stopping (ES) that captures the transition period. In this regard, themajority of DIP works for vision tasks only demonstrates the potential of themodels -- reporting the peak performance against the ground truth, but providesno clue about how to operationally obtain near-peak performance without accessto the groundtruth. In this paper, we set to break this practicality barrier ofDIP, and propose an efficient ES strategy, which consistently detects near-peakperformance across several vision tasks and DIP variants. Based on a simplemeasure of dispersion of consecutive DIP reconstructions, our ES method notonly outpaces the existing ones -- which only work in very narrow domains, butalso remains effective when combined with a number of methods that try tomitigate the overfitting. The code is available athttps://github.com/sun-umn/Early_Stopping_for_DIP.</description><author>Hengkang Wang, Taihui Li, Zhong Zhuang, Tiancong Chen, Hengyue Liang, Ju Sun</author><pubDate>Fri, 25 Aug 2023 14:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.06074v3</guid></item><item><title>Q-Learning based system for path planning with unmanned aerial vehicles swarms in obstacle environments</title><link>http://arxiv.org/abs/2303.17655v2</link><description>Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV)swarms are on the rise because of all the advantages they bring. There are moreand more scenarios where autonomous control of multiple UAVs is required. Mostof these scenarios present a large number of obstacles, such as power lines ortrees. If all UAVs can be operated autonomously, personnel expenses can bedecreased. In addition, if their flight paths are optimal, energy consumptionis reduced. This ensures that more battery time is left for other operations.In this paper, a Reinforcement Learning based system is proposed for solvingthis problem in environments with obstacles by making use of Q-Learning. Thismethod allows a model, in this particular case an Artificial Neural Network, toself-adjust by learning from its mistakes and achievements. Regardless of thesize of the map or the number of UAVs in the swarm, the goal of these paths isto ensure complete coverage of an area with fixed obstacles for tasks, likefield prospecting. Setting goals or having any prior information aside from theprovided map is not required. For experimentation, five maps of different sizeswith different obstacles were used. The experiments were performed withdifferent number of UAVs. For the calculation of the results, the number ofactions taken by all UAVs to complete the task in each experiment is taken intoaccount. The lower the number of actions, the shorter the path and the lowerthe energy consumption. The results are satisfactory, showing that the systemobtains solutions in fewer movements the more UAVs there are. For a betterpresentation, these results have been compared to another state-of-the-artapproach.</description><author>Alejandro Puente-Castro, Daniel Rivero, Eurico Pedrosa, Artur Pereira, Nuno Lau, Enrique Fernandez-Blanco</author><pubDate>Fri, 25 Aug 2023 14:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17655v2</guid></item><item><title>Benchmarking Neural Network Generalization for Grammar Induction</title><link>http://arxiv.org/abs/2308.08253v2</link><description>How well do neural networks generalize? Even for grammar induction tasks,where the target generalization is fully known, previous works have left thequestion open, testing very limited ranges beyond the training set and usingdifferent success criteria. We provide a measure of neural networkgeneralization based on fully specified formal languages. Given a model and aformal grammar, the method assigns a generalization score representing how wella model generalizes to unseen samples in inverse relation to the amount of datait was trained on. The benchmark includes languages such as $a^nb^n$,$a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selectedarchitectures using the benchmark and find that networks trained with a MinimumDescription Length objective (MDL) generalize better and using less data thannetworks trained using standard loss functions. The benchmark is available athttps://github.com/taucompling/bliss.</description><author>Nur Lan, Emmanuel Chemla, Roni Katzir</author><pubDate>Fri, 25 Aug 2023 14:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08253v2</guid></item><item><title>Enhanced Mortality Prediction In Patients With Subarachnoid Haemorrhage Using A Deep Learning Model Based On The Initial CT Scan</title><link>http://arxiv.org/abs/2308.13373v1</link><description>PURPOSE: Subarachnoid hemorrhage (SAH) entails high morbidity and mortalityrates. Convolutional neural networks (CNN), a form of deep learning, arecapable of generating highly accurate predictions from imaging data. Ourobjective was to predict mortality in SAH patients by processing the initial CTscan on a CNN based algorithm. METHODS: Retrospective multicentric study of a consecutive cohort of patientswith SAH between 2011-2022. Demographic, clinical and radiological variableswere analyzed. Pre-processed baseline CT scan images were used as the input fortraining a CNN using AUCMEDI Framework. Our model's architecture leverages theDenseNet-121 structure, employing transfer learning principles. The outputvariable was mortality in the first three months. Performance of the model wasevaluated by statistical parameters conventionally used in studies involvingartificial intelligence methods. RESULTS: Images from 219 patients were processed, 175 for training andvalidation of the CNN and 44 for its evaluation. 52%(115/219) of patients werefemale, and the median age was 58(SD=13.06) years. 18.5%(39/219) wereidiopathic SAH. Mortality rate was 28.5%(63/219). The model showed goodaccuracy at predicting mortality in SAH patients exclusively using the imagesof the initial CT scan (Accuracy=74%, F1=75% and AUC=82%). CONCLUSION: Modernimage processing techniques based on AI and CNN make possible to predictmortality in SAH patients with high accuracy using CT scan images as the onlyinput. These models might be optimized by including more data and patientsresulting in better training, development and performance on tasks which arebeyond the skills of conventional clinical knowledge.</description><author>Sergio Garcia-Garcia, Santiago Cepeda, Dominik Muller, Alejandra Mosteiro, Ramon Torne, Silvia Agudo, Natalia de la Torre, Ignacio Arrese, Rosario Sarabia</author><pubDate>Fri, 25 Aug 2023 14:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13373v1</guid></item><item><title>EOG Artifact Removal from Single and Multi-channel EEG Recordings through the combination of Long Short-Term Memory Networks and Independent Component Analysis</title><link>http://arxiv.org/abs/2308.13371v1</link><description>Introduction: Electroencephalogram (EEG) signals have gained significantpopularity in various applications due to their rich information content.However, these signals are prone to contamination from various sources ofartifacts, notably the electrooculogram (EOG) artifacts caused by eyemovements. The most effective approach to mitigate EOG artifacts involvesrecording EOG signals simultaneously with EEG and employing blind sourceseparation techniques, such as independent component analysis (ICA).Nevertheless, the availability of EOG recordings is not always feasible,particularly in pre-recorded datasets. Objective: In this paper, we present anovel methodology that combines a long short-term memory (LSTM)-based neuralnetwork with ICA to address the challenge of EOG artifact removal fromcontaminated EEG signals. Approach: Our approach aims to accomplish two primaryobjectives: 1) estimate the horizontal and vertical EOG signals from thecontaminated EEG data, and 2) employ ICA to eliminate the estimated EOG signalsfrom the EEG, thereby producing an artifact-free EEG signal. Main results: Toevaluate the performance of our proposed method, we conducted experiments on apublicly available dataset comprising recordings from 27 participants. Weemployed well-established metrics such as mean squared error, mean absoluteerror, and mean error to assess the quality of our artifact removal technique.Significance: Furthermore, we compared the performance of our approach with twostate-of-the-art deep learning-based methods reported in the literature,demonstrating the superior performance of our proposed methodology.</description><author>Behrad TaghiBeyglou, Fatemeh Bagheri</author><pubDate>Fri, 25 Aug 2023 14:32:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13371v1</guid></item><item><title>Towards Learning and Explaining Indirect Causal Effects in Neural Networks</title><link>http://arxiv.org/abs/2303.13850v2</link><description>Recently, there has been a growing interest in learning and explaining causaleffects within Neural Network (NN) models. By virtue of NN architectures,previous approaches consider only direct and total causal effects assumingindependence among input variables. We view an NN as a structural causal model(SCM) and extend our focus to include indirect causal effects by introducingfeedforward connections among input neurons. We propose an ante-hoc method thatcaptures and maintains direct, indirect, and total causal effects during NNmodel training. We also propose an algorithm for quantifying learned causaleffects in an NN model and efficient approximation strategies for quantifyingcausal effects in high-dimensional data. Extensive experiments conducted onsynthetic and real-world datasets demonstrate that the causal effects learnedby our ante-hoc method better approximate the ground truth effects compared toexisting methods.</description><author>Abbaavaram Gowtham Reddy, Saketh Bachu, Harsharaj Pathak, Benin L Godfrey, Vineeth N. Balasubramanian, Varshaneya V, Satya Narayanan Kar</author><pubDate>Fri, 25 Aug 2023 14:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13850v2</guid></item><item><title>Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition</title><link>http://arxiv.org/abs/2009.12462v4</link><description>We focus on reinforcement learning (RL) in relational problems that arenaturally defined in terms of objects, their relations, and object-centricactions. These problems are characterized by variable state and action spaces,and finding a fixed-length representation, required by most existing RLmethods, is difficult, if not impossible. We present a deep RL framework basedon graph neural networks and auto-regressive policy decomposition thatnaturally works with these problems and is completely domain-independent. Wedemonstrate the framework's broad applicability in three distinct domains andshow impressive zero-shot generalization over different problem sizes.</description><author>Jaromír Janisch, Tomáš Pevný, Viliam Lisý</author><pubDate>Fri, 25 Aug 2023 14:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.12462v4</guid></item><item><title>Distribution-Aligned Diffusion for Human Mesh Recovery</title><link>http://arxiv.org/abs/2308.13369v1</link><description>Recovering a 3D human mesh from a single RGB image is a challenging task dueto depth ambiguity and self-occlusion, resulting in a high degree ofuncertainty. Meanwhile, diffusion models have recently seen much success ingenerating high-quality outputs by progressively denoising noisy inputs.Inspired by their capability, we explore a diffusion-based approach for humanmesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework whichframes mesh recovery as a reverse diffusion process. We also propose aDistribution Alignment Technique (DAT) that injects input-specific distributioninformation into the diffusion process, and provides useful prior knowledge tosimplify the mesh recovery task. Our method achieves state-of-the-artperformance on three widely used datasets. Project page:https://gongjia0208.github.io/HMDiff/.</description><author>Lin Geng Foo, Jia Gong, Hossein Rahmani, Jun Liu</author><pubDate>Fri, 25 Aug 2023 14:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13369v1</guid></item><item><title>E-commerce users' preferences for delivery options</title><link>http://arxiv.org/abs/2301.00666v2</link><description>Many e-commerce marketplaces offer their users fast delivery options for freeto meet the increasing needs of users, imposing an excessive burden on citylogistics. Therefore, understanding e-commerce users' preference for deliveryoptions is a key to designing logistics policies. To this end, this studydesigns a stated choice survey in which respondents are faced with choice tasksamong different delivery options and time slots, which was completed by 4,062users from the three major metropolitan areas in Japan. To analyze the data,mixed logit models capturing taste heterogeneity as well as flexiblesubstitution patterns have been estimated. The model estimation resultsindicate that delivery attributes including fee, time, and time slot size aresignificant determinants of the delivery option choices. Associations betweenusers' preferences and socio-demographic characteristics, such as age, gender,teleworking frequency and the presence of a delivery box, were also suggested.Moreover, we analyzed two willingness-to-pay measures for delivery, namely, thevalue of delivery time savings (VODT) and the value of time slot shortening(VOTS), and applied a non-semiparametric approach to estimate theirdistributions in a data-oriented manner. Although VODT has a largeheterogeneity among respondents, the estimated median VODT is 25.6 JPY/day,implying that more than half of the respondents would wait an additional day ifthe delivery fee were increased by only 26 JPY, that is, they do notnecessarily need a fast delivery option but often request it when cheap oralmost free. Moreover, VOTS was found to be low, distributed with the median of5.0 JPY/hour; that is, users do not highly value the reduction in time slotsize in monetary terms. These findings on e-commerce users' preferences canhelp in designing levels of service for last-mile delivery to significantlyimprove its efficiency.</description><author>Yuki Oyama, Daisuke Fukuda, Naoto Imura, Katsuhiro Nishinari</author><pubDate>Fri, 25 Aug 2023 14:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00666v2</guid></item><item><title>Burnt area extraction from high-resolution satellite images based on anomaly detection</title><link>http://arxiv.org/abs/2308.13367v1</link><description>Wildfire detection using satellite images is a widely studied task in remotesensing with many applications to fire delineation and mapping. Recently, deeplearning methods have become a scalable solution to automate this task,especially in the field of unsupervised learning where no training data isavailable. This is particularly important in the context of emergency riskmonitoring where fast and effective detection is needed, generally based onhigh-resolution satellite data. Among various approaches, Anomaly Detection(AD) appears to be highly potential thanks to its broad applications incomputer vision, medical imaging, as well as remote sensing. In this work, webuild upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE),a popular reconstruction-based AD method with discrete latent spaces, toperform unsupervised burnt area extraction. We integrate VQ-VAE into anend-to-end framework with an intensive post-processing step using dedicatedvegetation, water and brightness indexes. Our experiments conducted onhigh-resolution SPOT-6/7 images provide promising results of the proposedtechnique, showing its high potential in future research on unsupervised burntarea extraction.</description><author>Oscar David Rafael Narvaez Luces, Minh-Tan Pham, Quentin Poterek, Rémi Braun</author><pubDate>Fri, 25 Aug 2023 14:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13367v1</guid></item><item><title>CS-Mixer: A Cross-Scale Vision MLP Model with Spatial-Channel Mixing</title><link>http://arxiv.org/abs/2308.13363v1</link><description>Despite their simpler information fusion designs compared with VisionTransformers and Convolutional Neural Networks, Vision MLP architectures havedemonstrated strong performance and high data efficiency in recent research.However, existing works such as CycleMLP and Vision Permutator typically modelspatial information in equal-size spatial regions and do not considercross-scale spatial interactions. Further, their token mixers only model 1- or2-axis correlations, avoiding 3-axis spatial-channel mixing due to itscomputational demands. We therefore propose CS-Mixer, a hierarchical Vision MLPthat learns dynamic low-rank transformations for spatial-channel mixing throughcross-scale local and global aggregation. The proposed methodology achievescompetitive results on popular image recognition benchmarks without incurringsubstantially more compute. Our largest model, CS-Mixer-L, reaches 83.2% top-1accuracy on ImageNet-1k with 13.7 GFLOPs and 94 M parameters.</description><author>Jonathan Cui, David A. Araujo, Suman Saha, Md. Faisal Kabir</author><pubDate>Fri, 25 Aug 2023 14:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13363v1</guid></item><item><title>Prototypical few-shot segmentation for cross-institution male pelvic structures with spatial registration</title><link>http://arxiv.org/abs/2209.05160v3</link><description>The prowess that makes few-shot learning desirable in medical image analysisis the efficient use of the support image data, which are labelled to classifyor segment new classes, a task that otherwise requires substantially moretraining images and expert annotations. This work describes a fully 3Dprototypical few-shot segmentation algorithm, such that the trained networkscan be effectively adapted to clinically interesting structures that are absentin training, using only a few labelled images from a different institute.First, to compensate for the widely recognised spatial variability betweeninstitutions in episodic adaptation of novel classes, a novel spatialregistration mechanism is integrated into prototypical learning, consisting ofa segmentation head and an spatial alignment module. Second, to assist thetraining with observed imperfect alignment, support mask conditioning module isproposed to further utilise the annotation available from the support images.Extensive experiments are presented in an application of segmenting eightanatomical structures important for interventional planning, using a data setof 589 pelvic T2-weighted MR images, acquired at seven institutes. The resultsdemonstrate the efficacy in each of the 3D formulation, the spatialregistration, and the support mask conditioning, all of which made positivecontributions independently or collectively. Compared with the previouslyproposed 2D alternatives, the few-shot segmentation performance was improvedwith statistical significance, regardless whether the support data come fromthe same or different institutes.</description><author>Yiwen Li, Yunguan Fu, Iani Gayo, Qianye Yang, Zhe Min, Shaheer Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, Matthew J. Clarkson, Henkjan Huisman, Dean Barratt, Victor Adrian Prisacariu, Yipeng Hu</author><pubDate>Fri, 25 Aug 2023 14:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05160v3</guid></item></channel></rss>