<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 20 Mar 2024 06:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title><link>http://arxiv.org/abs/2403.12968v1</link><description>This paper focuses on task-agnostic prompt compression for bettergeneralizability and efficiency. Considering the redundancy in naturallanguage, existing approaches compress prompts by removing tokens or lexicalunits according to their information entropy obtained from a causal languagemodel such as LLaMa-7B. The challenge is that information entropy may be asuboptimal compression metric: (i) it only leverages unidirectional context andmay fail to capture all essential information needed for prompt compression;(ii) it is not aligned with the prompt compression objective. To address these issues, we propose a data distillation procedure to deriveknowledge from an LLM to compress prompts without losing crucial information,and meantime, introduce an extractive text compression dataset. We formulateprompt compression as a token classification problem to guarantee thefaithfulness of the compressed prompt to the original one, and use aTransformer encoder as the base architecture to capture all essentialinformation for prompt compression from the full bidirectional context. Ourapproach leads to lower latency by explicitly learning the compressionobjective with smaller models such as XLM-RoBERTa-large and mBERT. We evaluate our method on both in-domain and out-of-domain datasets,including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite itssmall size, our model shows significant performance gains over strong baselinesand demonstrates robust generalization ability across different LLMs.Additionally, our model is 3x-6x faster than existing prompt compressionmethods, while accelerating the end-to-end latency by 1.6x-2.9x withcompression ratios of 2x-5x.</description><author>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor RÃ¼hle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang</author><pubDate>Tue, 19 Mar 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12968v1</guid></item><item><title>Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment</title><link>http://arxiv.org/abs/2403.12965v1</link><description>This paper introduces a novel framework for virtual try-on, termedWear-Any-Way. Different from previous methods, Wear-Any-Way is a customizablesolution. Besides generating high-fidelity results, our method supports usersto precisely manipulate the wearing style. To achieve this goal, we firstconstruct a strong pipeline for standard virtual try-on, supportingsingle/multiple garment try-on and model-to-model settings in complicatedscenarios. To make it manipulable, we propose sparse correspondence alignmentwhich involves point-based control to guide the generation for specificlocations. With this design, Wear-Any-Way gets state-of-the-art performance forthe standard setting and provides a novel interaction form for customizing thewearing style. For instance, it supports users to drag the sleeve to make itrolled up, drag the coat to make it open, and utilize clicks to control thestyle of tuck, etc. Wear-Any-Way enables more liberated and flexibleexpressions of the attires, holding profound implications in the fashionindustry.</description><author>Mengting Chen, Xi Chen, Zhonghua Zhai, Chen Ju, Xuewen Hong, Jinsong Lan, Shuai Xiao</author><pubDate>Tue, 19 Mar 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12965v1</guid></item><item><title>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</title><link>http://arxiv.org/abs/2403.12966v1</link><description>In the realm of vision-language understanding, the proficiency of models ininterpreting and reasoning over visual content has become a cornerstone fornumerous applications. However, it is challenging for the visual encoder inLarge Vision-Language Models (LVLMs) to extract useful features tailored toquestions that aid the language model's response. Furthermore, a commonpractice among existing LVLMs is to utilize lower-resolution images, whichrestricts the ability for visual recognition. Our work introduces theChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novelapproach that enhances feature extraction by focusing on key regions ofinterest (ROI) within the image, corresponding to the posed questions orinstructions. This technique allows LVLMs to access more detailed visualinformation without altering the original image resolution, thereby offeringmulti-granularity image features. By integrating Chain-of-Spot withinstruct-following LLaVA-1.5 models, the process of image reasoningconsistently improves performance across a wide range of multimodal datasetsand benchmarks without bells and whistles and achieves new state-of-the-artresults. Our empirical findings demonstrate a significant improvement in LVLMs'ability to understand and reason about visual content, paving the way for moresophisticated visual instruction-following applications. Code and models areavailable at https://github.com/dongyh20/Chain-of-Spot</description><author>Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, Jiwen Lu</author><pubDate>Tue, 19 Mar 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12966v1</guid></item><item><title>Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models</title><link>http://arxiv.org/abs/2403.12964v1</link><description>Recently, large-scale pre-trained Vision-Language Models (VLMs) havedemonstrated great potential in learning open-world visual representations, andexhibit remarkable performance across a wide range of downstream tasks throughefficient fine-tuning. In this work, we innovatively introduce the concept ofdual learning into fine-tuning VLMs, i.e., we not only learn what an image is,but also what an image isn't. Building on this concept, we introduce a novelDualAdapter approach to enable dual-path adaptation of VLMs from both positiveand negative perspectives with only limited annotated samples. In the inferencestage, our DualAdapter performs unified predictions by simultaneouslyconducting complementary positive selection and negative exclusion acrosstarget classes, thereby enhancing the overall recognition accuracy of VLMs indownstream tasks. Our extensive experimental results across 15 datasetsvalidate that the proposed DualAdapter outperforms existing state-of-the-artmethods on both few-shot learning and domain generalization tasks whileachieving competitive computational efficiency. Code is available athttps://github.com/zhangce01/DualAdapter.</description><author>Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie</author><pubDate>Tue, 19 Mar 2024 18:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12964v1</guid></item><item><title>FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis</title><link>http://arxiv.org/abs/2403.12963v1</link><description>In this study, we delve into the generation of high-resolution images frompre-trained diffusion models, addressing persistent challenges, such asrepetitive patterns and structural distortions, that emerge when models areapplied beyond their trained resolutions. To address this issue, we introducean innovative, training-free approach FouriScale from the perspective offrequency domain analysis. We replace the original convolutional layers inpre-trained diffusion models by incorporating a dilation technique along with alow-pass operation, intending to achieve structural consistency and scaleconsistency across resolutions, respectively. Further enhanced by apadding-then-crop strategy, our method can flexibly handle text-to-imagegeneration of various aspect ratios. By using the FouriScale as guidance, ourmethod successfully balances the structural integrity and fidelity of generatedimages, achieving an astonishing capacity of arbitrary-size, high-resolution,and high-quality generation. With its simplicity and compatibility, our methodcan provide valuable insights for future explorations into the synthesis ofultra-high-resolution images. The code will be released athttps://github.com/LeonHLJ/FouriScale.</description><author>Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li</author><pubDate>Tue, 19 Mar 2024 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12963v1</guid></item><item><title>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</title><link>http://arxiv.org/abs/2403.12962v1</link><description>The remarkable efficacy of text-to-image diffusion models has motivatedextensive exploration of their potential application in video domains.Zero-shot methods seek to extend image diffusion models to videos withoutnecessitating model training. Recent methods mainly focus on incorporatinginter-frame correspondence into attention mechanisms. However, the softconstraint imposed on determining where to attend to valid features cansometimes be insufficient, resulting in temporal inconsistency. In this paper,we introduce FRESCO, intra-frame correspondence alongside inter-framecorrespondence to establish a more robust spatial-temporal constraint. Thisenhancement ensures a more consistent transformation of semantically similarcontent across frames. Beyond mere attention guidance, our approach involves anexplicit update of features to achieve high spatial-temporal consistency withthe input video, significantly improving the visual coherence of the resultingtranslated videos. Extensive experiments demonstrate the effectiveness of ourproposed framework in producing high-quality, coherent videos, marking anotable improvement over existing zero-shot methods.</description><author>Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy</author><pubDate>Tue, 19 Mar 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12962v1</guid></item><item><title>TexTile: A Differentiable Metric for Texture Tileability</title><link>http://arxiv.org/abs/2403.12961v1</link><description>We introduce TexTile, a novel differentiable metric to quantify the degreeupon which a texture image can be concatenated with itself without introducingrepeating artifacts (i.e., the tileability). Existing methods for tileabletexture synthesis focus on general texture quality, but lack explicit analysisof the intrinsic repeatability properties of a texture. In contrast, ourTexTile metric effectively evaluates the tileable properties of a texture,opening the door to more informed synthesis and analysis of tileable textures.Under the hood, TexTile is formulated as a binary classifier carefully builtfrom a large dataset of textures of different styles, semantics, regularities,and human annotations.Key to our method is a set of architectural modificationsto baseline pre-train image classifiers to overcome their shortcomings atmeasuring tileability, along with a custom data augmentation and trainingregime aimed at increasing robustness and accuracy. We demonstrate that TexTilecan be plugged into different state-of-the-art texture synthesis methods,including diffusion-based strategies, and generate tileable textures whilekeeping or even improving the overall texture quality. Furthermore, we showthat TexTile can objectively evaluate any tileable texture synthesis method,whereas the current mix of existing metrics produces uncorrelated scores whichheavily hinders progress in the field.</description><author>Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno</author><pubDate>Tue, 19 Mar 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12961v1</guid></item><item><title>FaceXFormer: A Unified Transformer for Facial Analysis</title><link>http://arxiv.org/abs/2403.12960v1</link><description>In this work, we introduce FaceXformer, an end-to-end unified transformermodel for a comprehensive range of facial analysis tasks such as face parsing,landmark detection, head pose estimation, attributes recognition, andestimation of age, gender, race, and landmarks visibility. Conventional methodsin face analysis have often relied on task-specific designs and preprocessingtechniques, which limit their approach to a unified architecture. Unlike theseconventional methods, our FaceXformer leverages a transformer-basedencoder-decoder architecture where each task is treated as a learnable token,enabling the integration of multiple tasks within a single framework. Moreover,we propose a parameter-efficient decoder, FaceX, which jointly processes faceand task tokens, thereby learning generalized and robust face representationsacross different tasks. To the best of our knowledge, this is the first work topropose a single model capable of handling all these facial analysis tasksusing transformers. We conducted a comprehensive analysis of effectivebackbones for unified face task processing and evaluated different task queriesand the synergy between them. We conduct experiments against state-of-the-artspecialized models and previous multi-task models in both intra-dataset andcross-dataset evaluations across multiple benchmarks. Additionally, our modeleffectively handles images "in-the-wild," demonstrating its robustness andgeneralizability across eight different tasks, all while maintaining thereal-time performance of 37 FPS.</description><author>Kartik Narayan, Vibashan VS, Rama Chellappa, Vishal M. Patel</author><pubDate>Tue, 19 Mar 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12960v1</guid></item><item><title>WHAC: World-grounded Humans and Cameras</title><link>http://arxiv.org/abs/2403.12959v1</link><description>Estimating human and camera trajectories with accurate scale in the worldcoordinate system from a monocular video is a highly desirable yet challengingand ill-posed problem. In this study, we aim to recover expressive parametrichuman models (i.e., SMPL-X) and corresponding camera poses jointly, byleveraging the synergy between three critical players: the world, the human,and the camera. Our approach is founded on two key observations. Firstly,camera-frame SMPL-X estimation methods readily recover absolute human depth.Secondly, human motions inherently provide absolute spatial cues. Byintegrating these insights, we introduce a novel framework, referred to asWHAC, to facilitate world-grounded expressive human pose and shape estimation(EHPS) alongside camera pose estimation, without relying on traditionaloptimization techniques. Additionally, we present a new synthetic dataset,WHAC-A-Mole, which includes accurately annotated humans and cameras, andfeatures diverse interactive human motions as well as realistic cameratrajectories. Extensive experiments on both standard and newly establishedbenchmarks highlight the superiority and efficacy of our framework. We willmake the code and dataset publicly available.</description><author>Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang</author><pubDate>Tue, 19 Mar 2024 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12959v1</guid></item><item><title>Dated Data: Tracing Knowledge Cutoffs in Large Language Models</title><link>http://arxiv.org/abs/2403.12958v1</link><description>Released Large Language Models (LLMs) are often paired with a claimedknowledge cutoff date, or the dates at which training data was gathered. Suchinformation is crucial for applications where the LLM must provide up to dateinformation. However, this statement only scratches the surface: do allresources in the training data share the same knowledge cutoff date? Does themodel's demonstrated knowledge for these subsets closely align to their cutoffdates? In this work, we define the notion of an effective cutoff. This isdistinct from the LLM designer reported cutoff and applies separately tosub-resources and topics. We propose a simple approach to estimate effectivecutoffs on the resource-level temporal alignment of an LLM by probing acrossversions of the data. Using this analysis, we find that effective cutoffs oftendiffer from reported cutoffs. To understand the root cause of this observation,we conduct a direct large-scale analysis on open pre-training datasets. Ouranalysis reveals two reasons for these inconsistencies: (1) temporal biases ofCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)complications in LLM deduplication schemes involving semantic duplicates andlexical near-duplicates. Overall, our results show that knowledge cutoffs arenot as simple as they have seemed and that care must be taken both by LLMdataset curators as well as practitioners who seek to use information fromthese models.</description><author>Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</author><pubDate>Tue, 19 Mar 2024 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12958v1</guid></item><item><title>GVGEN: Text-to-3D Generation with Volumetric Representation</title><link>http://arxiv.org/abs/2403.12957v1</link><description>In recent years, 3D Gaussian splatting has emerged as a powerful techniquefor 3D reconstruction and generation, known for its fast and high-qualityrendering capabilities. To address these shortcomings, this paper introduces anovel diffusion-based framework, GVGEN, designed to efficiently generate 3DGaussian representations from text input. We propose two innovativetechniques:(1) Structured Volumetric Representation. We first arrangedisorganized 3D Gaussian points as a structured form GaussianVolume. Thistransformation allows the capture of intricate texture details within a volumecomposed of a fixed number of Gaussians. To better optimize the representationof these details, we propose a unique pruning and densifying method named theCandidate Pool Strategy, enhancing detail fidelity through selectiveoptimization. (2) Coarse-to-fine Generation Pipeline. To simplify thegeneration of GaussianVolume and empower the model to generate instances withdetailed 3D geometry, we propose a coarse-to-fine pipeline. It initiallyconstructs a basic geometric structure, followed by the prediction of completeGaussian attributes. Our framework, GVGEN, demonstrates superior performance inqualitative and quantitative assessments compared to existing 3D generationmethods. Simultaneously, it maintains a fast generation speed ($\sim$7seconds), effectively striking a balance between quality and efficiency.</description><author>Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He</author><pubDate>Tue, 19 Mar 2024 18:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12957v1</guid></item><item><title>FutureDepth: Learning to Predict the Future Improves Video Depth Estimation</title><link>http://arxiv.org/abs/2403.12953v1</link><description>In this paper, we propose a novel video depth estimation approach,FutureDepth, which enables the model to implicitly leverage multi-frame andmotion cues to improve depth estimation by making it learn to predict thefuture at training. More specifically, we propose a future prediction network,F-Net, which takes the features of multiple consecutive frames and is trainedto predict multi-frame features one time step ahead iteratively. In this way,F-Net learns the underlying motion and correspondence information, and weincorporate its features into the depth decoding process. Additionally, toenrich the learning of multiframe correspondence cues, we further leverage areconstruction network, R-Net, which is trained via adaptively maskedauto-encoding of multiframe feature volumes. At inference time, both F-Net andR-Net are used to produce queries to work with the depth decoder, as well as afinal refinement network. Through extensive experiments on several benchmarks,i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, andopen-domain scenarios, we show that FutureDepth significantly improves uponbaseline models, outperforms existing video depth estimation methods, and setsnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is moreefficient than existing SOTA video depth estimation models and has similarlatencies when comparing to monocular models</description><author>Rajeev Yasarla, Manish Kumar Singh, Hong Cai, Yunxiao Shi, Jisoo Jeong, Yinhao Zhu, Shizhong Han, Risheek Garrepalli, Fatih Porikli</author><pubDate>Tue, 19 Mar 2024 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12953v1</guid></item><item><title>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models</title><link>http://arxiv.org/abs/2403.12952v1</link><description>Advancements in vision-language models (VLMs) have propelled the field ofcomputer vision, particularly in the zero-shot learning setting. Despite theirpromise, the effectiveness of these models often diminishes due to domainshifts in test environments. To address this, we introduce the Test-TimePrototype Shifting (TPS) framework, a pioneering approach designed to adaptVLMs to test datasets using unlabeled test inputs. Our method is based on thenotion of modulating per-class prototypes in the shared embedding space. Bypre-computing and caching prototypes generated with the pre-trained textencoder, TPS not only facilitates optimization-free prototype reuse forsubsequent predictions but also enables seamless integration with currentadvancements in prompt engineering. At test-time, TPS dynamically learns shiftvectors for each prototype based solely on the given test sample, effectivelybridging the domain gap and enhancing classification accuracy. A notable aspectof our framework is its significantly reduced memory and computational demandswhen compared to conventional text-prompt tuning methods. Extensive evaluationsacross 15 datasets involving natural distribution shifts and cross-datasetgeneralization demonstrate TPS's superior performance, achievingstate-of-the-art results while reducing resource requirements.</description><author>Elaine Sui, Xiaohan Wang, Serena Yeung-Levy</author><pubDate>Tue, 19 Mar 2024 18:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12952v1</guid></item><item><title>Resolution- and Stimulus-agnostic Super-Resolution of Ultra-High-Field Functional MRI: Application to Visual Studies</title><link>http://arxiv.org/abs/2311.14918v2</link><description>High-resolution fMRI provides a window into the brain's mesoscaleorganization. Yet, higher spatial resolution increases scan times, tocompensate for the low signal and contrast-to-noise ratio. This work introducesa deep learning-based 3D super-resolution (SR) method for fMRI. Byincorporating a resolution-agnostic image augmentation framework, our methodadapts to varying voxel sizes without retraining. We apply this innovativetechnique to localize fine-scale motion-selective sites in the early visualareas. Detection of these sites typically requires a resolution higher than 1mm isotropic, whereas here, we visualize them based on lower resolution (2-3mmisotropic) fMRI data. Remarkably, the super-resolved fMRI is able to recoverhigh-frequency detail of the interdigitated organization of these sites(relative to the color-selective sites), even with training data sourced fromdifferent subjects and experimental paradigms -- including non-visualresting-state fMRI, underscoring its robustness and versatility. Quantitativeand qualitative results indicate that our method has the potential to enhancethe spatial resolution of fMRI, leading to a drastic reduction in acquisitiontime.</description><author>Hongwei Bran Li, Matthew S. Rosen, Shahin Nasr, Juan Eugenio Iglesias</author><pubDate>Tue, 19 Mar 2024 18:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14918v2</guid></item><item><title>RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models</title><link>http://arxiv.org/abs/2403.06420v2</link><description>Reinforcement learning (RL) has demonstrated its capability in solvingvarious tasks but is notorious for its low sample efficiency. In this paper, wepropose RLingua, a framework that can leverage the internal knowledge of largelanguage models (LLMs) to reduce the sample complexity of RL in roboticmanipulations. To this end, we first present a method for extracting the priorknowledge of LLMs by prompt engineering so that a preliminary rule-based robotcontroller for a specific task can be generated in a user-friendly manner.Despite being imperfect, the LLM-generated robot controller is utilized toproduce action samples during rollouts with a decaying probability, therebyimproving RL's sample efficiency. We employ TD3, the widely-used RL baselinemethod, and modify the actor loss to regularize the policy learning towards theLLM-generated controller. RLingua also provides a novel method of improving theimperfect LLM-generated robot controllers by RL. We demonstrate that RLinguacan significantly reduce the sample complexity of TD3 in four robot tasks ofpanda_gym and achieve high success rates in 12 sampled sparsely rewarded robottasks in RLBench, where the standard TD3 fails. Additionally, We validatedRLingua's effectiveness in real-world robot experiments through Sim2Real,demonstrating that the learned policies are effectively transferable to realrobot tasks. Further details about our work are available at our projectwebsite https://rlingua.github.io.</description><author>Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang</author><pubDate>Tue, 19 Mar 2024 18:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06420v2</guid></item><item><title>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</title><link>http://arxiv.org/abs/2311.10081v2</link><description>We present DRESS, a large vision language model (LVLM) that innovativelyexploits Natural Language feedback (NLF) from Large Language Models to enhanceits alignment and interactions by addressing two key limitations in thestate-of-the-art LVLMs. First, prior LVLMs generally rely only on theinstruction finetuning stage to enhance alignment with human preferences.Without incorporating extra feedback, they are still prone to generateunhelpful, hallucinated, or harmful responses. Second, while the visualinstruction tuning data is generally structured in a multi-turn dialogueformat, the connections and dependencies among consecutive conversational turnsare weak. This reduces the capacity for effective multi-turn interactions. Totackle these, we propose a novel categorization of the NLF into two key types:critique and refinement. The critique NLF identifies the strengths andweaknesses of the responses and is used to align the LVLMs with humanpreferences. The refinement NLF offers concrete suggestions for improvement andis adopted to improve the interaction ability of the LVLMs-- which focuses onLVLMs' ability to refine responses by incorporating feedback in multi-turninteractions. To address the non-differentiable nature of NLF, we generalizeconditional reinforcement learning for training. Our experimental resultsdemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), andharmless (21.03%) responses, and more effectively learn from feedback duringmulti-turn interactions compared to SOTA LVMLs.</description><author>Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran</author><pubDate>Tue, 19 Mar 2024 18:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10081v2</guid></item><item><title>The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold</title><link>http://arxiv.org/abs/2305.01604v3</link><description>We develop information-geometric techniques to analyze the trajectories ofthe predictions of deep networks during training. By examining the underlyinghigh-dimensional probabilistic models, we reveal that the training processexplores an effectively low-dimensional manifold. Networks with a wide range ofarchitectures, sizes, trained using different optimization methods,regularization techniques, data augmentation techniques, and weightinitializations lie on the same manifold in the prediction space. We study thedetails of this manifold to find that networks with different architecturesfollow distinguishable trajectories but other factors have a minimal influence;larger networks train along a similar manifold as that of smaller networks,just faster; and networks initialized at very different parts of the predictionspace converge to the solution along a similar manifold.</description><author>Jialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang, Mark K. Transtrum, James P. Sethna, Pratik Chaudhari</author><pubDate>Tue, 19 Mar 2024 18:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01604v3</guid></item><item><title>Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion</title><link>http://arxiv.org/abs/2403.12950v1</link><description>In dueling bandits, the learner receives preference feedback between arms,and the regret of an arm is defined in terms of its suboptimality to a winnerarm. The more challenging and practically motivated non-stationary variant ofdueling bandits, where preferences change over time, has been the focus ofseveral recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk andAgarwal, 2023). The goal is to design algorithms without foreknowledge of theamount of change. The bulk of known results here studies the Condorcet winner setting, where anarm preferred over any other exists at all times. Yet, such a winner may notexist and, to contrast, the Borda version of this problem (which is alwayswell-defined) has received little attention. In this work, we establish thefirst optimal and adaptive Borda dynamic regret upper bound, which highlightsfundamental differences in the learnability of severe non-stationarity betweenCondorcet vs. Borda regret objectives in dueling bandits. Surprisingly, our techniques for non-stationary Borda dueling bandits alsoyield improved rates within the Condorcet winner setting, and reveal newpreference models where tighter notions of non-stationarity are adaptivelylearnable. This is accomplished through a novel generalized Borda scoreframework which unites the Borda and Condorcet problems, thus allowingreduction of Condorcet regret to a Borda-like task. Such a generalization wasnot previously known and is likely to be of independent interest.</description><author>Joe Suk, Arpit Agarwal</author><pubDate>Tue, 19 Mar 2024 18:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12950v1</guid></item><item><title>On Safety in Safe Bayesian Optimization</title><link>http://arxiv.org/abs/2403.12948v1</link><description>Optimizing an unknown function under safety constraints is a central task inrobotics, biomedical engineering, and many other disciplines, and increasinglysafe Bayesian Optimization (BO) is used for this. Due to the safety criticalnature of these applications, it is of utmost importance that theoreticalsafety guarantees for these algorithms translate into the real world. In thiswork, we investigate three safety-related issues of the popular class ofSafeOpt-type algorithms. First, these algorithms critically rely on frequentistuncertainty bounds for Gaussian Process (GP) regression, but concreteimplementations typically utilize heuristics that invalidate all safetyguarantees. We provide a detailed analysis of this problem and introduceReal-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recentGP bounds and thus retains all theoretical guarantees. Second, we identifyassuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm ofthe target function, a key technical assumption in SafeOpt-like algorithms, asa central obstacle to real-world usage. To overcome this challenge, weintroduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,which guarantees safety without an assumption on the RKHS bound, andempirically show that this algorithm is not only safe, but also exhibitssuperior performance compared to the state-of-the-art on several functionclasses. Third, SafeOpt and derived algorithms rely on a discrete search space,making them difficult to apply to higher-dimensional problems. To widen theapplicability of these algorithms, we introduce Lipschitz-only GP-UCB(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensionalproblems, while retaining safety.</description><author>Christian Fiedler, Johanna Menn, Lukas KreiskÃ¶ther, Sebastian Trimpe</author><pubDate>Tue, 19 Mar 2024 18:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12948v1</guid></item><item><title>Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes</title><link>http://arxiv.org/abs/2403.12946v1</link><description>In offline reinforcement learning (RL), the absence of active explorationcalls for attention on the model robustness to tackle the sim-to-real gap,where the discrepancy between the simulated and deployed environments cansignificantly undermine the performance of the learned policy. To endow thelearned policy with robustness in a sample-efficient manner in the presence ofhigh-dimensional state-action space, this paper considers the sample complexityof distributionally robust linear Markov decision processes (MDPs) with anuncertainty set characterized by the total variation distance using offlinedata. We develop a pessimistic model-based algorithm and establish its samplecomplexity bound under minimal data coverage assumptions, which outperformsprior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. Wefurther improve the performance guarantee of the proposed algorithm byincorporating a carefully-designed variance estimator.</description><author>He Wang, Laixi Shi, Yuejie Chi</author><pubDate>Tue, 19 Mar 2024 18:48:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12946v1</guid></item><item><title>Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers</title><link>http://arxiv.org/abs/2403.12943v1</link><description>While large-scale robotic systems typically rely on textual instructions fortasks, this work explores a different approach: can robots infer the taskdirectly from observing humans? This shift necessitates the robot's ability todecode human intent and translate it into executable actions within itsphysical constraints and environment. We introduce Vid2Robot, a novelend-to-end video-based learning framework for robots. Given a videodemonstration of a manipulation task and current visual observations, Vid2Robotdirectly produces robot actions. This is achieved through a unifiedrepresentation model trained on a large dataset of human video and robottrajectory. The model leverages cross-attention mechanisms to fuse prompt videofeatures to the robot's current state and generate appropriate actions thatmimic the observed task. To further improve policy performance, we proposeauxiliary contrastive losses that enhance the alignment between human and robotvideo representations. We evaluate Vid2Robot on real-world robots,demonstrating a 20% improvement in performance compared to othervideo-conditioned policies when using human demonstration videos. Additionally,our model exhibits emergent capabilities, such as successfully transferringobserved motions from one object to another, and long-horizon composition, thusshowcasing its potential for real-world applications. Project website:vid2robot.github.io</description><author>Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi</author><pubDate>Tue, 19 Mar 2024 18:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12943v1</guid></item><item><title>Neural Differential Algebraic Equations</title><link>http://arxiv.org/abs/2403.12938v1</link><description>Differential-Algebraic Equations (DAEs) describe the temporal evolution ofsystems that obey both differential and algebraic constraints. Of particularinterest are systems that contain implicit relationships between theircomponents, such as conservation relationships. Here, we present NeuralDifferential-Algebraic Equations (NDAEs) suitable for data-driven modeling ofDAEs. This methodology is built upon the concept of the Universal DifferentialEquation; that is, a model constructed as a system of Neural OrdinaryDifferential Equations informed by theory from particular science domains. Inthis work, we show that the proposed NDAEs abstraction is suitable for relevantsystem-theoretic data-driven modeling tasks. Presented examples include (i) theinverse problem of tank-manifold dynamics and (ii) discrepancy modeling of anetwork of pumps, tanks, and pipes. Our experiments demonstrate the proposedmethod's robustness to noise and extrapolation ability to (i) learn thebehaviors of the system components and their interaction physics and (ii)disambiguate between data trends and mechanistic relationships contained in thesystem.</description><author>James Koch, Madelyn Shapiro, Himanshu Sharma, Draguna Vrabie, Jan Drgona</author><pubDate>Tue, 19 Mar 2024 18:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12938v1</guid></item><item><title>Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models</title><link>http://arxiv.org/abs/2403.12936v1</link><description>Court transcripts and judgments are rich repositories of legal knowledge,detailing the intricacies of cases and the rationale behind judicial decisions.The extraction of key information from these documents provides a conciseoverview of a case, crucial for both legal experts and the public. With theadvent of large language models (LLMs), automatic information extraction hasbecome increasingly feasible and efficient. This paper presents a comprehensivestudy on the application of GPT-4, a large language model, for automaticinformation extraction from UK Employment Tribunal (UKET) cases. Wemeticulously evaluated GPT-4's performance in extracting critical informationwith a manual verification process to ensure the accuracy and relevance of theextracted data. Our research is structured around two primary extraction tasks:the first involves a general extraction of eight key aspects that holdsignificance for both legal specialists and the general public, including thefacts of the case, the claims made, references to legal statutes, references toprecedents, general case outcomes and corresponding labels, detailed order andremedies and reasons for the decision. The second task is more focused, aimedat analysing three of those extracted features, namely facts, claims andoutcomes, in order to facilitate the development of a tool capable ofpredicting the outcome of employment law disputes. Through our analysis, wedemonstrate that LLMs like GPT-4 can obtain high accuracy in legal informationextraction, highlighting the potential of LLMs in revolutionising the way legalinformation is processed and utilised, offering significant implications forlegal research and practice.</description><author>Joana Ribeiro de Faria, Huiyuan Xie, Felix Steffek</author><pubDate>Tue, 19 Mar 2024 18:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12936v1</guid></item><item><title>EscherNet: A Generative Model for Scalable View Synthesis</title><link>http://arxiv.org/abs/2402.03908v2</link><description>We introduce EscherNet, a multi-view conditioned diffusion model for viewsynthesis. EscherNet learns implicit and generative 3D representations coupledwith a specialised camera positional encoding, allowing precise and continuousrelative control of the camera transformation between an arbitrary number ofreference and target views. EscherNet offers exceptional generality,flexibility, and scalability in view synthesis -- it can generate more than 100consistent target views simultaneously on a single consumer-grade GPU, despitebeing trained with a fixed number of 3 reference views to 3 target views. As aresult, EscherNet not only addresses zero-shot novel view synthesis, but alsonaturally unifies single- and multi-image 3D reconstruction, combining thesediverse tasks into a single, cohesive framework. Our extensive experimentsdemonstrate that EscherNet achieves state-of-the-art performance in multiplebenchmarks, even when compared to methods specifically tailored for eachindividual problem. This remarkable versatility opens up new directions fordesigning scalable neural architectures for 3D vision. Project page:https://kxhit.github.io/EscherNet.</description><author>Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</author><pubDate>Tue, 19 Mar 2024 18:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03908v2</guid></item><item><title>Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability</title><link>http://arxiv.org/abs/2403.10682v2</link><description>Glasses form the basis of many modern applications and also hold greatpotential for future medical and environmental applications. However, theirstructural complexity and large composition space make design and optimizationchallenging for certain applications. Of particular importance for glassprocessing is an estimate of a given composition's glass-forming ability (GFA).However, there remain many open questions regarding the physical mechanisms ofglass formation, especially in oxide glasses. It is apparent that a proxy forGFA would be highly useful in glass processing and design, but identifying sucha surrogate property has proven itself to be difficult. Here, we explore theapplication of an open-source pre-trained NN model, GlassNet, that can predictthe characteristic temperatures necessary to compute glass stability (GS) andassess the feasibility of using these physics-informed ML (PIML)-predicted GSparameters to estimate GFA. In doing so, we track the uncertainties at eachstep of the computation - from the original ML prediction errors, to thecompounding of errors during GS estimation, and finally to the final estimationof GFA. While GlassNet exhibits reasonable accuracy on all individualproperties, we observe a large compounding of error in the combination of theseindividual predictions for the prediction of GS, finding that random forestmodels offer similar accuracy to GlassNet. We also breakdown the ML performanceon different glass families and find that the error in GS prediction iscorrelated with the error in crystallization peak temperature prediction.Lastly, we utilize this finding to assess the relationship betweentop-performing GS parameters and GFA for two ternary glass systems: sodiumborosilicate and sodium iron phosphate glasses. We conclude that to obtain trueML predictive capability of GFA, significantly more data needs to be collected.</description><author>Sarah I. Allec, Xiaonan Lu, Daniel R. Cassar, Xuan T. Nguyen, Vinay I. Hegde, Thiruvillamalai Mahadevan, Miroslava Peterson, Jincheng Du, Brian J. Riley, John D. Vienna, James E. Saal</author><pubDate>Tue, 19 Mar 2024 18:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10682v2</guid></item><item><title>Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties</title><link>http://arxiv.org/abs/2403.12935v1</link><description>Grape cluster architecture and compactness are complex traits influencingdisease susceptibility, fruit quality, and yield. Evaluation methods for thesetraits include visual scoring, manual methodologies, and computer vision, withthe latter being the most scalable approach. Most of the existing computervision approaches for processing cluster images often rely on conventionalsegmentation or machine learning with extensive training and limitedgeneralization. The Segment Anything Model (SAM), a novel foundation modeltrained on a massive image dataset, enables automated object segmentationwithout additional training. This study demonstrates out-of-the-box SAM's highaccuracy in identifying individual berries in 2D cluster images. Using thismodel, we managed to segment approximately 3,500 cluster images, generatingover 150,000 berry masks, each linked with spatial coordinates within theirclusters. The correlation between human-identified berries and SAM predictionswas very strong (Pearson r2=0.96). Although the visible berry count in imagestypically underestimates the actual cluster berry count due to visibilityissues, we demonstrated that this discrepancy could be adjusted using a linearregression model (adjusted R2=0.87). We emphasized the critical importance ofthe angle at which the cluster is imaged, noting its substantial effect onberry counts and architecture. We proposed different approaches in which berrylocation information facilitated the calculation of complex features related tocluster architecture and compactness. Finally, we discussed SAM's potentialintegration into currently available pipelines for image generation andprocessing in vineyard conditions.</description><author>Efrain Torres-Lomas, Jimena Lado-Jimena, Guillermo Garcia-Zamora, Luis Diaz-Garcia</author><pubDate>Tue, 19 Mar 2024 18:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12935v1</guid></item><item><title>Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</title><link>http://arxiv.org/abs/2403.12933v1</link><description>Understanding illumination and reducing the need for supervision pose asignificant challenge in low-light enhancement. Current approaches are highlysensitive to data usage during training and illumination-specifichyper-parameters, limiting their ability to handle unseen scenarios. In thispaper, we propose a new zero-reference low-light enhancement frameworktrainable solely with normal light images. To accomplish this, we devise anillumination-invariant prior inspired by the theory of physical light transfer.This prior serves as the bridge between normal and low-light images. Then, wedevelop a prior-to-image framework trained without low-light data. Duringtesting, this framework is able to restore our illumination-invariant priorback to images, automatically achieving low-light enhancement. Within thisframework, we leverage a pretrained generative diffusion model for modelability, introduce a bypass decoder to handle detail distortion, as well asoffer a lightweight version for practicality. Extensive experiments demonstrateour framework's superiority in various scenarios as well as goodinterpretability, robustness, and efficiency. Code is available on our projecthomepage: http://daooshee.github.io/QuadPrior-Website/</description><author>Wenjing Wang, Huan Yang, Jianlong Fu, Jiaying Liu</author><pubDate>Tue, 19 Mar 2024 18:36:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12933v1</guid></item><item><title>Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation</title><link>http://arxiv.org/abs/2306.02960v2</link><description>In the field of robotics, event-based cameras are emerging as a promisinglow-power alternative to traditional frame-based cameras for capturinghigh-speed motion and high dynamic range scenes. This is due to their sparseand asynchronous event outputs. Spiking Neural Networks (SNNs) with theirasynchronous event-driven compute, show great potential for extracting thespatio-temporal features from these event streams. In contrast, the standardAnalog Neural Networks (ANNs) fail to process event data effectively. However,training SNNs is difficult due to additional trainable parameters (thresholdsand leaks), vanishing spikes at deeper layers, and a non-differentiable binaryactivation function. Furthermore, an additional data structure, membranepotential, responsible for keeping track of temporal information, must befetched and updated at every timestep in SNNs. To overcome these challenges, wepropose a novel SNN-ANN hybrid architecture that combines the strengths ofboth. Specifically, we leverage the asynchronous compute capabilities of SNNlayers to effectively extract the input temporal information. Concurrently, theANN layers facilitate training and efficient hardware deployment on traditionalmachine learning hardware such as GPUs. We provide extensive experimentalanalysis for assigning each layer to be spiking or analog, leading to a networkconfiguration optimized for performance and ease of training. We evaluate ourhybrid architecture for optical flow estimation on DSEC-flow and Multi-VehicleStereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybridSNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE)with 22% lower energy consumption compared to Full-SNN, and 48% lower AEEcompared to Full-ANN, while maintaining comparable energy usage.</description><author>Shubham Negi, Deepika Sharma, Adarsh Kumar Kosta, Kaushik Roy</author><pubDate>Tue, 19 Mar 2024 18:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02960v2</guid></item><item><title>You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs</title><link>http://arxiv.org/abs/2403.12931v1</link><description>We introduce YOSO, a novel generative model designed for rapid, scalable, andhigh-fidelity one-step image synthesis. This is achieved by integrating thediffusion process with GANs. Specifically, we smooth the distribution by thedenoising generator itself, performing self-cooperative learning. We show thatour method can serve as a one-step generation model training from scratch withcompetitive performance. Moreover, we show that our method can be extended tofinetune pre-trained text-to-image diffusion for high-quality one-steptext-to-image synthesis even with LoRA fine-tuning. In particular, we providethe first diffusion transformer that can generate images in one step trained on512 resolution, with the capability of adapting to 1024 resolution withoutexplicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.</description><author>Yihong Luo, Xiaolong Chen, Jing Tang</author><pubDate>Tue, 19 Mar 2024 18:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12931v1</guid></item><item><title>Supporting Energy Policy Research with Large Language Models</title><link>http://arxiv.org/abs/2403.12924v1</link><description>The recent growth in renewable energy development in the United States hasbeen accompanied by a simultaneous surge in renewable energy siting ordinances.These zoning laws play a critical role in dictating the placement of wind andsolar resources that are critical for achieving low-carbon energy futures. Inthis context, efficient access to and management of siting ordinance databecomes imperative. The National Renewable Energy Laboratory (NREL) recentlyintroduced a public wind and solar siting database to fill this need. Thispaper presents a method for harnessing Large Language Models (LLMs) to automatethe extraction of these siting ordinances from legal documents, enabling thisdatabase to maintain accurate up-to-date information in the rapidly changingenergy policy landscape. A novel contribution of this research is theintegration of a decision tree framework with LLMs. Our results show that thisapproach is 85 to 90% accurate with outputs that can be used directly indownstream quantitative modeling. We discuss opportunities to use this work tosupport similar large-scale policy research in the energy sector. By unlockingnew efficiencies in the extraction and analysis of legal documents using LLMs,this study enables a path forward for automated large-scale energy policyresearch.</description><author>Grant Buster, Pavlo Pinchuk, Jacob Barrons, Ryan McKeever, Aaron Levine, Anthony Lopez</author><pubDate>Tue, 19 Mar 2024 18:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12924v1</guid></item><item><title>Contextual AD Narration with Interleaved Multimodal Sequence</title><link>http://arxiv.org/abs/2403.12922v1</link><description>The Audio Description (AD) task aims to generate descriptions of visualelements for visually impaired individuals to help them access long-form videocontents, like movie. With video feature, text, character bank and contextinformation as inputs, the generated ADs are able to correspond to thecharacters by name and provide reasonable, contextual descriptions to helpaudience understand the storyline of movie. To achieve this goal, we propose toleverage pre-trained foundation models through a simple and unified frameworkto generate ADs with interleaved multimodal sequence as input, termed asUni-AD. To enhance the alignment of features across various modalities withfiner granularity, we introduce a simple and lightweight module that maps videofeatures into the textual feature space. Moreover, we also propose acharacter-refinement module to provide more precise information by identifyingthe main characters who play more significant role in the video context. Withthese unique designs, we further incorporate contextual information and acontrastive loss into our architecture to generate more smooth and contextualADs. Experiments on the MAD-eval dataset show that Uni-AD can achievestate-of-the-art performance on AD generation, which demonstrates theeffectiveness of our approach. Code will be available athttps://github.com/MCG-NJU/Uni-AD.</description><author>Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang</author><pubDate>Tue, 19 Mar 2024 18:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12922v1</guid></item><item><title>Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization</title><link>http://arxiv.org/abs/2205.07473v3</link><description>Spiking neural networks (SNNs) operating with asynchronous discrete eventsshow higher energy efficiency with sparse computation. A popular approach forimplementing deep SNNs is ANN-SNN conversion combining both efficient trainingof ANNs and efficient inference of SNNs. However, the accuracy loss is usuallynon-negligible, especially under a few time steps, which restricts theapplications of SNN on latency-sensitive edge devices greatly. In this paper,we first identify that such performance degradation stems from themisrepresentation of the negative or overflow residual membrane potential inSNNs. Inspired by this, we decompose the conversion error into three parts:quantization error, clipping error, and residual membrane potentialrepresentation error. With such insights, we propose a two-stage conversionalgorithm to minimize those errors respectively. Besides, We show each stageachieves significant performance gains in a complementary manner. By evaluatingon challenging datasets including CIFAR-10, CIFAR- 100 and ImageNet, theproposed method demonstrates the state-of-the-art performance in terms ofaccuracy, latency and energy preservation. Furthermore, our method is evaluatedusing a more challenging object detection task, revealing notable gains inregression performance under ultra-low latency when compared to existingspike-based detection algorithms. Codes are available athttps://github.com/Windere/snn-cvt-dual-phase.</description><author>Ziming Wang, Shuang Lian, Yuhao Zhang, Xiaoxin Cui, Rui Yan, Huajin Tang</author><pubDate>Tue, 19 Mar 2024 18:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.07473v3</guid></item><item><title>Semantic Layering in Room Segmentation via LLMs</title><link>http://arxiv.org/abs/2403.12920v1</link><description>In this paper, we introduce Semantic Layering in Room Segmentation via LLMs(SeLRoS), an advanced method for semantic room segmentation by integratingLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlikeprevious approaches that solely focus on the geometric segmentation of indoorenvironments, our work enriches segmented maps with semantic data, includingobject identification and spatial relationships, to enhance robotic navigation.By leveraging LLMs, we provide a novel framework that interprets and organizescomplex information about each segmented area, thereby improving the accuracyand contextual relevance of room segmentation. Furthermore, SeLRoS overcomesthe limitations of existing algorithms by using a semantic evaluation method toaccurately distinguish true room divisions from those erroneously generated byfurniture and segmentation inaccuracies. The effectiveness of SeLRoS isverified through its application across 30 different 3D environments. Sourcecode and experiment videos for this work are available at:https://sites.google.com/view/selros.</description><author>Taehyeon Kim, Byung-Cheol Min</author><pubDate>Tue, 19 Mar 2024 18:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12920v1</guid></item><item><title>Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts</title><link>http://arxiv.org/abs/2403.12918v1</link><description>Pretrained Language Models (PLMs) have advanced Natural Language Processing(NLP) tasks significantly, but finetuning PLMs on low-resource datasets posessignificant challenges such as instability and overfitting. Previous methodstackle these issues by finetuning a strategically chosen subnetwork on adownstream task, while keeping the remaining weights fixed to the pretrainedweights. However, they rely on a suboptimal criteria for sub-network selection,leading to suboptimal solutions. To address these limitations, we propose aregularization method based on attention-guided weight mixup for finetuningPLMs. Our approach represents each network weight as a mixup of task-specificweight and pretrained weight, controlled by a learnable attention parameter,providing finer control over sub-network selection. Furthermore, we employ abi-level optimization (BLO) based framework on two separate splits of thetraining dataset, improving generalization and combating overfitting. Wevalidate the efficacy of our proposed method through extensive experiments,demonstrating its superiority over previous methods, particularly in thecontext of finetuning PLMs on low-resource datasets.</description><author>Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie</author><pubDate>Tue, 19 Mar 2024 18:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12918v1</guid></item><item><title>Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling</title><link>http://arxiv.org/abs/2403.11942v2</link><description>Facial Expression Recognition (FER) plays a crucial role in computer visionand finds extensive applications across various fields. This paper aims topresent our approach for the upcoming 6th Affective Behavior Analysisin-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facialexpression recognition task, The limited size of the FER dataset poses achallenge to the expression recognition model's generalization ability,resulting in subpar recognition performance. To address this problem, we employa semi-supervised learning technique to generate expression categorypseudo-labels for unlabeled face data. At the same time, we uniformly sampledthe labeled facial expression samples and implemented a debiased feedbacklearning strategy to address the problem of category imbalance in the datasetand the possible data bias in semi-supervised learning. Moreover, to furthercompensate for the limitation and bias of features obtained only from staticimages, we introduced a Temporal Encoder to learn and capture temporalrelationships between neighbouring expression image features. In the 6th ABAWcompetition, our method achieved outstanding results on the official validationset, a result that fully confirms the effectiveness and competitiveness of ourproposed method.</description><author>Jun Yu, Zhihong Wei, Zhongpeng Cai, Gongpeng Zhao, Zerui Zhang, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu</author><pubDate>Tue, 19 Mar 2024 18:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11942v2</guid></item><item><title>Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition</title><link>http://arxiv.org/abs/2311.15619v2</link><description>Large-scale visual-language pre-trained models have achieved significantsuccess in various video tasks. However, most existing methods follow an "adaptthen align" paradigm, which adapts pre-trained image encoders to modelvideo-level representations and utilizes one-hot or text embedding of theaction labels for supervision. This paradigm overlooks the challenge of mappingfrom static images to complicated activity concepts. In this paper, we proposea novel "Align before Adapt" (ALT) paradigm. Prior to adapting to videorepresentation learning, we exploit the entity-to-region alignments for eachframe. The alignments are fulfilled by matching the region-aware imageembeddings to an offline-constructed text corpus. With the aligned entities, wefeed their text embeddings to a transformer-based video adapter as the queries,which can help extract the semantics of the most important entities from avideo to a vector. This paradigm reuses the visual-language alignment of VLPduring adaptation and tries to explain an action by the underlying entities.This helps understand actions by bridging the gap with complex activitysemantics, particularly when facing unfamiliar or unseen categories. ALTdemonstrates competitive performance while maintaining remarkably lowcomputational costs. In fully supervised experiments, it achieves 88.1% top-1accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms theprevious state-of-the-art methods in both zero-shot and few-shot experiments,emphasizing its superior generalizability across various learning scenarios.</description><author>Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng</author><pubDate>Tue, 19 Mar 2024 18:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15619v2</guid></item><item><title>Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model</title><link>http://arxiv.org/abs/2403.12915v1</link><description>We introduce the Pyramid Diffusion Model (PDM), a novel architecture designedfor ultra-high-resolution image synthesis. PDM utilizes a pyramid latentrepresentation, providing a broader design space that enables more flexible,structured, and efficient perceptual compression which enable AutoEncoder andNetwork of Diffusion to equip branches and deeper layers. To enhance PDM'scapabilities for generative tasks, we propose the integration ofSpatial-Channel Attention and Res-Skip Connection, along with the utilizationof Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network andAutoEncoder. In summary, PDM achieves the synthesis of images with a 2Kresolution for the first time, demonstrated on two new datasets comprisingimages of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believethat this work offers an alternative approach to designing scalable imagegenerative models, while also providing incremental reinforcement for existingframeworks.</description><author>Jiajie Yang</author><pubDate>Tue, 19 Mar 2024 18:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12915v1</guid></item><item><title>Fixed points of nonnegative neural networks</title><link>http://arxiv.org/abs/2106.16239v8</link><description>We use fixed point theory to analyze nonnegative neural networks, which wedefine as neural networks that map nonnegative vectors to nonnegative vectors.We first show that nonnegative neural networks with nonnegative weights andbiases can be recognized as monotonic and (weakly) scalable mappings within theframework of nonlinear Perron-Frobenius theory. This fact enables us to provideconditions for the existence of fixed points of nonnegative neural networkshaving inputs and outputs of the same dimension, and these conditions areweaker than those recently obtained using arguments in convex analysis.Furthermore, we prove that the shape of the fixed point set of nonnegativeneural networks with nonnegative weights and biases is an interval, which undermild conditions degenerates to a point. These results are then used to obtainthe existence of fixed points of more general nonnegative neural networks. Froma practical perspective, our results contribute to the understanding of thebehavior of autoencoders, and we also offer valuable mathematical machinery forfuture developments in deep equilibrium models.</description><author>Tomasz J. Piotrowski, Renato L. G. Cavalcante, Mateusz Gabor</author><pubDate>Tue, 19 Mar 2024 18:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.16239v8</guid></item><item><title>BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP</title><link>http://arxiv.org/abs/2309.13173v2</link><description>Large Language Models (LLMs) have emerged as one of the most importantbreakthroughs in NLP for their impressive skills in language generation andother language-specific tasks. Though LLMs have been evaluated in varioustasks, mostly in English, they have not yet undergone thorough evaluation inunder-resourced languages such as Bengali (Bangla). To this end, this paperintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs tobenchmark their performance in the Bengali language that has modest resources.In this regard, we select various important and diverse Bengali NLP tasks, suchas text summarization, question answering, paraphrasing, natural languageinference, transliteration, text classification, and sentiment analysis forzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, andClaude-2. Our experimental results demonstrate that while in some Bengali NLPtasks, zero-shot LLMs could achieve performance on par, or even better thancurrent SOTA fine-tuned models; in most tasks, their performance is quite poor(with the performance of open-source LLMs like LLaMA-2-13b-chat beingsignificantly bad) in comparison to the current SOTA results. Therefore, itcalls for further efforts to develop a better understanding of LLMs inmodest-resourced languages like Bengali.</description><author>Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque</author><pubDate>Tue, 19 Mar 2024 18:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13173v2</guid></item><item><title>Yell At Your Robot: Improving On-the-Fly from Language Corrections</title><link>http://arxiv.org/abs/2403.12910v1</link><description>Hierarchical policies that combine language and low-level control have beenshown to perform impressively long-horizon robotic tasks, by leveraging eitherzero-shot high-level planners like pretrained language and vision-languagemodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.However, for complex and dexterous skills, attaining high success rates onlong-horizon tasks still represents a major challenge -- the longer the taskis, the more likely it is that some stage will fail. Can humans help the robotto continuously improve its long-horizon task performance through intuitive andnatural feedback? In this paper, we make the following observation: high-levelpolicies that index into sufficiently rich and expressive low-levellanguage-conditioned skills can be readily supervised with human feedback inthe form of language corrections. We show that even fine-grained corrections,such as small movements ("move a bit to the left"), can be effectivelyincorporated into high-level policies, and that such corrections can be readilyobtained from humans observing the robot and making occasional suggestions.This framework enables robots not only to rapidly adapt to real-time languagefeedback, but also incorporate this feedback into an iterative training schemethat improves the high-level policy's ability to correct errors in bothlow-level execution and high-level decision-making purely from verbal feedback.Our evaluation on real hardware shows that this leads to significantperformance improvement in long-horizon, dexterous manipulation tasks withoutthe need for any additional teleoperation. Videos and code are available athttps://yay-robot.github.io/.</description><author>Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn</author><pubDate>Tue, 19 Mar 2024 18:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12910v1</guid></item><item><title>The Alignment Problem from a Deep Learning Perspective</title><link>http://arxiv.org/abs/2209.00626v6</link><description>In coming years or decades, artificial general intelligence (AGI) may surpasshuman capabilities at many critical tasks. We argue that, without substantialeffort to prevent it, AGIs could learn to pursue goals that are in conflict(i.e. misaligned) with human interests. If trained like today's most capablemodels, AGIs could learn to act deceptively to receive higher reward, learnmisaligned internally-represented goals which generalize beyond theirfine-tuning distributions, and pursue those goals using power-seekingstrategies. We review emerging evidence for these properties. AGIs with theseproperties would be difficult to align and may appear aligned even when theyare not. Finally, we briefly outline how the deployment of misaligned AGIsmight irreversibly undermine human control over the world, and we reviewresearch directions aimed at preventing this outcome.</description><author>Richard Ngo, Lawrence Chan, SÃ¶ren Mindermann</author><pubDate>Tue, 19 Mar 2024 18:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00626v6</guid></item><item><title>Vertical Federated Image Segmentation</title><link>http://arxiv.org/abs/2401.07931v2</link><description>With the popularization of AI solutions for image based problems, there hasbeen a growing concern for both data privacy and acquisition. In a large numberof cases, information is located on separate data silos and it can be difficultfor a developer to consolidate all of it in a fashion that is appropriate formachine learning model development. Alongside this, a portion of theselocalized data regions may not have access to a labelled ground truth. Thisindicates that they have the capacity to reach conclusions numerically, but arenot able to assign classifications amid a lack of pertinent information. Such adetermination is often negligible, especially when attempting to develop imagebased solutions that often necessitate this capability. With this being thecase, we propose an innovative vertical federated learning (VFL) modelarchitecture that can operate under this common set of conditions. This is thefirst (and currently the only) implementation of a system that can work underthe constraints of a VFL environment and perform image segmentation whilemaintaining nominal accuracies. We achieved this by utilizing an FCN thatboasts the ability to operate on federates that lack labelled data andprivately share the respective weights with a central server, that of whichhosts the necessary features for classification. Tests were conducted on theCamVid dataset in order to determine the impact of heavy feature compressionrequired for the transfer of information between federates, as well as to reachnominal conclusions about the overall performance metrics when working undersuch constraints.</description><author>Paul K. Mandal, Cole Leo</author><pubDate>Tue, 19 Mar 2024 18:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07931v2</guid></item><item><title>Self-Supervised Learning for Image Super-Resolution and Deblurring</title><link>http://arxiv.org/abs/2312.11232v2</link><description>Self-supervised methods have recently proved to be nearly as effective assupervised methods in various imaging inverse problems, paving the way forlearning-based methods in scientific and medical imaging applications whereground truth data is hard or expensive to obtain. This is the case in magneticresonance imaging and computed tomography. These methods critically rely oninvariance to translations and/or rotations of the image distribution to learnfrom incomplete measurement data alone. However, existing approaches fail toobtain competitive performances in the problems of image super-resolution anddeblurring, which play a key role in most imaging systems. In this work, weshow that invariance to translations and rotations is insufficient to learnfrom measurements that only contain low-frequency information. Instead, wepropose a new self-supervised approach that leverages the fact that many imagedistributions are approximately scale-invariant, and that enables recoveringhigh-frequency information lost in the measurement process. We demonstratethroughout a series of experiments on real datasets that the proposed methodoutperforms other self-supervised approaches, and obtains performances on parwith fully supervised learning.</description><author>JÃ©rÃ©my Scanvic, Mike Davies, Patrice Abry, JuliÃ¡n Tachella</author><pubDate>Tue, 19 Mar 2024 18:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11232v2</guid></item><item><title>SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction</title><link>http://arxiv.org/abs/2403.11492v2</link><description>Predicting the future motion of surrounding agents is essential forautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixedenvironments. Context information, such as road maps and surrounding agents'states, provides crucial geometric and semantic information for motion behaviorprediction. To this end, recent works explore two-stage prediction frameworkswhere coarse trajectories are first proposed, and then used to select criticalcontext information for trajectory refinement. However, they either incur alarge amount of computation or bring limited improvement, if not both. In thispaper, we introduce a novel scenario-adaptive refinement strategy, namedSmartRefine, to refine prediction with minimal additional computation.Specifically, SmartRefine can comprehensively adapt refinement configurationsbased on each scenario's properties, and smartly chooses the number ofrefinement iterations by introducing a quality score to measure the predictionquality and remaining refinement potential of each scenario. SmartRefine isdesigned as a generic and flexible approach that can be seamlessly integratedinto most state-of-the-art motion prediction models. Experiments on Argoverse(1 &amp; 2) show that our method consistently improves the prediction accuracy ofmultiple state-of-the-art prediction models. Specifically, by addingSmartRefine to QCNet, we outperform all published ensemble-free works on theArgoverse 2 leaderboard (single agent track) at submission. Comprehensivestudies are also conducted to ablate design choices and explore the mechanismbehind multi-iteration refinement. Codes are available athttps://github.com/opendilab/SmartRefine/</description><author>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu</author><pubDate>Tue, 19 Mar 2024 18:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11492v2</guid></item><item><title>TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation</title><link>http://arxiv.org/abs/2403.12906v1</link><description>Texturing 3D humans with semantic UV maps remains a challenge due to thedifficulty of acquiring reasonably unfolded UV. Despite recent text-to-3Dadvancements in supervising multi-view renderings using large text-to-image(T2I) models, issues persist with generation speed, text consistency, andtexture quality, resulting in data scarcity among existing datasets. We presentTexDreamer, the first zero-shot multimodal high-fidelity 3D human texturegeneration model. Utilizing an efficient texture adaptation finetuningstrategy, we adapt large T2I model to a semantic UV structure while preservingits original generalization capability. Leveraging a novel feature translatormodule, the trained model is capable of generating high-fidelity 3D humantextures from either text or image within seconds. Furthermore, we introduceArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024)3D human texture dataset which contains 50k high-fidelity textures with textdescriptions.</description><author>Yufei Liu, Junwei Zhu, Junshu Tang, Shijie Zhang, Jiangning Zhang, Weijian Cao, Chengjie Wang, Yunsheng Wu, Dongjin Huang</author><pubDate>Tue, 19 Mar 2024 18:02:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12906v1</guid></item><item><title>Radiology-GPT: A Large Language Model for Radiology</title><link>http://arxiv.org/abs/2306.08666v2</link><description>We introduce Radiology-GPT, a large language model for radiology. Using aninstruction tuning approach on an extensive dataset of radiology domainknowledge, Radiology-GPT demonstrates superior performance compared to generallanguage models such as StableLM, Dolly and LLaMA. It exhibits significantversatility in radiological diagnosis, research, and communication. This workserves as a catalyst for future developments in clinical NLP. The successfulimplementation of Radiology-GPT is indicative of the potential of localizinggenerative large language models, specifically tailored for distinctive medicalspecialties, while ensuring adherence to privacy standards such as HIPAA. Theprospect of developing individualized, large-scale language models that caterto specific needs of various hospitals presents a promising direction. Thefusion of conversational competence and domain-specific knowledge in thesemodels is set to foster future development in healthcare AI. A demo ofRadiology-GPT is available athttps://huggingface.co/spaces/allen-eric/radiology-gpt.</description><author>Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, Tianming Liu</author><pubDate>Tue, 19 Mar 2024 18:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08666v2</guid></item><item><title>The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun</title><link>http://arxiv.org/abs/2402.10311v4</link><description>The word order of a sentence is shaped by multiple principles. The principleof syntactic dependency distance minimization is in conflict with the principleof surprisal minimization (or predictability maximization) in single headsyntactic dependency structures: while the former predicts that the head shouldbe placed at the center of the linear arrangement, the latter predicts that thehead should be placed at one of the ends (either first or last). A criticalquestion is when surprisal minimization (or predictability maximization) shouldsurpass syntactic dependency distance minimization. In the context of singlehead structures, it has been predicted that this is more likely to happen whentwo conditions are met, i.e. (a) fewer words are involved and (b) words areshorter. Here we test the prediction on the noun phrase when it is composed ofa demonstrative, a numeral, an adjective and a noun. We find that, acrosspreferred orders in languages, the noun tends to be placed at one of the ends,confirming the theoretical prediction. We also show evidence of anti localityeffects: syntactic dependency distances in preferred orders are longer thanexpected by chance.</description><author>Ramon Ferrer-i-Cancho</author><pubDate>Tue, 19 Mar 2024 17:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10311v4</guid></item><item><title>SynCDR : Training Cross Domain Retrieval Models with Synthetic Data</title><link>http://arxiv.org/abs/2401.00420v2</link><description>In cross-domain retrieval, a model is required to identify images from thesame semantic category across two visual domains. For instance, given a sketchof an object, a model needs to retrieve a real image of it from an onlinestore's catalog. A standard approach for such a problem is learning a featurespace of images where Euclidean distances reflect similarity. Even withouthuman annotations, which may be expensive to acquire, prior methods functionreasonably well using unlabeled images for training. Our problem constrainttakes this further to scenarios where the two domains do not necessarily shareany common categories in training data. This can occur when the two domains inquestion come from different versions of some biometric sensor recordingidentities of different people. We posit a simple solution, which is togenerate synthetic data to fill in these missing category examples acrossdomains. This, we do via category preserving translation of images from onevisual domain to another. We compare approaches specifically trained for thistranslation for a pair of domains, as well as those that can use large-scalepre-trained text-to-image diffusion models via prompts, and find that thelatter can generate better replacement synthetic data, leading to more accuratecross-domain retrieval models. Our best SynCDR model can outperform prior artby up to 15\%. Code for our work is available athttps://github.com/samarth4149/SynCDR .</description><author>Samarth Mishra, Carlos D. Castillo, Hongcheng Wang, Kate Saenko, Venkatesh Saligrama</author><pubDate>Tue, 19 Mar 2024 17:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00420v2</guid></item><item><title>Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions</title><link>http://arxiv.org/abs/2403.12007v2</link><description>Digital Behavior Change Interventions (DBCIs) are supporting development ofnew health behaviors. Evaluating their effectiveness is crucial for theirimprovement and understanding of success factors. However, comprehensiveguidance for developers, particularly in small-scale studies with ethicalconstraints, is limited. Building on the CAPABLE project, this study aims todefine effective engagement with DBCIs for supporting cancer patients inenhancing their quality of life. We identify metrics for measuring engagement,explore the interest of both patients and clinicians in DBCIs, and proposehypotheses for assessing the impact of DBCIs in such contexts. Our findingssuggest that clinician prescriptions significantly increase sustainedengagement with mobile DBCIs. In addition, while one weekly engagement with aDBCI is sufficient to maintain well-being, transitioning from extrinsic tointrinsic motivation may require a higher level of engagement.</description><author>Aneta Lisowska, Szymon Wilk, Laura Locati, Mimma Rizzo, Lucia Sacchi, Silvana Quaglini, Matteo Terzaghi, Valentina Tibollo, Mor Peleg</author><pubDate>Tue, 19 Mar 2024 17:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12007v2</guid></item><item><title>Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference</title><link>http://arxiv.org/abs/2403.12900v1</link><description>The rapid advancement of Generative Artificial Intelligence (GenAI) acrossdiverse sectors raises significant environmental concerns, notably the carbonemissions from their cloud and high performance computing (HPC) infrastructure.This paper presents Sprout, an innovative framework designed to address theseconcerns by reducing the carbon footprint of generative Large Language Model(LLM) inference services. Sprout leverages the innovative concept of"generation directives" to guide the autoregressive generation process, therebyenhancing carbon efficiency. Our proposed method meticulously balances the needfor ecological sustainability with the demand for high-quality generationoutcomes. Employing a directive optimizer for the strategic assignment ofgeneration directives to user prompts and an original offline qualityevaluator, Sprout demonstrates a significant reduction in carbon emissions byover 40% in real-world evaluations using the Llama2 LLM and global electricitygrid data. This research marks a critical step toward aligning AI technologywith sustainable practices, highlighting the potential for mitigatingenvironmental impacts in the rapidly expanding domain of generative artificialintelligence.</description><author>Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari</author><pubDate>Tue, 19 Mar 2024 17:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12900v1</guid></item><item><title>Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions</title><link>http://arxiv.org/abs/2309.07875v3</link><description>Training large language models to follow instructions makes them performbetter on a wide range of tasks and generally become more helpful. However, aperfectly helpful model will follow even the most malicious instructions andreadily generate harmful content. In this paper, we raise concerns over thesafety of models that only emphasize helpfulness, not harmlessness, in theirinstruction-tuning. We show that several popular instruction-tuned models arehighly unsafe. Moreover, we show that adding just 3% safety examples (a fewhundred demonstrations) when fine-tuning a model like LLaMA can substantiallyimprove its safety. Our safety-tuning does not make models significantly lesscapable or helpful as measured by standard benchmarks. However, we do findexaggerated safety behaviours, where too much safety-tuning makes models refuseperfectly safe prompts if they superficially resemble unsafe ones. As a whole,our results illustrate trade-offs in training LLMs to be helpful and trainingthem to be safe.</description><author>Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul RÃ¶ttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou</author><pubDate>Tue, 19 Mar 2024 17:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07875v3</guid></item><item><title>mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding</title><link>http://arxiv.org/abs/2403.12895v1</link><description>Structure information is critical for understanding the semantics oftext-rich images, such as documents, tables, and charts. Existing MultimodalLarge Language Models (MLLMs) for Visual Document Understanding are equippedwith text recognition ability but lack general structure understandingabilities for text-rich document images. In this work, we emphasize theimportance of structure information in Visual Document Understanding andpropose the Unified Structure Learning to boost the performance of MLLMs. OurUnified Structure Learning comprises structure-aware parsing tasks andmulti-grained text localization tasks across 5 domains: document, webpage,table, chart, and natural image. To better encode structure information, wedesign a simple and effective vision-to-text module H-Reducer, which can notonly maintain the layout information but also reduce the length of visualfeatures by merging horizontal adjacent patches through convolution, enablingthe LLM to understand high-resolution images more efficiently. Furthermore, byconstructing structure-aware text sequences and multi-grained pairs of textsand bounding boxes for publicly available text-rich images, we build acomprehensive training set DocStruct4M to support structure learning. Finally,we construct a small but high-quality reasoning tuning dataset DocReason25K totrigger the detailed explanation ability in the document domain. Our modelDocOwl 1.5 achieves state-of-the-art performance on 10 visual documentunderstanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLMby more than 10 points in 5/10 benchmarks. Our codes, models, and datasets arepublicly available athttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.</description><author>Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou</author><pubDate>Tue, 19 Mar 2024 17:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12895v1</guid></item><item><title>Clinical Reasoning over Tabular Data and Text with Bayesian Networks</title><link>http://arxiv.org/abs/2403.09481v2</link><description>Bayesian networks are well-suited for clinical reasoning on tabular data, butare less compatible with natural language data, for which neural networksprovide a successful framework. This paper compares and discusses strategies toaugment Bayesian networks with neural text representations, both in agenerative and discriminative manner. This is illustrated with simulationresults for a primary care use case (diagnosis of pneumonia) and discussed in abroader clinical context.</description><author>Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester</author><pubDate>Tue, 19 Mar 2024 17:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09481v2</guid></item><item><title>Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization</title><link>http://arxiv.org/abs/2403.04720v2</link><description>Effectively representing heterogeneous tabular datasets for meta-learningremains an open problem. Previous approaches rely on predefined meta-features,for example, statistical measures or landmarkers. Encoder-based models, such asDataset2Vec, allow us to extract significant meta-features automaticallywithout human intervention. This research introduces a novel encoder-basedrepresentation of tabular datasets implemented within the liltab packageavailable on GitHub https://github.com/azoz01/liltab. Our package is based onan established model for heterogeneous tabular data proposed in [Tomoharu Iwataand Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous AttributeSpaces. In Advances in Neural Information Processing Systems, 2020]. Theproposed approach employs a different model for encoding feature relationships,generating alternative representations compared to existing methods likeDataset2Vec. Both of them leverage the fundamental assumption of datasetsimilarity learning. In this work, we evaluate Dataset2Vec and liltab on twocommon meta-tasks - representing entire datasets and hyperparameteroptimization warm-start. However, validation on an independent metaMIMICdataset highlights the nuanced challenges in representation learning. We showthat general representations may not suffice for some meta-tasks whererequirements are not explicitly considered during extraction.</description><author>Dawid PÅudowski, Antoni Zajko, Anna Kozak, Katarzyna WoÅºnica</author><pubDate>Tue, 19 Mar 2024 17:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04720v2</guid></item><item><title>Bayesian Nonparametrics Meets Data-Driven Robust Optimization</title><link>http://arxiv.org/abs/2401.15771v3</link><description>Training machine learning and statistical models often involves optimizing adata-driven risk criterion. The risk is usually computed with respect to theempirical data distribution, but this may result in poor and unstableout-of-sample performance due to distributional uncertainty. In the spirit ofdistributionally robust optimization, we propose a novel robust criterion bycombining insights from Bayesian nonparametric (i.e., Dirichlet Process) theoryand recent decision-theoretic models of smooth ambiguity-averse preferences.First, we highlight novel connections with standard regularized empirical riskminimization techniques, among which Ridge and LASSO regressions. Then, wetheoretically demonstrate the existence of favorable finite-sample andasymptotic statistical guarantees on the performance of the robust optimizationprocedure. For practical implementation, we propose and study tractableapproximations of the criterion based on well-known Dirichlet Processrepresentations. We also show that the smoothness of the criterion naturallyleads to standard gradient-based numerical optimization. Finally, we provideinsights into the workings of our method by applying it to high-dimensionalsparse linear regression, binary classification, and robust location parameterestimation tasks.</description><author>Nicola Bariletto, Nhat Ho</author><pubDate>Tue, 19 Mar 2024 17:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15771v3</guid></item><item><title>MEDBind: Unifying Language and Multimodal Medical Data Embeddings</title><link>http://arxiv.org/abs/2403.12894v1</link><description>Medical vision-language pretraining models (VLPM) have achieved remarkableprogress in fusing chest X-rays (CXR) with clinical texts, introducingimage-text data binding approaches that enable zero-shot learning anddownstream clinical tasks. However, the current landscape lacks the holisticintegration of additional medical modalities, such as electrocardiograms (ECG).We present MEDBind (Medical Electronic patient recorD), which learns jointembeddings across CXR, ECG, and medical text. Using text data as the centralanchor, MEDBind features tri-modality binding, delivering competitiveperformance in top-K retrieval, zero-shot, and few-shot benchmarks againstestablished VLPM, and the ability for CXR-to-ECG zero-shot classification andretrieval. This seamless integration is achieved through combination ofcontrastive loss on modality-text pairs with our proposed contrastive lossfunction, Edge-Modality Contrastive Loss, fostering a cohesive embedding spacefor CXR, ECG, and text. Finally, we demonstrate that MEDBind can improvedownstream tasks by directly integrating CXR and ECG embeddings into alarge-language model for multimodal prompt tuning.</description><author>Yuan Gao, Sangwook Kim, David E Austin, Chris McIntosh</author><pubDate>Tue, 19 Mar 2024 17:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12894v1</guid></item><item><title>Generic 3D Diffusion Adapter Using Controlled Multi-View Editing</title><link>http://arxiv.org/abs/2403.12032v2</link><description>Open-domain 3D object synthesis has been lagging behind image synthesis dueto limited data and higher computational complexity. To bridge this gap, recentworks have investigated multi-view diffusion but often fall short in either 3Dconsistency, visual quality, or efficiency. This paper proposes MVEdit, whichfunctions as a 3D counterpart of SDEdit, employing ancestral sampling tojointly denoise multi-view images and output high-quality textured meshes.Built on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistencythrough a training-free 3D Adapter, which lifts the 2D views of the lasttimestep into a coherent 3D representation, then conditions the 2D views of thenext timestep using rendered views, without uncompromising visual quality. Withan inference time of only 2-5 minutes, this framework achieves better trade-offbetween quality and speed than score distillation. MVEdit is highly versatileand extendable, with a wide range of applications including text/image-to-3Dgeneration, 3D-to-3D editing, and high-quality texture synthesis. Inparticular, evaluations demonstrate state-of-the-art performance in bothimage-to-3D and text-guided texture generation tasks. Additionally, weintroduce a method for fine-tuning 2D latent diffusion models on small 3Ddatasets with limited resources, enabling fast low-resolution text-to-3Dinitialization.</description><author>Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, Leonidas Guibas</author><pubDate>Tue, 19 Mar 2024 17:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12032v2</guid></item><item><title>Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</title><link>http://arxiv.org/abs/2402.10109v2</link><description>Many diagnostic errors occur because clinicians cannot easily access relevantinformation in patient Electronic Health Records (EHRs). In this work wepropose a method to use LLMs to identify pieces of evidence in patient EHR datathat indicate increased or decreased risk of specific diagnoses; our ultimateaim is to increase access to evidence and reduce diagnostic errors. Inparticular, we propose a Neural Additive Model to make predictions backed byevidence with individualized risk estimates at time-points where clinicians arestill uncertain, aiming to specifically mitigate delays in diagnosis and errorsstemming from an incomplete differential. To train such a model, it isnecessary to infer temporally fine-grained retrospective labels of eventual"true" diagnoses. We do so with LLMs, to ensure that the input text is frombefore a confident diagnosis can be made. We use an LLM to retrieve an initialpool of evidence, but then refine this set of evidence according tocorrelations learned by the model. We conduct an in-depth evaluation of theusefulness of our approach by simulating how it might be used by a clinician todecide between a pre-defined list of differential diagnoses.</description><author>Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C. Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace</author><pubDate>Tue, 19 Mar 2024 17:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10109v2</guid></item><item><title>Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects</title><link>http://arxiv.org/abs/2402.04906v2</link><description>Knowledge of the effect of interventions, called the treatment effect, isparamount for decision-making. Approaches to estimating this treatment effect,e.g. by using Conditional Average Treatment Effect (CATE) estimators, oftenonly provide a point estimate of this treatment effect, while additionaluncertainty quantification is frequently desired instead. Therefore, we presenta novel method, the Conformal Monte Carlo (CMC) meta-learners, leveragingconformal predictive systems, Monte Carlo sampling, and CATE meta-learners, toinstead produce a predictive distribution usable in individualizeddecision-making. Furthermore, we show how specific assumptions on the noisedistribution of the outcome heavily affect these uncertainty predictions.Nonetheless, the CMC framework shows strong experimental coverage whileretaining small interval widths to provide estimates of the true individualtreatment effect.</description><author>Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke</author><pubDate>Tue, 19 Mar 2024 17:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04906v2</guid></item><item><title>Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types</title><link>http://arxiv.org/abs/2403.12891v1</link><description>In this study, we introduce a novel visual imitation network with a spatialattention module for robotic assisted feeding (RAF). The goal is to acquire(i.e., scoop) food items from a bowl. However, achieving robust and adaptivefood manipulation is particularly challenging. To deal with this, we propose aframework that integrates visual perception with imitation learning to enablethe robot to handle diverse scenarios during scooping. Our approach, named AVIL(adaptive visual imitation learning), exhibits adaptability and robustnessacross different bowl configurations in terms of material, size, and position,as well as diverse food types including granular, semi-solid, and liquid, evenin the presence of distractors. We validate the effectiveness of our approachby conducting experiments on a real robot. We also compare its performance witha baseline. The results demonstrate improvement over the baseline across allscenarios, with an enhancement of up to 2.5 times in terms of a success metric.Notably, our model, trained solely on data from a transparent glass bowlcontaining granular cereals, showcases generalization ability when testedzero-shot on other bowl configurations with different types of food.</description><author>Rui Liu, Amisha Bhaskar, Pratap Tokekar</author><pubDate>Tue, 19 Mar 2024 17:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12891v1</guid></item><item><title>BugNIST - a Large Volumetric Dataset for Object Detection under Domain Shift</title><link>http://arxiv.org/abs/2304.01838v2</link><description>Domain shift significantly influences the performance of deep learningalgorithms, particularly for object detection within volumetric 3D images.Annotated training data is essential for deep learning-based object detection.However, annotating densely packed objects is time-consuming and costly.Instead, we suggest training models on individually scanned objects, causing adomain shift between training and detection data. To address this challenge, weintroduce the BugNIST dataset, comprising 9154 micro-CT volumes of 12 bug typesand 388 volumes of tightly packed bug mixtures. This dataset is characterizedby having objects with the same appearance in the source and target domain,which is uncommon for other benchmark datasets for domain shift. Duringtraining, individual bug volumes labeled by class are utilized, while testingemploys mixtures with center point annotations and bug type labels. Togetherwith the dataset, we provide a baseline detection analysis, aiming at advancingthe field of 3D object detection methods.</description><author>Patrick MÃ¸ller Jensen, Vedrana Andersen Dahl, Carsten Gundlach, Rebecca Engberg, Hans Martin Kjer, Anders Bjorholm Dahl</author><pubDate>Tue, 19 Mar 2024 17:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01838v2</guid></item><item><title>Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML</title><link>http://arxiv.org/abs/2306.05109v4</link><description>Medical applications of machine learning (ML) have experienced a surge inpopularity in recent years. The intensive care unit (ICU) is a natural habitatfor ML given the abundance of available data from electronic health records.Models have been proposed to address numerous ICU prediction tasks like theearly detection of complications. While authors frequently reportstate-of-the-art performance, it is challenging to verify claims ofsuperiority. Datasets and code are not always published, and cohortdefinitions, preprocessing pipelines, and training setups are difficult toreproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modularframework that allows researchers to define reproducible and comparableclinical ML experiments; we offer an end-to-end solution from cohort definitionto model evaluation. The framework natively supports most open-access ICUdatasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to futureICU datasets. Combined with a transparent preprocessing pipeline and extensibletraining code for multiple ML and deep learning models, YAIB enables unifiedmodel development. Our benchmark comes with five predefined establishedprediction tasks (mortality, acute kidney injury, sepsis, kidney function, andlength of stay) developed in collaboration with clinicians. Adding furthertasks is straightforward by design. Using YAIB, we demonstrate that the choiceof dataset, cohort definition, and preprocessing have a major impact on theprediction performance - often more so than model class - indicating an urgentneed for YAIB as a holistic benchmarking tool. We provide our work to theclinical ML community to accelerate method development and enable real-worldclinical implementations. Software Repository:https://github.com/rvandewater/YAIB.</description><author>Robin van de Water, Hendrik Schmidt, Paul Elbers, Patrick Thoral, Bert Arnrich, Patrick Rockenschaub</author><pubDate>Tue, 19 Mar 2024 17:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05109v4</guid></item><item><title>MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title><link>http://arxiv.org/abs/2403.09611v2</link><description>In this work, we discuss building performant Multimodal Large Language Models(MLLMs). In particular, we study the importance of various architecturecomponents and data choices. Through careful and comprehensive ablations of theimage encoder, the vision language connector, and various pre-training datachoices, we identified several crucial design lessons. For example, wedemonstrate that for large-scale multimodal pre-training using a careful mix ofimage-caption, interleaved image-text, and text-only data is crucial forachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,compared to other published pre-training results. Further, we show that theimage encoder together with image resolution and the image token count hassubstantial impact, while the vision-language connector design is ofcomparatively negligible importance. By scaling up the presented recipe, webuild MM1, a family of multimodal models up to 30B parameters, including bothdense models and mixture-of-experts (MoE) variants, that are SOTA inpre-training metrics and achieve competitive performance after supervisedfine-tuning on a range of established multimodal benchmarks. Thanks tolarge-scale pre-training, MM1 enjoys appealing properties such as enhancedin-context learning, and multi-image reasoning, enabling few-shotchain-of-thought prompting.</description><author>Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu HÃ¨, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang</author><pubDate>Tue, 19 Mar 2024 17:37:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09611v2</guid></item><item><title>Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport</title><link>http://arxiv.org/abs/2403.12887v1</link><description>We study the convergence of gradient flow for the training of deep neuralnetworks. If Residual Neural Networks are a popular example of very deeparchitectures, their training constitutes a challenging optimization problemdue notably to the non-convexity and the non-coercivity of the objective. Yet,in applications, those tasks are successfully solved by simple optimizationalgorithms such as gradient descent. To better understand this phenomenon, wefocus here on a ``mean-field'' model of infinitely deep and arbitrarily wideResNet, parameterized by probability measures over the product set of layersand parameters and with constant marginal on the set of layers. Indeed, in thecase of shallow neural networks, mean field models have proven to benefit fromsimplified loss-landscapes and good theoretical guarantees when trained withgradient flow for the Wasserstein metric on the set of probability measures.Motivated by this approach, we propose to train our model with gradient floww.r.t. the conditional Optimal Transport distance: a restriction of theclassical Wasserstein distance which enforces our marginal condition. Relyingon the theory of gradient flows in metric spaces we first show thewell-posedness of the gradient flow equation and its consistency with thetraining of ResNets at finite width. Performing a local Polyak-\L{}ojasiewiczanalysis, we then show convergence of the gradient flow for well-choseninitializations: if the number of features is finite but sufficiently large andthe risk is sufficiently small at initialization, the gradient flow convergestowards a global minimizer. This is the first result of this type forinfinitely deep and arbitrarily wide ResNets.</description><author>RaphaÃ«l Barboni, Gabriel PeyrÃ©, FranÃ§ois-Xavier Vialard</author><pubDate>Tue, 19 Mar 2024 17:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12887v1</guid></item><item><title>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</title><link>http://arxiv.org/abs/2402.03094v2</link><description>This paper studies the challenging cross-domain few-shot object detection(CD-FSOD), aiming to develop an accurate object detector for novel domains withminimal labeled examples. While transformer-based open-set detectors, such asDE-ViT, show promise in traditional few-shot object detection, theirgeneralization to CD-FSOD remains unclear: 1) can such open-set detectionmethods easily generalize to CD-FSOD? 2) If not, how can models be enhancedwhen facing huge domain gaps? To answer the first question, we employ measuresincluding style, inter-class variance (ICV), and indefinable boundaries (IB) tounderstand the domain gap. Based on these measures, we establish a newbenchmark named CD-FSOD to evaluate object detection methods, revealing thatmost of the current approaches fail to generalize across domains. Technically,we observe that the performance decline is associated with our proposedmeasures: style, ICV, and IB. Consequently, we propose several novel modules toaddress these issues. First, the learnable instance features align initialfixed instances with target categories, enhancing feature distinctiveness.Second, the instance reweighting module assigns higher importance tohigh-quality instances with slight IB. Third, the domain prompter encouragesfeatures resilient to different styles by synthesizing imaginary domainswithout altering semantic contents. These techniques collectively contribute tothe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),significantly improving upon the base DE-ViT. Experimental results validate theefficacy of our model. All datasets, codes, and models will be released to thecommunity.</description><author>Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang</author><pubDate>Tue, 19 Mar 2024 17:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03094v2</guid></item><item><title>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</title><link>http://arxiv.org/abs/2403.12886v1</link><description>The domain of 3D talking head generation has witnessed significant progressin recent years. A notable challenge in this field consists in blendingspeech-related motions with expression dynamics, which is primarily caused bythe lack of comprehensive 3D datasets that combine diversity in spokensentences with a variety of facial expressions. Whereas literature worksattempted to exploit 2D video data and parametric 3D models as a workaround,these still show limitations when jointly modeling the two motions. In thiswork, we address this problem from a different perspective, and propose aninnovative data-driven technique that we used for creating a synthetic dataset,called EmoVOCA, obtained by combining a collection of inexpressive 3D talkingheads and a set of 3D expressive sequences. To demonstrate the advantages ofthis approach, and the quality of the dataset, we then designed and trained anemotional 3D talking head generator that accepts a 3D face, an audio file, anemotion label, and an intensity value as inputs, and learns to animate theaudio-synchronized lip movements with expressive traits of the face.Comprehensive experiments, both quantitative and qualitative, using our dataand generator evidence superior ability in synthesizing convincing animations,when compared with the best performing methods in the literature. Our code andpre-trained model will be made available.</description><author>Federico Nocentini, Claudio Ferrari, Stefano Berretti</author><pubDate>Tue, 19 Mar 2024 17:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12886v1</guid></item><item><title>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning</title><link>http://arxiv.org/abs/2403.12884v1</link><description>Recent advances in visual reasoning (VR), particularly with the aid of LargeVision-Language Models (VLMs), show promise but require access to large-scaledatasets and face challenges such as high computational costs and limitedgeneralization capabilities. Compositional visual reasoning approaches haveemerged as effective strategies; however, they heavily rely on the commonsenseknowledge encoded in Large Language Models (LLMs) to perform planning,reasoning, or both, without considering the effect of their decisions on thevisual reasoning process, which can lead to errors or failed procedures. Toaddress these challenges, we introduce HYDRA, a multi-stage dynamiccompositional visual reasoning framework designed for reliable andincrementally progressive general reasoning. HYDRA integrates three essentialmodules: a planner, a Reinforcement Learning (RL) agent serving as a cognitivecontroller, and a reasoner. The planner and reasoner modules utilize an LLM togenerate instruction samples and executable code from the selected instruction,respectively, while the RL agent dynamically interacts with these modules,making high-level decisions on selection of the best instruction sample giveninformation from the historical state stored through a feedback loop. Thisadaptable design enables HYDRA to adjust its actions based on previous feedbackreceived during the reasoning process, leading to more reliable reasoningoutputs and ultimately enhancing its overall effectiveness. Our frameworkdemonstrates state-of-the-art performance in various VR tasks on four differentwidely-used datasets.</description><author>Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi</author><pubDate>Tue, 19 Mar 2024 17:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12884v1</guid></item><item><title>Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments</title><link>http://arxiv.org/abs/2403.12883v1</link><description>In this paper, we address unsupervised domain adaptation under noisyenvironments, which is more challenging and practical than traditional domainadaptation. In this scenario, the model is prone to overfitting noisy labels,resulting in a more pronounced domain shift and a notable decline in theoverall model performance. Previous methods employed prototype methods fordomain adaptation on robust feature spaces. However, these approaches struggleto effectively classify classes with similar features under noisy environments.To address this issue, we propose a new method to detect and correct confusingclass pair. We first divide classes into easy and hard classes based on thesmall loss criterion. We then leverage the top-2 predictions for each sampleafter aligning the source and target domain to find the confusing pair in thehard classes. We apply label correction to the noisy samples within theconfusing pair. With the proposed label correction method, we can train ourmodel with more accurate labels. Extensive experiments confirm theeffectiveness of our method and demonstrate its favorable performance comparedwith existing state-of-the-art methods. Our codes are publicly available athttps://github.com/Hehxcf/CPC/.</description><author>Churan Zhi, Junbao Zhuo, Shuhui Wang</author><pubDate>Tue, 19 Mar 2024 17:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12883v1</guid></item><item><title>Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation</title><link>http://arxiv.org/abs/2311.09684v2</link><description>This study examines the effect of prompt engineering on the performance ofLarge Language Models (LLMs) in clinical note generation. We introduce anAutomatic Prompt Optimization (APO) framework to refine initial prompts andcompare the outputs of medical experts, non-medical experts, and APO-enhancedGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance instandardizing prompt quality across clinical note sections. A human-in-the-loopapproach shows that experts maintain content quality post-APO, with apreference for their own modifications, suggesting the value of expertcustomization. We recommend a two-phase optimization process, leveragingAPO-GPT4 for consistency and expert input for personalization.</description><author>Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu</author><pubDate>Tue, 19 Mar 2024 17:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09684v2</guid></item><item><title>Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models</title><link>http://arxiv.org/abs/2403.12881v1</link><description>Open-sourced Large Language Models (LLMs) have achieved great success invarious NLP tasks, however, they are still far inferior to API-based modelswhen acting as agents. How to integrate agent ability into general LLMs becomesa crucial and urgent problem. This paper first delivers three key observations:(1) the current agent training corpus is entangled with both formats followingand agent reasoning, which significantly shifts from the distribution of itspre-training data; (2) LLMs exhibit different learning speeds on thecapabilities required by agent tasks; and (3) current approaches haveside-effects when improving agent abilities by introducing hallucinations.Based on the above findings, we propose Agent-FLAN to effectively Fine-tuneLANguage models for Agents. Through careful decomposition and redesign of thetraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by3.5\% across various agent evaluation datasets. With comprehensivelyconstructed negative samples, Agent-FLAN greatly alleviates the hallucinationissues based on our established evaluation benchmark. Besides, it consistentlyimproves the agent capability of LLMs when scaling model sizes while slightlyenhancing the general capability of LLMs. The code will be available athttps://github.com/InternLM/Agent-FLAN.</description><author>Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao</author><pubDate>Tue, 19 Mar 2024 17:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12881v1</guid></item><item><title>Clustered Mallows Model</title><link>http://arxiv.org/abs/2403.12880v1</link><description>Rankings are a type of preference elicitation that arise in experiments whereassessors arrange items, for example, in decreasing order of utility. Orderingsof n items labelled {1,...,n} denoted are permutations that reflect strictpreferences. For a number of reasons, strict preferences can be unrealisticassumptions for real data. For example, when items share common traits it maybe reasonable to attribute them equal ranks. Also, there can be differentimportance attributions to decisions that form the ranking. In a situationwith, for example, a large number of items, an assessor may wish to rank at topa certain number items; to rank other items at the bottom and to expressindifference to all others. In addition, when aggregating opinions, a judgingbody might be decisive about some parts of the rank but ambiguous for others.In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) toaccommodate item indifference, a phenomenon that can be in place for a varietyof reasons, such as those above mentioned.The underlying grouping of similaritems motivates the proposed Clustered Mallows Model (CMM). The CMM can beinterpreted as a Mallows distribution for tied ranks where ties are learnedfrom the data. The CMM provides the flexibility to combine strict andindifferent relations, achieving a simpler and robust representation of rankcollections in the form of ordered clusters. Bayesian inference for the CMM isin the class of doubly-intractable problems since the model's normalisationconstant is not available in closed form. We overcome this challenge bysampling from the posterior with a version of the exchange algorithm\citep{murray2006}. Real data analysis of food preferences and results ofFormula 1 races are presented, illustrating the CMM in practical situations.</description><author>Luiza S. C. Piancastelli, Nial Friel</author><pubDate>Tue, 19 Mar 2024 17:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12880v1</guid></item><item><title>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</title><link>http://arxiv.org/abs/2402.09283v2</link><description>Large Language Models (LLMs) are now commonplace in conversationapplications. However, their risks of misuse for generating harmful responseshave raised serious societal concerns and spurred recent research on LLMconversation safety. Therefore, in this survey, we provide a comprehensiveoverview of recent studies, covering three critical aspects of LLM conversationsafety: attacks, defenses, and evaluations. Our goal is to provide a structuredsummary that enhances understanding of LLM conversation safety and encouragesfurther investigation into this important subject. For easy reference, we havecategorized all the studies mentioned in this survey according to our taxonomy,available at: https://github.com/niconi19/LLM-conversation-safety.</description><author>Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</author><pubDate>Tue, 19 Mar 2024 17:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09283v2</guid></item><item><title>JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning</title><link>http://arxiv.org/abs/2403.11366v2</link><description>The scaling of Large Language Models (LLMs) for retrieval-based tasks,particularly in Retrieval Augmented Generation (RAG), faces significant memoryconstraints, especially when fine-tuning extensive prompt sequences. Currentopen-source libraries support full-model inference and fine-tuning acrossmultiple GPUs but fall short of accommodating the efficient parameterdistribution required for retrieved context. Addressing this gap, we introducea novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveragingdistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)compilation and tensor-sharding for efficient resource management, therebyenabling accelerated fine-tuning with reduced memory requirements. Thisadvancement significantly improves the scalability and feasibility offine-tuning LLMs for complex RAG applications, even on systems with limited GPUresources. Our experiments show more than 12x improvement in runtime comparedto Hugging Face/DeepSpeed implementation with four GPUs while consuming lessthan half the VRAM per GPU.</description><author>Anique Tahir, Lu Cheng, Huan Liu</author><pubDate>Tue, 19 Mar 2024 17:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11366v2</guid></item><item><title>Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints</title><link>http://arxiv.org/abs/2403.12873v1</link><description>We report a data-parsimonious machine learning model for short-termforecasting of solar irradiance. The model inputs include sky camera imagesthat are reduced to scalar features to meet data transmission constraints. Theoutput irradiance values are transformed to focus on unknown short-termdynamics. Inspired by control theory, a noise input is used to reflectunmeasured variables and is shown to improve model predictions, oftenconsiderably. Five years of data from the NREL Solar Radiation ResearchLaboratory were used to create three rolling train-validate sets and determinethe best representations for time, the optimal span of input measurements, andthe most impactful model input data (features). For the chosen test data, themodel achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline134.35 $W/m^2$ using the persistence of cloudiness model.</description><author>Joshua Edward Hammond, Ricardo A. Lara Orozco, Michael Baldea, Brian A. Korgel</author><pubDate>Tue, 19 Mar 2024 17:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12873v1</guid></item><item><title>Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram</title><link>http://arxiv.org/abs/2402.09450v3</link><description>Electrocardiograms (ECG) are widely employed as a diagnostic tool formonitoring electrical signals originating from a heart. Recent machine learningresearch efforts have focused on the application of screening various diseasesusing ECG signals. However, adapting to the application of screening disease ischallenging in that labeled ECG data are limited. Achieving generalrepresentation through self-supervised learning (SSL) is a well-known approachto overcome the scarcity of labeled data; however, a naive application of SSLto ECG data, without considering the spatial-temporal relationships inherent inECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learnspatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEMoutperforms other SSL baseline methods in various experimental settings forarrhythmia classification tasks. Moreover, we demonstrate that ST-MEM isadaptable to various lead combinations. Through quantitative and qualitativeanalysis, we show a spatio-temporal relationship within ECG data. Our code isavailable at https://github.com/bakqui/ST-MEM.</description><author>Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo</author><pubDate>Tue, 19 Mar 2024 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09450v3</guid></item><item><title>Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy Learning for Robotic Navigation</title><link>http://arxiv.org/abs/2306.06192v4</link><description>Trajectory length stands as a crucial hyperparameter within reinforcementlearning (RL) algorithms, significantly contributing to the sample inefficiencyin robotics applications. Motivated by the pivotal role trajectory length playsin the training process, we introduce Ada-NAV, a novel adaptive trajectorylength scheme designed to enhance the training sample efficiency of RLalgorithms in robotic navigation tasks. Unlike traditional approaches thattreat trajectory length as a fixed hyperparameter, we propose to dynamicallyadjust it based on the entropy of the underlying navigation policy.Interestingly, Ada-NAV can be applied to both existing on-policy and off-policyRL methods, which we demonstrate by empirically validating its efficacy onthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), andSoft Actor-Critic (SAC). We demonstrate through simulated and real-worldrobotic experiments that Ada-NAV outperforms conventional methods that employconstant or randomly sampled trajectory lengths. Specifically, for a fixedsample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a20-38\% reduction in navigation path length, and a 9.32\% decrease in elevationcosts. Furthermore, we showcase the versatility of Ada-NAV by integrating itwith the Clearpath Husky robot, illustrating its applicability in complexoutdoor environments.</description><author>Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha</author><pubDate>Tue, 19 Mar 2024 17:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06192v4</guid></item><item><title>Wildfire danger prediction optimization with transfer learning</title><link>http://arxiv.org/abs/2403.12871v1</link><description>Convolutional Neural Networks (CNNs) have proven instrumental across variouscomputer science domains, enabling advancements in object detection,classification, and anomaly detection. This paper explores the application ofCNNs to analyze geospatial data specifically for identifying wildfire-affectedareas. Leveraging transfer learning techniques, we fine-tuned CNNhyperparameters and integrated the Canadian Fire Weather Index (FWI) to assessmoisture conditions. The study establishes a methodology for computing wildfirerisk levels on a scale of 0 to 5, dynamically linked to weather patterns.Notably, through the integration of transfer learning, the CNN model achievedan impressive accuracy of 95\% in identifying burnt areas. This research shedslight on the inner workings of CNNs and their practical, real-time utility inpredicting and mitigating wildfires. By combining transfer learning and CNNs,this study contributes a robust approach to assess burnt areas, facilitatingtimely interventions and preventative measures against conflagrations.</description><author>Spiros Maggioros, Nikos Tsalkitzis</author><pubDate>Tue, 19 Mar 2024 17:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12871v1</guid></item><item><title>PoNQ: a Neural QEM-based Mesh Representation</title><link>http://arxiv.org/abs/2403.12870v1</link><description>Although polygon meshes have been a standard representation in geometryprocessing, their irregular and combinatorial nature hinders their suitabilityfor learning-based applications. In this work, we introduce a novel learnablemesh representation through a set of local 3D sample Points and theirassociated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,which we denote PoNQ. A global mesh is directly derived from PoNQ byefficiently leveraging the knowledge of the local quadric errors. Besidesmarking the first use of QEM within a neural shape representation, ourcontribution guarantees both topological and geometrical properties by ensuringthat a PoNQ mesh does not self-intersect and is always the boundary of avolume. Notably, our representation does not rely on a regular grid, issupervised directly by the target surface alone, and also handles open surfaceswith boundaries and/or sharp features. We demonstrate the efficacy of PoNQthrough a learning-based mesh prediction from SDF grids and show that ourmethod surpasses recent state-of-the-art techniques in terms of both surfaceand edge-based metrics.</description><author>Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun</author><pubDate>Tue, 19 Mar 2024 17:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12870v1</guid></item><item><title>Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing</title><link>http://arxiv.org/abs/2305.16635v2</link><description>We present Impossible Distillation, a novel framework for paraphrasing andsentence summarization, that distills a high-quality dataset and model from alow-quality teacher that itself cannot perform these tasks. Unlike prior worksthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specificarchitecture, we hypothesize and verify the paraphrastic proximity intrinsic topre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace inthe LM distribution. By identifying and distilling generations from thesesubspaces, Impossible Distillation produces a high-quality dataset and modeleven from GPT2-scale LMs. We evaluate our method on multiple benchmarksspanning unconstrained / syntax-controlled paraphrase generation and sentencesummarization. Our model with 770M parameters consistently outperforms strongbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPTitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higherdiversity and fidelity than up to 13 times larger datasets.</description><author>Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi</author><pubDate>Tue, 19 Mar 2024 17:14:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16635v2</guid></item><item><title>Regularization in Spider-Style Strategy Discovery and Schedule Construction</title><link>http://arxiv.org/abs/2403.12869v1</link><description>To achieve the best performance, automatic theorem provers often rely onschedules of diverse proving strategies to be tried out (either sequentially orin parallel) on a given problem. In this paper, we report on a large-scaleexperiment with discovering strategies for the Vampire prover, targeting theFOF fragment of the TPTP library and constructing a schedule for it, based onthe ideas of Andrei Voronkov's system Spider. We examine the process fromvarious angles, discuss the difficulty (or ease) of obtaining a strong Vampireschedule for the CASC competition, and establish how well a schedule can beexpected to generalize to unseen problems and what factors influence thisproperty.</description><author>Filip BÃ¡rtek, Karel ChvalovskÃ½, Martin Suda</author><pubDate>Tue, 19 Mar 2024 17:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12869v1</guid></item><item><title>MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation</title><link>http://arxiv.org/abs/2402.18451v2</link><description>The recent Mamba model has shown remarkable adaptability for visualrepresentation learning, including in medical imaging tasks. This studyintroduces MambaMIR, a Mamba-based model for medical image reconstruction, aswell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Ourproposed MambaMIR inherits several advantages, such as linear complexity,global receptive fields, and dynamic weights, from the original Mamba model.The innovated arbitrary-mask mechanism effectively adapt Mamba to our imagereconstruction task, providing randomness for subsequent Monte Carlo-baseduncertainty estimation. Experiments conducted on various medical imagereconstruction tasks, including fast MRI and SVCT, which cover anatomicalregions such as the knee, chest, and abdomen, have demonstrated that MambaMIRand MambaMIR-GAN achieve comparable or superior reconstruction results relativeto state-of-the-art methods. Additionally, the estimated uncertainty maps offerfurther insights into the reliability of the reconstruction quality. The codeis publicly available at https://github.com/ayanglab/MambaMIR.</description><author>Jiahao Huang, Liutao Yang, Fanwen Wang, Yinzhe Wu, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane SchÃ¶nlieb, Daoqiang Zhang, Guang Yang</author><pubDate>Tue, 19 Mar 2024 17:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18451v2</guid></item><item><title>Hypergraph-MLP: Learning on Hypergraphs without Message Passing</title><link>http://arxiv.org/abs/2312.09778v2</link><description>Hypergraphs are vital in modelling data with higher-order relationscontaining more than two entities, gaining prominence in machine learning andsignal processing. Many hypergraph neural networks leverage message passingover hypergraph structures to enhance node representation learning, yieldingimpressive performances in tasks like hypergraph node classification. However,these message-passing-based models face several challenges, includingoversmoothing as well as high latency and sensitivity to structuralperturbations at inference time. To tackle those challenges, we propose analternative approach where we integrate the information about hypergraphstructures into training supervision without explicit message passing, thusalso removing the reliance on it at inference. Specifically, we introduceHypergraph-MLP, a novel learning framework for hypergraph-structured data,where the learning model is a straightforward multilayer perceptron (MLP)supervised by a loss function based on a notion of signal smoothness onhypergraphs. Experiments on hypergraph node classification tasks demonstratethat Hypergraph-MLP achieves competitive performance compared to existingbaselines, and is considerably faster and more robust against structuralperturbations at inference.</description><author>Bohan Tang, Siheng Chen, Xiaowen Dong</author><pubDate>Tue, 19 Mar 2024 17:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09778v2</guid></item><item><title>Low-power, Continuous Remote Behavioral Localization with Event Cameras</title><link>http://arxiv.org/abs/2312.03799v2</link><description>Researchers in natural science need reliable methods for quantifying animalbehavior. Recently, numerous computer vision methods emerged to automate theprocess. However, observing wild species at remote locations remains achallenging task due to difficult lighting conditions and constraints on powersupply and data storage. Event cameras offer unique advantages forbattery-dependent remote monitoring due to their low power consumption and highdynamic range capabilities. We use this novel sensor to quantify a behavior inChinstrap penguins called ecstatic display. We formulate the problem as atemporal action detection task, determining the start and end times of thebehavior. For this purpose, we recorded a colony of breeding penguins inAntarctica for several weeks and labeled event data on 16 nests. The developedmethod consists of a generator of candidate time intervals (proposals) and aclassifier of the actions within them. The experiments show that the eventcameras' natural response to motion is effective for continuous behaviormonitoring and detection, reaching a mean average precision (mAP) of 58% (whichincreases to 63% in good weather conditions). The results also demonstrate therobustness against various lighting conditions contained in the challengingdataset. The low-power capabilities of the event camera allow it to recordsignificantly longer than with a conventional camera. This work pioneers theuse of event cameras for remote wildlife observation, opening newinterdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/</description><author>Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego</author><pubDate>Tue, 19 Mar 2024 17:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03799v2</guid></item><item><title>LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with Conditional Generative Adversarial Neural Network</title><link>http://arxiv.org/abs/2301.13853v2</link><description>Education is a right of all, however, every individual is different thanothers. Teachers in post-communism era discover inherent individualism toequally train all towards job market of fourth industrial revolution. We canconsider scenario of ethnic minority education in academic practices. Ethnicminority group has grown in their own culture and would prefer to be taught intheir native way. We have formulated such linguistic anthropology(how peoplelearn)based engagement as semi-supervised problem. Then, we have developed anconditional deep generative adversarial network algorithm namely LA-GAN toclassify linguistic ethnographic features in student engagement. Theoreticaljustification proves the objective, regularization and loss function of oursemi-supervised adversarial model. Survey questions are prepared to reach someform of assumptions about z-generation and ethnic minority group, whoselearning style, learning approach and preference are our main area of interest.</description><author>Rossi Kamal, Zuzana Kubincova</author><pubDate>Tue, 19 Mar 2024 17:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13853v2</guid></item><item><title>A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection</title><link>http://arxiv.org/abs/2403.12864v1</link><description>Spacecraft operations are highly critical, demanding impeccable reliabilityand safety. Ensuring the optimal performance of a spacecraft requires the earlydetection and mitigation of anomalies, which could otherwise result in unit ormission failures. With the advent of deep learning, a surge of interest hasbeen seen in leveraging these sophisticated algorithms for anomaly detection inspace operations. This study aims to compare the efficacy of various deeplearning architectures in detecting anomalies in spacecraft data. The deeplearning models under investigation include Convolutional Neural Networks(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)networks, and Transformer-based architectures. Each of these models was trainedand validated using a comprehensive dataset sourced from multiple spacecraftmissions, encompassing diverse operational scenarios and anomaly types. Initialresults indicate that while CNNs excel in identifying spatial patterns and maybe effective for some classes of spacecraft data, LSTMs and RNNs show a markedproficiency in capturing temporal anomalies seen in time-series spacecrafttelemetry. The Transformer-based architectures, given their ability to focus onboth local and global contexts, have showcased promising results, especially inscenarios where anomalies are subtle and span over longer durations.Additionally, considerations such as computational efficiency, ease ofdeployment, and real-time processing capabilities were evaluated. While CNNsand LSTMs demonstrated a balance between accuracy and computational demands,Transformer architectures, though highly accurate, require significantcomputational resources. In conclusion, the choice of deep learningarchitecture for spacecraft anomaly detection is highly contingent on thenature of the data, the type of anomalies, and operational constraints.</description><author>Daniel Lakey, Tim Schlippe</author><pubDate>Tue, 19 Mar 2024 17:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12864v1</guid></item><item><title>Behave-XAI: Deep Explainable Learning of Behavioral Representational Data</title><link>http://arxiv.org/abs/2301.00016v2</link><description>According to the latest trend of artificial intelligence, AI-systems needs toclarify regarding general,specific decisions,services provided by it. Onlyconsumer is satisfied, with explanation , for example, why any classificationresult is the outcome of any given time. This actually motivates us usingexplainable or human understandable AI for a behavioral mining scenario, whereusers engagement on digital platform is determined from context, such asemotion, activity, weather, etc. However, the output of AI-system is not alwayssystematically correct, and often systematically correct, but apparentlynot-perfect and thereby creating confusions, such as, why the decision isgiven? What is the reason underneath? In this context, we first formulate thebehavioral mining problem in deep convolutional neural network architecture.Eventually, we apply a recursive neural network due to the presence oftime-series data from users physiological and environmental sensor-readings.Once the model is developed, explanations are presented with the advent of XAImodels in front of users. This critical step involves extensive trial withusers preference on explanations over conventional AI, judgement of credibilityof explanation.</description><author>Rossi Kamal, Zuzana Kubincova</author><pubDate>Tue, 19 Mar 2024 17:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00016v2</guid></item><item><title>Deep Recurrent Learning Through Long Short Term Memory and TOPSIS</title><link>http://arxiv.org/abs/2301.00693v2</link><description>Enterprise resource planning (ERP) software brings resources, data togetherto keep software-flow within business processes in a company. However, cloudcomputing's cheap, easy and quick management promise pushes business-owners fora transition from monolithic to a data-center/cloud based ERP. Since cloud-ERPdevelopment involves a cyclic process, namely planning, implementing, testingand upgrading, its adoption is realized as a deep recurrent neural networkproblem. Eventually, a classification algorithm based on long short term memory(LSTM) and TOPSIS is proposed to identify and rank, respectively, adoptionfeatures. Our theoretical model is validated over a reference model byarticulating key players, services, architecture, functionalities. Qualitativesurvey is conducted among users by considering technology, innovation andresistance issues, to formulate hypotheses on key adoption factors.</description><author>Rossi Kamal, Zuzana Kubincova, Mosaddek Hossain Kamal, Upama Kabir</author><pubDate>Tue, 19 Mar 2024 17:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00693v2</guid></item><item><title>Epistemology of Language Models: Do Language Models Have Holistic Knowledge?</title><link>http://arxiv.org/abs/2403.12862v1</link><description>This paper investigates the inherent knowledge in language models from theperspective of epistemological holism. The purpose of this paper is to explorewhether LLMs exhibit characteristics consistent with epistemological holism.These characteristics suggest that core knowledge, such as general scientificknowledge, each plays a specific role, serving as the foundation of ourknowledge system and being difficult to revise. To assess these traits relatedto holism, we created a scientific reasoning dataset and examined theepistemology of language models through three tasks: Abduction, Revision, andArgument Generation. In the abduction task, the language models explainedsituations while avoiding revising the core knowledge. However, in other tasks,the language models were revealed not to distinguish between core andperipheral knowledge, showing an incomplete alignment with holistic knowledgeprinciples.</description><author>Minsu Kim, James Thorne</author><pubDate>Tue, 19 Mar 2024 17:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12862v1</guid></item><item><title>D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation</title><link>http://arxiv.org/abs/2403.12861v1</link><description>Mastering dexterous robotic manipulation of deformable objects is vital forovercoming the limitations of parallel grippers in real-world applications.Current trajectory optimisation approaches often struggle to solve such tasksdue to the large search space and the limited task information available from acost function. In this work, we propose D-Cubed, a novel trajectoryoptimisation method using a latent diffusion model (LDM) trained from atask-agnostic play dataset to solve dexterous deformable object manipulationtasks. D-Cubed learns a skill-latent space that encodes short-horizon actionsin the play dataset using a VAE and trains a LDM to compose the skill latentsinto a skill trajectory, representing a long-horizon action trajectory in thedataset. To optimise a trajectory for a target task, we introduce a novelgradient-free guided sampling method that employs the Cross-Entropy methodwithin the reverse diffusion process. In particular, D-Cubed samples a smallnumber of noisy skill trajectories using the LDM for exploration and evaluatesthe trajectories in simulation. Then, D-Cubed selects the trajectory with thelowest cost for the subsequent reverse process. This effectively explorespromising solution areas and optimises the sampled trajectories towards atarget task throughout the reverse diffusion process. Through empiricalevaluation on a public benchmark of dexterous deformable object manipulationtasks, we demonstrate that D-Cubed outperforms traditional trajectoryoptimisation and competitive baseline approaches by a significant margin. Wefurther demonstrate that trajectories found by D-Cubed readily transfer to areal-world LEAP hand on a folding task.</description><author>Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner</author><pubDate>Tue, 19 Mar 2024 17:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12861v1</guid></item><item><title>GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning</title><link>http://arxiv.org/abs/2203.07738v4</link><description>Few-shot learning (FSL), purposing to resolve the problem of data-scarce, hasattracted considerable attention in recent years. A popular FSL frameworkcontains two phases: (i) the pre-train phase employs the base data to train aCNN-based feature extractor. (ii) the meta-test phase applies the frozenfeature extractor to novel data (novel data has different categories from basedata) and designs a classifier for recognition. To correct few-shot datadistribution, researchers propose Semi-Supervised Few-Shot Learning (SSFSL) byintroducing unlabeled data. Although SSFSL has been proved to achieveoutstanding performances in the FSL community, there still exists a fundamentalproblem: the pre-trained feature extractor can not adapt to the novel dataflawlessly due to the cross-category setting. Usually, large amounts of noisesare introduced to the novel feature. We dub it as Feature-Extractor-Maladaptive(FEM) problem. To tackle FEM, we make two efforts in this paper. First, wepropose a novel label prediction method, Isolated Graph Learning (IGL). IGLintroduces the Laplacian operator to encode the raw data to graph space, whichhelps reduce the dependence on features when classifying, and then projectgraph representation to label space for prediction. The key point is that: IGLcan weaken the negative influence of noise from the feature representationperspective, and is also flexible to independently complete training andtesting procedures, which is suitable for SSFSL. Second, we propose GraphCo-Training (GCT) to tackle this challenge from a multi-modal fusionperspective by extending the proposed IGL to the co-training framework. GCT isa semi-supervised method that exploits the unlabeled samples with two modalfeatures to crossly strengthen the IGL classifier.</description><author>Rui Xu, Lei Xing, Shuai Shao, Lifei Zhao, Baodi Liu, Weifeng Liu, Yicong Zhou</author><pubDate>Tue, 19 Mar 2024 17:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07738v4</guid></item><item><title>Primal Methods for Variational Inequality Problems with Functional Constraints</title><link>http://arxiv.org/abs/2403.12859v1</link><description>Constrained variational inequality problems are recognized for their broadapplications across various fields including machine learning and operationsresearch. First-order methods have emerged as the standard approach for solvingthese problems due to their simplicity and scalability. However, they typicallyrely on projection or linear minimization oracles to navigate the feasible set,which becomes computationally expensive in practical scenarios featuringmultiple functional constraints. Existing efforts to tackle such functionalconstrained variational inequality problems have centered on primal-dualalgorithms grounded in the Lagrangian function. These algorithms along withtheir theoretical analysis often require the existence and prior knowledge ofthe optimal Lagrange multipliers. In this work, we propose a simple primalmethod, termed Constrained Gradient Method (CGM), for addressing functionalconstrained variational inequality problems, without necessitating anyinformation on the optimal Lagrange multipliers. We establish a non-asymptoticconvergence analysis of the algorithm for variational inequality problems withmonotone operators under smooth constraints. Remarkably, our algorithms matchthe complexity of projection-based methods in terms of operator queries forboth monotone and strongly monotone settings, while utilizing significantlycheaper oracles based on quadratic programming. Furthermore, we provide severalnumerical examples to evaluate the efficacy of our algorithms.</description><author>Liang Zhang, Niao He, Michael Muehlebach</author><pubDate>Tue, 19 Mar 2024 17:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12859v1</guid></item><item><title>Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning</title><link>http://arxiv.org/abs/2403.12856v1</link><description>In reinforcement learning (RL), exploiting environmental symmetries cansignificantly enhance efficiency, robustness, and performance. However,ensuring that the deep RL policy and value networks are respectivelyequivariant and invariant to exploit these symmetries is a substantialchallenge. Related works try to design networks that are equivariant andinvariant by construction, limiting them to a very restricted library ofcomponents, which in turn hampers the expressiveness of the networks. Thispaper proposes a method to construct equivariant policies and invariant valuefunctions without specialized neural network components, which we termequivariant ensembles. We further add a regularization term for addinginductive bias during training. In a map-based path planning case study, weshow how equivariant ensembles and regularization benefit sample efficiency andperformance.</description><author>Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</author><pubDate>Tue, 19 Mar 2024 17:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12856v1</guid></item><item><title>RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems</title><link>http://arxiv.org/abs/2403.12853v1</link><description>Realizing consumer-grade drones that are as useful as robot vacuumsthroughout our homes or personal smartphones in our daily lives requires dronesto sense, actuate, and respond to general scenarios that may arise. Towardsthis vision, we propose RASP, a modular and reconfigurable sensing andactuation platform that allows drones to autonomously swap onboard sensors andactuators in only 25 seconds, allowing a single drone to quickly adapt to adiverse range of tasks. RASP consists of a mechanical layer to physically swapsensor modules, an electrical layer to maintain power and communication linesto the sensor/actuator, and a software layer to maintain a common interfacebetween the drone and any sensor module in our platform. Leveraging recentadvances in large language and visual language models, we further introduce thearchitecture, implementation, and real-world deployments of a personalassistant system utilizing RASP. We demonstrate that RASP can enable a diverserange of useful tasks in home, office, lab, and other indoor settings.</description><author>Minghui Zhao, Junxi Xia, Kaiyuan Hou, Yanchen Liu, Stephen Xia, Xiaofan Jiang</author><pubDate>Tue, 19 Mar 2024 16:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12853v1</guid></item><item><title>Generative Enhancement for 3D Medical Images</title><link>http://arxiv.org/abs/2403.12852v1</link><description>The limited availability of 3D medical image datasets, due to privacyconcerns and high collection or annotation costs, poses significant challengesin the field of medical imaging. While a promising alternative is the use ofsynthesized medical data, there are few solutions for realistic 3D medicalimage synthesis due to difficulties in backbone design and fewer 3D trainingsamples compared to 2D counterparts. In this paper, we propose GEM-3D, a novelgenerative approach to the synthesis of 3D medical images and the enhancementof existing datasets using conditional diffusion models. Our method begins witha 2D slice, noted as the informed slice to serve the patient prior, andpropagates the generation process using a 3D segmentation mask. By decomposingthe 3D medical images into masks and patient prior information, GEM-3D offers aflexible yet effective solution for generating versatile 3D images fromexisting datasets. GEM-3D can enable dataset enhancement by combining informedslice selection and generation at random positions, along with editable maskvolumes to introduce large variations in diffusion sampling. Moreover, as theinformed slice contains patient-wise information, GEM-3D can also facilitatecounterfactual image synthesis and dataset-level de-enhancement with desiredcontrol. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3Dis capable of synthesizing high-quality 3D medical images with volumetricconsistency, offering a straightforward solution for dataset enhancement duringinference. The code is available at https://github.com/HKU-MedAI/GEM-3D.</description><author>Lingting Zhu, Noel Codella, Dongdong Chen, Zhenchao Jin, Lu Yuan, Lequan Yu</author><pubDate>Tue, 19 Mar 2024 16:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12852v1</guid></item><item><title>Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation</title><link>http://arxiv.org/abs/2403.12848v1</link><description>Compositional 3D scene synthesis has diverse applications across a spectrumof industries such as robotics, films, and video games, as it closely mirrorsthe complexity of real-world multi-object environments. Early works typicallyemploy shape retrieval based frameworks which naturally suffer from limitedshape diversity. Recent progresses have been made in shape generation withpowerful generative models, such as diffusion models, which increases the shapefidelity. However, these approaches separately treat 3D shape generation andlayout generation. The synthesized scenes are usually hampered by layoutcollision, which implies that the scene-level fidelity is still under-explored.In this paper, we aim at generating realistic and reasonable 3D scenes fromscene graph. To enrich the representation capability of the given scene graphinputs, large language model is utilized to explicitly aggregate the globalgraph features with local relationship features. With a unified graphconvolution network (GCN), graph features are extracted from scene graphsupdated via joint layout-shape distribution. During scene generation, anIoU-based regularization loss is introduced to constrain the predicted 3Dlayouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3Dscene synthesis, especially in terms of scene-level fidelity. The source codewill be released after publication.</description><author>Yao Wei, Martin Renqiang Min, George Vosselman, Li Erran Li, Michael Ying Yang</author><pubDate>Tue, 19 Mar 2024 16:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12848v1</guid></item><item><title>Policy Bifurcation in Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2403.12847v1</link><description>Safe reinforcement learning (RL) offers advanced solutions to constrainedoptimal control problems. Existing studies in safe RL implicitly assumecontinuity in policy functions, where policies map states to actions in asmooth, uninterrupted manner; however, our research finds that in somescenarios, the feasible policy should be discontinuous or multi-valued,interpolating between discontinuous local optima can inevitably lead toconstraint violations. We are the first to identify the generating mechanism ofsuch a phenomenon, and employ topological analysis to rigorously prove theexistence of policy bifurcation in safe RL, which corresponds to thecontractibility of the reachable tuple. Our theorem reveals that in scenarioswhere the obstacle-free state space is non-simply connected, a feasible policyis required to be bifurcated, meaning its output action needs to changeabruptly in response to the varying state. To train such a bifurcated policy,we propose a safe RL algorithm called multimodal policy optimization (MUPO),which utilizes a Gaussian mixture distribution as the policy output. Thebifurcated behavior can be achieved by selecting the Gaussian component withthe highest mixing coefficient. Besides, MUPO also integrates spectralnormalization and forward KL divergence to enhance the policy's capability ofexploring different modes. Experiments with vehicle control tasks show that ouralgorithm successfully learns the bifurcated policy and ensures satisfyingsafety, while a continuous policy suffers from inevitable constraintviolations.</description><author>Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li</author><pubDate>Tue, 19 Mar 2024 16:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12847v1</guid></item><item><title>MELTing point: Mobile Evaluation of Language Transformers</title><link>http://arxiv.org/abs/2403.12844v1</link><description>Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence''. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs). To achievethis, we have created our own automation infrastructure, MELT, which supportsthe headless execution and benchmarking of LLMs on device, supporting differentmodels, devices and frameworks, including Android, iOS and Nvidia Jetsondevices. We evaluate popular instruction fine-tuned LLMs and leverage differentframeworks to measure their end-to-end and granular performance, tracing theirmemory and energy requirements along the way. Our analysis is the first systematic study of on-device LLM execution,quantifying performance, energy efficiency and accuracy across variousstate-of-the-art models and showcases the state of on-device intelligence inthe era of hyperscale models. Results highlight the performance heterogeneityacross targets and corroborates that LLM inference is largely memory-bound.Quantization drastically reduces memory requirements and renders executionviable, but at a non-negligible accuracy cost. Drawing from its energyfootprint and thermal behavior, the continuous execution of LLMs remainselusive, as both factors negatively affect user experience. Last, ourexperience shows that the ecosystem is still in its infancy, and algorithmic aswell as hardware breakthroughs can significantly shift the execution cost. Weexpect NPU acceleration, and framework-hardware co-design to be the biggest bettowards efficient standalone execution, with the alternative of offloadingtailored towards edge deployments.</description><author>Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, Hamed Haddadi</author><pubDate>Tue, 19 Mar 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12844v1</guid></item><item><title>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model</title><link>http://arxiv.org/abs/2401.02317v3</link><description>In this paper, we address the challenge of image resolution variation for theSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,exhibits a performance degradation when faced with datasets with varying imagesizes. Previous approaches tend to resize the image to a fixed size or adoptstructure modifications, hindering the preservation of SAM's rich priorknowledge. Besides, such task-specific tuning necessitates a completeretraining of the model, which is cost-expensive and unacceptable fordeployment in the downstream tasks. In this paper, we reformulate this issue asa length extrapolation problem, where token sequence length varies whilemaintaining a consistent patch size for images of different sizes. To this end,we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM'sadaptability to varying image resolutions while eliminating the need forstructure modifications. Firstly, we introduce a new scaling factor to ensureconsistent magnitude in the attention layer's dot product values when the tokensequence length changes. Secondly, we present a bias-mode attention mask thatallows each token to prioritize neighboring information, mitigating the impactof untrained distant information. Our BA-SAM demonstrates efficacy in twoscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability tosignificantly mitigate performance degradation in the zero-shot setting andachieve state-of-the-art performance with minimal fine-tuning. Furthermore, wepropose a generalized model and benchmark, showcasing BA-SAM's generalizabilityacross all four datasets simultaneously.</description><author>Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma</author><pubDate>Tue, 19 Mar 2024 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02317v3</guid></item><item><title>Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach</title><link>http://arxiv.org/abs/2210.12624v2</link><description>Machine learning problems with multiple objective functions appear either inlearning with multiple criteria where learning has to make a trade-off betweenmultiple performance metrics such as fairness, safety and accuracy; or, inmulti-task learning where multiple tasks are optimized jointly, sharinginductive bias between them. This problems are often tackled by themulti-objective optimization framework. However, existing stochasticmulti-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad,etc.) all adopt a biased noisy gradient direction, which leads to degradedempirical performance. To this end, we develop a stochastic Multi-objectivegradient Correction (MoCo) method for multi-objective optimization. The uniquefeature of our method is that it can guarantee convergence without increasingthe batch size even in the non-convex setting. Simulations on multi-tasksupervised and reinforcement learning demonstrate the effectiveness of ourmethod relative to state-of-the-art methods.</description><author>Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, Tianyi Chen</author><pubDate>Tue, 19 Mar 2024 16:47:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12624v2</guid></item><item><title>Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering</title><link>http://arxiv.org/abs/2403.12839v1</link><description>Neural radiance fields~(NeRF) have recently been applied to renderlarge-scale scenes. However, their limited model capacity typically results inblurred rendering results. Existing large-scale NeRFs primarily address thislimitation by partitioning the scene into blocks, which are subsequentlyhandled by separate sub-NeRFs. These sub-NeRFs, trained from scratch andprocessed independently, lead to inconsistencies in geometry and appearanceacross the scene. Consequently, the rendering quality fails to exhibitsignificant improvement despite the expansion of model capacity. In this work,we present global-guided focal neural radiance field (GF-NeRF) that achieveshigh-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes atwo-stage (Global and Focal) architecture and a global-guided trainingstrategy. The global stage obtains a continuous representation of the entirescene while the focal stage decomposes the scene into multiple blocks andfurther processes them with distinct sub-encoders. Leveraging this two-stagearchitecture, sub-encoders only need fine-tuning based on the global encoder,thus reducing training complexity in the focal stage while maintainingscene-wide consistency. Spatial information and error information from theglobal stage also benefit the sub-encoders to focus on crucial areas andeffectively capture more details of large-scale scenes. Notably, our approachdoes not rely on any prior knowledge about the target scene, attributingGF-NeRF adaptable to various large-scale scene types, including street-view andaerial-view scenes. We demonstrate that our method achieves high-fidelity,natural rendering results on various types of large-scale datasets. Our projectpage: https://shaomq2187.github.io/GF-NeRF/</description><author>Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</author><pubDate>Tue, 19 Mar 2024 16:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12839v1</guid></item></channel></rss>