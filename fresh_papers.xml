<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Mar 2024 14:00:38 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation</title><link>http://arxiv.org/abs/2403.11722v1</link><description>We propose a novel quaternionic time-series compression methodology where wedivide a long time-series into segments of data, extract the min, max, mean andstandard deviation of these chunks as representative features and encapsulatethem in a quaternion, yielding a quaternion valued time-series. Thistime-series is processed using quaternion valued neural network layers, wherewe aim to preserve the relation between these features through the usage of theHamilton product. To train this quaternion neural network, we derive quaternionbackpropagation employing the GHR calculus, which is required for a validproduct and chain rule in quaternion space. Furthermore, we investigate theconnection between the derived update rules and automatic differentiation. Weapply our proposed compression method on the Tennessee Eastman Dataset, wherewe perform fault classification using the compressed data in two settings: afully supervised one and in a semi supervised, contrastive learning setting.Both times, we were able to outperform real valued counterparts as well as twobaseline models: one with the uncompressed time-series as the input and theother with a regular downsampling using the mean. Further, we could improve theclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.</description><author>Johannes Pöppelbaum, Andreas Schwung</author><pubDate>Mon, 18 Mar 2024 13:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11722v1</guid></item><item><title>A Temporal Bias Correction using a Machine Learning Attention model</title><link>http://arxiv.org/abs/2402.14169v3</link><description>Climate models are biased with respect to real world observations and usuallyneed to be calibrated prior to impact studies. The suite of statistical methodsthat enable such calibrations is called bias correction (BC). However, currentBC methods struggle to adjust for temporal biases, because they disregard thedependence between consecutive time-points. As a result, climate statisticswith long-range temporal properties, such as heatwave duration and frequency,cannot be corrected accurately, making it more difficult to produce reliableimpact studies on such climate statistics. In this paper, we offer a novel BCmethodology to correct for temporal biases. This is made possible by i)re-thinking BC as a probability model rather than an algorithmic procedure, andii) adapting state-of-the-art machine-learning (ML) probabilistic attentionmodels to fit the BC task. With a case study of heatwave duration statistics inAbuja, Nigeria, and Tokyo, Japan, we show striking results compared to currentclimate model outputs and alternative BC methods.</description><author>Omer Nivron, Damon J. Wischik, Mathieu Vrac, Emily Shuckburgh</author><pubDate>Mon, 18 Mar 2024 13:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14169v3</guid></item><item><title>Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification</title><link>http://arxiv.org/abs/2403.11708v1</link><description>Visible-Infrared Person Re-identification (VI-ReID) is a challengingcross-modal pedestrian retrieval task, due to significant intra-classvariations and cross-modal discrepancies among different cameras. Existingworks mainly focus on embedding images of different modalities into a unifiedspace to mine modality-shared features. They only seek distinctive informationwithin these shared features, while ignoring the identity-aware usefulinformation that is implicit in the modality-specific features. To address thisissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)network to uncover and leverage the implicit discriminative informationcontained within the modality-specific. First, we extract modality-specific andmodality-shared features using a novel dual-stream network. Then, themodality-specific features undergo purification to reduce their modality stylediscrepancies while preserving identity-aware discriminative knowledge.Subsequently, this kind of implicit knowledge is distilled into themodality-shared feature to enhance its distinctiveness. Finally, an alignmentloss is proposed to minimize modality discrepancy on enhanced modality-sharedfeatures. Extensive experiments on multiple public datasets demonstrate thesuperiority of IDKL network over the state-of-the-art methods. Code isavailable at https://github.com/1KK077/IDKL.</description><author>Kaijie Ren, Lei Zhang</author><pubDate>Mon, 18 Mar 2024 13:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11708v1</guid></item><item><title>Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models</title><link>http://arxiv.org/abs/2403.11706v1</link><description>Multi-Source Diffusion Models (MSDM) allow for compositional musicalgeneration tasks: generating a set of coherent sources, creatingaccompaniments, and performing source separation. Despite their versatility,they require estimating the joint distribution over the sources, necessitatingpre-separated musical data, which is rarely available, and fixing the numberand type of sources at training time. This paper generalizes MSDM to arbitrarytime-domain diffusion models conditioned on text embeddings. These models donot require separated data as they are trained on mixtures, can parameterize anarbitrary number of sources, and allow for rich semantic control. We propose aninference procedure enabling the coherent generation of sources andaccompaniments. Additionally, we adapt the Dirac separator of MSDM to performsource separation. We experiment with diffusion models trained on Slakh2100 andMTG-Jamendo, showcasing competitive generation and separation results in arelaxed data setting.</description><author>Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà</author><pubDate>Mon, 18 Mar 2024 13:08:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11706v1</guid></item><item><title>Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning</title><link>http://arxiv.org/abs/2403.09401v2</link><description>Identifying highlight moments of raw video materials is crucial for improvingthe efficiency of editing videos that are pervasive on internet platforms.However, the extensive work of manually labeling footage has created obstaclesto applying supervised methods to videos of unseen categories. The absence ofan audio modality that contains valuable cues for highlight detection in manyvideos also makes it difficult to use multimodal strategies. In this paper, wepropose a novel model with cross-modal perception for unsupervised highlightdetection. The proposed model learns representations with visual-audio levelsemantics from image-audio pair data via a self-reconstruction task. To achieveunsupervised highlight detection, we investigate the latent representations ofthe network and propose the representation activation sequence learning (RASL)module with k-point contrastive learning to learn significant representationactivations. To connect the visual modality with the audio modality, we use thesymmetric contrastive learning (SCL) module to learn the paired visual andaudio representations. Furthermore, an auxiliary task of masked feature vectorsequence (FVS) reconstruction is simultaneously conducted during pretrainingfor representation enhancement. During inference, the cross-modal pretrainedmodel can generate representations with paired visual-audio semantics givenonly the visual modality. The RASL module is used to output the highlightscores. The experimental results show that the proposed framework achievessuperior performance compared to other state-of-the-art approaches.</description><author>Tingtian Li, Zixun Sun, Xinyu Xiao</author><pubDate>Mon, 18 Mar 2024 13:08:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09401v2</guid></item><item><title>Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach</title><link>http://arxiv.org/abs/2403.11705v1</link><description>Frustrated itinerant magnets often exhibit complex noncollinear ornoncoplanar magnetic orders which support topological electronic structures. Acanonical example is the anomalous quantum Hall state with a chiral spin orderstabilized by electron-spin interactions on a triangular lattice. While along-range magnetic order cannot survive thermal fluctuations in twodimensions, the chiral order which results from the breaking of a discreteIsing symmetry persists even at finite temperatures. We present a scalablemachine learning (ML) framework to model the complex electron-mediatedspin-spin interactions that stabilize the chiral magnetic domains in atriangular lattice. Large-scale dynamical simulations, enabled by the MLforce-field models, are performed to investigate the coarsening of chiraldomains after a thermal quench. While the chiral phase is described by a broken$Z_2$ Ising-type symmetry, we find that the characteristic size of chiraldomains increases linearly with time, in stark contrast to the expectedAllen-Cahn domain growth law for a non-conserved Ising order parameter field.The linear growth of the chiral domains is attributed to the orientationalanisotropy of domain boundaries. Our work also demonstrates the promisingpotential of ML models for large-scale spin dynamics of itinerant magnets.</description><author>Yunhao Fan, Sheng Zhang, Gia-Wei Chern</author><pubDate>Mon, 18 Mar 2024 13:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11705v1</guid></item><item><title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title><link>http://arxiv.org/abs/2403.11703v1</link><description>Visual encoding constitutes the basis of large multimodal models (LMMs) inunderstanding the visual world. Conventional LMMs process images in fixed sizesand limited resolutions, while recent explorations in this direction arelimited in adaptivity, efficiency, and even correctness. In this work, we firsttake GPT-4V and LLaVA-1.5 as representative examples and expose systematicflaws rooted in their visual encoding strategy. To address the challenges, wepresent LLaVA-UHD, a large multimodal model that can efficiently perceiveimages in any aspect ratio and high resolution. LLaVA-UHD includes three keycomponents: (1) An image modularization strategy that divides native-resolutionimages into smaller variable-sized slices for efficient and extensibleencoding, (2) a compression module that further condenses image tokens fromvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.Comprehensive experiments show that LLaVA-UHD outperforms established LMMstrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, ourmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)resolution images using only 94% inference computation, and achieves 6.4accuracy improvement on TextVQA. Moreover, the model can be efficiently trainedin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours ofLLaVA-1.5). We make the data and code publicly available athttps://github.com/thunlp/LLaVA-UHD.</description><author>Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, Gao Huang</author><pubDate>Mon, 18 Mar 2024 13:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11703v1</guid></item><item><title>A Spatial-Temporal Progressive Fusion Network for Breast Lesion Segmentation in Ultrasound Videos</title><link>http://arxiv.org/abs/2403.11699v1</link><description>Ultrasound video-based breast lesion segmentation provides a valuableassistance in early breast lesion detection and treatment. However, existingworks mainly focus on lesion segmentation based on ultrasound breast imageswhich usually can not be adapted well to obtain desirable results on ultrasoundvideos. The main challenge for ultrasound video-based breast lesionsegmentation is how to exploit the lesion cues of both intra-frame andinter-frame simultaneously. To address this problem, we propose a novelSpatial-Temporal Progressive Fusion Network (STPFNet) for video based breastlesion segmentation problem. The main aspects of the proposed STPFNet arethreefold. First, we propose to adopt a unified network architecture to captureboth spatial dependences within each ultrasound frame and temporal correlationsbetween different frames together for ultrasound data representation. Second,we propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), tofuse spatial and temporal cues together for lesion detection. MSFF can help todetermine the boundary contour of lesion region to overcome the issue of lesionboundary blurring. Third, we propose to exploit the segmentation result ofprevious frame as the prior knowledge to suppress the noisy background andlearn more robust representation. In particular, we introduce a new publiclyavailable ultrasound video breast lesion segmentation dataset, termed UVBLS200,which is specifically dedicated to breast lesion segmentation. It contains 200videos, including 80 videos of benign lesions and 120 videos of malignantlesions. Experiments on the proposed dataset demonstrate that the proposedSTPFNet achieves better breast lesion detection performance thanstate-of-the-art methods.</description><author>Zhengzheng Tu, Zigang Zhu, Yayang Duan, Bo Jiang, Qishun Wang, Chaoxue Zhang</author><pubDate>Mon, 18 Mar 2024 12:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11699v1</guid></item><item><title>Urban Scene Diffusion through Semantic Occupancy Map</title><link>http://arxiv.org/abs/2403.11697v1</link><description>Generating unbounded 3D scenes is crucial for large-scale scene understandingand simulation. Urban scenes, unlike natural landscapes, consist of variouscomplex man-made objects and structures such as roads, traffic signs, vehicles,and buildings. To create a realistic and detailed urban scene, it is crucial toaccurately represent the geometry and semantics of the underlying objects,going beyond their visual appearance. In this work, we propose UrbanDiffusion,a 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map andgenerates an urban scene with geometry and semantics in the form of semanticoccupancy map. Our model introduces a novel paradigm that learns the datadistribution of scene-level structures within a latent space and furtherenables the expansion of the synthesized scene into an arbitrary scale. Aftertraining on real-world driving datasets, our model can generate a wide range ofdiverse urban scenes given the BEV maps from the held-out set and alsogeneralize to the synthesized maps from a driving simulator. We furtherdemonstrate its application to scene image synthesis with a pretrained imagegenerator as a prior.</description><author>Junge Zhang, Qihang Zhang, Li Zhang, Ramana Rao Kompella, Gaowen Liu, Bolei Zhou</author><pubDate>Mon, 18 Mar 2024 12:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11697v1</guid></item><item><title>Generalization error of spectral algorithms</title><link>http://arxiv.org/abs/2403.11696v1</link><description>The asymptotically precise estimation of the generalization of kernel methodshas recently received attention due to the parallels between neural networksand their associated kernels. However, prior works derive such estimates fortraining by kernel ridge regression (KRR), whereas neural networks aretypically trained with gradient descent (GD). In the present work, we considerthe training of kernels with a family of $\textit{spectral algorithms}$specified by profile $h(\lambda)$, and including KRR and GD as special cases.Then, we derive the generalization error as a functional of learning profile$h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensionaltranslation-invariant model. Under power-law assumptions on the spectrum of thekernel and target, we use our framework to (i) give full loss asymptotics forboth noisy and noiseless observations (ii) show that the loss localizes oncertain spectral scales, giving a new perspective on the KRR saturationphenomenon (iii) conjecture, and demonstrate for the considered data models,the universality of the loss w.r.t. non-spectral details of the problem, butonly in case of noisy observation.</description><author>Maksim Velikanov, Maxim Panov, Dmitry Yarotsky</author><pubDate>Mon, 18 Mar 2024 12:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11696v1</guid></item><item><title>Truly No-Regret Learning in Constrained MDPs</title><link>http://arxiv.org/abs/2402.15776v2</link><description>Constrained Markov decision processes (CMDPs) are a common way to modelsafety constraints in reinforcement learning. State-of-the-art methods forefficiently solving CMDPs are based on primal-dual algorithms. For thesealgorithms, all currently known regret bounds allow for error cancellations --one can compensate for a constraint violation in one round with a strictconstraint satisfaction in another. This makes the online learning processunsafe since it only guarantees safety for the final (mixture) policy but notduring learning. As Efroni et al. (2020) pointed out, it is an open questionwhether primal-dual algorithms can provably achieve sublinear regret if we donot allow error cancellations. In this paper, we give the first affirmativeanswer. We first generalize a result on last-iterate convergence of regularizedprimal-dual schemes to CMDPs with multiple constraints. Building upon thisinsight, we propose a model-based primal-dual algorithm to learn in an unknownCMDP. We prove that our algorithm achieves sublinear regret without errorcancellations.</description><author>Adrian Müller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He</author><pubDate>Mon, 18 Mar 2024 12:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15776v2</guid></item><item><title>TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction</title><link>http://arxiv.org/abs/2403.11695v1</link><description>Autonomous driving systems are a rapidly evolving technology that enablesdriverless car production. Trajectory prediction is a critical component ofautonomous driving systems, enabling cars to anticipate the movements ofsurrounding objects for safe navigation. Trajectory prediction using Lidarpoint-cloud data performs better than 2D images due to providing 3Dinformation. However, processing point-cloud data is more complicated andtime-consuming than 2D images. Hence, state-of-the-art 3D trajectorypredictions using point-cloud data suffer from slow and erroneous predictions.This paper introduces TrajectoryNAS, a pioneering method that focuses onutilizing point cloud data for trajectory prediction. By leveraging NeuralArchitecture Search (NAS), TrajectoryNAS automates the design of trajectoryprediction models, encompassing object detection, tracking, and forecasting ina cohesive manner. This approach not only addresses the complexinterdependencies among these tasks but also emphasizes the importance ofaccuracy and efficiency in trajectory modeling. Through empirical studies,TrajectoryNAS demonstrates its effectiveness in enhancing the performance ofautonomous driving systems, marking a significant advancement in thefield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8higger accuracy and 1.1* lower latency over competing methods on the NuScenesdataset.</description><author>Ali Asghar Sharifi, Ali Zoljodi, Masoud Daneshtalab</author><pubDate>Mon, 18 Mar 2024 12:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11695v1</guid></item><item><title>Object Segmentation-Assisted Inter Prediction for Versatile Video Coding</title><link>http://arxiv.org/abs/2403.11694v1</link><description>In modern video coding standards, block-based inter prediction is widelyadopted, which brings high compression efficiency. However, in natural videos,there are usually multiple moving objects of arbitrary shapes, resulting incomplex motion fields that are difficult to compactly represent. This problemhas been tackled by more flexible block partitioning methods in the VersatileVideo Coding (VVC) standard, but the more flexible partitions require moreoverhead bits to signal and still cannot be made arbitrary shaped. To addressthis limitation, we propose an object segmentation-assisted inter predictionmethod (SAIP), where objects in the reference frames are segmented by someadvanced technologies. With a proper indication, the object segmentation maskis translated from the reference frame to the current frame as thearbitrary-shaped partition of different regions without any extra signal. Usingthe segmentation mask, motion compensation is separately performed fordifferent regions, achieving higher prediction accuracy. The segmentation maskis further used to code the motion vectors of different regions moreefficiently. Moreover, segmentation mask is considered in the jointrate-distortion optimization for motion estimation and partition estimation toderive the motion vector of different regions and partition more accurately.The proposed method is implemented into the VVC reference software, VTM version12.0. Experimental results show that the proposed method achieves up to 1.98%,1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for commontest sequences, under the Low-delay P, Low-delay B, and Random Accessconfigurations, respectively.</description><author>Zhuoyuan Li, Zikun Yuan, Li Li, Dong Liu, Xiaohu Tang, Feng Wu</author><pubDate>Mon, 18 Mar 2024 12:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11694v1</guid></item><item><title>TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models</title><link>http://arxiv.org/abs/2403.11691v1</link><description>Test-Time Training (TTT) proposes to adapt a pre-trained network to changingdata distributions on-the-fly. In this work, we propose the first TTT methodfor 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD)from foundation models (e.g. DINOv2) as a self-supervised objective foradaptation to distribution shifts at test-time. Given access to pairedimage-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone forthe main task of semantic segmentation using the pointclouds and the task of 2D$\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model. Attest-time, our TTT-KD updates the 3D segmentation backbone for each testsample, by using the self-supervised task of knowledge distillation, beforeperforming the final prediction. Extensive evaluations on multiple indoor andoutdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improvesperformance for both in-distribution (ID) and out-of-distribution (ODO) testdatasets. We achieve a gain of up to 13% mIoU (7% on average) when the trainand test distributions are similar and up to 45% (20% on average) when adaptingto OOD test samples.</description><author>Lisa Weijler, Muhammad Jehanzeb Mirza, Leon Sick, Can Ekkazan, Pedro Hermosilla</author><pubDate>Mon, 18 Mar 2024 12:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11691v1</guid></item><item><title>MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.11689v1</link><description>The task of single-source domain generalization (SDG) in medical imagesegmentation is crucial due to frequent domain shifts in clinical imagedatasets. To address the challenge of poor generalization across differentdomains, we introduce a Plug-and-Play module for data augmentation calledMoreStyle. MoreStyle diversifies image styles by relaxing low-frequencyconstraints in Fourier space, guiding the image reconstruction network. Withthe help of adversarial learning, MoreStyle further expands the style range andpinpoints the most intricate style combinations within latent features. Tohandle significant style variations, we introduce an uncertainty-weighted loss.This loss emphasizes hard-to-classify pixels resulting only from style shiftswhile mitigating true hard-to-classify pixels in both MoreStyle-generated andoriginal images. Extensive experiments on two widely used benchmarksdemonstrate that the proposed MoreStyle effectively helps to achieve gooddomain generalization ability, and has the potential to further boost theperformance of some state-of-the-art SDG methods.</description><author>Haoyu Zhao, Wenhui Dong, Rui Yu, Zhou Zhao, Du Bo, Yongchao Xu</author><pubDate>Mon, 18 Mar 2024 12:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11689v1</guid></item><item><title>Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates</title><link>http://arxiv.org/abs/2403.11687v1</link><description>We study the problem of efficiently computing the derivative of thefixed-point of a parametric non-differentiable contraction map. This problemhas wide applications in machine learning, including hyperparameteroptimization, meta-learning and data poisoning attacks. We analyze two popularapproaches: iterative differentiation (ITD) and approximate implicitdifferentiation (AID). A key challenge behind the nonsmooth setting is that thechain rule does not hold anymore. Building upon the recent work by Bolte et al.(2022), who proved the linear convergence of non-differentiable ITD, we providerefined linear convergence rates for both ITD and AID in the deterministiccase. We further introduce NSID, a new method to compute the implicitderivative when the fixed point is defined as the composition of an outer mapand an inner map which is accessible only through a stochastic unbiasedestimator. We establish rates for the convergence of NSID to the truederivative, encompassing the best available rates in the smooth setting. Wepresent illustrative experiments confirming our analysis.</description><author>Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo</author><pubDate>Mon, 18 Mar 2024 12:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11687v1</guid></item><item><title>Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding</title><link>http://arxiv.org/abs/2403.11686v1</link><description>Predicting physical properties of materials from their crystal structures isa fundamental problem in materials science. In peripheral areas such as theprediction of molecular properties, fully connected attention networks havebeen shown to be successful. However, unlike these finite atom arrangements,crystal structures are infinitely repeating, periodic arrangements of atoms,whose fully connected attention results in infinitely connected attention. Inthis work, we show that this infinitely connected attention can lead to acomputationally tractable formulation, interpreted as neural potentialsummation, that performs infinite interatomic potential summations in a deeplylearned feature space. We then propose a simple yet effective Transformer-basedencoder architecture for crystal structures called Crystalformer. Compared toan existing Transformer-based model, the proposed model requires only 29.4% ofthe number of parameters, with minimal modifications to the originalTransformer architecture. Despite the architectural simplicity, the proposedmethod outperforms state-of-the-art methods for various property regressiontasks on the Materials Project and JARVIS-DFT datasets.</description><author>Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, Kanta Ono</author><pubDate>Mon, 18 Mar 2024 12:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11686v1</guid></item><item><title>MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</title><link>http://arxiv.org/abs/2403.11681v1</link><description>Surface prediction and completion have been widely studied in variousapplications. Recently, research in surface completion has evolved from smallobjects to complex large-scale scenes. As a result, researchers have begunincreasing the volume of data and leveraging a greater variety of datamodalities including rendered RGB images, descriptive texts, depth images, etc,to enhance algorithm performance. However, existing datasets suffer from adeficiency in the amounts of scene-level models along with the correspondingmulti-modal information. Therefore, a method to scale the datasets and generatemulti-modal information in them efficiently is essential. To bridge thisresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset witha verSatile Toolchain for surfAce pRediction and completion. We develop aversatile and efficient toolchain for processing the raw 3D data from theenvironments. It screens out a set of fine-grained scene models and generatesthe corresponding multi-modal data. Utilizing the toolchain, we then generatean example dataset composed of over a thousand scene-level models with partialreal-world data added. We compare MASSTAR with the existing datasets, whichvalidates its superiority: the ability to efficiently extract high-qualitymodels from complex scenarios to expand the dataset. Additionally, severalrepresentative surface completion algorithms are benchmarked on MASSTAR, whichreveals that existing algorithms can hardly deal with scene-level completion.We will release the source code of our toolchain and the dataset. For moredetails, please see our project page at https://sysu-star.github.io/MASSTAR.</description><author>Guiyong Zheng, Jinqi Jiang, Chen Feng, Shaojie Shen, Boyu Zhou</author><pubDate>Mon, 18 Mar 2024 12:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11681v1</guid></item><item><title>NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2403.11679v1</link><description>We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3DGaussian representation, that enables robust 3D semantic mapping, accuratecamera tracking, and high-quality rendering in real-time. In the system, wepropose a Spatially Consistent Feature Fusion model to reduce the effect oferroneous estimates from pre-trained segmentation head on semanticreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, weemploy a lightweight encoder-decoder to compress the high-dimensional semanticfeatures into a compact 3D Gaussian representation, mitigating the burden ofexcessive memory consumption. Furthermore, we leverage the advantage of 3DGaussian splatting, which enables efficient and differentiable novel viewrendering, and propose a Virtual Camera View Pruning method to eliminateoutlier GS points, thereby effectively enhancing the quality of scenerepresentations. Our NEDS-SLAM method demonstrates competitive performance overexisting dense semantic SLAM methods in terms of mapping and tracking accuracyon Replica and ScanNet datasets, while also showing excellent capabilities in3D dense semantic mapping.</description><author>Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie</author><pubDate>Mon, 18 Mar 2024 12:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11679v1</guid></item><item><title>Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes</title><link>http://arxiv.org/abs/2403.11678v1</link><description>We present a method enabling the scaling of NeRFs to learn a large number ofsemantically-similar scenes. We combine two techniques to improve the requiredtraining time and memory cost per scene. First, we learn a 3D-aware latentspace in which we train Tri-Plane scene representations, hence reducing theresolution at which scenes are learned. Moreover, we present a way to sharecommon information across scenes, hence allowing for a reduction of modelcomplexity to learn a particular scene. Our method reduces effective per-scenememory costs by 44% and per-scene time costs by 86% when training 1000 scenes.Our project page can be found at https://3da-ae.github.io .</description><author>Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet</author><pubDate>Mon, 18 Mar 2024 12:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11678v1</guid></item><item><title>Better (pseudo-)labels for semi-supervised instance segmentation</title><link>http://arxiv.org/abs/2403.11675v1</link><description>Despite the availability of large datasets for tasks like imageclassification and image-text alignment, labeled data for more complexrecognition tasks, such as detection and segmentation, is less abundant. Inparticular, for instance segmentation annotations are time-consuming toproduce, and the distribution of instances is often highly skewed acrossclasses. While semi-supervised teacher-student distillation methods showpromise in leveraging vast amounts of unlabeled data, they suffer frommiscalibration, resulting in overconfidence in frequently represented classesand underconfidence in rarer ones. Additionally, these methods encounterdifficulties in efficiently learning from a limited set of examples. Weintroduce a dual-strategy to enhance the teacher model's training process,substantially improving the performance on few-shot learning. Secondly, wepropose a calibration correction mechanism that that enables the student modelto correct the teacher's calibration errors. Using our approach, we observedmarked improvements over a state-of-the-art supervised baseline performance onthe LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3%gain in AP for rare classes.</description><author>François Porcher, Camille Couprie, Marc Szafraniec, Jakob Verbeek</author><pubDate>Mon, 18 Mar 2024 12:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11675v1</guid></item><item><title>Towards Generalizing to Unseen Domains with Few Labels</title><link>http://arxiv.org/abs/2403.11674v1</link><description>We approach the challenge of addressing semi-supervised domain generalization(SSDG). Specifically, our aim is to obtain a model that learnsdomain-generalizable features by leveraging a limited subset of labelled dataalongside a substantially larger pool of unlabeled data. Existing domaingeneralization (DG) methods which are unable to exploit unlabeled data performpoorly compared to semi-supervised learning (SSL) methods under SSDG setting.Nevertheless, SSL methods have considerable room for performance improvementwhen compared to fully-supervised DG training. To tackle this underexplored,yet highly practical problem of SSDG, we make the following core contributions.First, we propose a feature-based conformity technique that matches theposterior distributions from the feature space with the pseudo-label from themodel's output space. Second, we develop a semantics alignment loss to learnsemantically-compatible representations by regularizing the semantic structurein the feature space. Our method is plug-and-play and can be readily integratedwith different SSL-based SSDG baselines without introducing any additionalparameters. Extensive experimental results across five challenging DGbenchmarks with four strong SSL baselines suggest that our method providesconsistent and notable gains in two different SSDG settings.</description><author>Chamuditha Jayanga Galappaththige, Sanoojan Baliah, Malitha Gunawardhana, Muhammad Haris Khan</author><pubDate>Mon, 18 Mar 2024 12:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11674v1</guid></item><item><title>WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising</title><link>http://arxiv.org/abs/2403.11672v1</link><description>In clinical examinations and diagnoses, low-dose computed tomography (LDCT)is crucial for minimizing health risks compared with normal-dose computedtomography (NDCT). However, reducing the radiation dose compromises thesignal-to-noise ratio, leading to degraded quality of CT images. To addressthis, we analyze LDCT denoising task based on experimental results from thefrequency perspective, and then introduce a novel self-supervised CT imagedenoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2NDcomprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-AwareMulti-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT bymainly adding noise to the high-frequency components, which is the maindifference between LDCT and NDCT. Second, to better capture high-frequencycomponents and detailed information, Frequency-Aware Multi-scale Loss (FAM) isproposed by effectively utilizing multi-scale feature space. Extensiveexperiments on two public LDCT denoising datasets demonstrate that ourWIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-artweakly-supervised and self-supervised methods.</description><author>Haoyu Zhao, Guyu Liang, Zhou Zhao, Bo Du, Yongchao Xu, Rui Yu</author><pubDate>Mon, 18 Mar 2024 12:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11672v1</guid></item><item><title>HDLdebugger: Streamlining HDL debugging with Large Language Models</title><link>http://arxiv.org/abs/2403.11671v1</link><description>In the domain of chip design, Hardware Description Languages (HDLs) play apivotal role. However, due to the complex syntax of HDLs and the limitedavailability of online resources, debugging HDL codes remains a difficult andtime-intensive task, even for seasoned engineers. Consequently, there is apressing need to develop automated HDL code debugging models, which canalleviate the burden on hardware engineers. Despite the strong capabilities ofLarge Language Models (LLMs) in generating, completing, and debugging softwarecode, their utilization in the specialized field of HDL debugging has beenlimited and, to date, has not yielded satisfactory results. In this paper, wepropose an LLM-assisted HDL debugging framework, namely HDLdebugger, whichconsists of HDL debugging data generation via a reverse engineering approach, asearch engine for retrieval-augmented generation, and a retrieval-augmented LLMfine-tuning approach. Through the integration of these components, HDLdebuggercan automate and streamline HDL debugging for chip design. Our comprehensiveexperiments, conducted on an HDL code dataset sourced from Huawei, reveal thatHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptionaleffectiveness in HDL code debugging.</description><author>Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu</author><pubDate>Mon, 18 Mar 2024 12:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11671v1</guid></item><item><title>Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models</title><link>http://arxiv.org/abs/2403.04325v2</link><description>The process of meaning composition, wherein smaller units like morphemes orwords combine to form the meaning of phrases and sentences, is essential forhuman sentence comprehension. Despite extensive neurolinguistic research intothe brain regions involved in meaning composition, a computational metric toquantify the extent of composition is still lacking. Drawing on the key-valuememory interpretation of transformer feed-forward network blocks, we introducethe Composition Score, a novel model-based metric designed to quantify thedegree of meaning composition during sentence comprehension. Experimentalfindings show that this metric correlates with brain clusters associated withword frequency, structural processing, and general sensitivity to words,suggesting the multifaceted nature of meaning composition during human sentencecomprehension.</description><author>Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang</author><pubDate>Mon, 18 Mar 2024 12:17:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04325v2</guid></item><item><title>Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2403.11667v1</link><description>The high performance of denoising diffusion models for image generation haspaved the way for their application in unsupervised medical anomaly detection.As diffusion-based methods require a lot of GPU memory and have long samplingtimes, we present a novel and fast unsupervised anomaly detection approachbased on latent Bernoulli diffusion models. We first apply an autoencoder tocompress the input images into a binary latent representation. Next, adiffusion model that follows a Bernoulli noise schedule is employed to thislatent space and trained to restore binary latent representations fromperturbed ones. The binary nature of this diffusion model allows us to identifyentries in the latent space that have a high probability of flipping theirbinary code during the denoising process, which indicates out-of-distributiondata. We propose a masking algorithm based on these probabilities, whichimproves the anomaly detection scores. We achieve state-of-the-art performancecompared to other diffusion-based unsupervised anomaly detection algorithmswhile significantly reducing sampling time and memory consumption. The code isavailable at https://github.com/JuliaWolleb/Anomaly_berdiff.</description><author>Julia Wolleb, Florentin Bieder, Paul Friedrich, Peter Zhang, Alicia Durrer, Philippe C. Cattin</author><pubDate>Mon, 18 Mar 2024 12:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11667v1</guid></item><item><title>Normalized Validity Scores for DNNs in Regression based Eye Feature Extraction</title><link>http://arxiv.org/abs/2403.11665v1</link><description>We propose an improvement to the landmark validity loss. Landmark detectionis widely used in head pose estimation, eyelid shape extraction, as well aspupil and iris segmentation. There are numerous additional applications wherelandmark detection is used to estimate the shape of complex objects. One partof this process is the accurate and fine-grained detection of the shape. Theother part is the validity or inaccuracy per landmark, which can be used todetect unreliable areas, where the shape possibly does not fit, and to improvethe accuracy of the entire shape extraction by excluding inaccurate landmarks.We propose a normalization in the loss formulation, which improves the accuracyof the entire approach due to the numerical balance of the normalizedinaccuracy. In addition, we propose a margin for the inaccuracy to reduce theimpact of gradients, which are produced by negligible errors close to theground truth.</description><author>Wolfgang Fuhl</author><pubDate>Mon, 18 Mar 2024 12:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11665v1</guid></item><item><title>Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</title><link>http://arxiv.org/abs/2403.03121v2</link><description>Large language models (LLMs) reflect societal norms and biases, especiallyabout gender. While societal biases and stereotypes have been extensivelyresearched in various NLP applications, there is a surprising gap for emotionanalysis. However, emotion and gender are closely linked in societal discourse.E.g., women are often thought of as more empathetic, while men's anger is moresocially accepted. To fill this gap, we present the first comprehensive studyof gendered emotion attribution in five state-of-the-art LLMs (open- andclosed-source). We investigate whether emotions are gendered, and whether thesevariations are based on societal stereotypes. We prompt the models to adopt agendered persona and attribute emotions to an event like 'When I had a seriousargument with a dear person'. We then analyze the emotions generated by themodels in relation to the gender-event pairs. We find that all modelsconsistently exhibit gendered emotions, influenced by gender stereotypes. Thesefindings are in line with established research in psychology and genderstudies. Our study sheds light on the complex societal interplay betweenlanguage, gender, and emotion. The reproduction of emotion stereotypes in LLMsallows us to use those models to study the topic in detail, but raisesquestions about the predictive use of those same LLMs for emotion applications.</description><author>Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie, Dirk Hovy</author><pubDate>Mon, 18 Mar 2024 12:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03121v2</guid></item><item><title>Histo-Genomic Knowledge Distillation For Cancer Prognosis From Histopathology Whole Slide Images</title><link>http://arxiv.org/abs/2403.10040v2</link><description>Histo-genomic multi-modal methods have recently emerged as a powerfulparadigm, demonstrating significant potential for improving cancer prognosis.However, genome sequencing, unlike histopathology imaging, is still not widelyaccessible in underdeveloped regions, limiting the application of thesemulti-modal approaches in clinical settings. To address this, we propose anovel Genome-informed Hyper-Attention Network, termed G-HANet, which is capableof effectively distilling the histo-genomic knowledge during training toelevate uni-modal whole slide image (WSI)-based inference for the first time.Compared with traditional knowledge distillation methods (i.e., teacher-studentarchitecture) in other tasks, our end-to-end model is superior in terms oftraining efficiency and learning cross-modal interactions. Specifically, thenetwork comprises the cross-modal associating branch (CAB) and hyper-attentionsurvival branch (HSB). Through the genomic data reconstruction from WSIs, CABeffectively distills the associations between functional genotypes andmorphological phenotypes and offers insights into the gene expression profilesin the feature space. Subsequently, HSB leverages the distilled histo-genomicassociations as well as the generated morphology-based weights to achieve thehyper-attention modeling of the patients from both histopathology and genomicperspectives to improve cancer prognosis. Extensive experiments are conductedon five TCGA benchmarking datasets and the results demonstrate that G-HANetsignificantly outperforms the state-of-the-art WSI-based methods and achievescompetitive performance with genome-based and multi-modal methods. G-HANet isexpected to be explored as a useful tool by the research community to addressthe current bottleneck of insufficient histo-genomic data pairing in thecontext of cancer prognosis and precision oncology.</description><author>Zhikang Wang, Yumeng Zhang, Yingxue Xu, Seiya Imoto, Hao Chen, Jiangning Song</author><pubDate>Mon, 18 Mar 2024 12:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10040v2</guid></item><item><title>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images</title><link>http://arxiv.org/abs/2402.14899v2</link><description>Recently, Multimodal LLMs (MLLMs) have shown a great ability to understandimages. However, like traditional vision models, they are still vulnerable toadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widelyexplored on MLLMs, which not only improves model's performance, but alsoenhances model's explainability by giving intermediate reasoning steps.Nevertheless, there is still a lack of study regarding MLLMs' adversarialrobustness with CoT and an understanding of what the rationale looks like whenMLLMs infer wrong answers with adversarial images. Our research evaluates theadversarial robustness of MLLMs when employing CoT reasoning, finding that CoTmarginally improves adversarial robustness against existing attack methods.Moreover, we introduce a novel stop-reasoning attack technique that effectivelybypasses the CoT-induced robustness enhancements. Finally, we demonstrate thealterations in CoT reasoning when MLLMs confront adversarial images, sheddinglight on their reasoning process under adversarial attacks.</description><author>Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</author><pubDate>Mon, 18 Mar 2024 11:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14899v2</guid></item><item><title>ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training</title><link>http://arxiv.org/abs/2312.13316v2</link><description>Despite significant advancements in medical vision-language pre-training,existing methods have largely overlooked the inherent entity-specific contextwithin radiology reports and the complex cross-modality contextualrelationships between text and images. To close this gap, we propose a novelEntity-centered Context-aware Medical Vision-language Pre-training (ECAMP)framework, which is designed to enable a more entity-centered andcontext-sensitive interpretation of medical data. Utilizing the recent powerfullarge language model, we distill entity-centered context from medical reports,which enables ECAMP to gain more effective supervision from the text modality.By further pre-training our model with carefully designed entity-aware,context-enhanced masked language modeling and context-guided super-resolutiontasks, ECAMP significantly refines the interplay between text and imagemodalities, leading to an enhanced ability to extract entity-centeredcontextual features. Besides, our proposed multi-scale context fusion designalso improves the semantic integration of both coarse and fine-level imagerepresentations, prompting better performance for multi-scale downstreamapplications. Combining these components leads to significant performance leapsover current state-of-the-art methods and establishes a new standard forcross-modality learning in medical imaging, whose effectiveness is demonstratedby our extensive experiments on various tasks including classification,segmentation, and detection across several public datasets. Code and models areavailable at https://github.com/ToniChopp/ECAMP.</description><author>Rongsheng Wang, Qingsong Yao, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, S. Kevin Zhou</author><pubDate>Mon, 18 Mar 2024 11:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13316v2</guid></item><item><title>LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model</title><link>http://arxiv.org/abs/2403.11656v1</link><description>Previous work has shown that well-crafted adversarial perturbations canthreaten the security of video recognition systems. Attackers can invade suchmodels with a low query budget when the perturbations are semantic-invariant,such as StyleFool. Despite the query efficiency, the naturalness of the minutiaareas still requires amelioration, since StyleFool leverages style transfer toall pixels in each frame. To close the gap, we propose LocalStyleFool, animproved black-box video adversarial attack that superimposes regionalstyle-transfer-based perturbations on videos. Benefiting from the popularityand scalably usability of Segment Anything Model (SAM), we first extractdifferent regions according to semantic information and then track them throughthe video stream to maintain the temporal consistency. Then, we addstyle-transfer-based perturbations to several regions selected based on theassociative criterion of transfer-based gradient information and regional area.Perturbation fine adjustment is followed to make stylized videos adversarial.We demonstrate that LocalStyleFool can improve both intra-frame and inter-framenaturalness through a human-assessed survey, while maintaining competitivefooling rate and query efficiency. Successful experiments on thehigh-resolution dataset also showcase that scrupulous segmentation of SAM helpsto improve the scalability of adversarial attacks under high-resolution data.</description><author>Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu</author><pubDate>Mon, 18 Mar 2024 11:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11656v1</guid></item><item><title>Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions</title><link>http://arxiv.org/abs/2403.01222v2</link><description>Emotions are a central aspect of communication. Consequently, emotionanalysis (EA) is a rapidly growing field in natural language processing (NLP).However, there is no consensus on scope, direction, or methods. In this paper,we conduct a thorough review of 154 relevant NLP publications from the lastdecade. Based on this review, we address four different questions: (1) How areEA tasks defined in NLP? (2) What are the most prominent emotion frameworks andwhich emotions are modeled? (3) Is the subjectivity of emotions considered interms of demographics and cultural factors? and (4) What are the primary NLPapplications for EA? We take stock of trends in EA and tasks, emotionframeworks used, existing datasets, methods, and applications. We then discussfour lacunae: (1) the absence of demographic and cultural aspects does notaccount for the variation in how emotions are perceived, but instead assumesthey are universally experienced in the same manner; (2) the poor fit ofemotion categories from the two main emotion theories to the task; (3) the lackof standardized EA terminology hinders gap identification, comparison, andfuture goals; and (4) the absence of interdisciplinary research isolates EAfrom insights in other fields. Our work will enable more focused research intoEA and a more holistic approach to modeling emotions in NLP.</description><author>Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy</author><pubDate>Mon, 18 Mar 2024 11:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01222v2</guid></item><item><title>Prioritized Semantic Learning for Zero-shot Instance Navigation</title><link>http://arxiv.org/abs/2403.11650v1</link><description>We study zero-shot instance navigation, in which the agent navigates to aspecific object without using object annotations for training. Previous objectnavigation approaches apply the image-goal navigation (ImageNav) task (go tothe location of an image) for pretraining, and transfer the agent to achieveobject goals using a vision-language model. However, these approaches lead toissues of semantic neglect, where the model fails to learn meaningful semanticalignments. In this paper, we propose a Prioritized Semantic Learning (PSL)method to improve the semantic understanding ability of navigation agents.Specifically, a semantic-enhanced PSL agent is proposed and a prioritizedsemantic training strategy is introduced to select goal images that exhibitclear semantic supervision and relax the reward function from strict exact viewmatching. At inference time, a semantic expansion inference scheme is designedto preserve the same granularity level of the goal-semantic as training.Furthermore, for the popular HM3D environment, we present an InstanceNavigation (InstanceNav) task that requires going to a specific object instancewith detailed descriptions, as opposed to the Object Navigation (ObjectNav)task where the goal is defined merely by the object category. Our PSL agentoutperforms the previous state-of-the-art by 66% on zero-shot ObjectNav interms of success rate and is also superior on the new InstanceNav task. Codewill be released at https://anonymous.4open. science/r/PSL/.</description><author>Xander Sun, Louis Lau, Hoyard Zhi, Ronghe Qiu, Junwei Liang</author><pubDate>Mon, 18 Mar 2024 11:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11650v1</guid></item><item><title>Gridless 2D Recovery of Lines using the Sliding Frank-Wolfe Algorithm</title><link>http://arxiv.org/abs/2403.11649v1</link><description>We present a new approach leveraging the Sliding Frank--Wolfe algorithm toaddress the challenge of line recovery in degraded images. Building uponadvances in conditional gradient methods for sparse inverse problems withdifferentiable measurement models, we propose two distinct models tailored forline detection tasks within the realm of blurred line deconvolution and ridgedetection of linear chirps in spectrogram images.</description><author>Kévin Polisano, Basile Dubois-Bonnaire, Sylvain Meignen</author><pubDate>Mon, 18 Mar 2024 11:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11649v1</guid></item><item><title>MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks</title><link>http://arxiv.org/abs/2403.11646v1</link><description>Transfer learning has become a powerful tool to initialize deep learningmodels to achieve faster convergence and higher performance. This is especiallyuseful in the medical imaging analysis domain, where data scarcity limitspossible performance gains for deep learning models. Some advancements havebeen made in boosting the transfer learning performance gain by merging modelsstarting from the same initialization. However, in the medical imaging analysisdomain, there is an opportunity in merging models starting from differentinitialisations, thus combining the features learnt from different tasks. Inthis work, we propose MedMerge, a method whereby the weights of differentmodels can be merged, and their features can be effectively utilized to boostperformance on a new task. With MedMerge, we learn kernel-level weights thatcan later be used to merge the models into a single model, even when startingfrom different initializations. Testing on various medical imaging analysistasks, we show that our merged model can achieve significant performance gains,with up to 3% improvement on the F1 score. The code implementation of this workwill be available at www.github.com/BioMedIA-MBZUAI/MedMerge.</description><author>Ibrahim Almakky, Santosh Sanjeev, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Mohammad Yaqub</author><pubDate>Mon, 18 Mar 2024 11:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11646v1</guid></item><item><title>Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification</title><link>http://arxiv.org/abs/2401.09493v3</link><description>Cloud radiative feedback impacts early tropical cyclone (TC) intensification,but limitations in existing diagnostic frameworks make them unsuitable forstudying asymmetric or transient radiative heating. We propose a linearVariational Encoder-Decoder (VED) to learn the hidden relationship betweenradiation and the surface intensification of realistic simulated TCs. LimitingVED model inputs enables using its uncertainty to identify periods whenradiation has more importance for intensification. A close examination of theextracted 3D radiative structures suggests that longwave radiative forcing frominner core deep convection and shallow clouds both contribute tointensification, with the deep convection having the most impact overall. Wefind that deep convection downwind of the shallow clouds is critical to theintensification of Haiyan. Our work demonstrates that machine learning candiscover thermodynamic-kinematic relationships without relying on axisymmetricor deterministic assumptions, paving the way towards the objective discovery ofprocesses leading to TC intensification in realistic conditions.</description><author>Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr</author><pubDate>Mon, 18 Mar 2024 11:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09493v3</guid></item><item><title>Diffusion-Based Environment-Aware Trajectory Prediction</title><link>http://arxiv.org/abs/2403.11643v1</link><description>The ability to predict the future trajectories of traffic participants iscrucial for the safe and efficient operation of autonomous vehicles. In thispaper, a diffusion-based generative model for multi-agent trajectory predictionis proposed. The model is capable of capturing the complex interactions betweentraffic participants and the environment, accurately learning the multimodalnature of the data. The effectiveness of the approach is assessed onlarge-scale datasets of real-world traffic scenarios, showing that our modeloutperforms several well-established methods in terms of prediction accuracy.By the incorporation of differential motion constraints on the model output, weillustrate that our model is capable of generating a diverse set of realisticfuture trajectories. Through the use of an interaction-aware guidance signal,we further demonstrate that the model can be adapted to predict the behavior ofless cooperative agents, emphasizing its practical applicability underuncertain traffic conditions.</description><author>Theodor Westny, Björn Olofsson, Erik Frisk</author><pubDate>Mon, 18 Mar 2024 11:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11643v1</guid></item><item><title>Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring</title><link>http://arxiv.org/abs/2403.11642v1</link><description>Counterfactual explanations suggest what should be different in the inputinstance to change the outcome of an AI system. When dealing withcounterfactual explanations in the field of Predictive Process Monitoring,however, control flow relationships among events have to be carefullyconsidered. A counterfactual, indeed, should not violate control flowrelationships among activities (temporal background knowledege). Within thefield of Explainability in Predictive Process Monitoring, there have been aseries of works regarding counterfactual explanations for outcome-basedpredictions. However, none of them consider the inclusion of temporalbackground knowledge when generating these counterfactuals. In this work, weadapt state-of-the-art techniques for counterfactual generation in the domainof XAI that are based on genetic algorithms to consider a series of temporalconstraints at runtime. We assume that this temporal background knowledge isgiven, and we adapt the fitness function, as well as the crossover and mutationoperators, to maintain the satisfaction of the constraints. The proposedmethods are evaluated with respect to state-of-the-art genetic algorithms forcounterfactual generation and the results are presented. We showcase that theinclusion of temporal background knowledge allows the generation ofcounterfactuals more conformant to the temporal background knowledge, withouthowever losing in terms of the counterfactual traditional quality metrics.</description><author>Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan Donadello, Fabrizio Maria Maggi</author><pubDate>Mon, 18 Mar 2024 11:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11642v1</guid></item><item><title>Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised Learning</title><link>http://arxiv.org/abs/2310.01012v3</link><description>The Canonical Correlation Analysis (CCA) family of methods is foundational inmultiview learning. Regularised linear CCA methods can be seen to generalisePartial Least Squares (PLS) and be unified with a Generalized EigenvalueProblem (GEP) framework. However, classical algorithms for these linear methodsare computationally infeasible for large-scale data. Extensions to Deep CCAshow great promise, but current training procedures are slow and complicated.First we propose a novel unconstrained objective that characterizes the topsubspace of GEPs. Our core contribution is a family of fast algorithms forstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applyingstochastic gradient descent (SGD) to the corresponding CCA objectives. Ouralgorithms show far faster convergence and recover higher correlations than theprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. Theseimprovements allow us to perform a first-of-its-kind PLS analysis of anextremely large biomedical dataset from the UK Biobank, with over 33,000individuals and 500,000 features. Finally, we apply our algorithms to match theperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10and CIFAR-100 with minimal hyper-parameter tuning, and also present theory toclarify the links between these methods and classical CCA, laying thegroundwork for future insights.</description><author>James Chapman, Lennie Wells, Ana Lawry Aguila</author><pubDate>Mon, 18 Mar 2024 11:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01012v3</guid></item><item><title>Arc2Face: A Foundation Model of Human Faces</title><link>http://arxiv.org/abs/2403.11641v1</link><description>This paper presents Arc2Face, an identity-conditioned face foundation model,which, given the ArcFace embedding of a person, can generate diversephoto-realistic images with an unparalleled degree of face similarity thanexisting models. Despite previous attempts to decode face recognition featuresinto detailed images, we find that common high-resolution datasets (e.g. FFHQ)lack sufficient identities to reconstruct any subject. To that end, wemeticulously upsample a significant portion of the WebFace42M database, thelargest public dataset for face recognition (FR). Arc2Face builds upon apretrained Stable Diffusion model, yet adapts it to the task of ID-to-facegeneration, conditioned solely on ID vectors. Deviating from recent works thatcombine ID with text embeddings for zero-shot personalization of text-to-imagemodels, we emphasize on the compactness of FR features, which can fully capturethe essence of the human face, as opposed to hand-crafted prompts. Crucially,text-augmented models struggle to decouple identity and text, usuallynecessitating some description of the given face to achieve satisfactorysimilarity. Arc2Face, however, only needs the discriminative features ofArcFace to guide the generation, offering a robust prior for a plethora oftasks where ID consistency is of paramount importance. As an example, we traina FR model on synthetic images from our model and achieve superior performanceto existing synthetic datasets.</description><author>Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, Stefanos Zafeiriou</author><pubDate>Mon, 18 Mar 2024 11:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11641v1</guid></item><item><title>A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning</title><link>http://arxiv.org/abs/2312.00502v2</link><description>Despite the recent increase in research activity, deep-learning models havenot yet been widely accepted in several real-world settings, such as medicine.The shortage of high-quality annotated data often hinders the development ofrobust and generalizable models, which do not suffer from degradedeffectiveness when presented with newly-collected, out-of-distribution (OOD)datasets. Contrastive Self-Supervised Learning (SSL) offers a potentialsolution to labeled data scarcity, as it takes advantage of unlabeled data toincrease model effectiveness and robustness. In this research, we proposeapplying contrastive SSL for detecting abnormalities in 1D phonocardiogram(PCG) samples by learning a generalized representation of the signal.Specifically, we perform an extensive comparative evaluation of a wide range ofaudio-based augmentations, evaluate trained classifiers on multiple datasetsacross different downstream tasks, and finally report on the impact of eachaugmentation in model training. We experimentally demonstrate that, dependingon its training distribution, the effectiveness of a fully-supervised model candegrade up to 32% when evaluated on unseen data, while SSL models only lose upto 10% or even improve in some cases. We argue and experimentally demonstratethat, contrastive SSL pretraining can assist in providing robust classifierswhich can generalize to unseen, OOD data, without relying on time- andlabor-intensive annotation processes by medical experts. Furthermore, theproposed extensive evaluation protocol sheds light on the most promising andappropriate augmentations for robust PCG signal processing, by calculatingtheir effect size on model training. Finally, we provide researchers andpractitioners with a roadmap towards producing robust models for PCGclassification, in addition to an open-source codebase for developing novelapproaches.</description><author>Aristotelis Ballas, Vasileios Papapanagiotou, Christos Diou</author><pubDate>Mon, 18 Mar 2024 11:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00502v2</guid></item><item><title>An Accurate and Real-time Relative Pose Estimation from Triple Point-line Images by Decoupling Rotation and Translation</title><link>http://arxiv.org/abs/2403.11639v1</link><description>Line features are valid complements for point features in man-madeenvironments. 3D-2D constraints provided by line features have been widely usedin Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, howto accurately solve three-view relative motion only with 2D observations ofpoints and lines in real time has not been fully explored. In this paper, wepropose a novel three-view pose solver based on rotation-translation decoupledestimation. First, a high-precision rotation estimation method based on normalvector coplanarity constraints that consider the uncertainty of observations isproposed, which can be solved by Levenberg-Marquardt (LM) algorithmefficiently. Second, a robust linear translation constraint that minimizes thedegree of the rotation components and feature observation components inequations is elaborately designed for estimating translations accurately.Experiments on synthetic data and real-world data show that the proposedapproach improves both rotation and translation accuracy compared to theclassical trifocal-tensor-based method and the state-of-the-art two-viewalgorithm in outdoor and indoor environments.</description><author>Zewen Xu, Yijia He, Hao Wei, Bo Xu, BinJian Xie, Yihong Wu</author><pubDate>Mon, 18 Mar 2024 11:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11639v1</guid></item><item><title>The Value of Reward Lookahead in Reinforcement Learning</title><link>http://arxiv.org/abs/2403.11637v1</link><description>In reinforcement learning (RL), agents sequentially interact with changingenvironments while aiming to maximize the obtained rewards. Usually, rewardsare observed only after acting, and so the goal is to maximize the expectedcumulative reward. Yet, in many practical settings, reward information isobserved in advance -- prices are observed before performing transactions;nearby traffic information is partially known; and goals are oftentimes givento agents prior to the interaction. In this work, we aim to quantifiablyanalyze the value of such future reward information through the lens ofcompetitive analysis. In particular, we measure the ratio between the value ofstandard RL agents and that of agents with partial future-reward lookahead. Wecharacterize the worst-case reward distribution and derive exact ratios for theworst-case reward expectations. Surprisingly, the resulting ratios relate toknown quantities in offline RL and reward-free exploration. We further providetight bounds for the ratio given the worst-case dynamics. Our results cover thefull spectrum between observing the immediate rewards before acting toobserving all the rewards before the interaction starts.</description><author>Nadav Merlis, Dorian Baudry, Vianney Perchet</author><pubDate>Mon, 18 Mar 2024 11:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11637v1</guid></item><item><title>Personalized 3D Human Pose and Shape Refinement</title><link>http://arxiv.org/abs/2403.11634v1</link><description>Recently, regression-based methods have dominated the field of 3D human poseand shape estimation. Despite their promising results, a common issue is themisalignment between predictions and image observations, often caused by minorjoint rotation errors that accumulate along the kinematic chain. To addressthis issue, we propose to construct dense correspondences between initial humanmodel estimates and the corresponding images that can be used to refine theinitial predictions. To this end, we utilize renderings of the 3D models topredict per-pixel 2D displacements between the synthetic renderings and the RGBimages. This allows us to effectively integrate and exploit appearanceinformation of the persons. Our per-pixel displacements can be efficientlytransformed to per-visible-vertex displacements and then used for 3D modelrefinement by minimizing a reprojection loss. To demonstrate the effectivenessof our approach, we refine the initial 3D human mesh predictions of multiplemodels using different refinement procedures on 3DPW and RICH. We show that ourapproach not only consistently leads to better image-model alignment, but alsoto improved 3D accuracy.</description><author>Tom Wehrbein, Bodo Rosenhahn, Iain Matthews, Carsten Stoll</author><pubDate>Mon, 18 Mar 2024 11:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11634v1</guid></item><item><title>Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning</title><link>http://arxiv.org/abs/2403.07440v2</link><description>Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) havebeen proven to significantly enhance model performance on a variety ofdownstream tasks and effectively control the output behaviors of LPLMs. Recentstudies have proposed numerous methods for fine-tuning a small number ofparameters based on open-source LPLMs, reducing the demand for computationaland storage resources. Among these, reparameterization fine-tuning methodsrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find thatalthough these methods perform well in many aspects, there is stillconsiderable room for improvement in terms of complex task adaptability,performance, stability, and algorithm complexity. In response to this, inspiredby the idea that the functions of the brain are shaped by its geometricstructure, this paper integrates this idea into LoRA technology and proposes anew matrix transformation-based reparameterization method for efficientfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).MTLoRA aims to dynamically alter its spatial geometric structure by applying atransformation-matrix T to perform linear transformations, such as rotation,scaling, and translation, on the task-specific parameter matrix, generating newmatrix feature patterns (eigenvectors) to mimic the fundamental influence ofcomplex geometric structure feature patterns in the brain on functions, therebyenhancing the model's performance in downstream tasks. In Natural LanguageUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, andthe results reveal that MTLoRA achieves an overall performance increase ofabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,MTLoRA improves performance by an average of 0.95% and 0.56% in the DART andWebNLG tasks, respectively.</description><author>Yao Liang, Yuwei Wang, Yang Li, Yi Zeng</author><pubDate>Mon, 18 Mar 2024 11:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07440v2</guid></item><item><title>Compositional Kronecker Context Optimization for Vision-Language Models</title><link>http://arxiv.org/abs/2403.11631v1</link><description>Context Optimization (CoOp) has emerged as a simple yet effective techniquefor adapting CLIP-like vision-language models to downstream image recognitiontasks. Nevertheless, learning compact context with satisfactory base-to-new,domain and cross-task generalization ability while adapting to new tasks isstill a challenge. To tackle such a challenge, we propose a lightweight yetgeneralizable approach termed Compositional Kronecker Context Optimization(CK-CoOp). Technically, the prompt's context words in CK-CoOp are learnablevectors, which are crafted by linearly combining base vectors sourced from adictionary. These base vectors consist of a non-learnable component obtained byquantizing the weights in the token embedding layer, and a learnable componentconstructed by applying Kronecker product on several learnable tiny matrices.Intuitively, the compositional structure mitigates the risk of overfitting ontraining data by remembering more pre-trained knowledge. Meantime, theKronecker product breaks the non-learnable restrictions of the dictionary,thereby enhancing representation ability with minimal additional parameters.Extensive experiments confirm that CK-CoOp achieves state-of-the-artperformance under base-to-new, domain and cross-task generalization evaluation,but also has the metrics of fewer learnable parameters and efficient trainingand inference speed.</description><author>Kun Ding, Xiaohui Li, Qiang Yu, Ying Wang, Haojian Zhang, Shiming Xiang</author><pubDate>Mon, 18 Mar 2024 11:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11631v1</guid></item><item><title>Properties of Discrete Sliced Wasserstein Losses</title><link>http://arxiv.org/abs/2307.10352v3</link><description>The Sliced Wasserstein (SW) distance has become a popular alternative to theWasserstein distance for comparing probability measures. Widespreadapplications include image processing, domain adaptation and generativemodelling, where it is common to optimise some parameters in order to minimiseSW, which serves as a loss function between discrete probability measures(since measures admitting densities are numerically unattainable). All theseoptimisation problems bear the same sub-problem, which is minimising the SlicedWasserstein energy. In this paper we study the properties of $\mathcal{E}: Y\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance betweentwo uniform discrete measures with the same amount of points as a function ofthe support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. Weinvestigate the regularity and optimisation properties of this energy, as wellas its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation inSW using only $p$ samples) and show convergence results on the critical pointsof $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniformconvergence. Finally, we show that in a certain sense, Stochastic GradientDescent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards(Clarke) critical points of these energies.</description><author>Eloi Tanguy, Rémi Flamary, Julie Delon</author><pubDate>Mon, 18 Mar 2024 11:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10352v3</guid></item><item><title>LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models</title><link>http://arxiv.org/abs/2403.11627v1</link><description>Customization generation techniques have significantly advanced the synthesisof specific concepts across varied contexts. Multi-concept customizationemerges as the challenging task within this domain. Existing approaches oftenrely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRAto merge various concepts into a single image. However, we identify thisstraightforward method faces two major challenges: 1) concept confusion, whichoccurs when the model cannot preserve distinct individual characteristics, and2) concept vanishing, where the model fails to generate the intended subjects.To address these issues, we introduce LoRA-Composer, a training-free frameworkdesigned for seamlessly integrating multiple LoRAs, thereby enhancing theharmony among different concepts within generated images. LoRA-Composeraddresses concept vanishing through Concept Injection Constraints, enhancingconcept visibility via an expanded cross-attention mechanism. To combat conceptconfusion, Concept Isolation Constraints are introduced, refining theself-attention computation. Furthermore, Latent Re-initialization is proposedto effectively stimulate concept-specific latent within designated regions. Ourextensive testing showcases a notable enhancement in LoRA-Composer'sperformance compared to standard baselines, especially when eliminating theimage-based conditions like canny edge or pose estimations. Code is released athttps://github.com/Young98CN/LoRA\_Composer.</description><author>Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, Wei Liu</author><pubDate>Mon, 18 Mar 2024 10:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11627v1</guid></item><item><title>QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation</title><link>http://arxiv.org/abs/2403.11626v1</link><description>The study of music-generated dance is a novel and challenging Imagegeneration task. It aims to input a piece of music and seed motions, thengenerate natural dance movements for the subsequent music. Transformer-basedmethods face challenges in time series prediction tasks related to humanmovements and music due to their struggle in capturing the nonlinearrelationship and temporal aspects. This can lead to issues like jointdeformation, role deviation, floating, and inconsistencies in dance movementsgenerated in response to the music. In this paper, we propose aQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from aquaternion perspective, which consists of a Spin Position Embedding (SPE)module and a Quaternion Rotary Attention (QRA) module. First, SPE embedsposition information into self-attention in a rotational manner, leading tobetter learning of features of movement sequences and audio sequences, andimproved understanding of the connection between music and dance. Second, QRArepresents and fuses 3D motion features and audio features in the form of aseries of quaternions, enabling the model to better learn the temporalcoordination of music and dance under the complex temporal cycle conditions ofdance generation. Finally, we conducted experiments on the dataset AIST++, andthe results show that our approach achieves better and more robust performancein generating accurate, high-quality dance movements. Our source code anddataset can be available from https://github.com/MarasyZZ/QEAN andhttps://google.github.io/aistplusplus_dataset respectively.</description><author>Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian Huang, Zinuo Li</author><pubDate>Mon, 18 Mar 2024 10:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11626v1</guid></item><item><title>GaussNav: Gaussian Splatting for Visual Navigation</title><link>http://arxiv.org/abs/2403.11625v1</link><description>In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent tolocate a specific object depicted in a goal image within an unexploredenvironment. The primary difficulty of IIN stems from the necessity ofrecognizing the target object across varying viewpoints and rejecting potentialdistractors. Existing map-based navigation methods largely adopt the representation formof Bird's Eye View (BEV) maps, which, however, lack the representation ofdetailed textures in a scene. To address the above issues, we propose a new Gaussian Splatting Navigation(abbreviated as GaussNav) framework for IIN task, which constructs a novel maprepresentation based on 3D Gaussian Splatting (3DGS). The proposed framework enables the agent to not only memorize the geometryand semantic information of the scene, but also retain the textural features ofobjects. Our GaussNav framework demonstrates a significant leap in performance,evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset. Our code will be made publicly available.</description><author>Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li</author><pubDate>Mon, 18 Mar 2024 10:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11625v1</guid></item><item><title>Dual-Channel Multiplex Graph Neural Networks for Recommendation</title><link>http://arxiv.org/abs/2403.11624v1</link><description>Efficient recommender systems play a crucial role in accurately capturinguser and item attributes that mirror individual preferences. Some existingrecommendation techniques have started to shift their focus towards modelingvarious types of interaction relations between users and items in real-worldrecommendation scenarios, such as clicks, marking favorites, and purchases ononline shopping platforms. Nevertheless, these approaches still grapple withtwo significant shortcomings: (1) Insufficient modeling and exploitation of theimpact of various behavior patterns formed by multiplex relations between usersand items on representation learning, and (2) ignoring the effect of differentrelations in the behavior patterns on the target relation in recommender systemscenarios. In this study, we introduce a novel recommendation framework,Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses theaforementioned challenges. It incorporates an explicit behavior patternrepresentation learner to capture the behavior patterns composed of multiplexuser-item interaction relations, and includes a relation chain representationlearning and a relation chain-aware encoder to discover the impact of variousauxiliary relations on the target relation, the dependencies between differentrelations, and mine the appropriate order of relations in a behavior pattern.Extensive experiments on three real-world datasets demonstrate that our \modelsurpasses various state-of-the-art recommendation methods. It outperforms thebest baselines by 10.06\% and 12.15\% on average across all datasets in termsof R@10 and N@10 respectively.</description><author>Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu Dong, Yanwei Yu</author><pubDate>Mon, 18 Mar 2024 10:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11624v1</guid></item><item><title>Matching Non-Identical Objects</title><link>http://arxiv.org/abs/2403.08227v2</link><description>Not identical but similar objects are everywhere in the world. Examplesinclude four-legged animals such as dogs and cats, cars of different models,akin flowers in various colors, and countless others. In this study, we addressa novel task of matching such non-identical objects. We propose a simpleweighting scheme of descriptors that enhances various sparse image matchingmethods, which were originally designed for matching identical objects capturedfrom different perspectives, and achieve semantically robust matching. Theexperiments show successful matching between non-identical objects in variouscases including domain shift. Further, we present a first evaluation of therobustness of the image matching methods under common corruptions, which is asort of domain shift, and the proposed method improves the matching in thiscase as well.</description><author>Yusuke Marumo, Kazuhiko Kawamoto, Hiroshi Kera</author><pubDate>Mon, 18 Mar 2024 10:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08227v2</guid></item><item><title>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</title><link>http://arxiv.org/abs/2307.11714v3</link><description>Optimal Transport has sparked vivid interest in recent years, in particularthanks to the Wasserstein distance, which provides a geometrically sensible andintuitive way of comparing probability measures. For computational reasons, theSliced Wasserstein (SW) distance was introduced as an alternative to theWasserstein distance, and has seen uses for training generative Neural Networks(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observedpractically in such a setting, there is to our knowledge no theoreticalguarantee for this observation. Leveraging recent works on convergence of SGDon non-smooth and non-convex functions by Bianchi et al. (2022), we aim tobridge that knowledge gap, and provide a realistic context under whichfixed-step SGD trajectories for the SW loss on NN parameters converge. Moreprecisely, we show that the trajectories approach the set of (sub)-gradientflow equations as the step decreases. Under stricter assumptions, we show amuch stronger convergence result for noised and projected SGD schemes, namelythat the long-run limits of the trajectories approach a set of generalisedcritical points of the loss function.</description><author>Eloi Tanguy</author><pubDate>Mon, 18 Mar 2024 10:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11714v3</guid></item><item><title>Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model</title><link>http://arxiv.org/abs/2403.11621v1</link><description>Large Language Models (LLMs) are composed of neurons that exhibit variousbehaviors and roles, which become increasingly diversified as models scale.Recent studies have revealed that not all neurons are active across differentdatasets, and this sparsity correlates positively with the task-specificability, leading to advancements in model pruning and training efficiency.Traditional fine-tuning methods engage all parameters of LLMs, which iscomputationally expensive and may not be necessary. In contrast,Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number oftrainable parameters, yet they still operate at a relatively macro scale (e.g.,layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approachthat refines the granularity of parameter training down to the individualneuron, enabling more precise and computationally efficient model updates. Theexperimental results show that NeFT not only exceeded the performance offull-parameter fine-tuning and PEFT but also provided insights into theanalysis of neurons.</description><author>Haoyun Xu, Runzhe Zhan, Derek F. Wong, Lidia S. Chao</author><pubDate>Mon, 18 Mar 2024 10:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11621v1</guid></item><item><title>Identifying Policy Gradient Subspaces</title><link>http://arxiv.org/abs/2401.06604v3</link><description>Policy gradient methods hold great potential for solving complex continuouscontrol tasks. Still, their training efficiency can be improved by exploitingstructure within the optimization problem. Recent work indicates thatsupervised learning can be accelerated by leveraging the fact that gradientslie in a low-dimensional and slowly-changing subspace. In this paper, weconduct a thorough evaluation of this phenomenon for two popular deep policygradient methods on various simulated benchmark tasks. Our results demonstratethe existence of such gradient subspaces despite the continuously changing datadistribution inherent to reinforcement learning. These findings revealpromising directions for future work on more efficient reinforcement learning,e.g., through improving parameter-space exploration or enabling second-orderoptimization.</description><author>Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler</author><pubDate>Mon, 18 Mar 2024 10:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06604v3</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v3</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Mon, 18 Mar 2024 10:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v3</guid></item><item><title>Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception</title><link>http://arxiv.org/abs/2403.11616v1</link><description>For training a video-based action recognition model that accepts multi-viewvideo, annotating frame-level labels is tedious and difficult. However, it isrelatively easy to annotate sequence-level labels. This kind of coarseannotations are called as weak labels. However, training a multi-viewvideo-based action recognition model with weak labels for frame-levelperception is challenging. In this paper, we propose a novel learningframework, where the weak labels are first used to train a multi-viewvideo-based base model, which is subsequently used for downstream frame-levelperception tasks. The base model is trained to obtain individual latentembeddings for each view in the multi-view input. For training the model usingthe weak labels, we propose a novel latent loss function. We also propose amodel that uses the view-specific latent embeddings for downstream frame-levelaction recognition and detection tasks. The proposed framework is evaluatedusing the MM Office dataset by comparing several baseline algorithms. Theresults show that the proposed base model is effectively trained using weaklabels and the latent embeddings help the downstream models improve accuracy.</description><author>Vijay John, Yasutomo Kawanishi</author><pubDate>Mon, 18 Mar 2024 10:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11616v1</guid></item><item><title>GRAM: Global Reasoning for Multi-Page VQA</title><link>http://arxiv.org/abs/2401.03411v2</link><description>The increasing use of transformer-based large language models brings forwardthe challenge of processing long sequences. In document visual questionanswering (DocVQA), leading methods focus on the single-page setting, whiledocuments can span hundreds of pages. We present GRAM, a method that seamlesslyextends pre-trained single-page models to the multi-page setting, withoutrequiring computationally-heavy pretraining. To do so, we leverage asingle-page encoder for local page-level understanding, and enhance it withdocument-level designated layers and learnable tokens, facilitating the flow ofinformation across pages for global reasoning. To enforce our model to utilizethe newly introduced document tokens, we propose a tailored bias adaptationmethod. For additional computational savings during decoding, we introduce anoptional compression stage using our compression-transformer(C-Former),reducing the encoded sequence length, thereby allowing a tradeoffbetween quality and latency. Extensive experiments showcase GRAM'sstate-of-the-art performance on the benchmarks for multi-page DocVQA,demonstrating the effectiveness of our approach.</description><author>Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman</author><pubDate>Mon, 18 Mar 2024 10:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03411v2</guid></item><item><title>EcoVal: An Efficient Data Valuation Framework for Machine Learning</title><link>http://arxiv.org/abs/2402.09288v3</link><description>Quantifying the value of data within a machine learning workflow can play apivotal role in making more strategic decisions in machine learninginitiatives. The existing Shapley value based frameworks for data valuation inmachine learning are computationally expensive as they require considerableamount of repeated training of the model to obtain the Shapley value. In thispaper, we introduce an efficient data valuation framework EcoVal, to estimatethe value of data for machine learning models in a fast and practical manner.Instead of directly working with individual data sample, we determine the valueof a cluster of similar data points. This value is further propagated amongstall the member cluster points. We show that the overall data value can bedetermined by estimating the intrinsic and extrinsic value of each data. Thisis enabled by formulating the performance of a model as a \textit{productionfunction}, a concept which is popularly used to estimate the amount of outputbased on factors like labor and capital in a traditional free economic market.We provide a formal proof of our valuation technique and elucidate theprinciples and mechanisms that enable its accelerated performance. Wedemonstrate the real-world applicability of our method by showcasing itseffectiveness for both in-distribution and out-of-sample data. This workaddresses one of the core challenges of efficient data valuation at scale inmachine learning models.</description><author>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan Kankanhalli</author><pubDate>Mon, 18 Mar 2024 10:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09288v3</guid></item><item><title>CRS-Diff: Controllable Generative Remote Sensing Foundation Model</title><link>http://arxiv.org/abs/2403.11614v1</link><description>The emergence of diffusion models has revolutionized the field of imagegeneration, providing new methods for creating high-quality, high-resolutionimages across various applications. However, the potential of these models forgenerating domain-specific images, particularly remote sensing (RS) images,remains largely untapped. RS images that are notable for their high resolution,extensive coverage, and rich information content, bring new challenges thatgeneral diffusion models may not adequately address. This paper proposesCRS-Diff, a pioneering diffusion modeling framework specifically tailored forgenerating remote sensing imagery, leveraging the inherent advantages ofdiffusion models while integrating advanced control mechanisms to ensure thatthe imagery is not only visually clear but also enriched with geographic andtemporal information. The model integrates global and local control inputs,enabling precise combinations of generation conditions to refine the generationprocess. A comprehensive evaluation of CRS-Diff has demonstrated its superiorcapability to generate RS imagery both in a single condition and multipleconditions compared with previous methods in terms of image quality anddiversity.</description><author>Datao Tang, Xiangyong Cao, Xingsong Hou, Zhongyuan Jiang, Deyu Meng</author><pubDate>Mon, 18 Mar 2024 10:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11614v1</guid></item><item><title>Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning</title><link>http://arxiv.org/abs/2403.06880v2</link><description>Toddlers evolve from free exploration with sparse feedback to exploitingprior experiences for goal-directed learning with denser rewards. Drawinginspiration from this Toddler-Inspired Reward Transition, we set out to explorethe implications of varying reward transitions when incorporated intoReinforcement Learning (RL) tasks. Central to our inquiry is the transitionfrom sparse to potential-based dense rewards, which share optimal strategiesregardless of reward changes. Through various experiments, including those inegocentric navigation and robotic arm manipulation tasks, we found that properreward transitions significantly influence sample efficiency and success rates.Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense(S2D) transition. Beyond these performance metrics, using Cross-DensityVisualizer technique, we observed that transitions, especially the S2D, smooththe policy loss landscape, promoting wide minima that enhance generalization inRL models.</description><author>Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang</author><pubDate>Mon, 18 Mar 2024 10:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06880v2</guid></item><item><title>BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives</title><link>http://arxiv.org/abs/2403.07800v2</link><description>This work addresses the Brain Magnetic Resonance Image Synthesis for TumorSegmentation (BraSyn) challenge, which was hosted as part of the Brain TumorSegmentation (BraTS) challenge in 2023. In this challenge, researchers areinvited to synthesize a missing magnetic resonance image sequence, given otheravailable sequences, to facilitate tumor segmentation pipelines trained oncomplete sets of image sequences. This problem can be tackled using deeplearning within the framework of paired image-to-image translation. In thisstudy, we propose investigating the effectiveness of a commonly used deeplearning framework, such as Pix2Pix, trained under the supervision of differentimage-quality loss functions. Our results indicate that the use of differentloss functions significantly affects the synthesis quality. We systematicallystudy the impact of various loss functions in the multi-sequence MR imagesynthesis setting of the BraSyn challenge. Furthermore, we demonstrate howimage synthesis performance can be optimized by combining different learningobjectives beneficially.</description><author>Ivo M. Baltruschat, Parvaneh Janbakhshi, Matthias Lenga</author><pubDate>Mon, 18 Mar 2024 10:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07800v2</guid></item><item><title>Deep Homography Estimation for Visual Place Recognition</title><link>http://arxiv.org/abs/2402.16086v2</link><description>Visual place recognition (VPR) is a fundamental task for many applicationssuch as robot localization and augmented reality. Recently, the hierarchicalVPR methods have received considerable attention due to the trade-off betweenaccuracy and efficiency. They usually first use global features to retrieve thecandidate images, then verify the spatial consistency of matched local featuresfor re-ranking. However, the latter typically relies on the RANSAC algorithmfor fitting homography, which is time-consuming and non-differentiable. Thismakes existing methods compromise to train the network only in global featureextraction. Here, we propose a transformer-based deep homography estimation(DHE) network that takes the dense feature map extracted by a backbone networkas input and fits homography for fast and learnable geometric verification.Moreover, we design a re-projection error of inliers loss to train the DHEnetwork without additional homography labels, which can also be jointly trainedwith the backbone network to help it extract the features that are moresuitable for local matching. Extensive experiments on benchmark datasets showthat our method can outperform several state-of-the-art methods. And it is morethan one order of magnitude faster than the mainstream hierarchical VPR methodsusing RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.</description><author>Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei Jiang, Chun Yuan</author><pubDate>Mon, 18 Mar 2024 10:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16086v2</guid></item><item><title>Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)</title><link>http://arxiv.org/abs/2403.11603v1</link><description>In intelligent Internet of Things (IoT) systems, edge servers within anetwork exchange information with their neighbors and collect data from sensorsto complete delivered tasks. In this paper, we propose a multiplayermulti-armed bandit model for intelligent IoT systems to facilitate datacollection and incorporate fairness considerations. In our model, we establishan effective communication protocol that helps servers cooperate with theirneighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,enabling servers to collaboratively select sensors to maximize data rates whilemaintaining fairness in their choices. We conduct an analysis of the rewardregret and fairness regret of DC-ULCB, and prove that both regrets havelogarithmic instance-dependent upper bounds. Additionally, through extensivesimulations, we validate that DC-ULCB outperforms existing algorithms inmaximizing reward and ensuring fairness.</description><author>Ziqun Chen, Kechao Cai, Jinbei Zhang, Zhigang Yu</author><pubDate>Mon, 18 Mar 2024 10:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11603v1</guid></item><item><title>Ricci flow-based brain surface covariance descriptors for diagnosing Alzheimer's disease</title><link>http://arxiv.org/abs/2403.06645v2</link><description>Automated feature extraction from MRI brain scans and diagnosis ofAlzheimer's disease are ongoing challenges. With advances in 3D imagingtechnology, 3D data acquisition is becoming more viable and efficient than its2D counterpart. Rather than using feature-based vectors, in this paper, for thefirst time, we suggest a pipeline to extract novel covariance-based descriptorsfrom the cortical surface using the Ricci energy optimization. The covariancedescriptors are components of the nonlinear manifold of symmetricpositive-definite matrices, thus we focus on using the Gaussian radial basisfunction to apply manifold-based classification to the 3D shape problem.Applying this novel signature to the analysis of abnormal cortical brainmorphometry allows for diagnosing Alzheimer's disease. Experimental studiesperformed on about two hundred 3D MRI brain models, gathered from Alzheimer'sDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness ofour descriptors in achieving remarkable classification accuracy.</description><author>Fatemeh Ahmadi, Mohamad Ebrahim Shiri, Behroz Bidabad, Maral Sedaghat, Pooran Memari</author><pubDate>Mon, 18 Mar 2024 10:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06645v2</guid></item><item><title>Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits</title><link>http://arxiv.org/abs/2403.11598v1</link><description>Layout synthesis is mapping a quantum circuit to a quantum processor. SWAPgate insertions are needed for scheduling 2-qubit gates only on connectedphysical qubits. With the ever-increasing number of qubits in NISQ processors,scalable layout synthesis is of utmost importance. With large optimality gapsobserved in heuristic approaches, scalable exact methods are needed. Whilerecent exact and near-optimal approaches scale to moderate circuits, large deepcircuits are still out of scope. In this work, we propose a SAT encoding based on parallel plans that apply 1SWAP and a group of CNOTs at each time step. Using domain-specific information,we maintain optimality in parallel plans while scaling to large and deepcircuits. From our results, we show the scalability of our approach whichsignificantly outperforms leading exact and near-optimal approaches (up to100x). For the first time, we can optimally map several 8, 14, and 16 qubitcircuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While addingoptimal SWAPs, we also report near-optimal depth in our mapped circuits.</description><author>Irfansha Shaik, Jaco van de Pol</author><pubDate>Mon, 18 Mar 2024 10:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11598v1</guid></item><item><title>End-to-end multi-modal product matching in fashion e-commerce</title><link>http://arxiv.org/abs/2403.11593v1</link><description>Product matching, the task of identifying different representations of thesame product for better discoverability, curation, and pricing, is a keycapability for online marketplace and e-commerce companies. We present a robustmulti-modal product matching system in an industry setting, where largedatasets, data distribution shifts and unseen domains pose challenges. Wecompare different approaches and conclude that a relatively straightforwardprojection of pretrained image and text encoders, trained through contrastivelearning, yields state-of-the-art results, while balancing cost andperformance. Our solution outperforms single modality matching systems andlarge pretrained models, such as CLIP. Furthermore we show how ahuman-in-the-loop process can be combined with model-based predictions toachieve near perfect precision in a production system.</description><author>Sándor Tóth, Stephen Wilson, Alexia Tsoukara, Enric Moreu, Anton Masalovich, Lars Roemheld</author><pubDate>Mon, 18 Mar 2024 10:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11593v1</guid></item><item><title>A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs</title><link>http://arxiv.org/abs/2403.11591v1</link><description>We present a physics-informed neural network (PINN) approach for thediscovery of slow invariant manifolds (SIMs), for the most general class offast/slow dynamical systems of ODEs. In contrast to other machine learning (ML)approaches that construct reduced order black box surrogate models using simpleregression, and/or require a priori knowledge of the fast and slow variables,our approach, simultaneously decomposes the vector field into fast and slowcomponents and provides a functional of the underlying SIM in a closed form.The decomposition is achieved by finding a transformation of the statevariables to the fast and slow ones, which enables the derivation of anexplicit, in terms of fast variables, SIM functional. The latter is obtained bysolving a PDE corresponding to the invariance equation within the GeometricSingular Perturbation Theory (GSPT) using a single-layer feedforward neuralnetwork with symbolic differentiation. The performance of the proposedphysics-informed ML framework is assessed via three benchmark problems: theMichaelis-Menten, the target mediated drug disposition (TMDD) reaction modeland a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide acomparison with other GPST methods, namely the quasi steady state approximation(QSSA), the partial equilibrium approximation (PEA) and CSP with one and twoiterations. We show that the proposed PINN scheme provides SIM approximations,of equivalent or even higher accuracy, than those provided by QSSA, PEA andCSP, especially close to the boundaries of the underlying SIMs.</description><author>Dimitrios G. Patsatzis, Lucia Russo, Constantinos Siettos</author><pubDate>Mon, 18 Mar 2024 10:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11591v1</guid></item><item><title>HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction</title><link>http://arxiv.org/abs/2403.11590v1</link><description>This article presents our results for the sixth Affective Behavior Analysisin-the-wild (ABAW) competition. To improve the trustworthiness of facialanalysis, we study the possibility of using pre-trained deep models thatextract reliable emotional features without the need to fine-tune the neuralnetworks for a downstream task. In particular, we introduce several lightweightmodels based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFNarchitectures trained in multi-task scenarios to recognize facial expressions,valence, and arousal on static photos. These neural networks extractframe-level features fed into a simple classifier, e.g., linear feed-forwardneural network, to predict emotion intensity, compound expressions, actionunits, facial expressions, and valence/arousal. Experimental results for fivetasks from the sixth ABAW challenge demonstrate that our approach lets ussignificantly improve quality metrics on validation sets compared to existingnon-ensemble techniques.</description><author>Andrey V. Savchenko</author><pubDate>Mon, 18 Mar 2024 10:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11590v1</guid></item><item><title>EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition</title><link>http://arxiv.org/abs/2310.16640v2</link><description>Facial Expression Recognition (FER) is a crucial task in affective computing,but its conventional focus on the seven basic emotions limits its applicabilityto the complex and expanding emotional spectrum. To address the issue of newand unseen emotions present in dynamic in-the-wild FER, we propose a novelvision-language model that utilises sample-level text descriptions (i.e.captions of the context, expressions or emotional cues) as natural languagesupervision, aiming to enhance the learning of rich latent representations, forzero-shot classification. To test this, we evaluate using zero-shotclassification of the model trained on sample-level descriptions on fourpopular dynamic FER datasets. Our findings show that this approach yieldssignificant improvements when compared to baseline methods. Specifically, forzero-shot video FER, we outperform CLIP by over 10\% in terms of WeightedAverage Recall and 5\% in terms of Unweighted Average Recall on severaldatasets. Furthermore, we evaluate the representations obtained from thenetwork trained using sample-level descriptions on the downstream task ofmental health symptom estimation, achieving performance comparable or superiorto state-of-the-art methods and strong agreement with human experts. Namely, weachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophreniasymptom severity estimation, which is comparable to human experts' agreement.The code is publicly available at: https://github.com/NickyFot/EmoCLIP.</description><author>Niki Maria Foteinopoulou, Ioannis Patras</author><pubDate>Mon, 18 Mar 2024 10:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16640v2</guid></item><item><title>Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World</title><link>http://arxiv.org/abs/2310.10207v5</link><description>We introduce Bongard-OpenWorld, a new benchmark for evaluating real-worldfew-shot reasoning for machine vision. It originates from the classical BongardProblems (BPs): Given two sets of images (positive and negative), the modelneeds to identify the set that query images belong to by inducing the visualconcepts, which is exclusively depicted by images from the positive set. Ourbenchmark inherits the few-shot concept induction of the original BPs whileadding the two novel layers of challenge: 1) open-world free-form concepts, asthe visual concepts in Bongard-OpenWorld are unique compositions of terms froman open vocabulary, ranging from object categories to abstract visualattributes and commonsense factual knowledge; 2) real-world images, as opposedto the synthetic diagrams used by many counterparts. In our exploration,Bongard-OpenWorld already imposes a significant challenge to current few-shotreasoning algorithms. We further investigate to which extent the recentlyintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) cansolve our task, by directly probing VLMs, and combining VLMs and LLMs in aninteractive reasoning scheme. We even conceived a neuro-symbolic reasoningapproach that reconciles LLMs &amp; VLMs with logical reasoning to emulate thehuman problem-solving process for Bongard Problems. However, none of theseapproaches manage to close the human-machine gap, as the best learner achieves64% accuracy while human participants easily reach 91%. We hopeBongard-OpenWorld can help us better understand the limitations of currentvisual intelligence and facilitate future research on visual agents withstronger few-shot visual reasoning capabilities.</description><author>Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun Zhu, Yizhou Wang</author><pubDate>Mon, 18 Mar 2024 10:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10207v5</guid></item><item><title>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</title><link>http://arxiv.org/abs/2403.11589v1</link><description>Reconstructing photo-realistic drivable human avatars from multi-view imagesequences has been a popular and challenging topic in the field of computervision and graphics. While existing NeRF-based methods can achieve high-qualitynovel view rendering of human models, both training and inference processes aretime-consuming. Recent approaches have utilized 3D Gaussians to represent thehuman body, enabling faster training and rendering. However, they undermine theimportance of the mesh guidance and directly predict Gaussians in 3D space withcoarse mesh guidance. This hinders the learning procedure of the Gaussians andtends to produce blurry textures. Therefore, we propose UV Gaussians, whichmodels the 3D human body by jointly learning mesh deformations and 2D UV-spaceGaussian textures. We utilize the embedding of UV map to learn Gaussiantextures in 2D space, leveraging the capabilities of powerful 2D networks toextract features. Additionally, through an independent Mesh network, weoptimize pose-dependent geometric deformations, thereby guiding Gaussianrendering and significantly enhancing rendering quality. We collect and processa new dataset of human motion, which includes multi-view images, scannedmodels, parametric model registration, and corresponding texture maps.Experimental results demonstrate that our method achieves state-of-the-artsynthesis of novel view and novel pose. The code and data will be madeavailable on the homepage https://alex-jyj.github.io/UV-Gaussians/ once thepaper is accepted.</description><author>Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</author><pubDate>Mon, 18 Mar 2024 10:03:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11589v1</guid></item><item><title>Learning Triangular Distribution in Visual World</title><link>http://arxiv.org/abs/2311.18605v3</link><description>Convolution neural network is successful in pervasive vision tasks, includinglabel distribution learning, which usually takes the form of learning aninjection from the non-linear visual features to the well-defined labels.However, how the discrepancy between features is mapped to the labeldiscrepancy is ambient, and its correctness is not guaranteed.To address theseproblems, we study the mathematical connection between feature and its label,presenting a general and simple framework for label distribution learning. Wepropose a so-called Triangular Distribution Transform (TDT) to build aninjective function between feature and label, guaranteeing that any symmetricfeature discrepancy linearly reflects the difference between labels. Theproposed TDT can be used as a plug-in in mainstream backbone networks toaddress different label distribution learning tasks. Experiments on Facial AgeRecognition, Illumination Chromaticity Estimation, and Aesthetics assessmentshow that TDT achieves on-par or better results than the prior arts.</description><author>Ping Chen, Xingpeng Zhang, Chengtao Zhou, Dichao Fan, Peng Tu, Le Zhang, Yanlin Qian</author><pubDate>Mon, 18 Mar 2024 10:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18605v3</guid></item><item><title>Learning Exhaustive Correlation for Spectral Super-Resolution: Where Spatial-Spectral Attention Meets Linear Dependence</title><link>http://arxiv.org/abs/2312.12833v2</link><description>Spectral super-resolution that aims to recover hyperspectral image (HSI) fromeasily obtainable RGB image has drawn increasing interest in the field ofcomputational photography. The crucial aspect of spectral super-resolution liesin exploiting the correlation within HSIs. However, two types of bottlenecks inexisting Transformers limit performance improvement and practical applications.First, existing Transformers often separately emphasize either spatial-wise orspectral-wise correlation, disrupting the 3D features of HSI and hindering theexploitation of unified spatial-spectral correlation. Second, existingself-attention mechanism always establishes full-rank correlation matrix bylearning the correlation between pairs of tokens, leading to its inability todescribe linear dependence widely existing in HSI among multiple tokens. Toaddress these issues, we propose a novel Exhaustive Correlation Transformer(ECT) for spectral super-resolution. First, we propose a Spectral-wiseDiscontinuous 3D (SD3D) splitting strategy, which models unifiedspatial-spectral correlation by integrating spatial-wise continuous splittingstrategy and spectral-wise discontinuous splitting strategy. Second, we proposea Dynamic Low-Rank Mapping (DLRM) model, which captures linear dependence amongmultiple tokens through a dynamically calculated low-rank dependence map. Byintegrating unified spatial-spectral attention and linear dependence, our ECTcan model exhaustive correlation within HSI. The experimental results on bothsimulated and real data indicate that our method achieves state-of-the-artperformance. Codes and pretrained models will be available later.</description><author>Hongyuan Wang, Lizhi Wang, Jiang Xu, Chang Chen, Xue Hu, Fenglong Song, Youliang Yan</author><pubDate>Mon, 18 Mar 2024 10:02:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12833v2</guid></item><item><title>CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark</title><link>http://arxiv.org/abs/2401.11944v2</link><description>As the capabilities of large multimodal models (LMMs) continue to advance,evaluating the performance of LMMs emerges as an increasing need. Additionally,there is an even larger gap in evaluating the advanced knowledge and reasoningabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,a new Chinese Massive Multi-discipline Multimodal Understanding benchmarkdesigned to evaluate LMMs on tasks demanding college-level subject knowledgeand deliberate reasoning in a Chinese context. CMMMU is inspired by andstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes 12k manually collected multimodal questions from collegeexams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design,Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp;Engineering, like its companion, MMMU. These questions span 30 subjects andcomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,tables, music sheets, and chemical structures. CMMMU focuses on complex perception and reasoning with domain-specificknowledge in the Chinese context. We evaluate 11 open-source LLMs and oneproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,indicating a large space for improvement. CMMMU will boost the community tobuild the next-generation LMMs towards expert artificial intelligence andpromote the democratization of LMMs by providing diverse language contexts.</description><author>Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu</author><pubDate>Mon, 18 Mar 2024 10:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11944v2</guid></item><item><title>Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction</title><link>http://arxiv.org/abs/2401.15603v2</link><description>In recent years, spectral graph neural networks, characterized by polynomialfilters, have garnered increasing attention and have achieved remarkableperformance in tasks such as node classification. These models typically assumethat eigenvalues for the normalized Laplacian matrix are distinct from eachother, thus expecting a polynomial filter to have a high fitting ability.However, this paper empirically observes that normalized Laplacian matricesfrequently possess repeated eigenvalues. Moreover, we theoretically establishthat the number of distinguishable eigenvalues plays a pivotal role indetermining the expressive power of spectral graph neural networks. In light ofthis observation, we propose an eigenvalue correction strategy that can freepolynomial filters from the constraints of repeated eigenvalue inputs.Concretely, the proposed eigenvalue correction strategy enhances the uniformdistribution of eigenvalues, thus mitigating repeated eigenvalues, andimproving the fitting capacity and expressive power of polynomial filters.Extensive experimental results on both synthetic and real-world datasetsdemonstrate the superiority of our method.</description><author>Kangkang Lu, Yanhua Yu, Hao Fei, Xuan Li, Zixuan Yang, Zirui Guo, Meiyu Liang, Mengran Yin, Tat-Seng Chua</author><pubDate>Mon, 18 Mar 2024 10:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15603v2</guid></item><item><title>DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction</title><link>http://arxiv.org/abs/2403.11586v1</link><description>This paper explores the problem of reconstructing temporally consistentsurfaces from a 3D point cloud sequence without correspondence. To address thischallenging task, we propose DynoSurf, an unsupervised learning frameworkintegrating a template surface representation with a learnable deformationfield. Specifically, we design a coarse-to-fine strategy for learning thetemplate surface based on the deformable tetrahedron representation.Furthermore, we propose a learnable deformation representation based on thelearnable control points and blending weights, which can deform the templatesurface non-rigidly while maintaining the consistency of the local shape.Experimental results demonstrate the significant superiority of DynoSurf overcurrent state-of-the-art approaches, showcasing its potential as a powerfultool for dynamic mesh reconstruction. The code is publicly available athttps://github.com/yaoyx689/DynoSurf.</description><author>Yuxin Yao, Siyu Ren, Junhui Hou, Zhi Deng, Juyong Zhang, Wenping Wang</author><pubDate>Mon, 18 Mar 2024 09:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11586v1</guid></item><item><title>Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</title><link>http://arxiv.org/abs/2403.11585v1</link><description>In the ever-evolving landscape of machine learning, seamless translation ofnatural language descriptions into executable code remains a formidablechallenge. This paper introduces Linguacodus, an innovative framework designedto tackle this challenge by deploying a dynamic pipeline that iterativelytransforms natural language task descriptions into code through high-leveldata-shaping instructions. The core of Linguacodus is a fine-tuned largelanguage model (LLM), empowered to evaluate diverse solutions for variousproblems and select the most fitting one for a given task. This paper detailsthe fine-tuning process, and sheds light on how natural language descriptionscan be translated into functional code. Linguacodus represents a substantialleap towards automated code generation, effectively bridging the gap betweentask descriptions and executable code. It holds great promise for advancingmachine learning applications across diverse domains. Additionally, we proposean algorithm capable of transforming a natural description of an ML task intocode with minimal human interaction. In extensive experiments on a vast machinelearning code dataset originating from Kaggle, we showcase the effectiveness ofLinguacodus. The investigations highlight its potential applications acrossdiverse domains, emphasizing its impact on applied machine learning in variousscientific fields.</description><author>Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin</author><pubDate>Mon, 18 Mar 2024 09:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11585v1</guid></item><item><title>OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation</title><link>http://arxiv.org/abs/2403.11582v1</link><description>Multi-target domain adaptation (MTDA) for semantic segmentation poses asignificant challenge, as it involves multiple target domains with varyingdistributions. The goal of MTDA is to minimize the domain discrepancies among asingle source and multi-target domains, aiming to train a single model thatexcels across all target domains. Previous MTDA approaches typically employmultiple teacher architectures, where each teacher specializes in one targetdomain to simplify the task. However, these architectures hinder the studentmodel from fully assimilating comprehensive knowledge from all target-specificteachers and escalate training costs with increasing target domains. In thispaper, we propose an ouroboric domain bridging (OurDB) framework, offering anefficient solution to the MTDA problem using a single teacher architecture.This framework dynamically cycles through multiple target domains, aligningeach domain individually to restrain the biased alignment problem, and utilizesFisher information to minimize the forgetting of knowledge from previous targetdomains. We also propose a context-guided class-wise mixup (CGMix) thatleverages contextual information tailored to diverse target contexts in MTDA.Experimental evaluations conducted on four urban driving datasets (i.e., GTA5,Cityscapes, IDD, and Mapillary) demonstrate the superiority of our method overexisting state-of-the-art approaches.</description><author>Seungbeom Woo, Geonwoo Baek, Taehoon Kim, Jaemin Na, Joong-won Hwang, Wonjun Hwang</author><pubDate>Mon, 18 Mar 2024 09:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11582v1</guid></item><item><title>Is it Really Negative? Evaluating Natural Language Video Localization Performance on Multiple Reliable Videos Pool</title><link>http://arxiv.org/abs/2309.16701v2</link><description>With the explosion of multimedia content in recent years, Video Corpus MomentRetrieval (VCMR), which aims to detect a video moment that matches a givennatural language query from multiple videos, has become a critical problem.However, existing VCMR studies have a significant limitation since they haveregarded all videos not paired with a specific query as negative, neglectingthe possibility of including false negatives when constructing the negativevideo set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)task that aims to localize video frames within a massive video set, mitigatingthe possibility of falsely distinguishing positive and negative videos. Forthis task, we suggest an automatic dataset construction framework by employingtextual and visual semantic matching evaluation methods on the existing videomoment search datasets and introduce three MVMR datasets. To solve MVMR task,we further propose a strong method, CroCs, which employs cross-directionalcontrastive learning that selectively identifies the reliable and informativenegatives, enhancing the robustness of a model on MVMR task. Experimentalresults on the introduced datasets reveal that existing video moment searchmodels are easily distracted by negative video frames, whereas our model showssignificant performance.</description><author>Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung</author><pubDate>Mon, 18 Mar 2024 09:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16701v2</guid></item><item><title>Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC Middleware: Applications in Quantum Simulations</title><link>http://arxiv.org/abs/2403.05828v2</link><description>Achieving high-performance computation on quantum systems presents aformidable challenge that necessitates bridging the capabilities betweenquantum hardware and classical computing resources. This study introduces aninnovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,which integrates cutting-edge quantum software framework works withhigh-performance classical computing resources to address challenges in quantumsimulation for materials and condensed matter physics. At the heart of thisarchitecture is the seamless integration of VQE algorithms running on QPUs forefficient quantum state preparation, Tensor Network states, and QCNNs forclassifying quantum states on classical hardware. For benchmarking quantum simulators, the QCQ architecture utilizes thecuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane'sLightning plugin, demonstrating up to tenfold increases in computational speedfor complex phase transition classification tasks compared to traditionalCPU-based methods. This significant acceleration enables models such as thetransverse field Ising and XXZ systems to accurately predict phase transitionswith a 99.5% accuracy. The architecture's ability to distribute computationbetween QPUs and classical resources addresses critical bottlenecks inQuantum-HPC, paving the way for scalable quantum simulation. The QCQ framework embodies a synergistic combination of quantum algorithms,machine learning, and Quantum-HPC capabilities, enhancing its potential toprovide transformative insights into the behavior of quantum systems acrossdifferent scales. As quantum hardware continues to improve, this hybriddistribution-aware framework will play a crucial role in realizing the fullpotential of quantum computing by seamlessly integrating distributed quantumresources with the state-of-the-art classical computing infrastructure.</description><author>Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu</author><pubDate>Mon, 18 Mar 2024 09:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05828v2</guid></item><item><title>3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration</title><link>http://arxiv.org/abs/2403.11577v1</link><description>Reliable multimodal sensor fusion algorithms re- quire accuratespatiotemporal calibration. Recently, targetless calibration techniques basedon implicit neural representations have proven to provide precise and robustresults. Nevertheless, such methods are inherently slow to train given the highcompu- tational overhead caused by the large number of sampled points requiredfor volume rendering. With the recent introduction of 3D Gaussian Splatting asa faster alternative to implicit representation methods, we propose to leveragethis new ren- dering approach to achieve faster multi-sensor calibration. Weintroduce 3DGS-Calib, a new calibration method that relies on the speed andrendering accuracy of 3D Gaussian Splatting to achieve multimodalspatiotemporal calibration that is accurate, robust, and with a substantialspeed-up compared to methods relying on implicit neural representations. Wedemonstrate the superiority of our proposal with experimental results onsequences from KITTI-360, a widely used driving dataset.</description><author>Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux</author><pubDate>Mon, 18 Mar 2024 09:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11577v1</guid></item><item><title>MISS: Memory-efficient Instance Segmentation Framework By Visual Inductive Priors Flow Propagation</title><link>http://arxiv.org/abs/2403.11576v1</link><description>Instance segmentation, a cornerstone task in computer vision, haswide-ranging applications in diverse industries. The advent of deep learningand artificial intelligence has underscored the criticality of trainingeffective models, particularly in data-scarce scenarios - a concern thatresonates in both academic and industrial circles. A significant impediment inthis domain is the resource-intensive nature of procuring high-quality,annotated data for instance segmentation, a hurdle that amplifies the challengeof developing robust models under resource constraints. In this context, thestrategic integration of a visual prior into the training dataset emerges as apotential solution to enhance congruity with the testing data distribution,consequently reducing the dependency on computational resources and the needfor highly complex models. However, effectively embedding a visual prior intothe learning process remains a complex endeavor. Addressing this challenge, weintroduce the MISS (Memory-efficient Instance Segmentation System) framework.MISS leverages visual inductive prior flow propagation, integrating intrinsicprior knowledge from the Synergy-basketball dataset at various stages: datapreprocessing, augmentation, training, and inference. Our empirical evaluationsunderscore the efficacy of MISS, demonstrating commendable performance inscenarios characterized by limited data availability and memory constraints.</description><author>Chih-Chung Hsu, Chia-Ming Lee</author><pubDate>Mon, 18 Mar 2024 09:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11576v1</guid></item><item><title>Exposure Bracketing is All You Need for Unifying Image Restoration and Enhancement Tasks</title><link>http://arxiv.org/abs/2401.00766v2</link><description>It is highly desired but challenging to acquire high-quality photos withclear content in low-light environments. Although multi-image processingmethods (using burst, dual-exposure, or multi-exposure images) have madesignificant progress in addressing this issue, they typically focus on specificrestoration or enhancement problems, being insufficient in exploitingmulti-image. Motivated by that multi-exposure images are complementary indenoising, deblurring, high dynamic range imaging, and super-resolution, wepropose to utilize exposure bracketing photography to unify restoration andenhancement tasks in this work. Due to the difficulty in collecting real-worldpairs, we suggest a solution that first pre-trains the model with syntheticpaired data and then adapts it to real-world unlabeled images. In particular, atemporally modulated recurrent network (TMRNet) and self-supervised adaptationmethod are proposed. Moreover, we construct a data simulation pipeline tosynthesize pairs and collect real-world images from 200 nighttime scenarios.Experiments on both datasets show that our method performs favorably againstthe state-of-the-art multi-image processing ones. The dataset, code, andpre-trained models are available at https://github.com/cszhilu1998/BracketIRE.</description><author>Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo</author><pubDate>Mon, 18 Mar 2024 09:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00766v2</guid></item><item><title>Offline Multitask Representation Learning for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.11574v1</link><description>We study offline multitask representation learning in reinforcement learning(RL), where a learner is provided with an offline dataset from different tasksthat share a common representation and is asked to learn the sharedrepresentation. We theoretically investigate offline multitask low-rank RL, andpropose a new algorithm called MORL for offline multitask representationlearning. Furthermore, we examine downstream RL in reward-free, offline andonline scenarios, where a new task is introduced to the agent that shares thesame representation as the upstream offline tasks. Our theoretical resultsdemonstrate the benefits of using the learned representation from the upstreamoffline task instead of directly learning the representation of the low-rankmodel.</description><author>Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup</author><pubDate>Mon, 18 Mar 2024 09:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11574v1</guid></item><item><title>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</title><link>http://arxiv.org/abs/2403.11573v1</link><description>Typical LiDAR-based 3D object detection models are trained in a supervisedmanner with real-world data collection, which is often imbalanced over classes(or long-tailed). To deal with it, augmenting minority-class examples bysampling ground truth (GT) LiDAR points from a database and pasting them into ascene of interest is often used, but challenges still remain: inflexibility inlocating GT samples and limited sample diversity. In this work, we propose toleverage pseudo-LiDAR point clouds generated (at a low cost) from videoscapturing a surround view of miniatures or real-world objects of minor classes.Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists ofthree main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3Dview synthesis model, (ii) object-level domain alignment with LiDAR intensityestimation and (iii) a hybrid context-aware placement method from ground andmap information. We demonstrate the superiority and generality of our methodthrough performance improvements in extensive experiments conducted on threepopular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for thedatasets with large domain gaps captured by different LiDAR configurations. Ourcode and data will be publicly available upon publication.</description><author>Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</author><pubDate>Mon, 18 Mar 2024 09:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11573v1</guid></item><item><title>Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale</title><link>http://arxiv.org/abs/2403.08293v2</link><description>A syntactic language model (SLM) incrementally generates a sentence with itssyntactic tree in a left-to-right manner. We present Generative PretrainedStructured Transformers (GPST), an unsupervised SLM at scale capable of beingpre-trained from scratch on raw texts with high parallelism. GPST circumventsthe limitations of previous SLMs such as relying on gold trees and sequentialtraining. It consists of two components, a usual SLM supervised by auni-directional language modeling loss, and an additional composition model,which induces syntactic parse trees and computes constituent representations,supervised by a bi-directional language modeling loss. We propose arepresentation surrogate to enable joint parallel training of the two models ina hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billiontokens, and demonstrate the superiority of GPST over GPT-2 with a comparablesize in numerous tasks covering both language understanding and languagegeneration. Meanwhile, GPST also significantly outperforms existingunsupervised SLMs on left-to-right grammar induction, while holding asubstantial acceleration on training.</description><author>Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu</author><pubDate>Mon, 18 Mar 2024 09:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08293v2</guid></item><item><title>Enhancing the Reliability of Segment Anything Model for Auto-Prompting Medical Image Segmentation with Uncertainty Rectification</title><link>http://arxiv.org/abs/2311.10529v3</link><description>The Segment Anything Model (SAM) has recently emerged as a groundbreakingfoundation model for prompt-driven image segmentation tasks. However, both theoriginal SAM and its medical variants require slice-by-slice manual promptingof target structures, which directly increase the burden for applications.Despite attempts of auto-prompting to turn SAM into a fully automatic manner,it still exhibits subpar performance and lacks of reliability especially in thefield of medical imaging. In this paper, we propose UR-SAM, an uncertaintyrectified SAM framework to enhance the reliability for auto-prompting medicalimage segmentation. Building upon a localization framework for automatic promptgeneration, our method incorporates a prompt augmentation module to obtain aseries of input prompts for SAM for uncertainty estimation and anuncertainty-based rectification module to further utilize the distribution ofestimated uncertainty to improve the segmentation performance. Extensiveexperiments on two public 3D medical datasets covering the segmentation of 35organs demonstrate that without supplementary training or fine-tuning, ourmethod further improves the segmentation performance with up to 10.7 % and 13.8% in dice similarity coefficient, demonstrating efficiency and broadcapabilities for medical image segmentation without manual prompting.</description><author>Yichi Zhang, Shiyao Hu, Sijie Ren, Chen Jiang, Yuan Cheng, Yuan Qi</author><pubDate>Mon, 18 Mar 2024 09:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10529v3</guid></item><item><title>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities</title><link>http://arxiv.org/abs/2401.14405v2</link><description>We propose to improve transformers of a specific modality with irrelevantdata from other modalities, e.g., improve an ImageNet model with audio or pointcloud datasets. We would like to highlight that the data samples of the targetmodality are irrelevant to the other modalities, which distinguishes our methodfrom other works utilizing paired (e.g., CLIP) or interleaved data of differentmodalities. We propose a methodology named Multimodal Pathway - given a targetmodality and a transformer designed for it, we use an auxiliary transformertrained with data of another modality and construct pathways to connectcomponents of the two models so that data of the target modality can beprocessed by both models. In this way, we utilize the universalsequence-to-sequence modeling abilities of transformers obtained from twomodalities. As a concrete implementation, we use a modality-specific tokenizerand task-specific head as usual but utilize the transformer blocks of theauxiliary model via a proposed method named Cross-Modal Re-parameterization,which exploits the auxiliary weights without any inference costs. On the image,point cloud, video, and audio recognition tasks, we observe significant andconsistent performance improvements with irrelevant data from other modalities.The code and models are available at https://github.com/AILab-CVC/M2PT.</description><author>Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue</author><pubDate>Mon, 18 Mar 2024 09:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14405v2</guid></item><item><title>Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes</title><link>http://arxiv.org/abs/2403.11572v1</link><description>Instance segmentation is a fundamental task in computer vision with broadapplications across various industries. In recent years, with the proliferationof deep learning and artificial intelligence applications, how to traineffective models with limited data has become a pressing issue for bothacademia and industry. In the Visual Inductive Priors challenge (VIPriors2023),participants must train a model capable of precisely locating individuals on abasketball court, all while working with limited data and without the use oftransfer learning or pre-trained models. We propose Memory effIciency inStanceSegmentation framework based on visual inductive prior flow propagation thateffectively incorporates inherent prior information from the dataset into boththe data preprocessing and data augmentation stages, as well as the inferencephase. Our team (ACVLAB) experiments demonstrate that our model achievespromising performance (0.509 AP@0.50:0.95) even under limited data and memoryconstraints.</description><author>Chih-Chung Hsu, Chia-Ming Lee, Ming-Shyen Wu</author><pubDate>Mon, 18 Mar 2024 09:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11572v1</guid></item><item><title>The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing</title><link>http://arxiv.org/abs/2309.16883v4</link><description>Real-life applications of deep neural networks are hindered by their unsteadypredictions when faced with noisy inputs and adversarial attacks. The certifiedradius in this context is a crucial indicator of the robustness of models.However how to design an efficient classifier with an associated certifiedradius? Randomized smoothing provides a promising framework by relying on noiseinjection into the inputs to obtain a smoothed and robust classifier. In thispaper, we first show that the variance introduced by the Monte-Carlo samplingin the randomized smoothing procedure estimate closely interacts with two otherimportant properties of the classifier, \textit{i.e.} its Lipschitz constantand margin. More precisely, our work emphasizes the dual impact of theLipschitz constant of the base classifier, on both the smoothed classifier andthe empirical variance. To increase the certified robust radius, we introduce adifferent way to convert logits to probability vectors for the base classifierto leverage the variance-margin trade-off. We leverage the use of Bernstein'sconcentration inequality along with enhanced Lipschitz bounds for randomizedsmoothing. Experimental results show a significant improvement in certifiedaccuracy compared to current state-of-the-art methods. Our novel certificationprocedure allows us to use pre-trained models with randomized smoothing,effectively improving the current certification radius in a zero-shot manner.</description><author>Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen</author><pubDate>Mon, 18 Mar 2024 09:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16883v4</guid></item><item><title>LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge</title><link>http://arxiv.org/abs/2403.11570v1</link><description>Large text-to-image models have achieved astonishing performance insynthesizing diverse and high-quality images guided by texts. Withdetail-oriented conditioning control, even finer-grained spatial control can beachieved. However, some generated images still appear unreasonable, even withplentiful object features and a harmonious style. In this paper, we delve intothe underlying causes and find that deep-level logical information, serving ascommon-sense knowledge, plays a significant role in understanding andprocessing images. Nonetheless, almost all models have neglected the importanceof logical relations in images, resulting in poor performance in this aspect.Following this observation, we propose LogicalDefender, which combines imageswith the logical knowledge already summarized by humans in text. Thisencourages models to learn logical knowledge faster and better, andconcurrently, extracts the widely applicable logical knowledge from both imagesand human knowledge. Experiments show that our model has achieved betterlogical performance, and the extracted logical knowledge can be effectivelyapplied to other scenarios.</description><author>Yuhe Liu, Mengxue Kang, Zengchang Qin, Xiangxiang Chu</author><pubDate>Mon, 18 Mar 2024 09:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11570v1</guid></item><item><title>EffiVED:Efficient Video Editing via Text-instruction Diffusion Models</title><link>http://arxiv.org/abs/2403.11568v1</link><description>Large-scale text-to-video models have shown remarkable abilities, but theirdirect application in video editing remains challenging due to limitedavailable datasets. Current video editing methods commonly require per-videofine-tuning of diffusion models or specific inversion optimization to ensurehigh-fidelity edits. In this paper, we introduce EffiVED, an efficientdiffusion-based model that directly supports instruction-guided video editing.To achieve this, we present two efficient workflows to gather video editingpairs, utilizing augmentation and fundamental vision-language techniques. Theseworkflows transform vast image editing datasets and open-world videos into ahigh-quality dataset for training EffiVED. Experimental results reveal thatEffiVED not only generates high-quality editing videos but also executesrapidly. Finally, we demonstrate that our data collection method significantlyimproves editing performance and can potentially tackle the scarcity of videoediting data. The datasets will be made publicly available upon publication.</description><author>Zhenghao Zhang, Zuozhuo Dai, Long Qin, Weizhi Wang</author><pubDate>Mon, 18 Mar 2024 09:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11568v1</guid></item><item><title>ProMISe: Promptable Medical Image Segmentation using SAM</title><link>http://arxiv.org/abs/2403.04164v2</link><description>With the proposal of the Segment Anything Model (SAM), fine-tuning SAM formedical image segmentation (MIS) has become popular. However, due to the largesize of the SAM model and the significant domain gap between natural andmedical images, fine-tuning-based strategies are costly with potential risk ofinstability, feature damage and catastrophic forgetting. Furthermore, somemethods of transferring SAM to a domain-specific MIS through fine-tuningstrategies disable the model's prompting capability, severely limiting itsutilization scenarios. In this paper, we propose an Auto-Prompting Module(APM), which provides SAM-based foundation model with Euclidean adaptiveprompts in the target domain. Our experiments demonstrate that such adaptiveprompts significantly improve SAM's non-fine-tuned performance in MIS. Inaddition, we propose a novel non-invasive method called Incremental PatternShifting (IPS) to adapt SAM to specific medical domains. Experimental resultsshow that the IPS enables SAM to achieve state-of-the-art or competitiveperformance in MIS without the need for fine-tuning. By coupling these twomethods, we propose ProMISe, an end-to-end non-fine-tuned framework forPromptable Medical Image Segmentation. Our experiments demonstrate that bothusing our methods individually or in combination achieves satisfactoryperformance in low-cost pattern shifting, with all of SAM's parameters frozen.</description><author>Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong Su, S. Kevin Zhou</author><pubDate>Mon, 18 Mar 2024 09:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04164v2</guid></item><item><title>Open Brain AI. Automatic Language Assessment</title><link>http://arxiv.org/abs/2306.06693v2</link><description>Language assessment plays a crucial role in diagnosing and treatingindividuals with speech, language, and communication disorders caused byneurogenic conditions, whether developmental or acquired. However, currentassessment methods are manual, laborious, and time-consuming to administer andscore, causing additional patient stress. To address these challenges, wedeveloped Open Brain AI (https://openbrainai.com). This computational platformharnesses innovative AI techniques, namely machine learning, natural languageprocessing, large language models, and automatic speech-to-text transcription,to automatically analyze multilingual spoken and written speech productions.This paper discusses the development of Open Brain AI, the AI languageprocessing modules, and the linguistic measurements of discoursemacro-structure and micro-structure. The fast and automatic analysis oflanguage alleviates the burden on clinicians, enabling them to streamline theirworkflow and allocate more time and resources to direct patient care. OpenBrain AI is freely accessible, empowering clinicians to conduct critical dataanalyses and give more attention and resources to other critical aspects oftherapy and treatment.</description><author>Charalambos Themistocleous</author><pubDate>Mon, 18 Mar 2024 09:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06693v2</guid></item><item><title>UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition</title><link>http://arxiv.org/abs/2311.15599v2</link><description>Large-kernel convolutional neural networks (ConvNets) have recently receivedextensive research attention, but two unresolved and critical issues demandfurther investigation. 1) The architectures of existing large-kernel ConvNetslargely follow the design principles of conventional ConvNets or transformers,while the architectural design for large-kernel ConvNets remainsunder-addressed. 2) As transformers have dominated multiple modalities, itremains to be investigated whether ConvNets also have a strong universalperception ability in domains beyond vision. In this paper, we contribute fromtwo aspects. 1) We propose four architectural guidelines for designinglarge-kernel ConvNets, the core of which is to exploit the essentialcharacteristics of large kernels that distinguish them from small kernels -they can see wide without going deep. Following such guidelines, our proposedlarge-kernel ConvNet shows leading performance in image recognition (ImageNetaccuracy of 88.0%, ADE20K mIoU of 55.6%, and COCO box AP of 56.4%),demonstrating better performance and higher speed than the recent powerfulcompetitors. 2) We discover large kernels are the key to unlocking theexceptional performance of ConvNets in domains where they were originally notproficient. With certain modality-related preprocessing approaches, theproposed model achieves state-of-the-art performance on time-series forecastingand audio recognition tasks even without modality-specific customization to thearchitecture. All the code and models are publicly available on GitHub andHuggingface.</description><author>Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, Ying Shan</author><pubDate>Mon, 18 Mar 2024 09:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15599v2</guid></item><item><title>Object-aware Inversion and Reassembly for Image Editing</title><link>http://arxiv.org/abs/2310.12149v2</link><description>By comparing the original and target prompts, we can obtain numerous editingpairs, each comprising an object and its corresponding editing target. To alloweditability while maintaining fidelity to the input image, existing editingmethods typically involve a fixed number of inversion steps that project thewhole input image to its noisier latent representation, followed by a denoisingprocess guided by the target prompt. However, we find that the optimal numberof inversion steps for achieving ideal editing results varies significantlyamong different editing pairs, owing to varying editing difficulties.Therefore, the current literature, which relies on a fixed number of inversionsteps, produces sub-optimal generation quality, especially when handlingmultiple editing pairs in a natural image. To this end, we propose a new imageediting paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enableobject-level fine-grained editing. Specifically, we design a new search metric,which determines the optimal inversion steps for each editing pair, by jointlyconsidering the editability of the target and the fidelity of the non-editingregion. We use our search metric to find the optimal inversion step for eachediting pair when editing an image. We then edit these editing pairs separatelyto avoid concept mismatch. Subsequently, we propose an additional reassemblystep to seamlessly integrate the respective editing results and the non-editingregion to obtain the final edited image. To systematically evaluate theeffectiveness of our method, we collect two datasets called OIRBench forbenchmarking single- and multi-object editing, respectively. Experimentsdemonstrate that our method achieves superior performance in editing objectshapes, colors, materials, categories, etc., especially in multi-object editingscenarios.</description><author>Zhen Yang, Ganggui Ding, Wen Wang, Hao Chen, Bohan Zhuang, Chunhua Shen</author><pubDate>Mon, 18 Mar 2024 09:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12149v2</guid></item><item><title>Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization</title><link>http://arxiv.org/abs/2403.11565v1</link><description>In this paper, we concentrate on decentralized optimization problems withnonconvex and nonsmooth objective functions, especially on the decentralizedtraining of nonsmooth neural networks. We introduce a unified framework, namedDSM, to analyze the global convergence of decentralized stochastic subgradientmethods. We prove the global convergence of our proposed framework under mildconditions, by establishing that the generated sequence asymptoticallyapproximates the trajectories of its associated differential inclusion.Furthermore, we establish that our proposed framework encompasses a wide rangeof existing efficient decentralized subgradient methods, includingdecentralized stochastic subgradient descent (DSGD), DSGD withgradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). Inaddition, we introduce SignSGD employing the sign map to regularize the updatedirections in DSGDm, and show it is enclosed in our proposed framework.Consequently, our convergence results establish, for the first time, globalconvergence of these methods when applied to nonsmooth nonconvex objectives.Preliminary numerical experiments demonstrate that our proposed frameworkyields highly efficient decentralized subgradient methods with convergenceguarantees in the training of nonsmooth neural networks.</description><author>Siyuan Zhang, Nachuan Xiao, Xin Liu</author><pubDate>Mon, 18 Mar 2024 09:35:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11565v1</guid></item><item><title>Advancing Neuromorphic Computing: Mixed-Signal Design Techniques Leveraging Brain Code Units and Fundamental Code Units</title><link>http://arxiv.org/abs/2403.11563v1</link><description>This paper introduces a groundbreaking digital neuromorphic architecture thatinnovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)using mixedsignal design methodologies. Leveraging open-source datasets and thelatest advances in materials science, our research focuses on enhancing thecomputational efficiency, accuracy, and adaptability of neuromorphic systems.The core of our approach lies in harmonizing the precision and scalability ofdigital systems with the robustness and energy efficiency of analog processing.Through experimentation, we demonstrate the effectiveness of our system acrossvarious metrics. The BCU achieved an accuracy of 88.0% and a power efficiencyof 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a powerefficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantlyimproved latency and throughput, achieving a latency as low as 0.75 ms andthroughput up to 213 TOP/s. These results firmly establish the potential of ourarchitecture in neuromorphic computing, providing a solid foundation for futuredevelopments in this domain. Our study underscores the feasibility ofmixedsignal neuromorphic systems and their promise in advancing the field,particularly in applications requiring high efficiency and adaptability</description><author>Murat Isik, Sols Miziev, Wiktoria Pawlak, Newton Howard</author><pubDate>Mon, 18 Mar 2024 09:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11563v1</guid></item></channel></rss>