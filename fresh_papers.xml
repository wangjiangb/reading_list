<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 28 Sep 2024 01:01:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner</title><link>http://arxiv.org/abs/2409.18128v1</link><description>Building on the success of diffusion models in visual generation, flow-basedmodels reemerge as another prominent family of generative models that haveachieved competitive or better performance in terms of both visual quality andinference speed. By learning the velocity field through flow-matching,flow-based models tend to produce a straighter sampling trajectory, which isadvantageous during the sampling process. However, unlike diffusion models forwhich fast samplers are well-developed, efficient sampling of flow-basedgenerative models has been rarely explored. In this paper, we propose aframework called FlowTurbo to accelerate the sampling of flow-based modelswhile still enhancing the sampling quality. Our primary observation is that thevelocity predictor's outputs in the flow-based models will become stable duringthe sampling, enabling the estimation of velocity via a lightweight velocityrefiner. Additionally, we introduce several techniques including a pseudocorrector and sample-aware compilation to further reduce inference time. SinceFlowTurbo does not change the multi-step sampling paradigm, it can beeffectively applied for various tasks such as image editing, inpainting, etc.By integrating FlowTurbo into different flow-based models, we obtain anacceleration ratio of 53.1%$\sim$58.3% on class-conditional generation and29.8%$\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FIDof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),achieving the real-time image generation and establishing the newstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.</description><author>Wenliang Zhao, Minglei Shi, Xumin Yu, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 26 Sep 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18128v1</guid></item><item><title>EgoLM: Multi-Modal Language Model of Egocentric Motions</title><link>http://arxiv.org/abs/2409.18127v1</link><description>As the prevalence of wearable devices, learning egocentric motions becomesessential to develop contextual AI. In this work, we present EgoLM, a versatileframework that tracks and understands egocentric motions from multi-modalinputs, e.g., egocentric videos and motion sensors. EgoLM exploits richcontexts for the disambiguation of egomotion tracking and understanding, whichare ill-posed under single modality conditions. To facilitate the versatile andmulti-modal framework, our key insight is to model the joint distribution ofegocentric motions and natural languages using large language models (LLM).Multi-modal sensor inputs are encoded and projected to the joint latent spaceof language models, and used to prompt motion generation or text generation foregomotion tracking or understanding, respectively. Extensive experiments onlarge-scale multi-modal human motion dataset validate the effectiveness ofEgoLM as a generalist model for universal egocentric learning.</description><author>Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, Lingni Ma</author><pubDate>Thu, 26 Sep 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18127v1</guid></item><item><title>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness</title><link>http://arxiv.org/abs/2409.18125v1</link><description>Recent advancements in Large Multimodal Models (LMMs) have greatly enhancedtheir proficiency in 2D visual understanding tasks, enabling them toeffectively process and understand images and videos. However, the developmentof LMMs with 3D-awareness for 3D scene understanding has been hindered by thelack of large-scale 3D vision-language datasets and powerful 3D encoders. Inthis paper, we introduce a simple yet effective framework called LLaVA-3D.Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3Defficiently adapts LLaVA for 3D scene understanding without compromising 2Dunderstanding capabilities. To achieve this, we employ a simple yet effectiverepresentation, 3D Patch, which connects 2D CLIP patch features with theircorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMsand employing joint 2D and 3D vision-language instruction tuning, we establisha unified architecture for both 2D image understanding and 3D sceneunderstanding. Experimental results show that LLaVA-3D converges 3.5x fasterthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,LLaVA-3D not only achieves state-of-the-art performance across various 3D tasksbut also maintains comparable 2D image understanding and vision-languageconversation capabilities with LLaVA.</description><author>Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu</author><pubDate>Thu, 26 Sep 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18125v1</guid></item><item><title>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction</title><link>http://arxiv.org/abs/2409.18124v1</link><description>Leveraging the visual priors of pre-trained text-to-image diffusion modelsoffers a promising solution to enhance zero-shot generalization in denseprediction tasks. However, existing methods often uncritically use the originaldiffusion formulation, which may not be optimal due to the fundamentaldifferences between dense prediction and image generation. In this paper, weprovide a systemic analysis of the diffusion formulation for the denseprediction, focusing on both quality and efficiency. And we find that theoriginal parameterization type for image generation, which learns to predictnoise, is harmful for dense prediction; the multi-step noising/denoisingdiffusion process is also unnecessary and challenging to optimize. Based onthese insights, we introduce Lotus, a diffusion-based visual foundation modelwith a simple yet effective adaptation protocol for dense prediction.Specifically, Lotus is trained to directly predict annotations instead ofnoise, thereby avoiding harmful variance. We also reformulate the diffusionprocess into a single-step procedure, simplifying optimization andsignificantly boosting inference speed. Additionally, we introduce a noveltuning strategy called detail preserver, which achieves more accurate andfine-grained predictions. Without scaling up the training data or modelcapacity, Lotus achieves SoTA performance in zero-shot depth and normalestimation across various datasets. It also significantly enhances efficiency,being hundreds of times faster than most existing diffusion-based methods.</description><author>Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, Ying-Cong Chen</author><pubDate>Thu, 26 Sep 2024 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18124v1</guid></item><item><title>Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</title><link>http://arxiv.org/abs/2409.18121v1</link><description>Humans can learn to manipulate new objects by simply watching others;providing robots with the ability to learn from such demonstrations wouldenable a natural interface specifying new behaviors. This work develops RobotSee Robot Do (RSRD), a method for imitating articulated object manipulationfrom a single monocular RGB human demonstration given a single staticmulti-view object scan. We first propose 4D Differentiable Part Models(4D-DPM), a method for recovering 3D part motion from a monocular video withdifferentiable rendering. This analysis-by-synthesis approach uses part-centricfeature fields in an iterative optimization which enables the use of geometricregularizers to recover 3D motions from only a single video. Given this 4Dreconstruction, the robot replicates object trajectories by planning bimanualarm motions that induce the demonstrated object part motion. By representingdemonstrations as part-centric trajectories, RSRD focuses on replicating thedemonstration's intended behavior while considering the robot's ownmorphological limits, rather than attempting to reproduce the hand's motion. Weevaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D parttrajectories and RSRD's physical execution performance on 9 objects across 10trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of87% success rate, for a total end-to-end success rate of 60% across 90 trials.Notably, this is accomplished using only feature fields distilled from largepretrained vision models -- without any task-specific training, fine-tuning,dataset collection, or annotation. Project page:https://robot-see-robot-do.github.io</description><author>Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi, Qianqian Wang, Ken Goldberg, Angjoo Kanazawa</author><pubDate>Thu, 26 Sep 2024 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18121v1</guid></item><item><title>EvMAPPER: High Altitude Orthomapping with Event Cameras</title><link>http://arxiv.org/abs/2409.18120v1</link><description>Traditionally, unmanned aerial vehicles (UAVs) rely on CMOS-based cameras tocollect images about the world below. One of the most successful applicationsof UAVs is to generate orthomosaics or orthomaps, in which a series of imagesare integrated together to develop a larger map. However, the use of CMOS-basedcameras with global or rolling shutters mean that orthomaps are vulnerable tochallenging light conditions, motion blur, and high-speed motion ofindependently moving objects under the camera. Event cameras are less sensitiveto these issues, as their pixels are able to trigger asynchronously onbrightness changes. This work introduces the first orthomosaic approach usingevent cameras. In contrast to existing methods relying only on CMOS cameras,our approach enables map generation even in challenging light conditions,including direct sunlight and after sunset.</description><author>Fernando Cladera, Kenneth Chaney, M. Ani Hsieh, Camillo J. Taylor, Vijay Kumar</author><pubDate>Thu, 26 Sep 2024 17:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18120v1</guid></item><item><title>Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography</title><link>http://arxiv.org/abs/2409.18119v1</link><description>Contrastive Language-Image Pre-training (CLIP) shows promise in medical imageanalysis but requires substantial data and computational resources. Due tothese restrictions, existing CLIP applications in medical imaging focus mainlyon modalities like chest X-rays that have abundant image-report data available,leaving many other important modalities under-explored. Here, we propose thefirst adaptation of the full CLIP model to mammography, which presentssignificant challenges due to labeled data scarcity, high-resolution imageswith small regions of interest, and data imbalance. We first develop aspecialized supervision framework for mammography that leverages its multi-viewnature. Furthermore, we design a symmetric local alignment module to betterfocus on detailed features in high-resolution images. Lastly, we incorporate aparameter-efficient fine-tuning approach for large language models pre-trainedwith medical knowledge to address data limitations. Our multi-view andmulti-scale alignment (MaMA) method outperforms state-of-the-art baselines forthree different tasks on two large real-world mammography datasets, EMBED andRSNA-Mammo, with only 52% model size compared with the largest baseline.</description><author>Yuexi Du, John Onofrey, Nicha C. Dvornek</author><pubDate>Thu, 26 Sep 2024 17:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18119v1</guid></item><item><title>Assumption violations in causal discovery and the robustness of score matching</title><link>http://arxiv.org/abs/2310.13387v2</link><description>When domain knowledge is limited and experimentation is restricted byethical, financial, or time constraints, practitioners turn to observationalcausal discovery methods to recover the causal structure, exploiting thestatistical properties of their data. Because causal discovery without furtherassumptions is an ill-posed problem, each algorithm comes with its own set ofusually untestable assumptions, some of which are hard to meet in realdatasets. Motivated by these considerations, this paper extensively benchmarksthe empirical performance of recent causal discovery methods on observationali.i.d. data generated under different background conditions, allowing forviolations of the critical assumptions required by each selected approach. Ourexperimental findings show that score matching-based methods demonstratesurprising performance in the false positive and false negative rate of theinferred graph in these challenging scenarios, and we provide theoreticalinsights into their performance. This work is also the first effort tobenchmark the stability of causal discovery algorithms with respect to thevalues of their hyperparameters. Finally, we hope this paper will set a newstandard for the evaluation of causal discovery methods and can serve as anaccessible entry point for practitioners interested in the field, highlightingthe empirical implications of different algorithm choices.</description><author>Francesco Montagna, Atalanti A. Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, Dominik Janzing, Bryon Aragam, Francesco Locatello</author><pubDate>Thu, 26 Sep 2024 17:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13387v2</guid></item><item><title>UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</title><link>http://arxiv.org/abs/2407.00312v2</link><description>Single-stage neural combinatorial optimization solvers have achievednear-optimal results on various small-scale combinatorial optimization (CO)problems without needing expert knowledge. However, these solvers exhibitsignificant performance degradation when applied to large-scale CO problems.Recently, two-stage neural methods with divide-and-conquer strategies haveshown efficiency in addressing large-scale CO problems. Nevertheless, theperformance of these methods highly relies on problem-specific heuristics ineither the divide or the conquer procedure, which limits their applicability togeneral CO problems. Moreover, these methods employ separate training schemesand ignore the interdependencies between the dividing and conqueringstrategies, which often leads to sub-optimal solutions. To tackle thesedrawbacks, this article develops a unified neural divide-and-conquer framework(i.e., UDC) for solving general large-scale CO problems. UDC offers aDivide-Conquer-Reunion (DCR) training method to eliminate the negative impactof a sub-optimal dividing policy. Employing a high-efficiency Graph NeuralNetwork (GNN) for global instance dividing and a fixed-length sub-path solverfor conquering divided sub-problems, the proposed UDC framework demonstratesextensive applicability, achieving superior performance in 10 representativelarge-scale CO problems. The code is available athttps://github.com/CIAM-Group/NCO_code/tree/main/single_objective/UDC-Large-scale-CO-master.</description><author>Zhi Zheng, Changliang Zhou, Tong Xialiang, Mingxuan Yuan, Zhenkun Wang</author><pubDate>Thu, 26 Sep 2024 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00312v2</guid></item><item><title>EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation</title><link>http://arxiv.org/abs/2409.18114v1</link><description>Current auto-regressive mesh generation methods suffer from issues such asincompleteness, insufficient detail, and poor generalization. In this paper, wepropose an Auto-regressive Auto-encoder (ArAE) model capable of generatinghigh-quality 3D meshes with up to 4,000 faces at a spatial resolution of$512^3$. We introduce a novel mesh tokenization algorithm that efficientlycompresses triangular meshes into 1D token sequences, significantly enhancingtraining efficiency. Furthermore, our model compresses variable-lengthtriangular meshes into a fixed-length latent space, enabling training latentdiffusion models for better generalization. Extensive experiments demonstratethe superior quality, diversity, and generalization capabilities of our modelin both point cloud and image-conditioned mesh generation tasks.</description><author>Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, Qinsheng Zhang</author><pubDate>Thu, 26 Sep 2024 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18114v1</guid></item><item><title>E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</title><link>http://arxiv.org/abs/2409.18111v1</link><description>Recent advances in Video Large Language Models (Video-LLMs) have demonstratedtheir great potential in general-purpose video understanding. To verify thesignificance of these models, a number of benchmarks have been proposed todiagnose their capabilities in different scenarios. However, existingbenchmarks merely evaluate models through video-level question-answering,lacking fine-grained event-level assessment and task diversity. To fill thisgap, we introduce E.T. Bench (Event-Level &amp; Time-Sensitive Video UnderstandingBenchmark), a large-scale and high-quality benchmark for open-ended event-levelvideo understanding. Categorized within a 3-level task taxonomy, E.T. Benchencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)under 8 domains, providing comprehensive evaluations. We extensively evaluated8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal thatstate-of-the-art models for coarse-level (video-level) understanding struggleto solve our fine-grained tasks, e.g., grounding event-of-interests withinvideos, largely due to the short video context length, improper timerepresentations, and lack of multi-event training data. Focusing on theseissues, we further propose a strong baseline model, E.T. Chat, together with aninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grainedevent-level understanding. Our simple but effective solution demonstratessuperior performance in multiple scenarios.</description><author>Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</author><pubDate>Thu, 26 Sep 2024 17:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18111v1</guid></item><item><title>Open-World Evaluation for Retrieving Diverse Perspectives</title><link>http://arxiv.org/abs/2409.18110v1</link><description>We study retrieving a set of documents that covers various perspectives on acomplex and contentious question (e.g., will ChatGPT do more harm than good?).We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),where each example consists of a question and diverse perspectives associatedwith the question, sourced from survey questions and debate websites. On thisdata, retrievers paired with a corpus are evaluated to surface a document setthat contains diverse perspectives. Our framing diverges from most retrievaltasks in that document relevancy cannot be decided by simple string matches toreferences. Instead, we build a language model based automatic evaluator thatdecides whether each retrieved document contains a perspective. This allows usto evaluate the performance of three different types of corpus (Wikipedia, websnapshot, and corpus constructed on the fly with retrieved pages from thesearch engine) paired with retrievers. Retrieving diverse documents remainschallenging, with the outputs from existing retrievers covering allperspectives on only 33.74% of the examples. We further study the impact ofquery expansion and diversity-focused reranking approaches and analyzeretriever sycophancy. Together, we lay the foundation for future studies inretrieval diversity handling complex queries.</description><author>Hung-Ting Chen, Eunsol Choi</author><pubDate>Thu, 26 Sep 2024 17:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18110v1</guid></item><item><title>Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats</title><link>http://arxiv.org/abs/2409.18104v1</link><description>Much of Earth's charismatic megafauna is endangered by human activities,particularly the rhino, which is at risk of extinction due to the poachingcrisis in Africa. Monitoring rhinos' movement is crucial to their protectionbut has unfortunately proven difficult because rhinos are elusive. Therefore,instead of tracking rhinos, we propose the novel approach of mapping communaldefecation sites, called middens, which give information about rhinos' spatialbehavior valuable to anti-poaching, management, and reintroduction efforts.This paper provides the first-ever mapping of rhino midden locations bybuilding classifiers to detect them using remotely sensed thermal, RGB, andLiDAR imagery in passive and active learning settings. As existing activelearning methods perform poorly due to the extreme class imbalance in ourdataset, we design MultimodAL, an active learning system employing a rankingtechnique and multimodality to achieve competitive performance with passivelearning models with 94% fewer labels. Our methods could therefore save over 76hours in labeling time when used on a similarly-sized dataset. Unexpectedly,our midden map reveals that rhino middens are not randomly distributedthroughout the landscape; rather, they are clustered. Consequently, rangersshould be targeted at areas with high midden densities to strengthenanti-poaching efforts, in line with UN Target 15.7.</description><author>Lucia Gordon, Nikhil Behari, Samuel Collier, Elizabeth Bondi-Kelly, Jackson A. Killian, Catherine Ressijac, Peter Boucher, Andrew Davies, Milind Tambe</author><pubDate>Thu, 26 Sep 2024 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18104v1</guid></item><item><title>MALPOLON: A Framework for Deep Species Distribution Modeling</title><link>http://arxiv.org/abs/2409.18102v1</link><description>This paper describes a deep-SDM framework, MALPOLON. Written in Python andbuilt upon the PyTorch library, this framework aims to facilitate training andinferences of deep species distribution models (deep-SDM) and sharing for userswith only general Python language skills (e.g., modeling ecologists) who areinterested in testing deep learning approaches to build new SDMs. More advancedusers can also benefit from the framework's modularity to run more specificexperiments by overriding existing classes while taking advantage ofpress-button examples to train neural networks on multiple classification tasksusing custom or provided raw and pre-processed datasets. The framework isopen-sourced on GitHub and PyPi along with extensive documentation and examplesof use in various scenarios. MALPOLON offers straightforward installation,YAML-based configuration, parallel computing, multi-GPU utilization, baselineand foundational models for benchmarking, and extensivetutorials/documentation, aiming to enhance accessibility and performancescalability for ecologists and researchers.</description><author>Theo Larcher, Lukas Picek, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Alexis Joly</author><pubDate>Thu, 26 Sep 2024 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18102v1</guid></item><item><title>AI-Powered Augmented Reality for Satellite Assembly, Integration and Test</title><link>http://arxiv.org/abs/2409.18101v1</link><description>The integration of Artificial Intelligence (AI) and Augmented Reality (AR) isset to transform satellite Assembly, Integration, and Testing (AIT) processesby enhancing precision, minimizing human error, and improving operationalefficiency in cleanroom environments. This paper presents a technicaldescription of the European Space Agency's (ESA) project "AI for AR inSatellite AIT," which combines real-time computer vision and AR systems toassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2as the AR interface, the system delivers context-aware instructions andreal-time feedback, tackling the complexities of object recognition and 6D poseestimation in AIT workflows. All AI models demonstrated over 70% accuracy, withthe detection model exceeding 95% accuracy, indicating a high level ofperformance and reliability. A key contribution of this work lies in theeffective use of synthetic data for training AI models in AR applications,addressing the significant challenges of obtaining real-world datasets inhighly dynamic satellite environments, as well as the creation of the SegmentedAnything Model for Automatic Labelling (SAMAL), which facilitates the automaticannotation of real data, achieving speeds up to 20 times faster than manualhuman annotation. The findings demonstrate the efficacy of AI-driven AR systemsin automating critical satellite assembly tasks, setting a foundation forfuture innovations in the space industry.</description><author>Alvaro Patricio, Joao Valente, Atabak Dehban, Ines Cadilha, Daniel Reis, Rodrigo Ventura</author><pubDate>Thu, 26 Sep 2024 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18101v1</guid></item><item><title>Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation</title><link>http://arxiv.org/abs/2409.18100v1</link><description>Self-supervised pretraining (SSP) has shown promising results in learningfrom large unlabeled datasets and, thus, could be useful for automatedcardiovascular magnetic resonance (CMR) short-axis cine segmentation. However,inconsistent reports of the benefits of SSP for segmentation have made itdifficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSPmethods for CMR cine segmentation. To this end, short-axis cine stacks of 296 subjects (90618 2D slices) wereused for unlabeled pretraining with four SSP methods; SimCLR, positionalcontrastive learning, DINO, and masked image modeling (MIM). Subsets of varyingnumbers of subjects were used for supervised fine-tuning of 2D models for eachSSP method, as well as to train a 2D baseline model from scratch. Thefine-tuned models were compared to the baseline using the 3D Dice similaritycoefficient (DSC) in a test dataset of 140 subjects. The SSP methods showed no performance gains with the largest supervisedfine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects(231 2D slices) are available for supervised training, SSP using MIM (DSC =0.86) improves over training from scratch (DSC = 0.82). This study found that SSP is valuable for CMR cine segmentation when labeledtraining data is scarce, but does not aid state-of-the-art deep learningmethods when ample labeled data is available. Moreover, the choice of SSPmethod is important. The code is publicly available at:https://github.com/q-cardIA/ssp-cmr-cine-segmentation</description><author>Rob A. J. de Mooij, Josien P. W. Pluim, Cian M. Scannell</author><pubDate>Thu, 26 Sep 2024 17:44:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18100v1</guid></item><item><title>EfficientCrackNet: A Lightweight Model for Crack Segmentation</title><link>http://arxiv.org/abs/2409.18099v1</link><description>Crack detection, particularly from pavement images, presents a formidablechallenge in the domain of computer vision due to several inherent complexitiessuch as intensity inhomogeneity, intricate topologies, low contrast, and noisybackgrounds. Automated crack detection is crucial for maintaining thestructural integrity of essential infrastructures, including buildings,pavements, and bridges. Existing lightweight methods often face challengesincluding computational inefficiency, complex crack patterns, and difficultbackgrounds, leading to inaccurate detection and impracticality for real-worldapplications. To address these limitations, we propose EfficientCrackNet, alightweight hybrid model combining Convolutional Neural Networks (CNNs) andtransformers for precise crack segmentation. EfficientCrackNet integratesdepthwise separable convolutions (DSC) layers and MobileViT block to captureboth global and local features. The model employs an Edge Extraction Method(EEM) and for efficient crack edge detection without pretraining, andUltra-Lightweight Subspace Attention Module (ULSAM) to enhance featureextraction. Extensive experiments on three benchmark datasets Crack500,DeepCrack, and GAPs384 demonstrate that EfficientCrackNet achieves superiorperformance compared to existing lightweight models, while requiring only 0.26Mparameters, and 0.483 FLOPs (G). The proposed model offers an optimal balancebetween accuracy and computational efficiency, outperforming state-of-the-artlightweight models, and providing a robust and adaptable solution forreal-world crack segmentation.</description><author>Abid Hasan Zim, Aquib Iqbal, Zaid Al-Huda, Asad Malik, Minoru Kuribayash</author><pubDate>Thu, 26 Sep 2024 17:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18099v1</guid></item><item><title>Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?</title><link>http://arxiv.org/abs/2406.12822v3</link><description>Multilingual large language models are designed, claimed, and expected tocater to speakers of varied languages. We hypothesise that the currentpractices of fine-tuning and evaluating these models may not perfectly alignwith this objective owing to a heavy reliance on translation, which cannotcover language-specific knowledge but can introduce translation defects. Itremains unknown whether the nature of the instruction data has an impact on themodel output; conversely, it is questionable whether translated test sets cancapture such nuances. Due to the often coupled practices of using translateddata in both stages, such imperfections could have been overlooked. This workinvestigates these issues using controlled native or translated data during theinstruction tuning and evaluation stages. We show that native or generationbenchmarks reveal a notable difference between native and translatedinstruction data especially when model performance is high, whereas other typesof test sets cannot. The comparison between round-trip and single-passtranslations reflects the importance of knowledge from language-nativeresources. Finally, we demonstrate that regularization is beneficial tobridging this gap on structured but not generative tasks.</description><author>Pinzhen Chen, Simon Yu, Zhicheng Guo, Barry Haddow</author><pubDate>Thu, 26 Sep 2024 17:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12822v3</guid></item><item><title>DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2409.18092v1</link><description>Perception systems play a crucial role in autonomous driving, incorporatingmultiple sensors and corresponding computer vision algorithms. 3D LiDAR sensorsare widely used to capture sparse point clouds of the vehicle's surroundings.However, such systems struggle to perceive occluded areas and gaps in the scenedue to the sparsity of these point clouds and their lack of semantics. Toaddress these challenges, Semantic Scene Completion (SSC) jointly predictsunobserved geometry and semantics in the scene given raw LiDAR measurements,aiming for a more complete scene representation. Building on promising resultsof diffusion models in image generation and super-resolution tasks, we proposetheir extension to SSC by implementing the noising and denoising diffusionprocesses in the point and semantic spaces individually. To control thegeneration, we employ semantic LiDAR point clouds as conditional input anddesign local and global regularization losses to stabilize the denoisingprocess. We evaluate our approach on autonomous driving datasets and ourapproach outperforms the state-of-the-art for SSC.</description><author>Helin Cao, Sven Behnke</author><pubDate>Thu, 26 Sep 2024 17:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18092v1</guid></item><item><title>Quantum Kernel Methods under Scrutiny: A Benchmarking Study</title><link>http://arxiv.org/abs/2409.04406v2</link><description>Since the entry of kernel theory in the field of quantum machine learning,quantum kernel methods (QKMs) have gained increasing attention with regard toboth probing promising applications and delivering intriguing researchinsights. Two common approaches for computing the underlying Gram matrix haveemerged: fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs).Benchmarking these methods is crucial to gain robust insights and to understandtheir practical utility. In this work, we present a comprehensive large-scalestudy examining QKMs based on FQKs and PQKs across a manifold of designchoices. Our investigation encompasses both classification and regression tasksfor five dataset families and 64 datasets, systematically comparing the use ofFQKs and PQKs quantum support vector machines and kernel ridge regression. Thisresulted in over 20,000 models that were trained and optimized using astate-of-the-art hyperparameter search to ensure robust and comprehensiveinsights. We delve into the importance of hyperparameters on model performancescores and support our findings through rigorous correlation analyses. In this,we also closely inspect two data encoding strategies. Moreover, we provide anin-depth analysis addressing the design freedom of PQKs and explore theunderlying principles responsible for learning. Our goal is not to identify thebest-performing model for a specific task but to uncover the mechanisms thatlead to effective QKMs and reveal universal patterns.</description><author>Jan Schnabel, Marco Roth</author><pubDate>Thu, 26 Sep 2024 17:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04406v2</guid></item><item><title>AI-driven View Guidance System in Intra-cardiac Echocardiography Imaging</title><link>http://arxiv.org/abs/2409.16898v2</link><description>Intra-cardiac Echocardiography (ICE) is a crucial imaging modality used inelectrophysiology (EP) and structural heart disease (SHD) interventions,providing real-time, high-resolution views from within the heart. Despite itsadvantages, effective manipulation of the ICE catheter requires significantexpertise, which can lead to inconsistent outcomes, particularly among lessexperienced operators. To address this challenge, we propose an AI-drivenclosed-loop view guidance system with human-in-the-loop feedback, designed toassist users in navigating ICE imaging without requiring specialized knowledge.Our method models the relative position and orientation vectors betweenarbitrary views and clinically defined ICE views in a spatial coordinatesystem, guiding users on how to manipulate the ICE catheter to transition fromthe current view to the desired view over time. Operating in a closed-loopconfiguration, the system continuously predicts and updates the necessarycatheter manipulations, ensuring seamless integration into existing clinicalworkflows. The effectiveness of the proposed system is demonstrated through asimulation-based evaluation, achieving an 89% success rate with the 6532 testdataset, highlighting its potential to improve the accuracy and efficiency ofICE imaging procedures.</description><author>Jaeyoung Huh, Paul Klein, Gareth Funka-Lea, Puneet Sharma, Ankur Kapoor, Young-Ho Kim</author><pubDate>Thu, 26 Sep 2024 17:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16898v2</guid></item><item><title>Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with Enhanced Generalization and Personalization Abilities</title><link>http://arxiv.org/abs/2409.16147v2</link><description>Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significantpotential for modeling 3D head avatars, providing greater flexibility thanmesh-based methods and more efficient rendering compared to NeRF-basedapproaches. Despite these advancements, the creation of controllable 3DGS-basedhead avatars remains time-intensive, often requiring tens of minutes to hours.To expedite this process, we here introduce the ``Gaussian D\'ej\`a-vu"framework, which first obtains a generalized model of the head avatar and thenpersonalizes the result. The generalized model is trained on large 2D(synthetic and real) image datasets. This model provides a well-initialized 3DGaussian head that is further refined using a monocular video to achieve thepersonalized head avatar. For personalizing, we propose learnableexpression-aware rectification blendmaps to correct the initial 3D Gaussians,ensuring rapid convergence without the reliance on neural networks. Experimentsdemonstrate that the proposed method meets its objectives. It outperformsstate-of-the-art 3D Gaussian head avatars in terms of photorealistic quality aswell as reduces training time consumption to at least a quarter of the existingmethods, producing the avatar in minutes.</description><author>Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du</author><pubDate>Thu, 26 Sep 2024 17:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16147v2</guid></item><item><title>GSON: A Group-based Social Navigation Framework with Large Multimodal Model</title><link>http://arxiv.org/abs/2409.18084v1</link><description>As the number of service robots and autonomous vehicles in human-centeredenvironments grows, their requirements go beyond simply navigating to adestination. They must also take into account dynamic social contexts andensure respect and comfort for others in shared spaces, which poses significantchallenges for perception and planning. In this paper, we present a group-basedsocial navigation framework GSON to enable mobile robots to perceive andexploit the social group of their surroundings by leveling the visual reasoningcapability of the Large Multimodal Model (LMM). For perception, we apply visualprompting techniques to zero-shot extract the social relationship amongpedestrians and combine the result with a robust pedestrian detection andtracking pipeline to alleviate the problem of low inference speed of the LMM.Given the perception result, the planning system is designed to avoiddisrupting the current social structure. We adopt a social structure-basedmid-level planner as a bridge between global path planning and local motionplanning to preserve the global context and reactive response. The proposedmethod is validated on real-world mobile robot navigation tasks involvingcomplex social structure understanding and reasoning. Experimental resultsdemonstrate the effectiveness of the system in these scenarios compared withseveral baselines.</description><author>Shangyi Luo, Ji Zhu, Peng Sun, Yuhong Deng, Cunjun Yu, Anxing Xiao, Xueqian Wang</author><pubDate>Thu, 26 Sep 2024 17:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18084v1</guid></item><item><title>Stable Video Portraits</title><link>http://arxiv.org/abs/2409.18083v1</link><description>Rapid advances in the field of generative AI and text-to-image methods inparticular have transformed the way we interact with and perceivecomputer-generated imagery today. In parallel, much progress has been made in3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, wepresent SVP, a novel hybrid 2D/3D generation method that outputs photorealisticvideos of talking faces leveraging a large pre-trained text-to-image prior(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specificfine-tuning of a general 2D stable diffusion model which we lift to a videomodel by providing temporal 3DMM sequences as conditioning and by introducing atemporal denoising procedure. As an output, this model generates temporallysmooth imagery of a person with 3DMM-based controls, i.e., a person-specificavatar. The facial appearance of this person-specific avatar can be edited andmorphed to text-defined celebrities, without any fine-tuning at test time. Themethod is analyzed quantitatively and qualitatively, and we show that ourmethod outperforms state-of-the-art monocular head avatar methods.</description><author>Mirela Ostrek, Justus Thies</author><pubDate>Thu, 26 Sep 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18083v1</guid></item><item><title>SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation</title><link>http://arxiv.org/abs/2409.18082v1</link><description>Automating garment manipulation poses a significant challenge for assistiverobotics due to the diverse and deformable nature of garments. Traditionalapproaches typically require separate models for each garment type, whichlimits scalability and adaptability. In contrast, this paper presents a unifiedapproach using vision-language models (VLMs) to improve keypoint predictionacross various garment categories. By interpreting both visual and semanticinformation, our model enables robots to manage different garment states with asingle model. We created a large-scale synthetic dataset using advancedsimulation techniques, allowing scalable training without extensive real-worlddata. Experimental results indicate that the VLM-based method significantlyenhances keypoint detection accuracy and task success rates, providing a moreflexible and general solution for robotic garment manipulation. In addition,this research also underscores the potential of VLMs to unify various garmentmanipulation tasks within a single framework, paving the way for broaderapplications in home automation and assistive robotics for future.</description><author>Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu</author><pubDate>Thu, 26 Sep 2024 17:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18082v1</guid></item><item><title>Infer Human's Intentions Before Following Natural Language Instructions</title><link>http://arxiv.org/abs/2409.18073v1</link><description>For AI agents to be helpful to humans, they should be able to follow naturallanguage instructions to complete everyday cooperative tasks in humanenvironments. However, real human instructions inherently possess ambiguity,because the human speakers assume sufficient prior knowledge about their hiddengoals and intentions. Standard language grounding and planning methods fail toaddress such ambiguities because they do not model human internal goals asadditional partially observable factors in the environment. We propose a newframework, Follow Instructions with Social and Embodied Reasoning (FISER),aiming for better natural language instruction following in collaborativeembodied tasks. Our framework makes explicit inferences about human goals andintentions as intermediate reasoning steps. We implement a set ofTransformer-based models and evaluate them over a challenging benchmark,HandMeThat. We empirically demonstrate that using social reasoning toexplicitly infer human intentions before making action plans surpasses purelyend-to-end approaches. We also compare our implementation with strongbaselines, including Chain of Thought prompting on the largest availablepre-trained language models, and find that FISER provides better performance onthe embodied social reasoning tasks under investigation, reaching thestate-of-the-art on HandMeThat.</description><author>Yanming Wan, Yue Wu, Yiping Wang, Jiayuan Mao, Natasha Jaques</author><pubDate>Thu, 26 Sep 2024 17:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18073v1</guid></item><item><title>FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction</title><link>http://arxiv.org/abs/2409.18071v1</link><description>Introducing user-specified visual concepts in image editing is highlypractical as these concepts convey the user's intent more precisely thantext-based descriptions. We propose FreeEdit, a novel approach for achievingsuch reference-based image editing, which can accurately reproduce the visualconcept from the reference image based on user-friendly language instructions.Our approach leverages the multi-modal instruction encoder to encode languageinstructions to guide the editing process. This implicit way of locating theediting area eliminates the need for manual editing masks. To enhance thereconstruction of reference details, we introduce the Decoupled ResidualReferAttention (DRRA) module. This module is designed to integrate fine-grainedreference features extracted by a detail extractor into the image editingprocess in a residual way without interfering with the original self-attention.Given that existing datasets are unsuitable for reference-based image editingtasks, particularly due to the difficulty in constructing image triplets thatinclude a reference image, we curate a high-quality dataset, FreeBench, using anewly developed twice-repainting scheme. FreeBench comprises the images beforeand after editing, detailed editing instructions, as well as a reference imagethat maintains the identity of the edited object, encompassing tasks such asobject addition, replacement, and deletion. By conducting phased training onFreeBench followed by quality tuning, FreeEdit achieves high-quality zero-shotediting through convenient language instructions. We conduct extensiveexperiments to evaluate the effectiveness of FreeEdit across multiple tasktypes, demonstrating its superiority over existing methods. The code will beavailable at: https://freeedit.github.io/.</description><author>Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, Si Liu</author><pubDate>Thu, 26 Sep 2024 17:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18071v1</guid></item><item><title>Learning Interactive Real-World Simulators</title><link>http://arxiv.org/abs/2310.06114v3</link><description>Generative models trained on internet data have revolutionized how text,image, and video content can be created. Perhaps the next milestone forgenerative models is to simulate realistic experience in response to actionstaken by humans, robots, and other interactive agents. Applications of areal-world simulator range from controllable content creation in games andmovies, to training embodied agents purely in simulation that can be directlydeployed in the real world. We explore the possibility of learning a universalsimulator (UniSim) of real-world interaction through generative modeling. Wefirst make the important observation that natural datasets available forlearning a real-world simulator are often rich along different dimensions(e.g., abundant objects in image data, densely sampled actions in roboticsdata, and diverse movements in navigation data). With careful orchestration ofdiverse datasets, each providing a different aspect of the overall experience,we can simulate the visual outcome of both high-level instructions such as"open the drawer" and low-level controls from otherwise static scenes andobjects. We use the simulator to train both high-level vision-language policiesand low-level reinforcement learning policies, each of which can be deployed inthe real world in zero shot after training purely in simulation. We also showthat other types of intelligence such as video captioning models can benefitfrom training with simulated experience, opening up even wider applications.Video demos can be found at https://universal-simulator.github.io.</description><author>Sherry Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie Kaelbling, Dale Schuurmans, Pieter Abbeel</author><pubDate>Thu, 26 Sep 2024 17:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06114v3</guid></item><item><title>Optimal Protocols for Continual Learning via Statistical Physics and Control Theory</title><link>http://arxiv.org/abs/2409.18061v1</link><description>Artificial neural networks often struggle with catastrophic forgetting whenlearning multiple tasks sequentially, as training on new tasks degrades theperformance on previously learned ones. Recent theoretical work has addressedthis issue by analysing learning curves in synthetic frameworks underpredefined training protocols. However, these protocols relied on heuristicsand lacked a solid theoretical foundation assessing their optimality. In thispaper, we fill this gap combining exact equations for training dynamics,derived using statistical physics techniques, with optimal control methods. Weapply this approach to teacher-student models for continual learning andmulti-task problems, obtaining a theory for task-selection protocols maximisingperformance while minimising forgetting. Our theoretical analysis offersnon-trivial yet interpretable strategies for mitigating catastrophicforgetting, shedding light on how optimal learning protocols can modulateestablished effects, such as the influence of task similarity on forgetting.Finally, we validate our theoretical findings on real-world data.</description><author>Francesco Mori, Stefano Sarao Mannelli, Francesca Mignacco</author><pubDate>Thu, 26 Sep 2024 17:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18061v1</guid></item><item><title>LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</title><link>http://arxiv.org/abs/2409.18057v1</link><description>Recent works have shown that neural radiance fields (NeRFs) on top ofparametric models have reached SOTA quality to build photorealistic headavatars from a monocular video. However, one major limitation of the NeRF-basedavatars is the slow rendering speed due to the dense point sampling of NeRF,preventing them from broader utility on resource-constrained devices. Weintroduce LightAvatar, the first head avatar model based on neural light fields(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera posevia a single network forward pass, without using mesh or volume rendering. Theproposed approach, while being conceptually appealing, poses a significantchallenge towards real-time efficiency and training stability. To resolve them,we introduce dedicated network designs to obtain proper representations for theNeLF model and maintain a low FLOPs budget. Meanwhile, we tap into adistillation-based training strategy that uses a pretrained avatar model asteacher to synthesize abundant pseudo data for training. A warping fieldnetwork is introduced to correct the fitting error in the real data so that themodel can learn better. Extensive experiments suggest that our method canachieve new SOTA image quality quantitatively or qualitatively, while beingsignificantly faster than the counterparts, reporting 174.1 FPS (512x512resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.</description><author>Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu</author><pubDate>Thu, 26 Sep 2024 17:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18057v1</guid></item><item><title>Visual Data Diagnosis and Debiasing with Concept Graphs</title><link>http://arxiv.org/abs/2409.18055v1</link><description>The widespread success of deep learning models today is owed to the curationof extensive datasets significant in size and complexity. However, such modelsfrequently pick up inherent biases in the data during the training process,leading to unreliable predictions. Diagnosing and debiasing datasets is thus anecessity to ensure reliable model performance. In this paper, we presentCONBIAS, a novel framework for diagnosing and mitigating Concept co-occurrenceBiases in visual datasets. CONBIAS represents visual datasets as knowledgegraphs of concepts, enabling meticulous analysis of spurious conceptco-occurrences to uncover concept imbalances across the whole dataset.Moreover, we show that by employing a novel clique-based concept balancingstrategy, we can mitigate these imbalances, leading to enhanced performance ondownstream tasks. Extensive experiments show that data augmentation based on abalanced concept distribution augmented by CONBIAS improves generalizationperformance across multiple datasets compared to state-of-the-art methods. Wewill make our code and data publicly available.</description><author>Rwiddhi Chakraborty, Yinong Wang, Jialu Gao, Runkai Zheng, Cheng Zhang, Fernando De la Torre</author><pubDate>Thu, 26 Sep 2024 16:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18055v1</guid></item><item><title>DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving</title><link>http://arxiv.org/abs/2409.18053v1</link><description>We present a novel autonomous driving framework, DualAD, designed to imitatehuman reasoning during driving. DualAD comprises two layers: a rule-basedmotion planner at the bottom layer that handles routine driving tasks requiringminimal reasoning, and an upper layer featuring a rule-based text encoder thatconverts driving scenarios from absolute states into text description. Thistext is then processed by a large language model (LLM) to make drivingdecisions. The upper layer intervenes in the bottom layer's decisions whenpotential danger is detected, mimicking human reasoning in critical situations.Closed-loop experiments demonstrate that DualAD, using a zero-shot pre-trainedmodel, significantly outperforms rule-based motion planners that lack reasoningabilities. Our experiments also highlight the effectiveness of the textencoder, which considerably enhances the model's scenario understanding.Additionally, the integrated DualAD model improves with stronger LLMs,indicating the framework's potential for further enhancement. We make code andbenchmarks publicly available.</description><author>Dingrui Wang, Marc Kaufeld, Johannes Betz</author><pubDate>Thu, 26 Sep 2024 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18053v1</guid></item><item><title>Explaining Explaining</title><link>http://arxiv.org/abs/2409.18052v1</link><description>Explanation is key to people having confidence in high-stakes AI systems.However, machine-learning-based systems - which account for almost all currentAI - can't explain because they are usually black boxes. The explainable AI(XAI) movement hedges this problem by redefining "explanation". Thehuman-centered explainable AI (HCXAI) movement identifies theexplanation-oriented needs of users but can't fulfill them because of itscommitment to machine learning. In order to achieve the kinds of explanationsneeded by real people operating in critical domains, we must rethink how toapproach AI. We describe a hybrid approach to developing cognitive agents thatuses a knowledge-based infrastructure supplemented by data obtained throughmachine learning when applicable. These agents will serve as assistants tohumans who will bear ultimate responsibility for the decisions and actions ofthe human-robot team. We illustrate the explanatory potential of such agentsusing the under-the-hood panels of a demonstration system in which a team ofsimulated robots collaborates on a search task assigned by a human.</description><author>Sergei Nirenburg, Marjorie McShane, Kenneth W. Goodman, Sanjay Oruganti</author><pubDate>Thu, 26 Sep 2024 16:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18052v1</guid></item><item><title>Inverse Reinforcement Learning with Multiple Planning Horizons</title><link>http://arxiv.org/abs/2409.18051v1</link><description>In this work, we study an inverse reinforcement learning (IRL) problem wherethe experts are planning under a shared reward function but with different,unknown planning horizons. Without the knowledge of discount factors, thereward function has a larger feasible solution set, which makes it harder forexisting IRL approaches to identify a reward function. To overcome thischallenge, we develop algorithms that can learn a global multi-agent rewardfunction with agent-specific discount factors that reconstruct the expertpolicies. We characterize the feasible solution space of the reward functionand discount factors for both algorithms and demonstrate the generalizabilityof the learned reward function across multiple domains.</description><author>Jiayu Yao, Weiwei Pan, Finale Doshi-Velez, Barbara E Engelhardt</author><pubDate>Thu, 26 Sep 2024 16:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18051v1</guid></item><item><title>Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers</title><link>http://arxiv.org/abs/2312.08168v3</link><description>Recent advancements in 3D Large Language Models (LLMs) have demonstratedpromising capabilities for 3D scene understanding. However, previous methodsexhibit deficiencies in general referencing and grounding capabilities forintricate scene comprehension. In this paper, we introduce the use of objectidentifiers and object-centric representations to interact with scenes at theobject level. Specifically, we decompose the input 3D scene into a set ofobject proposals, each assigned a unique identifier token, which enablesefficient object referencing and grounding during user-assistant interactions.Given the scarcity of scene-language data, we model the scene embeddings as asequence of explicit object-level embeddings, derived from semantic-rich 2D or3D representations. By employing object identifiers, we transform diverse 3Dscene-language tasks into a unified question-answering format, facilitatingjoint training without the need for additional task-specific heads. Withminimal fine-tuning on all downstream tasks, our model significantlyoutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.</description><author>Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, Zhou Zhao</author><pubDate>Thu, 26 Sep 2024 16:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08168v3</guid></item><item><title>Revisit Anything: Visual Place Recognition via Image Segment Retrieval</title><link>http://arxiv.org/abs/2409.18049v1</link><description>Accurately recognizing a revisited place is crucial for embodied agents tolocalize and navigate. This requires visual representations to be distinct,despite strong variations in camera viewpoint and scene appearance. Existingvisual place recognition pipelines encode the "whole" image and search formatches. This poses a fundamental challenge in matching two images of the sameplace captured from different camera viewpoints: "the similarity of whatoverlaps can be dominated by the dissimilarity of what does not overlap". Weaddress this by encoding and searching for "image segments" instead of thewhole images. We propose to use open-set image segmentation to decompose animage into `meaningful' entities (i.e., things and stuff). This enables us tocreate a novel image representation as a collection of multiple overlappingsubgraphs connecting a segment with its neighboring segments, dubbedSuperSegment. Furthermore, to efficiently encode these SuperSegments intocompact vector representations, we propose a novel factorized representation offeature aggregation. We show that retrieving these partial representationsleads to significantly higher recognition recall than the typical whole imagebased retrieval. Our segments-based approach, dubbed SegVLAD, sets a newstate-of-the-art in place recognition on a diverse selection of benchmarkdatasets, while being applicable to both generic and task-specialized imageencoders. Finally, we demonstrate the potential of our method to ``revisitanything'' by evaluating our method on an object instance retrieval task, whichbridges the two disparate areas of research: visual place recognition andobject-goal navigation, through their common aim of recognizing goal objectsspecific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.</description><author>Kartik Garg, Sai Shubodh Puligilla, Shishir Kolathaya, Madhava Krishna, Sourav Garg</author><pubDate>Thu, 26 Sep 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18049v1</guid></item><item><title>Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax Optimization</title><link>http://arxiv.org/abs/2408.11974v2</link><description>We provide a unified analysis of two-timescale gradient descent ascent(TTGDA) for solving structured nonconvex minimax optimization problems in theform of $\min_\textbf{x} \max_{\textbf{y} \in Y} f(\textbf{x}, \textbf{y})$,where the objective function $f(\textbf{x}, \textbf{y})$ is nonconvex in$\textbf{x}$ and concave in $\textbf{y}$, and the constraint set $Y \subseteq\mathbb{R}^n$ is convex and bounded. In the convex-concave setting, thesingle-timescale gradient descent ascent (GDA) algorithm is widely used inapplications and has been shown to have strong convergence guarantees. In moregeneral settings, however, it can fail to converge. Our contribution is todesign TTGDA algorithms that are effective beyond the convex-concave setting,efficiently finding a stationary point of the function $\Phi(\cdot) :=\max_{\textbf{y} \in Y} f(\cdot, \textbf{y})$. We also establish theoreticalbounds on the complexity of solving both smooth and nonsmooth nonconvex-concaveminimax optimization problems. To the best of our knowledge, this is the firstsystematic analysis of TTGDA for nonconvex minimax optimization, shedding lighton its superior performance in training generative adversarial networks (GANs)and in other real-world application problems.</description><author>Tianyi Lin, Chi Jin, Michael. I. Jordan</author><pubDate>Thu, 26 Sep 2024 16:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11974v2</guid></item><item><title>HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams</title><link>http://arxiv.org/abs/2409.18047v1</link><description>This paper presents a novel approach to multi-robot planning andcollaboration. We demonstrate a cognitive strategy for robots in human-robotteams that incorporates metacognition, natural language communication, andexplainability. The system is embodied using the HARMONIC architecture thatflexibly integrates cognitive and control capabilities across the team. Weevaluate our approach through simulation experiments involving a joint searchtask by a team of heterogeneous robots (a UGV and a drone) and a human. Wedetail the system's handling of complex, real-world scenarios, effective actioncoordination between robots with different capabilities, and naturalhuman-robot communication. This work demonstrates that the robots' ability toreason about plans, goals, and attitudes, and to provide explanations foractions and decisions are essential prerequisites for realistic human-robotteaming.</description><author>Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt</author><pubDate>Thu, 26 Sep 2024 16:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18047v1</guid></item><item><title>IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning</title><link>http://arxiv.org/abs/2409.18046v1</link><description>Recent advancements in image captioning have explored text-only trainingmethods to overcome the limitations of paired image-text data. However,existing text-only training methods often overlook the modality gap betweenusing text data during training and employing images during inference. Toaddress this issue, we propose a novel approach called Image-like Retrieval,which aligns text features with visually relevant features to mitigate themodality gap. Our method further enhances the accuracy of generated captions bydesigning a Fusion Module that integrates retrieved captions with inputfeatures. Additionally, we introduce a Frequency-based Entity Filteringtechnique that significantly improves caption quality. We integrate thesemethods into a unified framework, which we refer to as IFCap($\textbf{I}$mage-like Retrieval and $\textbf{F}$requency-based EntityFiltering for Zero-shot $\textbf{Cap}$tioning). Through extensiveexperimentation, our straightforward yet powerful approach has demonstrated itsefficacy, outperforming the state-of-the-art methods by a significant margin inboth image captioning and video captioning compared to zero-shot captioningbased on text-only training.</description><author>Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim</author><pubDate>Thu, 26 Sep 2024 16:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18046v1</guid></item><item><title>Unveiling the Role of Pretraining in Direct Speech Translation</title><link>http://arxiv.org/abs/2409.18044v1</link><description>Direct speech-to-text translation systems encounter an important drawback indata scarcity. A common solution consists on pretraining the encoder onautomatic speech recognition, hence losing efficiency in the training process.In this study, we compare the training dynamics of a system using a pretrainedencoder, the conventional approach, and one trained from scratch. We observethat, throughout the training, the randomly initialized model struggles toincorporate information from the speech inputs for its predictions. Hence, wehypothesize that this issue stems from the difficulty of effectively trainingan encoder for direct speech translation. While a model trained from scratchneeds to learn acoustic and semantic modeling simultaneously, a pretrained onecan just focus on the latter. Based on these findings, we propose a subtlechange in the decoder cross-attention to integrate source information fromearlier steps in training. We show that with this change, the model trainedfrom scratch can achieve comparable performance to the pretrained one, whilereducing the training time.</description><author>Belen Alastruey, Gerard I. Gállego, Marta R. Costa-jussà</author><pubDate>Thu, 26 Sep 2024 16:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18044v1</guid></item><item><title>EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</title><link>http://arxiv.org/abs/2409.18042v1</link><description>GPT-4o, an omni-modal model that enables vocal conversations with diverseemotions and tones, marks a milestone for omni-modal foundation models.However, empowering Large Language Models to perceive and generate images,texts, and speeches end-to-end with publicly available data remains challengingin the open-source community. Existing vision-language models rely on externaltools for the speech processing, while speech-language models still suffer fromlimited or even without vision-understanding abilities. To address this gap, wepropose EMOVA (EMotionally Omni-present Voice Assistant), to enable LargeLanguage Models with end-to-end speech capabilities while maintaining theleading vision-language performance. With a semantic-acoustic disentangledspeech tokenizer, we notice surprisingly that omni-modal alignment can furtherenhance vision-language and speech abilities compared with the correspondingbi-modal aligned counterparts. Moreover, a lightweight style module is proposedfor flexible speech style controls (e.g., emotions and pitches). For the firsttime, EMOVA achieves state-of-the-art performance on both the vision-languageand speech benchmarks, and meanwhile, supporting omni-modal spoken dialoguewith vivid emotions.</description><author>Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu</author><pubDate>Thu, 26 Sep 2024 16:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18042v1</guid></item><item><title>HARMONIC: A Framework for Explanatory Cognitive Robots</title><link>http://arxiv.org/abs/2409.18037v1</link><description>We present HARMONIC, a framework for implementing cognitive robots thattransforms general-purpose robots into trusted teammates capable of complexdecision-making, natural communication and human-level explanation. Theframework supports interoperability between a strategic (cognitive) layer forhigh-level decision-making and a tactical (robot) layer for low-level controland execution. We describe the core features of the framework and our initialimplementation, in which HARMONIC was deployed on a simulated UGV and droneinvolved in a multi-robot search and retrieval task.</description><author>Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt</author><pubDate>Thu, 26 Sep 2024 16:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18037v1</guid></item><item><title>Ascend HiFloat8 Format for Deep Learning</title><link>http://arxiv.org/abs/2409.16626v2</link><description>This preliminary white paper proposes a novel 8-bit floating-point dataformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features taperedprecision. For normal value encoding, it provides 7 exponent values with 3-bitmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).Meanwhile, HiF8 encodes all the special values except that positive zero andnegative zero are represented by only one bit-pattern. Thanks to the betterbalance between precision and dynamic range, HiF8 can be simultaneously used inboth forward and backward passes of AI training. In this paper, we willdescribe the definition and rounding methods of HiF8, as well as the tentativetraining and inference solutions. To demonstrate the efficacy of HiF8, massivesimulation results on various neural networks, including traditional neuralnetworks and large language models (LLMs), will also be presented.</description><author>Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang</author><pubDate>Thu, 26 Sep 2024 16:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16626v2</guid></item><item><title>Automated Detection and Analysis of Power Words in Persuasive Text Using Natural Language Processing</title><link>http://arxiv.org/abs/2409.18033v1</link><description>Power words are terms that evoke strong emotional responses and significantlyinfluence readers' behavior, playing a crucial role in fields like marketing,politics, and motivational writing. This study proposes a methodology for theautomated detection and analysis of power words in persuasive text using acustom lexicon and the TextBlob library in Python. By identifying the presenceand frequency of power words within a given text, we aim to classify andanalyze their impact on sentiment and reader engagement. This research examinesdiverse datasets across various domains to provide insights into theeffectiveness of power words, offering practical applications for contentcreators, advertisers, and policymakers.</description><author>Sahil Garje</author><pubDate>Thu, 26 Sep 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18033v1</guid></item><item><title>FlowBench: A Large Scale Benchmark for Flow Simulation over Complex Geometries</title><link>http://arxiv.org/abs/2409.18032v1</link><description>Simulating fluid flow around arbitrary shapes is key to solving variousengineering problems. However, simulating flow physics across complexgeometries remains numerically challenging and computationallyresource-intensive, particularly when using conventional PDE solvers. Machinelearning methods offer attractive opportunities to create fast and adaptablePDE solvers. However, benchmark datasets to measure the performance of suchmethods are scarce, especially for flow physics across complex geometries. Weintroduce FlowBench, a dataset for neural simulators with over 10K samples,which is currently larger than any publicly available flow physics dataset.FlowBench contains flow simulation data across complex geometries(\textit{parametric vs. non-parametric}), spanning a range of flow conditions(\textit{Reynolds number and Grashoff number}), capturing a diverse array offlow phenomena (\textit{steady vs. transient; forced vs. free convection}), andfor both 2D and 3D. FlowBench contains over 10K data samples, with each samplethe outcome of a fully resolved, direct numerical simulation using awell-validated simulator framework designed for modeling transport phenomena incomplex geometries. For each sample, we include velocity, pressure, andtemperature field data at 3 different resolutions and several summarystatistics features of engineering relevance (such as coefficients of lift anddrag, and Nusselt numbers). %Additionally, we include masks and signed distancefields for each shape. We envision that FlowBench will enable evaluating theinterplay between complex geometry, coupled flow phenomena, and datasufficiency on the performance of current, and future, neural PDE solvers. Weenumerate several evaluation metrics to help rank order the performance ofneural PDE solvers. We benchmark the performance of several baseline methodsincluding FNO, CNO, WNO, and DeepONet.</description><author>Ronak Tali, Ali Rabeh, Cheng-Hau Yang, Mehdi Shadkhah, Samundra Karki, Abhisek Upadhyaya, Suriya Dhakshinamoorthy, Marjan Saadati, Soumik Sarkar, Adarsh Krishnamurthy, Chinmay Hegde, Aditya Balu, Baskar Ganapathysubramanian</author><pubDate>Thu, 26 Sep 2024 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18032v1</guid></item><item><title>Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of Peptides</title><link>http://arxiv.org/abs/2408.15126v5</link><description>Molecular Dynamics (MD) is crucial in various fields such as materialsscience, chemistry, and pharmacology to name a few. Conventional MD softwarestruggles with the balance between time cost and prediction accuracy, whichrestricts its wider application. Recently, data-driven approaches based on deepgenerative models have been devised for time-coarsened dynamics, which aim atlearning dynamics of diverse molecular systems over a long timestep, enjoyingboth universality and efficiency. Nevertheless, most current methods aredesigned solely to learn from the data distribution regardless of theunderlying Boltzmann distribution, and the physics priors such as energies andforces are constantly overlooked. In this work, we propose a conditionalgenerative model called Force-guided Bridge Matching (FBM), which learnsfull-atom time-coarsened dynamics and targets the Boltzmann-constraineddistribution. With the guidance of our delicately-designed intermediate forcefield, FBM leverages favourable physics priors into the generation process,giving rise to enhanced simulations. Experiments on two datasets consisting ofpeptides verify our superiority in terms of comprehensive metrics anddemonstrate transferability to unseen systems.</description><author>Ziyang Yu, Wenbing Huang, Yang Liu</author><pubDate>Thu, 26 Sep 2024 16:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15126v5</guid></item><item><title>Exploring Event-based Human Pose Estimation with 3D Event Representations</title><link>http://arxiv.org/abs/2311.04591v4</link><description>Human pose estimation is a fundamental and appealing task in computer vision.Although traditional cameras are commonly applied, their reliability decreasesin scenarios under high dynamic range or heavy motion blur, where event camerasoffer a robust solution. Predominant event-based methods accumulate events intoframes, ignoring the asynchronous and high temporal resolution that is crucialfor distinguishing distinct actions. To address this issue and to unlock the 3Dpotential of event information, we introduce two 3D event representations: theRasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). TheRasEPC aggregates events within concise temporal slices at identical positions,preserving their 3D attributes along with statistical information, therebysignificantly reducing memory and computational demands. Meanwhile, the DEVrepresentation discretizes events into voxels and projects them across threeorthogonal planes, utilizing decoupled event attention to retrieve 3D cues fromthe 2D planes. Furthermore, we develop and release EV-3DPW, a syntheticevent-based dataset crafted to facilitate training and quantitative analysis inoutdoor scenes. Our methods are tested on the DHP19 public dataset, MMHPSDdataset, and our EV-3DPW dataset, with further qualitative validation via aderived driving scene dataset EV-JAAD and an outdoor collection vehicle. Ourcode and dataset have been made publicly available athttps://github.com/MasterHow/EventPointPose.</description><author>Xiaoting Yin, Hao Shi, Jiaan Chen, Ze Wang, Yaozu Ye, Kailun Yang, Kaiwei Wang</author><pubDate>Thu, 26 Sep 2024 16:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04591v4</guid></item><item><title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title><link>http://arxiv.org/abs/2409.13731v3</link><description>The recently developed retrieval-augmented generation (RAG) technology hasenabled the efficient construction of domain-specific applications. However, italso has limitations, including the gap between vector similarity and therelevance of knowledge reasoning, as well as insensitivity to knowledge logic,such as numerical values, temporal relations, expert rules, and others, whichhinder the effectiveness of professional knowledge services. In this work, weintroduce a professional domain knowledge service framework called KnowledgeAugmented Generation (KAG). KAG is designed to address the aforementionedchallenges with the motivation of making full use of the advantages ofknowledge graph(KG) and vector retrieval, and to improve generation andreasoning performance by bidirectionally enhancing large language models (LLMs)and KGs through five key aspects: (1) LLM-friendly knowledge representation,(2) mutual-indexing between knowledge graphs and original chunks, (3)logical-form-guided hybrid reasoning engine, (4) knowledge alignment withsemantic reasoning, and (5) model capability enhancement for KAG. We comparedKAG with existing RAG methods in multihop question answering and found that itsignificantly outperforms state-of-theart methods, achieving a relativeimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. Wehave successfully applied KAG to two professional knowledge Q&amp;A tasks of AntGroup, including E-Government Q&amp;A and E-Health Q&amp;A, achieving significantimprovement in professionalism compared to RAG methods.</description><author>Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou</author><pubDate>Thu, 26 Sep 2024 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13731v3</guid></item><item><title>Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective</title><link>http://arxiv.org/abs/2409.18028v1</link><description>A common practice in large language model (LLM) usage for complex analyticaltasks such as code generation, is to sample a solution for the entire taskwithin the model's context window. Previous works have shown that subtaskdecomposition within the model's context (chain of thought), is beneficial forsolving such tasks. In this work, we point a limitation of LLMs' ability toperform several sub-tasks within the same context window - an in-contexthardness of composition, pointing to an advantage for distributing a decomposedproblem in a multi-agent system of LLMs. The hardness of composition isquantified by a generation complexity metric, i.e., the number of LLMgenerations required to sample at least one correct solution. We find a gapbetween the generation complexity of solving a compositional problem within thesame context relative to distributing it among multiple agents, that increasesexponentially with the solution's length. We prove our results theoreticallyand demonstrate them empirically.</description><author>Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua</author><pubDate>Thu, 26 Sep 2024 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18028v1</guid></item><item><title>ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty Learning</title><link>http://arxiv.org/abs/2409.18026v1</link><description>Vision-centric semantic occupancy prediction plays a crucial role inautonomous driving, which requires accurate and reliable predictions fromlow-cost sensors. Although having notably narrowed the accuracy gap with LiDAR,there is still few research effort to explore the reliability in predictingsemantic occupancy from camera. In this paper, we conduct a comprehensiveevaluation of existing semantic occupancy prediction models from a reliabilityperspective for the first time. Despite the gradual alignment of camera-basedmodels with LiDAR in term of accuracy, a significant reliability gap persists.To addresses this concern, we propose ReliOcc, a method designed to enhance thereliability of camera-based occupancy networks. ReliOcc provides aplug-and-play scheme for existing models, which integrates hybrid uncertaintyfrom individual voxels with sampling-based noise and relative voxels throughmix-up learning. Besides, an uncertainty-aware calibration strategy is devisedto further enhance model reliability in offline mode. Extensive experimentsunder various settings demonstrate that ReliOcc significantly enhances modelreliability while maintaining the accuracy of both geometric and semanticpredictions. Importantly, our proposed approach exhibits robustness to sensorfailures and out of domain noises during inference.</description><author>Song Wang, Zhongdao Wang, Jiawei Yu, Wentong Li, Bailan Feng, Junbo Chen, Jianke Zhu</author><pubDate>Thu, 26 Sep 2024 16:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18026v1</guid></item><item><title>An Adversarial Perspective on Machine Unlearning for AI Safety</title><link>http://arxiv.org/abs/2409.18025v1</link><description>Large language models are finetuned to refuse questions about hazardousknowledge, but these protections can often be bypassed. Unlearning methods aimat completely removing hazardous capabilities from models and make theminaccessible to adversaries. This work challenges the fundamental differencesbetween unlearning and traditional safety post-training from an adversarialperspective. We demonstrate that existing jailbreak methods, previouslyreported as ineffective against unlearning, can be successful when appliedcarefully. Furthermore, we develop a variety of adaptive methods that recovermost supposedly unlearned capabilities. For instance, we show that finetuningon 10 unrelated examples or removing specific directions in the activationspace can recover most hazardous capabilities for models edited with RMU, astate-of-the-art unlearning method. Our findings challenge the robustness ofcurrent unlearning approaches and question their advantages over safetytraining.</description><author>Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando</author><pubDate>Thu, 26 Sep 2024 16:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18025v1</guid></item><item><title>DARE: Diverse Visual Question Answering with Robustness Evaluation</title><link>http://arxiv.org/abs/2409.18023v1</link><description>Vision Language Models (VLMs) extend remarkable capabilities of text-onlylarge language models and vision-only models, and are able to learn from andprocess multi-modal vision-text input. While modern VLMs perform well on anumber of standard image classification and image-text matching tasks, theystill struggle with a number of crucial vision-language (VL) reasoningabilities such as counting and spatial reasoning. Moreover, while they might bevery brittle to small variations in instructions and/or evaluation protocols,existing benchmarks fail to evaluate their robustness (or rather the lack ofit). In order to couple challenging VL scenarios with comprehensive robustnessevaluation, we introduce DARE, Diverse Visual Question Answering withRobustness Evaluation, a carefully created and curated multiple-choice VQAbenchmark. DARE evaluates VLM performance on five diverse categories andincludes four robustness-oriented evaluations based on the variations of:prompts, the subsets of answer options, the output format and the number ofcorrect answers. Among a spectrum of other findings, we report thatstate-of-the-art VLMs still struggle with questions in most categories and areunable to consistently deliver their peak performance across the testedrobustness evaluations. The worst case performance across the subsets ofoptions is up to 34% below the performance in the standard case. The robustnessof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match theclosed-source models such as GPT-4 and Gemini, but even the latter remain verybrittle to different variations.</description><author>Hannah Sterz, Jonas Pfeiffer, Ivan Vulić</author><pubDate>Thu, 26 Sep 2024 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18023v1</guid></item><item><title>Transferring disentangled representations: bridging the gap between synthetic and real images</title><link>http://arxiv.org/abs/2409.18017v1</link><description>Developing meaningful and efficient representations that separate thefundamental structure of the data generation mechanism is crucial inrepresentation learning. However, Disentangled Representation Learning has notfully shown its potential on real images, because of correlated generativefactors, their resolution and limited access to ground truth labels.Specifically on the latter, we investigate the possibility of leveragingsynthetic data to learn general-purpose disentangled representations applicableto real data, discussing the effect of fine-tuning and what properties ofdisentanglement are preserved after the transfer. We provide an extensiveempirical study to address these issues. In addition, we propose a newinterpretable intervention-based metric, to measure the quality of factorsencoding in the representation. Our results indicate that some level ofdisentanglement, transferring a representation from synthetic to real data, ispossible and effective.</description><author>Jacopo Dapueto, Nicoletta Noceti, Francesca Odone</author><pubDate>Thu, 26 Sep 2024 16:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18017v1</guid></item><item><title>Synthesizing Environment-Specific People in Photographs</title><link>http://arxiv.org/abs/2312.14579v2</link><description>We present ESP, a novel method for context-aware full-body generation, thatenables photo-realistic synthesis and inpainting of people wearing clothingthat is semantically appropriate for the scene depicted in an input photograph.ESP is conditioned on a 2D pose and contextual cues that are extracted from thephotograph of the scene and integrated into the generation process, where theclothing is modeled explicitly with human parsing masks (HPM). Generated HPMsare used as tight guiding masks for inpainting, such that no changes are madeto the original background. Our models are trained on a dataset containing aset of in-the-wild photographs of people covering a wide range of differentenvironments. The method is analyzed quantitatively and qualitatively, and weshow that ESP outperforms the state-of-the-art on the task of contextualfull-body generation.</description><author>Mirela Ostrek, Carol O'Sullivan, Michael J. Black, Justus Thies</author><pubDate>Thu, 26 Sep 2024 16:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14579v2</guid></item><item><title>Relating Superconducting Optoelectronic Networks to Classical Neurodynamics</title><link>http://arxiv.org/abs/2409.18016v1</link><description>The circuits comprising superconducting optoelectronic synapses, dendrites,and neurons are described by numerically cumbersome and formally opaque coupleddifferential equations. Reference 1 showed that a phenomenological model ofsuperconducting loop neurons eliminates the need to solve the Josephson circuitequations that describe synapses and dendrites. The initial goal of the modelwas to decrease the time required for simulations, yet an additional benefit ofthe model was increased transparency of the underlying neural circuitoperations and conceptual clarity regarding the connection of loop neurons toother physical systems. Whereas the original model simplified the treatment ofthe Josephson-junction dynamics, essentially by only considering low-passversions of the dendritic outputs, the model resorted to an awkward treatmentof spikes generated by semiconductor transmitter circuits that requiredexplicitly checking for threshold crossings and distinct treatment of timesteps wherein somatic threshold is reached. Here we extend that model tosimplify the treatment of spikes coming from somas, again making use of thefact that in neural systems the downstream recipients of spike events almostalways perform low-pass filtering. We provide comparisons between the first andsecond phenomenological models, quantifying the accuracy of the additionalapproximations. We identify regions of circuit parameter space in which theextended model works well and regions where it works poorly. For some circuitparameters it is possible to represent the downstream dendritic response to asingle spike as well as coincidences or sequences of spikes, indicating themodel is not simply a reduction to rate coding. The governing equations areshown to be nearly identical to those ubiquitous in the neuroscience literaturefor modeling leaky-integrator dendrites and neurons.</description><author>Jeffrey M. Shainline, Bryce A. Primavera, Ryan O'Loughlin</author><pubDate>Thu, 26 Sep 2024 16:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18016v1</guid></item><item><title>Role-RL: Online Long-Context Processing with Role Reinforcement Learning for Distinct LLMs in Their Optimal Roles</title><link>http://arxiv.org/abs/2409.18014v1</link><description>Large language models (LLMs) with long-context processing are stillchallenging because of their implementation complexity, training efficiency anddata sparsity. To address this issue, a new paradigm named Online Long-contextProcessing (OLP) is proposed when we process a document of unlimited length,which typically occurs in the information reception and organization of diversestreaming media such as automated news reporting, live e-commerce, and viralshort videos. Moreover, a dilemma was often encountered when we tried to selectthe most suitable LLM from a large number of LLMs amidst explosive growthaiming for outstanding performance, affordable prices, and short responsedelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)to automatically deploy different LLMs in their respective roles within the OLPpipeline according to their actual performance. Extensive experiments areconducted on our OLP-MINI dataset and it is found that OLP with Role-RLframework achieves OLP benchmark with an average recall rate of 93.2% and theLLM cost saved by 79.4%. The code and dataset are publicly available at:https://anonymous.4open.science/r/Role-RL.</description><author>Lewei He, Tianyu Shi, Pengran Huang, Bingzhi Chen, Qianglong Chen, Jiahui Pan</author><pubDate>Thu, 26 Sep 2024 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18014v1</guid></item><item><title>Spatiotemporal Learning on Cell-embedded Graphs</title><link>http://arxiv.org/abs/2409.18013v1</link><description>Data-driven simulation of physical systems has recently kindled significantattention, where many neural models have been developed. In particular,mesh-based graph neural networks (GNNs) have demonstrated significant potentialin predicting spatiotemporal dynamics across arbitrary geometric domains.However, the existing node-edge message passing mechanism in GNNs limits themodel's representation learning ability. In this paper, we proposed acell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics withlifted performance. Specifically, we introduce a learnable cell attribution tothe node-edge message passing process, which better captures the spatialdependency of regional features. Such a strategy essentially upgrades the localaggregation scheme from the first order (e.g., from edge to node) to a higherorder (e.g., from volume to edge and then to node), which takes advantage ofvolumetric information in message passing. Meanwhile, a novel feature-enhancedblock is designed to further improve the performance of CeGNN and relieve theover-smoothness problem, via treating the latent features as basis functions.The extensive experiments on various PDE systems and one real-world datasetdemonstrate that CeGNN achieves superior performance compared with otherbaseline models, particularly reducing the prediction error with up to 1 ordersof magnitude on several PDE systems.</description><author>Yuan Mi, Hao Sun</author><pubDate>Thu, 26 Sep 2024 16:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18013v1</guid></item><item><title>End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data</title><link>http://arxiv.org/abs/2409.18010v1</link><description>In this paper we propose an end-to-end algorithm for indirect data-drivencontrol for bilinear systems with stability guarantees. We consider the casewhere the collected i.i.d. data is affected by probabilistic noise withpossibly unbounded support and leverage tools from statistical learning theoryto derive finite sample identification error bounds. To this end, we solve thebilinear identification problem by solving a set of linear and affineidentification problems, by a particular choice of a control input during thedata collection phase. We provide a priori as well as data-dependent finitesample identification error bounds on the individual matrices as well asellipsoidal bounds, both of which are structurally suitable for control.Further, we integrate the structure of the derived identification error boundsin a robust controller design to obtain an exponentially stable closed-loop. Bymeans of an extensive numerical study we showcase the interplay between thecontroller design and the derived identification error bounds. Moreover, wenote appealing connections of our results to indirect data-driven control ofgeneral nonlinear systems through Koopman operator theory and discuss how ourresults may be applied in this setup.</description><author>Nicolas Chatzikiriakos, Robin Strässer, Frank Allgöwer, Andrea Iannelli</author><pubDate>Thu, 26 Sep 2024 16:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18010v1</guid></item><item><title>Control Industrial Automation System with Large Language Models</title><link>http://arxiv.org/abs/2409.18009v1</link><description>Traditional industrial automation systems require specialized expertise tooperate and complex reprogramming to adapt to new processes. Large languagemodels offer the intelligence to make them more flexible and easier to use.However, LLMs' application in industrial settings is underexplored. This paperintroduces a framework for integrating LLMs to achieve end-to-end control ofindustrial automation systems. At the core of the framework are an agent systemdesigned for industrial tasks, a structured prompting method, and anevent-driven information modeling mechanism that provides real-time data forLLM inference. The framework supplies LLMs with real-time events on differentcontext semantic levels, allowing them to interpret the information, generateproduction plans, and control operations on the automation system. It alsosupports structured dataset creation for fine-tuning on this downstreamapplication of LLMs. Our contribution includes a formal system design,proof-of-concept implementation, and a method for generating task-specificdatasets for LLM fine-tuning and testing. This approach enables a more adaptiveautomation system that can respond to spontaneous events, while allowing easieroperation and configuration through natural language for more intuitivehuman-machine interaction. We provide demo videos and detailed data on GitHub:https://github.com/YuchenXia/LLM4IAS</description><author>Yuchen Xia, Nasser Jazdi, Jize Zhang, Chaitanya Shah, Michael Weyrich</author><pubDate>Thu, 26 Sep 2024 16:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18009v1</guid></item><item><title>Multilingual Evaluation of Long Context Retrieval and Reasoning</title><link>http://arxiv.org/abs/2409.18006v1</link><description>Recent large language models (LLMs) demonstrate impressive capabilities inhandling long contexts, some exhibiting near-perfect recall on syntheticretrieval tasks. However, these evaluations have mainly focused on English textand involved a single target sentence within lengthy contexts. Our workinvestigates how LLM performance generalizes to multilingual settings withmultiple hidden target sentences. We comprehensively evaluate severallong-context LLMs on retrieval and reasoning tasks across five languages:English, Vietnamese, Indonesian, Swahili, and Somali. These languages share theLatin script but belong to distinct language families and resource levels. Ouranalysis reveals a significant performance gap between languages. Thebest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%accuracy in English to around 36% in Somali with a single target sentence.However, this accuracy drops to 40% in English and 0% in Somali when dealingwith three target sentences. Our findings highlight the challenges long-contextLLMs face when processing longer contexts, an increase in the number of targetsentences, or languages of lower resource levels.</description><author>Ameeta Agrawal, Andy Dang, Sina Bagheri Nezhad, Rhitabrat Pokharel, Russell Scheinberg</author><pubDate>Thu, 26 Sep 2024 16:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18006v1</guid></item><item><title>Valeo4Cast: A Modular Approach to End-to-End Forecasting</title><link>http://arxiv.org/abs/2406.08113v3</link><description>Motion forecasting is crucial in autonomous driving systems to anticipate thefuture trajectories of surrounding agents such as pedestrians, vehicles, andtraffic signals. In end-to-end forecasting, the model must jointly detect andtrack from sensor data (cameras or LiDARs) the past trajectories of thedifferent elements of the scene and predict their future locations. We departfrom the current trend of tackling this task via end-to-end training fromperception to forecasting, and instead use a modular approach. We individuallybuild and train detection, tracking and forecasting modules. We then only useconsecutive finetuning steps to integrate the modules better and alleviatecompounding errors. We conduct an in-depth study on the finetuning strategiesand it reveals that our simple yet effective approach significantly improvesperformance on the end-to-end forecasting benchmark. Consequently, our solutionranks first in the Argoverse 2 End-to-end Forecasting Challenge, with 63.82mAPf. We surpass forecasting results by +17.1 points over last year's winnerand by +13.3 points over this year's runner-up. This remarkable performance inforecasting can be explained by our modular paradigm, which integratesfinetuning strategies and significantly outperforms the end-to-end-trainedcounterparts. The code, model weights and results are made availablehttps://github.com/valeoai/valeo4cast.</description><author>Yihong Xu, Éloi Zablocki, Alexandre Boulch, Gilles Puy, Mickael Chen, Florent Bartoccioni, Nermin Samet, Oriane Siméoni, Spyros Gidaris, Tuan-Hung Vu, Andrei Bursuc, Eduardo Valle, Renaud Marlet, Matthieu Cord</author><pubDate>Thu, 26 Sep 2024 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08113v3</guid></item><item><title>Disentangled Clothed Avatar Generation from Text Descriptions</title><link>http://arxiv.org/abs/2312.05295v2</link><description>In this paper, we introduce a novel text-to-avatar generation method thatseparately generates the human body and the clothes and allows high-qualityanimation on the generated avatar. While recent advancements in text-to-avatargeneration have yielded diverse human avatars from text prompts, these methodstypically combine all elements-clothes, hair, and body-into a single 3Drepresentation. Such an entangled approach poses challenges for downstreamtasks like editing or animation. To overcome these limitations, we propose anovel disentangled 3D avatar representation named Sequentially Offset-SMPL(SO-SMPL), building upon the SMPL model. SO-SMPL represents the human body andclothes with two separate meshes but associates them with offsets to ensure thephysical alignment between the body and the clothes. Then, we design a ScoreDistillation Sampling (SDS)-based distillation framework to generate theproposed SO-SMPL representation from text prompts. Our approach not onlyachieves higher texture and geometry quality and better semantic alignment withtext prompts, but also significantly improves the visual quality of characteranimation, virtual try-on, and avatar editing. Project page:https://shanemankiw.github.io/SO-SMPL/.</description><author>Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, Li Song</author><pubDate>Thu, 26 Sep 2024 16:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05295v2</guid></item><item><title>Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel</title><link>http://arxiv.org/abs/2409.18000v1</link><description>Ensuring safety is a key aspect in sequential decision making problems, suchas robotics or process control. The complexity of the underlying systems oftenmakes finding the optimal decision challenging, especially when thesafety-critical system is time-varying. Overcoming the problem of optimizing anunknown time-varying reward subject to unknown time-varying safety constraints,we propose TVSafeOpt, a new algorithm built on Bayesian optimization with aspatio-temporal kernel. The algorithm is capable of safely tracking atime-varying safe region without the need for explicit change detection.Optimality guarantees are also provided for the algorithm when the optimizationproblem becomes stationary. We show that TVSafeOpt compares favorably againstSafeOpt on synthetic data, both regarding safety and optimality. Evaluation ona realistic case study with gas compressors confirms that TVSafeOpt ensuressafety when solving time-varying optimization problems with unknown reward andsafety functions.</description><author>Jialin Li, Marta Zagorowska, Giulia De Pasquale, Alisa Rupenyan, John Lygeros</author><pubDate>Thu, 26 Sep 2024 16:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18000v1</guid></item><item><title>PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging</title><link>http://arxiv.org/abs/2409.17996v1</link><description>Lensless cameras offer significant advantages in size, weight, and costcompared to traditional lens-based systems. Without a focusing lens, lenslesscameras rely on computational algorithms to recover the scenes from multiplexedmeasurements. However, current algorithms struggle with inaccurate forwardimaging models and insufficient priors to reconstruct high-quality images. Toovercome these limitations, we introduce a novel two-stage approach forconsistent and photorealistic lensless image reconstruction. The first stage ofour approach ensures data consistency by focusing on accurately reconstructingthe low-frequency content with a spatially varying deconvolution method thatadjusts to changes in the Point Spread Function (PSF) across the camera's fieldof view. The second stage enhances photorealism by incorporating a generativeprior from pre-trained diffusion models. By conditioning on the low-frequencycontent retrieved in the first stage, the diffusion model effectivelyreconstructs the high-frequency details that are typically lost in the lenslessimaging process, while also maintaining image fidelity. Our method achieves asuperior balance between data fidelity and visual quality compared to existingmethods, as demonstrated with two popular lensless systems, PhlatCam andDiffuserCam. Project website: https://phocolens.github.io/.</description><author>Xin Cai, Zhiyuan You, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue</author><pubDate>Thu, 26 Sep 2024 16:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17996v1</guid></item><item><title>Joint Localization and Planning using Diffusion</title><link>http://arxiv.org/abs/2409.17995v1</link><description>Diffusion models have been successfully applied to robotics problems such asmanipulation and vehicle path planning. In this work, we explore theirapplication to end-to-end navigation -- including both perception and planning-- by considering the problem of jointly performing global localization andpath planning in known but arbitrary 2D environments. In particular, weintroduce a diffusion model which produces collision-free paths in a globalreference frame given an egocentric LIDAR scan, an arbitrary map, and a desiredgoal position. To this end, we implement diffusion in the space of paths inSE(2), and describe how to condition the denoising process on both obstaclesand sensor observations. In our evaluation, we show that the proposedconditioning techniques enable generalization to realistic maps of considerablydifferent appearance than the training environment, demonstrate our model'sability to accurately describe ambiguous solutions, and run extensivesimulation experiments showcasing our model's use as a real-time, end-to-endlocalization and planning stack.</description><author>L. Lao Beyer, S. Karaman</author><pubDate>Thu, 26 Sep 2024 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17995v1</guid></item><item><title>CRoP: Context-wise Robust Static Human-Sensing Personalization</title><link>http://arxiv.org/abs/2409.17994v1</link><description>The advancement in deep learning and internet-of-things have led to diversehuman sensing applications. However, distinct patterns in human sensing,influenced by various factors or contexts, challenge generic neural networkmodel's performance due to natural distribution shifts. To address this,personalization tailors models to individual users. Yet most personalizationstudies overlook intra-user heterogeneity across contexts in sensory data,limiting intra-user generalizability. This limitation is especially critical inclinical applications, where limited data availability hampers bothgeneralizability and personalization. Notably, intra-user sensing attributesare expected to change due to external factors such as treatment progression,further complicating the challenges.This work introduces CRoP, a novel staticpersonalization approach using an off-the-shelf pre-trained model and pruningto optimize personalization and generalization. CRoP shows superiorpersonalization effectiveness and intra-user robustness across fourhuman-sensing datasets, including two from real-world health domains,highlighting its practical and social impact. Additionally, to support CRoP'sgeneralization ability and design choices, we provide empirical justificationthrough gradient inner product analysis, ablation studies, and comparisonsagainst state-of-the-art baselines.</description><author>Sawinder Kaur, Avery Gump, Jingyu Xin, Yi Xiao, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin</author><pubDate>Thu, 26 Sep 2024 16:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17994v1</guid></item><item><title>MLPs Learn In-Context on Regression and Classification Tasks</title><link>http://arxiv.org/abs/2405.15618v2</link><description>In-context learning (ICL), the remarkable ability to solve a task from onlyinput exemplars, is often assumed to be a unique hallmark of Transformermodels. By examining commonly employed synthetic ICL tasks, we demonstrate thatmulti-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, andthe closely related MLP-Mixer models, learn in-context competitively withTransformers given the same compute budget in this setting. We further showthat MLPs outperform Transformers on a series of classical tasks frompsychology designed to test relational reasoning, which are closely related toin-context classification. These results underscore a need for studyingin-context learning beyond attention-based architectures, while alsochallenging strong prior arguments about MLPs' limited ability to solverelational tasks. Altogether, our results highlight the unexpected competenceof MLPs, and support the growing interest in all-MLP alternatives totask-specific architectures.</description><author>William L. Tong, Cengiz Pehlevan</author><pubDate>Thu, 26 Sep 2024 16:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15618v2</guid></item><item><title>InterNet: Unsupervised Cross-modal Homography Estimation Based on Interleaved Modality Transfer and Self-supervised Homography Prediction</title><link>http://arxiv.org/abs/2409.17993v1</link><description>We propose a novel unsupervised cross-modal homography estimation framework,based on interleaved modality transfer and self-supervised homographyprediction, named InterNet. InterNet integrates modality transfer andself-supervised homography estimation, introducing an innovative interleavedoptimization framework to alternately promote both components. The modalitytransfer gradually narrows the modality gaps, facilitating the self-supervisedhomography estimation to fully leverage the synthetic intra-modal data. Theself-supervised homography estimation progressively achieves reliablepredictions, thereby providing robust cross-modal supervision for the modalitytransfer. To further boost the estimation accuracy, we also formulate afine-grained homography feature loss to improve the connection between twocomponents. Furthermore, we employ a simple yet effective distillation trainingtechnique to reduce model parameters and improve cross-domain generalizationability while maintaining comparable performance. Experiments reveal thatInterNet achieves the state-of-the-art (SOTA) performance among unsupervisedmethods, and even outperforms many supervised methods such as MHN andLocalTrans.</description><author>Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Jianxin Hu, Zhu Yu, Hui-liang Shen</author><pubDate>Thu, 26 Sep 2024 16:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17993v1</guid></item><item><title>LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots</title><link>http://arxiv.org/abs/2409.17992v1</link><description>Reinforcement Learning (RL) has shown its remarkable and generalizablecapability in legged locomotion through sim-to-real transfer. However, whileadaptive methods like domain randomization are expected to make policy morerobust to diverse environments, such comprehensiveness potentially detractsfrom the policy's performance in any specific environment according to the NoFree Lunch theorem, leading to a suboptimal solution once deployed in the realworld. To address this issue, we propose a lifelong policy adaptation frameworknamed LoopSR, which utilizes a transformer-based encoder to project real-worldtrajectories into a latent space, and accordingly reconstruct the real-worldenvironments back in simulation for further improvement. Autoencoderarchitecture and contrastive learning methods are adopted to better extract thecharacteristics of real-world dynamics. The simulation parameters for continualtraining are derived by combining predicted parameters from the decoder withretrieved parameters from the simulation trajectory dataset. By leveraging thecontinual training, LoopSR achieves superior data efficiency compared withstrong baselines, with only a limited amount of data to yield eminentperformance in both sim-to-sim and sim-to-real experiments.</description><author>Peilin Wu, Weiji Xie, Jiahang Cao, Hang Lai, Weinan Zhang</author><pubDate>Thu, 26 Sep 2024 16:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17992v1</guid></item><item><title>Dimension-independent learning rates for high-dimensional classification problems</title><link>http://arxiv.org/abs/2409.17991v1</link><description>We study the problem of approximating and estimating classification functionsthat have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$type arise naturally as solutions of regularized neural network learningproblems and neural networks can approximate these functions without the curseof dimensionality. We modify existing results to show that every $RBV^2$function can be approximated by a neural network with bounded weights.Thereafter, we prove the existence of a neural network with bounded weightsapproximating a classification function. And we leverage these bounds toquantify the estimation rates. Finally, we present a numerical study thatanalyzes the effect of different regularity conditions on the decisionboundaries.</description><author>Andres Felipe Lerma-Pineda, Philipp Petersen, Simon Frieder, Thomas Lukasiewicz</author><pubDate>Thu, 26 Sep 2024 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17991v1</guid></item><item><title>Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models</title><link>http://arxiv.org/abs/2409.17990v1</link><description>This paper proposes temporally aligned Large Language Models (LLMs) as a toolfor longitudinal analysis of social media data. We fine-tune Temporal Adaptersfor Llama 3 8B on full timelines from a panel of British Twitter users, andextract longitudinal aggregates of emotions and attitudes with establishedquestionnaires. We validate our estimates against representative British surveydata and find strong positive, significant correlations for several collectiveemotions. The obtained estimates are robust across multiple training seeds andprompt formulations, and in line with collective emotions extracted using atraditional classification model trained on labeled data. To the best of ourknowledge, this is the first work to extend the analysis of affect in LLMs to alongitudinal setting through Temporal Adapters. Our work enables new approachestowards the longitudinal analysis of social media data.</description><author>Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier</author><pubDate>Thu, 26 Sep 2024 16:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17990v1</guid></item><item><title>Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</title><link>http://arxiv.org/abs/2409.17988v1</link><description>The stark contrast in the design philosophy of an event camera makes itparticularly ideal for operating under high-speed, high dynamic range andlow-light conditions, where standard cameras underperform. Nonetheless, eventcameras still suffer from some amount of motion blur, especially under thesechallenging conditions, in contrary to what most think. This is attributed tothe limited bandwidth of the event sensor pixel, which is mostly proportionalto the light intensity. Thus, to ensure that event cameras can truly excel insuch conditions where it has an edge over standard cameras, it is crucial toaccount for event motion blur in downstream applications, especiallyreconstruction. However, none of the recent works on reconstructing NeuralRadiance Fields (NeRFs) from events, nor event simulators, have considered thefull effects of event motion blur. To this end, we propose, Deblur e-NeRF, anovel method to directly and effectively reconstruct blur-minimal NeRFs frommotion-blurred events generated under high-speed motion or low-lightconditions. The core component of this work is a physically-accurate pixelbandwidth model proposed to account for event motion blur under arbitrary speedand lighting conditions. We also introduce a novel threshold-normalized totalvariation loss to improve the regularization of large textureless patches.Experiments on real and novel realistically simulated sequences verify oureffectiveness. Our code, event simulator and synthetic event dataset will beopen-sourced.</description><author>Weng Fei Low, Gim Hee Lee</author><pubDate>Thu, 26 Sep 2024 15:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17988v1</guid></item><item><title>LLM4Brain: Training a Large Language Model for Brain Video Understanding</title><link>http://arxiv.org/abs/2409.17987v1</link><description>Decoding visual-semantic information from brain signals, such as functionalMRI (fMRI), across different subjects poses significant challenges, includinglow signal-to-noise ratio, limited data availability, and cross-subjectvariability. Recent advancements in large language models (LLMs) showremarkable effectiveness in processing multimodal information. In this study,we introduce an LLM-based approach for reconstructing visual-semanticinformation from fMRI signals elicited by video stimuli. Specifically, weemploy fine-tuning techniques on an fMRI encoder equipped with adaptors totransform brain responses into latent representations aligned with the videostimuli. Subsequently, these representations are mapped to textual modality byLLM. In particular, we integrate self-supervised domain adaptation methods toenhance the alignment between visual-semantic information and brain responses.Our proposed method achieves good results using various quantitative semanticmetrics, while yielding similarity with ground-truth information.</description><author>Ruizhe Zheng, Lichao Sun</author><pubDate>Thu, 26 Sep 2024 15:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17987v1</guid></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>http://arxiv.org/abs/2409.17986v1</link><description>Fully connected Graph Transformers (GT) have rapidly become prominent in thestatic graph community as an alternative to Message-Passing models, whichsuffer from a lack of expressivity, oversquashing, and under-reaching. However,in a dynamic context, by interconnecting all nodes at multiple snapshots withself-attention, GT loose both structural and temporal information. In thiswork, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs(SLATE), a new spatio-temporal encoding to leverage the GT architecture whilekeeping spatio-temporal information. Specifically, we transform Discrete TimeDynamic Graphs into multi-layer graphs and take advantage of the spectralproperties of their associated supra-Laplacian matrix. Our second contributionexplicitly model nodes' pairwise relationships with a cross-attentionmechanism, providing an accurate edge representation for dynamic linkprediction. SLATE outperforms numerous state-of-the-art methods based onMessage-Passing Graph Neural Networks combined with recurrent models (e.gLSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions toreproduce our results will be open-sourced.</description><author>Yannis Karmim, Marc Lafon, Raphaël Fournier S'niehotta, Nicolas Thome</author><pubDate>Thu, 26 Sep 2024 15:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17986v1</guid></item><item><title>A Stochastic Quasi-Newton Method for Non-convex Optimization with Non-uniform Smoothness</title><link>http://arxiv.org/abs/2403.15244v2</link><description>Classical convergence analyses for optimization algorithms rely on thewidely-adopted uniform smoothness assumption. However, recent experimentalstudies have demonstrated that many machine learning problems exhibitnon-uniform smoothness, meaning the smoothness factor is a function of themodel parameter instead of a universal constant. In particular, it has beenobserved that the smoothness grows with respect to the gradient norm along thetraining trajectory. Motivated by this phenomenon, the recently introduced$(L_0, L_1)$-smoothness is a more general notion, compared to traditional$L$-smoothness, that captures such positive relationship between smoothness andgradient norm. Under this type of non-uniform smoothness, existing literaturehas designed stochastic first-order algorithms by utilizing gradient clippingtechniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexityfor finding an $\epsilon$-approximate first-order stationary solution.Nevertheless, the studies of quasi-Newton methods are still lacking.Considering higher accuracy and more robustness for quasi-Newton methods, inthis paper we propose a fast stochastic quasi-Newton method when there existsnon-uniformity in smoothness. Leveraging gradient clipping and variancereduction, our algorithm can achieve the best-known$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedupwith simple hyperparameter tuning. Our numerical experiments show that ourproposed algorithm outperforms the state-of-the-art approaches.</description><author>Zhenyu Sun, Ermin Wei</author><pubDate>Thu, 26 Sep 2024 15:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15244v2</guid></item><item><title>HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions</title><link>http://arxiv.org/abs/2409.16427v2</link><description>AI agents are increasingly autonomous in their interactions with human usersand tools, leading to increased interactional safety risks. We presentHAICOSYSTEM, a framework examining AI agent safety within diverse and complexsocial interactions. HAICOSYSTEM features a modular sandbox environment thatsimulates multi-turn interactions between human users and AI agents, where theAI agents are equipped with a variety of tools (e.g., patient managementplatforms) to navigate diverse scenarios (e.g., a user attempting to accessother patients' profiles). To examine the safety of AI agents in theseinteractions, we develop a comprehensive multi-dimensional evaluation frameworkthat uses metrics covering operational, content-related, societal, and legalrisks. Through running 1840 simulations based on 92 scenarios across sevendomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEMcan emulate realistic user-AI interactions and complex tool use by AI agents.Our experiments show that state-of-the-art LLMs, both proprietary andopen-sourced, exhibit safety risks in over 50\% cases, with models generallyshowing higher risks when interacting with simulated malicious users. Ourfindings highlight the ongoing challenge of building agents that can safelynavigate complex interactions, particularly when faced with malicious users. Tofoster the AI agent safety ecosystem, we release a code platform that allowspractitioners to create custom scenarios, simulate interactions, and evaluatethe safety and performance of their agents.</description><author>Xuhui Zhou, Hyunwoo Kim, Faeze Brahman, Liwei Jiang, Hao Zhu, Ximing Lu, Frank Xu, Bill Yuchen Lin, Yejin Choi, Niloofar Mireshghallah, Ronan Le Bras, Maarten Sap</author><pubDate>Thu, 26 Sep 2024 15:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16427v2</guid></item><item><title>Hypergame Theory for Decentralized Resource Allocation in Multi-user Semantic Communications</title><link>http://arxiv.org/abs/2409.17985v1</link><description>Semantic communications (SC) is an emerging communication paradigm in whichwireless devices can send only relevant information from a source of data whilerelying on computing resources to regenerate missing data points. However, thedesign of a multi-user SC system becomes more challenging because of thecomputing and communication overhead required for coordination. Existingsolutions for learning the semantic language and performing resource allocationoften fail to capture the computing and communication tradeoffs involved inmultiuser SC. To address this gap, a novel framework for decentralizedcomputing and communication resource allocation in multiuser SC systems isproposed. The challenge of efficiently allocating communication and computingresources (for reasoning) in a decentralized manner to maximize the quality oftask experience for the end users is addressed through the application ofStackelberg hyper game theory. Leveraging the concept of second-level hypergames, novel analytical formulations are developed to model misperceptions ofthe users about each other's communication and control strategies. Further,equilibrium analysis of the learned resource allocation protocols examines theconvergence of the computing and communication strategies to a localStackelberg equilibria, considering misperceptions. Simulation results showthat the proposed Stackelberg hyper game results in efficient usage ofcommunication and computing resources while maintaining a high quality ofexperience for the users compared to state-of-the-art that does not account forthe misperceptions.</description><author>Christo Kurisummoottil Thomas, Walid Saad</author><pubDate>Thu, 26 Sep 2024 15:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17985v1</guid></item><item><title>BlinkTrack: Feature Tracking over 100 FPS via Events and Images</title><link>http://arxiv.org/abs/2409.17981v1</link><description>Feature tracking is crucial for, structure from motion (SFM), simultaneouslocalization and mapping (SLAM), object tracking and various computer visiontasks. Event cameras, known for their high temporal resolution and ability tocapture asynchronous changes, have gained significant attention for theirpotential in feature tracking, especially in challenging conditions. However,event cameras lack the fine-grained texture information that conventionalcameras provide, leading to error accumulation in tracking. To address this, wepropose a novel framework, BlinkTrack, which integrates event data with RGBimages for high-frequency feature tracking. Our method extends the traditionalKalman filter into a learning-based framework, utilizing differentiable Kalmanfilters in both event and image branches. This approach improvessingle-modality tracking, resolves ambiguities, and supports asynchronous datafusion. We also introduce new synthetic and augmented datasets to betterevaluate our model. Experimental results indicate that BlinkTrack significantlyoutperforms existing event-based methods, exceeding 100 FPS with preprocessedevent data and 80 FPS with multi-modality data.</description><author>Yichen Shen, Yijin Li, Shuo Chen, Guanglin Li, Zhaoyang Huang, Hujun Bao, Zhaopeng Cui, Guofeng Zhang</author><pubDate>Thu, 26 Sep 2024 15:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17981v1</guid></item><item><title>Message-Passing Monte Carlo: Generating low-discrepancy point sets via Graph Neural Networks</title><link>http://arxiv.org/abs/2405.15059v2</link><description>Discrepancy is a well-known measure for the irregularity of the distributionof a point set. Point sets with small discrepancy are called low-discrepancyand are known to efficiently fill the space in a uniform manner.Low-discrepancy points play a central role in many problems in science andengineering, including numerical integration, computer vision, machineperception, computer graphics, machine learning, and simulation. In this work,we present the first machine learning approach to generate a new class oflow-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points.Motivated by the geometric nature of generating low-discrepancy point sets, weleverage tools from Geometric Deep Learning and base our model on Graph NeuralNetworks. We further provide an extension of our framework to higherdimensions, which flexibly allows the generation of custom-made points thatemphasize the uniformity in specific dimensions that are primarily importantfor the particular problem at hand. Finally, we demonstrate that our proposedmodel achieves state-of-the-art performance superior to previous methods by asignificant margin. In fact, MPMC points are empirically shown to be eitheroptimal or near-optimal with respect to the discrepancy for low dimension andsmall number of points, i.e., for which the optimal discrepancy can bedetermined. Code for generating MPMC points can be found athttps://github.com/tk-rusch/MPMC.</description><author>T. Konstantin Rusch, Nathan Kirk, Michael M. Bronstein, Christiane Lemieux, Daniela Rus</author><pubDate>Thu, 26 Sep 2024 15:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15059v2</guid></item><item><title>HydraViT: Stacking Heads for a Scalable ViT</title><link>http://arxiv.org/abs/2409.17978v1</link><description>The architecture of Vision Transformers (ViTs), particularly the Multi-headAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTson devices with varying constraints, such as mobile phones, requires multiplemodels of different sizes. However, this approach has limitations, such astraining and storing each required model separately. This paper introducesHydraViT, a novel approach that addresses these limitations by stackingattention heads to achieve a scalable ViT. By repeatedly changing the size ofthe embedded dimensions throughout each layer and their corresponding number ofattention heads in MHA during training, HydraViT induces multiple subnetworks.Thereby, HydraViT achieves adaptability across a wide spectrum of hardwareenvironments while maintaining performance. Our experimental resultsdemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10subnetworks, covering a wide range of resource constraints. HydraViT achievesup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracywith the same throughput on ImageNet-1K compared to the baselines, making it aneffective solution for scenarios where hardware availability is diverse orvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.</description><author>Janek Haberer, Ali Hojjat, Olaf Landsiedel</author><pubDate>Thu, 26 Sep 2024 15:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17978v1</guid></item><item><title>Cross-Modality Attack Boosted by Gradient-Evolutionary Multiform Optimization</title><link>http://arxiv.org/abs/2409.17977v1</link><description>In recent years, despite significant advancements in adversarial attackresearch, the security challenges in cross-modal scenarios, such as thetransferability of adversarial attacks between infrared, thermal, and RGBimages, have been overlooked. These heterogeneous image modalities collected bydifferent hardware devices are widely prevalent in practical applications, andthe substantial differences between modalities pose significant challenges toattack transferability. In this work, we explore a novel cross-modaladversarial attack strategy, termed multiform attack. We propose a dual-layeroptimization framework based on gradient-evolution, facilitating efficientperturbation transfer between modalities. In the first layer of optimization,the framework utilizes image gradients to learn universal perturbations withineach modality and employs evolutionary algorithms to search for sharedperturbations with transferability across different modalities throughsecondary optimization. Through extensive testing on multiple heterogeneousdatasets, we demonstrate the superiority and robustness of Multiform Attackcompared to existing techniques. This work not only enhances thetransferability of cross-modal adversarial attacks but also provides a newperspective for understanding security vulnerabilities in cross-modal systems.</description><author>Yunpeng Gong, Qingyuan Zeng, Dejun Xu, Zhenzhong Wang, Min Jiang</author><pubDate>Thu, 26 Sep 2024 15:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17977v1</guid></item><item><title>BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search</title><link>http://arxiv.org/abs/2409.17972v1</link><description>Large Language Models (LLMs) have exhibited exceptional performance across abroad range of tasks and domains. However, they still encounter difficulties insolving mathematical problems due to the rigorous and logical nature ofmathematics. Previous studies have employed techniques such as supervisedfine-tuning (SFT), prompt engineering, and search-based methods to improve themathematical problem-solving abilities of LLMs. Despite these efforts, theirperformance remains suboptimal and demands substantial computational resources.To address this issue, we propose a novel approach, BEATS, to enhancemathematical problem-solving abilities. Our method leverages newly designedprompts that guide the model to iteratively rewrite, advance by one step, andgenerate answers based on previous steps. Additionally, we introduce a newback-verification technique that uses LLMs to validate the correctness of thegenerated answers. Furthermore, we employ a pruning tree search to optimizesearch time while achieving strong performance. Notably, our method improvesQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on theMATH benchmark.</description><author>Linzhuang Sun, Hao Liang, Wentao Zhang</author><pubDate>Thu, 26 Sep 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17972v1</guid></item><item><title>TypeFly: Flying Drones with Large Language Model</title><link>http://arxiv.org/abs/2312.14950v2</link><description>Recent advancements in robot control using large language models (LLMs) havedemonstrated significant potential, primarily due to LLMs' capabilities tounderstand natural language commands and generate executable plans in variouslanguages. However, in real-time and interactive applications involving mobilerobots, particularly drones, the sequential token generation process inherentto LLMs introduces substantial latency, i.e. response time, in control plangeneration. In this paper, we present a system called ChatFly that tackles this problemusing a combination of a novel programming language called MiniSpec and itsruntime to reduce the plan generation time and drone response time. That is,instead of asking an LLM to write a program (robotic plan) in the popular butverbose Python, ChatFly gets it to do it in MiniSpec specially designed fortoken efficiency and stream interpretation. Using a set of challenging dronetasks, we show that design choices made by ChatFly can reduce up to 62%response time and provide a more consistent user experience, enablingresponsive and intelligent LLM-based drone control with efficient completion.</description><author>Guojun Chen, Xiaojing Yu, Neiwen Ling, Lin Zhong</author><pubDate>Thu, 26 Sep 2024 15:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14950v2</guid></item><item><title>CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors</title><link>http://arxiv.org/abs/2409.17963v1</link><description>Prior works on physical adversarial camouflage against vehicle detectorsmainly focus on the effectiveness and robustness of the attack. The currentmost successful methods optimize 3D vehicle texture at a pixel level. However,this results in conspicuous and attention-grabbing patterns in the generatedcamouflage, which humans can easily identify. To address this issue, we proposea Customizable and Natural Camouflage Attack (CNCA) method by leveraging anoff-the-shelf pre-trained diffusion model. By sampling the optimal textureimage from the diffusion model with a user-specific text prompt, our method cangenerate natural and customizable adversarial camouflage while maintaining highattack performance. With extensive experiments on the digital and physicalworlds and user studies, the results demonstrate that our proposed method cangenerate significantly more natural-looking camouflage than thestate-of-the-art baselines while achieving competitive attack performance. Ourcode is available at\href{https://anonymous.4open.science/r/CNCA-1D54}{https://anonymous.4open.science/r/CNCA-1D54}</description><author>Linye Lyu, Jiawei Zhou, Daojing He, Yu Li</author><pubDate>Thu, 26 Sep 2024 15:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17963v1</guid></item><item><title>Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers</title><link>http://arxiv.org/abs/2308.10814v3</link><description>Quantization scale and bit-width are the most important parameters whenconsidering how to quantize a neural network. Prior work focuses on optimizingquantization scales in a global manner through gradient methods (gradientdescent \&amp; Hessian analysis). Yet, when applying perturbations to quantizationscales, we observe a very jagged, highly non-smooth test loss landscape. Infact, small perturbations in quantization scale can greatly affect accuracy,yielding a $0.5-0.8\%$ accuracy boost in 4-bit quantized vision transformers(ViTs). In this regime, gradient methods break down, since they cannot reliablyreach local minima. In our work, dubbed Evol-Q, we use evolutionary search toeffectively traverse the non-smooth landscape. Additionally, we propose usingan infoNCE loss, which not only helps combat overfitting on the smallcalibration dataset ($1,000$ images) but also makes traversing such a highlynon-smooth surface easier. Evol-Q improves the top-1 accuracy of a fullyquantized ViT-Base by $10.30\%$, $0.78\%$, and $0.15\%$ for $3$-bit, $4$-bit,and $8$-bit weight quantization levels. Extensive experiments on a variety ofCNN and ViT architectures further demonstrate its robustness in extremequantization scenarios. Our code is available athttps://github.com/enyac-group/evol-q</description><author>Natalia Frumkin, Dibakar Gope, Diana Marculescu</author><pubDate>Thu, 26 Sep 2024 15:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10814v3</guid></item><item><title>The Hard Positive Truth about Vision-Language Compositionality</title><link>http://arxiv.org/abs/2409.17958v1</link><description>Several benchmarks have concluded that our best vision-language models (e.g.,CLIP) are lacking in compositionality. Given an image, these benchmarks probe amodel's ability to identify its associated caption amongst a set ofcompositional distractors. In response, a surge of recent proposals showimprovements by finetuning CLIP with distractors as hard negatives. Ourinvestigations reveal that these improvements have, in fact, been significantlyoverstated -- because existing benchmarks do not probe whether finetunedvision-language models remain invariant to hard positives. By curating anevaluation dataset with 112,382 hard negatives and hard positives, we uncoverthat including hard positives decreases CLIP's performance by 12.9%, whilehumans perform effortlessly at 99%. CLIP finetuned with hard negatives resultsin an even larger decrease, up to 38.7%. With this finding, we then produce a1,775,259 image-text training set with both hard negative and hard positivecaptions. By training with both, we see improvements on existing benchmarkswhile simultaneously improving performance on hard positives, indicating a morerobust improvement in compositionality. Our work suggests the need for futureresearch to rigorously test and improve CLIP's understanding of semanticrelationships between related "positive" concepts.</description><author>Amita Kamath, Cheng-Yu Hsieh, Kai-Wei Chang, Ranjay Krishna</author><pubDate>Thu, 26 Sep 2024 15:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17958v1</guid></item><item><title>Recent Trends in Unsupervised Summarization</title><link>http://arxiv.org/abs/2305.11231v2</link><description>Unsupervised summarization is a powerful technique that enables trainingsummarizing models without requiring labeled datasets. This survey coversdifferent recent techniques and models used for unsupervised summarization. Wecover extractive, abstractive, and hybrid models and strategies used to achieveunsupervised summarization. While the main focus of this survey is on recentresearch, we also cover some of the important previous research. Weadditionally introduce a taxonomy, classifying different research based ontheir approach to unsupervised training. Finally, we discuss the currentapproaches and mention some datasets and evaluation methods.</description><author>Mohammad Khosravani, Amine Trabelsi</author><pubDate>Thu, 26 Sep 2024 15:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11231v2</guid></item><item><title>Enhancing elusive clues in knowledge learning by contrasting attention of language models</title><link>http://arxiv.org/abs/2409.17954v1</link><description>Causal language models acquire vast amount of knowledge from general textcorpus during pretraining, but the efficiency of knowledge learning is known tobe unsatisfactory, especially when learning from knowledge-dense andsmall-sized corpora. The deficiency can come from long-distance dependencieswhich are hard to capture by language models, and overfitting to co-occurrencepatterns and distracting clues in the training text. To address these issues,the paper proposes a method to enhance knowledge learning during language modelpretraining, by enhancing elusive but important clues in text discovered by thelanguage model themselves. We found that larger language models pay moreattention to non-obvious but important clues, which are often overlooked bysmaller language models. Therefore, we can identify these clues by contrastingthe attention weights of large and small language models. We use the identifiedclues as a guide to perform token-dropout data augmentation on the trainingtext, and observed a significant boost in both small and large models'performance in fact memorization. This shows that the behavior contrast betweenmore and less-performant language models contains important clues for knowledgelearning, and it can be ``amplified" for a straight-forward improvement inknowledge learning efficiency.</description><author>Jian Gao, Xiao Zhang, Ji Wu, Miao Li</author><pubDate>Thu, 26 Sep 2024 15:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17954v1</guid></item><item><title>LingoQA: Visual Question Answering for Autonomous Driving</title><link>http://arxiv.org/abs/2312.14115v4</link><description>We introduce LingoQA, a novel dataset and benchmark for visual questionanswering in autonomous driving. The dataset contains 28K unique short videoscenarios, and 419K annotations. Evaluating state-of-the-art vision-languagemodels on our benchmark shows that their performance is below humancapabilities, with GPT-4V responding truthfully to 59.6% of the questionscompared to 96.6% for humans. For evaluation, we propose a truthfulnessclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlationcoefficient to human evaluations, surpassing existing techniques like METEOR,BLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and runextensive ablation studies to understand its performance. We release ourdataset and benchmark as an evaluation platform for vision-language models inautonomous driving.</description><author>Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski</author><pubDate>Thu, 26 Sep 2024 15:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14115v4</guid></item><item><title>Spatial Hierarchy and Temporal Attention Guided Cross Masking for Self-supervised Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2409.17951v1</link><description>In self-supervised skeleton-based action recognition, the mask reconstructionparadigm is gaining interest in enhancing model refinement and robustnessthrough effective masking. However, previous works primarily relied on a singlemasking criterion, resulting in the model overfitting specific features andoverlooking other effective information. In this paper, we introduce ahierarchy and attention guided cross-masking framework (HA-CM) that appliesmasking to skeleton sequences from both spatial and temporal perspectives.Specifically, in spatial graphs, we utilize hyperbolic space to maintain jointdistinctions and effectively preserve the hierarchical structure ofhigh-dimensional skeletons, employing joint hierarchy as the masking criterion.In temporal flows, we substitute traditional distance metrics with the globalattention of joints for masking, addressing the convergence of distances inhigh-dimensional space and the lack of a global perspective. Additionally, weincorporate cross-contrast loss based on the cross-masking framework into theloss function to enhance the model's learning of instance-level features. HA-CMshows efficiency and universality on three public large-scale datasets, NTU-60,NTU-120, and PKU-MMD. The source code of our HA-CM is available athttps://github.com/YinxPeng/HA-CM-main.</description><author>Xinpeng Yin, Wenming Cao</author><pubDate>Thu, 26 Sep 2024 15:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17951v1</guid></item><item><title>Language agents achieve superhuman synthesis of scientific knowledge</title><link>http://arxiv.org/abs/2409.13740v2</link><description>Language models are known to hallucinate incorrect information, and it isunclear if they are sufficiently accurate and reliable for use in scientificresearch. We developed a rigorous human-AI comparison methodology to evaluatelanguage model agents on real-world literature search tasks coveringinformation retrieval, summarization, and contradiction detection tasks. Weshow that PaperQA2, a frontier language model agent optimized for improvedfactuality, matches or exceeds subject matter expert performance on threerealistic literature research tasks without any restrictions on humans (i.e.,full access to internet, search tools, and time). PaperQA2 writes cited,Wikipedia-style summaries of scientific topics that are significantly moreaccurate than existing, human-written Wikipedia articles. We also introduce ahard benchmark for scientific literature research called LitQA2 that guideddesign of PaperQA2, leading to it exceeding human performance. Finally, weapply PaperQA2 to identify contradictions within the scientific literature, animportant scientific task that is challenging for humans. PaperQA2 identifies2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, ofwhich 70% are validated by human experts. These results demonstrate thatlanguage model agents are now capable of exceeding domain experts acrossmeaningful tasks on scientific literature.</description><author>Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, Andrew D. White</author><pubDate>Thu, 26 Sep 2024 15:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13740v2</guid></item><item><title>TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with Tabular Node Features</title><link>http://arxiv.org/abs/2409.14500v2</link><description>Tabular machine learning is an important field for industry and science. Inthis field, table rows are usually treated as independent data samples, butadditional information about relations between them is sometimes available andcan be used to improve predictive performance. Such information can benaturally modeled with a graph, thus tabular machine learning may benefit fromgraph machine learning methods. However, graph machine learning models aretypically evaluated on datasets with homogeneous node features, which havelittle in common with heterogeneous mixtures of numerical and categoricalfeatures present in tabular datasets. Thus, there is a critical differencebetween the data used in tabular and graph machine learning studies, which doesnot allow one to understand how successfully graph models can be transferred totabular data. To bridge this gap, we propose a new benchmark of diverse graphswith heterogeneous tabular node features and realistic prediction tasks. We usethis benchmark to evaluate a vast set of models, including simple methodspreviously overlooked in the literature. Our experiments show that graph neuralnetworks (GNNs) can indeed often bring gains in predictive performance fortabular data, but standard tabular models also can be adapted to work withgraph data by using simple feature preprocessing, which sometimes enables themto compete with and even outperform GNNs. Based on our empirical study, weprovide insights for researchers and practitioners in both tabular and graphmachine learning fields.</description><author>Gleb Bazhenov, Oleg Platonov, Liudmila Prokhorenkova</author><pubDate>Thu, 26 Sep 2024 15:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14500v2</guid></item><item><title>Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of Anomalous Behavior in Bio-regenerative Life Support System Telemetry</title><link>http://arxiv.org/abs/2406.09825v2</link><description>The detection of abnormal or critical system states is essential in conditionmonitoring. While much attention is given to promptly identifying anomalies, aretrospective analysis of these anomalies can significantly enhance ourcomprehension of the underlying causes of observed undesired behavior. Thisaspect becomes particularly critical when the monitored system is deployed in avital environment. In this study, we delve into anomalies within the domain ofBio-Regenerative Life Support Systems (BLSS) for space exploration and analyzeanomalies found in telemetry data stemming from the EDEN ISS space greenhousein Antarctica. We employ time series clustering on anomaly detection results tocategorize various types of anomalies in both uni- and multivariate settings.We then assess the effectiveness of these methods in identifying systematicanomalous behavior. Additionally, we illustrate that the anomaly detectionmethods MDI and DAMP produce complementary results, as previously indicated byresearch.</description><author>Ferdinand Rewicki, Jakob Gawlikowski, Julia Niebling, Joachim Denzler</author><pubDate>Thu, 26 Sep 2024 15:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09825v2</guid></item><item><title>Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation</title><link>http://arxiv.org/abs/2409.17946v1</link><description>Despite being widely applied due to their exceptional capabilities, LargeLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.These attacks introduce targeted vulnerabilities into LLMs by poisoningtraining samples and full-parameter fine-tuning. However, this kind of backdoorattack is limited since they require significant computational resources,especially as the size of LLMs increases. Besides, parameter-efficientfine-tuning (PEFT) offers an alternative but the restricted parameter updatingmay impede the alignment of triggers with target labels. In this study, wefirst verify that backdoor attacks with PEFT may encounter challenges inachieving feasible performance. To address these issues and improve theeffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attackalgorithm from weak to strong based on contrastive knowledge distillation(W2SAttack). Specifically, we poison small-scale language models throughfull-parameter fine-tuning to serve as the teacher model. The teacher modelthen covertly transfers the backdoor to the large-scale student model throughcontrastive knowledge distillation, which employs PEFT. Theoretical analysisreveals that W2SAttack has the potential to augment the effectiveness ofbackdoor attacks. We demonstrate the superior performance of W2SAttack onclassification tasks across four language models, four backdoor attackalgorithms, and two different architectures of teacher models. Experimentalresults indicate success rates close to 100% for backdoor attacks targetingPEFT.</description><author>Shuai Zhao, Leilei Gan, Zhongliang Guo, Xiaobao Wu, Luwei Xiao, Xiaoyu Xu, Cong-Duy Nguyen, Luu Anh Tuan</author><pubDate>Thu, 26 Sep 2024 15:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17946v1</guid></item><item><title>On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms</title><link>http://arxiv.org/abs/2409.17943v1</link><description>The typical workflow for a professional translator to translate a documentfrom its source language (SL) to a target language (TL) is not always focusedon what many language models in natural language processing (NLP) do - predictthe next word in a series of words. While high-resource languages like Englishand French are reported to achieve near human parity using common metrics formeasurement such as BLEU and COMET, we find that an important step is beingmissed: the translation of technical terms, specifically acronyms. Somestate-of-the art machine translation systems like Google Translate which arepublicly available can be erroneous when dealing with acronyms - as much as 50%in our findings. This article addresses acronym disambiguation for MT systemsby proposing an additional step to the SL-TL (FR-EN) translation workflow wherewe first offer a new acronym corpus for public consumption and then experimentwith a search-based thresholding algorithm that achieves nearly 10% increasewhen compared to Google Translate and OpusMT.</description><author>Richard Yue, John E. Ortega, Kenneth Ward Church</author><pubDate>Thu, 26 Sep 2024 15:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17943v1</guid></item><item><title>Perturb, Attend, Detect and Localize (PADL): Robust Proactive Image Defense</title><link>http://arxiv.org/abs/2409.17941v1</link><description>Image manipulation detection and localization have received considerableattention from the research community given the blooming of Generative Models(GMs). Detection methods that follow a passive approach may overfit to specificGMs, limiting their application in real-world scenarios, due to the growingdiversity of generative models. Recently, approaches based on a proactiveframework have shown the possibility of dealing with this limitation. However,these methods suffer from two main limitations, which raises concerns aboutpotential vulnerabilities: i) the manipulation detector is not robust to noiseand hence can be easily fooled; ii) the fact that they rely on fixedperturbations for image protection offers a predictable exploit for maliciousattackers, enabling them to reverse-engineer and evade detection. To overcomethis issue we propose PADL, a new solution able to generate image-specificperturbations using a symmetric scheme of encoding and decoding based oncross-attention, which drastically reduces the possibility of reverseengineering, even when evaluated with adaptive attack [31]. Additionally, PADLis able to pinpoint manipulated areas, facilitating the identification ofspecific regions that have undergone alterations, and has more generalizationpower than prior art on held-out generative models. Indeed, although beingtrained only on an attribute manipulation GAN model [15], our methodgeneralizes to a range of unseen models with diverse architectural designs,such as StarGANv2, BlendGAN, DiffAE, StableDiffusion and StableDiffusionXL.Additionally, we introduce a novel evaluation protocol, which offers a fairevaluation of localisation performance in function of detection accuracy andbetter captures real-world scenarios.</description><author>Filippo Bartolucci, Iacopo Masi, Giuseppe Lisanti</author><pubDate>Thu, 26 Sep 2024 15:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17941v1</guid></item><item><title>Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods</title><link>http://arxiv.org/abs/2409.17939v1</link><description>Translation memories (TMs) are the backbone for professional translationtools called computer-aided translation (CAT) tools. In order to perform atranslation using a CAT tool, a translator uses the TM to gather translationssimilar to the desired segment to translate (s'). Many CAT tools offer afuzzy-match algorithm to locate segments (s) in the TM that are close indistance to s'. After locating two similar segments, the CAT tool will presentparallel segments (s, t) that contain one segment in the source language alongwith its translation in the target language. Additionally, CAT tools containfuzzy-match repair (FMR) techniques that will automatically use the parallelsegments from the TM to create new TM entries containing a modified version ofthe original with the idea in mind that it will be the translation of s'. MostFMR techniques use machine translation as a way of "repairing" those words thathave to be modified. In this article, we show that for a large part of thosewords which are anchored, we can use other techniques that are based on machinelearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, weshow that for anchored words that follow the continuous bag-of-words (CBOW)paradigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, forsome cases, better results than neural machine translation for translatinganchored words from French to English.</description><author>Richard Yue, John E. Ortega</author><pubDate>Thu, 26 Sep 2024 15:12:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17939v1</guid></item><item><title>Adaptive Stream Processing on Edge Devices through Active Inference</title><link>http://arxiv.org/abs/2409.17937v1</link><description>The current scenario of IoT is witnessing a constant increase on the volumeof data, which is generated in constant stream, calling for novel architecturaland logical solutions for processing it. Moving the data handling towards theedge of the computing spectrum guarantees better distribution of load and, inprinciple, lower latency and better privacy. However, managing such a structureis complex, especially when requirements, also referred to Service LevelObjectives (SLOs), specified by applications' owners and infrastructuremanagers need to be ensured. Despite the rich number of proposals of MachineLearning (ML) based management solutions, researchers and practitioners yetstruggle to guarantee long-term prediction and control, and accuratetroubleshooting. Therefore, we present a novel ML paradigm based on ActiveInference (AIF) -- a concept from neuroscience that describes how the brainconstantly predicts and evaluates sensory information to decrease long-termsurprise. We implement it and evaluate it in a heterogeneous real streamprocessing use case, where an AIF-based agent continuously optimizes thefulfillment of three SLOs for three autonomous driving services running onmultiple devices. The agent used causal knowledge to gradually develop anunderstanding of how its actions are related to requirements fulfillment, andwhich configurations to favor. Through this approach, our agent requires up tothirty iterations to converge to the optimal solution, showing the capabilityof offering accurate results in a short amount of time. Furthermore, thanks toAIF and its causal structures, our method guarantees full transparency on thedecision making, making the interpretation of the results and thetroubleshooting effortless.</description><author>Boris Sedlak, Victor Casamayor Pujol, Andrea Morichetta, Praveen Kumar Donta, Schahram Dustdar</author><pubDate>Thu, 26 Sep 2024 15:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17937v1</guid></item><item><title>Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in Dynamic Scenes</title><link>http://arxiv.org/abs/2312.15268v4</link><description>Despite advancements in self-supervised monocular depth estimation,challenges persist in dynamic scenarios due to the dependence on assumptionsabout a static world. In this paper, we present Manydepth2, a Motion-GuidedCost Volume Depth Net, to achieve precise depth estimation for both dynamicobjects and static backgrounds, all while maintaining computational efficiency.To tackle the challenges posed by dynamic content, we incorporate optical flowand coarse monocular depth to create a novel static reference frame. This frameis then utilized to build a motion-guided cost volume in collaboration with thetarget frame. Additionally, to enhance the accuracy and resilience of thenetwork structure, we introduce an attention-based depth net architecture toeffectively integrate information from feature maps with varying resolutions.Compared to methods with similar computational costs, Manydepth2 achieves asignificant reduction of approximately five percent in root-mean-square errorfor self-supervised monocular depth estimation on the KITTI-2015 dataset. Thecode could be found: https://github.com/kaichen-z/Manydepth2</description><author>Kaichen Zhou, Jia-Wang Bian, Qian Xie, Jian-Qing Zheng, Niki Trigoni, Andrew Markham</author><pubDate>Thu, 26 Sep 2024 15:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15268v4</guid></item><item><title>Sample compression unleashed : New generalization bounds for real valued losses</title><link>http://arxiv.org/abs/2409.17932v1</link><description>The sample compression theory provides generalization guarantees forpredictors that can be fully defined using a subset of the training dataset anda (short) message string, generally defined as a binary sequence. Previousworks provided generalization bounds for the zero-one loss, which isrestrictive, notably when applied to deep learning approaches. In this paper,we present a general framework for deriving new sample compression bounds thathold for real-valued losses. We empirically demonstrate the tightness of thebounds and their versatility by evaluating them on different types of models,e.g., neural networks and decision forests, trained with the Pick-To-Learn(P2L) meta-algorithm, which transforms the training method of anymachine-learning predictor to yield sample-compressed predictors. In contrastto existing P2L bounds, ours are valid in the non-consistent case.</description><author>Mathieu Bazinet, Valentina Zantedeschi, Pascal Germain</author><pubDate>Thu, 26 Sep 2024 15:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17932v1</guid></item></channel></rss>