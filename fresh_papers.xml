<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 09 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Stereo-Depth Fusion through Virtual Pattern Projection</title><link>http://arxiv.org/abs/2406.04345v1</link><description>This paper presents a novel general-purpose stereo and depth data fusionparadigm that mimics the active stereo principle by replacing the unreliablephysical pattern projector with a depth sensor. It works by projecting virtualpatterns consistent with the scene geometry onto the left and right imagesacquired by a conventional stereo camera, using the sparse hints obtained froma depth sensor, to facilitate the visual correspondence. Purposely, any depthsensing device can be seamlessly plugged into our framework, enabling thedeployment of a virtual active stereo setup in any possible environment andovercoming the severe limitations of physical pattern projection, such as thelimited working range and environmental conditions. Exhaustive experiments onindoor and outdoor datasets featuring both long and close range, includingthose providing raw, unfiltered depth hints from off-the-shelf depth sensors,highlight the effectiveness of our approach in notably boosting the robustnessand accuracy of algorithms and deep stereo without any code modification andeven without re-training. Additionally, we assess the performance of ourstrategy on active stereo evaluation datasets with conventional patternprojection. Indeed, in all these scenarios, our virtual pattern projectionparadigm achieves state-of-the-art performance. The source code is availableat: https://github.com/bartn8/vppstereo.</description><author>Luca Bartolomei, Matteo Poggi, Fabio Tosi, Andrea Conti, Stefano Mattoccia</author><pubDate>Thu, 06 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04345v1</guid></item><item><title>Verbalized Machine Learning: Revisiting Machine Learning with Language Models</title><link>http://arxiv.org/abs/2406.04344v1</link><description>Motivated by the large progress made by large language models (LLMs), weintroduce the framework of verbalized machine learning (VML). In contrast toconventional machine learning models that are typically optimized over acontinuous parameter space, VML constrains the parameter space to behuman-interpretable natural language. Such a constraint leads to a newperspective of function approximation, where an LLM with a text prompt can beviewed as a function parameterized by the text prompt. Guided by thisperspective, we revisit classical machine learning problems, such as regressionand classification, and find that these problems can be solved by anLLM-parameterized learner and optimizer. The major advantages of VML include(1) easy encoding of inductive bias: prior knowledge about the problem andhypothesis class can be encoded in natural language and fed into theLLM-parameterized learner; (2) automatic model class selection: the optimizercan automatically select a concrete model class based on data and verbalizedprior knowledge, and it can update the model class during training; and (3)interpretable learner updates: the LLM-parameterized optimizer can provideexplanations for why each learner update is performed. We conduct severalstudies to empirically evaluate the effectiveness of VML, and hope that VML canserve as a stepping stone to stronger interpretability and trustworthiness inML.</description><author>Tim Z. Xiao, Robert Bamler, Bernhard Schölkopf, Weiyang Liu</author><pubDate>Thu, 06 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04344v1</guid></item><item><title>Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image</title><link>http://arxiv.org/abs/2406.04343v1</link><description>In this paper, we propose Flash3D, a method for scene reconstruction andnovel view synthesis from a single image which is both very generalisable andefficient. For generalisability, we start from a "foundation" model formonocular depth estimation and extend it to a full 3D shape and appearancereconstructor. For efficiency, we base this extension on feed-forward GaussianSplatting. Specifically, we predict a first layer of 3D Gaussians at thepredicted depth, and then add additional layers of Gaussians that are offset inspace, allowing the model to complete the reconstruction behind occlusions andtruncations. Flash3D is very efficient, trainable on a single GPU in a day, andthus accessible to most researchers. It achieves state-of-the-art results whentrained and tested on RealEstate10k. When transferred to unseen datasets likeNYU it outperforms competitors by a large margin. More impressively, whentransferred to KITTI, Flash3D achieves better PSNR than methods trainedspecifically on that dataset. In some instances, it even outperforms recentmethods that use multiple views as input. Code, models, demo, and more resultsare available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.</description><author>Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, João F. Henriques, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Thu, 06 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04343v1</guid></item><item><title>Learning 1D Causal Visual Representation with De-focus Attention Networks</title><link>http://arxiv.org/abs/2406.04342v1</link><description>Modality differences have led to the development of heterogeneousarchitectures for vision and language models. While images typically require 2Dnon-causal modeling, texts utilize 1D causal modeling. This distinction posessignificant challenges in constructing unified multi-modal models. This paperexplores the feasibility of representing images using 1D causal modeling. Weidentify an "over-focus" issue in existing 1D causal vision models, whereattention overly concentrates on a small proportion of visual tokens. The issueof "over-focus" hinders the model's ability to extract diverse visual featuresand to receive effective gradients for optimization. To address this, wepropose De-focus Attention Networks, which employ learnable bandpass filters tocreate varied attention patterns. During training, large and scheduled droppath rates, and an auxiliary loss on globally pooled features for globalunderstanding tasks are introduced. These two strategies encourage the model toattend to a broader range of tokens and enhance network optimization. Extensiveexperiments validate the efficacy of our approach, demonstrating that 1D causalvisual representation can perform comparably to 2D non-causal representation intasks such as global perception, dense prediction, and multi-modalunderstanding. Code is released athttps://github.com/OpenGVLab/De-focus-Attention-Networks.</description><author>Chenxin Tao, Xizhou Zhu, Shiqian Su, Lewei Lu, Changyao Tian, Xuan Luo, Gao Huang, Hongsheng Li, Yu Qiao, Jie Zhou, Jifeng Dai</author><pubDate>Thu, 06 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04342v1</guid></item><item><title>Interpreting the Second-Order Effects of Neurons in CLIP</title><link>http://arxiv.org/abs/2406.04341v1</link><description>We interpret the function of individual neurons in CLIP by automaticallydescribing them using text. Analyzing the direct effects (i.e. the flow from aneuron through the residual stream to the output) or the indirect effects(overall contribution) fails to capture the neurons' function in CLIP.Therefore, we present the "second-order lens", analyzing the effect flowingfrom a neuron through the later attention heads, directly to the output. Wefind that these effects are highly selective: for each neuron, the effect issignificant for &lt;2% of the images. Moreover, each effect can be approximated bya single direction in the text-image space of CLIP. We describe neurons bydecomposing these directions into sparse sets of text representations. The setsreveal polysemantic behavior - each neuron corresponds to multiple, oftenunrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, wemass-produce "semantic" adversarial examples by generating images with conceptsspuriously correlated to the incorrect class. Additionally, we use thesecond-order effects for zero-shot segmentation and attribute discovery inimages. Our results indicate that a scalable understanding of neurons can beused for model deception and for introducing new model capabilities.</description><author>Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt</author><pubDate>Thu, 06 Jun 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04341v1</guid></item><item><title>GLACE: Global Local Accelerated Coordinate Encoding</title><link>http://arxiv.org/abs/2406.04340v1</link><description>Scene coordinate regression (SCR) methods are a family of visual localizationmethods that directly regress 2D-3D matches for camera pose estimation. Theyare effective in small-scale scenes but face significant challenges inlarge-scale scenes that are further amplified in the absence of ground truth 3Dpoint clouds for supervision. Here, the model can only rely on reprojectionconstraints and needs to implicitly triangulate the points. The challenges stemfrom a fundamental dilemma: The network has to be invariant to observations ofthe same landmark at different viewpoints and lighting conditions, etc., but atthe same time discriminate unrelated but similar observations. The latterbecomes more relevant and severe in larger scenes. In this work, we tackle thisproblem by introducing the concept of co-visibility to the network. We proposeGLACE, which integrates pre-trained global and local encodings and enables SCRto scale to large scenes with only a single small-sized network. Specifically,we propose a novel feature diffusion technique that implicitly groups thereprojection constraints with co-visibility and avoids overfitting to trivialsolutions. Additionally, our position decoder parameterizes the outputpositions for large-scale scenes more effectively. Without using 3D models ordepth maps for supervision, our method achieves state-of-the-art results onlarge-scale scenes with a low-map-size model. On Cambridge landmarks, with asingle model, we achieve 17% lower median position error than Poker, theensemble variant of the state-of-the-art SCR method ACE. Code is available at:https://github.com/cvg/glace.</description><author>Fangjinhua Wang, Xudong Jiang, Silvano Galliani, Christoph Vogel, Marc Pollefeys</author><pubDate>Thu, 06 Jun 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04340v1</guid></item><item><title>Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion</title><link>http://arxiv.org/abs/2406.04338v1</link><description>In recent years, there has been rapid development in 3D generation models,opening up new possibilities for applications such as simulating the dynamicmovements of 3D objects and customizing their behaviors. However, current 3Dgenerative models tend to focus only on surface features such as color andshape, neglecting the inherent physical properties that govern the behavior ofobjects in the real world. To accurately simulate physics-aligned dynamics, itis essential to predict the physical properties of materials and incorporatethem into the behavior prediction process. Nonetheless, predicting the diversematerials of real-world objects is still challenging due to the complex natureof their physical attributes. In this paper, we propose \textbf{Physics3D}, anovel method for learning various physical properties of 3D objects through avideo diffusion model. Our approach involves designing a highly generalizablephysical simulation system based on a viscoelastic material model, whichenables us to simulate a wide range of materials with high-fidelitycapabilities. Moreover, we distill the physical priors from a video diffusionmodel that contains more understanding of realistic object materials. Extensiveexperiments demonstrate the effectiveness of our method with both elastic andplastic materials. Physics3D shows great potential for bridging the gap betweenthe physical world and virtual neural space, providing a better integration andapplication of realistic physical principles in virtual environments. Projectpage: https://liuff19.github.io/Physics3D.</description><author>Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, Yueqi Duan</author><pubDate>Thu, 06 Jun 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04338v1</guid></item><item><title>RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation</title><link>http://arxiv.org/abs/2406.04339v1</link><description>A fundamental objective in robot manipulation is to enable models tocomprehend visual scenes and execute actions. Although existing robotMultimodal Large Language Models (MLLMs) can handle a range of basic tasks,they still face challenges in two areas: 1) inadequate reasoning ability totackle complex tasks, and 2) high computational costs for MLLM fine-tuning andinference. The recently proposed state space model (SSM) known as Mambademonstrates promising capabilities in non-trivial sequence modeling withlinear inference complexity. Inspired by this, we introduce RoboMamba, anend-to-end robotic MLLM that leverages the Mamba model to deliver both roboticreasoning and action capabilities, while maintaining efficient fine-tuning andinference. Specifically, we first integrate the vision encoder with Mamba,aligning visual data with language embedding through co-training, empoweringour model with visual common sense and robot-related reasoning. To furtherequip RoboMamba with action pose prediction abilities, we explore an efficientfine-tuning strategy with a simple policy head. We find that once RoboMambapossesses sufficient reasoning capability, it can acquire manipulation skillswith minimal fine-tuning parameters (0.1\% of the model) and time (20 minutes).In experiments, RoboMamba demonstrates outstanding reasoning capabilities ongeneral and robotic evaluation benchmarks. Meanwhile, our model showcasesimpressive pose prediction results in both simulation and real-worldexperiments, achieving inference speeds 7 times faster than existing robotMLLMs. Our project web page: https://sites.google.com/view/robomamba-web</description><author>Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Lily Lee, Kaichen Zhou, Pengju An, Senqiao Yang, Renrui Zhang, Yandong Guo, Shanghang Zhang</author><pubDate>Thu, 06 Jun 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04339v1</guid></item><item><title>Coherent Zero-Shot Visual Instruction Generation</title><link>http://arxiv.org/abs/2406.04337v1</link><description>Despite the advances in text-to-image synthesis, particularly with diffusionmodels, generating visual instructions that require consistent representationand smooth state transitions of objects across sequential steps remains aformidable challenge. This paper introduces a simple, training-free frameworkto tackle the issues, capitalizing on the advancements in diffusion models andlarge language models (LLMs). Our approach systematically integrates textcomprehension and image generation to ensure visual instructions are visuallyappealing and maintain consistency and accuracy throughout the instructionsequence. We validate the effectiveness by testing multi-step instructions andcomparing the text alignment and consistency with several baselines. Ourexperiments show that our approach can visualize coherent and visually pleasinginstructions</description><author>Quynh Phung, Songwei Ge, Jia-Bin Huang</author><pubDate>Thu, 06 Jun 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04337v1</guid></item><item><title>On the Expressive Power of Spectral Invariant Graph Neural Networks</title><link>http://arxiv.org/abs/2406.04336v1</link><description>Incorporating spectral information to enhance Graph Neural Networks (GNNs)has shown promising results but raises a fundamental challenge due to theinherent ambiguity of eigenvectors. Various architectures have been proposed toaddress this ambiguity, referred to as spectral invariant architectures.Notable examples include GNNs and Graph Transformers that use spectraldistances, spectral projection matrices, or other invariant spectral features.However, the potential expressive power of these spectral invariantarchitectures remains largely unclear. The goal of this work is to gain a deeptheoretical understanding of the expressive power obtainable when usingspectral features. We first introduce a unified message-passing framework fordesigning spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). Acomprehensive analysis shows that EPNN essentially unifies all prior spectralinvariant architectures, in that they are either strictly less expressive orequivalent to EPNN. A fine-grained expressiveness hierarchy among differentarchitectures is also established. On the other hand, we prove that EPNN itselfis bounded by a recently proposed class of Subgraph GNNs, implying that allthese spectral invariant architectures are strictly less expressive than 3-WL.Finally, we discuss whether using spectral features can gain additionalexpressiveness when combined with more expressive GNNs.</description><author>Bohang Zhang, Lingxiao Zhao, Haggai Maron</author><pubDate>Thu, 06 Jun 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04336v1</guid></item><item><title>Subhomogeneous Deep Equilibrium Models</title><link>http://arxiv.org/abs/2403.00720v2</link><description>Implicit-depth neural networks have grown as powerful alternatives totraditional networks in various applications in recent years. However, thesemodels often lack guarantees of existence and uniqueness, raising stability,performance, and reproducibility issues. In this paper, we present a newanalysis of the existence and uniqueness of fixed points for implicit-depthneural networks based on the concept of subhomogeneous operators and thenonlinear Perron-Frobenius theory. Compared to previous similar analyses, ourtheory allows for weaker assumptions on the parameter matrices, thus yielding amore flexible framework for well-defined implicit networks. We illustrate theperformance of the resulting subhomogeneous networks on feedforward,convolutional, and graph neural network examples.</description><author>Pietro Sittoni, Francesco Tudisco</author><pubDate>Thu, 06 Jun 2024 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00720v2</guid></item><item><title>DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs</title><link>http://arxiv.org/abs/2406.04334v1</link><description>Most large multimodal models (LMMs) are implemented by feeding visual tokensas a sequence into the first layer of a large language model (LLM). Theresulting architecture is simple but significantly increases computation andmemory costs, as it has to handle a large number of additional tokens in itsinput layer. This paper presents a new architecture DeepStack for LMMs.Considering $N$ layers in the language and vision transformer of LMMs, we stackthe visual tokens into $N$ groups and feed each group to its alignedtransformer layer \textit{from bottom to top}. Surprisingly, this simple methodgreatly enhances the power of LMMs to model interactions among visual tokensacross layers but with minimal additional cost. We apply DeepStack to bothlanguage and vision transformer in LMMs, and validate the effectiveness ofDeepStack LMMs with extensive empirical results. Using the same context length,our DeepStack 7B and 13B parameters surpass their counterparts by \textbf{2.7}and \textbf{2.9} on average across \textbf{9} benchmarks, respectively. Usingonly one-fifth of the context length, DeepStack rivals closely to thecounterparts that use the full context length. These gains are particularlypronounced on high-resolution tasks, e.g., \textbf{4.2}, \textbf{11.0}, and\textbf{4.0} improvements on TextVQA, DocVQA, and InfoVQA compared toLLaVA-1.5-7B, respectively. We further apply DeepStack to vision transformerlayers, which brings us a similar amount of improvements, \textbf{3.8} onaverage compared with LLaVA-1.5-7B.</description><author>Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, Yu-Gang Jiang</author><pubDate>Thu, 06 Jun 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04334v1</guid></item><item><title>Coarse-To-Fine Tensor Trains for Compact Visual Representations</title><link>http://arxiv.org/abs/2406.04332v1</link><description>The ability to learn compact, high-quality, and easy-to-optimizerepresentations for visual data is paramount to many applications such as novelview synthesis and 3D reconstruction. Recent work has shown substantial successin using tensor networks to design such compact and high-qualityrepresentations. However, the ability to optimize tensor-based representations,and in particular, the highly compact tensor train representation, is stilllacking. This has prevented practitioners from deploying the full potential oftensor networks for visual data. To this end, we propose 'ProlongationUpsampling Tensor Train (PuTT)', a novel method for learning tensor trainrepresentations in a coarse-to-fine manner. Our method involves the prolongingor `upsampling' of a learned tensor train representation, creating a sequenceof 'coarse-to-fine' tensor trains that are incrementally refined. We evaluateour representation along three axes: (1). compression, (2). denoisingcapability, and (3). image completion capability. To assess these axes, weconsider the tasks of image fitting, 3D fitting, and novel view synthesis,where our method shows an improved performance compared to state-of-the-arttensor-based methods. For full results see our project webpage:https://sebulo.github.io/PuTT_website/</description><author>Sebastian Loeschcke, Dan Wang, Christian Leth-Espensen, Serge Belongie, Michael J. Kastoryano, Sagie Benaim</author><pubDate>Thu, 06 Jun 2024 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04332v1</guid></item><item><title>BitsFusion: 1.99 bits Weight Quantization of Diffusion Model</title><link>http://arxiv.org/abs/2406.04333v1</link><description>Diffusion-based image generation models have achieved great success in recentyears by showing the capability of synthesizing high-quality content. However,these models contain a huge number of parameters, resulting in a significantlylarge model size. Saving and transferring them is a major bottleneck forvarious applications, especially those running on resource-constrained devices.In this work, we develop a novel weight quantization method that quantizes theUNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with 7.9Xsmaller size while exhibiting even better generation quality than the originalone. Our approach includes several novel techniques, such as assigning optimalbits to each layer, initializing the quantized model for better performance,and improving the training strategy to dramatically reduce quantization error.Furthermore, we extensively evaluate our quantized model across variousbenchmark datasets and through human evaluation to demonstrate its superiorgeneration quality.</description><author>Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey Tulyakov, Jian Ren</author><pubDate>Thu, 06 Jun 2024 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04333v1</guid></item><item><title>Simplified and Generalized Masked Diffusion for Discrete Data</title><link>http://arxiv.org/abs/2406.04329v1</link><description>Masked (or absorbing) diffusion is actively explored as an alternative toautoregressive models for generative modeling of discrete data. However,existing work in this area has been hindered by unnecessarily complex modelformulations and unclear relationships between different perspectives, leadingto suboptimal parameterization, training objectives, and ad hoc adjustments tocounteract these issues. In this work, we aim to provide a simple and generalframework that unlocks the full potential of masked diffusion models. We showthat the continuous-time variational objective of masked diffusion models is asimple weighted integral of cross-entropy losses. Our framework also enablestraining generalized masked diffusion models with state-dependent maskingschedules. When evaluated by perplexity, our models trained on OpenWebTextsurpass prior diffusion language models at GPT-2 scale and demonstrate superiorperformance on 4 out of 5 zero-shot language modeling tasks. Furthermore, ourmodels vastly outperform previous discrete diffusion models on pixel-levelimage modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\times$64) bitsper dimension that are comparable or better than autoregressive models ofsimilar sizes.</description><author>Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, Michalis K. Titsias</author><pubDate>Thu, 06 Jun 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04329v1</guid></item><item><title>PaCE: Parsimonious Concept Engineering for Large Language Models</title><link>http://arxiv.org/abs/2406.04331v1</link><description>Large Language Models (LLMs) are being used for a wide variety of tasks.While they are capable of generating human-like responses, they can alsoproduce undesirable output including potentially harmful information, racist orsexist language, and hallucinations. Alignment methods are designed to reducesuch undesirable output, via techniques such as fine-tuning, promptengineering, and representation engineering. However, existing methods faceseveral challenges: some require costly fine-tuning for every alignment task;some do not adequately remove undesirable concepts, failing alignment; someremove benign concepts, lowering the linguistic capabilities of LLMs. Toaddress these issues, we propose Parsimonious Concept Engineering (PaCE), anovel activation engineering framework for alignment. First, to sufficientlymodel the concepts, we construct a large-scale concept dictionary in theactivation space, in which each atom corresponds to a semantic concept. Then,given any alignment task, we instruct a concept partitioner to efficientlyannotate the concepts as benign or undesirable. Finally, at inference time, wedecompose the LLM activations along the concept dictionary via sparse coding,to accurately represent the activation as a linear combination of the benignand undesirable components. By removing the latter ones from the activation, wereorient the behavior of LLMs towards alignment goals. We conduct experimentson tasks such as response detoxification, faithfulness enhancement, andsentiment revising, and show that PaCE achieves state-of-the-art alignmentperformance while maintaining linguistic capabilities.</description><author>Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal</author><pubDate>Thu, 06 Jun 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04331v1</guid></item><item><title>Parameter-Inverted Image Pyramid Networks</title><link>http://arxiv.org/abs/2406.04330v1</link><description>Image pyramids are commonly used in modern computer vision tasks to obtainmulti-scale features for precise understanding of images. However, imagepyramids process multiple resolutions of images using the same large-scalemodel, which requires significant computational cost. To overcome this issue,we propose a novel network architecture known as the Parameter-Inverted ImagePyramid Networks (PIIP). Our core idea is to use models with differentparameter sizes to process different resolution levels of the image pyramid,thereby balancing computational efficiency and performance. Specifically, theinput to PIIP is a set of multi-scale images, where higher resolution imagesare processed by smaller networks. We further propose a feature interactionmechanism to allow features of different resolutions to complement each otherand effectively integrate information from different spatial scales. Extensiveexperiments demonstrate that the PIIP achieves superior performance in taskssuch as object detection, segmentation, and image classification, compared totraditional image pyramid methods and single-branch networks, while reducingcomputational cost. Notably, when applying our method on a large-scale visionfoundation model InternViT-6B, we improve its performance by 1%-2% on detectionand segmentation with only 40%-60% of the original computation. These resultsvalidate the effectiveness of the PIIP approach and provide a new technicaldirection for future vision computing tasks. Our code and models are availableat https://github.com/OpenGVLab/PIIP.</description><author>Xizhou Zhu, Xue Yang, Zhaokai Wang, Hao Li, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 06 Jun 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04330v1</guid></item><item><title>Causal Estimation of Memorisation Profiles</title><link>http://arxiv.org/abs/2406.04327v1</link><description>Understanding memorisation in language models has practical and societalimplications, e.g., studying models' training dynamics or preventing copyrightinfringements. Prior work defines memorisation as the causal effect of trainingwith an instance on the model's ability to predict that instance. Thisdefinition relies on a counterfactual: the ability to observe what would havehappened had the model not seen that instance. Existing methods struggle toprovide computationally efficient and accurate estimates of thiscounterfactual. Further, they often estimate memorisation for a modelarchitecture rather than for a specific model instance. This paper fills animportant gap in the literature, proposing a new, principled, and efficientmethod to estimate memorisation based on the difference-in-differences designfrom econometrics. Using this method, we characterise a model's memorisationprofile--its memorisation trends across training--by only observing itsbehaviour on a small set of instances throughout training. In experiments withthe Pythia model suite, we find that memorisation (i) is stronger and morepersistent in larger models, (ii) is determined by data order and learningrate, and (iii) has stable trends across model sizes, thus making memorisationin larger models predictable from smaller ones.</description><author>Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel</author><pubDate>Thu, 06 Jun 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04327v1</guid></item><item><title>The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning</title><link>http://arxiv.org/abs/2406.04328v1</link><description>The past few years have produced a series of spectacular advances in thedecoding of speech from brain activity. The engine of these advances has beenthe acquisition of labelled data, with increasingly large datasets acquiredfrom single subjects. However, participants exhibit anatomical and otherindividual differences, and datasets use varied scanners and task designs. As aresult, prior work has struggled to leverage data from multiple subjects,multiple datasets, multiple tasks, and unlabelled datasets. In turn, the fieldhas not benefited from the rapidly growing number of open neural datarepositories to exploit large-scale data and deep learning. To address this, wedevelop an initial set of neuroscience-inspired self-supervised objectives,together with a neural architecture, for representation learning fromheterogeneous and unlabelled neural recordings. Experimental results show thatrepresentations learned with these objectives generalise across subjects,datasets, and tasks, and are also learned faster than using only labelled data.In addition, we set new benchmarks for two foundational speech decoding tasks.Taken together, these methods now unlock the potential for training speechdecoding models with orders of magnitude more existing data.</description><author>Dulhan Jayalath, Gilad Landau, Brendan Shillingford, Mark Woolrich, Oiwi Parker Jones</author><pubDate>Thu, 06 Jun 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04328v1</guid></item><item><title>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions</title><link>http://arxiv.org/abs/2406.04325v1</link><description>We present the ShareGPT4Video series, aiming to facilitate the videounderstanding of large video-language models (LVLMs) and the video generationof text-to-video models (T2VMs) via dense and precise captions. The seriescomprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos withvarious lengths and sources, developed through carefully designed datafiltering and annotating strategy. 2) ShareCaptioner-Video, an efficient andcapable captioning model for arbitrary videos, with 4.8M high-quality aestheticvideos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM thatreached SOTA performance on three advancing video benchmarks. To achieve this,taking aside the non-scalable costly human annotators, we find using GPT4V tocaption video with a naive multi-frame or frame-concatenation input strategyleads to less detailed and sometimes temporal-confused results. We argue thechallenge of designing a high-quality video captioning strategy lies in threeaspects: 1) Inter-frame precise temporal change understanding. 2) Intra-framedetailed content description. 3) Frame-number scalability for arbitrary-lengthvideos. To this end, we meticulously designed a differential video captioningstrategy, which is stable, scalable, and efficient for generating captions forvideos with arbitrary resolution, aspect ratios, and length. Based on it, weconstruct ShareGPT4Video, which contains 40K high-quality videos spanning awide range of categories, and the resulting captions encompass rich worldknowledge, object attributes, camera movements, and crucially, detailed andprecise temporal descriptions of events. Based on ShareGPT4Video, we furtherdevelop ShareCaptioner-Video, a superior captioner capable of efficientlygenerating high-quality captions for arbitrary videos...</description><author>Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, Jiaqi Wang</author><pubDate>Thu, 06 Jun 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04325v1</guid></item><item><title>Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems</title><link>http://arxiv.org/abs/2310.12956v2</link><description>In this work, we study rapid improvements of the training loss intransformers when being confronted with multi-step decision tasks. We foundthat transformers struggle to learn the intermediate task and both training andvalidation loss saturate for hundreds of epochs. When transformers finallylearn the intermediate task, they do this rapidly and unexpectedly. We callthese abrupt improvements Eureka-moments, since the transformer appears tosuddenly learn a previously incomprehensible concept. We designed synthetictasks to study the problem in detail, but the leaps in performance can beobserved also for language modeling and in-context learning (ICL). We suspectthat these abrupt transitions are caused by the multi-step nature of thesetasks. Indeed, we find connections and show that ways to improve on thesynthetic multi-step tasks can be used to improve the training of languagemodeling and ICL. Using the synthetic data we trace the problem back to theSoftmax function in the self-attention block of transformers and show ways toalleviate the problem. These fixes reduce the required number of trainingsteps, lead to higher likelihood to learn the intermediate task, to higherfinal accuracy and training becomes more robust to hyper-parameters.</description><author>David T. Hoffmann, Simon Schrodi, Jelena Bratulić, Nadine Behrmann, Volker Fischer, Thomas Brox</author><pubDate>Thu, 06 Jun 2024 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12956v2</guid></item><item><title>SF-V: Single Forward Video Generation Model</title><link>http://arxiv.org/abs/2406.04324v1</link><description>Diffusion-based video generation models have demonstrated remarkable successin obtaining high-fidelity videos through the iterative denoising process.However, these models require multiple denoising steps during sampling,resulting in high computational costs. In this work, we propose a novelapproach to obtain single-step video generation models by leveragingadversarial training to fine-tune pre-trained video diffusion models. We showthat, through the adversarial training, the multi-steps video diffusion model,i.e., Stable Video Diffusion (SVD), can be trained to perform single forwardpass to synthesize high-quality videos, capturing both temporal and spatialdependencies in the video data. Extensive experiments demonstrate that ourmethod achieves competitive generation quality of synthesized videos withsignificantly reduced computational overhead for the denoising process (i.e.,around $23\times$ speedup compared with SVD and $6\times$ speedup compared withexisting works, with even better generation quality), paving the way forreal-time video synthesis and editing. More visualization results are madepublicly available at https://snap-research.github.io/SF-V.</description><author>Zhixing Zhang, Yanyu Li, Yushu Wu, Yanwu Xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, Jian Ren</author><pubDate>Thu, 06 Jun 2024 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04324v1</guid></item><item><title>DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data</title><link>http://arxiv.org/abs/2406.04322v1</link><description>We present DIRECT-3D, a diffusion-based 3D generative model for creatinghigh-quality 3D assets (represented by Neural Radiance Fields) from textprompts. Unlike recent 3D generative models that rely on clean and well-aligned3D data, limiting them to single or few-class generation, our model is directlytrained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigatingthe key challenge (i.e., data scarcity) in large-scale 3D generation. Inparticular, DIRECT-3D is a tri-plane diffusion model that integrates twoinnovations: 1) A novel learning framework where noisy data are filtered andaligned automatically during the training process. Specifically, after aninitial warm-up phase using a small set of clean data, an iterativeoptimization is introduced in the diffusion process to explicitly estimate the3D pose of objects and select beneficial data based on conditional density. 2)An efficient 3D representation that is achieved by disentangling objectgeometry and color features with two separate conditional diffusion models thatare optimized hierarchically. Given a prompt input, our model generateshigh-quality, high-resolution, realistic, and complex 3D objects with accurategeometric details in seconds. We achieve state-of-the-art performance in bothsingle-class generation and text-to-3D generation. We also demonstrate thatDIRECT-3D can serve as a useful 3D geometric prior of objects, for example toalleviate the well-known Janus problem in 2D-lifting methods such asDreamFusion. The code and models are available for research purposes at:https://github.com/qihao067/direct3d.</description><author>Qihao Liu, Yi Zhang, Song Bai, Adam Kortylewski, Alan Yuille</author><pubDate>Thu, 06 Jun 2024 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04322v1</guid></item><item><title>ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories</title><link>http://arxiv.org/abs/2406.04323v1</link><description>Training autonomous agents with sparse rewards is a long-standing problem inonline reinforcement learning (RL), due to low data efficiency. Prior workovercomes this challenge by extracting useful knowledge from offline data,often accomplished through the learning of action distribution from offlinedata and utilizing the learned distribution to facilitate online RL. However,since the offline data are given and fixed, the extracted knowledge isinherently limited, making it difficult to generalize to new tasks. We proposea novel approach that leverages offline data to learn a generative diffusionmodel, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generatessynthetic trajectories, serving as a form of data augmentation and consequentlyenhancing the performance of online RL methods. The key strength of ourdiffuser lies in its adaptability, allowing it to effectively handle varyingtrajectory lengths and mitigate distribution shifts between online and offlinedata. Because of its simplicity, ATraDiff seamlessly integrates with a widespectrum of RL methods. Empirical evaluation shows that ATraDiff consistentlyachieves state-of-the-art performance across a variety of environments, withparticularly pronounced improvements in complicated settings. Our code and demovideo are available at https://atradiff.github.io .</description><author>Qianlan Yang, Yu-Xiong Wang</author><pubDate>Thu, 06 Jun 2024 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04323v1</guid></item><item><title>VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling</title><link>http://arxiv.org/abs/2406.04321v1</link><description>In this work, we systematically study music generation conditioned solely onthe video. First, we present a large-scale dataset comprising 190K video-musicpairs, including various genres such as movie trailers, advertisements, anddocumentaries. Furthermore, we propose VidMuse, a simple framework forgenerating music aligned with video inputs. VidMuse stands out by producinghigh-fidelity music that is both acoustically and semantically aligned with thevideo. By incorporating local and global visual cues, VidMuse enables thecreation of musically coherent audio tracks that consistently match the videocontent through Long-Short-Term modeling. Through extensive experiments,VidMuse outperforms existing models in terms of audio quality, diversity, andaudio-visual alignment. The code and datasets will be available athttps://github.com/ZeyueT/VidMuse/.</description><author>Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Xiaoqiang Huang, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo</author><pubDate>Thu, 06 Jun 2024 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04321v1</guid></item><item><title>Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models</title><link>http://arxiv.org/abs/2406.04320v1</link><description>Modeling multivariate time series is a well-established problem with a widerange of applications from healthcare to financial markets. Traditional StateSpace Models (SSMs) are classical approaches for univariate time seriesmodeling due to their simplicity and expressive power to represent lineardependencies. They, however, have fundamentally limited expressive power tocapture non-linear dependencies, are slow in practice, and fail to model theinter-variate information flow. Despite recent attempts to improve theexpressive power of SSMs by using deep structured SSMs, the existing methodsare either limited to univariate time series, fail to model complex patterns(e.g., seasonal patterns), fail to dynamically model the dependencies ofvariate and time dimensions, and/or are input-independent. We present Chimerathat uses two input-dependent 2-D SSM heads with different discretizationprocesses to learn long-term progression and seasonal patterns. To improve theefficiency of complex 2D recurrence, we present a fast training using a new2-dimensional parallel selective scan. We further present and discuss2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Ourexperimental evaluation shows the superior performance of Chimera on extensiveand diverse benchmarks, including ECG and speech time series classification,long-term and short-term time series forecasting, and time series anomalydetection.</description><author>Ali Behrouz, Michele Santacatterina, Ramin Zabih</author><pubDate>Thu, 06 Jun 2024 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04320v1</guid></item><item><title>Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction</title><link>http://arxiv.org/abs/2406.04318v1</link><description>Magnetic Resonance (MR) imaging, despite its proven diagnostic utility,remains an inaccessible imaging modality for disease surveillance at thepopulation level. A major factor rendering MR inaccessible is lengthy scantimes. An MR scanner collects measurements associated with the underlyinganatomy in the Fourier space, also known as the k-space. Creating ahigh-fidelity image requires collecting large quantities of such measurements,increasing the scan time. Traditionally to accelerate an MR scan, imagereconstruction from under-sampled k-space data is the method of choice.However, recent works show the feasibility of bypassing image reconstructionand directly learning to detect disease directly from a sparser learned subsetof the k-space measurements. In this work, we propose Adaptive Sampling for MR(ASMR), a sampling method that learns an adaptive policy to sequentially selectk-space samples to optimize for target disease detection. On 6 out of 8pathology classification tasks spanning the Knee, Brain, and Prostate MR scans,ASMR reaches within 2% of the performance of a fully sampled classifier whileusing only 8% of the k-space, as well as outperforming prior state-of-the-artwork in k-space sampling such as EMRT, LOUPE, and DPS.</description><author>Chen-Yu Yen, Raghav Singhal, Umang Sharma, Rajesh Ranganath, Sumit Chopra, Lerrel Pinto</author><pubDate>Thu, 06 Jun 2024 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04318v1</guid></item><item><title>Regularized KL-Divergence for Well-Defined Function-Space Variational Inference in Bayesian neural networks</title><link>http://arxiv.org/abs/2406.04317v1</link><description>Bayesian neural networks (BNN) promise to combine the predictive performanceof neural networks with principled uncertainty modeling important forsafety-critical systems and decision making. However, posterior uncertaintyestimates depend on the choice of prior, and finding informative priors inweight-space has proven difficult. This has motivated variational inference(VI) methods that pose priors directly on the function generated by the BNNrather than on weights. In this paper, we address a fundamental issue with suchfunction-space VI approaches pointed out by Burt et al. (2020), who showed thatthe objective function (ELBO) is negative infinite for most priors of interest.Our solution builds on generalized VI (Knoblauch et al., 2019) with theregularized KL divergence (Quang, 2019) and is, to the best of our knowledge,the first well-defined variational objective for function-space inference inBNNs with Gaussian process (GP) priors. Experiments show that our methodincorporates the properties specified by the GP prior on synthetic and smallreal-world data sets, and provides competitive uncertainty estimates forregression, classification and out-of-distribution detection compared to BNNbaselines with both function and weight-space priors.</description><author>Tristan Cinquin, Robert Bamler</author><pubDate>Thu, 06 Jun 2024 18:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04317v1</guid></item><item><title>Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2406.04316v1</link><description>6D Object Pose Estimation is a crucial yet challenging task in computervision, suffering from a significant lack of large-scale datasets. Thisscarcity impedes comprehensive evaluation of model performance, limitingresearch advancements. Furthermore, the restricted number of availableinstances or categories curtails its applications. To address these issues,this paper introduces Omni6DPose, a substantial dataset characterized by itsdiversity in object categories, large scale, and variety in object materials.Omni6DPose is divided into three main components: ROPE (Real 6D Object PoseEstimation Dataset), which includes 332K images annotated with over 1.5Mannotations across 581 instances in 149 categories; SOPE(Simulated 6D ObjectPose Estimation Dataset), consisting of 475K images created in a mixed realitysetting with depth simulation, annotated with over 5M annotations across 4162instances in the same 149 categories; and the manually aligned real scannedobjects used in both ROPE and SOPE. Omni6DPose is inherently challenging due tothe substantial variations and ambiguities. To address this challenge, weintroduce GenPose++, an enhanced version of the SOTA category-level poseestimation framework, incorporating two pivotal improvements: Semantic-awarefeature extraction and Clustering-based aggregation. Moreover, we provide acomprehensive benchmarking analysis to evaluate the performance of previousmethods on this large-scale dataset in the realms of 6D object pose estimationand pose tracking.</description><author>Jiyao Zhang, Weiyao Huang, Bo Peng, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, Hao Dong</author><pubDate>Thu, 06 Jun 2024 18:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04316v1</guid></item><item><title>Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step</title><link>http://arxiv.org/abs/2406.04314v1</link><description>Recently, Direct Preference Optimization (DPO) has extended its success fromaligning large language models (LLMs) to aligning text-to-image diffusionmodels with human preferences. Unlike most existing DPO methods that assume alldiffusion steps share a consistent preference order with the final generatedimages, we argue that this assumption neglects step-specific denoisingperformance and that preference labels should be tailored to each step'scontribution. To address this limitation, we propose Step-aware PreferenceOptimization (SPO), a novel post-training approach that independently evaluatesand adjusts the denoising performance at each step, using a step-awarepreference model and a step-wise resampler to ensure accurate step-awaresupervision. Specifically, at each denoising step, we sample a pool of images,find a suitable win-lose pair, and, most importantly, randomly select a singleimage from the pool to initialize the next denoising step. This step-wiseresampler process ensures the next win-lose image pair comes from the sameimage, making the win-lose comparison independent of the previous step. Toassess the preferences at each step, we train a separate step-aware preferencemodel that can be applied to both noisy and clean images. Our experiments withStable Diffusion v1.5 and SDXL demonstrate that SPO significantly outperformsthe latest Diffusion-DPO in aligning generated images with complex, detailedprompts and enhancing aesthetics, while also achieving more than 20x timesfaster in training efficiency. Code and model:https://rockeycoss.github.io/spo.github.io/</description><author>Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, Liang Zheng</author><pubDate>Thu, 06 Jun 2024 18:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04314v1</guid></item><item><title>Improving Alignment and Robustness with Short Circuiting</title><link>http://arxiv.org/abs/2406.04313v1</link><description>AI systems can take harmful actions and are highly vulnerable to adversarialattacks. We present an approach, inspired by recent advances in representationengineering, that "short-circuits" models as they respond with harmful outputs.Existing techniques aimed at improving alignment, such as refusal training, areoften bypassed. Techniques such as adversarial training try to plug these holesby countering specific attacks. As an alternative to refusal training andadversarial training, short-circuiting directly controls the representationsthat are responsible for harmful outputs in the first place. Our technique canbe applied to both text-only and multimodal language models to prevent thegeneration of harmful outputs without sacrificing utility -- even in thepresence of powerful unseen attacks. Notably, while adversarial robustness instandalone image recognition remains an open challenge, short-circuiting allowsthe larger multimodal system to reliably withstand image "hijacks" that aim toproduce harmful content. Finally, we extend our approach to AI agents,demonstrating considerable reductions in the rate of harmful actions when theyare under attack. Our approach represents a significant step forward in thedevelopment of reliable safeguards to harmful behavior and adversarial attacks.</description><author>Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks</author><pubDate>Thu, 06 Jun 2024 18:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04313v1</guid></item><item><title>ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</title><link>http://arxiv.org/abs/2406.04312v1</link><description>Text-to-Image (T2I) models have made significant advancements in recentyears, but they still struggle to accurately capture intricate detailsspecified in complex compositional prompts. While fine-tuning T2I models withreward objectives has shown promise, it suffers from "reward hacking" and maynot generalize well to unseen prompt distributions. In this work, we proposeReward-based Noise Optimization (ReNO), a novel approach that enhances T2Imodels at inference by optimizing the initial noise based on the signal fromone or multiple human preference reward models. Remarkably, solving thisoptimization problem with gradient ascent for 50 iterations yields impressiveresults on four different one-step models across two competitive benchmarks,T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,ReNO-enhanced one-step models consistently surpass the performance of allcurrent open-source Text-to-Image models. Extensive user studies demonstratethat our model is preferred nearly twice as often compared to the popular SDXLmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.Moreover, given the same computational resources, a ReNO-optimized one-stepmodel outperforms widely-used open-source models such as SDXL andPixArt-$\alpha$, highlighting the efficiency and effectiveness of ReNO inenhancing T2I model performance at inference time. Code is available athttps://github.com/ExplainableML/ReNO.</description><author>Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, Zeynep Akata</author><pubDate>Thu, 06 Jun 2024 18:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04312v1</guid></item><item><title>ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation</title><link>http://arxiv.org/abs/2406.04309v1</link><description>The common trade-offs of state-of-the-art methods for multi-shaperepresentation (a single model "packing" multiple objects) involve tradingmodeling accuracy against memory and storage. We show how to encode multipleshapes represented as continuous neural fields with a higher degree ofprecision than previously possible and with low memory usage. Key to ourapproach is a recursive hierarchical formulation that exploits objectself-similarity, leading to a highly compressed and efficient shape latentspace. Thanks to the recursive formulation, our method supports spatial andglobal-to-local latent feature fusion without needing to initialize andmaintain auxiliary data structures, while still allowing for continuous fieldqueries to enable applications such as raytracing. In experiments on a set ofdiverse datasets, we provide compelling qualitative results and demonstratestate-of-the-art multi-scene reconstruction and compression results with asingle network per dataset.</description><author>Sergey Zakharov, Katherine Liu, Adrien Gaidon, Rares Ambrus</author><pubDate>Thu, 06 Jun 2024 18:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04309v1</guid></item><item><title>Approximation-Aware Bayesian Optimization</title><link>http://arxiv.org/abs/2406.04308v1</link><description>High-dimensional Bayesian optimization (BO) tasks such as molecular designoften require 10,000 function evaluations before obtaining meaningful results.While methods like sparse variational Gaussian processes (SVGPs) reducecomputational requirements in these settings, the underlying approximationsresult in suboptimal data acquisitions that slow the progress of optimization.In this paper we modify SVGPs to better align with the goals of BO: targetinginformed data acquisition rather than global posterior fidelity. Using theframework of utility-calibrated variational inference, we unify GPapproximation and data acquisition into a joint optimization problem, therebyensuring optimal decisions under a limited computational budget. Our approachcan be used with any decision-theoretic acquisition function and is compatiblewith trust region methods like TuRBO. We derive efficient joint objectives forthe expected improvement and knowledge gradient acquisition functions in boththe standard and batch BO settings. Our approach outperforms standard SVGPs onhigh-dimensional benchmark tasks in control and molecular design.</description><author>Natalie Maus, Kyurae Kim, Geoff Pleiss, David Eriksson, John P. Cunningham, Jacob R. Gardner</author><pubDate>Thu, 06 Jun 2024 18:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04308v1</guid></item><item><title>Semantically Diverse Language Generation for Uncertainty Estimation in Language Models</title><link>http://arxiv.org/abs/2406.04306v1</link><description>Large language models (LLMs) can suffer from hallucinations when generatingtext. These hallucinations impede various applications in society and industryby making LLMs untrustworthy. Current LLMs generate text in an autoregressivefashion by predicting and appending text tokens. When an LLM is uncertain aboutthe semantic meaning of the next tokens to generate, it is likely to starthallucinating. Thus, it has been suggested that hallucinations stem frompredictive uncertainty. We introduce Semantically Diverse Language Generation(SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM togenerate semantically diverse yet likely alternatives for an initiallygenerated text. This approach provides a precise measure of aleatoric semanticuncertainty, detecting whether the initial text is likely to be hallucinated.Experiments on question-answering tasks demonstrate that SDLG consistentlyoutperforms existing methods while being the most computationally efficient,setting a new standard for uncertainty estimation in LLMs.</description><author>Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, Sepp Hochreiter</author><pubDate>Thu, 06 Jun 2024 18:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04306v1</guid></item><item><title>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</title><link>http://arxiv.org/abs/2405.17398v2</link><description>World models can foresee the outcomes of different actions, which is ofparamount importance for autonomous driving. Nevertheless, existing drivingworld models still have limitations in generalization to unseen environments,prediction fidelity of critical details, and action controllability forflexible application. In this paper, we present Vista, a generalizable drivingworld model with high fidelity and versatile controllability. Based on asystematic diagnosis of existing methods, we introduce several key ingredientsto address these limitations. To accurately predict real-world dynamics at highresolution, we propose two novel losses to promote the learning of movinginstances and structural information. We also devise an effective latentreplacement approach to inject historical frames as priors for coherentlong-horizon rollouts. For action controllability, we incorporate a versatileset of controls from high-level intentions (command, goal point) to low-levelmaneuvers (trajectory, angle, and speed) through an efficient learningstrategy. After large-scale training, the capabilities of Vista can seamlesslygeneralize to different scenarios. Extensive experiments on multiple datasetsshow that Vista outperforms the most advanced general-purpose video generatorin over 70% of comparisons and surpasses the best-performing driving worldmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilizethe capacity of Vista itself to establish a generalizable reward for real-worldaction evaluation without accessing the ground truth actions.</description><author>Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, Hongyang Li</author><pubDate>Thu, 06 Jun 2024 18:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17398v2</guid></item><item><title>Event-Triggered Time-Varying Bayesian Optimization</title><link>http://arxiv.org/abs/2208.10790v5</link><description>We consider the problem of sequentially optimizing a time-varying objectivefunction using time-varying Bayesian optimization (TVBO). To cope with staledata arising from time variations, current approaches to TVBO require priorknowledge of a constant rate of change. However, in practice, the rate ofchange is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB,that treats the optimization problem as static until it detects changes in theobjective function and then resets the dataset. This allows the algorithm toadapt online to realized temporal changes without the need for exact priorknowledge. The event trigger is based on probabilistic uniform error boundsused in Gaussian process regression. We derive regret bounds of adaptive resetswithout exact prior knowledge on the temporal changes, and show in numericalexperiments that ET-GP-UCB outperforms state-of-the-art algorithms on bothsynthetic and real-world data. The results demonstrate that ET-GP-UCB isreadily applicable to various settings without extensive hyperparameter tuning.</description><author>Paul Brunzema, Alexander von Rohr, Friedrich Solowjow, Sebastian Trimpe</author><pubDate>Thu, 06 Jun 2024 18:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10790v5</guid></item><item><title>Vision-LSTM: xLSTM as Generic Vision Backbone</title><link>http://arxiv.org/abs/2406.04303v1</link><description>Transformers are widely used as generic backbones in computer vision, despiteinitially introduced for natural language processing. Recently, the LongShort-Term Memory (LSTM) has been extended to a scalable and performantarchitecture - the xLSTM - which overcomes long-standing LSTM limitations viaexponential gating and parallelizable matrix memory structure. In this report,we introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks tocomputer vision. ViL comprises a stack of xLSTM blocks where odd blocks processthe sequence of patch tokens from top to bottom while even blocks go frombottom to top. Experiments show that ViL holds promise to be further deployedas new generic backbone for computer vision architectures.</description><author>Benedikt Alkin, Maximilian Beck, Korbinian Pöppel, Sepp Hochreiter, Johannes Brandstetter</author><pubDate>Thu, 06 Jun 2024 18:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04303v1</guid></item><item><title>Proactive Detection of Voice Cloning with Localized Watermarking</title><link>http://arxiv.org/abs/2401.17264v2</link><description>In the rapidly evolving field of speech generative models, there is apressing need to ensure audio authenticity against the risks of voice cloning.We present AudioSeal, the first audio watermarking technique designedspecifically for localized detection of AI-generated speech. AudioSeal employsa generator/detector architecture trained jointly with a localization loss toenable localized watermark detection up to the sample level, and a novelperceptual loss inspired by auditory masking, that enables AudioSeal to achievebetter imperceptibility. AudioSeal achieves state-of-the-art performance interms of robustness to real life audio manipulations and imperceptibility basedon automatic and human evaluation metrics. Additionally, AudioSeal is designedwith a fast, single-pass detector, that significantly surpasses existing modelsin speed - achieving detection up to two orders of magnitude faster, making itideal for large-scale and real-time applications.</description><author>Robin San Roman, Pierre Fernandez, Alexandre Défossez, Teddy Furon, Tuan Tran, Hady Elsahar</author><pubDate>Thu, 06 Jun 2024 18:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17264v2</guid></item><item><title>Representational Alignment Supports Effective Machine Teaching</title><link>http://arxiv.org/abs/2406.04302v1</link><description>A good teacher should not only be knowledgeable; but should be able tocommunicate in a way that the student understands -- to share the student'srepresentation of the world. In this work, we integrate insights from machineteaching and pragmatic communication with the burgeoning literature onrepresentational alignment to characterize a utility curve defining arelationship between representational alignment and teacher capability forpromoting student learning. To explore the characteristics of this utilitycurve, we design a supervised learning environment that disentanglesrepresentational alignment from teacher accuracy. We conduct extensivecomputational experiments with machines teaching machines, complemented by aseries of experiments in which machines teach humans. Drawing on our findingsthat improved representational alignment with a student improves studentlearning outcomes (i.e., task accuracy), we design a classroom matchingprocedure that assigns students to teachers based on the utility curve. If weare to design effective machine teachers, it is not enough to build teachersthat are accurate -- we want teachers that can align, representationally, totheir students too.</description><author>Ilia Sucholutsky, Katherine M. Collins, Maya Malaviya, Nori Jacoby, Weiyang Liu, Theodore R. Sumers, Michalis Korakakis, Umang Bhatt, Mark Ho, Joshua B. Tenenbaum, Brad Love, Zachary A. Pardos, Adrian Weller, Thomas L. Griffiths</author><pubDate>Thu, 06 Jun 2024 18:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04302v1</guid></item><item><title>Neural Surface Reconstruction from Sparse Views Using Epipolar Geometry</title><link>http://arxiv.org/abs/2406.04301v1</link><description>This paper addresses the challenge of reconstructing surfaces from sparseview inputs, where ambiguity and occlusions due to missing information posesignificant hurdles. We present a novel approach, named EpiS, that incorporatesEpipolar information into the reconstruction process. Existing methods insparse-view neural surface learning have mainly focused on mean and varianceconsiderations using cost volumes for feature extraction. In contrast, ourmethod aggregates coarse information from the cost volume into Epipolarfeatures extracted from multiple source views, enabling the generation offine-grained Signal Distance Function (SDF)-aware features. Additionally, weemploy an attention mechanism along the line dimension to facilitate featurefusion based on the SDF feature. Furthermore, to address the information gapsin sparse conditions, we integrate depth information from monocular depthestimation using global and local regularization techniques. The globalregularization utilizes a triplet loss function, while the local regularizationemploys a derivative loss function. Extensive experiments demonstrate that ourapproach outperforms state-of-the-art methods, especially in cases with sparseand generalizable conditions.</description><author>Kaichen Zhou</author><pubDate>Thu, 06 Jun 2024 18:47:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04301v1</guid></item><item><title>TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification</title><link>http://arxiv.org/abs/2402.12991v2</link><description>Large Language Model (LLM) services and models often come with legal rules onwho can use them and how they must use them. Assessing the compliance of thereleased LLMs is crucial, as these rules protect the interests of the LLMcontributor and prevent misuse. In this context, we describe the novelfingerprinting problem of Black-box Identity Verification (BBIV). The goal isto determine whether a third-party application uses a certain LLM through itschat function. We propose a method called Targeted Random Adversarial Prompt(TRAP) that identifies the specific LLM in use. We repurpose adversarialsuffixes, originally proposed for jailbreaking, to get a pre-defined answerfrom the target LLM, while other models give random answers. TRAP detects thetarget LLMs with over 95% true positive rate at under 0.2% false positive rateeven after a single interaction. TRAP remains effective even if the LLM hasminor changes that do not significantly alter the original function.</description><author>Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun, Seong Joon Oh</author><pubDate>Thu, 06 Jun 2024 18:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12991v2</guid></item><item><title>Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation</title><link>http://arxiv.org/abs/2401.06688v2</link><description>Neural machine translation systems estimate probabilities of target sentencesgiven source sentences, yet these estimates may not align with humanpreferences. This work introduces QE-fusion, a method that synthesizestranslations using a quality estimation metric (QE), which correlates betterwith human judgments. QE-fusion leverages a pool of candidates sampled from amodel, combining spans from different candidates using a QE metric such asCometKiwi. We compare QE-fusion against beam search and recent rerankingtechniques, such as Minimum Bayes Risk decoding or QE-reranking. Our methodconsistently improves translation quality in terms of COMET and BLEURT scoreswhen applied to large language models (LLMs) used for translation (PolyLM,XGLM, Llama2, Mistral, ALMA, and Tower) and to multilingual translation models(NLLB), over five language pairs. Notably, QE-fusion exhibits largerimprovements for LLMs due to their ability to generate diverse outputs. Wedemonstrate that our approach generates novel translations in over half of thecases and consistently outperforms other methods across varying numbers ofcandidates (5-200). Furthermore, we empirically establish that QE-fusion scaleslinearly with the number of candidates in the pool.</description><author>Giorgos Vernikos, Andrei Popescu-Belis</author><pubDate>Thu, 06 Jun 2024 18:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06688v2</guid></item><item><title>NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise</title><link>http://arxiv.org/abs/2406.04299v1</link><description>Graph Neural Networks (GNNs) exhibit strong potential in node classificationtask through a message-passing mechanism. However, their performance oftenhinges on high-quality node labels, which are challenging to obtain inreal-world scenarios due to unreliable sources or adversarial attacks.Consequently, label noise is common in real-world graph data, negativelyimpacting GNNs by propagating incorrect information during training. To addressthis issue, the study of Graph Neural Networks under Label Noise (GLN) hasrecently gained traction. However, due to variations in dataset selection, datasplitting, and preprocessing techniques, the community currently lacks acomprehensive benchmark, which impedes deeper understanding and furtherdevelopment of GLN. To fill this gap, we introduce NoisyGL in this paper, thefirst comprehensive benchmark for graph neural networks under label noise.NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisylabeled graph data across various datasets, with unified experimental settingsand interface. Our benchmark has uncovered several important insights that weremissed in previous research, and we believe these findings will be highlybeneficial for future studies. We hope our open-source benchmark library willfoster further advancements in this field. The code of the benchmark can befound in https://github.com/eaglelab-zju/NoisyGL.</description><author>Zhonghao Wang, Danyu Sun, Sheng Zhou, Haobo Wang, Jiapei Fan, Longtao Huang, Jiajun Bu</author><pubDate>Thu, 06 Jun 2024 18:45:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04299v1</guid></item><item><title>Measuring and Addressing Indexical Bias in Information Retrieval</title><link>http://arxiv.org/abs/2406.04298v1</link><description>Information Retrieval (IR) systems are designed to deliver relevant content,but traditional systems may not optimize rankings for fairness, neutrality, orthe balance of ideas. Consequently, IR can often introduce indexical biases, orbiases in the positional order of documents. Although indexical bias candemonstrably affect people's opinion, voting patterns, and other behaviors,these issues remain understudied as the field lacks reliable metrics andprocedures for automatically measuring indexical bias. Towards this end, weintroduce the PAIR framework, which supports automatic bias audits for rankeddocuments or entire IR systems. After introducing DUO, the firstgeneral-purpose automatic bias metric, we run an extensive evaluation of 8 IRsystems on a new corpus of 32k synthetic and 4.7k natural documents, with 4kqueries spanning 1.4k controversial issue topics. A human behavioral studyvalidates our approach, showing that our bias metric can help predict when andhow indexical bias will shift a reader's opinion.</description><author>Caleb Ziems, William Held, Jane Dwivedi-Yu, Diyi Yang</author><pubDate>Thu, 06 Jun 2024 18:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04298v1</guid></item><item><title>LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</title><link>http://arxiv.org/abs/2403.07974v2</link><description>Large Language Models (LLMs) applied to code-related applications haveemerged as a prominent field, attracting significant interest from bothacademia and industry. However, as new and improved LLMs are developed,existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficientfor assessing their capabilities. In this work, we propose LiveCodeBench, acomprehensive and contamination-free evaluation of LLMs for code, whichcontinuously collects new problems over time from contests across threecompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, ourbenchmark also focuses on a broader range of code related capabilities, such asself-repair, code execution, and test output prediction, beyond just codegeneration. Currently, LiveCodeBench hosts four hundred high-quality codingproblems that were published between May 2023 and May 2024. We have evaluated18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We presentempirical findings on contamination, holistic performance comparisons,potential overfitting in existing benchmarks as well as individual modelcomparisons. We will release all prompts and model completions for furthercommunity analysis, along with a general toolkit for adding new scenarios andmodel</description><author>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica</author><pubDate>Thu, 06 Jun 2024 18:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07974v2</guid></item><item><title>Rolling Diffusion Models</title><link>http://arxiv.org/abs/2402.09470v2</link><description>Diffusion models have recently been increasingly applied to temporal datasuch as video, fluid mechanics simulations, or climate data. These methodsgenerally treat subsequent frames equally regarding the amount of noise in thediffusion process. This paper explores Rolling Diffusion: a new approach thatuses a sliding window denoising process. It ensures that the diffusion processprogressively corrupts through time by assigning more noise to frames thatappear later in a sequence, reflecting greater uncertainty about the future asthe generation process unfolds. Empirically, we show that when the temporaldynamics are complex, Rolling Diffusion is superior to standard diffusion. Inparticular, this result is demonstrated in a video prediction task using theKinetics-600 video dataset and in a chaotic fluid dynamics forecastingexperiment.</description><author>David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom</author><pubDate>Thu, 06 Jun 2024 18:39:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09470v2</guid></item><item><title>Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation</title><link>http://arxiv.org/abs/2406.01349v3</link><description>Using generative models to synthesize new data has become a de-facto standardin autonomous driving to address the data scarcity issue. Though existingapproaches are able to boost perception models, we discover that theseapproaches fail to improve the performance of planning of end-to-end autonomousdriving models as the generated videos are usually less than 8 frames and thespatial and temporal inconsistencies are not negligible. To this end, wepropose Delphi, a novel diffusion-based long video generation method with ashared noise modeling mechanism across the multi-views to increase spatialconsistency, and a feature-aligned module to achieves both precisecontrollability and temporal consistency. Our method can generate up to 40frames of video without loss of consistency which is about 5 times longercompared with state-of-the-art methods. Instead of randomly generating newdata, we further design a sampling policy to let Delphi generate new data thatare similar to those failure cases to improve the sample efficiency. This isachieved by building a failure-case driven framework with the help ofpre-trained visual language models. Our extensive experiment demonstrates thatour Delphi generates a higher quality of long videos surpassing previousstate-of-the-art methods. Consequentially, with only generating 4% of thetraining dataset size, our framework is able to go beyond perception andprediction tasks, for the first time to the best of our knowledge, boost theplanning performance of the end-to-end autonomous driving model by a margin of25%.</description><author>Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang, Haiyang Sun, Di Lin, Kaicheng Yu</author><pubDate>Thu, 06 Jun 2024 18:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01349v3</guid></item><item><title>Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment</title><link>http://arxiv.org/abs/2406.04295v1</link><description>Test-time adaptation (TTA) aims to enhance the performance of source-domainpretrained models when tested on unknown shifted target domains. TraditionalTTA methods primarily adapt model weights based on target data streams, makingmodel performance sensitive to the amount and order of target data. Recently,diffusion-driven TTA methods have demonstrated strong performance by using anunconditional diffusion model, which is also trained on the source domain totransform target data into synthetic data as a source domain projection. Thisallows the source model to make predictions without weight adaptation. In thispaper, we argue that the domains of the source model and the synthetic data indiffusion-driven TTA methods are not aligned. To adapt the source model to thesynthetic domain of the unconditional diffusion model, we introduce aSynthetic-Domain Alignment (SDA) framework to fine-tune the source model withsynthetic data. Specifically, we first employ a conditional diffusion model togenerate labeled samples, creating a synthetic dataset. Subsequently, we usethe aforementioned unconditional diffusion model to add noise to and denoiseeach sample before fine-tuning. This process mitigates the potential domain gapbetween the conditional and unconditional models. Extensive experiments acrossvarious models and benchmarks demonstrate that SDA achieves superior domainalignment and consistently outperforms existing diffusion-driven TTA methods.Our code is available athttps://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment.</description><author>Jiayi Guo, Junhao Zhao, Chunjiang Ge, Chaoqun Du, Zanlin Ni, Shiji Song, Humphrey Shi, Gao Huang</author><pubDate>Thu, 06 Jun 2024 18:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04295v1</guid></item><item><title>VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval</title><link>http://arxiv.org/abs/2406.04292v1</link><description>Multi-modal retrieval becomes increasingly popular in practice. However, theexisting retrievers are mostly text-oriented, which lack the capability toprocess visual information. Despite the presence of vision-language models likeCLIP, the current methods are severely limited in representing the text-onlyand image-only data. In this work, we present a new embedding model VISTA foruniversal multi-modal retrieval. Our work brings forth threefold technicalcontributions. Firstly, we introduce a flexible architecture which extends apowerful text encoder with the image understanding capability by introducingvisual token embeddings. Secondly, we develop two data generation strategies,which bring high-quality composed image-text to facilitate the training of theembedding model. Thirdly, we introduce a multi-stage training algorithm, whichfirst aligns the visual token embedding with the text encoder using massiveweakly labeled data, and then develops multi-modal representation capabilityusing the generated composed image-text data. In our experiments, VISTAachieves superior performances across a variety of multi-modal retrieval tasksin both zero-shot and supervised settings. Our model, data, and source code areavailable at https://github.com/FlagOpen/FlagEmbedding.</description><author>Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, Yongping Xiong</author><pubDate>Thu, 06 Jun 2024 18:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04292v1</guid></item><item><title>Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation</title><link>http://arxiv.org/abs/2406.04291v1</link><description>Prediction-powered inference (PPI) is a method that improves statisticalestimates based on limited human-labeled data. PPI achieves this by combiningsmall amounts of human-labeled data with larger amounts of data labeled by areasonably accurate -- but potentially biased -- automatic system, in a waythat results in tighter confidence intervals for certain parameters of interest(e.g., the mean performance of a language model). In this paper, we propose amethod called Stratified Prediction-Powered Inference (StratPPI), in which weshow that the basic PPI estimates can be considerably improved by employingsimple data stratification strategies. Without making any assumptions on theunderlying automatic labeling system or data distribution, we derive analgorithm for computing provably valid confidence intervals for populationparameters (such as averages) that is based on stratified sampling. Inparticular, we show both theoretically and empirically that, with appropriatechoices of stratification and sample allocation, our approach can providesubstantially tighter confidence intervals than unstratified approaches.Specifically, StratPPI is expected to improve in cases where the performance ofthe autorater varies across different conditional distributions of the targetdata.</description><author>Adam Fisch, Joshua Maynez, R. Alex Hofer, Bhuwan Dhingra, Amir Globerson, William W. Cohen</author><pubDate>Thu, 06 Jun 2024 18:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04291v1</guid></item><item><title>What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages</title><link>http://arxiv.org/abs/2406.04289v1</link><description>What can large language models learn? By definition, language models (LM) aredistributions over strings. Therefore, an intuitive way of addressing the abovequestion is to formalize it as a matter of learnability of classes ofdistributions over strings. While prior work in this direction focused onassessing the theoretical limits, in contrast, we seek to understand theempirical learnability. Unlike prior empirical work, we evaluate neural LMs ontheir home turf-learning probabilistic languages-rather than as classifiers offormal languages. In particular, we investigate the learnability of regular LMs(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMsas a function of various complexity parameters of the RLM and the hidden statesize of the neural LM. We find that the RLM rank, which corresponds to the sizeof linear space spanned by the logits of its conditional distributions, and theexpected length of sampled strings are strong and significant predictors oflearnability for both RNNs and Transformers. Several other predictors alsoreach significance, but with differing patterns between RNNs and Transformers.</description><author>Nadav Borenstein, Anej Svete, Robin Chan, Josef Valvoda, Franz Nowak, Isabelle Augenstein, Eleanor Chodroff, Ryan Cotterell</author><pubDate>Thu, 06 Jun 2024 18:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04289v1</guid></item><item><title>SpectralZoom: Efficient Segmentation with an Adaptive Hyperspectral Camera</title><link>http://arxiv.org/abs/2406.04287v1</link><description>Hyperspectral image segmentation is crucial for many fields such asagriculture, remote sensing, biomedical imaging, battlefield sensing andastronomy. However, the challenge of hyper and multi spectral imaging is itslarge data footprint. We propose both a novel camera design and a visiontransformer-based (ViT) algorithm that alleviate both the captured datafootprint and the computational load for hyperspectral segmentation. Our camerais able to adaptively sample image regions or patches at different resolutions,instead of capturing the entire hyperspectral cube at one high resolution. Oursegmentation algorithm works in concert with the camera, applying ViT-basedsegmentation only to adaptively selected patches. We show results both insimulation and on a real hardware platform demonstrating both accuratesegmentation results and reduced computational burden.</description><author>Jackson Arnold, Sophia Rossi, Chloe Petrosino, Ethan Mitchell, Sanjeev J. Koppal</author><pubDate>Thu, 06 Jun 2024 18:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04287v1</guid></item><item><title>ReGAL: Refactoring Programs to Discover Generalizable Abstractions</title><link>http://arxiv.org/abs/2401.16467v2</link><description>While large language models (LLMs) are increasingly being used for programsynthesis, they lack the global view needed to develop useful abstractions;they generally predict programs one at a time, often repeating the samefunctionality. Generating redundant code from scratch is both inefficient anderror-prone. To address this, we propose Refactoring for GeneralizableAbstraction Learning (ReGAL), a gradient-free method for learning a library ofreusable functions via code refactorization, i.e., restructuring code withoutchanging its execution output. ReGAL learns from a small set of existingprograms, iteratively verifying and refining its abstractions via execution. Wefind that the shared function libraries discovered by ReGAL make programseasier to predict across diverse domains. On five datasets -- LOGO graphicsgeneration, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, andTabMWP -- both open-source and proprietary LLMs improve in accuracy whenpredicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results inabsolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysisreveals ReGAL's abstractions encapsulate frequently-used subroutines as well asenvironment dynamics.</description><author>Elias Stengel-Eskin, Archiki Prasad, Mohit Bansal</author><pubDate>Thu, 06 Jun 2024 18:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16467v2</guid></item><item><title>ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions</title><link>http://arxiv.org/abs/2406.04286v1</link><description>We present ABEX, a novel and effective generative data augmentationmethodology for low-resource Natural Language Understanding (NLU) tasks. ABEXis based on ABstract-and-EXpand, a novel paradigm for generating diverse formsof an input document -- we first convert a document into its concise, abstractdescription and then generate new documents based on expanding the resultantabstraction. To learn the task of expanding abstract descriptions, we firsttrain BART on a large-scale synthetic dataset with abstract-document pairs.Next, to generate abstract descriptions for a document, we propose a simple,controllable, and training-free method based on editing AMR graphs. ABEX bringsthe best of both worlds: by expanding from abstract representations, itpreserves the original semantic properties of the documents, like style andmeaning, thereby maintaining alignment with the original label and datadistribution. At the same time, the fundamental process of elaborating onabstract descriptions facilitates diverse generations. We demonstrate theeffectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resourcesettings. ABEX outperforms all our baselines qualitatively with improvements of0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods fromliterature in terms of context and length diversity.</description><author>Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, C. K. Evuru, S Ramaneswaran, S Sakshi, Dinesh Manocha</author><pubDate>Thu, 06 Jun 2024 18:29:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04286v1</guid></item><item><title>What is Dataset Distillation Learning?</title><link>http://arxiv.org/abs/2406.04284v1</link><description>Dataset distillation has emerged as a strategy to overcome the hurdlesassociated with large datasets by learning a compact set of synthetic data thatretains essential information from the original dataset. While distilled datacan be used to train high performing models, little is understood about how theinformation is stored. In this study, we posit and answer three questions aboutthe behavior, representativeness, and point-wise information content ofdistilled data. We reveal distilled data cannot serve as a substitute for realdata during training outside the standard evaluation setting for datasetdistillation. Additionally, the distillation process retains high taskperformance by compressing information related to the early training dynamicsof real models. Finally, we provide an framework for interpreting distilleddata and reveal that individual distilled data points contain meaningfulsemantic information. This investigation sheds light on the intricate nature ofdistilled data, providing a better understanding on how they can be effectivelyutilized.</description><author>William Yang, Ye Zhu, Zhiwei Deng, Olga Russakovsky</author><pubDate>Thu, 06 Jun 2024 18:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04284v1</guid></item><item><title>xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology</title><link>http://arxiv.org/abs/2406.04280v1</link><description>Multiple instance learning (MIL) is an effective and widely used approach forweakly supervised machine learning. In histopathology, MIL models have achievedremarkable success in tasks like tumor detection, biomarker prediction, andoutcome prognostication. However, MIL explanation methods are still laggingbehind, as they are limited to small bag sizes or disregard instanceinteractions. We revisit MIL through the lens of explainable AI (XAI) andintroduce xMIL, a refined framework with more general assumptions. Wedemonstrate how to obtain improved MIL explanations using layer-wise relevancepropagation (LRP) and conduct extensive evaluation experiments on three toysettings and four real-world histopathology datasets. Our approach consistentlyoutperforms previous explanation attempts with particularly improvedfaithfulness scores on challenging biomarker prediction tasks. Finally, weshowcase how xMIL explanations enable pathologists to extract insights from MILmodels, representing a significant advance for knowledge discovery and modeldebugging in digital histopathology.</description><author>Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller</author><pubDate>Thu, 06 Jun 2024 18:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04280v1</guid></item><item><title>Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People</title><link>http://arxiv.org/abs/2406.04278v1</link><description>Conversational tones -- the manners and attitudes in which speakerscommunicate -- are essential to effective communication. Amidst the increasingpopularization of Large Language Models (LLMs) over recent years, it becomesnecessary to characterize the divergences in their conversational tonesrelative to humans. However, existing investigations of conversationalmodalities rely on pre-existing taxonomies or text corpora, which suffer fromexperimenter bias and may not be representative of real-world distributions forthe studies' psycholinguistic domains. Inspired by methods from cognitivescience, we propose an iterative method for simultaneously elicitingconversational tones and sentences, where participants alternate between twotasks: (1) one participant identifies the tone of a given sentence and (2) adifferent participant generates a sentence based on that tone. We run 100iterations of this process with human participants and GPT-4, then obtain adataset of sentences and frequent conversational tones. In an additionalexperiment, humans and GPT-4 annotated all sentences with all tones. With datafrom 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4queries, we show how our approach can be used to create an interpretablegeometric representation of relations between conversational tones in humansand GPT-4. This work demonstrates how combining ideas from machine learning andcognitive science can address challenges in human-computer interactions.</description><author>Dun-Ming Huang, Pol Van Rijn, Ilia Sucholutsky, Raja Marjieh, Nori Jacoby</author><pubDate>Thu, 06 Jun 2024 18:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04278v1</guid></item><item><title>VideoTetris: Towards Compositional Text-to-Video Generation</title><link>http://arxiv.org/abs/2406.04277v1</link><description>Diffusion models have demonstrated great success in text-to-video (T2V)generation. However, existing methods may face challenges when handling complex(long) video generation scenarios that involve multiple objects or dynamicchanges in object numbers. To address these limitations, we proposeVideoTetris, a novel framework that enables compositional T2V generation.Specifically, we propose spatio-temporal compositional diffusion to preciselyfollow complex textual semantics by manipulating and composing the attentionmaps of denoising networks spatially and temporally. Moreover, we propose anenhanced video data preprocessing to enhance the training data regarding motiondynamics and prompt understanding, equipped with a new reference frameattention mechanism to improve the consistency of auto-regressive videogeneration. Extensive experiments demonstrate that our VideoTetris achievesimpressive qualitative and quantitative results in compositional T2Vgeneration. Code is available at: https://github.com/YangLing0818/VideoTetris</description><author>Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, Bin Cui</author><pubDate>Thu, 06 Jun 2024 18:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04277v1</guid></item><item><title>Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks</title><link>http://arxiv.org/abs/2406.04276v1</link><description>In recent years, machine learning (ML) techniques have created numerousopportunities for intelligent mobile networks and have accelerated theautomation of network operations. However, complex network tasks may involvevariables and considerations even beyond the capacity of traditional MLalgorithms. On the other hand, large language models (LLMs) have recentlyemerged, demonstrating near-human-level performance in cognitive tasks acrossvarious fields. However, they remain prone to hallucinations and often lackcommon sense in basic tasks. Therefore, they are regarded as assistive toolsfor humans. In this work, we propose the concept of "generative AI-in-the-loop"and utilize the semantic understanding, context awareness, and reasoningabilities of LLMs to assist humans in handling complex or unforeseen situationsin mobile communication networks. We believe that combining LLMs and ML modelsallows both to leverage their respective capabilities and achieve betterresults than either model alone. To support this idea, we begin by analyzingthe capabilities of LLMs and compare them with traditional ML algorithms. Wethen explore potential LLM-based applications in line with the requirements ofnext-generation networks. We further examine the integration of ML and LLMs,discussing how they can be used together in mobile networks. Unlike existingstudies, our research emphasizes the fusion of LLMs with traditional ML-drivennext-generation networks and serves as a comprehensive refinement of existingsurveys. Finally, we provide a case study to enhance ML-based network intrusiondetection with synthesized data generated by LLMs. Our case study furtherdemonstrates the advantages of our proposed idea.</description><author>Han Zhang, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci</author><pubDate>Thu, 06 Jun 2024 18:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04276v1</guid></item><item><title>Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models</title><link>http://arxiv.org/abs/2406.04274v1</link><description>This work studies the challenge of aligning large language models (LLMs) withoffline preference data. We focus on alignment by Reinforcement Learning fromHuman Feedback (RLHF) in particular. While popular preference optimizationmethods exhibit good empirical performance in practice, they are nottheoretically guaranteed to converge to the optimal policy and can provablyfail when the data coverage is sparse by classical offline reinforcementlearning (RL) results. On the other hand, a recent line of work has focused ontheoretically motivated preference optimization methods with provableguarantees, but these are not computationally efficient for large-scaleapplications like LLM alignment. To bridge this gap, we propose SPAC, a newoffline preference optimization method with self-play, inspired by theon-average pessimism technique from the offline RL literature, to be the firstprovable and scalable approach to LLM alignment. We both provide theoreticalanalysis for its convergence under single-policy concentrability for thegeneral function approximation setting and demonstrate its competitiveempirical performance for LLM alignment on a 7B Mistral model with Open LLMLeaderboard evaluations.</description><author>Xiang Ji, Sanjeev Kulkarni, Mengdi Wang, Tengyang Xie</author><pubDate>Thu, 06 Jun 2024 18:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04274v1</guid></item><item><title>ELFS: Enhancing Label-Free Coreset Selection via Clustering-based Pseudo-Labeling</title><link>http://arxiv.org/abs/2406.04273v1</link><description>High-quality human-annotated data is crucial for modern deep learningpipelines, yet the human annotation process is both costly and time-consuming.Given a constrained human labeling budget, selecting an informative andrepresentative data subset for labeling can significantly reduce humanannotation effort. Well-performing state-of-the-art (SOTA) coreset selectionmethods require ground-truth labels over the whole dataset, failing to reducethe human labeling burden. Meanwhile, SOTA label-free coreset selection methodsdeliver inferior performance due to poor geometry-based scores. In this paper,we introduce ELFS, a novel label-free coreset selection method. ELFS employsdeep clustering to estimate data difficulty scores without ground-truth labels.Furthermore, ELFS uses a simple but effective double-end pruning method tomitigate bias on calculated scores, which further improves the performance onselected coresets. We evaluate ELFS on five vision benchmarks and show thatELFS consistently outperforms SOTA label-free baselines. For instance, at a 90%pruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10and 7.1% on CIFAR100. Moreover, ELFS even achieves comparable performance tosupervised coreset selection at low pruning rates (e.g., 30% and 50%) onCIFAR10 and ImageNet-1K.</description><author>Haizhong Zheng, Elisa Tsai, Yifu Lu, Jiachen Sun, Brian R. Bartoldson, Bhavya Kailkhura, Atul Prakash</author><pubDate>Thu, 06 Jun 2024 18:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04273v1</guid></item><item><title>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2406.04271v1</link><description>We introduce Buffer of Thoughts (BoT), a novel and versatilethought-augmented reasoning approach for enhancing accuracy, efficiency androbustness of large language models (LLMs). Specifically, we proposemeta-buffer to store a series of informative high-level thoughts, namelythought-template, distilled from the problem-solving processes across varioustasks. Then for each problem, we retrieve a relevant thought-template andadaptively instantiate it with specific reasoning structures to conductefficient reasoning. To guarantee the scalability and stability, we furtherpropose buffer-manager to dynamically update the meta-buffer, thus enhancingthe capacity of meta-buffer as more tasks are solved. We conduct extensiveexperiments on 10 challenging reasoning-intensive tasks, and achievesignificant performance improvements over previous SOTA methods: 11% on Game of24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysisdemonstrate the superior generalization ability and model robustness of ourBoT, while requiring only 12% of the cost of multi-query prompting methods(e.g., tree/graph of thoughts) on average. Notably, we find that ourLlama3-8B+BoT has the potential to surpass Llama3-70B model. Our project isavailable at: https://github.com/YangLing0818/buffer-of-thought-llm</description><author>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, Bin Cui</author><pubDate>Thu, 06 Jun 2024 18:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04271v1</guid></item><item><title>RECAP: Retrieval-Augmented Audio Captioning</title><link>http://arxiv.org/abs/2309.09836v2</link><description>We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel andeffective audio captioning system that generates captions conditioned on aninput audio and other captions similar to the audio retrieved from a datastore.Additionally, our proposed method can transfer to any domain without the needfor any additional fine-tuning. To generate a caption for an audio sample, weleverage an audio-text model CLAP to retrieve captions similar to it from areplaceable datastore, which are then used to construct a prompt. Next, we feedthis prompt to a GPT-2 decoder and introduce cross-attention layers between theCLAP encoder and GPT-2 to condition the audio for caption generation.Experiments on two benchmark datasets, Clotho and AudioCaps, show that RECAPachieves competitive performance in in-domain settings and significantimprovements in out-of-domain settings. Additionally, due to its capability toexploit a large text-captions-only datastore in a training-free fashion, RECAPshows unique capabilities of captioning novel audio events never seen duringtraining and compositional audios with multiple events. To promote research inthis space, we also release 150,000+ new weakly labeled captions for AudioSet,AudioCaps, and Clotho.</description><author>Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha</author><pubDate>Thu, 06 Jun 2024 18:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09836v2</guid></item><item><title>Language Models Don't Learn the Physical Manifestation of Language</title><link>http://arxiv.org/abs/2402.11349v2</link><description>We argue that language-only models don't learn the physical manifestation oflanguage. We present an empirical investigation of visual-auditory propertiesof language through a series of tasks, termed H-Test. These tasks highlight afundamental gap between human linguistic understanding and the sensory-deprivedlinguistic understanding of LLMs. In support of our hypothesis, 1. deliberatereasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from thesame model family (LLaMA 2 13B -&gt; LLaMA 2 70B) has no significant effect onH-Test performance. We bring in the philosophical case of Mary, who learns about the world in asensory-deprived environment as a useful conceptual framework to understand howlanguage-only models learn about the world (Jackson, 1986). Our experimentsshow that some of the strongest proprietary LLMs stay near random chancebaseline accuracy of 50%, highlighting the limitations of linguistic knowledgeacquired in the absence of sensory experience. Our code and data are availableat &lt;github.com/brucewlee/h-test&gt;.</description><author>Bruce W. Lee, JaeHyuk Lim</author><pubDate>Thu, 06 Jun 2024 18:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11349v2</guid></item><item><title>An operator learning perspective on parameter-to-observable maps</title><link>http://arxiv.org/abs/2402.06031v2</link><description>Computationally efficient surrogates for parametrized physical models play acrucial role in science and engineering. Operator learning provides data-drivensurrogates that map between function spaces. However, instead of full-fieldmeasurements, often the available data are only finite-dimensionalparametrizations of model inputs or finite observables of model outputs.Building on Fourier Neural Operators, this paper introduces the Fourier NeuralMappings (FNMs) framework that is able to accommodate such finite-dimensionalvector inputs or outputs. The paper develops universal approximation theoremsfor the method. Moreover, in many applications the underlyingparameter-to-observable (PtO) map is defined implicitly through aninfinite-dimensional operator, such as the solution operator of a partialdifferential equation. A natural question is whether it is more data-efficientto learn the PtO map end-to-end or first learn the solution operator andsubsequently compute the observable from the full-field solution. A theoreticalanalysis of Bayesian nonparametric regression of linear functionals, which isof independent interest, suggests that the end-to-end approach can actuallyhave worse sample complexity. Extending beyond the theory, numerical resultsfor the FNM approximation of three nonlinear PtO maps demonstrate the benefitsof the operator learning perspective that this paper adopts.</description><author>Daniel Zhengyu Huang, Nicholas H. Nelsen, Margaret Trautner</author><pubDate>Thu, 06 Jun 2024 18:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06031v2</guid></item><item><title>Open-Endedness is Essential for Artificial Superhuman Intelligence</title><link>http://arxiv.org/abs/2406.04268v1</link><description>In recent years there has been a tremendous surge in the general capabilitiesof AI systems, mainly fuelled by training foundation models on internetscaledata. Nevertheless, the creation of openended, ever self-improving AI remainselusive. In this position paper, we argue that the ingredients are now in placeto achieve openendedness in AI systems with respect to a human observer.Furthermore, we claim that such open-endedness is an essential property of anyartificial superhuman intelligence (ASI). We begin by providing a concreteformal definition of open-endedness through the lens of novelty andlearnability. We then illustrate a path towards ASI via open-ended systemsbuilt on top of foundation models, capable of making novel, humanrelevantdiscoveries. We conclude by examining the safety implications ofgenerally-capable openended AI. We expect that open-ended foundation modelswill prove to be an increasingly fertile and safety-critical area of researchin the near future.</description><author>Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, Tim Rocktaschel</author><pubDate>Thu, 06 Jun 2024 18:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04268v1</guid></item><item><title>Transformers need glasses! Information over-squashing in language tasks</title><link>http://arxiv.org/abs/2406.04267v1</link><description>We study how information propagates in decoder-only Transformers, which arethe architectural backbone of most existing frontier large language models(LLMs). We rely on a theoretical signal propagation analysis -- specifically,we analyse the representations of the last token in the final layer of theTransformer, as this is the representation used for next-token prediction. Ouranalysis reveals a representational collapse phenomenon: we prove that certaindistinct sequences of inputs to the Transformer can yield arbitrarily closerepresentations in the final token. This effect is exacerbated by thelow-precision floating-point formats frequently used in modern LLMs. As aresult, the model is provably unable to respond to these sequences in differentways -- leading to errors in, e.g., tasks involving counting or copying.Further, we show that decoder-only Transformer language models can losesensitivity to specific tokens in the input, which relates to the well-knownphenomenon of over-squashing in graph neural networks. We provide empiricalevidence supporting our claims on contemporary LLMs. Our theory also points tosimple solutions towards ameliorating these issues.</description><author>Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, João G. M. Araújo, Alex Vitvitskyi, Razvan Pascanu, Petar Veličković</author><pubDate>Thu, 06 Jun 2024 18:14:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04267v1</guid></item><item><title>Multi-group Learning for Hierarchical Groups</title><link>http://arxiv.org/abs/2402.00258v2</link><description>The multi-group learning model formalizes the learning scenario in which asingle predictor must generalize well on multiple, possibly overlappingsubgroups of interest. We extend the study of multi-group learning to thenatural case where the groups are hierarchically structured. We design analgorithm for this setting that outputs an interpretable and deterministicdecision tree predictor with near-optimal sample complexity. We then conduct anempirical evaluation of our algorithm and find that it achieves attractivegeneralization properties on real datasets with hierarchical group structure.</description><author>Samuel Deng, Daniel Hsu</author><pubDate>Thu, 06 Jun 2024 18:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00258v2</guid></item><item><title>MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding</title><link>http://arxiv.org/abs/2406.04264v1</link><description>The evaluation of Long Video Understanding (LVU) performance poses animportant but challenging research problem. Despite previous efforts, theexisting video understanding benchmarks are severely constrained by severalissues, especially the insufficient lengths of videos, a lack of diversity invideo types and evaluation tasks, and the inappropriateness for evaluating LVUperformances. To address the above problems, we propose a new benchmark, calledMLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive andin-depth evaluation of LVU. MLVU presents the following critical values: 1) Thesubstantial and flexible extension of video lengths, which enables thebenchmark to evaluate LVU performance across a wide range of durations. 2) Theinclusion of various video genres, e.g., movies, surveillance footage,egocentric videos, cartoons, game videos, etc., which reflects the models' LVUperformances in different scenarios. 3) The development of diversifiedevaluation tasks, which enables a comprehensive examination of MLLMs' keyabilities in long-video understanding. The empirical study with 20 latest MLLMsreveals significant room for improvement in today's technique, as all existingmethods struggle with most of the evaluation tasks and exhibit severeperformance degradation when handling longer videos. Additionally, it suggeststhat factors such as context length, image-understanding quality, and thechoice of LLM backbone can play critical roles in future advancements. Weanticipate that MLVU will advance the research of long video understanding byproviding a comprehensive and in-depth analysis of MLLMs.</description><author>Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu</author><pubDate>Thu, 06 Jun 2024 18:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04264v1</guid></item><item><title>Simulating, Fast and Slow: Learning Policies for Black-Box Optimization</title><link>http://arxiv.org/abs/2406.04261v1</link><description>In recent years, solving optimization problems involving black-box simulatorshas become a point of focus for the machine learning community due to theirubiquity in science and engineering. The simulators describe a forward process$f_{\mathrm{sim}}: (\psi, x) \rightarrow y$ from simulation parameters $\psi$and input data $x$ to observations $y$, and the goal of the optimizationproblem is to find parameters $\psi$ that minimize a desired loss function.Sophisticated optimization algorithms typically require gradient informationregarding the forward process, $f_{\mathrm{sim}}$, with respect to theparameters $\psi$. However, obtaining gradients from black-box simulators canoften be prohibitively expensive or, in some cases, impossible. Furthermore, inmany applications, practitioners aim to solve a set of related problems. Thus,starting the optimization ``ab initio", i.e. from scratch, each time might beinefficient if the forward model is expensive to evaluate. To address thosechallenges, this paper introduces a novel method for solving classes of similarblack-box optimization problems by learning an active learning policy thatguides a differentiable surrogate's training and uses the surrogate's gradientsto optimize the simulation parameters with gradient descent. After training thepolicy, downstream optimization of problems involving black-box simulatorsrequires up to $\sim$90\% fewer expensive simulator calls compared to baselinessuch as local surrogate-based approaches, numerical optimization, and Bayesianmethods.</description><author>Fabio Valerio Massoli, Tim Bakker, Thomas Hehn, Tribhuvanesh Orekondy, Arash Behboodi</author><pubDate>Thu, 06 Jun 2024 18:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04261v1</guid></item><item><title>Reflect-RL: Two-Player Online RL Fine-Tuning for LMs</title><link>http://arxiv.org/abs/2402.12621v2</link><description>As language models (LMs) demonstrate their capabilities in various fields,their application to tasks requiring multi-round interactions has becomeincreasingly popular. These tasks usually have complex dynamics, so supervisedfine-tuning (SFT) on a limited offline dataset does not yield good performance.However, only a few works attempted to directly train the LMs withininteractive decision-making environments. We aim to create an effectiveapproach to fine-tune LMs with online reinforcement learning (RL) in theseenvironments. We propose Reflect-RL, a two-player system to fine-tune an LMusing SFT and online RL, where a frozen reflection model (player) assists thepolicy model (player). To generate data for the warm-up SFT stage, we usenegative example generation to enhance the error-correction ability of thereflection model. Furthermore, we designed single-prompt action enumeration andapplied curriculum learning to allow the policy model to learn moreefficiently. Empirically, we verify that Reflect-RL outperforms SFT and onlineRL without reflection. Testing results indicate GPT-2 XL 1.56B fine-tuned withReflect-RL outperforms larger open-source LMs, such as Mistral 7B. Thebenchmarks, dataset, and code involved in this work are publicly available:https://github.com/zhourunlong/Reflect-RL.</description><author>Runlong Zhou, Simon S. Du, Beibin Li</author><pubDate>Thu, 06 Jun 2024 18:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12621v2</guid></item><item><title>Data Measurements for Decentralized Data Markets</title><link>http://arxiv.org/abs/2406.04257v1</link><description>Decentralized data markets can provide more equitable forms of dataacquisition for machine learning. However, to realize practical marketplaces,efficient techniques for seller selection need to be developed. We propose andbenchmark federated data measurements to allow a data buyer to find sellerswith relevant and diverse datasets. Diversity and relevance measures enable abuyer to make relative comparisons between sellers without requiringintermediate brokers and training task-dependent models.</description><author>Charles Lu, Mohammad Mohammadi Amiri, Ramesh Raskar</author><pubDate>Thu, 06 Jun 2024 18:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04257v1</guid></item><item><title>GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions</title><link>http://arxiv.org/abs/2406.04254v1</link><description>We introduce a new generative approach for synthesizing 3D geometry andimages from single-view collections. Most existing approaches predictvolumetric density to render multi-view consistent images. By employingvolumetric rendering using neural radiance fields, they inherit a keylimitation: the generated geometry is noisy and unconstrained, limiting thequality and utility of the output meshes. To address this issue, we proposeGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.Initially, we reinterpret the volumetric density as a Signed Distance Function(SDF). This allows us to introduce useful priors to generate valid meshes.However, those priors prevent the generative model from learning details,limiting the applicability of the method to real-world scenarios. To alleviatethat problem, we make the transformation learnable and constrain the rendereddepth map to be consistent with the zero-level set of the SDF. Through the lensof adversarial training, we encourage the network to produce higher fidelitydetails on the output meshes. For evaluation, we introduce a synthetic datasetof human avatars captured from 360-degree camera angles, to overcome thechallenges presented by real-world datasets, which often lack 3D consistencyand do not cover all camera angles. Our experiments on multiple datasets showthat GeoGen produces visually and quantitatively better geometry than theprevious generative models based on neural radiance fields.</description><author>Salvatore Esposito, Qingshan Xu, Kacper Kania, Charlie Hewitt, Octave Mariotti, Lohit Petikam, Julien Valentin, Arno Onken, Oisin Mac Aodha</author><pubDate>Thu, 06 Jun 2024 18:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04254v1</guid></item><item><title>A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation</title><link>http://arxiv.org/abs/2406.04253v1</link><description>3D modeling has long been an important area in computer vision and computergraphics. Recently, thanks to the breakthroughs in neural representations andgenerative models, we witnessed a rapid development of 3D modeling. 3D humanmodeling, lying at the core of many real-world applications, such as gaming andanimation, has attracted significant attention. Over the past few years, alarge body of work on creating 3D human avatars has been introduced, forming anew and abundant knowledge base for 3D human modeling. The scale of theliterature makes it difficult for individuals to keep track of all the works.This survey aims to provide a comprehensive overview of these emergingtechniques for 3D human avatar modeling, from both reconstruction andgeneration perspectives. Firstly, we review representative methods for 3D humanreconstruction, including methods based on pixel-aligned implicit function,neural radiance field, and 3D Gaussian Splatting, etc. We then summarizerepresentative methods for 3D human generation, especially those using largelanguage models like CLIP, diffusion models, and various 3D representations,which demonstrate state-of-the-art performance. Finally, we discuss ourreflection on existing methods and open challenges for 3D human avatarmodeling, shedding light on future research.</description><author>Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong</author><pubDate>Thu, 06 Jun 2024 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04253v1</guid></item><item><title>Localized Gaussian Point Management</title><link>http://arxiv.org/abs/2406.04251v1</link><description>Point management is a critical component in optimizing 3D Gaussian Splatting(3DGS) models, as the point initiation (e.g., via structure from motion) isdistributionally inappropriate. Typically, the Adaptive Density Control (ADC)algorithm is applied, leveraging view-averaged gradient magnitude thresholdingfor point densification, opacity thresholding for pruning, and regularall-points opacity reset. However, we reveal that this strategy is limited intackling intricate/special image regions (e.g., transparent) as it is unable toidentify all the 3D zones that require point densification, and lacking anappropriate mechanism to handle the ill-conditioned points with negativeimpacts (occlusion due to false high opacity). To address these limitations, wepropose a Localized Point Management (LPM) strategy, capable of identifyingthose error-contributing zones in the highest demand for both point additionand geometry calibration. Zone identification is achieved by leveraging theunderlying multiview geometry constraints, with the guidance of image renderingerrors. We apply point densification in the identified zone, whilst resettingthe opacity of those points residing in front of these regions so that a newopportunity is created to correct ill-conditioned points. Serving as aversatile plugin, LPM can be seamlessly integrated into existing 3D GaussianSplatting models. Experimental evaluation across both static 3D and dynamic 4Dscenes validate the efficacy of our LPM strategy in boosting a variety ofexisting 3DGS models both quantitatively and qualitatively. Notably, LPMimproves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-artrendering quality while retaining real-time speeds, outperforming onchallenging datasets such as Tanks &amp; Temples and the Neural 3D Video Dataset.</description><author>Haosen Yang, Chenhao Zhang, Wenqing Wang, Marco Volino, Adrian Hilton, Li Zhang, Xiatian Zhu</author><pubDate>Thu, 06 Jun 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04251v1</guid></item><item><title>Online learning of quantum processes</title><link>http://arxiv.org/abs/2406.04250v1</link><description>Among recent insights into learning quantum states, online learning andshadow tomography procedures are notable for their ability to accuratelypredict expectation values even of adaptively chosen observables. In contrastto the state case, quantum process learning tasks with a similarly adaptivenature have received little attention. In this work, we investigate onlinelearning tasks for quantum processes. Whereas online learning is infeasible forgeneral quantum channels, we show that channels of bounded gate complexity aswell as Pauli channels can be online learned in the regret and mistake-boundedmodels of online learning. In fact, we can online learn probabilistic mixturesof any exponentially large set of known channels. We also provide a provablysample-efficient shadow tomography procedure for Pauli channels. Our resultsextend beyond quantum channels to non-Markovian multi-time processes, withfavorable regret and mistake bounds, as well as a shadow tomography procedure.We complement our online learning upper bounds with mistake as well ascomputational lower bounds. On the technical side, we make use of themultiplicative weights update algorithm, classical adaptive data analysis, andBell sampling, as well as tools from the theory of quantum combs for multi-timequantum processes. Our work initiates a study of online learning for classes ofquantum channels and, more generally, non-Markovian quantum processes. Giventhe importance of online learning for state shadow tomography, this may serveas a step towards quantum channel variants of adaptive shadow tomography.</description><author>Asad Raza, Matthias C. Caro, Jens Eisert, Sumeet Khatri</author><pubDate>Thu, 06 Jun 2024 17:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04250v1</guid></item><item><title>Conv-INR: Convolutional Implicit Neural Representation for Multimodal Visual Signals</title><link>http://arxiv.org/abs/2406.04249v1</link><description>Implicit neural representation (INR) has recently emerged as a promisingparadigm for signal representations. Typically, INR is parameterized by amultiplayer perceptron (MLP) which takes the coordinates as the inputs andgenerates corresponding attributes of a signal. However, MLP-based INRs facetwo critical issues: i) individually considering each coordinate while ignoringthe connections; ii) suffering from the spectral bias thus failing to learnhigh-frequency components. While target visual signals usually exhibit stronglocal structures and neighborhood dependencies, and high-frequency componentsare significant in these signals, the issues harm the representational capacityof INRs. This paper proposes Conv-INR, the first INR model fully based onconvolution. Due to the inherent attributes of convolution, Conv-INR cansimultaneously consider adjacent coordinates and learn high-frequencycomponents effectively. Compared to existing MLP-based INRs, Conv-INR hasbetter representational capacity and trainability without requiring primaryfunction expansion. We conduct extensive experiments on four tasks, includingimage fitting, CT/MRI reconstruction, and novel view synthesis, Conv-INR allsignificantly surpasses existing MLP-based INRs, validating the effectiveness.Finally, we raise three reparameterization methods that can further enhance theperformance of the vanilla Conv-INR without introducing any extra inferencecost.</description><author>Zhicheng Cai</author><pubDate>Thu, 06 Jun 2024 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04249v1</guid></item><item><title>Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks</title><link>http://arxiv.org/abs/2312.14922v3</link><description>Neural networks excel at discovering statistical patterns in high-dimensionaldata sets. In practice, higher-order cumulants, which quantify the non-Gaussiancorrelations between three or more variables, are particularly important forthe performance of neural networks. But how efficient are neural networks atextracting features from higher-order cumulants? We study this question in thespiked cumulant model, where the statistician needs to recover a privilegeddirection or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensionalinputs. Existing literature established the presence of a widestatistical-to-computational gap in this problem. We deepen this line of workby finding an exact formula for the likelihood ratio norm which proves thatstatistical distinguishability requires $n\gtrsim d$ samples, whiledistinguishing the two distributions in polynomial time requires $n \gtrsimd^2$ samples for a wide class of algorithms, i.e. those covered by thelow-degree conjecture. Numerical experiments show that neural networks doindeed learn to distinguish the two distributions with quadratic samplecomplexity, while "lazy" methods like random features are not better thanrandom guessing in this regime. Our results show that neural networks extractinformation from higher-ordercorrelations in the spiked cumulant modelefficiently, and reveal a large gap in the amount of data required by neuralnetworks and random features to learn from higher-order cumulants.</description><author>Eszter Székely, Lorenzo Bardone, Federica Gerace, Sebastian Goldt</author><pubDate>Thu, 06 Jun 2024 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14922v3</guid></item><item><title>Online learning of a panoply of quantum objects</title><link>http://arxiv.org/abs/2406.04245v1</link><description>In many quantum tasks, there is an unknown quantum object that one wishes tolearn. An online strategy for this task involves adaptively refining ahypothesis to reproduce such an object or its measurement statistics. A commonevaluation metric for such a strategy is its regret, or roughly the accumulatederrors in hypothesis statistics. We prove a sublinear regret bound for learningover general subsets of positive semidefinite matrices via theregularized-follow-the-leader algorithm and apply it to various settings whereone wishes to learn quantum objects. For concrete applications, we present asublinear regret bound for learning quantum states, effects, channels,interactive measurements, strategies, co-strategies, and the collection ofinner products of pure states. Our bound applies to many other quantum objectswith compact, convex representations. In proving our regret bound, we establishvarious matrix analysis results useful in quantum information theory. Thisincludes a generalization of Pinsker's inequality for arbitrary positivesemidefinite operators with possibly different traces, which may be ofindependent interest and applicable to more general classes of divergences.</description><author>Akshay Bansal, Ian George, Soumik Ghosh, Jamie Sikora, Alice Zheng</author><pubDate>Thu, 06 Jun 2024 17:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04245v1</guid></item><item><title>Benchmark Data Contamination of Large Language Models: A Survey</title><link>http://arxiv.org/abs/2406.04244v1</link><description>The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3,and Gemini has transformed the field of natural language processing. However,it has also resulted in a significant issue known as Benchmark DataContamination (BDC). This occurs when language models inadvertently incorporateevaluation benchmark information from their training data, leading toinaccurate or unreliable performance during the evaluation phase of theprocess. This paper reviews the complex challenge of BDC in LLM evaluation andexplores alternative assessment methods to mitigate the risks associated withtraditional benchmarks. The paper also examines challenges and futuredirections in mitigating BDC risks, highlighting the complexity of the issueand the need for innovative solutions to ensure the reliability of LLMevaluation in real-world applications.</description><author>Cheng Xu, Shuhao Guan, Derek Greene, M-Tahar Kechadi</author><pubDate>Thu, 06 Jun 2024 17:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04244v1</guid></item><item><title>FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models</title><link>http://arxiv.org/abs/2402.14116v2</link><description>One type of question that is commonly found in day-to-day scenarios is``fan-out'' questions, complex multi-hop, multi-document reasoning questionsthat require finding information about a large number of entities. However,there exist few resources to evaluate this type of question-answeringcapability among large language models. To evaluate complex reasoning in LLMsmore fully, we present FanOutQA, a high-quality dataset of fan-outquestion-answer pairs and human-annotated decompositions with English Wikipediaas the knowledge base. We formulate three benchmark settings across our datasetand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,finding that contemporary models still have room to improve reasoning overinter-document dependencies in a long context. We provide our dataset andopen-source tools to run models to encourage evaluation at https://fanoutqa.com</description><author>Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch</author><pubDate>Thu, 06 Jun 2024 17:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14116v2</guid></item><item><title>Hypernetworks for Personalizing ASR to Atypical Speech</title><link>http://arxiv.org/abs/2406.04240v1</link><description>Parameter-efficient fine-tuning (PEFT) for personalizing automatic speechrecognition (ASR) has recently shown promise for adapting general populationmodels to atypical speech. However, these approaches assume a priori knowledgeof the atypical speech disorder being adapted for -- the diagnosis of whichrequires expert knowledge that is not always available. Even given thisknowledge, data scarcity and high inter/intra-speaker variability further limitthe effectiveness of traditional fine-tuning. To circumvent these challenges,we first identify the minimal set of model parameters required for ASRadaptation. Our analysis of each individual parameter's effect on adaptationperformance allows us to reduce Word Error Rate (WER) by half while adapting0.03\% of all weights. Alleviating the need for cohort-specific models, we nextpropose the novel use of a meta-learned hypernetwork to generate highlyindividualized, utterance-level adaptations on-the-fly for a diverse set ofatypical speech characteristics. Evaluating adaptation at the global, cohortand individual-level, we show that hypernetworks generalize better toout-of-distribution speakers, while maintaining an overall relative WERreduction of 75.2% using 0.1% of the full parameter budget.</description><author>Max Mueller-Eberstein, Dianna Yee, Karren Yang, Gautam Varma Mantena, Colin Lea</author><pubDate>Thu, 06 Jun 2024 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04240v1</guid></item><item><title>Solving Inverse Problems in Protein Space Using Diffusion-Based Priors</title><link>http://arxiv.org/abs/2406.04239v1</link><description>The interaction of a protein with its environment can be understood andcontrolled via its 3D structure. Experimental methods for protein structuredetermination, such as X-ray crystallography or cryogenic electron microscopy,shed light on biological processes but introduce challenging inverse problems.Learning-based approaches have emerged as accurate and efficient methods tosolve these inverse problems for 3D structure determination, but arespecialized for a predefined type of measurement. Here, we introduce aversatile framework to turn raw biophysical measurements of varying types into3D atomic models. Our method combines a physics-based forward model of themeasurement process with a pretrained generative model providing atask-agnostic, data-driven prior. Our method outperforms posterior samplingbaselines on both linear and non-linear inverse problems. In particular, it isthe first diffusion-based method for refining atomic models from cryo-EMdensity maps.</description><author>Axel Levy, Eric R. Chan, Sara Fridovich-Keil, Frédéric Poitevin, Ellen D. Zhong, Gordon Wetzstein</author><pubDate>Thu, 06 Jun 2024 17:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04239v1</guid></item><item><title>Sparsity-Accelerated Training for Large Language Models</title><link>http://arxiv.org/abs/2406.01392v2</link><description>Large language models (LLMs) have demonstrated proficiency across variousnatural language processing (NLP) tasks but often require additional training,such as continual pre-training and supervised fine-tuning. However, the costsassociated with this, primarily due to their large parameter count, remainhigh. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs toexpedite this training process. By observing sparsity in activated neuronsduring forward iterations, we identify the potential for computationalspeed-ups by excluding inactive neurons. We address associated challenges byextending existing neuron importance evaluation metrics and introducing aladder omission rate scheduler. Our experiments on Llama-2 demonstrate thatSparsity-Accelerated Training (SAT) achieves comparable or superior performanceto standard training while significantly accelerating the process.Specifically, SAT achieves a $45\%$ throughput improvement in continualpre-training and saves $38\%$ training time in supervised fine-tuning inpractice. It offers a simple, hardware-agnostic, and easily deployableframework for additional LLM training. Our code is available athttps://github.com/OpenDFM/SAT.</description><author>Da Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu, Shuai Fan, Kai Yu</author><pubDate>Thu, 06 Jun 2024 17:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01392v2</guid></item><item><title>How Private are DP-SGD Implementations?</title><link>http://arxiv.org/abs/2403.17673v2</link><description>We demonstrate a substantial gap between the privacy guarantees of theAdaptive Batch Linear Queries (ABLQ) mechanism under different types of batchsampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis ofDifferentially Private Stochastic Gradient Descent (DP-SGD) follows byinterpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD ismore commonly used in practical implementations, it has not been amenable toeasy privacy analysis, either analytically or even numerically. On the otherhand, Poisson subsampling-based DP-SGD is challenging to scalably implement,but has a well-understood privacy analysis, with multiple open-sourcenumerically tight privacy accountants available. This has led to a commonpractice of using shuffling-based DP-SGD in practice, but using the privacyanalysis for the corresponding Poisson subsampling version. Our result showsthat there can be a substantial gap between the privacy analysis when using thetwo types of batch sampling, and thus advises caution in reporting privacyparameters for DP-SGD.</description><author>Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</author><pubDate>Thu, 06 Jun 2024 17:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17673v2</guid></item><item><title>Understanding Information Storage and Transfer in Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2406.04236v1</link><description>Understanding the mechanisms of information storage and transfer inTransformer-based models is important for driving model understanding progress.Recent work has studied these mechanisms for Large Language Models (LLMs),revealing insights on how information is stored in a model's parameters and howinformation flows to and from these parameters in response to specific prompts.However, these studies have not yet been extended to Multi-modal Large LanguageModels (MLLMs). Given their expanding capabilities and real-world use, we startby studying one aspect of these models -- how MLLMs process information in afactual visual question answering task. We use a constraint-based formulationwhich views a visual question as having a set of visual or textual constraintsthat the model's generated answer must satisfy to be correct (e.g. What moviedirected by the director in this photo has won a Golden Globe?). Under thissetting, we contribute i) a method that extends causal information tracing frompure language to the multi-modal setting, and ii) VQA-Constraints, a test-bedof 9.7K visual questions annotated with constraints. We use these tools tostudy two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings showthat these MLLMs rely on MLP and self-attention blocks in much earlier layersfor information storage, compared to LLMs whose mid-layer MLPs are moreimportant. We also show that a consistent small subset of visual tokens outputby the vision encoder are responsible for transferring information from theimage to these causal blocks. We validate these mechanisms by introducingMultEdit, a model-editing algorithm that can correct errors and insert newlong-tailed information into MLLMs by targeting these causal blocks.</description><author>Samyadeep Basu, Martin Grayson, Cecily Morrison, Besmira Nushi, Soheil Feizi, Daniela Massiceti</author><pubDate>Thu, 06 Jun 2024 17:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04236v1</guid></item><item><title>Toward Artificial Open-Ended Evolution within Lenia using Quality-Diversity</title><link>http://arxiv.org/abs/2406.04235v1</link><description>From the formation of snowflakes to the evolution of diverse life forms,emergence is ubiquitous in our universe. In the quest to understand howcomplexity can arise from simple rules, abstract computational models, such ascellular automata, have been developed to study self-organization. However, thediscovery of self-organizing patterns in artificial systems is challenging andhas largely relied on manual or semi-automatic search in the past. In thispaper, we show that Quality-Diversity, a family of Evolutionary Algorithms, isan effective framework for the automatic discovery of diverse self-organizingpatterns in complex systems. Quality-Diversity algorithms aim to evolve a largepopulation of diverse individuals, each adapted to its ecological niche.Combined with Lenia, a family of continuous cellular automata, we demonstratethat our method is able to evolve a diverse population of lifelikeself-organizing autonomous patterns. Our framework, called Leniabreeder, canleverage both manually defined diversity criteria to guide the search towardinteresting areas, as well as unsupervised measures of diversity to broaden thescope of discoverable patterns. We demonstrate both qualitatively andquantitatively that Leniabreeder offers a powerful solution for discoveringself-organizing patterns. The effectiveness of unsupervised Quality-Diversitymethods combined with the rich landscape of Lenia exhibits a sustainedgeneration of diversity and complexity characteristic of biological evolution.We provide empirical evidence that suggests unbounded diversity and argue thatLeniabreeder is a step toward replicating open-ended evolution in silico.</description><author>Maxence Faldor, Antoine Cully</author><pubDate>Thu, 06 Jun 2024 17:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04235v1</guid></item><item><title>Exploiting Code Symmetries for Learning Program Semantics</title><link>http://arxiv.org/abs/2308.03312v8</link><description>This paper tackles the challenge of teaching code semantics to Large LanguageModels (LLMs) for program analysis by incorporating code symmetries into themodel architecture. We introduce a group-theoretic framework that defines codesymmetries as semantics-preserving transformations, where forming a codesymmetry group enables precise and efficient reasoning of code semantics. Oursolution, SymC, develops a novel variant of self-attention that is provablyequivariant to code symmetries from the permutation group defined over theprogram dependence graph. SymC obtains superior performance on five programanalysis tasks, outperforming state-of-the-art code models without anypre-training. Our results suggest that code LLMs that encode the codestructural prior via the code symmetry group generalize better and faster.</description><author>Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</author><pubDate>Thu, 06 Jun 2024 17:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03312v8</guid></item><item><title>FairytaleQA Translated: Enabling Educational Question and Answer Generation in Less-Resourced Languages</title><link>http://arxiv.org/abs/2406.04233v1</link><description>Question Answering (QA) datasets are crucial in assessing readingcomprehension skills for both machines and humans. While numerous datasets havebeen developed in English for this purpose, a noticeable void exists inless-resourced languages. To alleviate this gap, our paper introducesmachine-translated versions of FairytaleQA, a renowned QA dataset designed toassess and enhance narrative comprehension skills in young children. Byemploying fine-tuned, modest-scale models, we establish benchmarks for bothQuestion Generation (QG) and QA tasks within the translated datasets. Inaddition, we present a case study proposing a model for generatingquestion-answer pairs, with an evaluation incorporating quality metrics such asquestion well-formedness, answerability, relevance, and children suitability.Our evaluation prioritizes quantifying and describing error cases, along withproviding directions for future work. This paper contributes to the advancementof QA and QG research in less-resourced languages, promoting accessibility andinclusivity in the development of these models for reading comprehension. Thecode and data is publicly available atgithub.com/bernardoleite/fairytaleqa-translated.</description><author>Bernardo Leite, Tomás Freitas Osório, Henrique Lopes Cardoso</author><pubDate>Thu, 06 Jun 2024 17:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04233v1</guid></item><item><title>Quantifying Misalignment Between Agents</title><link>http://arxiv.org/abs/2406.04231v1</link><description>Growing concerns about the AI alignment problem have emerged in recent years,with previous work focusing mainly on (1) qualitative descriptions of thealignment problem; (2) attempting to align AI actions with human interests byfocusing on value specification and learning; and/or (3) focusing on a singleagent or on humanity as a singular unit. Recent work in sociotechnical AIalignment has made some progress in defining alignment inclusively, but thefield as a whole still lacks a systematic understanding of how to specify,describe, and analyze misalignment among entities, which may include individualhumans, AI agents, and complex compositional entities such as corporations,nation-states, and so forth. Previous work on controversy in computationalsocial science offers a mathematical model of contention among populations (ofhumans). In this paper, we adapt this contention model to the alignmentproblem, and show how misalignment can vary depending on the population ofagents (human or otherwise) being observed, the domain in question, and theagents' probability-weighted preferences between possible outcomes. Our modeldeparts from value specification approaches and focuses instead on the morassof complex, interlocking, sometimes contradictory goals that agents may have inpractice. We apply our model by analyzing several case studies ranging fromsocial media moderation to autonomous vehicle behavior. By applying our modelwith appropriately representative value data, AI engineers can ensure thattheir systems learn values maximally aligned with diverse human interests.</description><author>Aidan Kierans, Avijit Ghosh, Hananel Hazan, Shiri Dori-Hacohen</author><pubDate>Thu, 06 Jun 2024 17:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04231v1</guid></item><item><title>M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and RGB Data</title><link>http://arxiv.org/abs/2406.04230v1</link><description>Satellite-based remote sensing has revolutionised the way we address globalchallenges in a rapidly evolving world. Huge quantities of Earth Observation(EO) data are generated by satellite sensors daily, but processing these largedatasets for use in ML pipelines is technically and computationallychallenging. Specifically, different types of EO data are often hosted on avariety of platforms, with differing availability for Python preprocessingtools. In addition, spatial alignment across data sources and data tiling canpresent significant technical hurdles for novice users. While some preprocessedEO datasets exist, their content is often limited to optical or near-opticalwavelength data, which is ineffective at night or in adverse weatherconditions. Synthetic Aperture Radar (SAR), an active sensing technique basedon microwave length radiation, offers a viable alternative. However, theapplication of machine learning to SAR has been limited due to a lack ofML-ready data and pipelines, particularly for the full diversity of SAR data,including polarimetry, coherence and interferometry. We introduce M3LEO, amulti-modal, multi-label EO dataset that includes polarimetric,interferometric, and coherence SAR data derived from Sentinel-1, alongsideSentinel-2 RGB imagery and a suite of labelled tasks for model evaluation.M3LEO spans 17.5TB and contains approximately 10M data chips across sixgeographic regions. The dataset is complemented by a flexible PyTorch Lightningframework, with configuration management using Hydra. We provide tools toprocess any dataset available on popular platforms such as Google Earth Enginefor integration with our framework. Initial experiments validate the utility ofour data and framework, showing that SAR imagery contains informationadditional to that extractable from RGB data. Data at huggingface.co/M3LEO, andcode at github.com/spaceml-org/M3LEO.</description><author>Matthew J Allen, Francisco Dorr, Joseph Alejandro Gallego Mejia, Laura Martínez-Ferrer, Anna Jungbluth, Freddie Kalaitzis, Raúl Ramos-Pollán</author><pubDate>Thu, 06 Jun 2024 17:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04230v1</guid></item><item><title>The CLRS-Text Algorithmic Reasoning Language Benchmark</title><link>http://arxiv.org/abs/2406.04229v1</link><description>Eliciting reasoning capabilities from language models (LMs) is a criticaldirection on the path towards building intelligent systems. Most recent studiesdedicated to reasoning focus on out-of-distribution performance onprocedurally-generated synthetic benchmarks, bespoke-built to evaluate specificskills only. This trend makes results hard to transfer across publications,slowing down progress. Three years ago, a similar issue was identified andrectified in the field of neural algorithmic reasoning, with the advent of theCLRS benchmark. CLRS is a dataset generator comprising graph execution tracesof classical algorithms from the Introduction to Algorithms textbook. Inspiredby this, we propose CLRS-Text -- a textual version of these algorithmic traces.Out of the box, CLRS-Text is capable of procedurally generating trace data forthirty diverse, challenging algorithmic tasks across any desirable inputdistribution, while offering a standard pipeline in which any additionalalgorithmic tasks may be created in the benchmark. We fine-tune and evaluatevarious LMs as generalist executors on this benchmark, validating prior workand revealing a novel, interesting challenge for the LM reasoning community.Our code is available athttps://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.</description><author>Larisa Markeeva, Sean McLeish, Borja Ibarz, Wilfried Bounsi, Olga Kozlova, Alex Vitvitskyi, Charles Blundell, Tom Goldstein, Avi Schwarzschild, Petar Veličković</author><pubDate>Thu, 06 Jun 2024 17:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04229v1</guid></item><item><title>R-CONV: An Analytical Approach for Efficient Data Reconstruction via Convolutional Gradients</title><link>http://arxiv.org/abs/2406.04227v1</link><description>In the effort to learn from extensive collections of distributed data,federated learning has emerged as a promising approach for preserving privacyby using a gradient-sharing mechanism instead of exchanging raw data. However,recent studies show that private training data can be leaked through manygradient attacks. While previous analytical-based attacks have successfullyreconstructed input data from fully connected layers, their effectivenessdiminishes when applied to convolutional layers. This paper introduces anadvanced data leakage method to efficiently exploit convolutional layers'gradients. We present a surprising finding: even with non-fully invertibleactivation functions, such as ReLU, we can analytically reconstruct trainingsamples from the gradients. To the best of our knowledge, this is the firstanalytical approach that successfully reconstructs convolutional layer inputsdirectly from the gradients, bypassing the need to reconstruct layers' outputs.Prior research has mainly concentrated on the weight constraints of convolutionlayers, overlooking the significance of gradient constraints. Our findingsdemonstrate that existing analytical methods used to estimate the risk ofgradient attacks lack accuracy. In some layers, attacks can be launched withless than 5% of the reported constraints.</description><author>Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, Adnan Qayyum, Junaid Qadir</author><pubDate>Thu, 06 Jun 2024 17:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04227v1</guid></item><item><title>Machine Learning-Assisted Discovery of Flow Reactor Designs</title><link>http://arxiv.org/abs/2308.08841v3</link><description>Additive manufacturing has enabled the fabrication of advanced reactorgeometries, permitting larger, more complex design spaces. Identifyingpromising configurations within such spaces presents a significant challengefor current approaches. Furthermore, existing parameterisations of reactorgeometries are low-dimensional with expensive optimisation limiting morecomplex solutions. To address this challenge, we establish a machinelearning-assisted approach for the design of the next-generation of chemicalreactors, combining the application of high-dimensional parameterisations,computational fluid dynamics, and multi-fidelity Bayesian optimisation. Weassociate the development of mixing-enhancing vortical flow structures in novelcoiled reactors with performance, and use our approach to identify keycharacteristics of optimal designs. By appealing to the principles of flowdynamics, we rationalise the selection of novel design features that lead toexperimental plug flow performance improvements of 60% over conventionaldesigns. Our results demonstrate that coupling advanced manufacturingtechniques with `augmented-intelligence' approaches can lead to superior designperformance and, consequently, emissions-reduction and sustainability.</description><author>Tom Savage, Nausheen Basha, Jonathan McDonough, James Krassowski, Omar K Matar, Ehecatl Antonio del Rio Chanona</author><pubDate>Thu, 06 Jun 2024 17:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08841v3</guid></item><item><title>Wake Vision: A Large-scale, Diverse Dataset and Benchmark Suite for TinyML Person Detection</title><link>http://arxiv.org/abs/2405.00892v2</link><description>Tiny machine learning (TinyML), which enables machine learning applicationson extremely low-power devices, suffers from limited size and quality ofrelevant datasets. To address this issue, we introduce Wake Vision, alarge-scale, diverse dataset tailored for person detection, the canonical taskfor TinyML visual sensing. Wake Vision comprises over 6 million images,representing a hundredfold increase compared to the previous standard, and hasundergone thorough quality filtering. We provide two Wake Vision training sets:Wake Vision (Large) and Wake Vision (Quality), a smaller set withhigher-quality labels. Our results demonstrate that using the Wake Vision(Quality) training set produces more accurate models than the Wake Vision(Large) training set, strongly suggesting that label quality is more importantthan quantity in our setting. We find use for the large training set forpre-training and knowledge distillation. To minimize label errors that canobscure true model performance, we manually label the validation and test sets,improving the test set error rate from 7.8% in the prior standard to only 2.2%.In addition to the dataset, we provide a collection of five detailed benchmarksets to facilitate the evaluation of model quality in challenging real worldscenarios that are often ignored when focusing solely on overall accuracy.These novel fine-grained benchmarks assess model performance on specificsegments of the test data, such as varying lighting conditions, distances fromthe camera, and demographic characteristics of subjects. Our resultsdemonstrate that using Wake Vision for training results in a 2.49% increase inaccuracy compared to the established dataset. We also show the importance ofdataset quality for low-capacity models and the value of dataset size forhigh-capacity models. wakevision.ai</description><author>Colby Banbury, Emil Njor, Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries, Xenofon Fafoutis, Vijay Janapa Reddi</author><pubDate>Thu, 06 Jun 2024 17:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00892v2</guid></item><item><title>Matching Anything by Segmenting Anything</title><link>http://arxiv.org/abs/2406.04221v1</link><description>The robust association of the same objects across video frames in complexscenes is crucial for many applications, especially Multiple Object Tracking(MOT). Current methods predominantly rely on labeled domain-specific videodatasets, which limits the cross-domain generalization of learned similarityembeddings. We propose MASA, a novel method for robust instance associationlearning, capable of matching any objects within videos across diverse domainswithout tracking labels. Leveraging the rich object segmentation from theSegment Anything Model (SAM), MASA learns instance-level correspondence throughexhaustive data transformations. We treat the SAM outputs as dense objectregion proposals and learn to match those regions from a vast image collection.We further design a universal MASA adapter which can work in tandem withfoundational segmentation or detection models and enable them to track anydetected objects. Those combinations present strong zero-shot tracking abilityin complex domains. Extensive tests on multiple challenging MOT and MOTSbenchmarks indicate that the proposed method, using only unlabeled staticimages, achieves even better performance than state-of-the-art methods trainedwith fully annotated in-domain video sequences, in zero-shot association.Project Page: https://matchinganything.github.io/</description><author>Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu</author><pubDate>Thu, 06 Jun 2024 17:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04221v1</guid></item><item><title>BEADs: Bias Evaluation Across Domains</title><link>http://arxiv.org/abs/2406.04220v1</link><description>Recent improvements in large language models (LLMs) have significantlyenhanced natural language processing (NLP) applications. However, these modelscan also inherit and perpetuate biases from their training data. Addressingthis issue is crucial, yet many existing datasets do not offer evaluationacross diverse NLP tasks. To tackle this, we introduce the Bias EvaluationsAcross Domains (BEADs) dataset, designed to support a wide range of NLP tasks,including text classification, bias entity recognition, bias quantification,and benign language generation. BEADs uses AI-driven annotation combined withexperts' verification to provide reliable labels. This method overcomes thelimitations of existing datasets that typically depend on crowd-sourcing,expert-only annotations with limited bias evaluations, or unverified AIlabeling. Our empirical analysis shows that BEADs is effective in detecting andreducing biases across different language models, with smaller modelsfine-tuned on BEADs often outperforming LLMs in bias classification tasks.However, these models may still exhibit biases towards certain demographics.Fine-tuning LLMs with our benign language data also reduces biases whilepreserving the models' knowledge. Our findings highlight the importance ofcomprehensive bias evaluation and the potential of targeted fine-tuning forreducing the bias of LLMs. We are making BEADs publicly available athttps://huggingface.co/datasets/shainar/BEAD Warning: This paper contains examples that may be considered offensive.</description><author>Shaina Raza, Mizanur Rahman, Michael R. Zhang</author><pubDate>Thu, 06 Jun 2024 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04220v1</guid></item><item><title>Multi-Agent Imitation Learning: Value is Easy, Regret is Hard</title><link>http://arxiv.org/abs/2406.04219v1</link><description>We study a multi-agent imitation learning (MAIL) problem where we take theperspective of a learner attempting to coordinate a group of agents based ondemonstrations of an expert doing so. Most prior work in MAIL essentiallyreduces the problem to matching the behavior of the expert within the supportof the demonstrations. While doing so is sufficient to drive the value gapbetween the learner and the expert to zero under the assumption that agents arenon-strategic, it does not guarantee robustness to deviations by strategicagents. Intuitively, this is because strategic deviations can depend on acounterfactual quantity: the coordinator's recommendations outside of the statedistribution their recommendations induce. In response, we initiate the studyof an alternative objective for MAIL in Markov Games we term the regret gapthat explicitly accounts for potential deviations by agents in the group. Wefirst perform an in-depth exploration of the relationship between the value andregret gaps. First, we show that while the value gap can be efficientlyminimized via a direct extension of single-agent IL algorithms, even valueequivalence can lead to an arbitrarily large regret gap. This implies thatachieving regret equivalence is harder than achieving value equivalence inMAIL. We then provide a pair of efficient reductions to no-regret online convexoptimization that are capable of minimizing the regret gap (a) under a coverageassumption on the expert (MALICE) or (b) with access to a queryable expert(BLADES).</description><author>Jingwu Tang, Gokul Swamy, Fei Fang, Zhiwei Steven Wu</author><pubDate>Thu, 06 Jun 2024 17:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04219v1</guid></item><item><title>Rethinking LLM and Linguistic Steganalysis: An Efficient Detection of Strongly Concealed Stego</title><link>http://arxiv.org/abs/2406.04218v1</link><description>To detect stego (steganographic text) in complex scenarios, linguisticsteganalysis (LS) with various motivations has been proposed and achievedexcellent performance. However, with the development of generativesteganography, some stegos have strong concealment, especially after theemergence of LLMs-based steganography, the existing LS has low detection oreven cannot detect them. We designed a novel LS with two modes called LSGC. Inthe generation mode, we created an LS-task "description" and used thegeneration ability of LLM to explain whether texts to be detected are stegos.On this basis, we rethought the principle of LS and LLMs, and proposed theclassification mode. In this mode, LSGC deleted the LS-task "description" andchanged the "causalLM" LLMs to the "sequenceClassification" architecture. TheLS features can be extracted by only one pass of the model, and a linear layerwith initialization weights is added to obtain the classification probability.Experiments on strongly concealed stegos show that LSGC significantly improvesdetection and reaches SOTA performance. Additionally, LSGC in classificationmode greatly reduces training time while maintaining high performance.</description><author>Yifan Tang, Yihao Wang, Ru Zhang, Jianyi Liu</author><pubDate>Thu, 06 Jun 2024 17:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04218v1</guid></item></channel></rss>