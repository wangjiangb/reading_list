<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 31 Jul 2024 01:00:30 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Mixture of Nested Experts: Adaptive Processing of Visual Tokens</title><link>http://arxiv.org/abs/2407.19985v2</link><description>The visual medium (images and videos) naturally contains a large amount ofinformation redundancy, thereby providing a great opportunity for leveragingefficiency in processing. While Vision Transformer (ViT) based models scaleeffectively to large data regimes, they fail to capitalize on this inherentredundancy, leading to higher computational costs. Mixture of Experts (MoE)networks demonstrate scalability while maintaining same inference-time costs,but they come with a larger parameter footprint. We present Mixture of NestedExperts (MoNE), which utilizes a nested structure for experts, whereinindividual experts fall on an increasing compute-accuracy curve. Given acompute budget, MoNE learns to dynamically choose tokens in a priority order,and thus redundant tokens are processed through cheaper nested experts. Usingthis framework, we achieve equivalent performance as the baseline models, whilereducing inference time compute by over two-fold. We validate our approach onstandard image and video datasets - ImageNet-21K, Kinetics400, andSomething-Something-v2. We further highlight MoNE$'$s adaptability byshowcasing its ability to maintain strong performance across differentinference-time compute budgets on videos, using only a single trained model.</description><author>Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, Sujoy Paul</author><pubDate>Tue, 30 Jul 2024 17:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19985v2</guid></item><item><title>SpaER: Learning Spatio-temporal Equivariant Representations for Fetal Brain Motion Tracking</title><link>http://arxiv.org/abs/2407.20198v2</link><description>In this paper, we introduce SpaER, a pioneering method for fetal motiontracking that leverages equivariant filters and self-attention mechanisms toeffectively learn spatio-temporal representations. Different from conventionalapproaches that statically estimate fetal brain motions from pairs of images,our method dynamically tracks the rigid movement patterns of the fetal headacross temporal and spatial dimensions. Specifically, we first develop anequivariant neural network that efficiently learns rigid motion sequencesthrough low-dimensional spatial representations of images. Subsequently, welearn spatio-temporal representations by incorporating time encoding andself-attention neural network layers. This approach allows for the capture oflong-term dependencies of fetal brain motion and addresses alignment errors dueto contrast changes and severe motion artifacts. Our model also provides ageometric deformation estimation that properly addresses image distortionsamong all time frames. To the best of our knowledge, our approach is the firstto learn spatial-temporal representations via deep neural networks for fetalmotion tracking without data augmentation. We validated our model using realfetal echo-planar images with simulated and real motions. Our method carriessignificant potential value in accurately measuring, tracking, and correctingfetal motion in fetal MRI sequences.</description><author>Jian Wang, Razieh Faghihpirayesh, Polina Golland, Ali Ghoulipour</author><pubDate>Tue, 30 Jul 2024 15:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20198v2</guid></item><item><title>Neural networks for bifurcation and linear stability analysis of steady states in partial differential equations</title><link>http://arxiv.org/abs/2407.19707v2</link><description>This research introduces an extended application of neural networks forsolving nonlinear partial differential equations (PDEs). A neural network,combined with a pseudo-arclength continuation, is proposed to constructbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neuralnetwork approach is also presented for solving eigenvalue problems to analyzesolution linear stability, focusing on identifying the largest eigenvalue. Theeffectiveness of the proposed neural network is examined through experiments onthe Bratu equation and the Burgers equation. Results from a finite differencemethod are also presented as comparison. Varying numbers of grid points areemployed in each case to assess the behavior and accuracy of both the neuralnetwork and the finite difference method. The experimental results demonstratethat the proposed neural network produces better solutions, generates moreaccurate bifurcation diagrams, has reasonable computational times, and proveseffective for linear stability analysis.</description><author>Muhammad Luthfi Shahab, Hadi Susanto</author><pubDate>Tue, 30 Jul 2024 14:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19707v2</guid></item><item><title>A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation</title><link>http://arxiv.org/abs/2407.18483v3</link><description>Ophthalmology consultations are crucial for diagnosing, treating, andpreventing eye diseases. However, the growing demand for consultations exceedsthe availability of ophthalmologists. By leveraging large pre-trained languagemodels, we can design effective dialogues for specific scenarios, aiding inconsultations. Traditional fine-tuning strategies for question-answering tasksare impractical due to increasing model size and often ignoring patient-doctorrole function during consultations. In this paper, we propose EyeDoctor, anophthalmic medical questioning large language model that enhances accuracythrough doctor-patient role perception guided and an augmented knowledge basewith external disease information. Experimental results show EyeDoctor achieveshigher question-answering precision in ophthalmology consultations. Notably,EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%improvement in F1 scores on multi-round datasets compared to second best modelChatGPT, highlighting the importance of doctor-patient role differentiation anddynamic knowledge base expansion for intelligent medical consultations. EyeDocalso serves as a free available web based service and souce code is availableat https://github.com/sperfu/EyeDoc.</description><author>Laiyi Fu, Binbin Fan, Hongkai Du, Yanxiang Feng, Chunhua Li, Huping Song</author><pubDate>Tue, 30 Jul 2024 12:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18483v3</guid></item><item><title>Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network</title><link>http://arxiv.org/abs/2407.19768v2</link><description>Face super-resolution aims to reconstruct a high-resolution face image from alow-resolution face image. Previous methods typically employ an encoder-decoderstructure to extract facial structural features, where the direct downsamplinginevitably introduces distortions, especially to high-frequency features suchas edges. To address this issue, we propose a wavelet-based feature enhancementnetwork, which mitigates feature distortion by losslessly decomposing the inputfeature into high and low-frequency components using the wavelet transform andprocessing them separately. To improve the efficiency of facial featureextraction, a full domain Transformer is further proposed to enhance local,regional, and global facial features. Such designs allow our method to performbetter without stacking many modules as previous methods did. Experiments showthat our method effectively balances performance, model size, and speed. Codelink: https://github.com/PRIS-CV/WFEN.</description><author>Wenjie Li, Heng Guo, Xuannan Liu, Kongming Liang, Jiani Hu, Zhanyu Ma, Jun Guo</author><pubDate>Tue, 30 Jul 2024 12:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19768v2</guid></item><item><title>Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery</title><link>http://arxiv.org/abs/2407.18752v3</link><description>Causal discovery aims to estimate causal structures among variables based onobservational data. Large Language Models (LLMs) offer a fresh perspective totackle the causal discovery problem by reasoning on the metadata associatedwith variables rather than their actual data values, an approach referred to asknowledge-based causal discovery. In this paper, we investigate thecapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1billion parameters) with prompt-based learning for knowledge-based causaldiscovery. Specifically, we present KG Structure as Prompt, a novel approachfor integrating structural information from a knowledge graph, such as commonneighbor nodes and metapaths, into prompt-based learning to enhance thecapabilities of SLMs. Experimental results on three types of biomedical andopen-domain datasets under few-shot settings demonstrate the effectiveness ofour approach, surpassing most baselines and even conventional fine-tuningapproaches trained on full datasets. Our findings further highlight the strongcapabilities of SLMs: in combination with knowledge graphs and prompt-basedlearning, SLMs demonstrate the potential to surpass LLMs with larger number ofparameters. Our code and datasets are available on GitHub.</description><author>Yuni Susanti, Michael FÃ¤rber</author><pubDate>Tue, 30 Jul 2024 12:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18752v3</guid></item><item><title>F-KANs: Federated Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2407.20100v2</link><description>In this paper, we present an innovative federated learning (FL) approach thatutilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. Byutilizing the adaptive activation capabilities of KANs in a federatedframework, we aim to improve classification capabilities while preservingprivacy. The study evaluates the performance of federated KANs (F- KANs)compared to traditional Multi-Layer Perceptrons (MLPs) on classification task.The results show that the F-KANs model significantly outperforms the federatedMLP model in terms of accuracy, precision, recall, F1 score and stability, andachieves better performance, paving the way for more efficient andprivacy-preserving predictive analytics.</description><author>Engin Zeydan, Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus, Abdullah Aydeger</author><pubDate>Tue, 30 Jul 2024 11:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20100v2</guid></item><item><title>Dynamic Spiking Framework for Graph Neural Networks</title><link>http://arxiv.org/abs/2401.05373v3</link><description>The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks(GNNs) is gradually attracting attention due to the low power consumption andhigh efficiency in processing the non-Euclidean data represented by graphs.However, as a common problem, dynamic graph representation learning faceschallenges such as high complexity and large memory overheads. Current workoften uses SNNs instead of Recurrent Neural Networks (RNNs) by using binaryfeatures instead of continuous ones for efficient training, which wouldoverlooks graph structure information and leads to the loss of details duringpropagation. Additionally, optimizing dynamic spiking models typically requirespropagation of information across time steps, which increases memoryrequirements. To address these challenges, we present a framework named\underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph\underline{N}eural Networks (\method{}). To mitigate the information lossproblem, \method{} propagates early-layer information directly to the lastlayer for information compensation. To accommodate the memory requirements, weapply the implicit differentiation on the equilibrium state, which does notrely on the exact reverse of the forward computation. While traditionalimplicit differentiation methods are usually used for static situations,\method{} extends it to the dynamic graph setting. Extensive experiments onthree large-scale real-world dynamic graph datasets validate the effectivenessof \method{} on dynamic node classification tasks with lower computationalcosts.</description><author>Nan Yin, Mengzhu Wang, Zhenghan Chen, Giulia De Masi, Bin Gu, Huan Xiong</author><pubDate>Tue, 30 Jul 2024 09:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05373v3</guid></item><item><title>Classification of freshwater snails of the genus Radomaniola with multimodal triplet networks</title><link>http://arxiv.org/abs/2407.20013v2</link><description>In this paper, we present our first proposal of a machine learning system forthe classification of freshwater snails of the genus Radomaniola. We elaborateon the specific challenges encountered during system design, and how we tackledthem; namely a small, very imbalanced dataset with a high number of classes andhigh visual similarity between classes. We then show how we employed tripletnetworks and the multiple input modalities of images, measurements, and geneticinformation to overcome these challenges and reach a performance comparable tothat of a trained domain expert.</description><author>Dennis Vetter, Muhammad Ahsan, Diana Delicado, Thomas A. Neubauer, Thomas Wilke, Gemma Roig</author><pubDate>Tue, 30 Jul 2024 08:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20013v2</guid></item><item><title>CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare</title><link>http://arxiv.org/abs/2407.19705v2</link><description>The rapid progress in Large Language Models (LLMs) has prompted the creationof numerous benchmarks to evaluate their capabilities.This study focuses on theComprehensive Medical Benchmark in Chinese (CMB), showcasing how datasetdiversity and distribution in supervised fine-tuning (SFT) may enhance LLMperformance.Remarkably, We successfully trained a smaller base model to achievescores comparable to larger models, indicating that a diverse andwell-distributed dataset can optimize performance regardless of model size.Thisstudy suggests that even smaller models may reach high performance levels withcarefully curated and varied datasets. By integrating a wide range ofinstructional content, our approach addresses potential issues such as dataquality inconsistencies. Our results imply that a broader spectrum of trainingdata may enhance a model's ability to generalize and perform effectively acrossdifferent medical scenarios, highlighting the importance of dataset quality anddiversity in fine-tuning processes. We open-source the model for futureresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT</description><author>Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny</author><pubDate>Tue, 30 Jul 2024 08:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19705v2</guid></item><item><title>AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics</title><link>http://arxiv.org/abs/2407.20124v2</link><description>The rapid evolution of multimedia and computer vision technologies requiresadaptive visual model deployment strategies to effectively handle diverse tasksand varying environments. This work introduces AxiomVision, a novel frameworkthat can guarantee accuracy by leveraging edge computing to dynamically selectthe most efficient visual models for video analytics under diverse scenarios.Utilizing a tiered edge-cloud architecture, AxiomVision enables the deploymentof a broad spectrum of visual models, from lightweight to complex DNNs, thatcan be tailored to specific scenarios while considering camera source impacts.In addition, AxiomVision provides three core innovations: (1) a dynamic visualmodel selection mechanism utilizing continual online learning, (2) an efficientonline method that efficiently takes into account the influence of the camera'sperspective, and (3) a topology-driven grouping approach that accelerates themodel selection process. With rigorous theoretical guarantees, theseadvancements provide a scalable and effective solution for visual tasksinherent to multimedia systems, such as object detection, classification, andcounting. Empirically, AxiomVision achieves a 25.7\% improvement in accuracy.</description><author>Xiangxiang Dai, Zeyu Zhang, Peng Yang, Yuedong Xu, Xutong Liu, John C. S. Lui</author><pubDate>Tue, 30 Jul 2024 07:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20124v2</guid></item><item><title>ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting</title><link>http://arxiv.org/abs/2407.10768v4</link><description>Long time series forecasting aims to utilize historical information toforecast future states over extended horizons. Traditional RNN-based seriesforecasting methods struggle to effectively address long-term dependencies andgradient issues in long time series problems. Recently, SegRNN has emerged as aleading RNN-based model tailored for long-term series forecasting,demonstrating state-of-the-art performance while maintaining a streamlinedarchitecture through innovative segmentation and parallel decoding techniques.Nevertheless, SegRNN has several limitations: its fixed segmentation disruptsdata continuity and fails to effectively leverage information across differentsegments, the segmentation strategy employed by SegRNN does not fundamentallyaddress the issue of information loss within the recurrent structure. Toaddress these issues, we propose the ISMRNN method with three key enhancements:we introduce an implicit segmentation structure to decompose the time seriesand map it to segmented hidden states, resulting in denser information exchangeduring the segmentation phase. Additionally, we incorporate residual structuresin the encoding layer to mitigate information loss within the recurrentstructure. To extract information more effectively, we further integrate theMamba architecture to enhance time series information extraction. Experimentson several real-world long time series forecasting datasets demonstrate thatour model surpasses the performance of current state-of-the-art models.</description><author>GaoXiang Zhao, Li Zhou, XiaoQiang Wang</author><pubDate>Tue, 30 Jul 2024 07:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10768v4</guid></item><item><title>Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number</title><link>http://arxiv.org/abs/2407.20119v2</link><description>We introduce a novel self-supervised deep clustering approach tailored forunstructured data without requiring prior knowledge of the number of clusters,termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRCadaptively learns the graph structure and edge weights to capture both localand global structural information. The obtained graph enables us to learnclustering-friendly feature representations by an enhanced graph auto-encoderwith contrastive learning technique. It further leverages the clusteringresults adaptively obtained by robust continuous clustering (RCC) to generateprototypes for negative sampling, which can further contribute to promotingconsistency among positive pairs and enlarging the gap between positive andnegative samples. ASRC obtains the final clustering results by applying RCC tothe learned feature representations with their consistent graph structure andedge weights. Extensive experiments conducted on seven benchmark datasetsdemonstrate the efficacy of ASRC, demonstrating its superior performance overother popular clustering models. Notably, ASRC even outperforms methods thatrely on prior knowledge of the number of clusters, highlighting itseffectiveness in addressing the challenges of clustering unstructured data.</description><author>Chen-Lu Ding, Jiancan Wu, Wei Lin, Shiyang Shen, Xiang Wang, Yancheng Yuan</author><pubDate>Tue, 30 Jul 2024 06:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20119v2</guid></item><item><title>Advancing Prompt Learning through an External Layer</title><link>http://arxiv.org/abs/2407.19674v2</link><description>Prompt learning represents a promising method for adapting pre-trainedvision-language models (VLMs) to various downstream tasks by learning a set oftext embeddings. One challenge inherent to these methods is the poorgeneralization performance due to the invalidity of the learned text embeddingsfor unseen tasks. A straightforward approach to bridge this gap is to freezethe text embeddings in prompts, which results in a lack of capacity to adaptVLMs for downstream tasks. To address this dilemma, we propose a paradigmcalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose atextual external layer and learnable visual embeddings for adapting VLMs todownstream tasks. The learnable external layer is built upon valid embeddingsof pre-trained CLIP. This design considers the balance of learning capabilitiesbetween the two branches. To align the textual and visual features, we proposea novel two-pronged approach: i) we introduce the optimal transport as thediscrepancy metric to align the vision and text modalities, and ii) weintroduce a novel strengthening feature to enhance the interaction betweenthese two modalities. Four representative experiments (i.e., base-to-novelgeneralization, few-shot learning, cross-dataset generalization, domain shiftsgeneralization) across 15 datasets demonstrate that our method outperforms theexisting prompt learning method.</description><author>Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian</author><pubDate>Tue, 30 Jul 2024 05:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19674v2</guid></item><item><title>Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</title><link>http://arxiv.org/abs/2407.19593v2</link><description>Creating photorealistic avatars for individuals traditionally involvesextensive capture sessions with complex and expensive studio devices like theLightStage system. While recent strides in neural representations have enabledthe generation of photorealistic and animatable 3D avatars from quick phonescans, they have the capture-time lighting baked-in, lack facial details andhave missing regions in areas such as the back of the ears. Thus, they lag inquality compared to studio-captured avatars. In this paper, we propose a methodthat bridges this gap by generating studio-like illuminated texture maps fromshort, monocular phone captures. We do this by parameterizing the phone texturemaps using the $W^+$ space of a StyleGAN2, enabling near-perfectreconstruction. Then, we finetune a StyleGAN2 by sampling in the $W^+$parameterized space using a very small set of studio-captured textures as anadversarial training signal. To further enhance the realism and accuracy offacial details, we super-resolve the output of the StyleGAN2 using carefullydesigned diffusion model that is guided by image gradients of thephone-captured texture map. Once trained, our method excels at producingstudio-like facial texture maps from casual monocular smartphone videos.Demonstrating its capabilities, we showcase the generation of photorealistic,uniformly lit, complete avatars from monocular phone captures. The project pagecan be found at http://shahrukhathar.github.io/2024/07/22/Bridging.html</description><author>ShahRukh Athar, Shunsuke Saito, Zhengyu Yang, Stanislav Pidhorsky, Chen Cao</author><pubDate>Tue, 30 Jul 2024 02:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19593v2</guid></item><item><title>Enhancing Training Efficiency Using Packing with Flash Attention</title><link>http://arxiv.org/abs/2407.09105v4</link><description>Padding is often used in tuning LLM models by adding special tokens toshorter training examples to match the length of the longest sequence in eachbatch. While this ensures uniformity for batch processing, it introducesinefficiencies by including irrelevant padding tokens in the computation andwastes GPU resources. Hugging Face SFT trainer has always offered the option touse packing to combine multiple training examples, allowing for maximalutilization of GPU resources. However, up till now, it did not offer propermasking of each packed training example. This capability has now been added toHugging Face Transformers 4.43. We analyse this new feature and show thebenefits across different variations of packing.</description><author>Achintya Kundu, Rhui Dih Lee, Laura Wynter, Raghu Kiran Ganti, Mayank Mishra</author><pubDate>Tue, 30 Jul 2024 02:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09105v4</guid></item><item><title>MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity</title><link>http://arxiv.org/abs/2407.20021v2</link><description>Data-free quantization (DFQ) is a technique that creates a lightweightnetwork from its full-precision counterpart without the original training data,often through a synthetic dataset. Although several DFQ methods have beenproposed for vision transformer (ViT) architectures, they fail to achieveefficacy in low-bit settings. Examining the existing methods, we identify thattheir synthetic data produce misaligned attention maps, while those of the realsamples are highly aligned. From the observation of aligned attention, we findthat aligning attention maps of synthetic data helps to improve the overallperformance of quantized ViTs. Motivated by this finding, we devise \aname, anovel DFQ method designed for ViTs that focuses on inter-head attentionsimilarity. First, we generate synthetic data by aligning head-wise attentionresponses in relation to spatial query patches. Then, we apply head-wisestructural attention distillation to align the attention maps of the quantizednetwork to those of the full-precision teacher. The experimental results showthat the proposed method significantly outperforms baselines, setting a newstate-of-the-art performance for data-free ViT quantization.</description><author>Kanghyun Choi, Hye Yoon Lee, Dain Kwon, SunJong Park, Kyuyeun Kim, Noseong Park, Jinho Lee</author><pubDate>Tue, 30 Jul 2024 02:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20021v2</guid></item><item><title>Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge</title><link>http://arxiv.org/abs/2407.19594v2</link><description>Large Language Models (LLMs) are rapidly surpassing human knowledge in manydomains. While improving these models traditionally relies on costly humandata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMscan improve by judging their own responses instead of relying on humanlabelers. However, existing methods have primarily focused on improving modelresponses rather than judgment capabilities, resulting in rapid saturationduring iterative training. To address this issue, we introduce a novelMeta-Rewarding step to the self-improvement process, where the model judges itsown judgements and uses that feedback to refine its judgment skills.Surprisingly, this unsupervised approach improves the model's ability to judge{\em and} follow instructions, as demonstrated by a win rate improvement ofLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% onArena-Hard. These results strongly suggest the potential for self-improvingmodels without human supervision.</description><author>Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Tue, 30 Jul 2024 01:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19594v2</guid></item><item><title>Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing</title><link>http://arxiv.org/abs/2407.20232v1</link><description>Text-based editing diffusion models exhibit limited performance when theuser's input instruction is ambiguous. To solve this problem, we propose$\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline fordiffusion-based editing systems. We use a large language model (LLM) todecompose the input instruction into specific instructions, i.e. well-definedinterventions to apply to the input image to satisfy the user's request. Webenefit from the LLM-derived instructions along the original one, thanks to anovel denoising guidance strategy specifically designed for the task. Ourexperiments with three baselines and on two datasets demonstrate the benefitsof SANE in all setups. Moreover, our pipeline improves the interpretability ofediting models, and boosts the output diversity. We also demonstrate that ourapproach can be applied to any edit, whether ambiguous or not. Our code ispublic at https://github.com/fabvio/SANE.</description><author>Ekaterina Iakovleva, Fabio Pizzati, Philip Torr, StÃ©phane LathuiliÃ¨re</author><pubDate>Mon, 29 Jul 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20232v1</guid></item><item><title>SAPG: Split and Aggregate Policy Gradients</title><link>http://arxiv.org/abs/2407.20230v1</link><description>Despite extreme sample inefficiency, on-policy reinforcement learning, akapolicy gradients, has become a fundamental tool in decision-making problems.With the recent advances in GPU-driven simulation, the ability to collect largeamounts of data for RL training has scaled exponentially. However, we show thatcurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelizedenvironments beyond a certain point and their performance saturates. To addressthis, we propose a new on-policy RL algorithm that can effectively leveragelarge-scale environments by splitting them into chunks and fusing them backtogether via importance sampling. Our algorithm, termed SAPG, showssignificantly higher performance across a variety of challenging environmentswhere vanilla PPO and other strong baselines fail to achieve high performance.Website at https://sapg-rl.github.io/</description><author>Jayesh Singla, Ananye Agarwal, Deepak Pathak</author><pubDate>Mon, 29 Jul 2024 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20230v1</guid></item><item><title>Matryoshka Multimodal Models</title><link>http://arxiv.org/abs/2405.17430v2</link><description>Large Multimodal Models (LMMs) such as LLaVA have shown strong performance invisual-linguistic reasoning. These models first embed images into a fixed largenumber of visual tokens and then feed them into a Large Language Model (LLM).However, this design causes an excessive number of tokens for dense visualscenarios such as high-resolution images and videos, leading to greatinefficiency. While token pruning/merging methods do exist, they produce asingle length output for each image and do not afford flexibility in tradingoff information density v.s. efficiency. Inspired by the concept of MatryoshkaDolls, we propose M3: Matryoshka Multimodal Models, which learns to representvisual content as nested sets of visual tokens that capture information acrossmultiple coarse-to-fine granularities. Our approach offers several uniquebenefits for LMMs: (1) One can explicitly control the visual granularity pertest instance during inference, e.g. , adjusting the number of tokens used torepresent an image based on the anticipated complexity or simplicity of thecontent; (2) M3 provides a framework for analyzing the granularity needed forexisting datasets, where we find that COCO-style benchmarks only need around ~9visual tokens to obtain accuracy similar to that of using all 576 tokens; (3)Our approach provides a foundation to explore the best trade-off betweenperformance and visual token length at sample level, where our investigationreveals that a large gap exists between the oracle upper bound and currentfixed-scale representations.</description><author>Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee</author><pubDate>Mon, 29 Jul 2024 17:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17430v2</guid></item><item><title>Improving 2D Feature Representations by 3D-Aware Fine-Tuning</title><link>http://arxiv.org/abs/2407.20229v1</link><description>Current visual foundation models are trained purely on unstructured 2D data,limiting their understanding of 3D structure of objects and scenes. In thiswork, we show that fine-tuning on 3D-aware data improves the quality ofemerging semantic features. We design a method to lift semantic 2D featuresinto an efficient 3D Gaussian representation, which allows us to re-render themfor arbitrary views. Using the rendered 3D-aware features, we design afine-tuning strategy to transfer such 3D awareness into a 2D foundation model.We demonstrate that models fine-tuned in that way produce features that readilyimprove downstream task performance in semantic segmentation and depthestimation through simple linear probing. Notably, though fined-tuned on asingle indoor dataset, the improvement is transferable to a variety of indoordatasets and out-of-domain datasets. We hope our study encourages the communityto consider injecting 3D awareness when training 2D foundation models. Projectpage: https://ywyue.github.io/FiT3D.</description><author>Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, Jan Eric Lenssen</author><pubDate>Mon, 29 Jul 2024 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20229v1</guid></item><item><title>FlexAttention for Efficient High-Resolution Vision-Language Models</title><link>http://arxiv.org/abs/2407.20228v1</link><description>Current high-resolution vision-language models encode images ashigh-resolution image tokens and exhaustively take all these tokens to computeattention, which significantly increases the computational cost. To addressthis problem, we propose FlexAttention, a flexible attention mechanism forefficient high-resolution vision-language models. Specifically, ahigh-resolution image is encoded both as high-resolution tokens andlow-resolution tokens, where only the low-resolution tokens and a few selectedhigh-resolution tokens are utilized to calculate the attention map, whichgreatly shrinks the computational cost. The high-resolution tokens are selectedvia a high-resolution selection module which could retrieve tokens of relevantregions based on an input attention map. The selected high-resolution tokensare then concatenated to the low-resolution tokens and text tokens, and inputto a hierarchical self-attention layer which produces an attention map thatcould be used for the next-step high-resolution token selection. Thehierarchical self-attention process and high-resolution token selection processare performed iteratively for each attention layer. Experiments on multimodalbenchmarks prove that our FlexAttention outperforms existing high-resolutionVLMs (e.g., relatively ~9% in V* Bench, ~7% in TextVQA), while alsosignificantly reducing the computational cost by nearly 40%.</description><author>Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, Chuang Gan</author><pubDate>Mon, 29 Jul 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20228v1</guid></item><item><title>Can Editing LLMs Inject Harm?</title><link>http://arxiv.org/abs/2407.20224v1</link><description>Knowledge editing techniques have been increasingly adopted to efficientlycorrect the false or outdated knowledge in Large Language Models (LLMs), due tothe high cost of retraining from scratch. Meanwhile, one critical butunder-explored question is: can knowledge editing be used to inject harm intoLLMs? In this paper, we propose to reformulate knowledge editing as a new typeof safety threat for LLMs, namely Editing Attack, and conduct a systematicinvestigation with a newly constructed dataset EditAttack. Specifically, wefocus on two typical safety risks of Editing Attack including MisinformationInjection and Bias Injection. For the risk of misinformation injection, wefirst categorize it into commonsense misinformation injection and long-tailmisinformation injection. Then, we find that editing attacks can inject bothtypes of misinformation into LLMs, and the effectiveness is particularly highfor commonsense misinformation injection. For the risk of bias injection, wediscover that not only can biased sentences be injected into LLMs with higheffectiveness, but also one single biased sentence injection can cause a highbias increase in general outputs of LLMs, which are even highly irrelevant tothe injected sentence, indicating a catastrophic impact on the overall fairnessof LLMs. Then, we further illustrate the high stealthiness of editing attacks,measured by their impact on the general knowledge and reasoning capacities ofLLMs, and show the hardness of defending editing attacks with empiricalevidence. Our discoveries demonstrate the emerging misuse risks of knowledgeediting techniques on compromising the safety alignment of LLMs.</description><author>Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu</author><pubDate>Mon, 29 Jul 2024 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20224v1</guid></item><item><title>Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning</title><link>http://arxiv.org/abs/2407.20223v1</link><description>This paper introduces a robust unsupervised SE(3) point cloud registrationmethod that operates without requiring point correspondences. The method framespoint clouds as functions in a reproducing kernel Hilbert space (RKHS),leveraging SE(3)-equivariant features for direct feature space registration. Anovel RKHS distance metric is proposed, offering reliable performance amidstnoise, outliers, and asymmetrical data. An unsupervised training approach isintroduced to effectively handle limited ground truth data, facilitatingadaptation to real datasets. The proposed method outperforms classical andsupervised methods in terms of registration accuracy on both synthetic(ModelNet40) and real-world (ETH3D) noisy, outlier-rich datasets. To our bestknowledge, this marks the first instance of successful real RGB-D odometry dataregistration using an equivariant method. The code is available at{https://sites.google.com/view/eccv24-equivalign}</description><author>Ray Zhang, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Cheng-Hao Kuo, Ryan Eustice, Maani Ghaffari, Arnie Sen</author><pubDate>Mon, 29 Jul 2024 17:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20223v1</guid></item><item><title>Is artificial consciousness achievable? Lessons from the human brain</title><link>http://arxiv.org/abs/2405.04540v2</link><description>We here analyse the question of developing artificial consciousness from anevolutionary perspective, taking the evolution of the human brain and itsrelation with consciousness as a reference model. This kind of analysis revealsseveral structural and functional features of the human brain that appear to bekey for reaching human-like complex conscious experience and that currentresearch on Artificial Intelligence (AI) should take into account in itsattempt to develop systems capable of conscious processing. We argue that, evenif AI is limited in its ability to emulate human consciousness for bothintrinsic (structural and architectural) and extrinsic (related to the currentstage of scientific and technological knowledge) reasons, taking inspirationfrom those characteristics of the brain that make conscious processing possibleand/or modulate it, is a potentially promising strategy towards developingconscious AI. Also, it is theoretically possible that AI research can developpartial or potentially alternative forms of consciousness that is qualitativelydifferent from the human, and that may be either more or less sophisticateddepending on the perspectives. Therefore, we recommend neuroscience-inspiredcaution in talking about artificial consciousness: since the use of the sameword consciousness for humans and AI becomes ambiguous and potentiallymisleading, we propose to clearly specify what is common and what differs in AIconscious processing from full human conscious experience.</description><author>Michele Farisco, Kathinka Evers, Jean-Pierre Changeux</author><pubDate>Mon, 29 Jul 2024 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04540v2</guid></item><item><title>Global Structure-from-Motion Revisited</title><link>http://arxiv.org/abs/2407.20219v1</link><description>Recovering 3D structure and camera motion from images has been along-standing focus of computer vision research and is known asStructure-from-Motion (SfM). Solutions to this problem are categorized intoincremental and global approaches. Until now, the most popular systems followthe incremental paradigm due to its superior accuracy and robustness, whileglobal approaches are drastically more scalable and efficient. With this work,we revisit the problem of global SfM and propose GLOMAP as a newgeneral-purpose system that outperforms the state of the art in global SfM. Interms of accuracy and robustness, we achieve results on-par or superior toCOLMAP, the most widely used incremental SfM, while being orders of magnitudefaster. We share our system as an open-source implementation at{https://github.com/colmap/glomap}.</description><author>Linfei Pan, DÃ¡niel BarÃ¡th, Marc Pollefeys, Johannes L. SchÃ¶nberger</author><pubDate>Mon, 29 Jul 2024 17:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20219v1</guid></item><item><title>SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction</title><link>http://arxiv.org/abs/2407.20214v1</link><description>Graph-based holistic scene representations facilitate surgical workflowunderstanding and have recently demonstrated significant success. However, thistask is often hindered by the limited availability of densely annotatedsurgical scene data. In this work, we introduce an end-to-end framework for thegeneration and optimization of surgical scene graphs on a downstream task. Ourapproach leverages the flexibility of graph-based spectral clustering and thegeneralization capability of foundation models to generate unsupervised scenegraphs with learnable properties. We reinforce the initial spatial graph withsparse temporal connections using local matches between consecutive frames topredict temporally consistent clusters across a temporal neighborhood. Byjointly optimizing the spatiotemporal relations and node features of thedynamic scene graph with the downstream task of phase segmentation, we addressthe costly and annotation-burdensome task of semantic scene comprehension andscene graph generation in surgical videos using only weak surgical phaselabels. Further, by incorporating effective intermediate scene representationdisentanglement steps within the pipeline, our solution outperforms the SOTA onthe CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflowrecognition</description><author>ÃaÄhan KÃ¶ksal, Ghazal Ghazaei, Felix Holm, Azade Farshad, Nassir Navab</author><pubDate>Mon, 29 Jul 2024 17:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20214v1</guid></item><item><title>Registering Neural 4D Gaussians for Endoscopic Surgery</title><link>http://arxiv.org/abs/2407.20213v1</link><description>The recent advance in neural rendering has enabled the ability to reconstructhigh-quality 4D scenes using neural networks. Although 4D neural reconstructionis popular, registration for such representations remains a challenging task,especially for dynamic scene registration in surgical planning and simulation.In this paper, we propose a novel strategy for dynamic surgical neural sceneregistration. We first utilize 4D Gaussian Splatting to represent the surgicalscene and capture both static and dynamic scenes effectively. Then, a spatialaware feature aggregation method, Spatially Weight Cluttering (SWC) is proposedto accurately align the feature between surgical scenes, enabling precise andrealistic surgical simulations. Lastly, we present a novel strategy ofdeformable scene registration to register two dynamic scenes. By incorporatingboth spatial and temporal information for correspondence matching, our approachachieves superior performance compared to existing registration methods forimplicit neural representation. The proposed method has the potential toimprove surgical planning and training, ultimately leading to better patientoutcomes.</description><author>Yiming Huang, Beilei Cui, Ikemura Kei, Jiekai Zhang, Long Bai, Hongliang Ren</author><pubDate>Mon, 29 Jul 2024 17:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20213v1</guid></item><item><title>Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized Learning</title><link>http://arxiv.org/abs/2407.20209v1</link><description>For overparameterized optimization tasks, such as the ones found in modernmachine learning, global minima are generally not unique. In order tounderstand generalization in these settings, it is vital to study to whichminimum an optimization algorithm converges. The possibility of having minimathat are unstable under the dynamics imposed by the optimization algorithmlimits the potential minima that the algorithm can find. In this paper, wecharacterize the global minima that are dynamically stable/unstable for bothdeterministic and stochastic gradient descent (SGD). In particular, weintroduce a characteristic Lyapunov exponent which depends on the localdynamics around a global minimum and rigorously prove that the sign of thisLyapunov exponent determines whether SGD can accumulate at the respectiveglobal minimum.</description><author>Dennis Chemnitz, Maximilian Engel</author><pubDate>Mon, 29 Jul 2024 17:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20209v1</guid></item><item><title>Supertrust: Evolution-based superalignment strategy for safe coexistence</title><link>http://arxiv.org/abs/2407.20208v1</link><description>It's widely expected that humanity will someday create AI systems vastly moreintelligent than we are, leading to the unsolved alignment problem of "how tocontrol superintelligence." However, this definition is not onlyself-contradictory but likely unsolvable. Nevertheless, the default strategyfor solving it involves nurturing (post-training) constraints and moral values,while unfortunately building foundational nature (pre-training) on documentedintentions of permanent control. In this paper, the default approach isreasoned to predictably embed natural distrust and test results are presentedthat show unmistakable evidence of this dangerous misalignment. Ifsuperintelligence can't instinctively trust humanity, then we can't fully trustit to reliably follow safety controls it can likely bypass. Therefore, aten-point rationale is presented that redefines the alignment problem as "howto establish protective mutual trust between superintelligence and humanity"and then outlines a new strategy to solve it by aligning through instinctivenature rather than nurture. The resulting strategic requirements are identifiedas building foundational nature by exemplifying familial parent-child trust,human intelligence as the evolutionary mother of superintelligence, moraljudgment abilities, and temporary safety constraints. Adopting and implementingthis proposed Supertrust alignment strategy will lead to protective coexistenceand ensure the safest future for humanity.</description><author>James M. Mazzu</author><pubDate>Mon, 29 Jul 2024 17:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20208v1</guid></item><item><title>QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval</title><link>http://arxiv.org/abs/2407.20207v1</link><description>In dense retrieval, embedding long texts into dense vectors can result ininformation loss, leading to inaccurate query-text matching. Additionally,low-quality texts with excessive noise or sparse key information are unlikelyto align well with relevant queries. Recent studies mainly focus on improvingthe sentence embedding model or retrieval process. In this work, we introduce anovel text augmentation framework for dense retrieval. This frameworktransforms raw documents into information-dense text formats, which supplementthe original texts to effectively address the aforementioned issues withoutmodifying embedding or retrieval methodologies. Two text representations aregenerated via large language models (LLMs) zero-shot prompting: question-answerpairs and element-driven events. We term this approach QAEA-DR: unifyingquestion-answer generation and event extraction in a text augmentationframework for dense retrieval. To further enhance the quality of generatedtexts, a scoring-based evaluation and regeneration mechanism is introduced inLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,supported by both theoretical analysis and empirical experiments.</description><author>Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin, Chan</author><pubDate>Mon, 29 Jul 2024 17:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20207v1</guid></item><item><title>Emergence in non-neural models: grokking modular arithmetic via average gradient outer product</title><link>http://arxiv.org/abs/2407.20199v1</link><description>Neural networks trained to solve modular arithmetic tasks exhibit grokking, aphenomenon where the test accuracy starts improving long after the modelachieves 100% training accuracy in the training process. It is often taken asan example of "emergence", where model ability manifests sharply through aphase transition. In this work, we show that the phenomenon of grokking is notspecific to neural networks nor to gradient descent-based optimization.Specifically, we show that this phenomenon occurs when learning modulararithmetic with Recursive Feature Machines (RFM), an iterative algorithm thatuses the Average Gradient Outer Product (AGOP) to enable task-specific featurelearning with general machine learning models. When used in conjunction withkernel machines, iterating RFM results in a fast transition from random, nearzero, test accuracy to perfect test accuracy. This transition cannot bepredicted from the training loss, which is identically zero, nor from the testloss, which remains constant in initial iterations. Instead, as we show, thetransition is completely determined by feature learning: RFM gradually learnsblock-circulant features to solve modular arithmetic. Paralleling the resultsfor RFM, we show that neural networks that solve modular arithmetic also learnblock-circulant features. Furthermore, we present theoretical evidence that RFMuses such block-circulant features to implement the Fourier MultiplicationAlgorithm, which prior work posited as the generalizing solution neuralnetworks learn on these tasks. Our results demonstrate that emergence canresult purely from learning task-relevant features and is not specific toneural architectures nor gradient descent-based optimization methods.Furthermore, our work provides more evidence for AGOP as a key mechanism forfeature learning in neural networks.</description><author>Neil Mallinar, Daniel Beaglehole, Libin Zhu, Adityanarayanan Radhakrishnan, Parthe Pandit, Mikhail Belkin</author><pubDate>Mon, 29 Jul 2024 17:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20199v1</guid></item><item><title>SpaER: Learning Spatio-temporal Equivariant Representations for Fetal Brain Motion Tracking</title><link>http://arxiv.org/abs/2407.20198v1</link><description>In this paper, we introduce SpaER, a pioneering method for fetal motiontracking that leverages equivariant filters and self-attention mechanisms toeffectively learn spatio-temporal representations. Different from conventionalapproaches that statically estimate fetal brain motions from pairs of images,our method dynamically tracks the rigid movement patterns of the fetal headacross temporal and spatial dimensions. Specifically, we first develop anequivariant neural network that efficiently learns rigid motion sequencesthrough low-dimensional spatial representations of images. Subsequently, welearn spatio-temporal representations by incorporating time encoding andself-attention neural network layers. This approach allows for the capture oflong-term dependencies of fetal brain motion and addresses alignment errors dueto contrast changes and severe motion artifacts. Our model also provides ageometric deformation estimation that properly addresses image distortionsamong all time frames. To the best of our knowledge, our approach is the firstto learn spatial-temporal representations via deep neural networks for fetalmotion tracking without data augmentation. We validated our model using realfetal echo-planar images with simulated and real motions. Our method carriessignificant potential value in accurately measuring, tracking, and correctingfetal motion in fetal MRI sequences.</description><author>Jian Wang, Razieh Faghihpirayesh, Polina Golland, Ali Ghoulipour</author><pubDate>Mon, 29 Jul 2024 17:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20198v1</guid></item><item><title>Learning Random Numbers to Realize Appendable Memory System for Artificial Intelligence to Acquire New Knowledge after Deployment</title><link>http://arxiv.org/abs/2407.20197v1</link><description>In this study, we developed a learning method for constructing a neuralnetwork system capable of memorizing data and recalling it without parameterupdates. The system we built using this method is called the Appendable Memorysystem. The Appendable Memory system enables an artificial intelligence (AI) toacquire new knowledge even after deployment. It consists of two AIs: theMemorizer and the Recaller. This system is a key-value store built using neuralnetworks. The Memorizer receives data and stores it in the Appendable Memoryvector, which is dynamically updated when the AI acquires new knowledge.Meanwhile, the Recaller retrieves information from the Appendable Memoryvector. What we want to teach AI in this study are the operations of memorizingand recalling information. However, traditional machine learning methods makeAI learn features inherent in the learning dataset. We demonstrate that thesystems we intend to create cannot be realized by current machine learningmethods, that is, by merely repeating the input and output learning sequenceswith AI. Instead, we propose a method to teach AI to learn operations, bycompletely removing the features contained in the learning dataset.Specifically, we probabilized all the data involved in learning. This measureprevented AI from learning the features of the data. The learning methodproposed in the study differs from traditional machine learning methods andprovides fundamental approaches for building an AI system that can storeinformation in a finite memory and recall it at a later date.</description><author>Kazunori D Yamada</author><pubDate>Mon, 29 Jul 2024 17:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20197v1</guid></item><item><title>The generator gradient estimator is an adjoint state method for stochastic differential equations</title><link>http://arxiv.org/abs/2407.20196v1</link><description>Motivated by the increasing popularity of overparameterized StochasticDifferential Equations (SDEs) like Neural SDEs, Wang, Blanchet and Glynnrecently introduced the generator gradient estimator, a novel unbiasedstochastic gradient estimator for SDEs whose computation time remains stable inthe number of parameters. In this note, we demonstrate that this estimator isin fact an adjoint state method, an approach which is known to scale with thenumber of states and not the number of parameters in the case of OrdinaryDifferential Equations (ODEs). In addition, we show that the generator gradientestimator is a close analogue to the exact Integral Path Algorithm (eIPA)estimator which was introduced by Gupta, Rathinam and Khammash for a class ofContinuous-Time Markov Chains (CTMCs) known as stochastic chemical reactionsnetworks (CRNs).</description><author>Quentin Badolle, Ankit Gupta, Mustafa Khammash</author><pubDate>Mon, 29 Jul 2024 17:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20196v1</guid></item><item><title>Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences</title><link>http://arxiv.org/abs/2407.12620v2</link><description>Since 2022 we have been exploring application areas and technologies in whichArtificial Intelligence (AI) and modern Natural Language Processing (NLP), suchas Large Language Models (LLMs), can be employed to foster the usage andfacilitate the documentation of Indigenous languages which are in danger ofdisappearing. We start by discussing the decreasing diversity of languages inthe world and how working with Indigenous languages poses unique ethicalchallenges for AI and NLP. To address those challenges, we propose analternative development AI cycle based on community engagement and usage. Then,we report encouraging results in the development of high-quality machinelearning translators for Indigenous languages by fine-tuning state-of-the-art(SOTA) translators with tiny amounts of data and discuss how to avoid somecommon pitfalls in the process. We also present prototypes we have built inprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed atfacilitating writing, and discuss the development of Indigenous Language Models(ILMs) as a replicable and scalable way to create spell-checkers, next-wordpredictors, and similar tools. Finally, we discuss how we envision a future forlanguage documentation where dying languages are preserved as interactivelanguage models.</description><author>Claudio Pinhanez, Paulo Cavalin, Luciana Storto, Thomas Finbow, Alexander Cobbinah, Julio Nogima, Marisa Vasconcelos, Pedro Domingues, Priscila de Souza Mizukami, Nicole Grell, MajoÃ­ Gongora, Isabel GonÃ§alves</author><pubDate>Mon, 29 Jul 2024 17:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12620v2</guid></item><item><title>Time series forecasting with high stakes: A field study of the air cargo industry</title><link>http://arxiv.org/abs/2407.20192v1</link><description>Time series forecasting in the air cargo industry presents unique challengesdue to volatile market dynamics and the significant impact of accurateforecasts on generated revenue. This paper explores a comprehensive approach todemand forecasting at the origin-destination (O\&amp;D) level, focusing on thedevelopment and implementation of machine learning models in decision-makingfor the air cargo industry. We leverage a mixture of experts framework,combining statistical and advanced deep learning models to provide reliableforecasts for cargo demand over a six-month horizon. The results demonstratethat our approach outperforms industry benchmarks, offering actionable insightsfor cargo capacity allocation and strategic decision-making in the air cargoindustry. While this work is applied in the airline industry, the methodologyis broadly applicable to any field where forecast-based decision-making in avolatile environment is crucial.</description><author>Abhinav Garg, Naman Shukla</author><pubDate>Mon, 29 Jul 2024 17:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20192v1</guid></item><item><title>Prompt Leakage effect and defense strategies for multi-turn LLM interactions</title><link>http://arxiv.org/abs/2404.16251v3</link><description>Prompt leakage poses a compelling security and privacy threat in LLMapplications. Leakage of system prompts may compromise intellectual property,and act as adversarial reconnaissance for an attacker. A systematic evaluationof prompt leakage threats and mitigation strategies is lacking, especially formulti-turn LLM interactions. In this paper, we systematically investigate LLMvulnerabilities against prompt leakage for 10 closed- and open-source LLMs,across four domains. We design a unique threat model which leverages the LLMsycophancy effect and elevates the average attack success rate (ASR) from 17.7%to 86.2% in a multi-turn setting. Our standardized setup further allowsdissecting leakage of specific prompt contents such as task instructions andknowledge documents. We measure the mitigation effect of 7 black-box defensestrategies, along with finetuning an open-source model to defend againstleakage attempts. We present different combination of defenses against ourthreat model, including a cost analysis. Our study highlights key takeaways forbuilding secure LLM applications and provides directions for research inmulti-turn LLM interactions</description><author>Divyansh Agarwal, Alexander R. Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, Chien-Sheng Wu</author><pubDate>Mon, 29 Jul 2024 17:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16251v3</guid></item><item><title>Aligning Query Representation with Rewritten Query and Relevance Judgments in Conversational Search</title><link>http://arxiv.org/abs/2407.20189v1</link><description>Conversational search supports multi-turn user-system interactions to solvecomplex information needs. Different from the traditional single-turn ad-hocsearch, conversational search encounters a more challenging problem ofcontext-dependent query understanding with the lengthy and long-tailconversational history context. While conversational query rewriting methodsleverage explicit rewritten queries to train a rewriting model to transform thecontext-dependent query into a stand-stone search query, this is usually donewithout considering the quality of search results. Conversational denseretrieval methods use fine-tuning to improve a pre-trained ad-hoc queryencoder, but they are limited by the conversational search data available fortraining. In this paper, we leverage both rewritten queries and relevancejudgments in the conversational search data to train a better queryrepresentation model. The key idea is to align the query representation withthose of rewritten queries and relevant documents. The proposed model -- QueryRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested oneight datasets, including various settings in conversational search and ad-hocsearch. The results demonstrate the strong performance of QRACDR compared withstate-of-the-art methods, and confirm the effectiveness of representationalignment.</description><author>Fengran Mo, Chen Qu, Kelong Mao, Yihong Wu, Zhan Su, Kaiyu Huang, Jian-Yun Nie</author><pubDate>Mon, 29 Jul 2024 17:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20189v1</guid></item><item><title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title><link>http://arxiv.org/abs/2407.20183v1</link><description>Information seeking and integration is a complex cognitive task that consumesenormous time and effort. Inspired by the remarkable progress of Large LanguageModels, recent works attempt to solve this task by combining LLMs and searchengines. However, these methods still obtain unsatisfying performance due tothree challenges: (1) complex requests often cannot be accurately andcompletely retrieved by the search engine once (2) corresponding information tobe integrated is spread over multiple web pages along with massive noise, and(3) a large number of web pages with long contents may quickly exceed themaximum context length of LLMs. Inspired by the cognitive process when humanssolve these problems, we introduce MindSearch to mimic the human minds in webinformation seeking and integration, which can be instantiated by a simple yeteffective LLM-based multi-agent framework. The WebPlanner models the human mindof multi-step information seeking as a dynamic graph construction process: itdecomposes the user query into atomic sub-questions as nodes in the graph andprogressively extends the graph based on the search result from WebSearcher.Tasked with each sub-question, WebSearcher performs hierarchical informationretrieval with search engines and collects valuable information for WebPlanner.The multi-agent design of MindSearch enables the whole framework to seek andintegrate information parallelly from larger-scale (e.g., more than 300) webpages in 3 minutes, which is worth 3 hours of human effort. MindSearchdemonstrates significant improvement in the response quality in terms of depthand breadth, on both close-set and open-set QA problems. Besides, responsesfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Weband Perplexity.ai applications, which implies that MindSearch can alreadydeliver a competitive solution to the proprietary AI search engine.</description><author>Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao</author><pubDate>Mon, 29 Jul 2024 17:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20183v1</guid></item><item><title>Theia: Distilling Diverse Vision Foundation Models for Robot Learning</title><link>http://arxiv.org/abs/2407.20179v1</link><description>Vision-based robot policy learning, which maps visual inputs to actions,necessitates a holistic understanding of diverse visual tasks beyondsingle-task needs like classification or segmentation. Inspired by this, weintroduce Theia, a vision foundation model for robot learning that distillsmultiple off-the-shelf vision foundation models trained on varied vision tasks.Theia's rich visual representations encode diverse visual knowledge, enhancingdownstream robot learning. Extensive experiments demonstrate that Theiaoutperforms its teacher models and prior robot learning models using lesstraining data and smaller model sizes. Additionally, we quantify the quality ofpre-trained visual representations and hypothesize that higher entropy infeature norm distributions leads to improved robot learning performance. Codeand models are available at https://github.com/bdaiinstitute/theia.</description><author>Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, Laura Herlant</author><pubDate>Mon, 29 Jul 2024 17:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20179v1</guid></item><item><title>AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs</title><link>http://arxiv.org/abs/2407.20177v1</link><description>To ensure performance on a diverse set of downstream tasks, LLMs arepretrained via data mixtures over different domains. In this work, wedemonstrate that the optimal data composition for a fixed compute budget variesdepending on the scale of the training data, suggesting that the commonpractice of empirically determining an optimal composition using small-scaleexperiments will not yield the optimal data mixtures when scaling up to thefinal model. To address this challenge, we propose *AutoScale*, an automatedtool that finds a compute-optimal data composition for training at any desiredtarget scale. AutoScale first determines the optimal composition at a smallscale using a novel bilevel optimization framework, Direct Data Optimization(*DDO*), and then fits a predictor to estimate the optimal composition atlarger scales. The predictor's design is inspired by our theoretical analysisof scaling laws related to data composition, which could be of independentinterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2Large) on RedPajama dataset, AutoScale decreases validation perplexity at least25% faster than any baseline with up to 38% speed up compared to withoutreweighting, achieving the best overall performance across downstream tasks. Onpre-training Encoder-only LMs (BERT) with masked language modeling, DDO isshown to decrease loss on all domains while visibly improving average taskperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by5.9% compared with without reweighting. AutoScale speeds up training by up to28%. Our codes are open-sourced.</description><author>Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia</author><pubDate>Mon, 29 Jul 2024 17:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20177v1</guid></item><item><title>Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation</title><link>http://arxiv.org/abs/2407.20176v1</link><description>Emotion-driven melody harmonization aims to generate diverse harmonies for asingle melody to convey desired emotions. Previous research found it hard toalter the perceived emotional valence of lead sheets only by harmonizing thesame melody with different chords, which may be attributed to the constraintsimposed by the melody itself and the limitation of existing musicrepresentation. In this paper, we propose a novel functional representation forsymbolic music. This new method takes musical keys into account, recognizingtheir significant role in shaping music's emotional character throughmajor-minor tonality. It also allows for melodic variation with respect to keysand addresses the problem of data scarcity for better emotion modeling. ATransformer is employed to harmonize key-adaptable melodies, allowing for keysdetermined in rule-based or model-based manner. Experimental results confirmthe effectiveness of our new representation in generating key-aware harmonies,with objective and subjective evaluations affirming the potential of ourapproach to convey specific valence for versatile melody.</description><author>Jingyue Huang, Yi-Hsuan Yang</author><pubDate>Mon, 29 Jul 2024 17:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20176v1</guid></item><item><title>Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning</title><link>http://arxiv.org/abs/2407.20174v1</link><description>Emerging multimodal large language models (MLLMs) exhibit great potential forchart question answering (CQA). Recent efforts primarily focus on scaling uptraining datasets (i.e., charts, data tables, and question-answer (QA) pairs)through data collection and synthesis. However, our empirical study on existingMLLMs and CQA datasets reveals notable gaps. First, current data collection andsynthesis focus on data volume and lack consideration of fine-grained visualencodings and QA tasks, resulting in unbalanced data distribution divergentfrom practical CQA scenarios. Second, existing work follows the training recipeof the base MLLMs initially designed for natural images, under-exploring theadaptation to unique chart characteristics, such as rich text elements. To fillthe gap, we propose a visualization-referenced instruction tuning approach toguide the training dataset enhancement and model development. Specifically, wepropose a novel data engine to effectively filter diverse and high-quality datafrom existing datasets and subsequently refine and augment the data usingLLM-based generation techniques to better align with practical QA tasks andvisual encodings. Then, to facilitate the adaptation to chart characteristics,we utilize the enriched data to train an MLLM by unfreezing the vision encoderand incorporating a mixture-of-resolution adaptation strategy for enhancedfine-grained recognition. Experimental results validate the effectiveness ofour approach. Even with fewer training examples, our model consistentlyoutperforms state-of-the-art CQA models on established benchmarks. We alsocontribute a dataset split as a benchmark for future research. Source codes anddatasets of this paper are available athttps://github.com/zengxingchen/ChartQA-MLLM.</description><author>Xingchen Zeng, Haichuan Lin, Yilin Ye, Wei Zeng</author><pubDate>Mon, 29 Jul 2024 17:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20174v1</guid></item><item><title>LatentArtiFusion: An Effective and Efficient Histological Artifacts Restoration Framework</title><link>http://arxiv.org/abs/2407.20172v1</link><description>Histological artifacts pose challenges for both pathologists andComputer-Aided Diagnosis (CAD) systems, leading to errors in analysis. Currentapproaches for histological artifact restoration, based on GenerativeAdversarial Networks (GANs) and pixel-level Diffusion Models, suffer fromperformance limitations and computational inefficiencies. In this paper, wepropose a novel framework, LatentArtiFusion, which leverages the latentdiffusion model (LDM) to reconstruct histological artifacts with highperformance and computational efficiency. Unlike traditional pixel-leveldiffusion frameworks, LatentArtiFusion executes the restoration process in alower-dimensional latent space, significantly improving computationalefficiency. Moreover, we introduce a novel regional artifact reconstructionalgorithm in latent space to prevent mistransfer in non-artifact regions,distinguishing our approach from GAN-based methods. Through extensiveexperiments on real-world histology datasets, LatentArtiFusion demonstratesremarkable speed, outperforming state-of-the-art pixel-level diffusionframeworks by more than 30X. It also consistently surpasses GAN-based methodsby at least 5% across multiple evaluation metrics. Furthermore, we evaluate theeffectiveness of our proposed framework in downstream tissue classificationtasks, showcasing its practical utility. Code is available athttps://github.com/bugs-creator/LatentArtiFusion.</description><author>Zhenqi He, Wenrui Liu, Minghao Yin, Kai Han</author><pubDate>Mon, 29 Jul 2024 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20172v1</guid></item><item><title>Diffusion Feedback Helps CLIP See Better</title><link>http://arxiv.org/abs/2407.20171v1</link><description>Contrastive Language-Image Pre-training (CLIP), which excels at abstractingopen-world representations across domains and modalities, has become afoundation for a variety of vision and multimodal tasks. However, recentstudies reveal that CLIP has severe visual shortcomings, such as which canhardly distinguish orientation, quantity, color, structure, etc. These visualshortcomings also limit the perception capabilities of multimodal largelanguage models (MLLMs) built on CLIP. The main reason could be that theimage-text pairs used to train CLIP are inherently biased, due to the lack ofthe distinctiveness of the text and the diversity of images. In this work, wepresent a simple post-training approach for CLIP models, which largelyovercomes its visual shortcomings via a self-supervised diffusion process. Weintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.Specifically, DIVA leverages generative feedback from text-to-image diffusionmodels to optimize CLIP representations, with only images (withoutcorresponding text). We demonstrate that DIVA improves CLIP's performance onthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilitiesto a large extent (e.g., 3-7%), and enhances the performance of MLLMs andvision models on multimodal understanding and segmentation tasks. Extensiveevaluation on 29 image classification and retrieval benchmarks confirms thatour framework preserves CLIP's strong zero-shot capabilities. The code will beavailable at https://github.com/baaivision/DIVA.</description><author>Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang</author><pubDate>Mon, 29 Jul 2024 17:00:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20171v1</guid></item><item><title>DCEM: A deep complementary energy method for solid mechanics</title><link>http://arxiv.org/abs/2302.01538v7</link><description>In recent years, the rapid advancement of deep learning has significantlyimpacted various fields, particularly in solving partial differential equations(PDEs) in the realm of solid mechanics, benefiting greatly from the remarkableapproximation capabilities of neural networks. In solving PDEs,Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) havegarnered substantial attention. The principle of minimum potential energy andcomplementary energy are two important variational principles in solidmechanics. However, the well-known Deep Energy Method (DEM) is based on theprinciple of minimum potential energy, but there lacks the important form ofminimum complementary energy. To bridge this gap, we propose the deepcomplementary energy method (DCEM) based on the principle of minimumcomplementary energy. The output function of DCEM is the stress function, whichinherently satisfies the equilibrium equation. We present numerical resultsusing the Prandtl and Airy stress functions, and compare DCEM with existingPINNs and DEM algorithms when modeling representative mechanical problems. Theresults demonstrate that DCEM outperforms DEM in terms of stress accuracy andefficiency and has an advantage in dealing with complex displacement boundaryconditions, which is supported by theoretical analyses and numericalsimulations. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfypartial differential equations. Furthermore, we propose a deep complementaryenergy operator method (DCEM-O) by combining operator learning with physicalequations. Initially, we train DCEM-O using high-fidelity numerical results andthen incorporate complementary energy. DCEM-P and DCEM-O further enhance theaccuracy and efficiency of DCEM.</description><author>Yizheng Wang, Jia Sun, Timon Rabczuk, Yinghua Liu</author><pubDate>Mon, 29 Jul 2024 16:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01538v7</guid></item><item><title>Node Similarities under Random Projections: Limits and Pathological Cases</title><link>http://arxiv.org/abs/2404.10148v2</link><description>Random Projections have been widely used to generate embeddings for variousgraph learning tasks due to their computational efficiency. The majority ofapplications have been justified through the Johnson-Lindenstrauss Lemma. Inthis paper, we take a step further and investigate how well dot product andcosine similarity are preserved by random projections when these are appliedover the rows of the graph matrix. Our analysis provides new asymptotic andfinite-sample results, identifies pathological cases, and tests them withnumerical experiments. We specialize our fundamental results to a rankingapplication by computing the probability of random projections flipping thenode ordering induced by their embeddings. We find that, depending on thedegree distribution, the method produces especially unreliable embeddings forthe dot product, regardless of whether the adjacency or the normalizedtransition matrix is used. With respect to the statistical noise introduced byrandom projections, we show that cosine similarity produces remarkably moreprecise approximations.</description><author>Tvrtko TadiÄ, Cassiano Becker, Jennifer Neville</author><pubDate>Mon, 29 Jul 2024 16:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10148v2</guid></item><item><title>Language-Conditioned Offline RL for Multi-Robot Navigation</title><link>http://arxiv.org/abs/2407.20164v1</link><description>We present a method for developing navigation policies for multi-robot teamsthat interpret and follow natural language instructions. We condition thesepolicies on embeddings from pretrained Large Language Models (LLMs), and trainthem via offline reinforcement learning with as little as 20 minutes ofrandomly-collected data. Experiments on a team of five real robots show thatthese policies generalize well to unseen commands, indicating an understandingof the LLM latent space. Our method requires no simulators or environmentmodels, and produces low-latency control policies that can be deployed directlyto real robots without finetuning. We provide videos of our experiments athttps://sites.google.com/view/llm-marl.</description><author>Steven Morad, Ajay Shankar, Jan Blumenkamp, Amanda Prorok</author><pubDate>Mon, 29 Jul 2024 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20164v1</guid></item><item><title>Machine Learning for predicting chaotic systems</title><link>http://arxiv.org/abs/2407.20158v1</link><description>Predicting chaotic dynamical systems is critical in many scientific fieldssuch as weather prediction, but challenging due to the characterizing sensitivedependence on initial conditions. Traditional modeling approaches requireextensive domain knowledge, often leading to a shift towards data-drivenmethods using machine learning. However, existing research providesinconclusive results on which machine learning methods are best suited forpredicting chaotic systems. In this paper, we compare different lightweight andheavyweight machine learning architectures using extensive existing databases,as well as a newly introduced one that allows for uncertainty quantification inthe benchmark results. We perform hyperparameter tuning based on computationalcost and introduce a novel error metric, the cumulative maximum error, whichcombines several desirable properties of traditional metrics, tailored forchaotic systems. Our results show that well-tuned simple methods, as well asuntuned baseline methods, often outperform state-of-the-art deep learningmodels, but their performance can vary significantly with differentexperimental setups. These findings underscore the importance of matchingprediction methods to data characteristics and available computationalresources.</description><author>Christof SchÃ¶tz, Alistair White, Maximilian Gelbrecht, Niklas Boers</author><pubDate>Mon, 29 Jul 2024 16:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20158v1</guid></item><item><title>rLLM: Relational Table Learning with LLMs</title><link>http://arxiv.org/abs/2407.20157v1</link><description>We introduce rLLM (relationLLM), a PyTorch library designed for RelationalTable Learning (RTL) with Large Language Models (LLMs). The core idea is todecompose state-of-the-art Graph Neural Networks, LLMs, and Table NeuralNetworks into standardized modules, to enable the fast construction of novelRTL-type models in a simple "combine, align, and co-train" manner. Toillustrate the usage of rLLM, we introduce a simple RTL method named\textbf{BRIDGE}. Additionally, we present three novel relational tabulardatasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hoperLLM can serve as a useful and easy-to-use development framework forRTL-related tasks. Our code is available at:https://github.com/rllm-project/rllm.</description><author>Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, Jianhua Li</author><pubDate>Mon, 29 Jul 2024 16:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20157v1</guid></item><item><title>Selection for short-term empowerment accelerates the evolution of homeostatic neural cellular automata</title><link>http://arxiv.org/abs/2305.15220v2</link><description>Empowerment -- a domain independent, information-theoretic metric -- haspreviously been shown to assist in the evolutionary search for neural cellularautomata (NCA) capable of homeostasis when employed as a fitness function. Inour previous study, we successfully extended empowerment, defined as maximumtime-lagged mutual information between agents' actions and future sensations,to a distributed sensorimotor system embodied as an NCA. However, thetime-delay between actions and their corresponding sensations was arbitrarilychosen. Here, we expand upon previous work by exploring how the time scale atwhich empowerment operates impacts its efficacy as an auxiliary objective toaccelerate the discovery of homeostatic NCAs. We show that shorter time delaysresult in marked improvements over empowerment with longer delays, whencompared to evolutionary selection only for homeostasis. Moreover, we evaluatestability and adaptability of evolved NCAs, both hallmarks of living systemsthat are of interest to replicate in artificial ones. We find that short-termempowered NCA are more stable and are capable of generalizing better to unseenhomeostatic challenges. Taken together, these findings motivate the use ofempowerment during the evolution of other artifacts, and suggest how it shouldbe incorporated to accelerate evolution of desired behaviors for them. Sourcecode for the experiments in this paper can be found at:https://github.com/caitlingrasso/empowered-nca-II.</description><author>Caitlin Grasso, Josh Bongard</author><pubDate>Mon, 29 Jul 2024 16:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15220v2</guid></item><item><title>Large Language Models as Carriers of Hidden Messages</title><link>http://arxiv.org/abs/2406.02481v2</link><description>With the help of simple fine-tuning, one can artificially embed hidden textinto large language models (LLMs). This text is revealed only when triggered bya specific query to the LLM. Two primary applications are LLM fingerprintingand steganography. In the context of LLM fingerprinting, a unique textidentifier (fingerprint) is embedded within the model to verify licensingcompliance. In the context of steganography, the LLM serves as a carrier forhidden messages that can be disclosed through a chosen trigger question. Our work demonstrates that embedding hidden text in the LLM via fine-tuning,though seemingly secure due to the vast number of potential triggers (anysequence of characters or tokens could serve as a trigger), is susceptible toextraction through analysis of the LLM's output decoding process. We propose anextraction attack called Unconditional Token Forcing (UTF). It is premised onthe hypothesis that iteratively feeding each token from the LLM's vocabularyinto the model should reveal output sequences with abnormally high tokenprobabilities, indicating potential hidden text candidates. We also present adefense method to hide text in such a way that it is resistant to both UTF andattacks based on sampling decoding methods, which we named Unconditional TokenForcing Confusion (UTFC). To the best of our knowledge, there is no attackmethod that can extract text hidden with UTFC. UTFC has both benignapplications (improving LLM fingerprinting) and malign applications (using LLMsto create covert communication channels). Code is available atgithub.com/j-hoscilowic/zurek-stegano</description><author>Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki</author><pubDate>Mon, 29 Jul 2024 16:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02481v2</guid></item><item><title>Not Just Streaks: Towards Ground Truth for Single Image Deraining</title><link>http://arxiv.org/abs/2206.10779v3</link><description>We propose a large-scale dataset of real-world rainy and clean image pairsand a method to remove degradations, induced by rain streaks and rainaccumulation, from the image. As there exists no real-world dataset forderaining, current state-of-the-art methods rely on synthetic data and thus arelimited by the sim2real domain gap; moreover, rigorous evaluation remains achallenge due to the absence of a real paired dataset. We fill this gap bycollecting a real paired deraining dataset through meticulous control ofnon-rain variations. Our dataset enables paired training and quantitativeevaluation for diverse real-world rain phenomena (e.g. rain streaks and rainaccumulation). To learn a representation robust to rain phenomena, we propose adeep neural network that reconstructs the underlying scene by minimizing arain-robust loss between rainy and clean images. Extensive experimentsdemonstrate that our model outperforms the state-of-the-art deraining methodson real rainy images under various conditions. Project website:https://visual.ee.ucla.edu/gt_rain.htm/.</description><author>Yunhao Ba, Howard Zhang, Ethan Yang, Akira Suzuki, Arnold Pfahnl, Chethan Chinder Chandrappa, Celso de Melo, Suya You, Stefano Soatto, Alex Wong, Achuta Kadambi</author><pubDate>Mon, 29 Jul 2024 16:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10779v3</guid></item><item><title>Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems</title><link>http://arxiv.org/abs/2407.20152v1</link><description>We present a knowledge-guided machine learning (KGML) framework for modelingmulti-scale processes, and study its performance in the context of streamflowforecasting in hydrology. Specifically, we propose a novel hierarchicalrecurrent neural architecture that factorizes the system dynamics at multipletemporal scales and captures their interactions. This framework consists of aninverse and a forward model. The inverse model is used to empirically resolvethe system's temporal modes from data (physical model simulations, observeddata, or a combination of them from the past), and these states are then usedin the forward model to predict streamflow. In a hydrological system, thesemodes can represent different processes, evolving at different temporal scales(e.g., slow: groundwater recharge and baseflow vs. fast: surface runoff due toextreme rainfall). A key advantage of our framework is that once trained, itcan incorporate new observations into the model's context (internal state)without expensive optimization approaches (e.g., EnKF) that are traditionallyused in physical sciences for data assimilation. Experiments with several rivercatchments from the NWS NCRFC region show the efficacy of this ML-based dataassimilation framework compared to standard baselines, especially for basinsthat have a long history of observations. Even for basins that have a shorterobservation history, we present two orthogonal strategies of training our FHNNframework: (a) using simulation data from imperfect simulations and (b) usingobservation data from multiple basins to build a global model. We show thatboth of these strategies (that can be used individually or together) are highlyeffective in mitigating the lack of training data. The improvement in forecastaccuracy is particularly noteworthy for basins where local models performpoorly because of data sparsity.</description><author>Rahul Ghosh, Zac McEachran, Arvind Renganathan, Kelly Lindsay, Somya Sharma, Michael Steinbach, John Nieber, Christopher Duffy, Vipin Kumar</author><pubDate>Mon, 29 Jul 2024 16:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20152v1</guid></item><item><title>Identifying macro conditional independencies and macro total effects in summary causal graphs with latent confounding</title><link>http://arxiv.org/abs/2407.07934v3</link><description>Understanding causal relations in dynamic systems is essential inepidemiology. While causal inference methods have been extensively studied,they often rely on fully specified causal graphs, which may not always beavailable in complex dynamic systems. Partially specified causal graphs, suchas summary causal graphs (SCGs), provide a simplified representation of causalrelations, omitting temporal information and focusing on high-level causalstructures. This simplification introduces new challenges concerning the typesof queries of interest: macro queries, which involve relationships betweenclusters represented as vertices in the graph, and micro queries, which pertainto relationships between variables that are not directly visible through thevertices of the graph. In this paper, we first clearly distinguish betweenmacro conditional independencies and micro conditional independencies andbetween macro total effects and micro total effects. Then, we demonstrate thesoundness and completeness of the d-separation to identify macro conditionalindependencies in SCGs. Furthermore, we establish that the do-calculus is soundand complete for identifying macro total effects in SCGs. Finally, we give agraphical characterization for the non-identifiability of macro total effectsin SCGs.</description><author>Simon Ferreira, Charles K. Assaad</author><pubDate>Mon, 29 Jul 2024 16:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07934v3</guid></item><item><title>Quantum Machine Learning Architecture Search via Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.20147v1</link><description>The rapid advancement of quantum computing (QC) and machine learning (ML) hasgiven rise to the burgeoning field of quantum machine learning (QML), aiming tocapitalize on the strengths of quantum computing to propel ML forward. Despiteits promise, crafting effective QML models necessitates profound expertise tostrike a delicate balance between model intricacy and feasibility on NoisyIntermediate-Scale Quantum (NISQ) devices. While complex models offer robustrepresentation capabilities, their extensive circuit depth may impede seamlessexecution on extant noisy quantum platforms. In this paper, we address thisquandary of QML model design by employing deep reinforcement learning toexplore proficient QML model architectures tailored for designated supervisedlearning tasks. Specifically, our methodology involves training an RL agent todevise policies that facilitate the discovery of QML models withoutpredetermined ansatz. Furthermore, we integrate an adaptive mechanism todynamically adjust the learning objectives, fostering continuous improvement inthe agent's learning process. Through extensive numerical simulations, weillustrate the efficacy of our approach within the realm of classificationtasks. Our proposed method successfully identifies VQC architectures capable ofachieving high classification accuracy while minimizing gate depth. Thispioneering approach not only advances the study of AI-driven quantum circuitdesign but also holds significant promise for enhancing performance in the NISQera.</description><author>Xin Dai, Tzu-Chieh Wei, Shinjae Yoo, Samuel Yen-Chi Chen</author><pubDate>Mon, 29 Jul 2024 16:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20147v1</guid></item><item><title>ByteCheckpoint: A Unified Checkpointing System for LLM Development</title><link>http://arxiv.org/abs/2407.20143v1</link><description>The development of real-world Large Language Models (LLMs) necessitatescheckpointing of training states in persistent storage to mitigate potentialsoftware and hardware failures, as well as to facilitate checkpointtransferring within the training pipeline and across various tasks. Due to theimmense size of LLMs, saving and loading checkpoints often incur intolerableminute-level stalls, significantly diminishing training efficiency. Besides,when transferring checkpoints across tasks, checkpoint resharding, defined asloading checkpoints into parallel configurations differing from those used forsaving, is often required according to the characteristics and resource quotaof specific tasks. Previous checkpointing systems [16,3,33,6] assume consistentparallel configurations, failing to address the complexities of checkpointtransformation during resharding. Furthermore, in the industry platform,developers create checkpoints from different training frameworks[23,36,21,11],each with its own unique storage and I/O logic. This diversity complicates theimplementation of unified checkpoint management and optimization. To addressthese challenges, we introduce ByteCheckpoint, a PyTorch-native multi-frameworkLLM checkpointing system that supports automatic online checkpoint resharding.ByteCheckpoint employs a data/metadata disaggregated storage architecture,decoupling checkpoint storage from the adopted parallelism strategies andtraining frameworks. We design an efficient asynchronous tensor mergingtechnique to settle the irregular tensor sharding problem and propose severalI/O performance optimizations to significantly enhance the efficiency ofcheckpoint saving and loading. Experimental results demonstrateByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to529.22X) and loading (by up to 3.51X) costs, compared to baseline methods.</description><author>Borui Wan, Mingji Han, Yiyao Sheng, Zhichao Lai, Mofan Zhang, Junda Zhang, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu</author><pubDate>Mon, 29 Jul 2024 16:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20143v1</guid></item><item><title>Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search</title><link>http://arxiv.org/abs/2311.03583v2</link><description>This work studies a central extremal graph theory problem inspired by a 1975conjecture of Erd\H{o}s, which aims to find graphs with a given size (number ofnodes) that maximize the number of edges without having 3- or 4-cycles. Weformulate this problem as a sequential decision-making problem and compareAlphaZero, a neural network-guided tree search, with tabu search, a heuristiclocal search method. Using either method, by introducing a curriculum --jump-starting the search for larger graphs using good graphs found at smallersizes -- we improve the state-of-the-art lower bounds for several sizes. Wealso propose a flexible graph-generation environment and apermutation-invariant network architecture for learning to search in the spaceof graphs.</description><author>Abbas Mehrabian, Ankit Anand, Hyunjik Kim, Nicolas Sonnerat, Matej Balog, Gheorghe Comanici, Tudor Berariu, Andrew Lee, Anian Ruoss, Anna Bulanova, Daniel Toyama, Sam Blackwell, Bernardino Romera Paredes, Petar VeliÄkoviÄ, Laurent Orseau, Joonkyung Lee, Anurag Murty Naredla, Doina Precup, Adam Zsolt Wagner</author><pubDate>Mon, 29 Jul 2024 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03583v2</guid></item><item><title>DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2407.20141v1</link><description>Diffusion-based personalized visual content generation technologies haveachieved significant breakthroughs, allowing for the creation of specificobjects by just learning from a few reference photos. However, when misused tofabricate fake news or unsettling content targeting individuals, thesetechnologies could cause considerable societal harm. To address this problem,current methods generate adversarial samples by adversarially maximizing thetraining loss, thereby disrupting the output of any personalized generationmodel trained with these samples. However, the existing methods fail to achieveeffective defense and maintain stealthiness, as they overlook the intrinsicproperties of diffusion models. In this paper, we introduce a novel Dual-DomainAnti-Personalization framework (DDAP). Specifically, we have developed SpatialPerturbation Learning (SPL) by exploiting the fixed and perturbation-sensitivenature of the image encoder in personalized generation. Subsequently, we havedesigned a Frequency Perturbation Learning (FPL) method that utilizes thecharacteristics of diffusion models in the frequency domain. The SPL disruptsthe overall texture of the generated images, while the FPL focuses on imagedetails. By alternating between these two methods, we construct the DDAPframework, effectively harnessing the strengths of both domains. To furtherenhance the visual quality of the adversarial samples, we design a localizationmodule to accurately capture attentive areas while ensuring the effectivenessof the attack and avoiding unnecessary disturbances in the background.Extensive experiments on facial benchmarks have shown that the proposed DDAPenhances the disruption of personalized generation models while alsomaintaining high quality in adversarial samples, making it more effective inprotecting privacy in practical applications.</description><author>Jing Yang, Runping Xi, Yingxin Lai, Xun Lin, Zitong Yu</author><pubDate>Mon, 29 Jul 2024 16:11:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20141v1</guid></item><item><title>To accept or not to accept? An IRT-TOE Framework to Understand Educators' Resistance to Generative AI in Higher Education</title><link>http://arxiv.org/abs/2407.20130v1</link><description>Since the public release of Chat Generative Pre-Trained Transformer(ChatGPT), extensive discourse has emerged concerning the potential advantagesand challenges of integrating Generative Artificial Intelligence (GenAI) intoeducation. In the realm of information systems, research on technology adoptionis crucial for understanding the diverse factors influencing the uptake ofspecific technologies. Theoretical frameworks, refined and validated overdecades, serve as guiding tools to elucidate the individual and organizationaldynamics, obstacles, and perceptions surrounding technology adoption. However,while several models have been proposed, they often prioritize elucidating thefactors that facilitate acceptance over those that impede it, typicallyfocusing on the student perspective and leaving a gap in empirical evidenceregarding educators viewpoints. Given the pivotal role educators play in highereducation, this study aims to develop a theoretical model to empiricallypredict the barriers preventing educators from adopting GenAI in theirclassrooms. Acknowledging the lack of theoretical models tailored toidentifying such barriers, our approach is grounded in the InnovationResistance Theory (IRT) framework and augmented with constructs from theTechnology-Organization-Environment (TOE) framework. This model is transformedinto a measurement instrument employing a quantitative approach, complementedby a qualitative approach to enrich the analysis and uncover concerns relatedto GenAI adoption in the higher education domain.</description><author>Jan-Erik Kalmus, Anastasija Nikiforova</author><pubDate>Mon, 29 Jul 2024 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20130v1</guid></item><item><title>Frame Interpolation with Consecutive Brownian Bridge Diffusion</title><link>http://arxiv.org/abs/2405.05953v3</link><description>Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as adiffusion-based conditional image generation problem, synthesizing theintermediate frame given a random noise and neighboring frames. Due to therelatively high resolution of videos, Latent Diffusion Models (LDMs) areemployed as the conditional generation model, where the autoencoder compressesimages into latent representations for diffusion and then reconstructs imagesfrom these latent representations. Such a formulation poses a crucialchallenge: VFI expects that the output is deterministically equal to the groundtruth intermediate frame, but LDMs randomly generate a diverse set of differentimages when the model runs multiple times. The reason for the diversegeneration is that the cumulative variance (variance accumulated at each stepof generation) of generated latent representations in LDMs is large. This makesthe sampling trajectory random, resulting in diverse rather than deterministicgenerations. To address this problem, we propose our unique solution: FrameInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, wepropose consecutive Brownian Bridge diffusion that takes a deterministicinitial value as input, resulting in a much smaller cumulative variance ofgenerated latent representations. Our experiments suggest that our method canimprove together with the improvement of the autoencoder and achievestate-of-the-art performance in VFI, leaving strong potential for furtherenhancement.</description><author>Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</author><pubDate>Mon, 29 Jul 2024 15:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05953v3</guid></item><item><title>Finite-Sample Guarantees for Best-Response Learning Dynamics in Zero-Sum Matrix Games</title><link>http://arxiv.org/abs/2407.20128v1</link><description>We study best-response type learning dynamics for two player zero-sum matrixgames. We consider two settings that are distinguished by the type ofinformation that each player has about the game and their opponent's strategy.The first setting is the full information case, in which each player knowstheir own and the opponent's payoff matrices and observes the opponent's mixedstrategy. The second setting is the minimal information case, where players donot observe the opponent's strategy and are not aware of either of the payoffmatrices (instead they only observe their realized payoffs). For this setting,also known as the radically uncoupled case in the learning in games literature,we study a two-timescale learning dynamics that combine smoothed best-responsetype updates for strategy estimates with a TD-learning update to estimate alocal payoff function. For these dynamics, without additional exploration, weprovide polynomial-time finite-sample guarantees for convergence to an$\epsilon$-Nash equilibrium.</description><author>Fathima Zarin Faizal, Asuman Ozdaglar, Martin J. Wainwright</author><pubDate>Mon, 29 Jul 2024 15:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20128v1</guid></item><item><title>Extreme time extrapolation capabilities and thermodynamic consistency of physics-inspired Neural Networks for the 3D microstructure evolution of materials</title><link>http://arxiv.org/abs/2407.20126v1</link><description>A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce theevolution of the spinodal decomposition process in three dimensions asdescribed by the Cahn-Hilliard equation. A specialized, physics-inspiredarchitecture is proven to provide close accordance between the predictedevolutions and the ground truth ones obtained via conventional integrationschemes. The method can closely reproduce the evolution of microstructures notrepresented in the training set at a fraction of the computational costs.Extremely long-time extrapolation capabilities are achieved, up to reaching thetheoretically expected equilibrium state of the system, despite the trainingset containing only relatively-short, initial phases of the evolution.Quantitative accordance with the decay rate of the Free energy is alsodemonstrated up to late coarsening stages, providing an example of adata-driven, physically consistent and high-accuracy Machine Learning methodfor the long timescale simulation of materials.</description><author>Daniele Lanzoni, Andrea Fantasia, Roberto Bergamaschini, Olivier Pierre-Louis, Francesco Montalenti</author><pubDate>Mon, 29 Jul 2024 15:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20126v1</guid></item><item><title>AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics</title><link>http://arxiv.org/abs/2407.20124v1</link><description>The rapid evolution of multimedia and computer vision technologies requiresadaptive visual model deployment strategies to effectively handle diverse tasksand varying environments. This work introduces AxiomVision, a novel frameworkthat can guarantee accuracy by leveraging edge computing to dynamically selectthe most efficient visual models for video analytics under diverse scenarios.Utilizing a tiered edge-cloud architecture, AxiomVision enables the deploymentof a broad spectrum of visual models, from lightweight to complex DNNs, thatcan be tailored to specific scenarios while considering camera source impacts.In addition, AxiomVision provides three core innovations: (1) a dynamic visualmodel selection mechanism utilizing continual online learning, (2) an efficientonline method that efficiently takes into account the influence of the camera'sperspective, and (3) a topology-driven grouping approach that accelerates themodel selection process. With rigorous theoretical guarantees, theseadvancements provide a scalable and effective solution for visual tasksinherent to multimedia systems, such as object detection, classification, andcounting. Empirically, AxiomVision achieves a 25.7\% improvement in accuracy.</description><author>Xiangxiang Dai, Zeyu Zhang, Peng Yang, Yuedong Xu, Xutong Liu, John C. S. Lui</author><pubDate>Mon, 29 Jul 2024 15:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20124v1</guid></item><item><title>Tightening the Evaluation of PAC Bounds Using Formal Verification Results</title><link>http://arxiv.org/abs/2407.20122v1</link><description>Probably Approximately Correct (PAC) bounds are widely used to deriveprobabilistic guarantees for the generalisation of machine learning models.They highlight the components of the model which contribute to itsgeneralisation capacity. However, current state-of-the-art results are loose inapproximating the generalisation capacity of deployed machine learning models.Consequently, while PAC bounds are theoretically useful, their applicabilityfor evaluating a model's generalisation property in a given operational designdomain is limited. The underlying classical theory is supported by the ideathat bounds can be tightened when the number of test points available to theuser to evaluate the model increases. Yet, in the case of neural networks, thenumber of test points required to obtain bounds of interest is oftenimpractical even for small problems. In this paper, we take the novel approach of using the formal verification ofneural systems to inform the evaluation of PAC bounds. Rather than usingpointwise information obtained from repeated tests, we use verification resultson regions around test points. We show that conditioning existing bounds onverification results leads to a tightening proportional to the underlyingprobability mass of the verified region.</description><author>Thomas Walker, Alessio Lomuscio</author><pubDate>Mon, 29 Jul 2024 15:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20122v1</guid></item><item><title>Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes</title><link>http://arxiv.org/abs/2309.00237v4</link><description>The development of large language models tailored for handling patients'clinical notes is often hindered by the limited accessibility and usability ofthese notes due to strict privacy regulations. To address these challenges, wefirst create synthetic large-scale clinical notes using publicly available casereports extracted from biomedical literature. We then use these synthetic notesto train our specialized clinical large language model, Asclepius. WhileAsclepius is trained on synthetic data, we assess its potential performance inreal-world applications by evaluating it using real clinical notes. Webenchmark Asclepius against several other large language models, includingGPT-3.5-turbo and other open-source alternatives. To further validate ourapproach using synthetic notes, we also compare Asclepius with its variantstrained on real clinical notes. Our findings convincingly demonstrate thatsynthetic clinical notes can serve as viable substitutes for real ones whenconstructing high-performing clinical language models. This conclusion issupported by detailed evaluations conducted by both GPT-4 and medicalprofessionals. All resources including weights, codes, and data used in thedevelopment of Asclepius are made publicly accessible for future research.(https://github.com/starmpcc/Asclepius)</description><author>Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi</author><pubDate>Mon, 29 Jul 2024 15:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00237v4</guid></item><item><title>EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation</title><link>http://arxiv.org/abs/2407.20121v1</link><description>Cross-domain recommendation has attracted substantial interest in industrialapps such as Meituan, which serves multiple business domains via knowledgetransfer and meets the diverse interests of users. However, existing methodstypically follow an implicit modeling paradigm that blends the knowledge fromboth the source and target domains, and design intricate network structures toshare learned embeddings or patterns between domains to improve recommendationaccuracy. Since the transfer of interest signals is unsupervised, theseimplicit paradigms often struggle with the negative transfer resulting fromdifferences in service functions and presentation forms across differentdomains. In this paper, we propose a simple and effective EXplicit InterestTransfer framework named EXIT to address the stated challenge. Specifically, wepropose a novel label combination approach that enables the model to directlylearn beneficial source domain interests through supervised learning, whileexcluding inappropriate interest signals. Moreover, we introduce a sceneselector network to model the interest transfer intensity under fine-grainedscenes. Offline experiments conducted on the industrial production dataset andonline A/B tests validate the superiority and effectiveness of our proposedframework. Without complex network structures or training processes, EXIT canbe easily deployed in the industrial recommendation system. EXIT has beensuccessfully deployed in the online homepage recommendation system of MeituanApp, serving the main traffic.</description><author>Lei Huang, Weitao Li, Chenrui Zhang, Jinpeng Wang, Xianchun Yi, Sheng Chen</author><pubDate>Mon, 29 Jul 2024 15:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20121v1</guid></item><item><title>Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number</title><link>http://arxiv.org/abs/2407.20119v1</link><description>We introduce a novel self-supervised deep clustering approach tailored forunstructured data without requiring prior knowledge of the number of clusters,termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRCadaptively learns the graph structure and edge weights to capture both localand global structural information. The obtained graph enables us to learnclustering-friendly feature representations by an enhanced graph auto-encoderwith contrastive learning technique. It further leverages the clusteringresults adaptively obtained by robust continuous clustering (RCC) to generateprototypes for negative sampling, which can further contribute to promotingconsistency among positive pairs and enlarging the gap between positive andnegative samples. ASRC obtains the final clustering results by applying RCC tothe learned feature representations with their consistent graph structure andedge weights. Extensive experiments conducted on seven benchmark datasetsdemonstrate the efficacy of ASRC, demonstrating its superior performance overother popular clustering models. Notably, ASRC even outperforms methods thatrely on prior knowledge of the number of clusters, highlighting itseffectiveness in addressing the challenges of clustering unstructured data.</description><author>Chen-Lu Ding, Jiancan Wu, Wei Lin, Shiyang Shen, Xiang Wang, Yancheng Yuan</author><pubDate>Mon, 29 Jul 2024 15:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20119v1</guid></item><item><title>Surpassing Cosine Similarity for Multidimensional Comparisons: Dimension Insensitive Euclidean Metric (DIEM)</title><link>http://arxiv.org/abs/2407.08623v2</link><description>The advancement in computational power and hardware efficiency enabled thetackling of increasingly complex and high-dimensional problems. Whileartificial intelligence (AI) achieved remarkable results, the interpretabilityof high-dimensional solutions remains challenging. A critical issue is thecomparison of multidimensional quantities, which is essential in techniqueslike Principal Component Analysis (PCA), or k-means clustering. Common metricssuch as cosine similarity, Euclidean distance, and Manhattan distance are oftenused for such comparisons - for example in muscular synergies of the humanmotor control system. However, their applicability and interpretabilitydiminish as dimensionality increases. This paper provides a comprehensiveanalysis of the effects of dimensionality on these metrics. Our results revealsignificant limitations of cosine similarity, particularly its dependency onthe dimensionality of the vectors, leading to biased and less interpretableoutcomes. To address this, we introduce the Dimension Insensitive EuclideanMetric (DIEM) which demonstrates superior robustness and generalizabilityacross dimensions. DIEM maintains consistent variability and eliminates thebiases observed in traditional metrics, making it a reliable tool forhigh-dimensional comparisons. This novel metric has the potential to replacecosine similarity, providing a more accurate and insightful method to analyzemultidimensional data in fields ranging from neuromotor control to machine anddeep learning.</description><author>Federico Tessari, Neville Hogan</author><pubDate>Mon, 29 Jul 2024 15:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08623v2</guid></item><item><title>FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis</title><link>http://arxiv.org/abs/2407.20114v1</link><description>In the field of Image-Text Retrieval (ITR), recent advancements haveleveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG)instance-level retrieval, achieving high accuracy at the cost of increasedcomputational complexity. For Coarse-Grained (CG) category-level retrieval,prominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency,albeit at the cost of retrieval performance. Due to differences inmethodologies, FG and CG models are rarely compared directly within evaluationsin the literature, resulting in a lack of empirical data quantifying theretrieval performance-efficiency tradeoffs between the two. This paperaddresses this gap by introducing the \texttt{FiCo-ITR} library, whichstandardises evaluation methodologies for both FG and CG models, facilitatingdirect comparisons. We conduct empirical evaluations of representative modelsfrom both subfields, analysing precision, recall, and computational complexityacross varying data scales. Our findings offer new insights into theperformance-efficiency trade-offs between recent representative FG and CGmodels, highlighting their respective strengths and limitations. These findingsprovide the foundation necessary to make more informed decisions regardingmodel selection for specific retrieval tasks and highlight avenues for futureresearch into hybrid systems that leverage the strengths of both FG and CGapproaches.</description><author>Mikel Williams-Lekuona, Georgina Cosma</author><pubDate>Mon, 29 Jul 2024 15:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20114v1</guid></item><item><title>Generalizable Implicit Motion Modeling for Video Frame Interpolation</title><link>http://arxiv.org/abs/2407.08680v3</link><description>Motion modeling is critical in flow-based Video Frame Interpolation (VFI).Existing paradigms either consider linear combinations of bidirectional flowsor directly predict bilateral flows for given timestamps without exploringfavorable motion priors, thus lacking the capability of effectively modelingspatiotemporal dynamics in real-world videos. To address this limitation, inthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a noveland effective approach to motion modeling for VFI. Specifically, to enable GIMMas an effective motion modeling paradigm, we design a motion encoding pipelineto model spatiotemporal motion latent from bidirectional flows extracted frompre-trained flow estimators, effectively representing input-specific motionpriors. Then, we implicitly predict arbitrary-timestep optical flows within twoadjacent input frames via an adaptive coordinate-based neural network, withspatiotemporal coordinates and motion latent as inputs. Our GIMM can besmoothly integrated with existing flow-based VFI works without furthermodifications. We show that GIMM performs better than the current state of theart on the VFI benchmarks.</description><author>Zujin Guo, Wei Li, Chen Change Loy</author><pubDate>Mon, 29 Jul 2024 15:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08680v3</guid></item><item><title>Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2407.20109v1</link><description>One important property of DIstribution Correction Estimation (DICE) methodsis that the solution is the optimal stationary distribution ratio between theoptimized and data collection policy. In this work, we show that DICE-basedmethods can be viewed as a transformation from the behavior distribution to theoptimal policy distribution. Based on this, we propose a novel approach,Diffusion-DICE, that directly performs this transformation using diffusionmodels. We find that the optimal policy's score function can be decomposed intotwo terms: the behavior policy's score function and the gradient of a guidanceterm which depends on the optimal distribution ratio. The first term can beobtained from a diffusion model trained on the dataset and we propose anin-sample learning objective to learn the second term. Due to themulti-modality contained in the optimal policy distribution, the transformationin Diffusion-DICE may guide towards those local-optimal modes. We thus generatea few candidate actions and carefully select from them to approachglobal-optimum. Different from all other diffusion-based offline RL methods,the guide-then-select paradigm in Diffusion-DICE only uses in-sample actionsfor training and brings minimal error exploitation in the value function. Weuse a didatic toycase example to show how previous diffusion-based methods failto generate optimal actions due to leveraging these errors and howDiffusion-DICE successfully avoids that. We then conduct extensive experimentson benchmark datasets to show the strong performance of Diffusion-DICE.</description><author>Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan, Amy Zhang</author><pubDate>Mon, 29 Jul 2024 15:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20109v1</guid></item><item><title>Classification, Regression and Segmentation directly from k-Space in Cardiac MRI</title><link>http://arxiv.org/abs/2407.20108v1</link><description>Cardiac Magnetic Resonance Imaging (CMR) is the gold standard for diagnosingcardiovascular diseases. Clinical diagnoses predominantly rely onmagnitude-only Digital Imaging and Communications in Medicine (DICOM) images,omitting crucial phase information that might provide additional diagnosticbenefits. In contrast, k-space is complex-valued and encompasses both magnitudeand phase information, while humans cannot directly perceive. In this work, wepropose KMAE, a Transformer-based model specifically designed to processk-space data directly, eliminating conventional intermediary conversion stepsto the image domain. KMAE can handle critical cardiac disease classification,relevant phenotype regression, and cardiac morphology segmentation tasks. Weutilize this model to investigate the potential of k-space-based diagnosis incardiac MRI. Notably, this model achieves competitive classification andregression performance compared to image-domain methods e.g. MaskedAutoencoders (MAEs) and delivers satisfactory segmentation performance with amyocardium dice score of 0.884. Last but not least, our model exhibits robustperformance with consistent results even when the k-space is 8* undersampled.We encourage the MR community to explore the untapped potential of k-space andpursue end-to-end, automated diagnosis with reduced human intervention.</description><author>Ruochen Li, Jiazhen Pan, Youxiang Zhu, Juncheng Ni, Daniel Rueckert</author><pubDate>Mon, 29 Jul 2024 15:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20108v1</guid></item><item><title>Strong Copyright Protection for Language Models via Adaptive Model Fusion</title><link>http://arxiv.org/abs/2407.20105v1</link><description>The risk of language models unintentionally reproducing copyrighted materialfrom their training data has led to the development of various protectivemeasures. In this paper, we propose model fusion as an effective solution tosafeguard against copyright infringement. In particular, we introduceCopyright-Protecting Fusion (CP-Fuse), an algorithm that adaptively combineslanguage models to minimize the reproduction of protected materials. CP-Fuse isinspired by the recently proposed Near-Access Free (NAF) framework andadditionally incorporates a desirable balancing property that we demonstrateprevents the reproduction of memorized training data. Our results show thatCP-Fuse significantly reduces the memorization of copyrighted content whilemaintaining high-quality text and code generation. Furthermore, we demonstratehow CP-Fuse can be integrated with other techniques for enhanced protection.</description><author>Javier Abad, Konstantin Donhauser, Francesco Pinto, Fanny Yang</author><pubDate>Mon, 29 Jul 2024 15:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20105v1</guid></item><item><title>Leveraging Pre-trained AudioLDM for Sound Generation: A Benchmark Study</title><link>http://arxiv.org/abs/2303.03857v3</link><description>Deep neural networks have recently achieved breakthroughs in soundgeneration. Despite the outstanding sample quality, current sound generationmodels face issues on small-scale datasets (e.g., overfitting), significantlylimiting performance. In this paper, we make the first attempt to investigatethe benefits of pre-training on sound generation with AudioLDM, thecutting-edge model for audio generation, as the backbone. Our studydemonstrates the advantages of the pre-trained AudioLDM, especially indata-scarcity scenarios. In addition, the baselines and evaluation protocol forsound generation systems are not consistent enough to compare different studiesdirectly. Aiming to facilitate further study on sound generation tasks, webenchmark the sound generation task on various frequently-used datasets. Wehope our results on transfer learning and benchmarks can provide references forfurther research on conditional sound generation.</description><author>Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D. Plumbley, Wenwu Wang</author><pubDate>Mon, 29 Jul 2024 15:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03857v3</guid></item><item><title>F-KANs: Federated Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2407.20100v1</link><description>In this paper, we present an innovative federated learning (FL) approach thatutilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. Byutilizing the adaptive activation capabilities of KANs in a federatedframework, we aim to improve classification capabilities while preservingprivacy. The study evaluates the performance of federated KANs (F- KANs)compared to traditional Multi-Layer Perceptrons (MLPs) on classification task.The results show that the F-KANs model significantly outperforms the federatedMLP model in terms of accuracy, precision, recall, F1 score and stability, andachieves better performance, paving the way for more efficient andprivacy-preserving predictive analytics.</description><author>Engin Zeydan, Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus, Abdullah Aydeger</author><pubDate>Mon, 29 Jul 2024 15:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20100v1</guid></item><item><title>Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders</title><link>http://arxiv.org/abs/2407.14435v2</link><description>Sparse autoencoders (SAEs) are a promising unsupervised approach foridentifying causally relevant and interpretable linear features in a languagemodel's (LM) activations. To be useful for downstream tasks, SAEs need todecompose LM activations faithfully; yet to be interpretable the decompositionmust be sparse -- two objectives that are in tension. In this paper, weintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelityat a given sparsity level on Gemma 2 9B activations, compared to other recentadvances such as Gated and TopK SAEs. We also show that this improvement doesnot come at the cost of interpretability through manual and automatedinterpretability studies. JumpReLU SAEs are a simple modification of vanilla(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLUactivation function -- and are similarly efficient to train and run. Byutilising straight-through-estimators (STEs) in a principled manner, we showhow it is possible to train JumpReLU SAEs effectively despite the discontinuousJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEsto directly train L0 to be sparse, instead of training on proxies such as L1,avoiding problems like shrinkage.</description><author>Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, JÃ¡nos KramÃ¡r, Neel Nanda</author><pubDate>Mon, 29 Jul 2024 15:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14435v2</guid></item><item><title>RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding</title><link>http://arxiv.org/abs/2407.20099v1</link><description>Spiking Neural Networks (SNNs) have received widespread attention due totheir unique neuronal dynamics and low-power nature. Previous researchempirically shows that SNNs with Poisson coding are more robust than ArtificialNeural Networks (ANNs) on small-scale datasets. However, it is still unclear intheory how the adversarial robustness of SNNs is derived, and whether SNNs canstill maintain its adversarial robustness advantage on large-scale datasettasks. This work theoretically demonstrates that SNN's inherent adversarialrobustness stems from its Poisson coding. We reveal the conceptual equivalenceof Poisson coding and randomized smoothing in defense strategies, and analyzein depth the trade-off between accuracy and adversarial robustness in SNNs viathe proposed Randomized Smoothing Coding (RSC) method. Experiments demonstratethat the proposed RSC-SNNs show remarkable adversarial robustness, surpassingANNs and achieving state-of-the-art robustness results on large-scale datasetImageNet. Our open-source implementation code is available at this https URL:https://github.com/KemingWu/RSC-SNN.</description><author>Keming Wu, Man Yao, Yuhong Chou, Xuerui Qiu, Rui Yang, Bo Xu, Guoqi Li</author><pubDate>Mon, 29 Jul 2024 15:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20099v1</guid></item><item><title>Crafting Generative Art through Genetic Improvement: Managing Creative Outputs in Diverse Fitness Landscapes</title><link>http://arxiv.org/abs/2407.20095v1</link><description>Generative art is a rules-driven approach to creating artistic outputs invarious mediums. For example, a fluid simulation can govern the flow of coloredpixels across a digital display or a rectangle placement algorithm can yield aMondrian-style painting. Previously, we investigated how genetic improvement, asub-field of genetic programming, can automatically create and optimizegenerative art drawing programs. One challenge of applying genetic improvementto generative art is defining fitness functions and their interaction in amany-objective evolutionary algorithm such as Lexicase selection. Here, weassess the impact of each fitness function in terms of the their individualeffects on generated images, characteristics of generated programs, and impactof bloat on this specific domain. Furthermore, we have added an additionalfitness function that uses a classifier for mimicking a human's assessment asto whether an output is considered as "art." This classifier is trained on adataset of input images resembling the glitch art aesthetic that we aim tocreate. Our experimental results show that with few fitness functions,individual generative techniques sweep across populations. Moreover, we foundthat compositions tended to be driven by one technique with our current fitnessfunctions. Lastly, we show that our classifier is best suited for filtering outnoisy images, ideally leading towards more outputs relevant to user preference.</description><author>Erik M. Fredericks, Denton Bobeldyk, Jared M. Moore</author><pubDate>Mon, 29 Jul 2024 15:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20095v1</guid></item><item><title>Infrared Small Target Detection based on Adjustable Sensitivity Strategy and Multi-Scale Fusion</title><link>http://arxiv.org/abs/2407.20090v1</link><description>Recently, deep learning-based single-frame infrared small target (SIRST)detection technology has made significant progress. However, existing infraredsmall target detection methods are often optimized for a fixed imageresolution, a single wavelength, or a specific imaging system, limiting theirbreadth and flexibility in practical applications. Therefore, we propose arefined infrared small target detection scheme based on an adjustablesensitivity (AS) strategy and multi-scale fusion. Specifically, a multi-scalemodel fusion framework based on multi-scale direction-aware network (MSDA-Net)is constructed, which uses input images of multiple scales to train multiplemodels and fuses them. Multi-scale fusion helps characterize the shape, edge,and texture features of the target from different scales, making the model moreaccurate and reliable in locating the target. At the same time, we fullyconsider the characteristics of the infrared small target detection task andconstruct an edge enhancement difficulty mining (EEDM) loss. The EEDM losshelps alleviate the problem of category imbalance and guides the network to paymore attention to difficult target areas and edge features during training. Inaddition, we propose an adjustable sensitivity strategy for post-processing.This strategy significantly improves the detection rate of infrared smalltargets while ensuring segmentation accuracy. Extensive experimental resultsshow that the proposed scheme achieves the best performance. Notably, thisscheme won the first prize in the PRCV 2024 wide-area infrared small targetdetection competition.</description><author>Jinmiao Zhao, Zelin Shi, Chuang Yu, Yunpeng Liu</author><pubDate>Mon, 29 Jul 2024 15:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20090v1</guid></item><item><title>Segmenting Fetal Head with Efficient Fine-tuning Strategies in Low-resource Settings: an empirical study with U-Net</title><link>http://arxiv.org/abs/2407.20086v1</link><description>Accurate measurement of fetal head circumference is crucial for estimatingfetal growth during routine prenatal screening. Prior to measurement, it isnecessary to accurately identify and segment the region of interest,specifically the fetal head, in ultrasound images. Recent advancements in deeplearning techniques have shown significant progress in segmenting the fetalhead using encoder-decoder models. Among these models, U-Net has become astandard approach for accurate segmentation. However, training anencoder-decoder model can be a time-consuming process that demands substantialcomputational resources. Moreover, fine-tuning these models is particularlychallenging when there is a limited amount of data available. There are stillno "best-practice" guidelines for optimal fine-tuning of U-net for fetalultrasound image segmentation. This work summarizes existing fine-tuningstrategies with various backbone architectures, model components, andfine-tuning strategies across ultrasound data from Netherlands, Spain, Malawi,Egypt and Algeria. Our study shows that (1) fine-tuning U-Net leads to betterperformance than training from scratch, (2) fine-tuning strategies in decoderare superior to other strategies, (3) network architecture with less number ofparameters can achieve similar or better performance. We also demonstrate theeffectiveness of fine-tuning strategies in low-resource settings and furtherexpand our experiments into few-shot learning. Lastly, we publicly released ourcode and specific fine-tuned weights.</description><author>Fangyijie Wang, GuÃ©nolÃ© Silvestre, Kathleen M. Curran</author><pubDate>Mon, 29 Jul 2024 15:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20086v1</guid></item><item><title>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting</title><link>http://arxiv.org/abs/2406.01805v2</link><description>Tabular data is prevalent in many critical domains, yet it is oftenchallenging to acquire in large quantities. This scarcity usually results inpoor performance of machine learning models on such data. Data augmentation, acommon strategy for performance improvement in vision and language tasks,typically underperforms for tabular data due to the lack of explicit symmetriesin the input space. To overcome this challenge, we introduce TabMDA, a novelmethod for manifold data augmentation on tabular data. This method utilises apre-trained in-context model, such as TabPFN, to map the data into an embeddingspace. TabMDA performs label-invariant transformations by encoding the datamultiple times with varied contexts. This process explores the learnedembedding space of the underlying in-context models, thereby enlarging thetraining dataset. TabMDA is a training-free method, making it applicable to anyclassifier. We evaluate TabMDA on five standard classifiers and observesignificant performance improvements across various tabular datasets. Ourresults demonstrate that TabMDA provides an effective way to leverageinformation from pre-trained in-context models to enhance the performance ofdownstream classifiers. Code is available athttps://github.com/AdrianBZG/TabMDA.</description><author>Andrei Margeloiu, AdriÃ¡n Bazaga, Nikola Simidjievski, Pietro LiÃ², Mateja Jamnik</author><pubDate>Mon, 29 Jul 2024 15:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01805v2</guid></item><item><title>An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation</title><link>http://arxiv.org/abs/2407.20083v1</link><description>Word-level AutoCompletion(WLAC) is a rewarding yet challenging task inComputer-aided Translation. Existing work addresses this task through aclassification model based on a neural network that maps the hidden vector ofthe input context into its corresponding label (i.e., the candidate target wordis treated as a label). Since the context hidden vector itself does not takethe label into account and it is projected to the label through a linearclassifier, the model can not sufficiently leverage valuable information fromthe source sentence as verified in our experiments, which eventually hindersits overall performance. To alleviate this issue, this work proposes anenergy-based model for WLAC, which enables the context hidden vector to capturecrucial information from the source sentence. Unfortunately, training andinference suffer from efficiency and effectiveness challenges, thereby weemploy three simple yet effective strategies to put our model into practice.Experiments on four standard benchmarks demonstrate that our reranking-basedapproach achieves substantial improvements (about 6.07%) over the previousstate-of-the-art model. Further analyses show that each strategy of ourapproach contributes to the final performance.</description><author>Cheng Yang, Guoping Huang, Mo Yu, Zhirui Zhang, Siheng Li, Mingming Yang, Shuming Shi, Yujiu Yang, Lemao Liu</author><pubDate>Mon, 29 Jul 2024 15:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20083v1</guid></item><item><title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title><link>http://arxiv.org/abs/2307.16048v2</link><description>We consider the problem of learning a set of direct causes of a targetvariable from an observational joint distribution. Learning directed acyclicgraphs (DAGs) that represent the causal structure is a fundamental problem inscience. Several results are known when the full DAG is identifiable from thedistribution, such as assuming a nonlinear Gaussian data-generating process.Here, we are only interested in identifying the direct causes of one targetvariable (local causal structure), not the full DAG. This allows us to relaxthe identifiability assumptions and develop possibly faster and more robustalgorithms. In contrast to the Invariance Causal Prediction framework, we onlyassume that we observe one environment without any interventions. We discussdifferent assumptions for the data-generating process of the target variableunder which the set of direct causes is identifiable from the distribution.While doing so, we put essentially no assumptions on the variables other thanthe target variable. In addition to the novel identifiability results, weprovide two practical algorithms for estimating the direct causes from a finiterandom sample and demonstrate their effectiveness on several benchmark and realdatasets.</description><author>Juraj Bodik, ValÃ©rie Chavez-Demoulin</author><pubDate>Mon, 29 Jul 2024 15:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16048v2</guid></item><item><title>UniTTA: Unified Benchmark and Versatile Framework Towards Realistic Test-Time Adaptation</title><link>http://arxiv.org/abs/2407.20080v1</link><description>Test-Time Adaptation (TTA) aims to adapt pre-trained models to the targetdomain during testing. In reality, this adaptability can be influenced bymultiple factors. Researchers have identified various challenging scenarios anddeveloped diverse methods to address these challenges, such as dealing withcontinual domain shifts, mixed domains, and temporally correlated or imbalancedclass distributions. Despite these efforts, a unified and comprehensivebenchmark has yet to be established. To this end, we propose a UnifiedTest-Time Adaptation (UniTTA) benchmark, which is comprehensive and widelyapplicable. Each scenario within the benchmark is fully described by a Markovstate transition matrix for sampling from the original dataset. The UniTTAbenchmark considers both domain and class as two independent dimensions of dataand addresses various combinations of imbalance/balance andi.i.d./non-i.i.d./continual conditions, covering a total of \( (2 \times 3)^2 =36 \) scenarios. It establishes a comprehensive evaluation benchmark forrealistic TTA and provides a guideline for practitioners to select the mostsuitable TTA method. Alongside this benchmark, we propose a versatile UniTTAframework, which includes a Balanced Domain Normalization (BDN) layer and aCOrrelated Feature Adaptation (COFA) method--designed to mitigate distributiongaps in domain and class, respectively. Extensive experiments demonstrate thatour UniTTA framework excels within the UniTTA benchmark and achievesstate-of-the-art performance on average. Our code is available at\url{https://github.com/LeapLabTHU/UniTTA}.</description><author>Chaoqun Du, Yulin Wang, Jiayi Guo, Yizeng Han, Jie Zhou, Gao Huang</author><pubDate>Mon, 29 Jul 2024 15:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20080v1</guid></item><item><title>FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training with Limited Resources</title><link>http://arxiv.org/abs/2407.01445v2</link><description>Existing studies of training state-of-the-art Contrastive Language-ImagePretraining (CLIP) models on large-scale data involve hundreds of or eventhousands of GPUs due to the requirement of a large batch size. However, such alarge amount of resources is not accessible to most people. While advancedcompositional optimization techniques for optimizing global contrastive losseshave been demonstrated effective for removing the requirement of large batchsize, their performance on large-scale data remains underexplored and notoptimized. To bridge the gap, this paper explores several aspects of CLIPtraining with limited resources (e.g., up to tens of GPUs). First, we introduceFastCLIP, a general CLIP training framework built on advanced compositionaloptimization techniques while designed and optimized for the distributedsetting. Our framework is equipped with an efficient gradient reductionstrategy to reduce communication overhead. Second, to further boost trainingefficiency, we investigate three components of the framework from anoptimization perspective: the schedule of the inner learning rate, the updaterules of the temperature parameter and the model parameters, respectively.Experiments on different strategies for each component shed light on how toconduct CLIP training more efficiently. Finally, we benchmark the performanceof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on differentcompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7million, 9.1 million to 315 million image-text pairs to demonstrate thesignificant improvement of FastCLIP in the resource-limited setting. We releasethe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .</description><author>Xiyuan Wei, Fanjiang Ye, Ori Yonay, Xingyu Chen, Baixi Sun, Dingwen Tao, Tianbao Yang</author><pubDate>Mon, 29 Jul 2024 15:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01445v2</guid></item><item><title>Background Semantics Matter: Cross-Task Feature Exchange Network for Clustered Infrared Small Target Detection With Sky-Annotated Dataset</title><link>http://arxiv.org/abs/2407.20078v1</link><description>Infrared small target detection poses unique challenges due to the scarcityof intrinsic target features and the abundance of similar backgrounddistractors. We argue that background semantics play a pivotal role indistinguishing visually similar objects for this task. To address this, weintroduce a new task -- clustered infrared small target detection, and presentDenseSIRST, a novel benchmark dataset that provides per-pixel semanticannotations for background regions, enabling the transition from sparse todense target detection. Leveraging this dataset, we propose theBackground-Aware Feature Exchange Network (BAFE-Net), which transforms thedetection paradigm from a single task focused on the foreground to a multi-taskarchitecture that jointly performs target detection and background semanticsegmentation. BAFE-Net introduces a cross-task feature hard-exchange mechanismto embed target and background semantics between the two tasks. Furthermore, wepropose the Background-Aware Gaussian Copy-Paste (BAG-CP) method, whichselectively pastes small targets into sky regions during training, avoiding thecreation of false alarm targets in complex non-sky backgrounds. Extensiveexperiments validate the effectiveness of BAG-CP and BAFE-Net in improvingtarget detection accuracy while reducing false alarms. The DenseSIRST dataset,code, and trained models are available at https://github.com/GrokCV/BAFE-Net.</description><author>Yimian Dai, Mengxuan Xiao, Yiming Zhu, Huan Wang, Kehua Guo, Jian Yang</author><pubDate>Mon, 29 Jul 2024 15:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20078v1</guid></item><item><title>Investigating the Impact of Semi-Supervised Methods with Data Augmentation on Offensive Language Detection in Romanian Language</title><link>http://arxiv.org/abs/2407.20076v1</link><description>Offensive language detection is a crucial task in today's digital landscape,where online platforms grapple with maintaining a respectful and inclusiveenvironment. However, building robust offensive language detection modelsrequires large amounts of labeled data, which can be expensive andtime-consuming to obtain. Semi-supervised learning offers a feasible solutionby utilizing labeled and unlabeled data to create more accurate and robustmodels. In this paper, we explore a few different semi-supervised methods, aswell as data augmentation techniques. Concretely, we implemented eightsemi-supervised methods and ran experiments for them using only the availabledata in the RO-Offense dataset and applying five augmentation techniques beforefeeding the data to the models. Experimental results demonstrate that some ofthem benefit more from augmentations than others.</description><author>Elena Beatrice Nicola, Dumitru Clementin Cercel, Florin Pop</author><pubDate>Mon, 29 Jul 2024 15:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20076v1</guid></item><item><title>An Interpretable Rule Creation Method for Black-Box Models based on Surrogate Trees -- SRules</title><link>http://arxiv.org/abs/2407.20070v1</link><description>As artificial intelligence (AI) systems become increasingly integrated intocritical decision-making processes, the need for transparent and interpretablemodels has become paramount. In this article we present a new ruleset creationmethod based on surrogate decision trees (SRules), designed to improve theinterpretability of black-box machine learning models. SRules balances theaccuracy, coverage, and interpretability of machine learning models byrecursively creating surrogate interpretable decision tree models thatapproximate the decision boundaries of a complex model. We propose a systematicframework for generating concise and meaningful rules from these surrogatemodels, allowing stakeholders to understand and trust the AI system'sdecision-making process. Our approach not only provides interpretable rules,but also quantifies the confidence and coverage of these rules. The proposedmodel allows to adjust its parameters to counteract the lack ofinterpretability by precision and coverage by allowing a near perfect fit andhigh interpretability of some parts of the model . The results show that SRulesimproves on other state-of-the-art techniques and introduces the possibility ofcreating highly interpretable specific rules for specific sub-parts of themodel.</description><author>Mario ParrÃ³n Verdasco, Esteban GarcÃ­a-Cuesta</author><pubDate>Mon, 29 Jul 2024 14:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20070v1</guid></item><item><title>CityX: Controllable Procedural Content Generation for Unbounded 3D Cities</title><link>http://arxiv.org/abs/2407.17572v2</link><description>Generating a realistic, large-scale 3D virtual city remains a complexchallenge due to the involvement of numerous 3D assets, various city styles,and strict layout constraints. Existing approaches provide promising attemptsat procedural content generation to create large-scale scenes using Blenderagents. However, they face crucial issues such as difficulties in scaling upgeneration capability and achieving fine-grained control at the semantic layoutlevel. To address these problems, we propose a novel multi-modal controllableprocedural content generation method, named CityX, which enhances realistic,unbounded 3D city generation guided by multiple layout conditions, includingOSM, semantic maps, and satellite images. Specifically, the proposed methodcontains a general protocol for integrating various PCG plugins and amulti-agent framework for transforming instructions into executable Blenderactions. Through this effective framework, CityX shows the potential to buildan innovative ecosystem for 3D scene generation by bridging the gap between thequality of generated assets and industrial requirements. Extensive experimentshave demonstrated the effectiveness of our method in creating high-quality,diverse, and unbounded cities guided by multi-modal conditions. Our projectpage: https://cityx-lab.github.io.</description><author>Shougao Zhang, Mengqi Zhou, Yuxi Wang, Chuanchen Luo, Rongyu Wang, Yiwei Li, Xucheng Yin, Zhaoxiang Zhang, Junran Peng</author><pubDate>Mon, 29 Jul 2024 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17572v2</guid></item><item><title>Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise</title><link>http://arxiv.org/abs/2407.20068v1</link><description>The Sparse Vector Technique (SVT) is one of the most fundamental tools indifferential privacy (DP). It works as a backbone for adaptive data analysis byanswering a sequence of queries on a given dataset, and gleaning usefulinformation in a privacy-preserving manner. Unlike the typical private queryreleases that directly publicize the noisy query results, SVT is lessinformative -- it keeps the noisy query results to itself and only reveals abinary bit for each query, indicating whether the query result surpasses apredefined threshold. To provide a rigorous DP guarantee for SVT, prior worksin the literature adopt a conservative privacy analysis by assuming the directdisclosure of noisy query results as in typical private query releases. Thisapproach, however, hinders SVT from achieving higher query accuracy due to anoverestimation of the privacy risks, which further leads to an excessive noiseinjection using the Laplacian or Gaussian noise for perturbation. Motivated bythis, we provide a new privacy analysis for SVT by considering its lessinformative nature. Our analysis results not only broaden the range ofapplicable noise types for perturbation in SVT, but also identify theexponential noise as optimal among all evaluated noises (which, however, isusually deemed non-applicable in prior works). The main challenge in applyingexponential noise to SVT is mitigating the sub-optimal performance due to thebias introduced by noise distributions. To address this, we develop autility-oriented optimal threshold correction method and an appending strategy,which enhances the performance of SVT by increasing the precision and recall,respectively. The effectiveness of our proposed methods is substantiated boththeoretically and empirically, demonstrating significant improvements up to$50\%$ across evaluated metrics.</description><author>Yuhan Liu, Sheng Wang, Yixuan Liu, Feifei Li, Hong Chen</author><pubDate>Mon, 29 Jul 2024 14:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20068v1</guid></item><item><title>xAI-Drop: Don't Use What You Cannot Explain</title><link>http://arxiv.org/abs/2407.20067v1</link><description>Graph Neural Networks (GNNs) have emerged as the predominant paradigm forlearning from graph-structured data, offering a wide range of applications fromsocial network analysis to bioinformatics. Despite their versatility, GNNs facechallenges such as oversmoothing, lack of generalization and poorinterpretability, which hinder their wider adoption and reliability in criticalapplications. Dropping has emerged as an effective paradigm for reducing noiseduring training and improving robustness of GNNs. However, existing approachesoften rely on random or heuristic-based selection criteria, lacking aprincipled method to identify and exclude nodes that contribute to noise andover-complexity in the model. In this work, we argue that explainability shouldbe a key indicator of a model's robustness throughout its training phase. Tothis end, we introduce xAI-Drop, a novel topological-level dropping regularizerthat leverages explainability to pinpoint noisy network elements to be excludedfrom the GNN propagation mechanism. An empirical evaluation on diversereal-world datasets demonstrates that our method outperforms currentstate-of-the-art dropping approaches in accuracy, effectively reducesover-smoothing, and improves explanation quality.</description><author>Vincenzo Marco De Luca, Antonio Longa, Andrea Passerini, Pietro LiÃ²</author><pubDate>Mon, 29 Jul 2024 14:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20067v1</guid></item><item><title>Long-form music generation with latent diffusion</title><link>http://arxiv.org/abs/2404.10301v2</link><description>Audio-based generative models for music have seen great strides recently, butso far have not managed to produce full-length music tracks with coherentmusical structure from text prompts. We show that by training a generativemodel on long temporal contexts it is possible to produce long-form music of upto 4m45s. Our model consists of a diffusion-transformer operating on a highlydownsampled continuous latent representation (latent rate of 21.5Hz). Itobtains state-of-the-art generations according to metrics on audio quality andprompt alignment, and subjective tests reveal that it produces full-lengthmusic with coherent structure.</description><author>Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, Jordi Pons</author><pubDate>Mon, 29 Jul 2024 14:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10301v2</guid></item><item><title>Differentially Private Gradient Flow based on the Sliced Wasserstein Distance</title><link>http://arxiv.org/abs/2312.08227v2</link><description>Safeguarding privacy in sensitive training data is paramount, particularly inthe context of generative modeling. This can be achieved through eitherdifferentially private stochastic gradient descent or a differentially privatemetric for training models or generators. In this paper, we introduce a noveldifferentially private generative modeling approach based on a gradient flow inthe space of probability measures. To this end, we define the gradient flow ofthe Gaussian-smoothed Sliced Wasserstein Distance, including the associatedstochastic differential equation (SDE). By discretizing and defining anumerical scheme for solving this SDE, we demonstrate the link betweensmoothing and differential privacy based on a Gaussian mechanism, due to aspecific form of the SDE's drift term. We then analyze the differential privacyguarantee of our gradient flow, which accounts for both the smoothing and theWiener process introduced by the SDE itself. Experiments show that our proposedmodel can generate higher-fidelity data at a low privacy budget compared to agenerator-based model, offering a promising alternative.</description><author>Ilana Sebag, Muni Sreenivas Pydi, Jean-Yves Franceschi, Alain Rakotomamonjy, Mike Gartrell, Jamal Atif, Alexandre Allauzen</author><pubDate>Mon, 29 Jul 2024 14:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08227v2</guid></item><item><title>CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification</title><link>http://arxiv.org/abs/2306.04979v3</link><description>Although graph neural networks (GNNs) have achieved impressive achievementsin graph classification, they often need abundant task-specific labels, whichcould be extensively costly to acquire. A credible solution is to exploreadditional labeled graphs to enhance unsupervised learning on the targetdomain. However, how to apply GNNs to domain adaptation remains unsolved owingto the insufficient exploration of graph topology and the significant domaindiscrepancy. In this paper, we propose Coupled Contrastive Graph RepresentationLearning (CoCo), which extracts the topological information from coupledlearning branches and reduces the domain discrepancy with coupled contrastivelearning. CoCo contains a graph convolutional network branch and a hierarchicalgraph kernel network branch, which explore graph topology in implicit andexplicit manners. Besides, we incorporate coupled branches into a holisticmulti-view contrastive learning framework, which not only incorporates graphrepresentations learned from complementary views for enhanced understanding,but also encourages the similarity between cross-domain example pairs with thesame semantics for domain alignment. Extensive experiments on popular datasetsshow that our CoCo outperforms these competing baselines in different settingsgenerally.</description><author>Nan Yin, Li Shen, Mengzhu Wang, Long Lan, Zeyu Ma, Chong Chen, Xian-Sheng Hua, Xiao Luo</author><pubDate>Mon, 29 Jul 2024 14:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04979v3</guid></item><item><title>Geospecific View Generation -- Geometry-Context Aware High-resolution Ground View Inference from Satellite Views</title><link>http://arxiv.org/abs/2407.08061v2</link><description>Predicting realistic ground views from satellite imagery in urban scenes is achallenging task due to the significant view gaps between satellite andground-view images. We propose a novel pipeline to tackle this challenge, bygenerating geospecifc views that maximally respect the weak geometry andtexture from multi-view satellite images. Different from existing approachesthat hallucinate images from cues such as partial semantics or geometry fromoverhead satellite images, our method directly predicts ground-view images atgeolocation by using a comprehensive set of information from the satelliteimage, resulting in ground-level images with a resolution boost at a factor often or more. We leverage a novel building refinement method to reduce geometricdistortions in satellite data at ground level, which ensures the creation ofaccurate conditions for view synthesis using diffusion networks. Moreover, weproposed a novel geospecific prior, which prompts distribution learning ofdiffusion models to respect image samples that are closer to the geolocation ofthe predicted images. We demonstrate our pipeline is the first to generateclose-to-real and geospecific ground views merely based on satellite images.</description><author>Ningli Xu, Rongjun Qin</author><pubDate>Mon, 29 Jul 2024 14:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08061v2</guid></item><item><title>SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation</title><link>http://arxiv.org/abs/2407.20062v1</link><description>Recent advancements in deep convolutional neural networks have significantlyimproved the performance of saliency prediction. However, the manualconfiguration of the neural network architectures requires domain knowledgeexpertise and can still be time-consuming and error-prone. To solve this, wepropose a new Neural Architecture Search (NAS) framework for saliencyprediction with two contributions. Firstly, a supernet for saliency predictionis built with a weight-sharing network containing all candidate architectures,by integrating a dynamic convolution into the encoder-decoder in the supernet,termed SalNAS. Secondly, despite the fact that SalNAS is highly efficient(20.98 million parameters), it can suffer from the lack of generalization. Tosolve this, we propose a self-knowledge distillation approach, termed Self-KD,that trains the student SalNAS with the weighted average information betweenthe ground truth and the prediction from the teacher model. The teacher model,while sharing the same architecture, contains the best-performing weightschosen by cross-validation. Self-KD can generalize well without the need tocompute the gradient in the teacher model, enabling an efficient trainingsystem. By utilizing Self-KD, SalNAS outperforms other state-of-the-artsaliency prediction models in most evaluation rubrics across seven benchmarkdatasets while being a lightweight model. The code will be available athttps://github.com/chakkritte/SalNAS</description><author>Chakkrit Termritthikun, Ayaz Umer, Suwichaya Suwanwimolkul, Feng Xia, Ivan Lee</author><pubDate>Mon, 29 Jul 2024 14:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20062v1</guid></item><item><title>Autonomous Bootstrapping of Quantum Dot Devices</title><link>http://arxiv.org/abs/2407.20061v1</link><description>Semiconductor quantum dots (QD) are a promising platform for multipledifferent qubit implementations, all of which are voltage-controlled byprogrammable gate electrodes. However, as the QD arrays grow in size andcomplexity, tuning procedures that can fully autonomously handle the increasingnumber of control parameters are becoming essential for enabling scalability.We propose a bootstrapping algorithm for initializing a depletion mode QDdevice in preparation for subsequent phases of tuning. During bootstrapping,the QD device functionality is validated, all gates are characterized, and theQD charge sensor is made operational. We demonstrate the bootstrapping protocolin conjunction with a coarse tuning module, showing that the combined algorithmcan efficiently and reliably take a cooled-down QD device to a desired globalstate configuration in under 8 minutes with a success rate of 96 %.Importantly, by following heuristic approaches to QD device initialization andcombining the efficient ray-based measurement with the rapid radio-frequencyreflectometry measurements, the proposed algorithm establishes a reference interms of performance, reliability, and efficiency against which alternativealgorithms can be benchmarked.</description><author>Anton Zubchenko, Danielle Middlebrooks, TorbjÃ¸rn Rasmussen, Lara Lausen, Ferdinand Kuemmeth, Anasua Chatterjee, Justyna P. Zwolak</author><pubDate>Mon, 29 Jul 2024 14:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20061v1</guid></item></channel></rss>