<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 21 Dec 2023 06:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Generative Multimodal Models are In-Context Learners</title><link>http://arxiv.org/abs/2312.13286v1</link><description>The human ability to easily solve multimodal tasks in context (i.e., withonly a few demonstrations or simple instructions), is what current multimodalsystems have largely struggled to imitate. In this work, we demonstrate thatthe task-agnostic in-context learning capabilities of large multimodal modelscan be significantly enhanced by effective scaling-up. We introduce Emu2, agenerative multimodal model with 37 billion parameters, trained on large-scalemultimodal sequences with a unified autoregressive objective. Emu2 exhibitsstrong multimodal in-context learning abilities, even emerging to solve tasksthat require on-the-fly reasoning, such as visual prompting and object-groundedgeneration. The model sets a new record on multiple multimodal understandingtasks in few-shot settings. When instruction-tuned to follow specificinstructions, Emu2 further achieves new state-of-the-art on challenging taskssuch as question answering benchmarks for large multimodal models andopen-ended subject-driven generation. These achievements demonstrate that Emu2can serve as a base model and general-purpose interface for a wide range ofmultimodal tasks. Code and models are publicly available to facilitate futureresearch.</description><author>Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang</author><pubDate>Wed, 20 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13286v1</guid></item><item><title>UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections</title><link>http://arxiv.org/abs/2312.13285v1</link><description>Neural 3D scene representations have shown great potential for 3Dreconstruction from 2D images. However, reconstructing real-world captures ofcomplex scenes still remains a challenge. Existing generic 3D reconstructionmethods often struggle to represent fine geometric details and do notadequately model reflective surfaces of large-scale scenes. Techniques thatexplicitly focus on reflective surfaces can model complex and detailedreflections by exploiting better reflection parameterizations. However, weobserve that these methods are often not robust in real unbounded scenarioswhere non-reflective as well as reflective components are present. In thiswork, we propose UniSDF, a general purpose 3D reconstruction method that canreconstruct large complex scenes with reflections. We investigate bothview-based as well as reflection-based color prediction parameterizationtechniques and find that explicitly blending these representations in 3D spaceenables reconstruction of surfaces that are more geometrically accurate,especially for reflective surfaces. We further combine this representation witha multi-resolution grid backbone that is trained in a coarse-to-fine manner,enabling faster reconstructions than prior methods. Extensive experiments onobject-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF360 and Ref-NeRF real demonstrate that our method is able to robustlyreconstruct complex large-scale scenes with fine details and reflectivesurfaces. Please see our project page athttps://fangjinhuawang.github.io/UniSDF.</description><author>Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard Szeliski, Marc Pollefeys, Federico Tombari</author><pubDate>Wed, 20 Dec 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13285v1</guid></item><item><title>Integrating Human Vision Perception in Vision Transformers for Classifying Waste Items</title><link>http://arxiv.org/abs/2312.12143v2</link><description>In this paper, we propose an novel methodology aimed at simulating thelearning phenomenon of nystagmus through the application of differentialblurring on datasets. Nystagmus is a biological phenomenon that influenceshuman vision throughout life, notably by diminishing head shake from infancy toadulthood. Leveraging this concept, we address the issue of wasteclassification, a pressing global concern. The proposed framework comprises twomodules, with the second module closely resembling the original VisionTransformer, a state-of-the-art model model in classification tasks. Theprimary motivation behind our approach is to enhance the model's precision andadaptability, mirroring the real-world conditions that the human visual systemundergoes. This novel methodology surpasses the standard Vision Transformermodel in waste classification tasks, exhibiting an improvement with a margin of2%. This improvement underscores the potential of our methodology in improvingmodel precision by drawing inspiration from human vision perception. Furtherresearch in the proposed methodology could yield greater performance results,and can be extrapolated to other global issues.</description><author>Akshat Kishore Shrivastava, Tapan Kumar Gandhi</author><pubDate>Wed, 20 Dec 2023 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12143v2</guid></item><item><title>Hard Regularization to Prevent Deep Online Clustering Collapse without Data Augmentation</title><link>http://arxiv.org/abs/2303.16521v2</link><description>Online deep clustering refers to the joint use of a feature extractionnetwork and a clustering model to assign cluster labels to each new data pointor batch as it is processed. While faster and more versatile than offlinemethods, online clustering can easily reach the collapsed solution where theencoder maps all inputs to the same point and all are put into a singlecluster. Successful existing models have employed various techniques to avoidthis problem, most of which require data augmentation or which aim to make theaverage soft assignment across the dataset the same for each cluster. Wepropose a method that does not require data augmentation, and that, differentlyfrom existing methods, regularizes the hard assignments. Using a Bayesianframework, we derive an intuitive optimization objective that can bestraightforwardly included in the training of the encoder network. Tested onfour image datasets and one human-activity recognition dataset, it consistentlyavoids collapse more robustly than other methods and leads to more accurateclustering. We also conduct further experiments and analyses justifying ourchoice to regularize the hard cluster assignments. Code is available athttps://github.com/Lou1sM/online_hard_clustering.</description><author>Louis Mahon, Thomas Lukasiewicz</author><pubDate>Wed, 20 Dec 2023 18:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16521v2</guid></item><item><title>Deep Learning on 3D Neural Fields</title><link>http://arxiv.org/abs/2312.13277v1</link><description>In recent years, Neural Fields (NFs) have emerged as an effective tool forencoding diverse continuous signals such as images, videos, audio, and 3Dshapes. When applied to 3D data, NFs offer a solution to the fragmentation andlimitations associated with prevalent discrete representations. However, giventhat NFs are essentially neural networks, it remains unclear whether and howthey can be seamlessly integrated into deep learning pipelines for solvingdownstream tasks. This paper addresses this research problem and introducesnf2vec, a framework capable of generating a compact latent representation foran input NF in a single inference pass. We demonstrate that nf2vec effectivelyembeds 3D objects represented by the input NFs and showcase how the resultingembeddings can be employed in deep learning pipelines to successfully addressvarious tasks, all while processing exclusively NFs. We test this framework onseveral NFs used to represent 3D surfaces, such as unsigned/signed distance andoccupancy fields. Moreover, we demonstrate the effectiveness of our approachwith more complex NFs that encompass both geometry and appearance of 3D objectssuch as neural radiance fields.</description><author>Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano Cardace, Riccardo Spezialetti, Francesco Ballerini, Samuele Salti, Luigi Di Stefano</author><pubDate>Wed, 20 Dec 2023 18:56:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13277v1</guid></item><item><title>PnP for Two-Dimensional Pose Estimation</title><link>http://arxiv.org/abs/2312.08488v2</link><description>We propose a PnP algorithm for a camera constrained to two-dimensionalmovement (applicable, for instance, to many wheeled robotics platforms).Leveraging this assumption allows performance improvements over 3D PnPalgorithms due to the reduction in search space dimensionality. It also reducesthe incidence of ambiguous pose estimates (as, in most cases, the spurioussolutions fall outside the plane of movement). Our algorithm finds anapproximate solution using geometric criteria and refines its predictioniteratively. We compare this algorithm to existing 3D PnP algorithms in termsof accuracy, performance, and robustness to noise.</description><author>Joshua Wang</author><pubDate>Wed, 20 Dec 2023 18:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08488v2</guid></item><item><title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation</title><link>http://arxiv.org/abs/2305.15296v3</link><description>The recent popularity of text-to-image diffusion models (DM) can largely beattributed to the intuitive interface they provide to users. The intendedgeneration can be expressed in natural language, with the model producingfaithful interpretations of text prompts. However, expressing complex ornuanced ideas in text alone can be difficult. To ease image generation, wepropose MultiFusion that allows one to express complex and nuanced conceptswith arbitrarily interleaved inputs of multiple modalities and languages.MutliFusion leverages pre-trained models and aligns them for integration into acohesive system, thereby avoiding the need for extensive training from scratch.Our experimental results demonstrate the efficient transfer of capabilitiesfrom individual modules to the downstream model. Specifically, the fusion ofall independent components allows the image generation module to utilizemultilingual, interleaved multimodal inputs despite being trained solely onmonomodal data in a single language.</description><author>Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, Samuel Weinbach</author><pubDate>Wed, 20 Dec 2023 18:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15296v3</guid></item><item><title>Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting</title><link>http://arxiv.org/abs/2312.13271v1</link><description>Recent one image to 3D generation methods commonly adopt Score DistillationSampling (SDS). Despite the impressive results, there are multiple deficienciesincluding multi-view inconsistency, over-saturated and over-smoothed textures,as well as the slow generation speed. To address these deficiencies, we presentRepaint123 to alleviate multi-view bias as well as texture degradation andspeed up the generation process. The core idea is to combine the powerful imagegeneration capability of the 2D diffusion model and the texture alignmentability of the repainting strategy for generating high-quality multi-viewimages with consistency. We further propose visibility-aware adaptiverepainting strength for overlap regions to enhance the generated image qualityin the repainting process. The generated high-quality and multi-view consistentimages enable the use of simple Mean Square Error (MSE) loss for fast 3Dcontent generation. We conduct extensive experiments and show that our methodhas a superior ability to generate high-quality 3D content with multi-viewconsistency and fine textures in 2 minutes from scratch. Code is athttps://github.com/junwuzhang19/repaint123.</description><author>Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Wangbo Yu, Munan Ning, Li Yuan</author><pubDate>Wed, 20 Dec 2023 18:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13271v1</guid></item><item><title>ClassLIE: Structure- and Illumination-Adaptive Classification for Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2312.13265v1</link><description>Low-light images often suffer from limited visibility and multiple types ofdegradation, rendering low-light image enhancement (LIE) a non-trivial task.Some endeavors have been recently made to enhance low-light images usingconvolutional neural networks (CNNs). However, they have low efficiency inlearning the structural information and diverse illumination levels at thelocal regions of an image. Consequently, the enhanced results are affected byunexpected artifacts, such as unbalanced exposure, blur, and color bias. Tothis end, this paper proposes a novel framework, called ClassLIE, that combinesthe potential of CNNs and transformers. It classifies and adaptively learns thestructural and illumination information from the low-light images in a holisticand regional manner, thus showing better enhancement performance. Our frameworkfirst employs a structure and illumination classification (SIC) module to learnthe degradation information adaptively. In SIC, we decompose an input imageinto an illumination map and a reflectance map. A class prediction block isthen designed to classify the degradation information by calculating thestructure similarity scores on the reflectance map and mean square error on theillumination map. As such, each input image can be divided into patches withthree enhancement difficulty levels. Then, a feature learning and fusion (FLF)module is proposed to adaptively learn the feature information with CNNs fordifferent enhancement difficulty levels while learning the long-rangedependencies for the patches in a holistic manner. Experiments on fivebenchmark datasets consistently show our ClassLIE achieves new state-of-the-artperformance, with 25.74 PSNR and 0.92 SSIM on the LOL dataset.</description><author>Zixiang Wei, Yiting Wang, Lichao Sun, Athanasios V. Vasilakos, Lin Wang</author><pubDate>Wed, 20 Dec 2023 18:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13265v1</guid></item><item><title>dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models</title><link>http://arxiv.org/abs/2312.13264v1</link><description>Data is stored in both structured and unstructured form. Querying both, topower natural language conversations, is a challenge. This paper introducesdIR, Discrete Information Retrieval, providing a unified interface to queryboth free text and structured knowledge. Specifically, a Large Language Model(LLM) transforms text into expressive representation. After the text isextracted into columnar form, it can then be queried via a text-to-SQL SemanticParser, with an LLM converting natural language into SQL. Where desired, suchconversation may be effected by a multi-step reasoning conversational agent. Wevalidate our approach via a proprietary question/answer data set, concludingthat dIR makes a whole new class of queries on free text possible when comparedto traditionally fine-tuned dense-embedding-model-based Information Retrieval(IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIRcan succeed where no other method stands a chance.</description><author>Pablo M. Rodriguez Bertorello, Jean Rodmond Junior Laguerre</author><pubDate>Wed, 20 Dec 2023 18:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13264v1</guid></item><item><title>A note on regularised NTK dynamics with an application to PAC-Bayesian training</title><link>http://arxiv.org/abs/2312.13259v1</link><description>We establish explicit dynamics for neural networks whose training objectivehas a regularising term that constrains the parameters to remain close to theirinitial value. This keeps the network in a lazy training regime, where thedynamics can be linearised around the initialisation. The standard neuraltangent kernel (NTK) governs the evolution during the training in theinfinite-width limit, although the regularisation yields an additional termappears in the differential equation describing the dynamics. This settingprovides an appropriate framework to study the evolution of wide networkstrained to optimise generalisation objectives such as PAC-Bayes bounds, andhence potentially contribute to a deeper theoretical understanding of suchnetworks.</description><author>Eugenio Clerico, Benjamin Guedj</author><pubDate>Wed, 20 Dec 2023 18:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13259v1</guid></item><item><title>Conditional Image Generation with Pretrained Generative Model</title><link>http://arxiv.org/abs/2312.13253v1</link><description>In recent years, diffusion models have gained popularity for their ability togenerate higher-quality images in comparison to GAN models. However, like anyother large generative models, these models require a huge amount of data,computational resources, and meticulous tuning for successful training. Thisposes a significant challenge, rendering it infeasible for most individuals. Asa result, the research community has devised methods to leverage pre-trainedunconditional diffusion models with additional guidance for the purpose ofconditional image generative. These methods enable conditional imagegenerations on diverse inputs and, most importantly, circumvent the need fortraining the diffusion model. In this paper, our objective is to reduce thetime-required and computational overhead introduced by the addition of guidancein diffusion models -- while maintaining comparable image quality. We propose aset of methods based on our empirical analysis, demonstrating a reduction incomputation time by approximately threefold.</description><author>Rajesh Shrestha, Bowen Xie</author><pubDate>Wed, 20 Dec 2023 18:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13253v1</guid></item><item><title>Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model</title><link>http://arxiv.org/abs/2312.13252v1</link><description>While methods for monocular depth estimation have made significant strides onstandard benchmarks, zero-shot metric depth estimation remains unsolved.Challenges include the joint modeling of indoor and outdoor scenes, which oftenexhibit significantly different distributions of RGB and depth, and thedepth-scale ambiguity due to unknown camera intrinsics. Recent work hasproposed specialized multi-head architectures for jointly modeling indoor andoutdoor scenes. In contrast, we advocate a generic, task-agnostic diffusionmodel, with several advancements such as log-scale depth parameterization toenable joint modeling of indoor and outdoor scenes, conditioning on thefield-of-view (FOV) to handle scale ambiguity and synthetically augmenting FOVduring training to generalize beyond the limited camera intrinsics in trainingdatasets. Furthermore, by employing a more diverse training mixture than iscommon, and an efficient diffusion parameterization, our method, DMD (Diffusionfor Metric Depth) achieves a 25\% reduction in relative error (REL) onzero-shot indoor and 33\% reduction on zero-shot outdoor datasets over thecurrent SOTA using only a small number of denoising steps. For an overview seehttps://diffusion-vision.github.io/dmd</description><author>Saurabh Saxena, Junhwa Hur, Charles Herrmann, Deqing Sun, David J. Fleet</author><pubDate>Wed, 20 Dec 2023 18:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13252v1</guid></item><item><title>The role of data embedding in equivariant quantum convolutional neural networks</title><link>http://arxiv.org/abs/2312.13250v1</link><description>Geometric deep learning refers to the scenario in which the symmetries of adataset are used to constrain the parameter space of a neural network and thus,improve their trainability and generalization. Recently this idea has beenincorporated into the field of quantum machine learning, which has given riseto equivariant quantum neural networks (EQNNs). In this work, we investigatethe role of classical-to-quantum embedding on the performance of equivariantquantum convolutional neural networks (EQCNNs) for the classification ofimages. We discuss the connection between the data embedding method and theresulting representation of a symmetry group and analyze how changingrepresentation affects the expressibility of an EQCNN. We numerically comparethe classification accuracy of EQCNNs with three different basis-permutedamplitude embeddings to the one obtained from a non-equivariant quantumconvolutional neural network (QCNN). Our results show that all the EQCNNsachieve higher classification accuracy than the non-equivariant QCNN for smallnumbers of training iterations, while for large iterations this improvementcrucially depends on the used embedding. It is expected that the results ofthis work can be useful to the community for a better understanding of theimportance of data embedding choice in the context of geometric quantum machinelearning.</description><author>Sreetama Das, Stefano Martina, Filippo Caruso</author><pubDate>Wed, 20 Dec 2023 18:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13250v1</guid></item><item><title>Enhancing Neural Training via a Correlated Dynamics Model</title><link>http://arxiv.org/abs/2312.13247v1</link><description>As neural networks grow in scale, their training becomes both computationallydemanding and rich in dynamics. Amidst the flourishing interest in thesetraining dynamics, we present a novel observation: Parameters during trainingexhibit intrinsic correlations over time. Capitalizing on this, we introduceCorrelation Mode Decomposition (CMD). This algorithm clusters the parameterspace into groups, termed modes, that display synchronized behavior acrossepochs. This enables CMD to efficiently represent the training dynamics ofcomplex networks, like ResNets and Transformers, using only a few modes.Moreover, test set generalization is enhanced. We introduce an efficient CMDvariant, designed to run concurrently with training. Our experiments indicatethat CMD surpasses the state-of-the-art method for compactly modeled dynamicson image classification. Our modeling can improve training efficiency and lowercommunication overhead, as shown by our preliminary experiments in the contextof federated learning.</description><author>Jonathan Brokman, Roy Betser, Rotem Turjeman, Tom Berkov, Ido Cohen, Guy Gilboa</author><pubDate>Wed, 20 Dec 2023 18:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13247v1</guid></item><item><title>Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</title><link>http://arxiv.org/abs/2310.07811v2</link><description>We consider online reinforcement learning (RL) in episodic Markov decisionprocesses (MDPs) under the linear $q^\pi$-realizability assumption, where it isassumed that the action-values of all policies can be expressed as linearfunctions of state-action features. This class is known to be more general thanlinear MDPs, where the transition kernel and the reward function are assumed tobe linear functions of the feature vectors. As our first contribution, we showthat the difference between the two classes is the presence of states inlinearly $q^\pi$-realizable MDPs where for any policy, all the actions haveapproximately equal values, and skipping over these states by following anarbitrarily fixed policy in those states transforms the problem to a linearMDP. Based on this observation, we derive a novel (computationally inefficient)learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneouslylearns what states should be skipped over and runs another learning algorithmon the linear MDP hidden in the problem. The method returns an$\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactionswith the MDP, where $H$ is the time horizon and $d$ is the dimension of thefeature vectors, giving the first polynomial-sample-complexity online RLalgorithm for this setting. The results are proved for the misspecified case,where the sample complexity is shown to degrade gracefully with themisspecification error.</description><author>Gellért Weisz, András György, Csaba Szepesvári</author><pubDate>Wed, 20 Dec 2023 18:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07811v2</guid></item><item><title>Efficient Verification-Based Face Identification</title><link>http://arxiv.org/abs/2312.13240v1</link><description>We study the problem of performing face verification with an efficient neuralmodel $f$. The efficiency of $f$ stems from simplifying the face verificationproblem from an embedding nearest neighbor search into a binary problem; eachuser has its own neural network $f$. To allow information sharing betweendifferent individuals in the training set, we do not train $f$ directly butinstead generate the model weights using a hypernetwork $h$. This leads to thegeneration of a compact personalized model for face identification that can bedeployed on edge devices. Key to the method's success is a novel way ofgenerating hard negatives and carefully scheduling the training objectives. Ourmodel leads to a substantially small $f$ requiring only 23k parameters and 5Mfloating point operations (FLOPS). We use six face verification datasets todemonstrate that our method is on par or better than state-of-the-art models,with a significantly reduced number of parameters and computational burden.Furthermore, we perform an extensive ablation study to demonstrate theimportance of each element in our method.</description><author>Amit Rozner, Barak Battash, Ofir Lindenbaum, Lior Wolf</author><pubDate>Wed, 20 Dec 2023 18:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13240v1</guid></item><item><title>Diffusion Models With Learned Adaptive Noise</title><link>http://arxiv.org/abs/2312.13236v1</link><description>Diffusion models have gained traction as powerful algorithms for synthesizinghigh-quality images. Central to these algorithms is the diffusion process,which maps data to noise according to equations inspired by thermodynamics andcan significantly impact performance. A widely held assumption is that the ELBOobjective of a diffusion model is invariant to the noise process (Kingma etal.,2021). In this work, we dispel this assumption -- we propose multivariatelearned adaptive noise (MuLAN), a learned diffusion process that appliesGaussian noise at different rates across an image. Our method consists of threecomponents -- a multivariate noise schedule, instance-conditional diffusion,and auxiliary variables -- which ensure that the learning objective is nolonger invariant to the choice of the noise schedule as in previous works. Ourwork is grounded in Bayesian inference and casts the learned diffusion processas an approximate variational posterior that yields a tighter lower bound onmarginal likelihood. Empirically, MuLAN sets a new state-of-the-art in densityestimation on CIFAR-10 and ImageNet compared to classical diffusion. Code isavailable at https://github.com/s-sahoo/MuLAN</description><author>Subham Sekhar Sahoo, Aaron Gokaslan, Chris De Sa, Volodymyr Kuleshov</author><pubDate>Wed, 20 Dec 2023 18:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13236v1</guid></item><item><title>Position Paper: Bridging the Gap Between Machine Learning and Sensitivity Analysis</title><link>http://arxiv.org/abs/2312.13234v1</link><description>We argue that interpretations of machine learning (ML) models or themodel-building process can bee seen as a form of sensitivity analysis (SA), ageneral methodology used to explain complex systems in many fields such asenvironmental modeling, engineering, or economics. We address both researchersand practitioners, calling attention to the benefits of a unified SA-based viewof explanations in ML and the necessity to fully credit related work. We bridgethe gap between both fields by formally describing how (a) the ML process is asystem suitable for SA, (b) how existing ML interpretation methods relate tothis perspective, and (c) how other SA techniques could be applied to ML.</description><author>Christian A. Scholbeck, Julia Moosbauer, Giuseppe Casalicchio, Hoshin Gupta, Bernd Bischl, Christian Heumann</author><pubDate>Wed, 20 Dec 2023 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13234v1</guid></item><item><title>Featurizing Koopman Mode Decomposition</title><link>http://arxiv.org/abs/2312.09146v2</link><description>This article introduces an advanced Koopman mode decomposition (KMD)technique -- coined Featurized Koopman Mode Decomposition (FKMD) -- that usestime embedding and Mahalanobis scaling to enhance analysis and prediction ofhigh dimensional dynamical systems. The time embedding expands the observationspace to better capture underlying manifold structure, while the Mahalanobisscaling, applied to kernel or random Fourier features, adjusts observationsbased on the system's dynamics. This aids in featurizing KMD in cases wheregood features are not a priori known. We show that our method improves KMDpredictions for a high dimensional Lorenz attractor and for a cell signalingproblem from cancer research.</description><author>David Aristoff, Jeremy Copperman, Nathan Mankovich, Alexander Davies</author><pubDate>Wed, 20 Dec 2023 17:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09146v2</guid></item><item><title>StableKD: Breaking Inter-block Optimization Entanglement for Stable Knowledge Distillation</title><link>http://arxiv.org/abs/2312.13223v1</link><description>Knowledge distillation (KD) has been recognized as an effective tool tocompress and accelerate models. However, current KD approaches generally sufferfrom an accuracy drop and/or an excruciatingly long distillation process. Inthis paper, we tackle the issue by first providing a new insight into aphenomenon that we call the Inter-Block Optimization Entanglement (IBOE), whichmakes the conventional end-to-end KD approaches unstable with noisy gradients.We then propose StableKD, a novel KD framework that breaks the IBOE andachieves more stable optimization. StableKD distinguishes itself through twooperations: Decomposition and Recomposition, where the former divides a pair ofteacher and student networks into several blocks for separate distillation, andthe latter progressively merges them back, evolving towards end-to-enddistillation. We conduct extensive experiments on CIFAR100, Imagewoof, andImageNet datasets with various teacher-student pairs. Compared to other KDapproaches, our simple yet effective StableKD greatly boosts the model accuracyby 1% ~ 18%, speeds up the convergence up to 10 times, and outperforms themwith only 40% of the training data.</description><author>Shiu-hong Kao, Jierun Chen, S. H. Gary Chan</author><pubDate>Wed, 20 Dec 2023 17:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13223v1</guid></item><item><title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title><link>http://arxiv.org/abs/2311.16984v2</link><description>External control arms (ECA) can inform the early clinical development ofexperimental drugs and provide efficacy evidence for regulatory approval innon-randomized settings. However, the main challenge of implementing ECA liesin accessing real-world data or historical clinical trials. Indeed, datasharing is often not feasible due to privacy considerations related to dataleaving the original collection centers, along with pharmaceutical companies'competitive motives. In this paper, we leverage a privacy-enhancing technologycalled federated learning (FL) to remove some of the barriers to data sharing.We introduce a federated learning inverse probability of treatment weighted(IPTW) method for time-to-event outcomes called FedECA which eases theimplementation of ECA by limiting patients' data exposure. We show withextensive experiments that FedECA outperforms its closest competitor,matching-adjusted indirect comparison (MAIC), in terms of statistical power andability to balance the treatment and control groups. To encourage the use ofsuch methods, we publicly release our code which relies on Substra, anopen-source FL software with proven experience in privacy-sensitive contexts.</description><author>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Félix Balazard, Mathieu Andreux</author><pubDate>Wed, 20 Dec 2023 17:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16984v2</guid></item><item><title>Founder-GPT: Self-play to evaluate the Founder-Idea fit</title><link>http://arxiv.org/abs/2312.12037v2</link><description>This research introduces an innovative evaluation method for the"founder-idea" fit in early-stage startups, utilizing advanced large languagemodel techniques to assess founders' profiles against their startup ideas toenhance decision-making. Embeddings, self-play, tree-of-thought, andcritique-based refinement techniques show early promising results that eachidea's success patterns are unique and they should be evaluated based on thecontext of the founder's background.</description><author>Sichao Xiong, Yigit Ihlamur</author><pubDate>Wed, 20 Dec 2023 17:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12037v2</guid></item><item><title>Latency Adjustable Transformer Encoder for Language Understanding</title><link>http://arxiv.org/abs/2201.03327v7</link><description>Adjusting the latency, power, and accuracy of natural language understandingmodels is a desirable objective of an efficient architecture. This paperproposes an efficient Transformer architecture that adjusts the inferencecomputational cost adaptively with a desired inference latency speedup. Infine-tuning phase, the proposed method detects less important hidden sequenceelements (word-vectors) and eliminates them in each encoder layer using aproposed Attention Context Contribution (ACC) metric. After the fine-tuningphase, with the novel offline-tuning property, the inference latency of themodel can be adjusted in a wide range of inference speedup selections withoutany further training. The proposed method is applied to the BERT-base and GPT-2models for evaluation. Extensive experiments show that most of the word-vectorsin higher Transformer layers have less contribution to the subsequent layers;hence, they can be eliminated to improve the inference latency. Experimentalresults on extensive sentiment analysis, classification, text generation tasksand regression benchmarks like GLUE showed that the method is effective invarious datasets with minimal impact on global context. The proposed methodmathematically and experimentally improves the inference latency of BERT-baseand GPT-2 by up to 4.8 and 3.72 times with less than 0.75% accuracy drop andpassable perplexity on average. The suggested approach posits that in LargeLanguage Models (LLMs), although the complete network is necessary fortraining, it can be truncated during the fine-tuning phase.</description><author>Sajjad Kachuee, Mohammad Sharifkhani</author><pubDate>Wed, 20 Dec 2023 17:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.03327v7</guid></item><item><title>SISMIK for brain MRI: Deep-learning-based motion estimation and model-based motion correction in k-space</title><link>http://arxiv.org/abs/2312.13220v1</link><description>MRI, a widespread non-invasive medical imaging modality, is highly sensitiveto patient motion. Despite many attempts over the years, motion correctionremains a difficult problem and there is no general method applicable to allsituations. We propose a retrospective method for motion quantification andcorrection to tackle the problem of in-plane rigid-body motion, apt forclassical 2D Spin-Echo scans of the brain, which are regularly used in clinicalpractice. Due to the sequential acquisition of k-space, motion artifacts arewell localized. The method leverages the power of deep neural networks toestimate motion parameters in k-space and uses a model-based approach torestore degraded images to avoid ''hallucinations''. Notable advantages are itsability to estimate motion occurring in high spatial frequencies without theneed of a motion-free reference. The proposed method operates on the wholek-space dynamic range and is moderately affected by the lower SNR of higherharmonics. As a proof of concept, we provide models trained using supervisedlearning on 600k motion simulations based on motion-free scans of 43 differentsubjects. Generalization performance was tested with simulations as well asin-vivo. Qualitative and quantitative evaluations are presented for motionparameter estimations and image reconstruction. Experimental results show thatour approach is able to obtain good generalization performance on simulateddata and in-vivo acquisitions.</description><author>Oscar Dabrowski, Jean-Luc Falcone, Antoine Klauser, Julien Songeon, Michel Kocher, Bastien Chopard, François Lazeyras, Sébastien Courvoisier</author><pubDate>Wed, 20 Dec 2023 17:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13220v1</guid></item><item><title>Interactive Visual Task Learning for Robots</title><link>http://arxiv.org/abs/2312.13219v1</link><description>We present a framework for robots to learn novel visual concepts and tasksvia in-situ linguistic interactions with human users. Previous approaches haveeither used large pre-trained visual models to infer novel objects zero-shot,or added novel concepts along with their attributes and representations to aconcept hierarchy. We extend the approaches that focus on learning visualconcept hierarchies by enabling them to learn novel concepts and solve unseenrobotics tasks with them. To enable a visual concept learner to solve roboticstasks one-shot, we developed two distinct techniques. Firstly, we propose anovel approach, Hi-Viscont(HIerarchical VISual CONcept learner for Task), whichaugments information of a novel concept to its parent nodes within a concepthierarchy. This information propagation allows all concepts in a hierarchy toupdate as novel concepts are taught in a continual learning setting. Secondly,we represent a visual task as a scene graph with language annotations, allowingus to create novel permutations of a demonstrated task zero-shot in-situ. Wepresent two sets of results. Firstly, we compare Hi-Viscont with the baselinemodel (FALCON) on visual question answering(VQA) in three domains. While beingcomparable to the baseline model on leaf level concepts, Hi-Viscont achieves animprovement of over 9% on non-leaf concepts on average. We compare our model'sperformance against the baseline FALCON model. Our framework achieves 33%improvements in success rate metric, and 19% improvements in the object levelaccuracy compared to the baseline model. With both of these results wedemonstrate the ability of our model to learn tasks and concepts in a continuallearning setting on the robot.</description><author>Weiwei Gu, Anant Sah, Nakul Gopalan</author><pubDate>Wed, 20 Dec 2023 17:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13219v1</guid></item><item><title>FiFAR: A Fraud Detection Dataset for Learning to Defer</title><link>http://arxiv.org/abs/2312.13218v1</link><description>Public dataset limitations have significantly hindered the development andbenchmarking of learning to defer (L2D) algorithms, which aim to optimallycombine human and AI capabilities in hybrid decision-making systems. In suchsystems, human availability and domain-specific concerns introducedifficulties, while obtaining human predictions for training and evaluation iscostly. Financial fraud detection is a high-stakes setting where algorithms andhuman experts often work in tandem; however, there are no publicly availabledatasets for L2D concerning this important application of human-AI teaming. Tofill this gap in L2D research, we introduce the Financial Fraud Alert ReviewDataset (FiFAR), a synthetic bank account fraud detection dataset, containingthe predictions of a team of 50 highly complex and varied synthetic fraudanalysts, with varied bias and feature dependence. We also provide a realisticdefinition of human work capacity constraints, an aspect of L2D systems that isoften overlooked, allowing for extensive testing of assignment systems underreal-world conditions. We use our dataset to develop a capacity-aware L2Dmethod and rejection learning approach under realistic data availabilityconditions, and benchmark these baselines under an array of 300 distincttesting scenarios. We believe that this dataset will serve as a pivotalinstrument in facilitating a systematic, rigorous, reproducible, andtransparent evaluation and comparison of L2D methods, thereby fostering thedevelopment of more synergistic human-AI collaboration in decision-makingsystems. The public dataset and detailed synthetic expert information areavailable at: https://github.com/feedzai/fifar-dataset</description><author>Jean V. Alves, Diogo Leitão, Sérgio Jesus, Marco O. P. Sampaio, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro</author><pubDate>Wed, 20 Dec 2023 17:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13218v1</guid></item><item><title>Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps</title><link>http://arxiv.org/abs/2312.13216v1</link><description>Recent progress in self-supervised representation learning has resulted inmodels that are capable of extracting image features that are not onlyeffective at encoding image level, but also pixel-level, semantics. Thesefeatures have been shown to be effective for dense visual semanticcorrespondence estimation, even outperforming fully-supervised methods.Nevertheless, current self-supervised approaches still fail in the presence ofchallenging image characteristics such as symmetries and repeated parts. Toaddress these limitations, we propose a new approach for semanticcorrespondence estimation that supplements discriminative self-supervisedfeatures with 3D understanding via a weak geometric spherical prior. Comparedto more involved 3D pipelines, our model only requires weak viewpointinformation, and the simplicity of our spherical representation enables us toinject informative geometric priors into the model during training. We proposea new evaluation metric that better accounts for repeated part andsymmetry-induced mistakes. We present results on the challenging SPair-71kdataset, where we show that our approach demonstrates is capable ofdistinguishing between symmetric views and repeated parts across many objectcategories, and also demonstrate that we can generalize to unseen classes onthe AwA dataset.</description><author>Octave Mariotti, Oisin Mac Aodha, Hakan Bilen</author><pubDate>Wed, 20 Dec 2023 17:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13216v1</guid></item><item><title>A 3D super-resolution of wind fields via physics-informed pixel-wise self-attention generative adversarial network</title><link>http://arxiv.org/abs/2312.13212v1</link><description>To mitigate global warming, greenhouse gas sources need to be resolved at ahigh spatial resolution and monitored in time to ensure the reduction andultimately elimination of the pollution source. However, the complexity ofcomputation in resolving high-resolution wind fields left the simulationsimpractical to test different time lengths and model configurations. This studypresents a preliminary development of a physics-informed super-resolution (SR)generative adversarial network (GAN) that super-resolves the three-dimensional(3D) low-resolution wind fields by upscaling x9 times. We develop a pixel-wiseself-attention (PWA) module that learns 3D weather dynamics via aself-attention computation followed by a 2D convolution. We also employ a lossterm that regularizes the self-attention map during pretraining, capturing thevertical convection process from input wind data. The new PWA SR-GAN shows thehigh-fidelity super-resolved 3D wind data, learns a wind structure at thehigh-frequency domain, and reduces the computational cost of a high-resolutionwind simulation by x89.7 times.</description><author>Takuya Kurihana, Kyongmin Yeo, Daniela Szwarcman, Bruce Elmegreen, Karthik Mukkavilli, Johannes Schmude, Levente Klein</author><pubDate>Wed, 20 Dec 2023 17:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13212v1</guid></item><item><title>DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization</title><link>http://arxiv.org/abs/2312.13211v1</link><description>With the tremendous success of large transformer models in natural languageunderstanding, down-sizing them for cost-effective deployments has becomecritical. Recent studies have explored the low-rank weight factorizationtechniques which are efficient to train, and apply out-of-the-box to anytransformer architecture. Unfortunately, the low-rank assumption tends to beover-restrictive and hinders the expressiveness of the compressed model. Thispaper proposes, DSFormer, a simple alternative factorization scheme whichexpresses a target weight matrix as the product of a small dense and asemi-structured sparse matrix. The resulting approximation is more faithful tothe weight distribution in transformers and therefore achieves a strongerefficiency-accuracy trade-off. Another concern with existing factorizers istheir dependence on a task-unaware initialization step which degrades theaccuracy of the resulting model. DSFormer addresses this issue through a novelStraight-Through Factorizer (STF) algorithm that jointly learns all the weightfactorizations to directly maximize the final task accuracy. Extensiveexperiments on multiple natural language understanding benchmarks demonstratethat DSFormer obtains up to 40% better compression than the state-of-the-artlow-rank factorizers, leading semi-structured sparsity baselines and popularknowledge distillation approaches. Our approach is also orthogonal tomainstream compressors and offers up to 50% additional compression when addedto popular distilled, layer-shared and quantized transformers. We empiricallyevaluate the benefits of STF over conventional optimization practices.</description><author>Rahul Chand, Yashoteja Prabhu, Pratyush Kumar</author><pubDate>Wed, 20 Dec 2023 17:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13211v1</guid></item><item><title>LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces</title><link>http://arxiv.org/abs/2312.13208v1</link><description>Deep generative neural networks, such as Variational AutoEncoders (VAEs),offer an opportunity to better understand and control language models from theperspective of sentence-level latent spaces. To combine the controllability ofVAE latent spaces with the state-of-the-art performance of recent largelanguage models (LLMs), we present in this work LlaMaVAE, which combinesexpressive encoder and decoder models (sentenceT5 and LlaMA) with a VAEarchitecture, aiming to provide better text generation control to LLMs. Inaddition, to conditionally guide the VAE generation, we investigate a newapproach based on flow-based invertible neural networks (INNs) named InvertibleCVAE. Experimental results reveal that LlaMaVAE can outperform the previousstate-of-the-art VAE language model, Optimus, across various tasks, includinglanguage modelling, semantic textual similarity and definition modelling.Qualitative analysis on interpolation and traversal experiments also indicatesan increased degree of semantic clustering and geometric consistency, whichenables better generation control.</description><author>Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, André Freitas</author><pubDate>Wed, 20 Dec 2023 17:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13208v1</guid></item><item><title>Iterative Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2210.03087v2</link><description>We present Iterative Vision-and-Language Navigation (IVLN), a paradigm forevaluating language-guided agents navigating in a persistent environment overtime. Existing Vision-and-Language Navigation (VLN) benchmarks erase theagent's memory at the beginning of every episode, testing the ability toperform cold-start navigation with no prior information. However, deployedrobots occupy the same environment for long periods of time. The IVLN paradigmaddresses this disparity by training and evaluating VLN agents that maintainmemory across tours of scenes that consist of up to 100 orderedinstruction-following Room-to-Room (R2R) episodes, each defined by anindividual language instruction and a target path. We present discrete andcontinuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tourseach in 80 indoor scenes. We find that extending the implicit memory ofhigh-performing transformer VLN agents is not sufficient for IVLN, but agentsthat build maps can benefit from environment persistence, motivating a renewedfocus on map-building agents in VLN.</description><author>Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso, Peter Anderson, Stefan Lee, Jesse Thomason</author><pubDate>Wed, 20 Dec 2023 17:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03087v2</guid></item><item><title>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages</title><link>http://arxiv.org/abs/2305.16307v3</link><description>India has a rich linguistic landscape with languages from 4 major languagefamilies spoken by over a billion people. 22 of these languages are listed inthe Constitution of India (referred to as scheduled languages) are the focus ofthis work. Given the linguistic diversity, high-quality and accessible MachineTranslation (MT) systems are essential in a country like India. Prior to thiswork, there was (i) no parallel training data spanning all 22 languages, (ii)no robust benchmarks covering all these languages and containing contentrelevant to India, and (iii) no existing translation models which support allthe 22 scheduled languages of India. In this work, we aim to address this gapby focusing on the missing pieces required for enabling wide, easy, and openaccess to good machine translation systems for all 22 scheduled Indianlanguages. We identify four key areas of improvement: curating and creatinglarger training datasets, creating diverse and high-quality benchmarks,training multilingual models, and releasing models with open access. Our firstcontribution is the release of the Bharat Parallel Corpus Collection (BPCC),the largest publicly available parallel corpora for Indic languages. BPCCcontains a total of 230M bitext pairs, of which a total of 126M were newlyadded, including 644K manually translated sentence pairs created as part ofthis work. Our second contribution is the release of the first n-way parallelbenchmark covering all 22 Indian languages, featuring diverse domains,Indian-origin content, and source-original test sets. Next, we presentIndicTrans2, the first model to support all 22 languages, surpassing existingmodels on multiple existing and new benchmarks created as a part of this work.Lastly, to promote accessibility and collaboration, we release our models andassociated data with permissive licenses athttps://github.com/AI4Bharat/IndicTrans2.</description><author>Jay Gala, Pranjal A. Chitale, Raghavan AK, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan</author><pubDate>Wed, 20 Dec 2023 17:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16307v3</guid></item><item><title>HCDIR: End-to-end Hate Context Detection, and Intensity Reduction model for online comments</title><link>http://arxiv.org/abs/2312.13193v1</link><description>Warning: This paper contains examples of the language that some people mayfind offensive. Detecting and reducing hateful, abusive, offensive comments is a critical andchallenging task on social media. Moreover, few studies aim to mitigate theintensity of hate speech. While studies have shown that context-level semanticsare crucial for detecting hateful comments, most of this research focuses onEnglish due to the ample datasets available. In contrast, low-resourcelanguages, like Indian languages, remain under-researched because of limiteddatasets. Contrary to hate speech detection, hate intensity reduction remainsunexplored in high-resource and low-resource languages. In this paper, wepropose a novel end-to-end model, HCDIR, for Hate Context Detection, and HateIntensity Reduction in social media posts. First, we fine-tuned severalpre-trained language models to detect hateful comments to ascertain thebest-performing hateful comments detection model. Then, we identified thecontextual hateful words. Identification of such hateful words is justifiedthrough the state-of-the-art explainable learning model, i.e., IntegratedGradient (IG). Lastly, the Masked Language Modeling (MLM) model has beenemployed to capture domain-specific nuances to reduce hate intensity. We maskedthe 50\% hateful words of the comments identified as hateful and predicted thealternative words for these masked terms to generate convincing sentences. Anoptimal replacement for the original hate comments from the feasible sentencesis preferred. Extensive experiments have been conducted on several recentdatasets using automatic metric-based evaluation (BERTScore) and thorough humanevaluation. To enhance the faithfulness in human evaluation, we arranged agroup of three human annotators with varied expertise.</description><author>Neeraj Kumar Singh, Koyel Ghosh, Joy Mahapatra, Utpal Garain, Apurbalal Senapati</author><pubDate>Wed, 20 Dec 2023 17:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13193v1</guid></item><item><title>Self Contrastive Learning for Session-based Recommendation</title><link>http://arxiv.org/abs/2306.01266v2</link><description>Session-based recommendation, which aims to predict the next item of users'interest as per an existing sequence interaction of items, has attractedgrowing applications of Contrastive Learning (CL) with improved user and itemrepresentations. However, these contrastive objectives: (1) serve a similarrole as the cross-entropy loss while ignoring the item representation spaceoptimisation; and (2) commonly require complicated modelling, including complexpositive/negative sample constructions and extra data augmentation. In thiswork, we introduce Self-Contrastive Learning (SCL), which simplifies theapplication of CL and enhances the performance of state-of-the-art CL-basedrecommendation techniques. Specifically, SCL is formulated as an objectivefunction that directly promotes a uniform distribution among itemrepresentations and efficiently replaces all the existing contrastive objectivecomponents of state-of-the-art models. Unlike previous works, SCL eliminatesthe need for any positive/negative sample construction or data augmentation,leading to enhanced interpretability of the item representation space andfacilitating its extensibility to existing recommender systems. Throughexperiments on three benchmark datasets, we demonstrate that SCL consistentlyimproves the performance of state-of-the-art models with statisticalsignificance. Notably, our experiments show that SCL improves the performanceof two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.Additionally, our analysis elucidates the improvement in terms of alignment anduniformity of representations, as well as the effectiveness of SCL with a lowcomputational cost.</description><author>Zhengxiang Shi, Xi Wang, Aldo Lipani</author><pubDate>Wed, 20 Dec 2023 17:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01266v2</guid></item><item><title>Measurement-based quantum computation from Clifford quantum cellular automata</title><link>http://arxiv.org/abs/2312.13185v1</link><description>Measurement-based quantum computation (MBQC) is a paradigm for quantumcomputation where computation is driven by local measurements on a suitablyentangled resource state. In this work we show that MBQC is related to a modelof quantum computation based on Clifford quantum cellular automata (CQCA).Specifically, we show that certain MBQCs can be directly constructed from CQCAswhich yields a simple and intuitive circuit model representation of MBQC interms of quantum computation based on CQCA. We apply this description toconstruct various MBQC-based Ans\"atze for parameterized quantum circuits,demonstrating that the different Ans\"atze may lead to significantly differentperformances on different learning tasks. In this way, MBQC yields a family ofHardware-efficient Ans\"atze that may be adapted to specific problem settingsand is particularly well suited for architectures with translationallyinvariant gates such as neutral atoms.</description><author>Hendrik Poulsen Nautrup, Hans J. Briegel</author><pubDate>Wed, 20 Dec 2023 16:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13185v1</guid></item><item><title>On the Number of Regions of Piecewise Linear Neural Networks</title><link>http://arxiv.org/abs/2206.08615v2</link><description>Many feedforward neural networks (NNs) generate continuous andpiecewise-linear (CPWL) mappings. Specifically, they partition the input domaininto regions on which the mapping is affine. The number of these so-calledlinear regions offers a natural metric to characterize the expressiveness ofCPWL NNs. The precise determination of this quantity is often out of reach inpractice, and bounds have been proposed for specific architectures, includingfor ReLU and Maxout NNs. In this work, we generalize these bounds to NNs witharbitrary and possibly multivariate CPWL activation functions. We first provideupper and lower bounds on the maximal number of linear regions of a CPWL NNgiven its depth, width, and the number of linear regions of its activationfunctions. Our results rely on the combinatorial structure of convex partitionsand confirm the distinctive role of depth which, on its own, is able toexponentially increase the number of regions. We then introduce a complementarystochastic framework to estimate the average number of linear regions producedby a CPWL NN. Under reasonable assumptions, the expected density of linearregions along any 1D path is bounded by the product of depth, width, and ameasure of activation complexity (up to a scaling factor). This yields anidentical role to the three sources of expressiveness: no exponential growthwith depth is observed anymore.</description><author>Alexis Goujon, Arian Etemadi, Michael Unser</author><pubDate>Wed, 20 Dec 2023 16:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08615v2</guid></item><item><title>ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing</title><link>http://arxiv.org/abs/2309.09128v2</link><description>Evaluating outputs of large language models (LLMs) is challenging, requiringmaking -- and making sense of -- many responses. Yet tools that go beyond basicprompting tend to require knowledge of programming APIs, focus on narrowdomains, or are closed-source. We present ChainForge, an open-source visualtoolkit for prompt engineering and on-demand hypothesis testing of textgeneration LLMs. ChainForge provides a graphical interface for comparison ofresponses across models and prompt variations. Our system was designed tosupport three tasks: model selection, prompt template design, and hypothesistesting (e.g., auditing). We released ChainForge early in its development anditerated on its design with academics and online users. Through in-lab andinterview studies, we find that a range of people could use ChainForge toinvestigate hypotheses that matter to them, including in real-world settings.We identify three modes of prompt engineering and LLM hypothesis testing:opportunistic exploration, limited evaluation, and iterative refinement.</description><author>Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman</author><pubDate>Wed, 20 Dec 2023 16:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09128v2</guid></item><item><title>Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based Classification and Mode-Based Ranking</title><link>http://arxiv.org/abs/2312.11517v2</link><description>This research delves into the intricate landscape of Musculoskeletal Disorder(MSD) risk factors, employing a novel fusion of Natural Language Processing(NLP) techniques and mode-based ranking methodologies. The primary objective isto advance the comprehension of MSD risk factors, their classification, andtheir relative severity, facilitating more targeted preventive and managementinterventions. The study utilizes eight diverse models, integrating pre-trainedtransformers, cosine similarity, and various distance metrics to classify riskfactors into personal, biomechanical, workplace, psychological, andorganizational classes. Key findings reveal that the BERT model with cosinesimilarity attains an overall accuracy of 28%, while the sentence transformer,coupled with Euclidean, Bray-Curtis, and Minkowski distances, achieves aflawless accuracy score of 100%. In tandem with the classification efforts, theresearch employs a mode-based ranking approach on survey data to discern theseverity hierarchy of MSD risk factors. Intriguingly, the rankings alignprecisely with the previous literature, reaffirming the consistency andreliability of the approach. ``Working posture" emerges as the most severe riskfactor, emphasizing the critical role of proper posture in preventing MSDs. Thecollective perceptions of survey participants underscore the significance offactors like "Job insecurity," "Effort reward imbalance," and "Poor employeefacility" in contributing to MSD risks. The convergence of rankings providesactionable insights for organizations aiming to reduce the prevalence of MSDs.The study concludes with implications for targeted interventions,recommendations for improving workplace conditions, and avenues for futureresearch.</description><author>Md Abrar Jahin, Subrata Talapatra</author><pubDate>Wed, 20 Dec 2023 16:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11517v2</guid></item><item><title>Contextual Code Switching for Machine Translation using Language Models</title><link>http://arxiv.org/abs/2312.13179v1</link><description>Large language models (LLMs) have exerted a considerable impact on diverselanguage-related tasks in recent years. Their demonstrated state-of-the-artperformance is achieved through methodologies such as zero-shot or few-shotprompting. These models undergo training on extensive datasets that encompasssegments of the Internet and subsequently undergo fine-tuning tailored tospecific tasks. Notably, they exhibit proficiency in tasks such as translation,summarization, question answering, and creative writing, even in the absence ofexplicit training for those particular tasks. While they have shown substantialimprovement in the multilingual tasks their performance in the code switching,especially for machine translation remains relatively uncharted. In this paper,we present an extensive study on the code switching task specifically for themachine translation task comparing multiple LLMs. Our results indicate thatdespite the LLMs having promising results in the certain tasks, the models withrelatively lesser complexity outperform the multilingual large language modelsin the machine translation task. We posit that the efficacy of multilinguallarge language models in contextual code switching is constrained by theirtraining methodologies. In contrast, relatively smaller models, when trainedand fine-tuned on bespoke datasets, may yield superior results in comparison tothe majority of multilingual models.</description><author>Arshad Kaji, Manan Shah</author><pubDate>Wed, 20 Dec 2023 16:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13179v1</guid></item><item><title>Improved Differentially Private and Lazy Online Convex Optimization</title><link>http://arxiv.org/abs/2312.11534v2</link><description>We study the task of $(\epsilon, \delta)$-differentially private onlineconvex optimization (OCO). In the online setting, the release of each distinctdecision or iterate carries with it the potential for privacy loss. Thisproblem has a long history of research starting with Jain et al. [2012] and thebest known results for the regime of {\epsilon} not being very small arepresented in Agarwal et al. [2023]. In this paper we improve upon the resultsof Agarwal et al. [2023] in terms of the dimension factors as well as removingthe requirement of smoothness. Our results are now the best known rates forDP-OCO in this regime. Our algorithms builds upon the work of [Asi et al., 2023] which introducedthe idea of explicitly limiting the number of switches via rejection sampling.The main innovation in our algorithm is the use of sampling from a stronglylog-concave density which allows us to trade-off the dimension factors betterleading to improved results.</description><author>Naman Agarwal, Satyen Kale, Karan Singh, Abhradeep Guha Thakurta</author><pubDate>Wed, 20 Dec 2023 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11534v2</guid></item><item><title>Learning Fair Policies for Multi-stage Selection Problems from Observational Data</title><link>http://arxiv.org/abs/2312.13173v1</link><description>We consider the problem of learning fair policies for multi-stage selectionproblems from observational data. This problem arises in several high-stakesdomains such as company hiring, loan approval, or bail decisions where outcomes(e.g., career success, loan repayment, recidivism) are only observed for thoseselected. We propose a multi-stage framework that can be augmented with variousfairness constraints, such as demographic parity or equal opportunity. Thisproblem is a highly intractable infinite chance-constrained program involvingthe unknown joint distribution of covariates and outcomes. Motivated by thepotential impact of selection decisions on people's lives and livelihoods, wepropose to focus on interpretable linear selection rules. Leveraging tools fromcausal inference and sample average approximation, we obtain an asymptoticallyconsistent solution to this selection problem by solving a mixed binary conicoptimization problem, which can be solved using standard off-the-shelf solvers.We conduct extensive computational experiments on a variety of datasets adaptedfrom the UCI repository on which we show that our proposed approaches canachieve an 11.6% improvement in precision and a 38% reduction in the measure ofunfairness compared to the existing selection policy.</description><author>Zhuangzhuang Jia, Grani A. Hanasusanto, Phebe Vayanos, Weijun Xie</author><pubDate>Wed, 20 Dec 2023 16:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13173v1</guid></item><item><title>Automatic and effective discovery of quantum kernels</title><link>http://arxiv.org/abs/2209.11144v2</link><description>Quantum computing can empower machine learning models by enabling kernelmachines to leverage quantum kernels for representing similarity measuresbetween data. Quantum kernels are able to capture relationships in the datathat are not efficiently computable on classical devices. However, there is nostraightforward method to engineer the optimal quantum kernel for each specificuse case. While recent literature has focused on exploiting the potentialoffered by the presence of symmetries in the data to guide the construction ofquantum kernels, we adopt here a different approach, which employs optimizationtechniques, similar to those used in neural architecture search and AutoML, toautomatically find an optimal kernel in a heuristic manner. The algorithm wepresent constructs a quantum circuit implementing the similarity measure as acombinatorial object, which is evaluated based on a cost function and is theniteratively modified using a meta-heuristic optimization technique. The costfunction can encode many criteria ensuring favorable statistical properties ofthe candidate solution, such as the rank of the Dynamical Lie Algebra.Importantly, our approach is independent of the optimization techniqueemployed. The results obtained by testing our approach on a high-energy physicsproblem demonstrate that, in the best-case scenario, we can either match orimprove testing accuracy with respect to the manual design approach, showingthe potential of our technique to deliver superior results with reduced effort.</description><author>Massimiliano Incudini, Daniele Lizzio Bosco, Francesco Martini, Michele Grossi, Giuseppe Serra, Alessandra Di Pierro</author><pubDate>Wed, 20 Dec 2023 16:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11144v2</guid></item><item><title>Brain-Inspired Visual Odometry: Balancing Speed and Interpretability through a System of Systems Approach</title><link>http://arxiv.org/abs/2312.13162v1</link><description>In this study, we address the critical challenge of balancing speed andaccuracy while maintaining interpretablity in visual odometry (VO) systems, apivotal aspect in the field of autonomous navigation and robotics. TraditionalVO systems often face a trade-off between computational speed and the precisionof pose estimation. To tackle this issue, we introduce an innovative systemthat synergistically combines traditional VO methods with a specificallytailored fully connected network (FCN). Our system is unique in its approach tohandle each degree of freedom independently within the FCN, placing a strongemphasis on causal inference to enhance interpretability. This allows for adetailed and accurate assessment of relative pose error (RPE) across variousdegrees of freedom, providing a more comprehensive understanding of parametervariations and movement dynamics in different environments. Notably, our systemdemonstrates a remarkable improvement in processing speed without compromisingaccuracy. In certain scenarios, it achieves up to a 5% reduction in Root MeanSquare Error (RMSE), showcasing its ability to effectively bridge the gapbetween speed and accuracy that has long been a limitation in VO research. Thisadvancement represents a significant step forward in developing more efficientand reliable VO systems, with wide-ranging applications in real-time navigationand robotic systems.</description><author>Habib Boloorchi Tabrizi, Christopher Crick</author><pubDate>Wed, 20 Dec 2023 16:23:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13162v1</guid></item><item><title>AccidentGPT: Accident analysis and prevention from V2X Environmental Perception with Multi-modal Large Model</title><link>http://arxiv.org/abs/2312.13156v1</link><description>Traffic accidents, being a significant contributor to both human casualtiesand property damage, have long been a focal point of research for many scholarsin the field of traffic safety. However, previous studies, whether focusing onstatic environmental assessments or dynamic driving analyses, as well aspre-accident predictions or post-accident rule analyses, have typically beenconducted in isolation. There has been a lack of an effective framework fordeveloping a comprehensive understanding and application of traffic safety. Toaddress this gap, this paper introduces AccidentGPT, a comprehensive accidentanalysis and prevention multi-modal large model. AccidentGPT establishes amulti-modal information interaction framework grounded in multi-sensorperception, thereby enabling a holistic approach to accident analysis andprevention in the field of traffic safety. Specifically, our capabilities canbe categorized as follows: for autonomous driving vehicles, we providecomprehensive environmental perception and understanding to control the vehicleand avoid collisions. For human-driven vehicles, we offer proactive long-rangesafety warnings and blind-spot alerts while also providing safety drivingrecommendations and behavioral norms through human-machine dialogue andinteraction. Additionally, for traffic police and management agencies, ourframework supports intelligent and real-time analysis of traffic safety,encompassing pedestrian, vehicles, roads, and the environment throughcollaborative perception from multiple vehicles and road testing devices. Thesystem is also capable of providing a thorough analysis of accident causes andliability after vehicle collisions. Our framework stands as the first largemodel to integrate comprehensive scene understanding into traffic safetystudies.</description><author>Lening Wang, Han Jiang, Pinlong Cai, Daocheng Fu, Tianqi Wang, Zhiyong Cui, Yilong Ren, Haiyang Yu, Xuesong Wang, Yinhai Wang</author><pubDate>Wed, 20 Dec 2023 16:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13156v1</guid></item><item><title>Gappy local conformal auto-encoders for heterogeneous data fusion: in praise of rigidity</title><link>http://arxiv.org/abs/2312.13155v1</link><description>Fusing measurements from multiple, heterogeneous, partial sources, observinga common object or process, poses challenges due to the increasing availabilityof numbers and types of sensors. In this work we propose, implement andvalidate an end-to-end computational pipeline in the form of amultiple-auto-encoder neural network architecture for this task. The inputs tothe pipeline are several sets of partial observations, and the result is aglobally consistent latent space, harmonizing (rigidifying, fusing) allmeasurements. The key enabler is the availability of multiple slightlyperturbed measurements of each instance:, local measurement, "bursts", thatallows us to estimate the local distortion induced by each instrument. Wedemonstrate the approach in a sequence of examples, starting with simpletwo-dimensional data sets and proceeding to a Wi-Fi localization problem and tothe solution of a "dynamical puzzle" arising in spatio-temporal observations ofthe solutions of Partial Differential Equations.</description><author>Erez Peterfreund, Iryna Burak, Ofir Lindenbaum, Jim Gimlett, Felix Dietrich, Ronald R. Coifman, Ioannis G. Kevrekidis</author><pubDate>Wed, 20 Dec 2023 16:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13155v1</guid></item><item><title>Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach</title><link>http://arxiv.org/abs/2312.13152v1</link><description>Stochastic differential equations (SDEs) have been widely used to model realworld random phenomena. Existing works mainly focus on the case where the timeseries is modeled by a single SDE, which might be restrictive for modeling timeseries with distributional shift. In this work, we propose a change pointdetection algorithm for time series modeled as neural SDEs. Given a time seriesdataset, the proposed method jointly learns the unknown change points and theparameters of distinct neural SDE models corresponding to each change point.Specifically, the SDEs are learned under the framework of generativeadversarial networks (GANs) and the change points are detected based on theoutput of the GAN discriminator in a forward pass. At each step of the proposedalgorithm, the change points and the SDE model parameters are updated in analternating fashion. Numerical results on both synthetic and real datasets areprovided to validate the performance of our algorithm in comparison toclassical change point detection benchmarks, standard GAN-based neural SDEs,and other state-of-the-art deep generative models for time series data.</description><author>Zhongchang Sun, Yousef El-Laham, Svitlana Vyetrenko</author><pubDate>Wed, 20 Dec 2023 16:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13152v1</guid></item><item><title>Earthfarseer: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model</title><link>http://arxiv.org/abs/2312.08403v2</link><description>Efficiently modeling spatio-temporal (ST) physical processes and observationspresents a challenging problem for the deep learning community. Many recentstudies have concentrated on meticulously reconciling various advantages,leading to designed models that are neither simple nor practical. To addressthis issue, this paper presents a systematic study on existing shortcomingsfaced by off-the-shelf models, including lack of local fidelity, poorprediction performance over long time-steps,low scalability, and inefficiency.To systematically address the aforementioned problems, we propose anEarthFarseer, a concise framework that combines parallel local convolutions andglobal Fourier-based transformer architectures, enabling dynamically capturethe local-global spatial interactions and dependencies. EarthFarseer alsoincorporates a multi-scale fully convolutional and Fourier architectures toefficiently and effectively capture the temporal evolution. Our proposaldemonstrates strong adaptability across various tasks and datasets, with fastconvergence and better local fidelity in long time-steps predictions. Extensiveexperiments and visualizations over eight human society physical and naturalphysical datasets demonstrates the state-of-the-art performance ofEarthFarseer. We release our code athttps://github.com/easylearningscores/EarthFarseer.</description><author>Hao Wu, Shilong Wang, Yuxuan Liang, Zhengyang Zhou, Wei Huang, Wei Xiong, Kun Wang</author><pubDate>Wed, 20 Dec 2023 16:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08403v2</guid></item><item><title>Tuning the activation function to optimize the forecast horizon of a reservoir computer</title><link>http://arxiv.org/abs/2312.13151v1</link><description>Reservoir computing is a machine learning framework where the readouts from anonlinear system (the reservoir) are trained so that the output from thereservoir, when forced with an input signal, reproduces a desired outputsignal. A common implementation of reservoir computers is to use a recurrentneural network as the reservoir. The design of this network can havesignificant effects on the performance of the reservoir computer. In this paperwe study the effect of the node activation function on the ability of reservoircomputers to learn and predict chaotic time series. We find that the ForecastHorizon (FH), the time during which the reservoir's predictions remainaccurate, can vary by an order of magnitude across a set of 16 activationfunctions used in machine learning. By using different functions from this set,and by modifying their parameters, we explore whether the entropy of nodeactivation levels or the curvature of the activation functions determine thepredictive ability of the reservoirs. We find that the FH is low when theactivation function is used in a region where it has low curvature, and apositive correlation between curvature and FH. For the activation functionsstudied we find that the largest FH generally occurs at intermediate levels ofthe entropy of node activation levels. Our results show that the performance ofreservoir computers is very sensitive to the activation function shape.Therefore, modifying this shape in hyperparameter optimization algorithms canlead to improvements in reservoir computer performance.</description><author>Lauren A. Hurley, Juan G. Restrepo, Sean E. Shaheen</author><pubDate>Wed, 20 Dec 2023 16:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13151v1</guid></item><item><title>Re-Evaluating LiDAR Scene Flow for Autonomous Driving</title><link>http://arxiv.org/abs/2304.02150v2</link><description>Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, andFlyingThings3D) have unrealistic rates of dynamic motion, unrealisticcorrespondences, and unrealistic sampling patterns. As a result, progress onthese benchmarks is misleading and may cause researchers to focus on the wrongproblems. We evaluate a suite of top methods on a suite of real-world datasets(Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, wefind that performance on stereoKITTI is negatively correlated with performanceon real-world data. Second, we find that one of this task's key components --removing the dominant ego-motion -- is better solved by classic ICP than anytested method. Finally, we show that despite the emphasis placed on learning,most performance gains are caused by pre- and post-processing steps:piecewise-rigid refinement and ground removal. We demonstrate this through abaseline method that combines these processing steps with a learning-freetest-time flow optimization. This baseline outperforms every evaluated method.</description><author>Nathaniel Chodosh, Deva Ramanan, Simon Lucey</author><pubDate>Wed, 20 Dec 2023 16:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02150v2</guid></item><item><title>Splatter Image: Ultra-Fast Single-View 3D Reconstruction</title><link>http://arxiv.org/abs/2312.13150v1</link><description>We introduce the Splatter Image, an ultra-fast approach for monocular 3Dobject reconstruction which operates at 38 FPS. Splatter Image is based onGaussian Splatting, which has recently brought real-time rendering, fasttraining, and excellent scaling to multi-view reconstruction. For the firsttime, we apply Gaussian Splatting in a monocular reconstruction setting. Ourapproach is learning-based, and, at test time, reconstruction only requires thefeed-forward evaluation of a neural network. The main innovation of SplatterImage is the surprisingly straightforward design: it uses a 2D image-to-imagenetwork to map the input image to one 3D Gaussian per pixel. The resultingGaussians thus have the form of an image, the Splatter Image. We further extendthe method to incorporate more than one image as input, which we do by addingcross-view attention. Owning to the speed of the renderer (588 FPS), we can usea single GPU for training while generating entire images at each iteration inorder to optimize perceptual metrics like LPIPS. On standard benchmarks, wedemonstrate not only fast reconstruction but also better results than recentand much more expensive baselines in terms of PSNR, LPIPS, and other metrics.</description><author>Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Wed, 20 Dec 2023 16:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13150v1</guid></item><item><title>Partially factorized variational inference for high-dimensional mixed models</title><link>http://arxiv.org/abs/2312.13148v1</link><description>While generalized linear mixed models (GLMMs) are a fundamental tool inapplied statistics, many specifications -- such as those involving categoricalfactors with many levels or interaction terms -- can be computationallychallenging to estimate due to the need to compute or approximatehigh-dimensional integrals. Variational inference (VI) methods are a popularway to perform such computations, especially in the Bayesian context. However,naive VI methods can provide unreliable uncertainty quantification. We showthat this is indeed the case in the GLMM context, proving that standard VI(i.e. mean-field) dramatically underestimates posterior uncertainty inhigh-dimensions. We then show how appropriately relaxing the mean-fieldassumption leads to VI methods whose uncertainty quantification does notdeteriorate in high-dimensions, and whose total computational cost scaleslinearly with the number of parameters and observations. Our theoretical andnumerical results focus on GLMMs with Gaussian or binomial likelihoods, andrely on connections to random graph theory to obtain sharp high-dimensionalasymptotic analysis. We also provide generic results, which are of independentinterest, relating the accuracy of variational inference to the convergencerate of the corresponding coordinate ascent variational inference (CAVI)algorithm for Gaussian targets. Our proposed partially-factorized VI (PF-VI)methodology for GLMMs is implemented in the R package vglmer, seehttps://github.com/mgoplerud/vglmer . Numerical results with simulated and realdata examples illustrate the favourable computation cost versus accuracytrade-off of PF-VI.</description><author>Max Goplerud, Omiros Papaspiliopoulos, Giacomo Zanella</author><pubDate>Wed, 20 Dec 2023 16:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13148v1</guid></item><item><title>In Search of Projectively Equivariant Networks</title><link>http://arxiv.org/abs/2209.14719v3</link><description>Equivariance of linear neural network layers is well studied. In this work,we relax the equivariance condition to only be true in a projective sense. Wepropose a way to construct a projectively equivariant neural network throughbuilding a standard equivariant network where the linear group representationsacting on each intermediate feature space are "multiplicatively modified lifts"of projective group representations. By theoretically studying the relation ofprojectively and linearly equivariant linear layers, we show that our approachis the most general possible when building a network out of linear layers. Thetheory is showcased in two simple experiments.</description><author>Georg Bökman, Axel Flinth, Fredrik Kahl</author><pubDate>Wed, 20 Dec 2023 16:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14719v3</guid></item><item><title>Underwater Acoustic Signal Recognition Based on Salient Features</title><link>http://arxiv.org/abs/2312.13143v1</link><description>With the rapid advancement of technology, the recognition of underwateracoustic signals in complex environments has become increasingly crucial.Currently, mainstream underwater acoustic signal recognition relies primarilyon time-frequency analysis to extract spectral features, finding widespreadapplications in the field. However, existing recognition methods heavily dependon expert systems, facing limitations such as restricted knowledge bases andchallenges in handling complex relationships. These limitations stem from thecomplexity and maintenance difficulties associated with rules or inferenceengines. Recognizing the potential advantages of deep learning in handlingintricate relationships, this paper proposes a method utilizing neural networksfor underwater acoustic signal recognition. The proposed approach involvescontinual learning of features extracted from spectra for the classification ofunderwater acoustic signals. Deep learning models can automatically learnabstract features from data and continually adjust weights during training toenhance classification performance.</description><author>Minghao Chen</author><pubDate>Wed, 20 Dec 2023 16:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13143v1</guid></item><item><title>One step closer to unbiased aleatoric uncertainty estimation</title><link>http://arxiv.org/abs/2312.10469v2</link><description>Neural networks are powerful tools in various applications, and quantifyingtheir uncertainty is crucial for reliable decision-making. In the deep learningfield, the uncertainties are usually categorized into aleatoric (data) andepistemic (model) uncertainty. In this paper, we point out that the existingpopular variance attenuation method highly overestimates aleatoric uncertainty.To address this issue, we propose a new estimation method by activelyde-noising the observed data. By conducting a broad range of experiments, wedemonstrate that our proposed approach provides a much closer approximation tothe actual data uncertainty than the standard method.</description><author>Wang Zhang, Ziwen Ma, Subhro Das, Tsui-Wei Weng, Alexandre Megretski, Luca Daniel, Lam M. Nguyen</author><pubDate>Wed, 20 Dec 2023 16:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10469v2</guid></item><item><title>Augment on Manifold: Mixup Regularization with UMAP</title><link>http://arxiv.org/abs/2312.13141v1</link><description>Data augmentation techniques play an important role in enhancing theperformance of deep learning models. Despite their proven benefits in computervision tasks, their application in the other domains remains limited. Thispaper proposes a Mixup regularization scheme, referred to as UMAP Mixup,designed for "on-manifold" automated data augmentation for deep learningpredictive models. The proposed approach ensures that the Mixup operationsresult in synthesized samples that lie on the data manifold of the features andlabels by utilizing a dimensionality reduction technique known as uniformmanifold approximation and projection. Evaluations across diverse regressiontasks show that UMAP Mixup is competitive with or outperforms other Mixupvariants, show promise for its potential as an effective tool for enhancing thegeneralization performance of deep learning models.</description><author>Yousef El-Laham, Elizabeth Fons, Dillon Daudert, Svitlana Vyetrenko</author><pubDate>Wed, 20 Dec 2023 16:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13141v1</guid></item><item><title>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation</title><link>http://arxiv.org/abs/2312.13139v1</link><description>Generative pre-trained models have demonstrated remarkable effectiveness inlanguage and vision domains by learning useful representations. In this paper,we extend the scope of this effectiveness by showing that visual robotmanipulation can significantly benefit from large-scale video generativepre-training. We introduce GR-1, a straightforward GPT-style model designed formulti-task language-conditioned visual robot manipulation. GR-1 takes as inputsa language instruction, a sequence of observation images, and a sequence ofrobot states. It predicts robot actions as well as future images in anend-to-end manner. Thanks to a flexible design, GR-1 can be seamlesslyfinetuned on robot data after pre-trained on a large-scale video dataset. Weperform extensive experiments on the challenging CALVIN benchmark and a realrobot. On CALVIN benchmark, our method outperforms state-of-the-art baselinemethods and improves the success rate from 88.9% to 94.9%. In the setting ofzero-shot unseen scene generalization, GR-1 improves the success rate from53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baselinemethods and shows strong potentials in generalization to unseen scenes andobjects. We provide inaugural evidence that a unified GPT-style transformer,augmented with large-scale video generative pre-training, exhibits remarkablegeneralization to multi-task visual robot manipulation. Project page:https://GR1-Manipulation.github.io</description><author>Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong</author><pubDate>Wed, 20 Dec 2023 16:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13139v1</guid></item><item><title>FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline</title><link>http://arxiv.org/abs/2311.13073v2</link><description>Multimedia generation approaches occupy a prominent place in artificialintelligence research. Text-to-image models achieved high-quality results overthe last few years. However, video synthesis methods recently started todevelop. This paper presents a new two-stage latent diffusion text-to-videogeneration architecture based on the text-to-image diffusion model. The firststage concerns keyframes synthesis to figure the storyline of a video, whilethe second one is devoted to interpolation frames generation to make movementsof the scene and objects smooth. We compare several temporal conditioningapproaches for keyframes generation. The results show the advantage of usingseparate temporal blocks over temporal layers in terms of metrics reflectingvideo generation quality aspects and human preference. The design of ourinterpolation model significantly reduces computational costs compared to othermasked frame interpolation approaches. Furthermore, we evaluate differentconfigurations of MoVQ-based video decoding scheme to improve consistency andachieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare ourpipeline with existing solutions and achieve top-2 scores overall and top-1among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:https://ai-forever.github.io/kandinsky-video/</description><author>Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta Dakhova, Andrey Kuznetsov, Denis Dimitrov</author><pubDate>Wed, 20 Dec 2023 15:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13073v2</guid></item><item><title>Molecular Hypergraph Neural Networks</title><link>http://arxiv.org/abs/2312.13136v1</link><description>Graph neural networks (GNNs) have demonstrated promising performance acrossvarious chemistry-related tasks. However, conventional graphs only model thepairwise connectivity in molecules, failing to adequately representhigher-order connections like multi-center bonds and conjugated structures. Totackle this challenge, we introduce molecular hypergraphs and propose MolecularHypergraph Neural Networks (MHNN) to predict the optoelectronic properties oforganic semiconductors, where hyperedges represent conjugated structures. Ageneral algorithm is designed for irregular high-order connections, which canefficiently operate on molecular hypergraphs with hyperedges of various orders.The results show that MHNN outperforms all baseline models on most tasks ofOPV, OCELOTv1 and PCQM4Mv2 datasets. Notably, MHNN achieves this without any 3Dgeometric information, surpassing the baseline model that utilizes atompositions. Moreover, MHNN achieves better performance than pretrained GNNsunder limited training data, underscoring its excellent data efficiency. Thiswork provides a new strategy for more general molecular representations andproperty prediction tasks related to high-order connections.</description><author>Junwu Chen, Philippe Schwaller</author><pubDate>Wed, 20 Dec 2023 15:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13136v1</guid></item><item><title>Scaling Compute Is Not All You Need for Adversarial Robustness</title><link>http://arxiv.org/abs/2312.13131v1</link><description>The last six years have witnessed significant progress in adversariallyrobust deep learning. As evidenced by the CIFAR-10 dataset category inRobustBench benchmark, the accuracy under $\ell_\infty$ adversarialperturbations improved from 44\% in \citet{Madry2018Towards} to 71\% in\citet{peng2023robust}. Although impressive, existing state-of-the-art is stillfar from satisfactory. It is further observed that best-performing models areoften very large models adversarially trained by industrial labs withsignificant computational budgets. In this paper, we aim to understand: ``howmuch longer can computing power drive adversarial robustness advances?" Toanswer this question, we derive \emph{scaling laws for adversarial robustness}which can be extrapolated in the future to provide an estimate of how much costwe would need to pay to reach a desired level of robustness. We show thatincreasing the FLOPs needed for adversarial training does not bring as muchadvantage as it does for standard training in terms of performanceimprovements. Moreover, we find that some of the top-performing techniques aredifficult to exactly reproduce, suggesting that they are not robust enough forminor changes in the training setup. Our analysis also uncovers potentiallyworthwhile directions to pursue in future research. Finally, we make ourbenchmarking framework (built on top of \texttt{timm}~\citep{rw2019timm})publicly available to facilitate future analysis in efficient robust deeplearning.</description><author>Edoardo Debenedetti, Zishen Wan, Maksym Andriushchenko, Vikash Sehwag, Kshitij Bhardwaj, Bhavya Kailkhura</author><pubDate>Wed, 20 Dec 2023 15:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13131v1</guid></item><item><title>Distribution-Dependent Rates for Multi-Distribution Learning</title><link>http://arxiv.org/abs/2312.13130v1</link><description>To address the needs of modeling uncertainty in sensitive machine learningapplications, the setup of distributionally robust optimization (DRO) seeksgood performance uniformly across a variety of tasks. The recentmulti-distribution learning (MDL) framework tackles this objective in a dynamicinteraction with the environment, where the learner has sampling access to eachtarget distribution. Drawing inspiration from the field of pure-explorationmulti-armed bandits, we provide distribution-dependent guarantees in the MDLregime, that scale with suboptimality gaps and result in superior dependence onthe sample size when compared to the existing distribution-independentanalyses. We investigate two non-adaptive strategies, uniform and non-uniformexploration, and present non-asymptotic regret bounds using novel tools fromempirical process theory. Furthermore, we devise an adaptive optimisticalgorithm, LCB-DR, that showcases enhanced dependence on the gaps, mirroringthe contrast between uniform and optimistic allocation in the multi-armedbandit literature.</description><author>Rafael Hanashiro, Patrick Jaillet</author><pubDate>Wed, 20 Dec 2023 15:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13130v1</guid></item><item><title>How Far Have We Gone in Vulnerability Detection Using Large Language Models</title><link>http://arxiv.org/abs/2311.12420v2</link><description>As software becomes increasingly complex and prone to vulnerabilities,automated vulnerability detection is critically important, yet challenging.Given the significant successes of large language models (LLMs) in varioustasks, there is growing anticipation of their efficacy in vulnerabilitydetection. However, a quantitative understanding of their potential invulnerability detection is still missing. To bridge this gap, we introduce acomprehensive vulnerability benchmark VulBench. This benchmark aggregateshigh-quality data from a wide range of CTF (Capture-the-Flag) challenges andreal-world applications, with annotations for each vulnerable functiondetailing the vulnerability type and its root cause. Through our experimentsencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based modelsand static analyzers, we find that several LLMs outperform traditional deeplearning approaches in vulnerability detection, revealing an untapped potentialin LLMs. This work contributes to the understanding and utilization of LLMs forenhanced software security.</description><author>Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang</author><pubDate>Wed, 20 Dec 2023 15:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12420v2</guid></item><item><title>Pixel-to-Abundance Translation: Conditional Generative Adversarial Networks Based on Patch Transformer for Hyperspectral Unmixing</title><link>http://arxiv.org/abs/2312.13127v1</link><description>Spectral unmixing is a significant challenge in hyperspectral imageprocessing. Existing unmixing methods utilize prior knowledge about theabundance distribution to solve the regularization optimization problem, wherethe difficulty lies in choosing appropriate prior knowledge and solving thecomplex regularization optimization problem. To solve these problems, wepropose a hyperspectral conditional generative adversarial network (HyperGAN)method as a generic unmixing framework, based on the following assumption: theunmixing process from pixel to abundance can be regarded as a transformation oftwo modalities with an internal specific relationship. The proposed HyperGAN iscomposed of a generator and discriminator, the former completes the modalconversion from mixed hyperspectral pixel patch to the abundance ofcorresponding endmember of the central pixel and the latter is used todistinguish whether the distribution and structure of generated abundance arethe same as the true ones. We propose hyperspectral image (HSI) PatchTransformer as the main component of the generator, which utilize adaptiveattention score to capture the internal pixels correlation of the HSI patch andleverage the spatial-spectral information in a fine-grained way to achieveoptimization of the unmixing process. Experiments on synthetic data and realhyperspectral data achieve impressive results compared to state-of-the-artcompetitors.</description><author>Li Wang, Xiaohua Zhang, Longfei Li, Hongyun Meng, Xianghai Cao</author><pubDate>Wed, 20 Dec 2023 15:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13127v1</guid></item><item><title>Prometheus: Infrastructure Security Posture Analysis with AI-generated Attack Graphs</title><link>http://arxiv.org/abs/2312.13119v1</link><description>The rampant occurrence of cybersecurity breaches imposes substantiallimitations on the progress of network infrastructures, leading to compromiseddata, financial losses, potential harm to individuals, and disruptions inessential services. The current security landscape demands the urgentdevelopment of a holistic security assessment solution that encompassesvulnerability analysis and investigates the potential exploitation of thesevulnerabilities as attack paths. In this paper, we propose Prometheus, anadvanced system designed to provide a detailed analysis of the security postureof computing infrastructures. Using user-provided information, such as devicedetails and software versions, Prometheus performs a comprehensive securityassessment. This assessment includes identifying associated vulnerabilities andconstructing potential attack graphs that adversaries can exploit. Furthermore,Prometheus evaluates the exploitability of these attack paths and quantifiesthe overall security posture through a scoring mechanism. The system takes aholistic approach by analyzing security layers encompassing hardware, system,network, and cryptography. Furthermore, Prometheus delves into theinterconnections between these layers, exploring how vulnerabilities in onelayer can be leveraged to exploit vulnerabilities in others. In this paper, wepresent the end-to-end pipeline implemented in Prometheus, showcasing thesystematic approach adopted for conducting this thorough security analysis.</description><author>Xin Jin, Charalampos Katsis, Fan Sang, Jiahao Sun, Elisa Bertino, Ramana Rao Kompella, Ashish Kundu</author><pubDate>Wed, 20 Dec 2023 15:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13119v1</guid></item><item><title>LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate</title><link>http://arxiv.org/abs/2312.13118v1</link><description>The transferability of adversarial examples is of central importance totransfer-based black-box adversarial attacks. Previous works for generatingtransferable adversarial examples focus on attacking \emph{given} pretrainedsurrogate models while the connections between surrogate models and adversarialtrasferability have been overlooked. In this paper, we propose {\em LipschitzRegularized Surrogate} (LRS) for transfer-based black-box attacks, a novelapproach that transforms surrogate models towards favorable adversarialtransferability. Using such transformed surrogate models, any existingtransfer-based black-box attack can run without any change, yet achieving muchbetter performance. Specifically, we impose Lipschitz regularization on theloss landscape of surrogate models to enable a smoother and more controlledoptimization process for generating more transferable adversarial examples. Inaddition, this paper also sheds light on the connection between the innerproperties of surrogate models and adversarial transferability, where threefactors are identified: smaller local Lipschitz constant, smoother losslandscape, and stronger adversarial robustness. We evaluate our proposed LRSapproach by attacking state-of-the-art standard deep neural networks anddefense models. The results demonstrate significant improvement on the attacksuccess rates and transferability. Our code is available athttps://github.com/TrustAIoT/LRS.</description><author>Tao Wu, Tie Luo, Donald C. Wunsch</author><pubDate>Wed, 20 Dec 2023 15:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13118v1</guid></item><item><title>VSR-Net: Vessel-like Structure Rehabilitation Network with Graph Clustering</title><link>http://arxiv.org/abs/2312.13116v1</link><description>The morphologies of vessel-like structures, such as blood vessels and nervefibres, play significant roles in disease diagnosis, e.g., Parkinson's disease.Deep network-based refinement segmentation methods have recently achievedpromising vessel-like structure segmentation results. There are still twochallenges: (1) existing methods have limitations in rehabilitating subsectionruptures in segmented vessel-like structures; (2) they are often overconfidentin predicted segmentation results. To tackle these two challenges, this paperattempts to leverage the potential of spatial interconnection relationshipsamong subsection ruptures from the structure rehabilitation perspective. Basedon this, we propose a novel Vessel-like Structure Rehabilitation Network(VSR-Net) to rehabilitate subsection ruptures and improve the model calibrationbased on coarse vessel-like structure segmentation results. VSR-Net firstconstructs subsection rupture clusters with Curvilinear Clustering Module(CCM). Then, the well-designed Curvilinear Merging Module (CMM) is applied torehabilitate the subsection ruptures to obtain the refined vessel-likestructures. Extensive experiments on five 2D/3D medical image datasets showthat VSR-Net significantly outperforms state-of-the-art (SOTA) refinementsegmentation methods with lower calibration error. Additionally, we providequantitative analysis to explain the morphological difference between therehabilitation results of VSR-Net and ground truth (GT), which is smaller thanSOTA methods and GT, demonstrating that our method better rehabilitatesvessel-like structures by restoring subsection ruptures.</description><author>Haili Ye, Xiaoqing Zhang, Yan Hu, Huazhu Fu, Jiang Liu</author><pubDate>Wed, 20 Dec 2023 15:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13116v1</guid></item><item><title>Investigating Color Illusions from the Perspective of Computational Color Constancy</title><link>http://arxiv.org/abs/2312.13114v1</link><description>Color constancy and color illusion perception are two phenomena occurring inthe human visual system, which can help us reveal unknown mechanisms of humanperception. For decades computer vision scientists have developed numerouscolor constancy methods, which estimate the reflectance of the surface bydiscounting the illuminant. However, color illusions have not been analyzed indetail in the field of computational color constancy, which we find surprisingsince the relationship they share is significant and may let us design morerobust systems. We argue that any model that can reproduce our sensation oncolor illusions should also be able to provide pixel-wise estimates of thelight source. In other words, we suggest that the analysis of color illusionshelps us to improve the performance of the existing global color constancymethods, and enable them to provide pixel-wise estimates for scenes illuminatedby multiple light sources. In this study, we share the outcomes of ourinvestigation in which we take several color constancy methods and modify themto reproduce the behavior of the human visual system on color illusions. Also,we show that parameters purely extracted from illusions are able to improve theperformance of color constancy methods. A noteworthy outcome is that ourstrategy based on the investigation of color illusions outperforms thestate-of-the-art methods that are specifically designed to transform globalcolor constancy algorithms into multi-illuminant algorithms.</description><author>Oguzhan Ulucan, Diclehan Ulucan, Marc Ebner</author><pubDate>Wed, 20 Dec 2023 15:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13114v1</guid></item><item><title>Pre-training of Molecular GNNs as Conditional Boltzmann Generator</title><link>http://arxiv.org/abs/2312.13110v1</link><description>Learning representations of molecular structures using deep learning is afundamental problem in molecular property prediction tasks. Moleculesinherently exist in the real world as three-dimensional structures;furthermore, they are not static but in continuous motion in the 3D Euclideanspace, forming a potential energy surface. Therefore, it is desirable togenerate multiple conformations in advance and extract molecularrepresentations using a 4D-QSAR model that incorporates multiple conformations.However, this approach is impractical for drug and material discovery tasksbecause of the computational cost of obtaining multiple conformations. Toaddress this issue, we propose a pre-training method for molecular GNNs usingan existing dataset of molecular conformations to generate a latent vectoruniversal to multiple conformations from a 2D molecular graph. Our method,called Boltzmann GNN, is formulated by maximizing the conditional marginallikelihood of a conditional generative model for conformations generation. Weshow that our model has a better prediction performance for molecularproperties than existing pre-training methods using molecular graphs andthree-dimensional molecular structures.</description><author>Daiki Koge, Naoaki Ono, Shigehiko Kanaya</author><pubDate>Wed, 20 Dec 2023 15:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13110v1</guid></item><item><title>ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation</title><link>http://arxiv.org/abs/2312.13108v1</link><description>Graphical User Interface (GUI) automation holds significant promise forassisting users with complex tasks, thereby boosting human productivity.Existing works leveraging Large Language Model (LLM) or LLM-based AI agentshave shown capabilities in automating tasks on Android and Web platforms.However, these tasks are primarily aimed at simple device usage andentertainment operations. This paper presents a novel benchmark, AssistGUI, toevaluate whether models are capable of manipulating the mouse and keyboard onthe Windows platform in response to user-requested tasks. We carefullycollected a set of 100 tasks from nine widely-used software applications, suchas, After Effects and MS Word, each accompanied by the necessary project filesfor better evaluation. Moreover, we propose an advanced Actor-Critic EmbodiedAgent framework, which incorporates a sophisticated GUI parser driven by anLLM-agent and an enhanced reasoning mechanism adept at handling lengthyprocedural tasks. Our experimental results reveal that our GUI Parser andReasoning mechanism outshine existing methods in performance. Nevertheless, thepotential remains substantial, with the best model attaining only a 46% successrate on our benchmark. We conclude with a thorough analysis of the currentmethods' limitations, setting the stage for future breakthroughs in thisdomain.</description><author>Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, Mike Zheng Shou</author><pubDate>Wed, 20 Dec 2023 15:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13108v1</guid></item><item><title>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</title><link>http://arxiv.org/abs/2312.02464v2</link><description>Semantic segmentation of remote sensing imagery plays a pivotal role inextracting precise information for diverse down-stream applications. Recentdevelopment of the Segment Anything Model (SAM), an advanced general-purposesegmentation model, has revolutionized this field, presenting new avenues foraccurate and efficient segmentation. However, SAM is limited to generatingsegmentation results without class information. Consequently, the utilizationof such a powerful general vision model for semantic segmentation in remotesensing images has become a focal point of research. In this paper, we presenta streamlined framework aimed at leveraging the raw output of SAM by exploitingtwo novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary(SGB). More specifically, we propose a novel object loss and further introducea boundary loss as augmentative components to aid in model optimization in ageneral semantic segmentation framework. Taking into account the contentcharacteristics of SGO, we introduce the concept of object consistency toleverage segmented regions lacking semantic information. By imposingconstraints on the consistency of predicted values within objects, the objectloss aims to enhance semantic segmentation performance. Furthermore, theboundary loss capitalizes on the distinctive features of SGB by directing themodel's attention to the boundary information of the object. Experimentalresults on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,demonstrate the effectiveness of our proposed method. The source code for thiswork will be accessible at https://github.com/sstary/SSRS.</description><author>Xianping Ma, Qianqian Wu, Xingyu Zhao, Xiaokang Zhang, Man-On Pun, Bo Huang</author><pubDate>Wed, 20 Dec 2023 15:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02464v2</guid></item><item><title>Optimizing Ego Vehicle Trajectory Prediction: The Graph Enhancement Approach</title><link>http://arxiv.org/abs/2312.13104v1</link><description>Predicting the trajectory of an ego vehicle is a critical component ofautonomous driving systems. Current state-of-the-art methods typically rely onDeep Neural Networks (DNNs) and sequential models to process front-view imagesfor future trajectory prediction. However, these approaches often struggle withperspective issues affecting object features in the scene. To address this, weadvocate for the use of Bird's Eye View (BEV) perspectives, which offer uniqueadvantages in capturing spatial relationships and object homogeneity. In ourwork, we leverage Graph Neural Networks (GNNs) and positional encoding torepresent objects in a BEV, achieving competitive performance compared totraditional DNN-based methods. While the BEV-based approach loses some detailedinformation inherent to front-view images, we balance this by enriching the BEVdata by representing it as a graph where relationships between the objects in ascene are captured effectively.</description><author>Sushil Sharma, Aryan Singh, Ganesh Sistu, Mark Halton, Ciarán Eising</author><pubDate>Wed, 20 Dec 2023 15:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13104v1</guid></item><item><title>Achieving ${O}(ε^{-1.5})$ Complexity in Hessian/Jacobian-free Stochastic Bilevel Optimization</title><link>http://arxiv.org/abs/2312.03807v2</link><description>In this paper, we revisit the bilevel optimization problem, in which theupper-level objective function is generally nonconvex and the lower-levelobjective function is strongly convex. Although this type of problem has beenstudied extensively, it still remains an open question how to achieve an${O}(\epsilon^{-1.5})$ sample complexity in Hessian/Jacobian-free stochasticbilevel optimization without any second-order derivative computation. To fillthis gap, we propose a novel Hessian/Jacobian-free bilevel optimizer namedFdeHBO, which features a simple fully single-loop structure, a projection-aidedfinite-difference Hessian/Jacobian-vector approximation, and momentum-basedupdates. Theoretically, we show that FdeHBO requires ${O}(\epsilon^{-1.5})$iterations (each using ${O}(1)$ samples and only first-order gradientinformation) to find an $\epsilon$-accurate stationary point. As far as weknow, this is the first Hessian/Jacobian-free method with an${O}(\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convexstochastic bilevel optimization.</description><author>Yifan Yang, Peiyao Xiao, Kaiyi Ji</author><pubDate>Wed, 20 Dec 2023 15:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03807v2</guid></item><item><title>Exploring Multimodal Large Language Models for Radiology Report Error-checking</title><link>http://arxiv.org/abs/2312.13103v1</link><description>This paper proposes one of the first clinical applications of multimodallarge language models (LLMs) as an assistant for radiologists to check errorsin their reports. We created an evaluation dataset from two real-worldradiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.A subset of original reports was modified to contain synthetic errors byintroducing various type of mistakes. The evaluation contained two difficultylevels: SIMPLE for binary error-checking and COMPLEX for identifying errortypes. LLaVA (Large Language and Visual Assistant) variant models, includingour instruction-tuned model, were used for the evaluation. Additionally, adomain expert evaluation was conducted on a small test set. At the SIMPLElevel, the LLaVA v1.5 model outperformed other publicly available models.Instruction tuning significantly enhanced performance by 47.4% and 25.4% onMIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domainexperts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets(N=21) of the test set where a clinician did not achieve the correctconclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.This study marks a promising step toward utilizing multi-modal LLMs to enhancediagnostic accuracy in radiology. The ensemble model demonstrated comparableperformance to clinicians, even capturing errors overlooked by humans.Nevertheless, future work is needed to improve the model ability to identifythe types of inconsistency.</description><author>Jinge Wu, Yunsoo Kim, Eva C. Keller, Jamie Chow, Adam P. Levine, Nikolas Pontikos, Zina Ibrahim, Paul Taylor, Michelle C. Williams, Honghan Wu</author><pubDate>Wed, 20 Dec 2023 15:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13103v1</guid></item><item><title>SpecNeRF: Gaussian Directional Encoding for Specular Reflections</title><link>http://arxiv.org/abs/2312.13102v1</link><description>Neural radiance fields have achieved remarkable performance in modeling theappearance of 3D scenes. However, existing approaches still struggle with theview-dependent appearance of glossy surfaces, especially under complex lightingof indoor environments. Unlike existing methods, which typically assume distantlighting like an environment map, we propose a learnable Gaussian directionalencoding to better model the view-dependent effects under near-field lightingconditions. Importantly, our new directional encoding captures thespatially-varying nature of near-field lighting and emulates the behavior ofprefiltered environment maps. As a result, it enables the efficient evaluationof preconvolved specular color at any 3D location with varying roughnesscoefficients. We further introduce a data-driven geometry prior that helpsalleviate the shape radiance ambiguity in reflection modeling. We show that ourGaussian directional encoding and geometry prior significantly improve themodeling of challenging specular reflections in neural radiance fields, whichhelps decompose appearance into more physically meaningful components.</description><author>Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro Sander, Michael Zollhöfer, Christian Richardt</author><pubDate>Wed, 20 Dec 2023 15:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13102v1</guid></item><item><title>SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized Zero-Shot Learning</title><link>http://arxiv.org/abs/2312.13100v1</link><description>Generalized Zero-Shot Learning (GZSL) recognizes unseen classes bytransferring knowledge from the seen classes, depending on the inherentinteractions between visual and semantic data. However, the discrepancy betweenwell-prepared training data and unpredictable real-world test scenarios remainsa significant challenge. This paper introduces a dual strategy to address thegeneralization gap. Firstly, we incorporate semantic information through aninnovative encoder. This encoder effectively integrates class-specific semanticinformation by targeting the performance disparity, enhancing the producedfeatures to enrich the semantic space for class-specific attributes. Secondly,we refine our generative capabilities using a novel compositional lossfunction. This approach generates discriminative classes, effectivelyclassifying both seen and unseen classes. In addition, we extend theexploitation of the learned latent space by utilizing controlled semanticinputs, ensuring the robustness of the model in varying environments. Thisapproach yields a model that outperforms the state-of-the-art models in termsof both generalization and diverse settings, notably without requiringhyperparameter tuning or domain-specific adaptations. We also propose a set ofnovel evaluation metrics to provide a more detailed assessment of thereliability and reproducibility of the results. The complete code is madeavailable on https://github.com/william-heyden/SEER-ZeroShotLearning/.</description><author>William Heyden, Habib Ullah, M. Salman Siddiqui, Fadi Al Machot</author><pubDate>Wed, 20 Dec 2023 15:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13100v1</guid></item><item><title>In Generative AI we Trust: Can Chatbots Effectively Verify Political Information?</title><link>http://arxiv.org/abs/2312.13096v1</link><description>This article presents a comparative analysis of the ability of two largelanguage model (LLM)-based chatbots, ChatGPT and Bing Chat, recently rebrandedto Microsoft Copilot, to detect veracity of political information. We use AIauditing methodology to investigate how chatbots evaluate true, false, andborderline statements on five topics: COVID-19, Russian aggression againstUkraine, the Holocaust, climate change, and LGBTQ+ related debates. We comparehow the chatbots perform in high- and low-resource languages by using promptsin English, Russian, and Ukrainian. Furthermore, we explore the ability ofchatbots to evaluate statements according to political communication conceptsof disinformation, misinformation, and conspiracy theory, usingdefinition-oriented prompts. We also systematically test how such evaluationsare influenced by source bias which we model by attributing specific claims tovarious political and social actors. The results show high performance ofChatGPT for the baseline veracity evaluation task, with 72 percent of the casesevaluated correctly on average across languages without pre-training. Bing Chatperformed worse with a 67 percent accuracy. We observe significant disparitiesin how chatbots evaluate prompts in high- and low-resource languages and howthey adapt their evaluations to political communication concepts with ChatGPTproviding more nuanced outputs than Bing Chat. Finally, we find that for someveracity detection-related tasks, the performance of chatbots varied dependingon the topic of the statement or the source to which it is attributed. Thesefindings highlight the potential of LLM-based chatbots in tackling differentforms of false information in online environments, but also points to thesubstantial variation in terms of how such potential is realized due tospecific factors, such as language of the prompt or the topic.</description><author>Elizaveta Kuznetsova, Mykola Makhortykh, Victoria Vziatysheva, Martha Stolze, Ani Baghumyan, Aleksandra Urman</author><pubDate>Wed, 20 Dec 2023 15:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13096v1</guid></item><item><title>OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments</title><link>http://arxiv.org/abs/2312.12145v2</link><description>In reinforcement learning, the optimism in the face of uncertainty (OFU) is amainstream principle for directing exploration towards less explored areas,characterized by higher uncertainty. However, in the presence of environmentalstochasticity (noise), purely optimistic exploration may lead to excessiveprobing of high-noise areas, consequently impeding exploration efficiency.Hence, in exploring noisy environments, while optimism-driven explorationserves as a foundation, prudent attention to alleviating unnecessaryover-exploration in high-noise areas becomes beneficial. In this work, wepropose Optimistic Value Distribution Explorer (OVD-Explorer) to achieve anoise-aware optimistic exploration for continuous control. OVD-Explorerproposes a new measurement of the policy's exploration ability consideringnoise in optimistic perspectives, and leverages gradient ascent to driveexploration. Practically, OVD-Explorer can be easily integrated with continuouscontrol RL algorithms. Extensive evaluations on the MuJoCo and GridChaos tasksdemonstrate the superiority of OVD-Explorer in achieving noise-aware optimisticexploration.</description><author>Jinyi Liu, Zhi Wang, Yan Zheng, Jianye Hao, Chenjia Bai, Junjie Ye, Zhen Wang, Haiyin Piao, Yang Sun</author><pubDate>Wed, 20 Dec 2023 15:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12145v2</guid></item><item><title>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading</title><link>http://arxiv.org/abs/2312.13091v1</link><description>Reconstructing an avatar from a portrait image has many applications inmultimedia, but remains a challenging research problem. Extracting reflectancemaps and geometry from one image is ill-posed: recovering geometry is aone-to-many mapping problem and reflectance and light are difficult todisentangle. Accurate geometry and reflectance can be captured under thecontrolled conditions of a light stage, but it is costly to acquire largedatasets in this fashion. Moreover, training solely with this type of dataleads to poor generalization with in-the-wild images. This motivates theintroduction of MoSAR, a method for 3D avatar generation from monocular images.We propose a semi-supervised training scheme that improves generalization bylearning from both light stage and in-the-wild datasets. This is achieved usinga novel differentiable shading formulation. We show that our approacheffectively disentangles the intrinsic face parameters, producing relightableavatars. As a result, MoSAR estimates a richer set of skin reflectance maps,and generates more realistic avatars than existing state-of-the-art methods. Wealso introduce a new dataset, named FFHQ-UV-Intrinsics, the first publicdataset providing intrisic face attributes at scale (diffuse, specular, ambientocclusion and translucency maps) for a total of 10k subjects. The projectwebsite and the dataset are available on the following link:https://ubisoftlaforge.github.io/character/mosar</description><author>Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau</author><pubDate>Wed, 20 Dec 2023 15:12:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13091v1</guid></item><item><title>Perception Test 2023: A Summary of the First Challenge And Outcome</title><link>http://arxiv.org/abs/2312.13090v1</link><description>The First Perception Test challenge was held as a half-day workshop alongsidethe IEEE/CVF International Conference on Computer Vision (ICCV) 2023, with thegoal of benchmarking state-of-the-art video models on the recently proposedPerception Test benchmark. The challenge had six tracks covering low-level andhigh-level tasks, with both a language and non-language interface, acrossvideo, audio, and text modalities, and covering: object tracking, pointtracking, temporal action localisation, temporal sound localisation,multiple-choice video question-answering, and grounded videoquestion-answering. We summarise in this report the task descriptions, metrics,baselines, and results.</description><author>Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean</author><pubDate>Wed, 20 Dec 2023 15:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13090v1</guid></item><item><title>Uncertainty-aware Unsupervised Multi-Object Tracking</title><link>http://arxiv.org/abs/2307.15409v2</link><description>Without manually annotated identities, unsupervised multi-object trackers areinferior to learning reliable feature embeddings. It causes thesimilarity-based inter-frame association stage also be error-prone, where anuncertainty problem arises. The frame-by-frame accumulated uncertainty preventstrackers from learning the consistent feature embedding against time variation.To avoid this uncertainty problem, recent self-supervised techniques areadopted, whereas they failed to capture temporal relations. The interframeuncertainty still exists. In fact, this paper argues that though theuncertainty problem is inevitable, it is possible to leverage the uncertaintyitself to improve the learned consistency in turn. Specifically, anuncertainty-based metric is developed to verify and rectify the riskyassociations. The resulting accurate pseudo-tracklets boost learning thefeature consistency. And accurate tracklets can incorporate temporalinformation into spatial transformation. This paper proposes a tracklet-guidedaugmentation strategy to simulate tracklets' motion, which adopts ahierarchical uncertainty-based sampling mechanism for hard sample mining. Theultimate unsupervised MOT framework, namely U2MOT, is proven effective onMOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performanceamong the published supervised and unsupervised trackers.</description><author>Kai Liu, Sheng Jin, Zhihang Fu, Ze Chen, Rongxin Jiang, Jieping Ye</author><pubDate>Wed, 20 Dec 2023 15:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15409v2</guid></item><item><title>Exploiting Representation Bias for Data Distillation in Abstractive Text Summarization</title><link>http://arxiv.org/abs/2312.06022v2</link><description>Abstractive text summarization is surging with the number of training samplesto cater to the needs of the deep learning models. These models tend to exploitthe training data representations to attain superior performance by improvingthe quantitative element of the resultant summary. However, increasing the sizeof the training set may not always be the ideal solution to maximize theperformance, and therefore, a need to revisit the quality of training samplesand the learning protocol of deep learning models is a must. In this paper, weaim to discretize the vector space of the abstractive text summarization modelsto understand the characteristics learned between the input embedding space andthe models' encoder space. We show that deep models fail to capture thediversity of the input space. Further, the distribution of data points on theencoder space indicates that an unchecked increase in the training samples doesnot add value; rather, a tear-down of data samples is highly needed to make themodels focus on variability and faithfulness. We employ clustering techniquesto learn the diversity of a model's sample space and how data points are mappedfrom the embedding space to the encoder space and vice versa. Further, wedevise a metric to filter out redundant data points to make the model morerobust and less data hungry. We benchmark our proposed method usingquantitative metrics, such as Rouge, and qualitative metrics, such asBERTScore, FEQA and Pyramid score. We also quantify the reasons that inhibitthe models from learning the diversity from the varied input samples.</description><author>Yash Kumar Atri, Vikram Goyal, Tanmoy Chakraborty</author><pubDate>Wed, 20 Dec 2023 15:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06022v2</guid></item><item><title>Forecasting Trends in Food Security: a Reservoir Computing Approach</title><link>http://arxiv.org/abs/2312.00626v2</link><description>Early warning systems are an essential tool for effective humanitarianaction. Advance warnings on impending disasters facilitate timely and targetedresponse which help save lives, livelihoods, and scarce financial resources. Inthis work we present a new quantitative methodology to forecast levels of foodconsumption for 60 consecutive days, at the sub-national level, in fourcountries: Mali, Nigeria, Syria, and Yemen. The methodology is built onpublicly available data from the World Food Programme's integrated globalhunger monitoring system which collects, processes, and displays daily updateson key food security metrics, conflict, weather events, and other drivers offood insecurity across 90 countries (https://hungermap.wfp.org/). In thisstudy, we assessed the performance of various models including ARIMA, XGBoost,LSTMs, CNNs, and Reservoir Computing (RC), by comparing their Root Mean SquaredError (RMSE) metrics. This comprehensive analysis spanned classicalstatistical, machine learning, and deep learning approaches. Our findingshighlight Reservoir Computing as a particularly well-suited model in the fieldof food security given both its notable resistance to over-fitting on limiteddata samples and its efficient training capabilities. The methodology weintroduce establishes the groundwork for a global, data-driven early warningsystem designed to anticipate and detect food insecurity.</description><author>Joschka Herteux, Christoph Räth, Amine Baha, Giulia Martini, Duccio Piovani</author><pubDate>Wed, 20 Dec 2023 15:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00626v2</guid></item><item><title>Pyreal: A Framework for Interpretable ML Explanations</title><link>http://arxiv.org/abs/2312.13084v1</link><description>Users in many domains use machine learning (ML) predictions to help them makedecisions. Effective ML-based decision-making often requires explanations of MLmodels and their predictions. While there are many algorithms that explainmodels, generating explanations in a format that is comprehensible and usefulto decision-makers is a nontrivial task that can require extensive developmentoverhead. We developed Pyreal, a highly extensible system with a correspondingPython implementation for generating a variety of interpretable MLexplanations. Pyreal converts data and explanations between the feature spacesexpected by the model, relevant explanation algorithms, and human users,allowing users to generate interpretable explanations in a low-code manner. Ourstudies demonstrate that Pyreal generates more useful explanations thanexisting systems while remaining both easy-to-use and efficient.</description><author>Alexandra Zytek, Wei-En Wang, Dongyu Liu, Laure Berti-Equille, Kalyan Veeramachaneni</author><pubDate>Wed, 20 Dec 2023 15:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13084v1</guid></item><item><title>BEVSeg2TP: Surround View Camera Bird's-Eye-View Based Joint Vehicle Segmentation and Ego Vehicle Trajectory Prediction</title><link>http://arxiv.org/abs/2312.13081v1</link><description>Trajectory prediction is, naturally, a key task for vehicle autonomy. Whilethe number of traffic rules is limited, the combinations and uncertaintiesassociated with each agent's behaviour in real-world scenarios are nearlyimpossible to encode. Consequently, there is a growing interest inlearning-based trajectory prediction. The proposed method in this paperpredicts trajectories by considering perception and trajectory prediction as aunified system. In considering them as unified tasks, we show that there is thepotential to improve the performance of perception. To achieve these goals, wepresent BEVSeg2TP - a surround-view camera bird's-eye-view-based joint vehiclesegmentation and ego vehicle trajectory prediction system for autonomousvehicles. The proposed system uses a network trained on multiple camera views.The images are transformed using several deep learning techniques to performsemantic segmentation of objects, including other vehicles, in the scene. Thesegmentation outputs are fused across the camera views to obtain acomprehensive representation of the surrounding vehicles from thebird's-eye-view perspective. The system further predicts the future trajectoryof the ego vehicle using a spatiotemporal probabilistic network (STPN) tooptimize trajectory prediction. This network leverages information fromencoder-decoder transformers and joint vehicle segmentation.</description><author>Sushil Sharma, Arindam Das, Ganesh Sistu, Mark Halton, Ciarán Eising</author><pubDate>Wed, 20 Dec 2023 15:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13081v1</guid></item><item><title>Covariance Adaptive Best Arm Identification</title><link>http://arxiv.org/abs/2306.02630v2</link><description>We consider the problem of best arm identification in the multi-armed banditmodel, under fixed confidence. Given a confidence input $\delta$, the goal isto identify the arm with the highest mean reward with a probability of at least1 -- $\delta$, while minimizing the number of arm pulls. While the literatureprovides solutions to this problem under the assumption of independent armsdistributions, we propose a more flexible scenario where arms can be dependentand rewards can be sampled simultaneously. This framework allows the learner toestimate the covariance among the arms distributions, enabling a more efficientidentification of the best arm. The relaxed setting we propose is relevant invarious applications, such as clinical trials, where similarities betweenpatients or drugs suggest underlying correlations in the outcomes. We introducenew algorithms that adapt to the unknown covariance of the arms and demonstratethrough theoretical guarantees that substantial improvement can be achievedover the standard setting. Additionally, we provide new lower bounds for therelaxed setting and present numerical simulations that support theirtheoretical findings.</description><author>El Mehdi Saad, Gilles Blanchard, Nicolas Verzelen</author><pubDate>Wed, 20 Dec 2023 15:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02630v2</guid></item><item><title>SoftCorrect: Error Correction with Soft Detection for Automatic Speech Recognition</title><link>http://arxiv.org/abs/2212.01039v2</link><description>Error correction in automatic speech recognition (ASR) aims to correct thoseincorrect words in sentences generated by ASR models. Since recent ASR modelsusually have low word error rate (WER), to avoid affecting originally correcttokens, error correction models should only modify incorrect words, andtherefore detecting incorrect words is important for error correction. Previousworks on error correction either implicitly detect error words throughtarget-source attention or CTC (connectionist temporal classification) loss, orexplicitly locate specific deletion/substitution/insertion errors. However,implicit error detection does not provide clear signal about which tokens areincorrect and explicit error detection suffers from low detection accuracy. Inthis paper, we propose SoftCorrect with a soft error detection mechanism toavoid the limitations of both explicit and implicit error detection.Specifically, we first detect whether a token is correct or not through aprobability produced by a dedicatedly designed language model, and then designa constrained CTC loss that only duplicates the detected incorrect tokens tolet the decoder focus on the correction of error tokens. Compared with impliciterror detection with CTC loss, SoftCorrect provides explicit signal about whichwords are incorrect and thus does not need to duplicate every token but onlyincorrect tokens; compared with explicit error detection, SoftCorrect does notdetect specific deletion/substitution/insertion errors but just leaves it toCTC loss. Experiments on AISHELL-1 and Aidatatang datasets show thatSoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperformingprevious works by a large margin, while still enjoying fast speed of parallelgeneration.</description><author>Yichong Leng, Xu Tan, Wenjie Liu, Kaitao Song, Rui Wang, Xiang-Yang Li, Tao Qin, Edward Lin, Tie-Yan Liu</author><pubDate>Wed, 20 Dec 2023 15:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01039v2</guid></item><item><title>"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA</title><link>http://arxiv.org/abs/2312.11193v3</link><description>Although LLMs continue to iterate and improve, most open-source models stillhave a context window of no more than 4k, limiting their ability to handlelong-context problems. Most existing open-source models for long-context chatstill lack satisfactory accuracy. To address this issue, I approach it from theperspective of training data and theoretically prove that training thecapability to handle long contexts requires "effective" rather than "long"data. Based on this, I propose using the "original text paraphrase" task, andsuccessfully extend the context window of the existing model to 32k by alow-cost and effective method, achieving extremely high accuracy inmulti-document-QA and surpassing all existing open-source models of the samescale. The model and training data have been open-sourced onHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) andWiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).</description><author>Yijiong Yu</author><pubDate>Wed, 20 Dec 2023 14:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11193v3</guid></item><item><title>Functional Mixtures-of-Experts</title><link>http://arxiv.org/abs/2202.02249v2</link><description>We consider the statistical analysis of heterogeneous data for prediction insituations where the observations include functions, typically time series. Weextend the modeling with Mixtures-of-Experts (ME), as a framework of choice inmodeling heterogeneity in data for prediction with vectorial observations, tothis functional data analysis context. We first present a new family of MEmodels, named functional ME (FME) in which the predictors are potentially noisyobservations, from entire functions. Furthermore, the data generating processof the predictor and the real response, is governed by a hidden discretevariable representing an unknown partition. Second, by imposing sparsity onderivatives of the underlying functional parameters via Lasso-likeregularizations, we provide sparse and interpretable functional representationsof the FME models called iFME. We develop dedicated expectation--maximizationalgorithms for Lasso-like (EM-Lasso) regularized maximum-likelihood parameterestimation strategies to fit the models. The proposed models and algorithms arestudied in simulated scenarios and in applications to two real data sets, andthe obtained results demonstrate their performance in accurately capturingcomplex nonlinear relationships and in clustering the heterogeneous regressiondata.</description><author>Faïcel Chamroukhi, Nhat Thien Pham, Van Hà Hoang, Geoffrey J. McLachlan</author><pubDate>Wed, 20 Dec 2023 14:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.02249v2</guid></item><item><title>MADiff: Offline Multi-agent Learning with Diffusion Models</title><link>http://arxiv.org/abs/2305.17330v3</link><description>Diffusion model (DM), as a powerful generative model, recently achieved hugesuccess in various scenarios including offline reinforcement learning, wherethe policy learns to conduct planning by generating trajectory in the onlineevaluation. However, despite the effectiveness shown for single-agent learning,it remains unclear how DMs can operate in multi-agent problems, where agentscan hardly complete teamwork without good coordination by independentlymodeling each agent's trajectories. In this paper, we propose MADiff, a novelgenerative multi-agent learning framework to tackle this problem. MADiff isrealized with an attention-based diffusion model to model the complexcoordination among behaviors of multiple diffusion agents. To the best of ourknowledge, MADiff is the first diffusion-based multi-agent offline RLframework, which behaves as both a decentralized policy and a centralizedcontroller. During decentralized executions, MADiff simultaneously performsteammate modeling, and the centralized controller can also be applied inmulti-agent trajectory predictions. Our experiments show the superiorperformance of MADiff compared to baseline algorithms in a wide range ofmulti-agent learning tasks, which emphasizes the effectiveness of MADiff inmodeling complex multi-agent interactions. Our code is available athttps://github.com/zbzhu99/madiff.</description><author>Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon, Weinan Zhang</author><pubDate>Wed, 20 Dec 2023 14:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17330v3</guid></item><item><title>Point Deformable Network with Enhanced Normal Embedding for Point Cloud Analysis</title><link>http://arxiv.org/abs/2312.13071v1</link><description>Recently MLP-based methods have shown strong performance in point cloudanalysis. Simple MLP architectures are able to learn geometric features inlocal point groups yet fail to model long-range dependencies directly. In thispaper, we propose Point Deformable Network (PDNet), a concise MLP-based networkthat can capture long-range relations with strong representation ability.Specifically, we put forward Point Deformable Aggregation Module (PDAM) toimprove representation capability in both long-range dependency and adaptiveaggregation among points. For each query point, PDAM aggregates informationfrom deformable reference points rather than points in limited local areas. Thedeformable reference points are generated data-dependent, and we initializethem according to the input point positions. Additional offsets and modulationscalars are learned on the whole point features, which shift the deformablereference points to the regions of interest. We also suggest estimating thenormal vector for point clouds and applying Enhanced Normal Embedding (ENE) tothe geometric extractors to improve the representation ability of single-point.Extensive experiments and ablation studies on various benchmarks demonstratethe effectiveness and superiority of our PDNet.</description><author>Xingyilang Yin, Xi Yang, Liangchen Liu, Nannan Wang, Xinbo Gao</author><pubDate>Wed, 20 Dec 2023 14:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13071v1</guid></item><item><title>Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation</title><link>http://arxiv.org/abs/2212.06370v3</link><description>Accurate uncertainty quantification is necessary to enhance the reliabilityof deep learning models in real-world applications. In the case of regressiontasks, prediction intervals (PIs) should be provided along with thedeterministic predictions of deep learning models. Such PIs are useful or"high-quality" as long as they are sufficiently narrow and capture most of theprobability density. In this paper, we present a method to learn predictionintervals for regression-based neural networks automatically in addition to theconventional target predictions. In particular, we train two companion neuralnetworks: one that uses one output, the target estimate, and another that usestwo outputs, the upper and lower bounds of the corresponding PI. Our maincontribution is the design of a novel loss function for the PI-generationnetwork that takes into account the output of the target-estimation network andhas two optimization objectives: minimizing the mean prediction interval widthand ensuring the PI integrity using constraints that maximize the predictioninterval probability coverage implicitly. Furthermore, we introduce aself-adaptive coefficient that balances both objectives within the lossfunction, which alleviates the task of fine-tuning. Experiments using asynthetic dataset, eight benchmark datasets, and a real-world crop yieldprediction dataset showed that our method was able to maintain a nominalprobability coverage and produce significantly narrower PIs without detrimentto its target estimation accuracy when compared to those PIs generated by threestate-of-the-art neural-network-based methods. In other words, our method wasshown to produce higher-quality PIs.</description><author>Giorgio Morales, John W. Sheppard</author><pubDate>Wed, 20 Dec 2023 14:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06370v3</guid></item><item><title>Continuous-time Graph Representation with Sequential Survival Process</title><link>http://arxiv.org/abs/2312.13068v1</link><description>Over the past two decades, there has been a tremendous increase in the growthof representation learning methods for graphs, with numerous applicationsacross various fields, including bioinformatics, chemistry, and the socialsciences. However, current dynamic network approaches focus on discrete-timenetworks or treat links in continuous-time networks as instantaneous events.Therefore, these approaches have limitations in capturing the persistence orabsence of links that continuously emerge and disappear over time forparticular durations. To address this, we propose a novel stochastic processrelying on survival functions to model the durations of links and theirabsences over time. This forms a generic new likelihood specificationexplicitly accounting for intermittent edge-persistent networks, namely GraSSP:Graph Representation with Sequential Survival Process. We apply the developedframework to a recent continuous time dynamic latent distance modelcharacterizing network dynamics in terms of a sequence of piecewise linearmovements of nodes in latent space. We quantitatively assess the developedframework in various downstream tasks, such as link prediction and networkcompletion, demonstrating that the developed modeling framework accounting forlink persistence and absence well tracks the intrinsic trajectories of nodes ina latent space and captures the underlying characteristics of evolving networkstructure.</description><author>Abdulkadir Celikkanat, Nikolaos Nakis, Morten Mørup</author><pubDate>Wed, 20 Dec 2023 14:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13068v1</guid></item><item><title>PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation</title><link>http://arxiv.org/abs/2312.13066v1</link><description>Self-supervised monocular depth estimation is of significant importance withapplications spanning across autonomous driving and robotics. However, thereliance on self-supervision introduces a strong static-scene assumption,thereby posing challenges in achieving optimal performance in dynamic scenes,which are prevalent in most real-world situations. To address these issues, wepropose PPEA-Depth, a Progressive Parameter-Efficient Adaptation approach totransfer a pre-trained image model for self-supervised depth estimation. Thetraining comprises two sequential stages: an initial phase trained on a datasetprimarily composed of static scenes, succeeded by an expansion to moreintricate datasets involving dynamic scenes. To facilitate this process, wedesign compact encoder and decoder adapters to enable parameter-efficienttuning, allowing the network to adapt effectively. They not only upholdgeneralized patterns from pre-trained image models but also retain knowledgegained from the preceding phase into the subsequent one. Extensive experimentsdemonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI,CityScapes and DDAD datasets.</description><author>Yue-Jiang Dong, Yuan-Chen Guo, Ying-Tian Liu, Fang-Lue Zhang, Song-Hai Zhang</author><pubDate>Wed, 20 Dec 2023 14:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13066v1</guid></item><item><title>Recursively-Constrained Partially Observable Markov Decision Processes</title><link>http://arxiv.org/abs/2310.09688v2</link><description>In many problems, it is desirable to optimize an objective function whileimposing constraints on some other objectives. A Constrained PartiallyObservable Markov Decision Process (C-POMDP) allows modeling of such problemsunder transition uncertainty and partial observability. Typically, theconstraints in C-POMDPs enforce a threshold on expected cumulative costsstarting from an initial state distribution. In this work, we first show thatoptimal C-POMDP policies may violate Bellman's principle of optimality and thusmay exhibit unintuitive behaviors, which can be undesirable for some (e.g.,safety critical) applications. Additionally, online re-planning with C-POMDPsis often ineffective due to the inconsistency resulting from the violation ofBellman's principle of optimality. To address these drawbacks, we introduce anew formulation: the Recursively-Constrained POMDP (RC-POMDP), that imposesadditional history-dependent cost constraints on the C-POMDP. We show that,unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies, and thatoptimal policies obey Bellman's principle of optimality. We also present apoint-based dynamic programming algorithm that synthesizes admissiblenear-optimal policies for RC-POMDPs. Evaluations on a set of benchmark problemsdemonstrate the efficacy of our algorithm and show that policies for RC-POMDPsproduce more desirable behaviors than policies for C-POMDPs.</description><author>Qi Heng Ho, Tyler Becker, Benjamin Kraske, Zakariya Laouar, Martin S. Feather, Federico Rossi, Morteza Lahijanian, Zachary N. Sunberg</author><pubDate>Wed, 20 Dec 2023 14:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09688v2</guid></item><item><title>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</title><link>http://arxiv.org/abs/2311.14521v4</link><description>3D editing plays a crucial role in many areas such as gaming and virtualreality. Traditional 3D editing methods, which rely on representations likemeshes and point clouds, often fall short in realistically depicting complexscenes. On the other hand, methods based on implicit 3D representations, likeNeural Radiance Field (NeRF), render complex scenes effectively but suffer fromslow processing speeds and limited control over specific scene areas. Inresponse to these challenges, our paper presents GaussianEditor, an innovativeand efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3Drepresentation. GaussianEditor enhances precision and control in editingthrough our proposed Gaussian semantic tracing, which traces the editing targetthroughout the training process. Additionally, we propose Hierarchical Gaussiansplatting (HGS) to achieve stabilized and fine results under stochasticgenerative guidance from 2D diffusion models. We also develop editingstrategies for efficient object removal and integration, a challenging task forexisting methods. Our comprehensive experiments demonstrate GaussianEditor'ssuperior control, efficacy, and rapid performance, marking a significantadvancement in 3D editing. Project Page:https://buaacyw.github.io/gaussian-editor/</description><author>Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</author><pubDate>Wed, 20 Dec 2023 14:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14521v4</guid></item><item><title>Graph Neural Network-based EEG Classification: A Survey</title><link>http://arxiv.org/abs/2310.02152v2</link><description>Graph neural networks (GNN) are increasingly used to classify EEG for taskssuch as emotion recognition, motor imagery and neurological diseases anddisorders. A wide range of methods have been proposed to design GNN-basedclassifiers. Therefore, there is a need for a systematic review andcategorisation of these approaches. We exhaustively search the publishedliterature on this topic and derive several categories for comparison. Thesecategories highlight the similarities and differences among the methods. Theresults suggest a prevalence of spectral graph convolutional layers overspatial. Additionally, we identify standard forms of node features, with themost popular being the raw EEG signal and differential entropy. Our resultssummarise the emerging trends in GNN-based approaches for EEG classification.Finally, we discuss several promising research directions, such as exploringthe potential of transfer learning methods and appropriate modelling ofcross-frequency interactions.</description><author>Dominik Klepl, Min Wu, Fei He</author><pubDate>Wed, 20 Dec 2023 14:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02152v2</guid></item><item><title>Quantifying Bias in Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2312.13053v1</link><description>Bias in text-to-image (T2I) models can propagate unfair socialrepresentations and may be used to aggressively market ideas or pushcontroversial agendas. Existing T2I model bias evaluation methods only focus onsocial biases. We look beyond that and instead propose an evaluationmethodology to quantify general biases in T2I generative models, without anypreconceived notions. We assess four state-of-the-art T2I models and comparetheir baseline bias characteristics to their respective variants (two foreach), where certain biases have been intentionally induced. We propose threeevaluation metrics to assess model biases including: (i) Distribution bias,(ii) Jaccard hallucination and (iii) Generative miss-rate. We conduct twoevaluation studies, modelling biases under general, and task-orientedconditions, using a marketing scenario as the domain for the latter. We alsoquantify social biases to compare our findings to related works. Finally, ourmethodology is transferred to evaluate captioned-image datasets and measuretheir bias. Our approach is objective, domain-agnostic and consistentlymeasures different forms of T2I model biases. We have developed a webapplication and practical implementation of what has been proposed in thiswork, which is at https://huggingface.co/spaces/JVice/try-before-you-bias. Avideo series with demonstrations is available athttps://www.youtube.com/channel/UCk-0xyUyT0MSd_hkp4jQt1Q</description><author>Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian</author><pubDate>Wed, 20 Dec 2023 14:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13053v1</guid></item><item><title>Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing</title><link>http://arxiv.org/abs/2301.03713v3</link><description>Human respiratory rate and its pattern convey essential information about thephysical and psychological states of the subject. Abnormal breathing canindicate fatal health issues leading to further diagnosis and treatment.Wireless light-wave sensing (LWS) using incoherent infrared light shows promisein safe, discreet, efficient, and non-invasive human breathing monitoringwithout raising privacy concerns. The respiration monitoring system needs to betrained on different types of breathing patterns to identify breathinganomalies.The system must also validate the collected data as a breathingwaveform, discarding any faulty data caused by external interruption, usermovement, or system malfunction. To address these needs, this study simulatednormal and different types of abnormal respiration using a robot that mimicshuman breathing patterns. Then, time-series respiration data were collectedusing infrared light-wave sensing technology. Three machine learningalgorithms, decision tree, random forest and XGBoost, were applied to detectbreathing anomalies and faulty data. Model performances were evaluated throughcross-validation, assessing classification accuracy, precision and recallscores. The random forest model achieved the highest classification accuracy of96.75% with data collected at a 0.5m distance. In general, ensemble models likerandom forest and XGBoost performed better than a single model in classifyingthe data collected at multiple distances from the light-wave sensing setup.</description><author>Md Zobaer Islam, Brenden Martin, Carly Gotcher, Tyler Martinez, John F. O'Hara, Sabit Ekin</author><pubDate>Wed, 20 Dec 2023 14:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03713v3</guid></item><item><title>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities</title><link>http://arxiv.org/abs/2307.13986v2</link><description>Purpose: Manual annotations for training deep learning (DL) models inauto-segmentation are time-intensive. This study introduces a hybridrepresentation-enhanced sampling strategy that integrates both density anddiversity criteria within an uncertainty-based Bayesian active learning (BAL)framework to reduce annotation efforts by selecting the most informativetraining samples. Methods: The experiments are performed on two lower extremity(LE) datasets of MRI and CT images, focusing on the segmentation of the femur,pelvis, sacrum, quadriceps femoris, hamstrings, adductors, sartorius, andiliopsoas, utilizing a U-net-based BAL framework. Our method selects uncertainsamples with high density and diversity for manual revision, optimizing formaximal similarity to unlabeled instances and minimal similarity to existingtraining data. We assess the accuracy and efficiency using Dice and a proposedmetric called reduced annotation cost (RAC), respectively. We further evaluatethe impact of various acquisition rules on BAL performance and design anablation study for effectiveness estimation. Results: In MRI and CT datasets,our method was superior or comparable to existing ones, achieving a 0.8\% Diceand 1.0\% RAC increase in CT (statistically significant), and a 0.8\% Dice and1.1\% RAC increase in MRI (not statistically significant) in volume-wiseacquisition. Our ablation study indicates that combining density and diversitycriteria enhances the efficiency of BAL in musculoskeletal segmentationcompared to using either criterion alone. Conclusion: Our sampling method isproven efficient in reducing annotation costs in image segmentation tasks. Thecombination of the proposed method and our BAL framework provides asemi-automatic way for efficient annotation of medical image datasets.</description><author>Ganping Li, Yoshito Otake, Mazen Soufi, Masashi Taniguchi, Masahide Yagi, Noriaki Ichihashi, Keisuke Uemura, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato</author><pubDate>Wed, 20 Dec 2023 14:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13986v2</guid></item><item><title>What Makes Forest-Based Heterogeneous Treatment Effect Estimators Work?</title><link>http://arxiv.org/abs/2206.10323v2</link><description>Estimation of heterogeneous treatment effects (HTE) is of prime importance inmany disciplines, ranging from personalized medicine to economics among manyothers. Random forests have been shown to be a flexible and powerful approachto HTE estimation in both randomized trials and observational studies. Inparticular "causal forests", introduced by Athey, Tibshirani and Wager (2019),along with the R implementation in package grf were rapidly adopted. A relatedapproach, called "model-based forests", that is geared towards randomizedtrials and simultaneously captures effects of both prognostic and predictivevariables, was introduced by Seibold, Zeileis and Hothorn (2018) along with amodular implementation in the R package model4you. Here, we present a unifying view that goes beyond the theoretical motivationsand investigates which computational elements make causal forests so successfuland how these can be blended with the strengths of model-based forests. To doso, we show that both methods can be understood in terms of the same parametersand model assumptions for an additive model under L2 loss. This theoreticalinsight allows us to implement several flavors of "model-based causal forests"and dissect their different elements in silico. The original causal forests and model-based forests are compared with the newblended versions in a benchmark study exploring both randomized trials andobservational settings. In the randomized setting, both approaches performedakin. If confounding was present in the data generating process, we found localcentering of the treatment indicator with the corresponding propensities to bethe main driver for good performance. Local centering of the outcome was lessimportant, and might be replaced or enhanced by simultaneous split selectionwith respect to both prognostic and predictive effects.</description><author>Susanne Dandl, Torsten Hothorn, Heidi Seibold, Erik Sverdrup, Stefan Wager, Achim Zeileis</author><pubDate>Wed, 20 Dec 2023 14:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10323v2</guid></item></channel></rss>