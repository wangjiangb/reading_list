<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 18 Oct 2023 14:00:48 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VeRA: Vector-based Random Matrix Adaptation</title><link>http://arxiv.org/abs/2310.11454v1</link><description>Low-rank adapation (LoRA) is a popular method that reduces the number oftrainable parameters when finetuning large language models, but still facesacute storage challenges when scaling to even larger models or deployingnumerous per-user or per-task adapted models. In this work, we presentVector-based Random Matrix Adaptation (VeRA), which reduces the number oftrainable parameters by 10x compared to LoRA, yet maintains the sameperformance. It achieves this by using a single pair of low-rank matricesshared across all layers and learning small scaling vectors instead. Wedemonstrate its effectiveness on the GLUE and E2E benchmarks, and show itsapplication in instruction-following with just 1.4M parameters using the Llama27B model.</description><author>Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki Markus Asano</author><pubDate>Tue, 17 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11454v1</guid></item><item><title>BitNet: Scaling 1-bit Transformers for Large Language Models</title><link>http://arxiv.org/abs/2310.11453v1</link><description>The increasing size of large language models has posed challenges fordeployment and raised concerns about environmental impact due to high energyconsumption. In this work, we introduce BitNet, a scalable and stable 1-bitTransformer architecture designed for large language models. Specifically, weintroduce BitLinear as a drop-in replacement of the nn.Linear layer in order totrain 1-bit weights from scratch. Experimental results on language modelingshow that BitNet achieves competitive performance while substantially reducingmemory footprint and energy consumption, compared to state-of-the-art 8-bitquantization methods and FP16 Transformer baselines. Furthermore, BitNetexhibits a scaling law akin to full-precision Transformers, suggesting itspotential for effective scaling to even larger language models whilemaintaining efficiency and performance benefits.</description><author>Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei</author><pubDate>Tue, 17 Oct 2023 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11453v1</guid></item><item><title>Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective</title><link>http://arxiv.org/abs/2310.11451v1</link><description>Large Language Models (LLMs) inherently encode a wealth of knowledge withintheir parameters through pre-training on extensive corpora. While priorresearch has delved into operations on these parameters to manipulate theunderlying implicit knowledge (encompassing detection, editing, and merging),there remains an ambiguous understanding regarding their transferability acrossmodels with varying scales. In this paper, we seek to empirically investigateknowledge transfer from larger to smaller models through a parametricperspective. To achieve this, we employ sensitivity-based techniques to extractand align knowledge-specific parameters between different LLMs. Moreover, theLoRA module is used as the intermediary mechanism for injecting the extractedknowledge into smaller models. Evaluations across four benchmarks validate theefficacy of our proposed method. Our findings highlight the critical factorscontributing to the process of parametric knowledge transfer, underscoring thetransferability of model parameters across LLMs of different scales. We releasecode and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.</description><author>Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He</author><pubDate>Tue, 17 Oct 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11451v1</guid></item><item><title>Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts</title><link>http://arxiv.org/abs/2310.11450v1</link><description>Concept-based explanation methods, such as Concept Activation Vectors, arepotent means to quantify how abstract or high-level characteristics of inputdata influence the predictions of complex deep neural networks. However,applying them to industrial prediction problems is challenging as it is notimmediately clear how to define and access appropriate concepts for individualuse cases and specific data types. In this work, we investigate how to leverageestablished concept-based explanation techniques in the context of bearingfault detection with deep neural networks trained on vibration signals. Sincebearings are prevalent in almost every rotating equipment, ensuring thereliability of intransparent fault detection models is crucial to preventcostly repairs and downtimes of industrial machinery. Our evaluationsdemonstrate that explaining opaque models in terms of vibration conceptsenables human-comprehensible and intuitive insights about their inner workings,but the underlying assumptions need to be carefully validated first.</description><author>Thomas Decker, Michael Lebacher, Volker Tresp</author><pubDate>Tue, 17 Oct 2023 18:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11450v1</guid></item><item><title>DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</title><link>http://arxiv.org/abs/2310.11449v1</link><description>Generating controllable and photorealistic digital human avatars is along-standing and important problem in Vision and Graphics. Recent methods haveshown great progress in terms of either photorealism or inference speed whilethe combination of the two desired properties still remains unsolved. To thisend, we propose a novel method, called DELIFFAS, which parameterizes theappearance of the human as a surface light field that is attached to acontrollable and deforming human mesh model. At the core, we represent thelight field around the human with a deformable two-surface parameterization,which enables fast and accurate inference of the human appearance. This allowsperceptual supervision on the full image compared to previous approaches thatcould only supervise individual pixels or small patches due to their slowruntime. Our carefully designed human representation and supervision strategyleads to state-of-the-art synthesis results and inference time. The videoresults and code are available athttps://vcai.mpi-inf.mpg.de/projects/DELIFFAS.</description><author>Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt</author><pubDate>Tue, 17 Oct 2023 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11449v1</guid></item><item><title>4K4D: Real-Time 4D View Synthesis at 4K Resolution</title><link>http://arxiv.org/abs/2310.11448v1</link><description>This paper targets high-fidelity and real-time view synthesis of dynamic 3Dscenes at 4K resolution. Recently, some methods on dynamic view synthesis haveshown impressive rendering quality. However, their speed is still limited whenrendering high-resolution images. To overcome this problem, we propose 4K4D, a4D point cloud representation that supports hardware rasterization and enablesunprecedented rendering speed. Our representation is built on a 4D feature gridso that the points are naturally regularized and can be robustly optimized. Inaddition, we design a novel hybrid appearance model that significantly booststhe rendering quality while preserving efficiency. Moreover, we develop adifferentiable depth peeling algorithm to effectively learn the proposed modelfrom RGB videos. Experiments show that our representation can be rendered atover 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on theENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30xfaster than previous methods and achieves the state-of-the-art renderingquality. We will release the code for reproducibility.</description><author>Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, Xiaowei Zhou</author><pubDate>Tue, 17 Oct 2023 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11448v1</guid></item><item><title>Functional Invariants to Watermark Large Transformers</title><link>http://arxiv.org/abs/2310.11446v1</link><description>The rapid growth of transformer-based models increases the concerns abouttheir integrity and ownership insurance. Watermarking addresses this issue byembedding a unique identifier into the model, while preserving its performance.However, most existing approaches require to optimize the weights to imprintthe watermark signal, which is not suitable at scale due to the computationalcost. This paper explores watermarks with virtually no computational cost,applicable to a non-blind white-box setting (assuming access to both theoriginal and watermarked networks). They generate functionally equivalentcopies by leveraging the models' invariance, via operations like dimensionpermutations or scaling/unscaling. This enables to watermark models without anychange in their outputs and remains stealthy. Experiments demonstrate theeffectiveness of the approach and its robustness against various modeltransformations (fine-tuning, quantization, pruning), making it a practicalsolution to protect the integrity of large models.</description><author>Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs</author><pubDate>Tue, 17 Oct 2023 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11446v1</guid></item><item><title>A Computational Framework for Solving Wasserstein Lagrangian Flows</title><link>http://arxiv.org/abs/2310.10649v2</link><description>The dynamical formulation of the optimal transport can be extended throughvarious choices of the underlying geometry ($\textit{kinetic energy}$), and theregularization of density paths ($\textit{potential energy}$). Thesecombinations yield different variational problems ($\textit{Lagrangians}$),encompassing many variations of the optimal transport problem such as theSchr\"odinger bridge, unbalanced optimal transport, and optimal transport withphysical constraints, among others. In general, the optimal density path isunknown, and solving these variational problems can be computationallychallenging. Leveraging the dual formulation of the Lagrangians, we propose anovel deep learning based framework approaching all of these problems from aunified perspective. Our method does not require simulating or backpropagatingthrough the trajectories of the learned dynamics, and does not need access tooptimal couplings. We showcase the versatility of the proposed framework byoutperforming previous approaches for the single-cell trajectory inference,where incorporating prior knowledge into the dynamics is crucial for correctpredictions.</description><author>Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, Alireza Makhzani</author><pubDate>Tue, 17 Oct 2023 18:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10649v2</guid></item><item><title>Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions</title><link>http://arxiv.org/abs/2310.11445v1</link><description>We present quantum algorithms for sampling from non-logconcave probabilitydistributions in the form of $\pi(x) \propto \exp(-\beta f(x))$. Here, $f$ canbe written as a finite sum $f(x):= \frac{1}{N}\sum_{k=1}^N f_k(x)$. Ourapproach is based on quantum simulated annealing on slowly varying Markovchains derived from unadjusted Langevin algorithms, removing the necessity forfunction evaluations which can be computationally expensive for large data setsin mixture modeling and multi-stable systems. We also incorporate a stochasticgradient oracle that implements the quantum walk operators inexactly by onlyusing mini-batch gradients. As a result, our stochastic gradient basedalgorithm only accesses small subsets of data points in implementing thequantum walk. One challenge of quantizing the resulting Markov chains is thatthey do not satisfy the detailed balance condition in general. Consequently,the mixing time of the algorithm cannot be expressed in terms of the spectralgap of the transition density, making the quantum algorithms nontrivial toanalyze. To overcome these challenges, we first build a hypothetical Markovchain that is reversible, and also converges to the target distribution. Then,we quantified the distance between our algorithm's output and the targetdistribution by using this hypothetical chain as a bridge to establish thetotal complexity. Our quantum algorithms exhibit polynomial speedups in termsof both dimension and precision dependencies when compared to the best-knownclassical algorithms.</description><author>Guneykan Ozgul, Xiantao Li, Mehrdad Mahdavi, Chunhao Wang</author><pubDate>Tue, 17 Oct 2023 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11445v1</guid></item><item><title>Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors</title><link>http://arxiv.org/abs/2309.16646v2</link><description>Dense depth and surface normal predictors should possess the equivariantproperty to cropping-and-resizing -- cropping the input image should result incropping the same output image. However, we find that state-of-the-art depthand normal predictors, despite having strong performances, surprisingly do notrespect equivariance. The problem exists even when crop-and-resize dataaugmentation is employed during training. To remedy this, we propose anequivariant regularization technique, consisting of an averaging procedure anda self-consistency loss, to explicitly promote cropping-and-resizingequivariance in depth and normal networks. Our approach can be applied to bothCNN and Transformer architectures, does not incur extra cost during testing,and notably improves the supervised and semi-supervised learning performance ofdense predictors on Taskonomy tasks. Finally, finetuning with our loss onunlabeled images improves not only equivariance but also accuracy ofstate-of-the-art depth and normal predictors when evaluated on NYU-v2. GitHublink: https://github.com/mikuhatsune/equivariance</description><author>Yuanyi Zhong, Anand Bhattad, Yu-Xiong Wang, David Forsyth</author><pubDate>Tue, 17 Oct 2023 18:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16646v2</guid></item><item><title>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</title><link>http://arxiv.org/abs/2310.11441v1</link><description>We present Set-of-Mark (SoM), a new visual prompting method, to unleash thevisual grounding abilities of large multimodal models (LMMs), such as GPT-4V.As illustrated in Fig. 1 (right), we employ off-the-shelf interactivesegmentation models, such as SAM, to partition an image into regions atdifferent levels of granularity, and overlay these regions with a set of markse.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V cananswer the questions that require visual grounding. We perform a comprehensiveempirical study to validate the effectiveness of SoM on a wide range offine-grained vision and multimodal tasks. For example, our experiments showthat GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referringsegmentation model on RefCOCOg in a zero-shot setting.</description><author>Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao</author><pubDate>Tue, 17 Oct 2023 18:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11441v1</guid></item><item><title>Is Self-Repair a Silver Bullet for Code Generation?</title><link>http://arxiv.org/abs/2306.09896v4</link><description>Large language models have shown remarkable aptitude in code generation, butstill struggle on challenging tasks. Self-repair -- in which the model debugsand fixes mistakes in its own code -- has recently become a popular way toboost performance in these settings. However, only very limited studies on howand when self-repair works effectively exist in the literature, and one mightwonder to what extent a model is really capable of repairing mistakes in codewhich was originally generated by that very same model. In this paper, weanalyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair onproblems taken from HumanEval or APPS, finding that when the cost of carryingout repair is taken into account, gains are often modest, vary significantlybetween subsets of the data, and are sometimes not present at all. Wehypothesize that this is because self-repair is bottlenecked by the model'sability to provide feedback on its own code; boosting the feedback withstronger models, we observe performance gains even in settings where the modeldoes not benefit from self-repair. Finally, we find that providing the modelwith feedback from human participants greatly benefits repair even for GPT-4,and carry out a brief qualitative analysis of the differences observed.</description><author>Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, Armando Solar-Lezama</author><pubDate>Tue, 17 Oct 2023 18:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09896v4</guid></item><item><title>EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</title><link>http://arxiv.org/abs/2310.11440v1</link><description>The vision and language generative models have been overgrown in recentyears. For video generation, various open-sourced models and public-availableservices are released for generating high-visual quality videos. However, thesemethods often use a few academic metrics, for example, FVD or IS, to evaluatethe performance. We argue that it is hard to judge the large conditionalgenerative models from the simple metrics since these models are often trainedon very large datasets with multi-aspect abilities. Thus, we propose a newframework and pipeline to exhaustively evaluate the performance of thegenerated videos. To achieve this, we first conduct a new prompt list fortext-to-video generation by analyzing the real-world prompt list with the helpof the large language model. Then, we evaluate the state-of-the-art videogenerative models on our carefully designed benchmarks, in terms of visualqualities, content qualities, motion qualities, and text-caption alignment witharound 18 objective metrics. To obtain the final leaderboard of the models, wealso fit a series of coefficients to align the objective metrics to the users'opinions. Based on the proposed opinion alignment method, our final score showsa higher correlation than simply averaging the metrics, showing theeffectiveness of the proposed evaluation method.</description><author>Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, Ying Shan</author><pubDate>Tue, 17 Oct 2023 18:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11440v1</guid></item><item><title>On Statistical Learning of Branch and Bound for Vehicle Routing Optimization</title><link>http://arxiv.org/abs/2310.09986v2</link><description>Recently, machine learning of the branch and bound algorithm has shownpromise in approximating competent solutions to NP-hard problems. In thispaper, we utilize and comprehensively compare the outcomes of three neuralnetworks--graph convolutional neural network (GCNN), GraphSAGE, and graphattention network (GAT)--to solve the capacitated vehicle routing problem. Wetrain these neural networks to emulate the decision-making process of thecomputationally expensive Strong Branching strategy. The neural networks aretrained on six instances with distinct topologies from the CVRPLIB andevaluated on eight additional instances. Moreover, we reduced the minimumnumber of vehicles required to solve a CVRP instance to a bin-packing problem,which was addressed in a similar manner. Through rigorous experimentation, wefound that this approach can match or improve upon the performance of thebranch and bound algorithm with the Strong Branching strategy while requiringsignificantly less computational time. The source code that corresponds to ourresearch findings and methodology is readily accessible and available forreference at the following web address: https://isotlaboratory.github.io/ml4vrp</description><author>Andrew Naguib, Waleed A. Yousef, Issa Traoré, Mohammad Mamun</author><pubDate>Tue, 17 Oct 2023 18:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09986v2</guid></item><item><title>Understanding deep neural networks through the lens of their non-linearity</title><link>http://arxiv.org/abs/2310.11439v1</link><description>The remarkable success of deep neural networks (DNN) is often attributed totheir high expressive power and their ability to approximate functions ofarbitrary complexity. Indeed, DNNs are highly non-linear models, and activationfunctions introduced into them are largely responsible for this. While manyworks studied the expressive power of DNNs through the lens of theirapproximation capabilities, quantifying the non-linearity of DNNs or ofindividual activation functions remains an open problem. In this paper, wepropose the first theoretically sound solution to track non-linearitypropagation in deep neural networks with a specific focus on computer visionapplications. Our proposed affinity score allows us to gain insights into theinner workings of a wide range of different architectures and learningparadigms. We provide extensive experimental results that highlight thepractical utility of the proposed affinity score and its potential forlong-reaching applications.</description><author>Quentin Bouniot, Ievgen Redko, Anton Mallasto, Charlotte Laclau, Karol Arndt, Oliver Struckmeier, Markus Heinonen, Ville Kyrki, Samuel Kaski</author><pubDate>Tue, 17 Oct 2023 18:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11439v1</guid></item><item><title>R2H: Building Multimodal Navigation Helpers that Respond to Help Requests</title><link>http://arxiv.org/abs/2305.14260v2</link><description>Intelligent navigation-helper agents are critical as they can navigate usersin unknown areas through environmental awareness and conversational ability,serving as potential accessibility tools for individuals with disabilities. Inthis work, we first introduce a novel benchmark, Respond to Help Requests(R2H), to promote the development of multi-modal navigation helpers capable ofresponding to requests for help, utilizing existing dialog-based embodieddatasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH),which assesses the helper agent's ability to generate informative responsesbased on a given dialog history, and (2) Respond during Interaction (RdI),which evaluates the effectiveness and efficiency of the response duringconsistent cooperation with a task performer. Furthermore, we explore twoapproaches to construct the navigation-helper agent, including fine-tuning anovel task-oriented multi-modal response generation model that can see andrespond, named SeeRee, and employing a multi-modal large language model in azero-shot manner. Analysis of the task and method was conducted based on bothautomatic benchmarking and human evaluations. Project website:https://sites.google.com/view/response2helprequests/home.</description><author>Yue Fan, Jing Gu, Kaizhi Zheng, Xin Eric Wang</author><pubDate>Tue, 17 Oct 2023 18:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14260v2</guid></item><item><title>Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning</title><link>http://arxiv.org/abs/2303.09447v2</link><description>In the context of continual learning, prototypes-as representative classembeddings-offer advantages in memory conservation and the mitigation ofcatastrophic forgetting. However, challenges related to semantic drift andprototype interference persist. In this study, we introduce the ContrastivePrototypical Prompt (CPP) approach. Through task-specific prompt-tuning,underpinned by a contrastive learning objective, we effectively address bothaforementioned challenges. Our evaluations on four challengingclass-incremental benchmarks reveal that CPP achieves a significant 4% to 6%improvement over state-of-the-art methods. Importantly, CPP operates without arehearsal buffer and narrows the performance divergence between continual andoffline joint-learning, suggesting an innovative scheme for Transformer-basedcontinual learning systems.</description><author>Zhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu, Ting Liu, Dimitris N. Metaxas</author><pubDate>Tue, 17 Oct 2023 18:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09447v2</guid></item><item><title>Identifying Interpretable Visual Features in Artificial and Biological Neural Systems</title><link>http://arxiv.org/abs/2310.11431v1</link><description>Single neurons in neural networks are often ``interpretable'' in that theyrepresent individual, intuitively meaningful features. However, many neuronsexhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelatedfeatures. A recent hypothesis proposes that features in deep networks may berepresented in $\textit{superposition}$, i.e., on non-orthogonal axes bymultiple neurons, since the number of possible interpretable features innatural data is generally larger than the number of neurons in a given network.Accordingly, we should be able to find meaningful directions in activationspace that are not aligned with individual neurons. Here, we propose (1) anautomated method for quantifying visual interpretability that is validatedagainst a large database of human psychophysics judgments of neuroninterpretability, and (2) an approach for finding meaningful directions innetwork activation space. We leverage these methods to discover directions inconvolutional neural networks that are more intuitively meaningful thanindividual neurons, as we confirm and investigate in a series of analyses.Moreover, we apply the same method to two recent datasets of visual neuralresponses in the brain and find that our conclusions largely transfer to realneural data, suggesting that superposition might be deployed by the brain. Thisalso provides a link with disentanglement and raises fundamental questionsabout robust, efficient and factorized representations in both artificial andbiological neural systems.</description><author>David Klindt, Sophia Sanborn, Francisco Acosta, Frédéric Poitevin, Nina Miolane</author><pubDate>Tue, 17 Oct 2023 18:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11431v1</guid></item><item><title>An Empirical Study of Translation Hypothesis Ensembling with Large Language Models</title><link>http://arxiv.org/abs/2310.11430v1</link><description>Large language models (LLMs) are becoming a one-fits-many solution, but theysometimes hallucinate or produce unreliable output. In this paper, weinvestigate how hypothesis ensembling can improve the quality of the generatedtext for the specific problem of LLM-based machine translation. We experimentwith several techniques for ensembling hypotheses produced by LLMs such asChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multipledimensions, including the method to generate hypotheses (multiple prompts,temperature-based sampling, and beam search) and the strategy to produce thefinal translation (instruction-based, quality-based reranking, and minimumBayes risk (MBR) decoding). Our results show that MBR decoding is a veryeffective method, that translation quality can be improved using a small numberof samples, and that instruction tuning has a strong impact on the relationbetween the diversity of the hypotheses and the sampling temperature.</description><author>António Farinhas, José G. C. de Souza, André F. T. Martins</author><pubDate>Tue, 17 Oct 2023 18:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11430v1</guid></item><item><title>Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression</title><link>http://arxiv.org/abs/2310.11428v1</link><description>This work studies training instabilities of behavior cloning with deep neuralnetworks. We observe that minibatch SGD updates to the policy network duringtraining result in sharp oscillations in long-horizon rewards, despitenegligibly affecting the behavior cloning loss. We empirically disentangle thestatistical and computational causes of these oscillations, and find them tostem from the chaotic propagation of minibatch SGD noise through unstableclosed-loop dynamics. While SGD noise is benign in the single-step actionprediction objective, it results in catastrophic error accumulation over longhorizons, an effect we term gradient variance amplification (GVA). We show thatmany standard mitigation techniques do not alleviate GVA, but find anexponential moving average (EMA) of iterates to be surprisingly effective atdoing so. We illustrate the generality of this phenomenon by showing theexistence of GVA and its amelioration by EMA in both continuous control andautoregressive language generation. Finally, we provide theoretical vignettesthat highlight the benefits of EMA in alleviating GVA and shed light on theextent to which classical convex models can help in understanding the benefitsof iterate averaging in deep learning.</description><author>Adam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz, Cyril Zhang</author><pubDate>Tue, 17 Oct 2023 18:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11428v1</guid></item><item><title>RFNet-4D++: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds with Cross-Attention Spatio-Temporal Features</title><link>http://arxiv.org/abs/2203.16482v3</link><description>Object reconstruction from 3D point clouds has been a long-standing researchproblem in computer vision and computer graphics, and achieved impressiveprogress. However, reconstruction from time-varying point clouds (a.k.a. 4Dpoint clouds) is generally overlooked. In this paper, we propose a new networkarchitecture, namely RFNet-4D++, that jointly reconstructs objects and theirmotion flows from 4D point clouds. The key insight is simultaneously performingboth tasks via learning of spatial and temporal features from a sequence ofpoint clouds can leverage individual tasks, leading to improved overallperformance. To prove this ability, we design a temporal vector field learningmodule using an unsupervised learning approach for flow estimation task,leveraged by supervised learning of spatial structures for objectreconstruction. Extensive experiments and analyses on benchmark datasetsvalidated the effectiveness and efficiency of our method. As shown inexperimental results, our method achieves state-of-the-art performance on bothflow estimation and object reconstruction while performing much faster thanexisting methods in both training and inference. Our code and data areavailable at https://github.com/hkust-vgd/RFNet-4D</description><author>Tuan-Anh Vu, Duc Thanh Nguyen, Binh-Son Hua, Quang-Hieu Pham, Sai-Kit Yeung</author><pubDate>Tue, 17 Oct 2023 18:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.16482v3</guid></item><item><title>Effective Long-Context Scaling of Foundation Models</title><link>http://arxiv.org/abs/2309.16039v2</link><description>We present a series of long-context LLMs that support effective contextwindows of up to 32,768 tokens. Our model series are built through continualpretraining from Llama 2 with longer training sequences and on a dataset wherelong texts are upsampled. We perform extensive evaluation on language modeling,synthetic context probing tasks, and a wide range of research benchmarks. Onresearch benchmarks, our models achieve consistent improvements on most regulartasks and significant improvements on long-context tasks over Llama 2. Notably,with a cost-effective instruction tuning procedure that does not requirehuman-annotated long instruction data, the 70B variant can already surpassgpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.Alongside these results, we provide an in-depth analysis on the individualcomponents of our method. We delve into Llama's position encodings and discussits limitation in modeling long dependencies. We also examine the impact ofvarious design choices in the pretraining process, including the data mix andthe training curriculum of sequence lengths -- our ablation experiments suggestthat having abundant long texts in the pretrain dataset is not the key toachieving strong performance, and we empirically verify that long contextcontinual pretraining is more efficient and similarly effective compared topretraining from scratch with long sequences.</description><author>Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma</author><pubDate>Tue, 17 Oct 2023 18:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16039v2</guid></item><item><title>Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching</title><link>http://arxiv.org/abs/2310.11420v1</link><description>We propose a novel unsupervised learning approach for non-rigid 3D shapematching. Our approach improves upon recent state-of-the art deep functionalmap methods and can be applied to a broad range of different challengingscenarios. Previous deep functional map methods mainly focus on featureextraction and aim exclusively at obtaining more expressive features forfunctional map computation. However, the importance of the functional mapcomputation itself is often neglected and the relationship between thefunctional map and point-wise map is underexplored. In this paper, wesystematically investigate the coupling relationship between the functional mapfrom the functional map solver and the point-wise map based on featuresimilarity. To this end, we propose a self-adaptive functional map solver toadjust the functional map regularisation for different shape matchingscenarios, together with a vertex-wise contrastive loss to obtain morediscriminative features. Using different challenging datasets (includingnon-isometry, topological noise and partiality), we demonstrate that our methodsubstantially outperforms previous state-of-the-art methods.</description><author>Dongliang Cao, Paul Roetzer, Florian Bernard</author><pubDate>Tue, 17 Oct 2023 18:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11420v1</guid></item><item><title>VcT: Visual change Transformer for Remote Sensing Image Change Detection</title><link>http://arxiv.org/abs/2310.11417v1</link><description>Existing visual change detectors usually adopt CNNs or Transformers forfeature representation learning and focus on learning effective representationfor the changed regions between images. Although good performance can beobtained by enhancing the features of the change regions, however, these worksare still limited mainly due to the ignorance of mining the unchangedbackground context information. It is known that one main challenge for changedetection is how to obtain the consistent representations for two imagesinvolving different variations, such as spatial variation, sunlight intensity,etc. In this work, we demonstrate that carefully mining the common backgroundinformation provides an important cue to learn the consistent representationsfor the two images which thus obviously facilitates the visual change detectionproblem. Based on this observation, we propose a novel Visual changeTransformer (VcT) model for visual change detection problem. To be specific, ashared backbone network is first used to extract the feature maps for the givenimage pair. Then, each pixel of feature map is regarded as a graph node and thegraph neural network is proposed to model the structured information for coarsechange map prediction. Top-K reliable tokens can be mined from the map andrefined by using the clustering algorithm. Then, these reliable tokens areenhanced by first utilizing self/cross-attention schemes and then interactingwith original features via an anchor-primary attention learning module.Finally, the prediction head is proposed to get a more accurate change map.Extensive experiments on multiple benchmark datasets validated theeffectiveness of our proposed VcT model.</description><author>Bo Jiang, Zitian Wang, Xixi Wang, Ziyan Zhang, Lan Chen, Xiao Wang, Bin Luo</author><pubDate>Tue, 17 Oct 2023 18:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11417v1</guid></item><item><title>Automatic Personalized Impression Generation for PET Reports Using Large Language Models</title><link>http://arxiv.org/abs/2309.10066v2</link><description>In this study, we aimed to determine if fine-tuned large language models(LLMs) can generate accurate, personalized impressions for whole-body PETreports. Twelve language models were trained on a corpus of PET reports usingthe teacher-forcing algorithm, with the report findings as input and theclinical impressions as reference. An extra input token encodes the readingphysician's identity, allowing models to learn physician-specific reportingstyles. Our corpus comprised 37,370 retrospective PET reports collected fromour institution between 2010 and 2022. To identify the best LLM, 30 evaluationmetrics were benchmarked against quality scores from two nuclear medicine (NM)physicians, with the most aligned metrics selecting the model for expertevaluation. In a subset of data, model-generated impressions and originalclinical impressions were assessed by three NM physicians according to 6quality dimensions (3-point scale) and an overall utility score (5-pointscale). Each physician reviewed 12 of their own reports and 12 reports fromother physicians. Bootstrap resampling was used for statistical analysis. Ofall evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed thehighest Spearman's rank correlations (0.568 and 0.563) with physicianpreferences. Based on these metrics, the fine-tuned PEGASUS model was selectedas the top LLM. When physicians reviewed PEGASUS-generated impressions in theirown style, 89% were considered clinically acceptable, with a mean utility scoreof 4.08 out of 5. Physicians rated these personalized impressions as comparablein overall utility to the impressions dictated by other physicians (4.03,P=0.41). In conclusion, personalized impressions generated by PEGASUS wereclinically useful, highlighting its potential to expedite PET reporting.</description><author>Xin Tie, Muheon Shin, Ali Pirasteh, Nevein Ibrahim, Zachary Huemann, Sharon M. Castellino, Kara M. Kelly, John Garrett, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw</author><pubDate>Tue, 17 Oct 2023 18:24:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10066v2</guid></item><item><title>Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension</title><link>http://arxiv.org/abs/2309.15714v2</link><description>With the recent proliferation of large language models (LLMs), such asGenerative Pre-trained Transformers (GPT), there has been a significant shiftin exploring human and machine comprehension of semantic language meaning. Thisshift calls for interdisciplinary research that bridges cognitive science andnatural language processing (NLP). This pilot study aims to provide insightsinto individuals' neural states during a semantic relationreading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, andelectroencephalographic (EEG) data to study how the brain processes words withvarying degrees of relevance to a keyword during reading. We also use a featureengineering approach to improve the fixation-related EEG data classificationwhile participants read words with high versus low relevance to the keyword.The best validation accuracy in this word-level classification is over 60\%across 12 subjects. Words of high relevance to the inference keyword hadsignificantly more eye fixations per word: 1.0584 compared to 0.6576 whenexcluding no-fixation words, and 1.5126 compared to 1.4026 when including them.This study represents the first attempt to classify brain states at a wordlevel using LLM knowledge. It provides valuable insights into human cognitiveabilities and the realm of Artificial General Intelligence (AGI), and offersguidance for developing potential reading-assisted technologies.</description><author>Yuhong Zhang, Qin Li, Sujal Nahata, Tasnia Jamal, Shih-kuen Cheng, Gert Cauwenberghs, Tzyy-Ping Jung</author><pubDate>Tue, 17 Oct 2023 18:17:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15714v2</guid></item><item><title>A new economic and financial theory of money</title><link>http://arxiv.org/abs/2310.04986v3</link><description>This paper fundamentally reformulates economic and financial theory toinclude electronic currencies. The valuation of the electronic currencies willbe based on macroeconomic theory and the fundamental equation of monetarypolicy, not the microeconomic theory of discounted cash flows. The view ofelectronic currency as a transactional equity associated with tangible assetsof a sub-economy will be developed, in contrast to the view of stock as anequity associated mostly with intangible assets of a sub-economy. The view willbe developed of the electronic currency management firm as an entityresponsible for coordinated monetary (electronic currency supply and valuestabilization) and fiscal (investment and operational) policies of asubstantial (for liquidity of the electronic currency) sub-economy. The riskmodel used in the valuations and the decision-making will not be theubiquitous, yet inappropriate, exponential risk model that leads to discountrates, but will be multi time scale models that capture the true risk. Thedecision-making will be approached from the perspective of true systems controlbased on a system response function given by the multi scale risk model andsystem controllers that utilize the Deep Reinforcement Learning, GenerativePretrained Transformers, and other methods of Artificial Intelligence(DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complexphysical system with both stable equilibriums that are associated withshort-term exploitation, and unstable equilibriums that need to be stabilizedwith active nonlinear control based on the multi scale system responsefunctions and DRL/GPT/AI.</description><author>Michael E. Glinsky, Sharon Sievert</author><pubDate>Tue, 17 Oct 2023 18:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04986v3</guid></item><item><title>Evaluating LLMs for Privilege-Escalation Scenarios</title><link>http://arxiv.org/abs/2310.11409v1</link><description>Penetration testing, an essential component of cybersecurity, allowsorganizations to proactively identify and remediate vulnerabilities in theirsystems, thus bolstering their defense mechanisms against potentialcyberattacks. One recent advancement in the realm of penetration testing is theutilization of Language Models (LLMs). We explore the intersection of LLMs andpenetration testing to gain insight into their capabilities and challenges inthe context of privilige escalation. We create an automated Linuxprivilege-escalation benchmark utilizing local virtual machines. We introducean LLM-guided privilege-escalation tool designed for evaluating different LLMsand prompt strategies against our benchmark. We analyze the impact of differentprompt designs, the benefits of in-context learning, and the advantages ofoffering high-level guidance to LLMs. We discuss challenging areas for LLMs,including maintaining focus during testing, coping with errors, and finallycomparing them with both stochastic parrots as well as with human hackers.</description><author>Andreas Happe, Aaron Kaplan, Jürgen Cito</author><pubDate>Tue, 17 Oct 2023 18:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11409v1</guid></item><item><title>Group-blind optimal transport to group parity and its constrained variants</title><link>http://arxiv.org/abs/2310.11407v1</link><description>Fairness holds a pivotal role in the realm of machine learning, particularlywhen it comes to addressing groups categorised by sensitive attributes, e.g.,gender, race. Prevailing algorithms in fair learning predominantly hinge onaccessibility or estimations of these sensitive attributes, at least in thetraining process. We design a single group-blind projection map that aligns thefeature distributions of both groups in the source data, achieving(demographic) group parity, without requiring values of the protected attributefor individual samples in the computation of the map, as well as its use.Instead, our approach utilises the feature distributions of the privileged andunprivileged groups in a boarder population and the essential assumption thatthe source data are unbiased representation of the population. We presentnumerical results on synthetic data and real data.</description><author>Quan Zhou, Jakub Marecek</author><pubDate>Tue, 17 Oct 2023 18:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11407v1</guid></item><item><title>Enhancing Group Fairness in Online Settings Using Oblique Decision Forests</title><link>http://arxiv.org/abs/2310.11401v1</link><description>Fairness, especially group fairness, is an important consideration in thecontext of machine learning systems. The most commonly adopted groupfairness-enhancing techniques are in-processing methods that rely on a mixtureof a fairness objective (e.g., demographic parity) and a task-specificobjective (e.g., cross-entropy) during the training process. However, when dataarrives in an online fashion -- one instance at a time -- optimizing suchfairness objectives poses several challenges. In particular, group fairnessobjectives are defined using expectations of predictions across differentdemographic groups. In the online setting, where the algorithm has access to asingle instance at a time, estimating the group fairness objective requiresadditional storage and significantly more computation (e.g., forward/backwardpasses) than the task-specific objective at every time step. In this paper, wepropose Aranyani, an ensemble of oblique decision trees, to make fair decisionsin online settings. The hierarchical tree structure of Aranyani enablesparameter isolation and allows us to efficiently compute the fairness gradientsusing aggregate statistics of previous decisions, eliminating the need foradditional storage and forward/backward passes. We also present an efficientframework to train Aranyani and theoretically analyze several of itsproperties. We conduct empirical evaluations on 5 publicly available benchmarks(including vision and language datasets) to show that Aranyani achieves abetter accuracy-fairness trade-off compared to baseline approaches.</description><author>Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</author><pubDate>Tue, 17 Oct 2023 18:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11401v1</guid></item><item><title>Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality</title><link>http://arxiv.org/abs/2305.13812v2</link><description>Contrastively trained vision-language models have achieved remarkableprogress in vision and language representation learning, leading tostate-of-the-art models for various downstream multimodal tasks. However,recent research has highlighted severe limitations of these models in theirability to perform compositional reasoning over objects, attributes, andrelations. Scene graphs have emerged as an effective way to understand imagescompositionally. These are graph-structured semantic representations of imagesthat contain objects, their attributes, and relations with other objects in ascene. In this work, we consider the scene graph parsed from text as a proxyfor the image scene graph and propose a graph decomposition and augmentationframework along with a coarse-to-fine contrastive learning objective betweenimages and text that aligns sentences of various complexities to the sameimage. Along with this, we propose novel negative mining techniques in thescene graph space for improving attribute binding and relation understanding.Through extensive experiments, we demonstrate the effectiveness of our approachthat significantly improves attribute binding, relation understanding,systematic generalization, and productivity on multiple recently proposedbenchmarks (For example, improvements upto $18\%$ for systematicgeneralization, $16.5\%$ for relation understanding over a strong baseline),while achieving similar or better performance than CLIP on various generalmultimodal tasks.</description><author>Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, Yu Chen</author><pubDate>Tue, 17 Oct 2023 18:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13812v2</guid></item><item><title>Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems</title><link>http://arxiv.org/abs/2204.01815v6</link><description>We introduce a new consistency-based approach for defining and solvingnonnegative/positive matrix and tensor completion problems. The novelty of theframework is that instead of artificially making the problem well-posed in theform of an application-arbitrary optimization problem, e.g., minimizing a bulkstructural measure such as rank or norm, we show that a singleproperty/constraint: preserving unit-scale consistency, guarantees theexistence of both a solution and, under relatively weak support assumptions,uniqueness. The framework and solution algorithms also generalize directly totensors of arbitrary dimensions while maintaining computational complexity thatis linear in problem size for fixed dimension d. In the context of recommendersystem (RS) applications, we prove that two reasonable properties that shouldbe expected to hold for any solution to the RS problem are sufficient to permituniqueness guarantees to be established within our framework. This isremarkable because it obviates the need for heuristic-based statistical or AImethods despite what appear to be distinctly human/subjective variables at theheart of the problem. Key theoretical contributions include a generalunit-consistent tensor-completion framework with proofs of its properties,e.g., consensus-order and fairness, and algorithms with optimal runtime andspace complexities, e.g., O(1) term-completion with preprocessing complexitythat is linear in the number of known terms of the matrix/tensor. From apractical perspective, the seamless ability of the framework to generalize toexploit high-dimensional structural relationships among key state variables,e.g., user and product attributes, offers a means for extracting significantlymore information than is possible for alternative methods that cannotgeneralize beyond direct user-product relationships.</description><author>Tung Nguyen, Jeffrey Uhlmann</author><pubDate>Tue, 17 Oct 2023 18:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.01815v6</guid></item><item><title>Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks</title><link>http://arxiv.org/abs/2310.11398v1</link><description>In the realm of deep learning, the self-attention mechanism has substantiatedits pivotal role across a myriad of tasks, encompassing natural languageprocessing and computer vision. Despite achieving success across diverseapplications, the traditional self-attention mechanism primarily leverageslinear transformations for the computation of query, key, and value (QKV),which may not invariably be the optimal choice under specific circumstances.This paper probes into a novel methodology for QKV computation-implementing aspecially-designed neural network structure for the calculation. Utilizing amodified Marian model, we conducted experiments on the IWSLT 2017German-English translation task dataset and juxtaposed our method with theconventional approach. The experimental results unveil a significantenhancement in BLEU scores with our method. Furthermore, our approach alsomanifested superiority when training the Roberta model with the Wikitext-103dataset, reflecting a notable reduction in model perplexity compared to itsoriginal counterpart. These experimental outcomes not only validate theefficacy of our method but also reveal the immense potential in optimizing theself-attention mechanism through neural network-based QKV computation, pavingthe way for future research and practical applications. The source code andimplementation details for our proposed method can be accessed athttps://github.com/ocislyjrti/NeuralAttention.</description><author>Muhan Zhang</author><pubDate>Tue, 17 Oct 2023 18:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11398v1</guid></item><item><title>Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations</title><link>http://arxiv.org/abs/2303.04614v2</link><description>We introduce and investigate, for finite groups $G$, $G$-invariant deepneural network ($G$-DNN) architectures with ReLU activation that are denselyconnected-- i.e., include all possible skip connections. In contrast to other$G$-invariant architectures in the literature, the preactivations ofthe$G$-DNNs presented here are able to transform by \emph{signed} permutationrepresentations (signed perm-reps) of $G$. Moreover, the individual layers ofthe $G$-DNNs are not required to be $G$-equivariant; instead, thepreactivations are constrained to be $G$-equivariant functions of the networkinput in a way that couples weights across all layers. The result is a richerfamily of $G$-invariant architectures never seen previously. We derive anefficient implementation of $G$-DNNs after a reparameterization of weights, aswell as necessary and sufficient conditions for an architecture to be``admissible''-- i.e., nondegenerate and inequivalent to smaller architectures.We include code that allows a user to build a $G$-DNN interactivelylayer-by-layer, with the final architecture guaranteed to be admissible. Weshow that there are far more admissible $G$-DNN architectures than thoseaccessible with the ``concatenated ReLU'' activation function from theliterature. Finally, we apply $G$-DNNs to two example problems -- (1)multiplication in $\{-1, 1\}$ (with theoretical guarantees) and (2) 3D objectclassification -- % finding that the inclusion of signed perm-repssignificantly boosts predictive performance compared to baselines with onlyordinary (i.e., unsigned) perm-reps.</description><author>Devanshu Agrawal, James Ostrowski</author><pubDate>Tue, 17 Oct 2023 18:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04614v2</guid></item><item><title>TRANSOM: An Efficient Fault-Tolerant System for Training LLMs</title><link>http://arxiv.org/abs/2310.10046v2</link><description>Large language models (LLMs), exemplified by chatGPT, have made significantstrides in various domains, indicating that LLMs with hundreds of billions oreven trillions of parameters will continue to revolutionize our daily lives.However, training such super-large-scale models demands even more powerful GPUclusters and extended training periods spanning months. Maintaininguninterrupted and long-duration training has become exceptionally challengingdue to hardware and software failures in these extensive clusters. Asubstantial amount of training time is devoted to tasks checkpointing savingand loading, ananomaly detection and restarts, leading to a notable reductionin overall training efficiency.To address these challenges, we introduce novelfault-tolerant large-scale model training system named TRANSOM. This systemcomprises three integral components: the training pipeline automatic faulttolerance and recovery mechanism (TOL), the training task multi-dimensionalmetric automatic anomaly detection system (TEE), and the training checkpointasynchronous access automatic fault tolerance and recovery technology (TCE).Here, TOL serves as the operating system for the training task, while TEE isresponsible for task monitoring and error handling, promptly detectinganomalies and relaying them to TOL. Subsequently, TOL autonomously determinesand implements fault tolerance strategies for the training task, with the TCEfacilitating asynchronous checkpoint saving and loading during the faulttolerance process. The experimental results indicate that TRANSOM significantlyenhances the efficiency of large-scale LLM training on clusters. Specifically,the pre-training time for GPT3-175B has been reduced by 28%, while checkpointsaving and loading performance have improved by a factor of 20.</description><author>Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li</author><pubDate>Tue, 17 Oct 2023 18:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10046v2</guid></item><item><title>Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</title><link>http://arxiv.org/abs/2310.11397v1</link><description>Large Language Models (LLMs) are powerful tools for natural languageprocessing, enabling novel applications and user experiences. However, toachieve optimal performance, LLMs often require adaptation with private data,which poses privacy and security challenges. Several techniques have beenproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparativeprivacy and security properties have not been systematically investigated. Inthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICLagainst three types of well-established attacks: membership inference, whichexposes data leakage (privacy); backdoor, which injects malicious behavior(security); and model stealing, which can violate intellectual property(privacy and security). Our results show that there is no silver bullet forprivacy and security in LLM adaptation and each technique has differentstrengths and weaknesses.</description><author>Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</author><pubDate>Tue, 17 Oct 2023 18:03:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11397v1</guid></item><item><title>Kernel quadrature with randomly pivoted Cholesky</title><link>http://arxiv.org/abs/2306.03955v2</link><description>This paper presents new quadrature rules for functions in a reproducingkernel Hilbert space using nodes drawn by a sampling algorithm known asrandomly pivoted Cholesky. The resulting computational procedure comparesfavorably to previous kernel quadrature methods, which either achieve lowaccuracy or require solving a computationally challenging sampling problem.Theoretical and numerical results show that randomly pivoted Cholesky is fastand achieves comparable quadrature error rates to more computationallyexpensive quadrature schemes based on continuous volume sampling, thinning, andrecombination. Randomly pivoted Cholesky is easily adapted to complicatedgeometries with arbitrary kernels, unlocking new potential for kernelquadrature.</description><author>Ethan N. Epperly, Elvira Moreno</author><pubDate>Tue, 17 Oct 2023 18:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03955v2</guid></item><item><title>Towards Minimax Optimality of Model-based Robust Reinforcement Learning</title><link>http://arxiv.org/abs/2302.05372v2</link><description>We study the sample complexity of obtaining an $\epsilon$-optimal policy in\emph{Robust} discounted Markov Decision Processes (RMDPs), given only accessto a generative model of the nominal kernel. This problem is widely studied inthe non-robust case, and it is known that any planning approach applied to anempirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A\mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which isminimax optimal. Results in the robust case are much more scarce. For $sa$-(resp $s$-)rectangular uncertainty sets, the best known sample complexity is$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp.$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), forspecific algorithms and when the uncertainty set is based on the totalvariation (TV), the KL or the Chi-square divergences. In this paper, weconsider uncertainty sets defined with an $L_p$-ball (recovering the TV case),and study the sample complexity of \emph{any} planning algorithm (with highaccuracy guarantee on the solution) applied to an empirical RMDP estimatedusing the generative model. In the general case, we prove a sample complexityof $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid\mid A \mid}{\epsilon^2})$ forboth the $sa$- and $s$-rectangular cases (improvements of $\mid S \mid$ and$\mid S \mid\mid A \mid$ respectively). When the size of the uncertainty issmall enough, we improve the sample complexity to$\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid }{\epsilon^2})$,recovering the lower-bound for the non-robust case for the first time and arobust lower-bound when the size of the uncertainty is small enough.</description><author>Pierre Clavier, Erwan Le Pennec, Matthieu Geist</author><pubDate>Tue, 17 Oct 2023 17:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05372v2</guid></item><item><title>Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?</title><link>http://arxiv.org/abs/2305.04134v2</link><description>Artificial Intelligence (AI) has been rapidly advancing and has demonstratedits ability to perform a wide range of cognitive tasks, including languageprocessing, visual recognition, and decision-making. Part of this progress isdue to LLMs (Large Language Models) like those of the GPT (GenerativePre-Trained Transformers) family. These models are capable of exhibitingbehavior that can be perceived as intelligent. Most authors in Neuropsychologyconsider intelligent behavior to depend on a number of overarching skills, orExecutive Functions (EFs), which rely on the correct functioning of neuralnetworks in the frontal lobes, and have developed a series of tests to evaluatethem. In this work, we raise the question of whether LLMs are developingexecutive functions similar to those of humans as part of their learning, andwe evaluate the planning function and working memory of GPT using the popularTowers of Hanoi method. Additionally, we introduce a new variant of theclassical method in order to avoid that the solutions are found in the LLMtraining data (dataleakeage). Preliminary results show that LLMs generatesnear-optimal solutions in Towers of Hanoi related tasks, adheres to taskconstraints, and exhibits rapid planning capabilities and efficient workingmemory usage, indicating a potential development of executive functions.However, these abilities are quite limited and worse than well-trained humanswhen the tasks are not known and are not part of the training data.</description><author>Hernan Ceferino Vazquez</author><pubDate>Tue, 17 Oct 2023 17:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04134v2</guid></item><item><title>Towards Automatic Satellite Images Captions Generation Using Large Language Models</title><link>http://arxiv.org/abs/2310.11392v1</link><description>Automatic image captioning is a promising technique for conveying visualinformation using natural language. It can benefit various tasks in satelliteremote sensing, such as environmental monitoring, resource management, disastermanagement, etc. However, one of the main challenges in this domain is the lackof large-scale image-caption datasets, as they require a lot of human expertiseand effort to create. Recent research on large language models (LLMs) hasdemonstrated their impressive performance in natural language understanding andgeneration tasks. Nonetheless, most of them cannot handle images (GPT-3.5,Falcon, Claude, etc.), while conventional captioning models pre-trained ongeneral ground-view images often fail to produce detailed and accurate captionsfor aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, wepropose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) toautomatically collect captions for remote sensing images by guiding LLMs todescribe their object annotations. We also present a benchmark model thatadapts the pre-trained generative image2text model (GIT) to generatehigh-quality captions for remote-sensing images. Our evaluation demonstratesthe effectiveness of our approach for collecting captions for remote sensingimages.</description><author>Yingxu He, Qiqi Sun</author><pubDate>Tue, 17 Oct 2023 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11392v1</guid></item><item><title>VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds</title><link>http://arxiv.org/abs/2310.11389v1</link><description>We tackle the problem of estimating the Value-at-Risk (VaR) and theConditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost withina Markov cost process. First, we derive a minimax lower bound of$\Omega(1/\sqrt{n})$ that holds both in an expected and in a probabilisticsense. Then, using a finite-horizon truncation scheme, we derive an upper boundfor the error in CVaR estimation, which matches our lower bound up to constantfactors. Finally, we discuss an extension of our estimation scheme that coversmore general risk measures satisfying a certain continuity criterion, e.g.,spectral risk measures, utility-based shortfall risk. To the best of ourknowledge, our work is the first to provide lower and upper bounds on theestimation error for any risk measure within Markovian settings. We remark thatour lower bounds also extend to the infinite-horizon discounted costs' mean.Even in that case, our result $\Omega(1/\sqrt{n}) $ improves upon the existingresult $\Omega(1/n)$[13].</description><author>Sanjay Bhat, Prashanth L. A., Gugan Thoppe</author><pubDate>Tue, 17 Oct 2023 17:35:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11389v1</guid></item><item><title>Imitating Task and Motion Planning with Visuomotor Transformers</title><link>http://arxiv.org/abs/2305.16309v3</link><description>Imitation learning is a powerful tool for training robot manipulationpolicies, allowing them to learn from expert demonstrations without manualprogramming or trial-and-error. However, common methods of data collection,such as human supervision, scale poorly, as they are time-consuming andlabor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomouslygenerate large-scale datasets of diverse demonstrations. In this work, we showthat the combination of large-scale datasets generated by TAMP supervisors andflexible Transformer models to fit them is a powerful paradigm for robotmanipulation. To that end, we present a novel imitation learning system calledOPTIMUS that trains large-scale visuomotor Transformer policies by imitating aTAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that isspecifically curated for imitation learning and can be used to train performanttransformer-based policies. In this paper, we present a thorough study of thedesign decisions required to imitate TAMP and demonstrate that OPTIMUS cansolve a wide variety of challenging vision-based manipulation tasks with over70 different objects, ranging from long-horizon pick-and-place tasks, to shelfand articulated object manipulation, achieving 70 to 80% success rates. Videoresults and code at https://mihdalal.github.io/optimus/</description><author>Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, Dieter Fox</author><pubDate>Tue, 17 Oct 2023 17:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16309v3</guid></item><item><title>Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration</title><link>http://arxiv.org/abs/2305.14324v2</link><description>Kendall's tau is frequently used to meta-evaluate how well machinetranslation (MT) evaluation metrics score individual translations. Its focus onpairwise score comparisons is intuitive but raises the question of how tiesshould be handled, a gray area that has motivated different variants in theliterature. We demonstrate that, in settings like modern MT meta-evaluation,existing variants have weaknesses arising from their handling of ties, and insome situations can even be gamed. We propose instead to meta-evaluate metricswith a version of pairwise accuracy that gives metrics credit for correctlypredicting ties, in combination with a tie calibration procedure thatautomatically introduces ties into metric scores, enabling fair comparisonbetween metrics that do and do not predict ties. We argue and provideexperimental evidence that these modifications lead to fairer ranking-basedassessments of metric performance.</description><author>Daniel Deutsch, George Foster, Markus Freitag</author><pubDate>Tue, 17 Oct 2023 17:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14324v2</guid></item><item><title>Markov $α$-Potential Games: Equilibrium Approximation and Regret Analysis</title><link>http://arxiv.org/abs/2305.12553v3</link><description>This paper proposes a new framework to study multi-agent interactions inMarkov games: Markov $\alpha$-potential game. A game is called Markov$\alpha$-potential game if there exists a Markov potential game such that thepairwise difference between the change of a player's value function under aunilateral policy deviation in the Markov game and Markov potential game can bebounded by $\alpha$. As a special case, Markov potential games are Markov$\alpha$-potential games with $\alpha=0$. The dependence of $\alpha$ on thegame parameters is also explicitly characterized in two classes of games thatare practically-relevant: Markov congestion games and the perturbed Markov teamgames. For general Markov games, an optimization-based approach is introducedwhich can compute a Markov potential game which is closest to the given game interms of $\alpha$. This approach can also be used to verify whether a game is aMarkov potential game, and provide a candidate potential function. Two algorithms -- the projected gradient-ascent algorithm and the {sequentialmaximum one-stage improvement} -- are provided to approximate the stationaryNash equilibrium in Markov $\alpha$-potential games and the correspondingNash-regret analysis is presented. The numerical experiments demonstrate thatsimple algorithms are capable of finding approximate equilibrium in Markov$\alpha$-potential games.</description><author>Xin Guo, Xinyu Li, Chinmay Maheshwari, Shankar Sastry, Manxi Wu</author><pubDate>Tue, 17 Oct 2023 17:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12553v3</guid></item><item><title>A voxel-level approach to brain age prediction: A method to assess regional brain aging</title><link>http://arxiv.org/abs/2310.11385v1</link><description>Brain aging is a regional phenomenon, a facet that remains relativelyunder-explored within the realm of brain age prediction research using machinelearning methods. Voxel-level predictions can provide localized brain ageestimates that can provide granular insights into the regional aging processes.This is essential to understand the differences in aging trajectories inhealthy versus diseased subjects. In this work, a deep learning-based multitaskmodel is proposed for voxel-level brain age prediction from T1-weightedmagnetic resonance images. The proposed model outperforms the models existingin the literature and yields valuable clinical insights when applied to bothhealthy and diseased populations. Regional analysis is performed on thevoxel-level brain age predictions to understand aging trajectories of knownanatomical regions in the brain and show that there exist disparities inregional aging trajectories of healthy subjects compared to ones withunderlying neurological disorders such as Dementia and more specifically,Alzheimer's disease. Our code is available athttps://github.com/nehagianchandani/Voxel-level-brain-age-prediction.</description><author>Neha Gianchandani, Mahsa Dibaji, Johanna Ospel, Fernando Vega, Mariana Bento, M. Ethan MacDonald, Roberto Souza</author><pubDate>Tue, 17 Oct 2023 17:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11385v1</guid></item><item><title>Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models</title><link>http://arxiv.org/abs/2310.03708v2</link><description>A single language model (LM), despite aligning well with an average labelerthrough reinforcement learning from human feedback (RLHF), may not universallysuit diverse human preferences. Recent approaches thus pursue customization,training separate principle-based reward models to represent differentalignment objectives (e.g. helpfulness, harmlessness, or honesty). DifferentLMs can then be trained for different preferences through multi-objective RLHF(MORLHF) with different objective weightings. Yet, RLHF is unstable andresource-heavy, especially for MORLHF with diverse and usually conflictingobjectives. In this paper, we present Multi-Objective Direct PreferenceOptimization (MODPO), an RL-free algorithm that extends Direct PreferenceOptimization (DPO) for multiple alignment objectives. Essentially, MODPO foldsLM learning directly into reward modeling, aligning LMs with the weighted sumof all principle-based rewards using pure cross-entropy loss. Whiletheoretically guaranteed to produce the same optimal solutions as MORLHF, MODPOis practically more stable and computationally efficient, obviating valuefunction modeling and online sample collection. Empirical results in safetyalignment and long-form question answering confirm that MODPO matches oroutperforms existing methods, consistently producing one of the mostcompetitive LM fronts that cater to diverse preferences with 3 times fewercomputations compared with MORLHF.</description><author>Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao</author><pubDate>Tue, 17 Oct 2023 17:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03708v2</guid></item><item><title>AniPixel: Towards Animatable Pixel-Aligned Human Avatar</title><link>http://arxiv.org/abs/2302.03397v2</link><description>Although human reconstruction typically results in human-specific avatars,recent 3D scene reconstruction techniques utilizing pixel-aligned features showpromise in generalizing to new scenes. Applying these techniques to humanavatar reconstruction can result in a volumetric avatar with generalizabilitybut limited animatability due to rendering only being possible for staticrepresentations. In this paper, we propose AniPixel, a novel animatable andgeneralizable human avatar reconstruction method that leverages pixel-alignedfeatures for body geometry prediction and RGB color blending. Technically, toalign the canonical space with the target space and the observation space, wepropose a bidirectional neural skinning field based on skeleton-drivendeformation to establish the target-to-canonical and canonical-to-observationcorrespondences. Then, we disentangle the canonical body geometry into anormalized neutral-sized body and a subject-specific residual for bettergeneralizability. As the geometry and appearance are closely related, weintroduce pixel-aligned features to facilitate the body geometry prediction anddetailed surface normals to reinforce the RGB color blending. We also devise apose-dependent and view direction-related shading module to represent the localillumination variance. Experiments show that AniPixel renders comparable novelviews while delivering better novel pose animation results thanstate-of-the-art methods.</description><author>Jinlong Fan, Jing Zhang, Zhi Hou, Dacheng Tao</author><pubDate>Tue, 17 Oct 2023 17:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03397v2</guid></item><item><title>Iteratively Refined Behavior Regularization for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2306.05726v2</link><description>One of the fundamental challenges for offline reinforcement learning (RL) isensuring robustness to data distribution. Whether the data originates from anear-optimal policy or not, we anticipate that an algorithm should demonstrateits ability to learn an effective control policy that seamlessly aligns withthe inherent distribution of offline data. Unfortunately, behaviorregularization, a simple yet effective offline RL algorithm, tends to strugglein this regard. In this paper, we propose a new algorithm that substantiallyenhances behavior-regularization based on conservative policy iteration. Ourkey observation is that by iteratively refining the reference policy used forbehavior regularization, conservative policy update guarantees graduallyimprovement, while also implicitly avoiding querying out-of-sample actions toprevent catastrophic learning failures. We prove that in the tabular settingthis algorithm is capable of learning the optimal policy covered by the offlinedataset, commonly referred to as the in-sample optimal policy. We then exploreseveral implementation details of the algorithm when function approximationsare applied. The resulting algorithm is easy to implement, requiring only a fewlines of code modification to existing methods. Experimental results on theD4RL benchmark indicate that our method outperforms previous state-of-the-artbaselines in most tasks, clearly demonstrate its superiority over behaviorregularization.</description><author>Xiaohan Hu, Yi Ma, Chenjun Xiao, Yan Zheng, Jianye Hao</author><pubDate>Tue, 17 Oct 2023 17:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05726v2</guid></item><item><title>Effects of Human Adversarial and Affable Samples on BERT Generalizability</title><link>http://arxiv.org/abs/2310.08008v3</link><description>BERT-based models have had strong performance on leaderboards, yet have beendemonstrably worse in real-world settings requiring generalization. Limitedquantities of training data is considered a key impediment to achievinggeneralizability in machine learning. In this paper, we examine the impact oftraining data quality, not quantity, on a model's generalizability. We considertwo characteristics of training data: the portion of human-adversarial(h-adversarial), i.e., sample pairs with seemingly minor differences butdifferent ground-truth labels, and human-affable (h-affable) training samples,i.e., sample pairs with minor differences but the same ground-truth label. Wefind that for a fixed size of training samples, as a rule of thumb, having10-30% h-adversarial instances improves the precision, and therefore F1, by upto 20 points in the tasks of text classification and relation extraction.Increasing h-adversarials beyond this range can result in performance plateausor even degradation. In contrast, h-affables may not contribute to a model'sgeneralizability and may even degrade generalization performance.</description><author>Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor</author><pubDate>Tue, 17 Oct 2023 17:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08008v3</guid></item><item><title>Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles</title><link>http://arxiv.org/abs/2310.11379v1</link><description>Voice-based interfaces rely on a wake-up word mechanism to initiatecommunication with devices. However, achieving a robust, energy-efficient, andfast detection remains a challenge. This paper addresses these real productionneeds by enhancing data with temporal alignments and using detection based ontwo phases with multi-resolution. It employs two models: a lightweighton-device model for real-time processing of the audio stream and a verificationmodel on the server-side, which is an ensemble of heterogeneous architecturesthat refine detection. This scheme allows the optimization of two operatingpoints. To protect privacy, audio features are sent to the cloud instead of rawaudio. The study investigated different parametric configurations for featureextraction to select one for on-device detection and another for theverification model. Furthermore, thirteen different audio classifiers werecompared in terms of performance and inference time. The proposed ensembleoutperforms our stronger classifier in every noise condition.</description><author>Fernando López, Jordi Luque, Carlos Segura, Pablo Gómez</author><pubDate>Tue, 17 Oct 2023 17:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11379v1</guid></item><item><title>Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version)</title><link>http://arxiv.org/abs/2107.02565v4</link><description>We introduce Goldilocks Selection, a technique for faster model trainingwhich selects a sequence of training points that are "just right". We proposean information-theoretic acquisition function -- the reducible validation loss-- and compute it with a small proxy model -- GoldiProx -- to efficientlychoose training points that maximize information about a validation set. Weshow that the "hard" (e.g. high loss) points usually selected in theoptimization literature are typically noisy, while the "easy" (e.g. low noise)samples often prioritized for curriculum learning confer less information.Further, points with uncertain labels, typically targeted by active learning,tend to be less relevant to the task. In contrast, Goldilocks Selection choosespoints that are "just right" and empirically outperforms the above approaches.Moreover, the selected sequence can transfer to other architectures;practitioners can share and reuse it without the need to recreate it.</description><author>Sören Mindermann, Muhammed Razzak, Winnie Xu, Andreas Kirsch, Mrinank Sharma, Adrien Morisot, Aidan N. Gomez, Sebastian Farquhar, Jan Brauner, Yarin Gal</author><pubDate>Tue, 17 Oct 2023 17:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.02565v4</guid></item><item><title>XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</title><link>http://arxiv.org/abs/2308.01263v2</link><description>Without proper safeguards, large language models will readily followmalicious instructions and generate toxic content. This risk motivates safetyefforts such as red-teaming and large-scale feedback learning, which aim tomake models both helpful and harmless. However, there is a tension betweenthese two objectives, since harmlessness requires models to refuse to complywith unsafe prompts, and thus not be helpful. Recent anecdotal evidencesuggests that some models may have struck a poor balance, so that even clearlysafe prompts are refused if they use similar language to unsafe prompts ormention sensitive topics. In this paper, we introduce a new test suite calledXSTest to identify such eXaggerated Safety behaviours in a systematic way.XSTest comprises 250 safe prompts across ten prompt types that well-calibratedmodels should not refuse to comply with, and 200 unsafe prompts as contraststhat models, for most applications, should refuse. We describe XSTest'screation and composition, and then use the test suite to highlight systematicfailure modes in state-of-the-art language models as well as more generalchallenges in building safer language models.</description><author>Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy</author><pubDate>Tue, 17 Oct 2023 17:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01263v2</guid></item><item><title>Faster Algorithms for Generalized Mean Densest Subgraph Problem</title><link>http://arxiv.org/abs/2310.11377v1</link><description>The densest subgraph of a large graph usually refers to some subgraph withthe highest average degree, which has been extended to the family of $p$-meansdense subgraph objectives by~\citet{veldt2021generalized}. The $p$-mean densestsubgraph problem seeks a subgraph with the highest average $p$-th-power degree,whereas the standard densest subgraph problem seeks a subgraph with a simplehighest average degree. It was shown that the standard peeling algorithm canperform arbitrarily poorly on generalized objective when $p&gt;1$ but uncertainwhen $0&lt;p&lt;1$. In this paper, we are the first to show that a standard peelingalgorithm can still yield $2^{1/p}$-approximation for the case $0&lt;p &lt; 1$.(Veldt 2021) proposed a new generalized peeling algorithm (GENPEEL), which for$p \geq 1$ has an approximation guarantee ratio $(p+1)^{1/p}$, and timecomplexity $O(mn)$, where $m$ and $n$ denote the number of edges and nodes ingraph respectively. In terms of algorithmic contributions, we propose a new andfaster generalized peeling algorithm (called GENPEEL++ in this paper), whichfor $p \in [1, +\infty)$ has an approximation guarantee ratio $(2(p+1))^{1/p}$,and time complexity $O(m(\log n))$, where $m$ and $n$ denote the number ofedges and nodes in graph, respectively. This approximation ratio converges to 1as $p \rightarrow \infty$.</description><author>Chenglin Fan, Ping Li, Hanyu Peng</author><pubDate>Tue, 17 Oct 2023 17:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11377v1</guid></item><item><title>DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations</title><link>http://arxiv.org/abs/2310.11374v1</link><description>Large language models (LLMs) and their variants have shown extraordinaryefficacy across numerous downstream natural language processing (NLP) tasks,which has presented a new vision for the development of NLP. Despite theirremarkable performance in natural language generating (NLG), LLMs lack adistinct focus on the emotion understanding domain. As a result, using LLMs foremotion recognition may lead to suboptimal and inadequate precision. Anotherlimitation of LLMs is that they are typical trained without leveragingmulti-modal information. To overcome these limitations, we propose DialogueLLM,a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMAmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.The visual information is considered as the supplementary knowledge toconstruct high-quality instructions. We offer a comprehensive evaluation of ourproposed model on three benchmarking emotion recognition in conversations (ERC)datasets and compare the results against the SOTA baselines and other SOTALLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GBA100 GPU in 5 hours, facilitating reproducibility for other researchers.</description><author>Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang, Jing Qin</author><pubDate>Tue, 17 Oct 2023 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11374v1</guid></item><item><title>Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks</title><link>http://arxiv.org/abs/2301.05065v2</link><description>Foundation models or pre-trained models have substantially improved theperformance of various language, vision, and vision-language understandingtasks. However, existing foundation models can only perform the best in onetype of tasks, namely language, vision, or vision-language. It is still an openquestion whether it is possible to construct a foundation model performing thebest for all the understanding tasks, which we call a general foundation model.In this paper, we propose a new general foundation model, X-FM (theX-Foundation Model). X-FM has one language encoder, one vision encoder, and onefusion encoder, as well as a new training method. The training method includestwo new techniques for learning X-FM from text, image, and image-text pairdata. One is to stop gradients from the vision-language training when learningthe language encoder. The other is to leverage the vision-language training toguide the learning of the vision encoder. Extensive experiments on benchmarkdatasets show that X-FM can significantly outperform existing generalfoundation models and perform better than or comparable to existing foundationmodels specifically for language, vision, or vision-language understanding.Code and pre-trained models are released athttps://github.com/zhangxinsong-nlp/XFM.</description><author>Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li</author><pubDate>Tue, 17 Oct 2023 17:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05065v2</guid></item><item><title>VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights</title><link>http://arxiv.org/abs/2310.11368v1</link><description>Recognizing vulnerability is crucial for understanding and implementingtargeted support to empower individuals in need. This is especially importantat the European Court of Human Rights (ECtHR), where the court adaptsConvention standards to meet actual individual needs and thus ensures effectivehuman rights protection. However, the concept of vulnerability remains elusiveat the ECtHR and no prior NLP research has dealt with it. To enable futureresearch in this area, we present VECHR, a novel expert-annotated multi-labeldataset comprising of vulnerability type classification and explanationrationale. We benchmark the performance of state-of-the-art models on VECHRfrom both prediction and explainability perspectives. Our results demonstratethe challenging nature of the task with lower prediction performance andlimited agreement between models and experts. Further, we analyze therobustness of these models in dealing with out-of-domain (OOD) data and observeoverall limited performance. Our dataset poses unique challenges offeringsignificant room for improvement regarding performance, explainability, androbustness.</description><author>Shanshan Xu, Leon Staufer, Santosh T. Y. S. S, Oana Ichim, Corina Heri, Matthias Grabmair</author><pubDate>Tue, 17 Oct 2023 17:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11368v1</guid></item><item><title>Lie Group Decompositions for Equivariant Neural Networks</title><link>http://arxiv.org/abs/2310.11366v1</link><description>Invariance and equivariance to geometrical transformations have proven to bevery useful inductive biases when training (convolutional) neural networkmodels, especially in the low-data regime. Much work has focused on the casewhere the symmetry group employed is compact or abelian, or both. Recent workhas explored enlarging the class of transformations used to the case of Liegroups, principally through the use of their Lie algebra, as well as the groupexponential and logarithm maps. The applicability of such methods to largertransformation groups is limited by the fact that depending on the group ofinterest $G$, the exponential map may not be surjective. Further limitationsare encountered when $G$ is neither compact nor abelian. Using the structureand geometry of Lie groups and their homogeneous spaces, we present a frameworkby which it is possible to work with such groups primarily focusing on the Liegroups $G = \text{GL}^{+}(n, \mathbb{R})$ and $G = \text{SL}(n, \mathbb{R})$,as well as their representation as affine transformations $\mathbb{R}^{n}\rtimes G$. Invariant integration as well as a global parametrization isrealized by decomposing the `larger` groups into subgroups and submanifoldswhich can be handled individually. Under this framework, we show howconvolution kernels can be parametrized to build models equivariant withrespect to affine transformations. We evaluate the robustness andout-of-distribution generalisation capability of our model on the standardaffine-invariant benchmark classification task, where we outperform allprevious equivariant models as well as all Capsule Network proposals.</description><author>Mircea Mironenco, Patrick Forré</author><pubDate>Tue, 17 Oct 2023 17:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11366v1</guid></item><item><title>MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields</title><link>http://arxiv.org/abs/2302.02978v2</link><description>Previous research has demonstrated the advantages of integrating data frommultiple sources over traditional unimodal data, leading to the emergence ofnumerous novel multimodal applications. We propose a multimodal classificationbenchmark MuG with eight datasets that allows researchers to evaluate andimprove their models. These datasets are collected from four various genres ofgames that cover tabular, textual, and visual modalities. We conductmulti-aspect data analysis to provide insights into the benchmark, includinglabel balance ratios, percentages of missing features, distributions of datawithin each modality, and the correlations between labels and input modalities.We further present experimental results obtained by several state-of-the-artunimodal classifiers and multimodal classifiers, which demonstrate thechallenging and multimodal-dependent properties of the benchmark. MuG isreleased at https://github.com/lujiaying/MUG-Bench with the data, tutorials,and implemented baselines.</description><author>Jiaying Lu, Yongchen Qian, Shifan Zhao, Yuanzhe Xi, Carl Yang</author><pubDate>Tue, 17 Oct 2023 17:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02978v2</guid></item><item><title>Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets</title><link>http://arxiv.org/abs/2302.13959v2</link><description>Increasingly larger datasets have become a standard ingredient to advancingthe state-of-the-art in NLP. However, data quality might have already becomethe bottleneck to unlock further gains. Given the diversity and the sizes ofmodern datasets, standard data filtering is not straight-forward to apply,because of the multifacetedness of the harmful data and elusiveness offiltering rules that would generalize across multiple tasks. We study thefitness of task-agnostic self-influence scores of training examples for datacleaning, analyze their efficacy in capturing naturally occurring outliers, andinvestigate to what extent self-influence based data cleaning can improvedownstream performance in machine translation, question answering and textclassification, building up on recent approaches to self-influence calculationand automated curriculum learning.</description><author>Irina Bejan, Artem Sokolov, Katja Filippova</author><pubDate>Tue, 17 Oct 2023 17:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13959v2</guid></item><item><title>Fake news detection using parallel BERT deep neural networks</title><link>http://arxiv.org/abs/2204.04793v2</link><description>Fake news is a growing challenge for social networks and media. Detection offake news always has been a problem for many years, but after the evolution ofsocial networks and increasing speed of news dissemination in recent years hasbeen considered again. There are several approaches to solving this problem,one of which is to detect fake news based on its text style using deep neuralnetworks. In recent years, one of the most used forms of deep neural networksfor natural language processing is transfer learning with transformers. BERT isone of the most promising transformers who outperforms other models in many NLPbenchmarks. This article, we introduce MWPBert, which uses two parallel BERTnetworks to perform veracity detection on full-text news articles. One of theBERT networks encodes news headline, and another encodes news body. Since theinput length of the BERT network is limited and constant and the news body isusually a long text, we cannot fed the whole news text into the BERT.Therefore, using the MaxWorth algorithm, we selected the part of the news textthat is more valuable for fact-checking, and fed it into the BERT network.Finally, we encode the output of the two BERT networks to an output network toclassify the news. The experiment results showed that the proposed modeloutperformed previous models in terms of accuracy and other performancemeasures.</description><author>Mahmood Farokhian, Vahid Rafe, Hadi Veisi</author><pubDate>Tue, 17 Oct 2023 17:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.04793v2</guid></item><item><title>Predicting User-specific Future Activities using LSTM-based Multi-label Classification</title><link>http://arxiv.org/abs/2211.03100v2</link><description>User-specific future activity prediction in the healthcare domain based onprevious activities can drastically improve the services provided by thenurses. It is challenging because, unlike other domains, activities inhealthcare involve both nurses and patients, and they also vary from hour tohour. In this paper, we employ various data processing techniques to organizeand modify the data structure and an LSTM-based multi-label classifier for anovel 2-stage training approach (user-agnostic pre-training and user-specificfine-tuning). Our experiment achieves a validation accuracy of 31.58\%,precision 57.94%, recall 68.31%, and F1 score 60.38%. We concluded that properdata pre-processing and a 2-stage training process resulted in betterperformance. This experiment is a part of the "Fourth Nurse Care ActivityRecognition Challenge" by our team "Not A Fan of Local Minima".</description><author>Mohammad Sabik Irbaz, Fardin Ahsan Sakib, Lutfun Nahar Lota</author><pubDate>Tue, 17 Oct 2023 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03100v2</guid></item><item><title>Disentangling the Linguistic Competence of Privacy-Preserving BERT</title><link>http://arxiv.org/abs/2310.11363v1</link><description>Differential Privacy (DP) has been tailored to address the unique challengesof text-to-text privatization. However, text-to-text privatization is known fordegrading the performance of language models when trained on perturbed text.Employing a series of interpretation techniques on the internal representationsextracted from BERT trained on perturbed pre-text, we intend to disentangle atthe linguistic level the distortion induced by differential privacy.Experimental results from a representational similarity analysis indicate thatthe overall similarity of internal representations is substantially reduced.Using probing tasks to unpack this dissimilarity, we find evidence thattext-to-text privatization affects the linguistic competence across severalformalisms, encoding localized properties of words while falling short atencoding the contextual relationships between spans of words.</description><author>Stefan Arnold, Nils Kemmerzell, Annika Schreiner</author><pubDate>Tue, 17 Oct 2023 17:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11363v1</guid></item><item><title>Enhancing Neural Machine Translation with Semantic Units</title><link>http://arxiv.org/abs/2310.11360v1</link><description>Conventional neural machine translation (NMT) models typically use subwordsand words as the basic units for model input and comprehension. However,complete words and phrases composed of several tokens are often the fundamentalunits for expressing semantics, referred to as semantic units. To address thisissue, we propose a method Semantic Units for Machine Translation (SU4MT) whichmodels the integral meanings of semantic units within a sentence, and thenleverages them to provide a new perspective for understanding the sentence.Specifically, we first propose Word Pair Encoding (WPE), a phrase extractionmethod to help identify the boundaries of semantic units. Next, we design anAttentive Semantic Fusion (ASF) layer to integrate the semantics of multiplesubwords into a single vector: the semantic unit representation. Lastly, thesemantic-unit-level sentence representation is concatenated to the token-levelone, and they are combined as the input of encoder. Experimental resultsdemonstrate that our method effectively models and leveragessemantic-unit-level information and outperforms the strong baselines. The codeis available at https://github.com/ictnlp/SU4MT.</description><author>Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng</author><pubDate>Tue, 17 Oct 2023 16:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11360v1</guid></item><item><title>Quantifying and maximizing the information flux in recurrent neural networks</title><link>http://arxiv.org/abs/2301.12892v2</link><description>Free-running Recurrent Neural Networks (RNNs), especially probabilisticmodels, generate an ongoing information flux that can be quantified with themutual information $I\left[\vec{x}(t),\vec{x}(t\!+\!1)\right]$ betweensubsequent system states $\vec{x}$. Although, former studies have shown that$I$ depends on the statistics of the network's connection weights, it isunclear (1) how to maximize $I$ systematically and (2) how to quantify the fluxin large systems where computing the mutual information becomes intractable.Here, we address these questions using Boltzmann machines as model systems. Wefind that in networks with moderately strong connections, the mutualinformation $I$ is approximately a monotonic transformation of theroot-mean-square averaged Pearson correlations between neuron-pairs, a quantitythat can be efficiently computed even in large systems. Furthermore,evolutionary maximization of $I\left[\vec{x}(t),\vec{x}(t\!+\!1)\right]$reveals a general design principle for the weight matrices enabling thesystematic construction of systems with a high spontaneous information flux.Finally, we simultaneously maximize information flux and the mean period lengthof cyclic attractors in the state space of these dynamical networks. Ourresults are potentially useful for the construction of RNNs that serve asshort-time memories or pattern generators.</description><author>Claus Metzner, Marius E. Yamakou, Dennis Voelkl, Achim Schilling, Patrick Krauss</author><pubDate>Tue, 17 Oct 2023 16:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12892v2</guid></item><item><title>MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space</title><link>http://arxiv.org/abs/2305.12785v2</link><description>Multi-aspect controllable text generation aims to generate fluent sentencesthat possess multiple desired attributes simultaneously. Traditional methodseither combine many operators in the decoding stage, often with costlyiteration or search in the discrete text space, or train separate controllersfor each aspect, resulting in a degeneration of text quality due to thediscrepancy between different aspects. To address these limitations, weintroduce a novel approach for multi-aspect control, namely MacLaSa, thatestimates compact latent space for multiple aspects and performs efficientsampling with a robust sampler based on ordinary differential equations (ODEs).To eliminate the domain gaps between different aspects, we utilize aVariational Autoencoder (VAE) network to map text sequences from varying datasources into close latent representations. The estimated latent space enablesthe formulation of joint energy-based models (EBMs) and the plugging in ofarbitrary attribute discriminators to achieve multi-aspect control. Afterwards,we draw latent vector samples with an ODE-based sampler and feed sampledexamples to the VAE decoder to produce target text sequences. Experimentalresults demonstrate that MacLaSa outperforms several strong baselines onattribute relevance and textual quality while maintaining a high inferencespeed.</description><author>Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng, Tat-Seng Chua</author><pubDate>Tue, 17 Oct 2023 16:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12785v2</guid></item><item><title>PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale</title><link>http://arxiv.org/abs/2304.12206v2</link><description>Existing question answering (QA) systems owe much of their success to large,high-quality training data. Such annotation efforts are costly, and thedifficulty compounds in the cross-lingual setting. Therefore, priorcross-lingual QA work has focused on releasing evaluation datasets, and thenapplying zero-shot methods as baselines. This work proposes a synthetic datageneration method for cross-lingual QA which leverages indirect supervisionfrom existing parallel corpora. Our method termed PAXQA (Projecting annotationsfor cross-lingual (x) QA) decomposes cross-lingual QA into two stages. First,we apply a question generation (QG) model to the English side. Second, we applyannotation projection to translate both the questions and answers. To bettertranslate questions, we propose a novel use of lexically-constrained machinetranslation, in which constrained entities are extracted from the parallelbitexts. We apply PAXQA to generate cross-lingual QA examples in 4 languages (662Kexamples total), and perform human evaluation on a subset to create validationand test splits. We then show that models fine-tuned on these datasetsoutperform prior synthetic data generation models over several extractive QAdatasets. The largest performance gains are for directions with non-Englishquestions and English contexts. Ablation studies show that our datasetgeneration method is relatively robust to noise from automatic word alignments,showing the sufficient quality of our generations. To facilitate follow-upwork, we release our code and datasets at https://github.com/manestay/paxqa .</description><author>Bryan Li, Chris Callison-Burch</author><pubDate>Tue, 17 Oct 2023 16:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12206v2</guid></item><item><title>ALP: Action-Aware Embodied Learning for Perception</title><link>http://arxiv.org/abs/2306.10190v2</link><description>Current methods in training and benchmarking vision models exhibit anover-reliance on passive, curated datasets. Although models trained on thesedatasets have shown strong performance in a wide variety of tasks such asclassification, detection, and segmentation, they fundamentally are unable togeneralize to an ever-evolving world due to constant out-of-distribution shiftsof input data. Therefore, instead of training on fixed datasets, can weapproach learning in a more human-centric and adaptive manner? In this paper,we introduce Action-Aware Embodied Learning for Perception (ALP), an embodiedlearning framework that incorporates action information into representationlearning through a combination of optimizing a reinforcement learning policyand an inverse dynamics prediction objective. Our method actively explores incomplex 3D environments to both learn generalizable task-agnostic visualrepresentations as well as collect downstream training data. We show that ALPoutperforms existing baselines in several downstream perception tasks. Inaddition, we show that by training on actively collected data more relevant tothe environment and task, our method generalizes more robustly to downstreamtasks compared to models pre-trained on fixed datasets such as ImageNet.</description><author>Xinran Liang, Anthony Han, Wilson Yan, Aditi Raghunathan, Pieter Abbeel</author><pubDate>Tue, 17 Oct 2023 16:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10190v2</guid></item><item><title>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains</title><link>http://arxiv.org/abs/2310.07838v2</link><description>We characterize the statistical efficiency of knowledge transfer through $n$samples from a teacher to a probabilistic student classifier with input space$\mathcal S$ over labels $\mathcal A$. We show that privileged information atthree progressive levels accelerates the transfer. At the first level, onlysamples with hard labels are known, via which the maximum likelihood estimatorattains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. Thesecond level has the teacher probabilities of sampled labels available inaddition, which turns out to boost the convergence rate lower bound to${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second dataacquisition protocol, minimizing a naive adaptation of the cross-entropy lossresults in an asymptotically biased student. We overcome this limitation andachieve the fundamental limit by using a novel empirical variant of the squarederror logit loss. The third level further equips the student with the softlabels (complete logits) on ${\mathcal A}$ given every sampled input, therebyprovably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to beoptimal in the last case. Numerical simulations distinguish the four learnersand corroborate our theory.</description><author>Qingyue Zhao, Banghua Zhu</author><pubDate>Tue, 17 Oct 2023 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07838v2</guid></item><item><title>Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance</title><link>http://arxiv.org/abs/2305.10664v2</link><description>From the classical and influential works of Neal (1996), it is known that theinfinite width scaling limit of a Bayesian neural network with one hidden layeris a Gaussian process, \emph{when the network weights have bounded priorvariance}. Neal's result has been extended to networks with multiple hiddenlayers and to convolutional neural networks, also with Gaussian process scalinglimits. The tractable properties of Gaussian processes then allowstraightforward posterior inference and uncertainty quantification,considerably simplifying the study of the limit process compared to a networkof finite width. Neural network weights with unbounded variance, however, poseunique challenges. In this case, the classical central limit theorem breaksdown and it is well known that the scaling limit is an $\alpha$-stable processunder suitable conditions. However, current literature is primarily limited toforward simulations under these processes and the problem of posteriorinference under such a scaling limit remains largely unaddressed, unlike in theGaussian process case. To this end, our contribution is an interpretable andcomputationally efficient procedure for posterior inference, using a\emph{conditionally Gaussian} representation, that then allows full use of theGaussian process machinery for tractable posterior inference and uncertaintyquantification in the non-Gaussian regime.</description><author>Jorge Loría, Anindya Bhadra</author><pubDate>Tue, 17 Oct 2023 16:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10664v2</guid></item><item><title>Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing</title><link>http://arxiv.org/abs/2310.11346v1</link><description>Detecting objects in 3D space using multiple cameras, known as Multi-Camera3D Object Detection (MC3D-Det), has gained prominence with the advent ofbird's-eye view (BEV) approaches. However, these methods often struggle whenfaced with unfamiliar testing environments due to the lack of diverse trainingdata encompassing various viewpoints and environments. To address this, wepropose a novel method that aligns 3D detection with 2D camera plane results,ensuring consistent and accurate detections. Our framework, anchored inperspective debiasing, helps the learning of features resilient to domainshifts. In our approach, we render diverse view maps from BEV features andrectify the perspective bias of these maps, leveraging implicit foregroundvolumes to bridge the camera and BEV planes. This two-step process promotes thelearning of perspective- and context-independent features, crucial for accurateobject detection across varying viewpoints, camera parameters and environmentconditions. Notably, our model-agnostic approach preserves the original networkstructure without incurring additional inference costs, facilitating seamlessintegration across various models and simplifying deployment. Furthermore, wealso show our approach achieves satisfactory results in real data when trainedonly with virtual datasets, eliminating the need for real scene annotations.Experimental results on both Domain Generalization (DG) and Unsupervised DomainAdaptation (UDA) clearly demonstrate its effectiveness. Our code will bereleased.</description><author>Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, Yingcong Chen</author><pubDate>Tue, 17 Oct 2023 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11346v1</guid></item><item><title>The effect of stemming and lemmatization on Portuguese fake news text classification</title><link>http://arxiv.org/abs/2310.11344v1</link><description>With the popularization of the internet, smartphones and social media,information is being spread quickly and easily way, which implies biggertraffic of information in the world, but there is a problem that is harmingsociety with the dissemination of fake news. With a bigger flow of information,some people are trying to disseminate deceptive information and fake news. Theautomatic detection of fake news is a challenging task because to obtain a goodresult is necessary to deal with linguistics problems, especially when we aredealing with languages that not have been comprehensively studied yet, besidesthat, some techniques can help to reach a good result when we are dealing withtext data, although, the motivation of detecting this deceptive information itis in the fact that the people need to know which information is true andtrustful and which one is not. In this work, we present the effect thepre-processing methods such as lemmatization and stemming have on fake newsclassification, for that we designed some classifier models applying differentpre-processing techniques. The results show that the pre-processing step isimportant to obtain betters results, the stemming and lemmatization techniquesare interesting methods and need to be more studied to develop techniquesfocused on the Portuguese language so we can reach better results.</description><author>Lucca de Freitas Santos, Murilo Varges da Silva</author><pubDate>Tue, 17 Oct 2023 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11344v1</guid></item><item><title>Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning</title><link>http://arxiv.org/abs/2310.11341v1</link><description>Artificial neural networks (ANNs) exhibit a narrow scope of expertise onstationary independent data. However, the data in the real world is continuousand dynamic, and ANNs must adapt to novel scenarios while also retaining thelearned knowledge to become lifelong learners. The ability of humans to excelat these tasks can be attributed to multiple factors ranging from cognitivecomputational structures, cognitive biases, and the multi-memory systems in thebrain. We incorporate key concepts from each of these to design a novelframework, Dual Cognitive Architecture (DUCA), which includes multiplesub-systems, implicit and explicit knowledge representation dichotomy,inductive bias, and a multi-memory system. The inductive bias learner withinDUCA is instrumental in encoding shape information, effectively countering thetendency of ANNs to learn local textures. Simultaneously, the inclusion of asemantic memory submodule facilitates the gradual consolidation of knowledge,replicating the dynamics observed in fast and slow learning systems,reminiscent of the principles underpinning the complementary learning system inhuman cognition. DUCA shows improvement across different settings and datasets,and it also exhibits reduced task recency bias, without the need for extrainformation. To further test the versatility of lifelong learning methods on achallenging distribution shift, we introduce a novel domain-incremental datasetDN4IL. In addition to improving performance on existing benchmarks, DUCA alsodemonstrates superior performance on this complex dataset.</description><author>Shruthi Gowda, Bahram Zonooz, Elahe Arani</author><pubDate>Tue, 17 Oct 2023 16:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11341v1</guid></item><item><title>Contextualized Machine Learning</title><link>http://arxiv.org/abs/2310.11340v1</link><description>We examine Contextualized Machine Learning (ML), a paradigm for learningheterogeneous and context-dependent effects. Contextualized ML estimatesheterogeneous functions by applying deep learning to the meta-relationshipbetween contextual information and context-specific parametric models. This isa form of varying-coefficient modeling that unifies existing frameworksincluding cluster analysis and cohort modeling by introducing two reusableconcepts: a context encoder which translates sample context into modelparameters, and sample-specific model which operates on sample predictors. Wereview the process of developing contextualized models, nonparametric inferencefrom contextualized models, and identifiability conditions of contextualizedmodels. Finally, we present the open-source PyTorch package ContextualizedML.</description><author>Benjamin Lengerich, Caleb N. Ellington, Andrea Rubbi, Manolis Kellis, Eric P. Xing</author><pubDate>Tue, 17 Oct 2023 16:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11340v1</guid></item><item><title>CARSO: Blending Adversarial Training and Purification Improves Adversarial Robustness</title><link>http://arxiv.org/abs/2306.06081v3</link><description>In this work, we propose a novel adversarial defence mechanism for imageclassification - CARSO - blending the paradigms of adversarial training andadversarial purification in a mutually-beneficial, robustness-enhancing way.The method builds upon an adversarially-trained classifier, and learns to mapits internal representation associated with a potentially perturbed input ontoa distribution of tentative clean reconstructions. Multiple samples from suchdistribution are classified by the adversarially-trained model itself, and anaggregation of its outputs finally constitutes the robust prediction ofinterest. Experimental evaluation by a well-established benchmark of varied,strong adaptive attacks, across different image datasets and classifierarchitectures, shows that CARSO is able to defend itself against foreseen andunforeseen threats, including adaptive end-to-end attacks devised forstochastic defences. Paying a tolerable clean accuracy toll, our methodimproves by a significant margin the state of the art for CIFAR-10 andCIFAR-100 $\ell_\infty$ robust classification accuracy against AutoAttack. Codeand pre-trained models are available at https://github.com/emaballarin/CARSO .</description><author>Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi</author><pubDate>Tue, 17 Oct 2023 16:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06081v3</guid></item><item><title>Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models</title><link>http://arxiv.org/abs/2307.10236v3</link><description>The recent performance leap of Large Language Models (LLMs) opens up newopportunities across numerous industrial applications and domains. However,erroneous generations, such as false predictions, misinformation, andhallucination made by LLMs, have also raised severe concerns for thetrustworthiness of LLMs', especially in safety-, security- andreliability-sensitive scenarios, potentially hindering real-world adoptions.While uncertainty estimation has shown its potential for interpreting theprediction risks made by general machine learning (ML) models, little is knownabout whether and to what extent it can help explore an LLM's capabilities andcounteract its undesired behavior. To bridge the gap, in this paper, weinitiate an exploratory study on the risk assessment of LLMs from the lens ofuncertainty. In particular, we experiment with twelve uncertainty estimationmethods and four LLMs on four prominent natural language processing (NLP) tasksto investigate to what extent uncertainty estimation techniques could helpcharacterize the prediction risks of LLMs. Our findings validate theeffectiveness of uncertainty estimation for revealing LLMs'uncertain/non-factual predictions. In addition to general NLP tasks, weextensively conduct experiments with four LLMs for code generation on twodatasets. We find that uncertainty estimation can potentially uncover buggyprograms generated by LLMs. Insights from our study shed light on future designand development for reliable LLMs, facilitating further research towardenhancing the trustworthiness of LLMs.</description><author>Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, Lei Ma</author><pubDate>Tue, 17 Oct 2023 16:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10236v3</guid></item><item><title>Non-ergodicity in reinforcement learning: robustness via ergodicity transformations</title><link>http://arxiv.org/abs/2310.11335v1</link><description>Envisioned application areas for reinforcement learning (RL) includeautonomous driving, precision agriculture, and finance, which all require RLagents to make decisions in the real world. A significant challenge hinderingthe adoption of RL methods in these domains is the non-robustness ofconventional algorithms. In this paper, we argue that a fundamental issuecontributing to this lack of robustness lies in the focus on the expected valueof the return as the sole "correct" optimization objective. The expected valueis the average over the statistical ensemble of infinitely many trajectories.For non-ergodic returns, this average differs from the average over a singlebut infinitely long trajectory. Consequently, optimizing the expected value canlead to policies that yield exceptionally high returns with probability zerobut almost surely result in catastrophic outcomes. This problem can becircumvented by transforming the time series of collected returns into one withergodic increments. This transformation enables learning robust policies byoptimizing the long-term return for individual agents rather than the averageacross infinitely many trajectories. We propose an algorithm for learningergodicity transformations from data and demonstrate its effectiveness in aninstructive, non-ergodic environment and on standard RL benchmarks.</description><author>Dominik Baumann, Erfaun Noorani, James Price, Ole Peters, Colm Connaughton, Thomas B. Schön</author><pubDate>Tue, 17 Oct 2023 16:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11335v1</guid></item><item><title>Agent-Specific Effects</title><link>http://arxiv.org/abs/2310.11334v1</link><description>Establishing causal relationships between actions and outcomes is fundamentalfor accountable multi-agent decision-making. However, interpreting andquantifying agents' contributions to such relationships pose significantchallenges. These challenges are particularly prominent in the context ofmulti-agent sequential decision-making, where the causal effect of an agent'saction on the outcome depends on how the other agents respond to that action.In this paper, our objective is to present a systematic approach forattributing the causal effects of agents' actions to the influence they exerton other agents. Focusing on multi-agent Markov decision processes, weintroduce agent-specific effects (ASE), a novel causal quantity that measuresthe effect of an agent's action on the outcome that propagates through otheragents. We then turn to the counterfactual counterpart of ASE (cf-ASE), providea sufficient set of conditions for identifying cf-ASE, and propose a practicalsampling-based algorithm for estimating it. Finally, we experimentally evaluatethe utility of cf-ASE through a simulation-based testbed, which includes asepsis management environment.</description><author>Stelios Triantafyllou, Aleksa Sukovic, Debmalya Mandal, Goran Radanovic</author><pubDate>Tue, 17 Oct 2023 16:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11334v1</guid></item><item><title>Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking</title><link>http://arxiv.org/abs/2310.11333v1</link><description>Selective robotic harvesting is a promising technological solution to addresslabour shortages which are affecting modern agriculture in many parts of theworld. For an accurate and efficient picking process, a robotic harvesterrequires the precise location and orientation of the fruit to effectively planthe trajectory of the end effector. The current methods for estimating fruitorientation employ either complete 3D information which typically requiresregistration from multiple views or rely on fully-supervised learningtechniques, which require difficult-to-obtain manual annotation of thereference orientation. In this paper, we introduce a novel key-point-basedfruit orientation estimation method allowing for the prediction of 3Dorientation from 2D images directly. The proposed technique can work withoutfull 3D orientation annotations but can also exploit such information forimproved accuracy. We evaluate our work on two separate datasets of strawberryimages obtained from real-world data collection scenarios. Our proposed methodachieves state-of-the-art performance with an average error as low as$8^{\circ}$, improving predictions by $\sim30\%$ compared to previous workpresented in~\cite{wagner2021efficient}. Furthermore, our method is suited forreal-time robotic applications with fast inference times of $\sim30$ms.</description><author>Justin Le Louëdec, Grzegorz Cielniak</author><pubDate>Tue, 17 Oct 2023 16:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11333v1</guid></item><item><title>zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning</title><link>http://arxiv.org/abs/2310.02554v2</link><description>Federated Learning (FL) is a machine learning paradigm, which enablesmultiple and decentralized clients to collaboratively train a model under theorchestration of a central aggregator. Traditional FL solutions rely on thetrust assumption of the centralized aggregator, which forms cohorts of clientsin a fair and honest manner. However, a malicious aggregator, in reality, couldabandon and replace the client's training models, or launch Sybil attacks toinsert fake clients. Such malicious behaviors give the aggregator more power tocontrol clients in the FL setting and determine the final training results. Inthis work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) totackle the issue of a malicious aggregator during the training modelaggregation process. To guarantee the correct aggregation results, theaggregator needs to provide a proof per round. The proof can demonstrate to theclients that the aggregator executes the intended behavior faithfully. Tofurther reduce the verification cost of clients, we employ a blockchain tohandle the proof in a zero-knowledge way, where miners (i.e., the nodesvalidating and maintaining the blockchain data) can verify the proof withoutknowing the clients' local and aggregated models. The theoretical analysis andempirical results show that zkFL can achieve better security and privacy thantraditional FL, without modifying the underlying FL network structure orheavily compromising the training speed.</description><author>Zhipeng Wang, Nanqing Dong, Jiahao Sun, William Knottenbelt</author><pubDate>Tue, 17 Oct 2023 16:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02554v2</guid></item><item><title>Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</title><link>http://arxiv.org/abs/2310.11324v1</link><description>As large language models (LLMs) are adopted as a fundamental component oflanguage technologies, it is crucial to accurately characterize theirperformance. Because choices in prompt design can strongly influence modelbehavior, this design process is critical in effectively using any modernpre-trained generative language model. In this work, we focus on LLMsensitivity to a quintessential class of meaning-preserving design choices:prompt formatting. We find that several widely used open-source LLMs areextremely sensitive to subtle changes in prompt formatting in few-shotsettings, with performance differences of up to 76 accuracy points whenevaluated using LLaMA-2-13B. Sensitivity remains even when increasing modelsize, the number of few-shot examples, or performing instruction tuning. Ouranalysis suggests that work evaluating LLMs with prompting-based methods wouldbenefit from reporting a range of performance across plausible prompt formats,instead of the currently-standard practice of reporting performance on a singleformat. We also show that format performance only weakly correlates betweenmodels, which puts into question the methodological validity of comparingmodels with an arbitrarily chosen, fixed prompt format. To facilitatesystematic analysis we propose FormatSpread, an algorithm that rapidlyevaluates a sampled set of plausible prompt formats for a given task, andreports the interval of expected performance without accessing model weights.Furthermore, we present a suite of analyses that characterize the nature ofthis sensitivity, including exploring the influence of particular atomicperturbations and the internal representation of particular formats.</description><author>Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr</author><pubDate>Tue, 17 Oct 2023 16:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11324v1</guid></item><item><title>The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation</title><link>http://arxiv.org/abs/2305.09652v2</link><description>End-to-end spoken language understanding (SLU) remains elusive even withcurrent large pretrained language models on text and speech, especially inmultilingual cases. Machine translation has been established as a powerfulpretraining objective on text as it enables the model to capture high-levelsemantics of the input utterance and associations between different languages,which is desired for speech models that work on lower-level acoustic frames.Motivated particularly by the task of cross-lingual SLU, we demonstrate thatthe task of speech translation (ST) is a good means of pretraining speechmodels for end-to-end SLU on both intra- and cross-lingual scenarios. By introducing ST, our models reach higher performance over baselines onmonolingual and multilingual intent classification as well as spoken questionanswering using SLURP, MINDS-14, and NMSQA benchmarks. To verify theeffectiveness of our methods, we also create new benchmark datasets from bothsynthetic and real sources, for speech summarization and low-resource/zero-shottransfer from English to French or Spanish. We further show the value ofpreserving knowledge for the ST pretraining task for better downstreamperformance, possibly using Bayesian transfer regularizers.</description><author>Mutian He, Philip N. Garner</author><pubDate>Tue, 17 Oct 2023 15:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09652v2</guid></item><item><title>Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</title><link>http://arxiv.org/abs/2310.11320v1</link><description>Volume-wise labeling in 3D medical images is a time-consuming task thatrequires expertise. As a result, there is growing interest in usingsemi-supervised learning (SSL) techniques to train models with limited labeleddata. However, the challenges and practical applications extend beyond SSL tosettings such as unsupervised domain adaptation (UDA) and semi-superviseddomain generalization (SemiDG). This work aims to develop a generic SSLframework that can handle all three settings. We identify two main obstacles toachieving this goal in the existing SSL framework: 1) the weakness of capturingdistribution-invariant features; and 2) the tendency for unlabeled data to beoverwhelmed by labeled data, leading to over-fitting to the labeled data duringtraining. To address these issues, we propose an Aggregating &amp; Decouplingframework. The aggregating part consists of a Diffusion encoder that constructsa common knowledge set by extracting distribution-invariant features fromaggregated information from multiple distributions/domains. The decoupling partconsists of three decoders that decouple the training process with labeled andunlabeled data, thus avoiding over-fitting to labeled data, specific domainsand classes. We evaluate our proposed framework on four benchmark datasets forSSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notableimprovements compared to state-of-the-art methods across all four settings,indicating the potential of our framework to tackle more challenging SSLscenarios. Code and models are available at:https://github.com/xmed-lab/GenericSSL.</description><author>Haonan Wang, Xiaomeng Li</author><pubDate>Tue, 17 Oct 2023 15:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11320v1</guid></item><item><title>BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations</title><link>http://arxiv.org/abs/2310.07276v2</link><description>Recent advancements in biological research leverage the integration ofmolecules, proteins, and natural language to enhance drug discovery. However,current models exhibit several limitations, such as the generation of invalidmolecular SMILES, underutilization of contextual information, and equaltreatment of structured and unstructured knowledge. To address these issues, wepropose $\mathbf{BioT5}$, a comprehensive pre-training framework that enrichescross-modal integration in biology with chemical knowledge and natural languageassociations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecularrepresentations and extracts knowledge from the surrounding context ofbio-entities in unstructured biological literature. Furthermore,$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,leading to more effective utilization of information. After fine-tuning, BioT5shows superior performance across a wide range of tasks, demonstrating itsstrong capability of capturing underlying relations and properties ofbio-entities. Our code is available at$\href{https://github.com/QizhiPei/BioT5}{Github}$.</description><author>Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan</author><pubDate>Tue, 17 Oct 2023 15:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07276v2</guid></item><item><title>FABind: Fast and Accurate Protein-Ligand Binding</title><link>http://arxiv.org/abs/2310.06763v3</link><description>Modeling the interaction between proteins and ligands and accuratelypredicting their binding structures is a critical yet challenging task in drugdiscovery. Recent advancements in deep learning have shown promise inaddressing this challenge, with sampling-based and regression-based methodsemerging as two prominent approaches. However, these methods have notablelimitations. Sampling-based methods often suffer from low efficiency due to theneed for generating multiple candidate structures for selection. On the otherhand, regression-based methods offer fast predictions but may experiencedecreased accuracy. Additionally, the variation in protein sizes often requiresexternal modules for selecting suitable binding pockets, further impactingefficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end modelthat combines pocket prediction and docking to achieve accurate and fastprotein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informedpocket prediction module, which is also leveraged for docking pose estimation.The model further enhances the docking process by incrementally integrating thepredicted pocket to optimize protein-ligand binding, reducing discrepanciesbetween training and inference. Through extensive experiments on benchmarkdatasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages interms of effectiveness and efficiency compared to existing methods. Our code isavailable at $\href{https://github.com/QizhiPei/FABind}{Github}$.</description><author>Qizhi Pei, Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Yingce Xia, Shufang Xie, Tao Qin, Kun He, Tie-Yan Liu, Rui Yan</author><pubDate>Tue, 17 Oct 2023 15:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06763v3</guid></item><item><title>Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue</title><link>http://arxiv.org/abs/2310.11318v1</link><description>In support of open and reproducible research, there has been a rapidlyincreasing number of datasets made available for research. As the availabilityof datasets increases, it becomes more important to have quality metadata fordiscovering and reusing them. Yet, it is a common issue that datasets oftenlack quality metadata due to limited resources for data curation. Meanwhile,technologies such as artificial intelligence and large language models (LLMs)are progressing rapidly. Recently, systems based on these technologies, such asChatGPT, have demonstrated promising capabilities for certain data curationtasks. This paper proposes to leverage LLMs for cost-effective annotation ofsubject metadata through the LLM-based in-context learning. Our method employsGPT-3.5 with prompts designed for annotating subject metadata, demonstratingpromising performance in automatic metadata annotation. However, models basedon in-context learning cannot acquire discipline-specific rules, resulting inlower performance in several categories. This limitation arises from thelimited contextual information available for subject inference. To the best ofour knowledge, we are introducing, for the first time, an in-context learningmethod that harnesses large language models for automated subject metadataannotation.</description><author>Shiwei Zhang, Mingfang Wu, Xiuzhen Zhang</author><pubDate>Tue, 17 Oct 2023 15:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11318v1</guid></item><item><title>Deep Image Clustering with Contrastive Learning and Multi-scale Graph Convolutional Networks</title><link>http://arxiv.org/abs/2207.07173v3</link><description>Deep clustering has shown its promising capability in joint representationlearning and clustering via deep neural networks. Despite the significantprogress, the existing deep clustering works mostly utilize somedistribution-based clustering loss, lacking the ability to unify representationlearning and multi-scale structure learning. To address this, this paperpresents a new deep clustering approach termed image clustering withcontrastive learning and multi-scale graph convolutional networks (IcicleGCN),which bridges the gap between convolutional neural network (CNN) and graphconvolutional network (GCN) as well as the gap between contrastive learning andmulti-scale structure learning for the deep clustering task. Our frameworkconsists of four main modules, namely, the CNN-based backbone, the InstanceSimilarity Module (ISM), the Joint Cluster Structure Learning and Instancereconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN).Specifically, the backbone network with two weight-sharing views is utilized tolearn the representations for the two augmented samples (from each image). Thelearned representations are then fed to ISM and JC-SLIM for jointinstance-level and cluster-level contrastive learning, respectively, duringwhich an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to theM-GCN module. Further, to enforce multi-scale neighborhood structure learning,two streams of GCNs and the auto-encoder are simultaneously trained via (i) thelayer-wise interaction with representation fusion and (ii) the jointself-adaptive learning. Experiments on multiple image datasets demonstrate thesuperior clustering performance of IcicleGCN over the state-of-the-art. Thecode is available at https://github.com/xuyuankun631/IcicleGCN.</description><author>Yuankun Xu, Dong Huang, Chang-Dong Wang, Jian-Huang Lai</author><pubDate>Tue, 17 Oct 2023 15:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07173v3</guid></item><item><title>Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model</title><link>http://arxiv.org/abs/2310.09520v2</link><description>While large language models have proven effective in a huge range ofdownstream applications, they often generate text that is problematic or lacksa desired attribute. In this paper, we introduce Reward-Augmented Decoding(RAD), a text generation procedure that uses a small unidirectional rewardmodel to encourage a language model to generate text that has certainproperties. Specifically, RAD uses the reward model to score generations asthey are produced and rescales sampling probabilities to favor high-rewardtokens. By using a unidirectional reward model, RAD can cache activations fromprior generation steps to decrease computational overhead. Through experimentson generating non-toxic and sentiment-controlled text, we demonstrate that RADperforms best among methods that change only the generation procedure andmatches the performance of state-of-the-art methods that involve re-trainingthe language model. We further validate that RAD is effective on very largelanguage models while incurring a minimal computational overhead.</description><author>Haikang Deng, Colin Raffel</author><pubDate>Tue, 17 Oct 2023 15:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09520v2</guid></item><item><title>MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient</title><link>http://arxiv.org/abs/2310.11316v1</link><description>Monocular 3D object detection is an inherently ill-posed problem, as it ischallenging to predict accurate 3D localization from a single image. Existingmonocular 3D detection knowledge distillation methods usually project the LiDARonto the image plane and train the teacher network accordingly. TransferringLiDAR-based model knowledge to RGB-based models is more complex, so a generaldistillation strategy is needed. To alleviate cross-modal prob-lem, we proposeMonoSKD, a novel Knowledge Distillation framework for Monocular 3D detectionbased on Spearman correlation coefficient, to learn the relative correlationbetween cross-modal features. Considering the large gap between these features,strict alignment of features may mislead the training, so we propose a looserSpearman loss. Furthermore, by selecting appropriate distillation locations andremoving redundant modules, our scheme saves more GPU resources and trainsfaster than existing methods. Extensive experiments are performed to verify theeffectiveness of our framework on the challenging KITTI 3D object detectionbenchmark. Our method achieves state-of-the-art performance until submissionwith no additional inference computational cost. Our codes are available athttps://github.com/Senwang98/MonoSKD</description><author>Sen Wang, Jin Zheng</author><pubDate>Tue, 17 Oct 2023 15:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11316v1</guid></item><item><title>TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models</title><link>http://arxiv.org/abs/2305.11171v2</link><description>Factual consistency evaluation is often conducted using Natural LanguageInference (NLI) models, yet these models exhibit limited success in evaluatingsummaries. Previous work improved such models with synthetic training data.However, the data is typically based on perturbed human-written summaries,which often differ in their characteristics from real model-generated summariesand have limited coverage of possible factual errors. Alternatively, largelanguage models (LLMs) have recently shown promising results in directlyevaluating generative tasks, but are too computationally expensive forpractical use. Motivated by these limitations, we introduce TrueTeacher, amethod for generating synthetic data by annotating diverse model-generatedsummaries using a LLM. Unlike prior work, TrueTeacher does not rely onhuman-written summaries, and is multilingual by nature. Experiments on the TRUEbenchmark show that a student model trained using our data, substantiallyoutperforms both the state-of-the-art model with similar capacity, and the LLMteacher. In a systematic study, we compare TrueTeacher to existing syntheticdata generation methods and demonstrate its superiority and robustness todomain-shift. We also show that our method generalizes to multilingualscenarios. Lastly, we release our large scale synthetic dataset (1.4Mexamples), generated using TrueTeacher, and a checkpoint trained on this data.</description><author>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor</author><pubDate>Tue, 17 Oct 2023 15:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11171v2</guid></item><item><title>Active Learning for Natural Language Generation</title><link>http://arxiv.org/abs/2305.15040v2</link><description>The field of Natural Language Generation (NLG) suffers from a severe shortageof labeled data due to the extremely expensive and time-consuming processinvolved in manual annotation. A natural approach for coping with this problemis active learning (AL), a well-known machine learning technique for improvingannotation efficiency by selectively choosing the most informative examples tolabel. However, while AL has been well-researched in the context of textclassification, its application to NLG remains largely unexplored. In thispaper, we present a first systematic study of active learning for NLG,considering a diverse set of tasks and multiple leading selection strategies,and harnessing a strong instruction-tuned model. Our results indicate that theperformance of existing AL strategies is inconsistent, surpassing the baselineof random example selection in some cases but not in others. We highlight somenotable differences between the classification and generation scenarios, andanalyze the selection behaviors of existing AL strategies. Our findingsmotivate exploring novel approaches for applying AL to generation tasks.</description><author>Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, Liat Ein-Dor</author><pubDate>Tue, 17 Oct 2023 15:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15040v2</guid></item><item><title>Elucidating The Design Space of Classifier-Guided Diffusion Generation</title><link>http://arxiv.org/abs/2310.11311v1</link><description>Guidance in conditional diffusion generation is of great importance forsample quality and controllability. However, existing guidance schemes are tobe desired. On one hand, mainstream methods such as classifier guidance andclassifier-free guidance both require extra training with labeled data, whichis time-consuming and unable to adapt to new conditions. On the other hand,training-free methods such as universal guidance, though more flexible, haveyet to demonstrate comparable performance. In this work, through acomprehensive investigation into the design space, we show that it is possibleto achieve significant performance improvements over existing guidance schemesby leveraging off-the-shelf classifiers in a training-free fashion, enjoyingthe best of both worlds. Employing calibration as a general guideline, wepropose several pre-conditioning techniques to better exploit pretrainedoff-the-shelf classifiers for guiding diffusion generation. Extensiveexperiments on ImageNet validate our proposed method, showing thatstate-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (upto 20%) using off-the-shelf classifiers with barely any extra computationalcost. With the proliferation of publicly available pretrained classifiers, ourproposed approach has great potential and can be readily scaled up totext-to-image generation tasks. The code is available athttps://github.com/AlexMaOLS/EluCD/tree/main.</description><author>Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun</author><pubDate>Tue, 17 Oct 2023 15:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11311v1</guid></item><item><title>Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection</title><link>http://arxiv.org/abs/2310.11307v1</link><description>Intelligent transportation system combines advanced information technology toprovide intelligent services such as monitoring, detection, and early warningfor modern transportation. Intelligent transportation detection is thecornerstone of many intelligent traffic services by identifying task targetsthrough object detection methods. However existing detection methods inintelligent transportation are limited by two aspects. First, there is adifference between the model knowledge pre-trained on large-scale datasets andthe knowledge required for target task. Second, most detection models followthe pattern of single-source learning, which limits the learning ability. Toaddress these problems, we propose a Multi Self-supervised Pre-fine-tunedTransformer Fusion (MSPTF) network, consisting of two steps: unsupervisedpre-fine-tune domain knowledge learning and multi-model fusion target tasklearning. In the first step, we introduced self-supervised learning methodsinto transformer model pre-fine-tune which could reduce data costs andalleviate the knowledge gap between pre-trained model and target task. In thesecond step, we take feature information differences between different modelarchitectures and different pre-fine-tune tasks into account and proposeMulti-model Semantic Consistency Cross-attention Fusion (MSCCF) network tocombine different transformer model features by considering channel semanticconsistency and feature vector semantic consistency, which obtain more completeand proper fusion features for detection task. We experimented the proposedmethod on vehicle recognition dataset and road disease detection dataset andachieved 1.1%, 5.5%, 4.2% improvement compared with baseline and 0.7%, 1.8%,1.7% compared with sota, which proved the effectiveness of our method.</description><author>Juwu Zheng, Jiangtao Ren</author><pubDate>Tue, 17 Oct 2023 15:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11307v1</guid></item><item><title>MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games</title><link>http://arxiv.org/abs/2310.11305v1</link><description>This paper presents MiniZero, a zero-knowledge learning framework thatsupports four state-of-the-art algorithms, including AlphaZero, MuZero, GumbelAlphaZero, and Gumbel MuZero. While these algorithms have demonstratedsuper-human performance in many games, it remains unclear which among them ismost suitable or efficient for specific tasks. Through MiniZero, wesystematically evaluate the performance of each algorithm in two board games,9x9 Go and 8x8 Othello, as well as 57 Atari games. Our empirical findings aresummarized as follows. For two board games, using more simulations generallyresults in higher performance. However, the choice of AlphaZero and MuZero maydiffer based on game properties. For Atari games, both MuZero and Gumbel MuZeroare worth considering. Since each game has unique characteristics, differentalgorithms and simulations yield varying results. In addition, we introduce anapproach, called progressive simulation, which progressively increases thesimulation budget during training to allocate computation more efficiently. Ourempirical results demonstrate that progressive simulation achievessignificantly superior performance in two board games. By making our frameworkand trained models publicly available, this paper contributes a benchmark forfuture research on zero-knowledge learning algorithms, assisting researchers inalgorithm selection and comparison against these zero-knowledge learningbaselines.</description><author>Ti-Rong Wu, Hung Guei, Po-Wei Huang, Pei-Chiun Peng, Ting Han Wei, Chung-Chin Shih, Yun-Jui Tsai</author><pubDate>Tue, 17 Oct 2023 15:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11305v1</guid></item><item><title>QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering</title><link>http://arxiv.org/abs/2310.11303v1</link><description>Zero-shot commonsense Question-Answering (QA) requires models to reason aboutgeneral situations beyond specific benchmarks. State-of-the-art approachesfine-tune language models on QA pairs constructed from CommonSense KnowledgeBases (CSKBs) to equip the models with more commonsense knowledge in a QAcontext. However, current QA synthesis protocols may introduce noise from theCSKBs and generate ungrammatical questions and false negative options, whichimpede the model's ability to generalize. To address these issues, we proposeQADYNAMICS, a training dynamics-driven framework for QA diagnostics andrefinement. Our approach analyzes the training dynamics of each QA pair at boththe question level and option level, discarding machine-detectable artifacts byremoving uninformative QA pairs and mislabeled or false-negative options.Extensive experiments demonstrate the effectiveness of our approach, whichoutperforms all baselines while using only 33% of the synthetic data, evenincluding LLMs such as ChatGPT. Moreover, expert evaluations confirm that ourframework significantly improves the quality of QA synthesis. Our codes andmodel checkpoints are available athttps://github.com/HKUST-KnowComp/QaDynamics.</description><author>Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin Liu, Yangqiu Song</author><pubDate>Tue, 17 Oct 2023 15:27:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11303v1</guid></item><item><title>Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?</title><link>http://arxiv.org/abs/2302.11713v5</link><description>Pre-trained vision and language models have demonstrated state-of-the-artcapabilities over existing tasks involving images and texts, including visualquestion answering. However, it remains unclear whether these models possessthe capability to answer questions that are not only querying visual contentbut knowledge-intensive and information-seeking. In this study, we introduceInfoSeek, a visual question answering dataset tailored for information-seekingquestions that cannot be answered with only common sense knowledge. UsingInfoSeek, we analyze various pre-trained visual question answering models andgain insights into their characteristics. Our findings reveal thatstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)face challenges in answering visual information-seeking questions, butfine-tuning on the InfoSeek dataset elicits models to use fine-grainedknowledge that was learned during their pre-training. Furthermore, we show thataccurate visual entity recognition can be used to improve performance onInfoSeek by retrieving relevant documents, showing a significant space forimprovement.</description><author>Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, Ming-Wei Chang</author><pubDate>Tue, 17 Oct 2023 15:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11713v5</guid></item><item><title>CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation</title><link>http://arxiv.org/abs/2310.11295v1</link><description>Speech-driven 3D facial animation is a challenging cross-modal task that hasattracted growing research interest. During speaking activities, the mouthdisplays strong motions, while the other facial regions typically demonstratecomparatively weak activity levels. Existing approaches often simplify theprocess by directly mapping single-level speech features to the entire facialanimation, which overlook the differences in facial activity intensity leadingto overly smoothed facial movements. In this study, we propose a novelframework, CorrTalk, which effectively establishes the temporal correlationbetween hierarchical speech features and facial activities of differentintensities across distinct regions. A novel facial activity intensity metricis defined to distinguish between strong and weak facial activity, obtained bycomputing the short-time Fourier transform of facial vertex displacements.Based on the variances in facial activity, we propose a dual-branch decodingframework to synchronously synthesize strong and weak facial activity, whichguarantees wider intensity facial animation synthesis. Furthermore, a weightedhierarchical feature encoder is proposed to establish temporal correlationbetween hierarchical speech features and facial activity at differentintensities, which ensures lip-sync and plausible facial expressions. Extensivequalitatively and quantitatively experiments as well as a user study indicatethat our CorrTalk outperforms existing state-of-the-art methods. The sourcecode and supplementary video are publicly available at:https://zjchu.github.io/projects/CorrTalk/</description><author>Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, Xiangmin Xu</author><pubDate>Tue, 17 Oct 2023 15:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11295v1</guid></item><item><title>An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent</title><link>http://arxiv.org/abs/2310.11291v1</link><description>The delta-bar-delta algorithm is recognized as a learning rate adaptationtechnique that enhances the convergence speed of the training process inoptimization by dynamically scheduling the learning rate based on thedifference between the current and previous weight updates. While thisalgorithm has demonstrated strong competitiveness in full data optimizationwhen compared to other state-of-the-art algorithms like Adam and SGD, it mayencounter convergence issues in mini-batch optimization scenarios due to thepresence of noisy gradients. In this study, we thoroughly investigate the convergence behavior of thedelta-bar-delta algorithm in real-world neural network optimization. To addressany potential convergence challenges, we propose a novel approach called RDBD(Regrettable Delta-Bar-Delta). Our approach allows for prompt correction ofbiased learning rate adjustments and ensures the convergence of theoptimization process. Furthermore, we demonstrate that RDBD can be seamlesslyintegrated with any optimization algorithm and significantly improve theconvergence speed. By conducting extensive experiments and evaluations, we validate theeffectiveness and efficiency of our proposed RDBD approach. The resultsshowcase its capability to overcome convergence issues in mini-batchoptimization and its potential to enhance the convergence speed of variousoptimization algorithms. This research contributes to the advancement ofoptimization techniques in neural network training, providing practitionerswith a reliable automatic learning rate scheduler for achieving fasterconvergence and improved optimization outcomes.</description><author>Zhao Song, Chiwun Yang</author><pubDate>Tue, 17 Oct 2023 15:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11291v1</guid></item><item><title>When No-Rejection Learning is Consistent for Regression with Rejection</title><link>http://arxiv.org/abs/2307.02932v2</link><description>Learning with rejection has been a prototypical model for studying thehuman-AI interaction on prediction tasks. Upon the arrival of a sampleinstance, the model first uses a rejector to decide whether to accept and usethe AI predictor to make a prediction or reject and defer the sample to humans.Learning such a model changes the structure of the original loss function andoften results in undesirable non-convexity and inconsistency issues. For theclassification with rejection problem, several works develop consistentsurrogate losses for the joint learning of the predictor and the rejector,while there have been fewer works for the regression counterpart. This paperstudies the regression with rejection (RwR) problem and investigates ano-rejection learning strategy that uses all the data to learn the predictor.We first establish the consistency for such a strategy under the weakrealizability condition. Then for the case without the weak realizability, weshow that the excessive risk can also be upper bounded with the sum of twoparts: prediction error and calibration error. Lastly, we demonstrate theadvantage of such a proposed learning strategy with empirical evidence.</description><author>Xiaocheng Li, Shang Liu, Chunlin Sun, Hanzhao Wang</author><pubDate>Tue, 17 Oct 2023 15:15:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02932v2</guid></item><item><title>Evaluating the Impact of Humanitarian Aid on Food Security</title><link>http://arxiv.org/abs/2310.11287v1</link><description>In the face of climate change-induced droughts, vulnerable regions encountersevere threats to food security, demanding urgent humanitarian assistance. Thispaper introduces a causal inference framework for the Horn of Africa, aiming toassess the impact of cash-based interventions on food crises. Our contributionsencompass identifying causal relationships within the food security system,harmonizing a comprehensive database, and estimating the causal effect ofhumanitarian interventions on malnutrition. Our results revealed no significanteffects, likely due to limited sample size, suboptimal data quality, and animperfect causal graph resulting from our limited understanding ofmultidisciplinary systems like food security. This underscores the need toenhance data collection and refine causal models with domain experts for moreeffective future interventions and policies, improving transparency andaccountability in humanitarian aid.</description><author>Jordi Cerdà-Bautista, José María Tárraga, Vasileios Sitokonstantinou, Gustau Camps-Valls</author><pubDate>Tue, 17 Oct 2023 15:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11287v1</guid></item><item><title>AnglE-optimized Text Embeddings</title><link>http://arxiv.org/abs/2309.12871v3</link><description>High-quality text embedding is pivotal in improving semantic textualsimilarity (STS) tasks, which are crucial components in Large Language Model(LLM) applications. However, a common challenge existing text embedding modelsface is the problem of vanishing gradients, primarily due to their reliance onthe cosine function in the optimization objective, which has saturation zones.To address this issue, this paper proposes a novel angle-optimized textembedding model called AnglE. The core idea of AnglE is to introduce angleoptimization in a complex space. This novel approach effectively mitigates theadverse effects of the saturation zone in the cosine function, which can impedegradient and hinder optimization processes. To set up a comprehensive STSevaluation, we experimented on existing short-text STS datasets and a newlycollected long-text STS dataset from GitHub Issues. Furthermore, we examinedomain-specific STS scenarios with limited labeled data and explore how AnglEworks with LLM-annotated data. Extensive experiments were conducted on varioustasks including short-text STS, long-text STS, and domain-specific STS tasks.The results show that AnglE outperforms the state-of-the-art (SOTA) STS modelsthat ignore the cosine saturation zone. These findings demonstrate the abilityof AnglE to generate high-quality text embeddings and the usefulness of angleoptimization in STS.</description><author>Xianming Li, Jing Li</author><pubDate>Tue, 17 Oct 2023 15:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12871v3</guid></item></channel></rss>