<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 21 Sep 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Instigating Cooperation among LLM Agents Using Adaptive Information Modulation</title><link>http://arxiv.org/abs/2409.10372v2</link><description>This paper introduces a novel framework combining LLM agents as proxies forhuman strategic behavior with reinforcement learning (RL) to engage theseagents in evolving strategic interactions within team environments. Ourapproach extends traditional agent-based simulations by using strategic LLMagents (SLA) and introducing dynamic and adaptive governance through apro-social promoting RL agent (PPA) that modulates information access acrossagents in a network, optimizing social welfare and promoting pro-socialbehavior. Through validation in iterative games, including the prisonerdilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.The PPA agent effectively learns to adjust information transparency, resultingin enhanced cooperation rates. This framework offers significant insights intoAI-mediated social dynamics, contributing to the deployment of AI in real-worldteam settings.</description><author>Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</author><pubDate>Thu, 19 Sep 2024 16:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10372v2</guid></item><item><title>LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba</title><link>http://arxiv.org/abs/2408.02615v3</link><description>Recent Transformer-based diffusion models have shown remarkable performance,largely attributed to the ability of the self-attention mechanism to accuratelycapture both global and local contexts by computing all-pair interactions amonginput tokens. However, their quadratic complexity poses significantcomputational challenges for long-sequence inputs. Conversely, a recent statespace model called Mamba offers linear complexity by compressing a filteredglobal context into a hidden state. Despite its efficiency, compressioninevitably leads to information loss of fine-grained local dependencies amongtokens, which are crucial for effective visual generative modeling. Motivatedby these observations, we introduce Local Attentional Mamba (LaMamba) blocksthat combine the strengths of self-attention and Mamba, capturing both globalcontexts and local details with linear complexity. Leveraging the efficientU-Net architecture, our model exhibits exceptional scalability and surpassesthe performance of DiT across various model scales on ImageNet at 256x256resolution, all while utilizing substantially fewer GFLOPs and a comparablenumber of parameters. Compared to state-of-the-art diffusion models on ImageNet256x256 and 512x512, our largest model presents notable advantages, such as areduction of up to 62% GFLOPs compared to DiT-XL/2, while achieving superiorperformance with comparable or fewer parameters. Our code is available athttps://github.com/yunxiangfu2001/LaMamba-Diff.</description><author>Yunxiang Fu, Chaoqi Chen, Yizhou Yu</author><pubDate>Thu, 19 Sep 2024 16:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02615v3</guid></item><item><title>LOLA -- An Open-Source Massively Multilingual Large Language Model</title><link>http://arxiv.org/abs/2409.11272v3</link><description>This paper presents LOLA, a massively multilingual large language modeltrained on more than 160 languages using a sparse Mixture-of-ExpertsTransformer architecture. Our architectural and implementation choices addressthe challenge of harnessing linguistic diversity while maintaining efficiencyand avoiding the common pitfalls of multilinguality. Our analysis of theevaluation results shows competitive performance in natural language generationand understanding tasks. Additionally, we demonstrate how the learnedexpert-routing mechanism exploits implicit phylogenetic linguistic patterns topotentially alleviate the curse of multilinguality. We provide an in-depth lookat the training process, an analysis of the datasets, and a balancedexploration of the model's strengths and limitations. As an open-source model,LOLA promotes reproducibility and serves as a robust foundation for futureresearch. Our findings enable the development of compute-efficient multilingualmodels with strong, scalable performance across languages.</description><author>Nikit Srivastava, Denis Kuchelev, Tatiana Moteu Ngoli, Kshitij Shetty, Michael RÃ¶der, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo</author><pubDate>Thu, 19 Sep 2024 15:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11272v3</guid></item><item><title>Evolving a Multi-Population Evolutionary-QAOA on Distributed QPUs</title><link>http://arxiv.org/abs/2409.10739v2</link><description>Our research combines an Evolutionary Algorithm (EA) with a QuantumApproximate Optimization Algorithm (QAOA) to update the ansatz parameters, inplace of traditional gradient-based methods, and benchmark on the Max-Cutproblem. We demonstrate that our Evolutionary-QAOA (E-QAOA) pairing performs onpar or better than a COBYLA-based QAOA in terms of solution accuracy andvariance, for $d$-3 regular graphs between 4 and 26 nodes, using both$max\_count$ and Conditional Value at Risk (CVaR) for fitness functionevaluations. Furthermore, we take our algorithm one step further and present anovel approach by presenting a multi-population EA distributed on two QPUs,which evolves independent and isolated populations in parallel, classicallycommunicating elite individuals. Experiments were conducted on both simulatorsand IBM quantum hardware, and we investigated the relative performance accuracyand variance.</description><author>Francesca Schiavello, Edoardo Altamura, Ivano Tavernelli, Stefano Mensa, Benjamin Symons</author><pubDate>Thu, 19 Sep 2024 14:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10739v2</guid></item><item><title>NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images</title><link>http://arxiv.org/abs/2303.17448v3</link><description>Change detection (CD) in heterogeneous remote sensing images has been widelyused for disaster monitoring and land-use management. In the past decade, theheterogeneous CD problem has significantly benefited from the development ofdeep neural networks (DNNs). However, the purely data-driven DNNs perform likea black box where the lack of interpretability limits the trustworthiness andcontrollability of DNNs in most practical CD applications. As a powerfulknowledge-driven tool, copula theory performs well in modeling relationshipsamong random variables. To enhance the interpretability of existing neuralnetworks for CD, we propose a knowledge-data-driven heterogeneous CD methodbased on a copula-guided neural network, named NN-Copula-CD. In ourNN-Copula-CD, the mathematical characteristics of copula are employed as theloss functions to supervise a neural network to learn the dependence betweenbi-temporal heterogeneous superpixel pairs, and then the changed regions areidentified via binary classification based on the degrees of dependence of allthe superpixel pairs in the bi-temporal images. We conduct in-depth experimentson three datasets with heterogeneous images, where both quantitative and visualresults demonstrate the effectiveness of our proposed NN-Copula-CD method.</description><author>Weiming Li, Xueqian Wang, Gang Li, Baocheng Geng, Pramod K. Varshney</author><pubDate>Thu, 19 Sep 2024 14:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17448v3</guid></item><item><title>Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval</title><link>http://arxiv.org/abs/2409.12097v2</link><description>Finding the perfect match between a job proposal and a set of freelancers isnot an easy task to perform at scale, especially in multiple languages. In thispaper, we propose a novel neural retriever architecture that tackles thisproblem in a multilingual setting. Our method encodes project descriptions andfreelancer profiles by leveraging pre-trained multilingual language models. Thelatter are used as backbone for a custom transformer architecture that aims tokeep the structure of the profiles and project. This model is trained with acontrastive loss on historical data. Thanks to several experiments, we showthat this approach effectively captures skill matching similarity andfacilitates efficient matching, outperforming traditional methods.</description><author>Warren Jouanneau, Marc Palyart, Emma Jouffroy</author><pubDate>Thu, 19 Sep 2024 12:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12097v2</guid></item><item><title>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</title><link>http://arxiv.org/abs/2409.10173v3</link><description>We introduce jina-embeddings-v3, a novel text embedding model with 570million parameters, achieves state-of-the-art performance on multilingual dataand long-context retrieval tasks, supporting context lengths of up to 8192tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)adapters to generate high-quality embeddings for query-document retrieval,clustering, classification, and text matching. Evaluation on the MTEB benchmarkshows that jina-embeddings-v3 outperforms the latest proprietary embeddingsfrom OpenAI and Cohere on English tasks, while achieving superior performancecompared to multilingual-e5-large-instruct across all multilingual tasks. Witha default output dimension of 1024, users can flexibly reduce the embeddingdimensions to as low as 32 without compromising performance, enabled byMatryoshka Representation Learning.</description><author>Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael GÃ¼nther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao</author><pubDate>Thu, 19 Sep 2024 11:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10173v3</guid></item><item><title>The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives</title><link>http://arxiv.org/abs/2409.11261v3</link><description>This paper introduces the concept of an education tool that utilizesGenerative Artificial Intelligence (GenAI) to enhance storytelling forchildren. The system combines GenAI-driven narrative co-creation,text-to-speech conversion, and text-to-video generation to produce an engagingexperience for learners. We describe the co-creation process, the adaptation ofnarratives into spoken words using text-to-speech models, and thetransformation of these narratives into contextually relevant visuals throughtext-to-video technology. Our evaluation covers the linguistics of thegenerated stories, the text-to-speech conversion quality, and the accuracy ofthe generated visuals.</description><author>Samee Arif, Taimoor Arif, Muhammad Saad Haroon, Aamina Jamal Khan, Agha Ali Raza, Awais Athar</author><pubDate>Thu, 19 Sep 2024 09:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11261v3</guid></item><item><title>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</title><link>http://arxiv.org/abs/2409.11752v2</link><description>In recent years, significant progress has been made in tumor segmentationwithin the field of digital pathology. However, variations in organs, tissuepreparation methods, and image acquisition processes can lead to domaindiscrepancies among digital pathology images. To address this problem, in thispaper, we use Rein, a fine-tuning method, to parametrically and efficientlyfine-tune various vision foundation models (VFMs) for MICCAI 2024 Cross-Organand Cross-Scanner Adenocarcinoma Segmentation (COSAS2024). The core of Reinconsists of a set of learnable tokens, which are directly linked to instances,improving functionality at the instance level in each layer. In the dataenvironment of the COSAS2024 Challenge, extensive experiments demonstrate thatRein fine-tuned the VFMs to achieve satisfactory results. Specifically, we usedRein to fine-tune ConvNeXt and DINOv2. Our team used the former to achievescores of 0.7719 and 0.7557 on the preliminary test phase and final test phasein task1, respectively, while the latter achieved scores of 0.8848 and 0.8192on the preliminary test phase and final test phase in task2. Code is availableat GitHub.</description><author>Pengzhou Cai, Xueyuan Zhang, Libin Lan, Ze Zhao</author><pubDate>Thu, 19 Sep 2024 09:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11752v2</guid></item><item><title>Opponent Shaping for Antibody Development</title><link>http://arxiv.org/abs/2409.10588v2</link><description>Anti-viral therapies are typically designed or evolved towards the currentstrains of a virus. In learning terms, this corresponds to a myopic bestresponse, i.e., not considering the possible adaptive moves of the opponent.However, therapy-induced selective pressures act on viral antigens to drive theemergence of mutated strains, against which initial therapies have reducedefficacy. To motivate our work, we consider antibody designs that target notonly the current viral strains but also the wide range of possible futurevariants that the virus might evolve into under the evolutionary pressureexerted by said antibodies. Building on a computational model of bindingbetween antibodies and viral antigens (the Absolut! framework), we design andimplement a genetic simulation of the viral evolutionary escape. Crucially,this allows our antibody optimisation algorithm to consider and influence theentire escape curve of the virus, i.e. to guide (or ''shape'') the viralevolution. This is inspired by opponent shaping which, in general-sum learning,accounts for the adaptation of the co-player rather than playing a myopic bestresponse. Hence we call the optimised antibodies shapers. Within oursimulations, we demonstrate that our shapers target both current and simulatedfuture viral variants, outperforming the antibodies chosen in a myopic way.Furthermore, we show that shapers exert specific evolutionary pressure on thevirus compared to myopic antibodies. Altogether, shapers modify theevolutionary trajectories of viral strains and minimise the viral escapecompared to their myopic counterparts. While this is a simple model, we hopethat our proposed paradigm will enable the discovery of better long-livedvaccines and antibody therapies in the future, enabled by rapid advancements inthe capabilities of simulation tools.</description><author>Sebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, Ming-Han Chloe Tsai, Harrison Steel, Jakob N. Foerster</author><pubDate>Thu, 19 Sep 2024 09:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10588v2</guid></item><item><title>Representing Positional Information in Generative World Models for Object Manipulation</title><link>http://arxiv.org/abs/2409.12005v2</link><description>Object manipulation capabilities are essential skills that set apart embodiedagents engaging with the world, especially in the realm of robotics. Theability to predict outcomes of interactions with objects is paramount in thissetting. While model-based control methods have started to be employed fortackling manipulation tasks, they have faced challenges in accuratelymanipulating objects. As we analyze the causes of this limitation, we identifythe cause of underperformance in the way current world models represent crucialpositional information, especially about the target's goal specification forobject positioning tasks. We introduce a general approach that empowers worldmodel-based agents to effectively solve object-positioning tasks. We proposetwo declinations of this approach for generative world models:position-conditioned (PCP) and latent-conditioned (LCP) policy learning. Inparticular, LCP employs object-centric latent representations that explicitlycapture object positional information for goal specification. This naturallyleads to the emergence of multimodal capabilities, enabling the specificationof goals through spatial coordinates or a visual goal. Our methods arerigorously evaluated across several manipulation environments, showingfavorable performance compared to current model-based control approaches.</description><author>Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Sai Rajeswar</author><pubDate>Thu, 19 Sep 2024 07:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12005v2</guid></item><item><title>Adversarial Attack for Explanation Robustness of Rationalization Models</title><link>http://arxiv.org/abs/2408.10795v3</link><description>Rationalization models, which select a subset of input text asrationale-crucial for humans to understand and trust predictions-have recentlyemerged as a prominent research area in eXplainable Artificial Intelligence.However, most of previous studies mainly focus on improving the quality of therationale, ignoring its robustness to malicious attack. Specifically, whetherthe rationalization models can still generate high-quality rationale under theadversarial attack remains unknown. To explore this, this paper proposes UAT2E,which aims to undermine the explainability of rationalization models withoutaltering their predictions, thereby eliciting distrust in these models fromhuman users. UAT2E employs the gradient-based search on triggers and theninserts them into the original input to conduct both the non-target and targetattack. Experimental results on five datasets reveal the vulnerability ofrationalization models in terms of explanation, where they tend to select moremeaningless tokens under attacks. Based on this, we make a series ofrecommendations for improving rationalization models in terms of explanation.</description><author>Yuankai Zhang, Lingxiao Kong, Haozhao Wang, Ruixuan Li, Jun Wang, Yuhua Li, Wei Liu</author><pubDate>Thu, 19 Sep 2024 07:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10795v3</guid></item><item><title>The Impact of Element Ordering on LM Agent Performance</title><link>http://arxiv.org/abs/2409.12089v2</link><description>There has been a surge of interest in language model agents that can navigatevirtual environments such as the web or desktop. To navigate such environments,agents benefit from information on the various elements (e.g., buttons, text,or images) present. It remains unclear which element attributes have thegreatest impact on agent performance, especially in environments that onlyprovide a graphical representation (i.e., pixels). Here we find that theordering in which elements are presented to the language model is surprisinglyimpactful--randomizing element ordering in a webpage degrades agent performancecomparably to removing all visible text from an agent's state representation.While a webpage provides a hierarchical ordering of elements, there is no suchordering when parsing elements directly from pixels. Moreover, as tasks becomemore challenging and models more sophisticated, our experiments suggest thatthe impact of ordering increases. Finding an effective ordering is non-trivial.We investigate the impact of various element ordering methods in web anddesktop environments. We find that dimensionality reduction provides a viableordering for pixel-only environments. We train a UI element detection model toderive elements from pixels and apply our findings to an agentbenchmark--OmniACT--where we only have access to pixels. Our method completesmore than two times as many tasks on average relative to the previousstate-of-the-art.</description><author>Wayne Chi, Ameet Talwalkar, Chris Donahue</author><pubDate>Thu, 19 Sep 2024 05:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12089v2</guid></item><item><title>Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing</title><link>http://arxiv.org/abs/2409.11738v2</link><description>Compressed sensing (CS) has emerged to overcome the inefficiency of Nyquistsampling. However, traditional optimization-based reconstruction is slow andcan not yield an exact image in practice. Deep learning-based reconstructionhas been a promising alternative to optimization-based reconstruction,outperforming it in accuracy and computation speed. Finding an efficientsampling method with deep learning-based reconstruction, especially for FourierCS remains a challenge. Existing joint optimization of sampling-reconstructionworks ($\mathcal{H}_1$) optimize the sampling mask but have low potential as itis not adaptive to each data point. Adaptive sampling ($\mathcal{H}_2$) hasalso disadvantages of difficult optimization and Pareto sub-optimality. Here,we propose a novel adaptive selection of sampling-reconstruction($\mathcal{H}_{1.5}$) framework that selects the best sampling mask andreconstruction network for each input data. We provide theorems that our methodhas a higher potential than $\mathcal{H}_1$ and effectively solves the Paretosub-optimality problem in sampling-reconstruction by using separatereconstruction networks for different sampling masks. To select the bestsampling mask, we propose to quantify the high-frequency Bayesian uncertaintyof the input, using a super-resolution space generation model. Our methodoutperforms joint optimization of sampling-reconstruction ($\mathcal{H}_1$) andadaptive sampling ($\mathcal{H}_2$) by achieving significant improvements onseveral Fourier CS problems.</description><author>Seongmin Hong, Jaehyeok Bae, Jongho Lee, Se Young Chun</author><pubDate>Thu, 19 Sep 2024 03:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11738v2</guid></item><item><title>Super Resolution On Global Weather Forecasts</title><link>http://arxiv.org/abs/2409.11502v2</link><description>Weather forecasting is a vitally important tool for tasks ranging fromplanning day to day activities to disaster response planning. However, modelingweather has proven to be challenging task due to its chaotic and unpredictablenature. Each variable, from temperature to precipitation to wind, all influencethe path the environment will take. As a result, all models tend to rapidlylose accuracy as the temporal range of their forecasts increase. Classicalforecasting methods use a myriad of physics-based, numerical, and stochastictechniques to predict the change in weather variables over time. However, suchforecasts often require a very large amount of data and are extremelycomputationally expensive. Furthermore, as climate and global weather patternschange, classical models are substantially more difficult and time-consuming toupdate for changing environments. Fortunately, with recent advances in deeplearning and publicly available high quality weather datasets, deployinglearning methods for estimating these complex systems has become feasible. Thecurrent state-of-the-art deep learning models have comparable accuracy to theindustry standard numerical models and are becoming more ubiquitous in practicedue to their adaptability. Our group seeks to improve upon existing deeplearning based forecasting methods by increasing spatial resolutions of globalweather predictions. Specifically, we are interested in performing superresolution (SR) on GraphCast temperature predictions by increasing the globalprecision from 1 degree of accuracy to 0.5 degrees, which is approximately111km and 55km respectively.</description><author>Lawrence Zhang, Adam Yang, Rodz Andrie Amor, Bryan Zhang, Dhruv Rao</author><pubDate>Thu, 19 Sep 2024 02:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11502v2</guid></item><item><title>Gender Representation and Bias in Indian Civil Service Mock Interviews</title><link>http://arxiv.org/abs/2409.12194v2</link><description>This paper makes three key contributions. First, via a substantial corpus of51,278 interview questions sourced from 888 YouTube videos of mock interviewsof Indian civil service candidates, we demonstrate stark gender bias in thebroad nature of questions asked to male and female candidates. Second, ourexperiments with large language models show a strong presence of gender bias inexplanations provided by the LLMs on the gender inference task. Finally, wepresent a novel dataset of 51,278 interview questions that can inform futuresocial science studies.</description><author>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</author><pubDate>Thu, 19 Sep 2024 02:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12194v2</guid></item><item><title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title><link>http://arxiv.org/abs/2409.12046v2</link><description>Tables, figures, and listings (TFLs) are essential tools for summarizingclinical trial data. Creation of TFLs for reporting activities is often atime-consuming task encountered routinely during the execution of clinicaltrials. This study explored the use of large language models (LLMs) to automatethe generation of TFLs through prompt engineering and few-shot transferlearning. Using public clinical trial data in ADaM format, our resultsdemonstrated that LLMs can efficiently generate TFLs with prompt instructions,showcasing their potential in this domain. Furthermore, we developed aconservational agent named Clinical Trial TFL Generation Agent: An app thatmatches user queries to predefined prompts that produce customized programs togenerate specific predefined TFLs.</description><author>Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu</author><pubDate>Thu, 19 Sep 2024 02:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12046v2</guid></item><item><title>Task and Domain Adaptive Reinforcement Learning for Robot Control</title><link>http://arxiv.org/abs/2404.18713v3</link><description>Deep reinforcement learning (DRL) has shown remarkable success in simulationdomains, yet its application in designing robot controllers remains limited,due to its single-task orientation and insufficient adaptability toenvironmental changes. To overcome these limitations, we present a noveladaptive agent that leverages transfer learning techniques to dynamically adaptpolicy in response to different tasks and environmental conditions. Theapproach is validated through the blimp control challenge, where multitaskingcapabilities and environmental adaptability are essential. The agent is trainedusing a custom, highly parallelized simulator built on IsaacGym. We performzero-shot transfer to fly the blimp in the real world to solve various tasks.We share our code at https://github.com/robot-perception-group/adaptive_agent.</description><author>Yu Tang Liu, Nilaksh Singh, Aamir Ahmad</author><pubDate>Thu, 19 Sep 2024 02:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18713v3</guid></item><item><title>Human-like Affective Cognition in Foundation Models</title><link>http://arxiv.org/abs/2409.11733v2</link><description>Understanding emotions is fundamental to human interaction and experience.Humans easily infer emotions from situations or facial expressions, situationsfrom emotions, and do a variety of other affective cognition. How adept ismodern AI at these inferences? We introduce an evaluation framework for testingaffective cognition in foundation models. Starting from psychological theory,we generate 1,280 diverse scenarios exploring relationships between appraisals,emotions, expressions, and outcomes. We evaluate the abilities of foundationmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefullyselected conditions. Our results show foundation models tend to agree withhuman intuitions, matching or exceeding interparticipant agreement. In someconditions, models are ``superhuman'' -- they better predict modal humanjudgements than the average human. All models benefit from chain-of-thoughtreasoning. This suggests foundation models have acquired a human-likeunderstanding of emotions and their influence on beliefs and behavior.</description><author>Kanishk Gandhi, Zoe Lynch, Jan-Philipp FrÃ¤nken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman</author><pubDate>Thu, 19 Sep 2024 02:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11733v2</guid></item><item><title>TTT-Unet: Enhancing U-Net with Test-Time Training Layers for Biomedical Image Segmentation</title><link>http://arxiv.org/abs/2409.11299v2</link><description>Biomedical image segmentation is crucial for accurately diagnosing andanalyzing various diseases. However, Convolutional Neural Networks (CNNs) andTransformers, the most commonly used architectures for this task, struggle toeffectively capture long-range dependencies due to the inherent locality ofCNNs and the computational complexity of Transformers. To address thislimitation, we introduce TTT-Unet, a novel framework that integrates Test-TimeTraining (TTT) layers into the traditional U-Net architecture for biomedicalimage segmentation. TTT-Unet dynamically adjusts model parameters during thetesting time, enhancing the model's ability to capture both local andlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,including 3D abdominal organ segmentation in CT and MR images, instrumentsegmentation in endoscopy images, and cell segmentation in microscopy images.The results demonstrate that TTT-Unet consistently outperforms state-of-the-artCNN-based and Transformer-based segmentation models across all tasks. The codeis available at https://github.com/rongzhou7/TTT-Unet.</description><author>Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun</author><pubDate>Wed, 18 Sep 2024 19:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11299v2</guid></item><item><title>Implicit Reasoning in Deep Time Series Forecasting</title><link>http://arxiv.org/abs/2409.10840v2</link><description>Recently, time series foundation models have shown promising zero-shotforecasting performance on time series from a wide range of domains. However,it remains unclear whether their success stems from a true understanding oftemporal dynamics or simply from memorizing the training data. While implicitreasoning in language models has been studied, similar evaluations for timeseries models have been largely unexplored. This work takes an initial steptoward assessing the reasoning abilities of deep time series forecastingmodels. We find that certain linear, MLP-based, and patch-based Transformermodels generalize effectively in systematically orchestratedout-of-distribution scenarios, suggesting underexplored reasoning capabilitiesbeyond simple pattern memorization.</description><author>Willa Potosnak, Cristian Challu, Mononito Goswami, MichaÅ WiliÅski, Nina Å»ukowska, Artur Dubrawski</author><pubDate>Wed, 18 Sep 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10840v2</guid></item><item><title>Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)</title><link>http://arxiv.org/abs/2409.10836v2</link><description>Implicit Neural Representation (INR), leveraging a neural network totransform coordinate input into corresponding attributes, has recently drivensignificant advances in several vision-related domains. However, theperformance of INR is heavily influenced by the choice of the nonlinearactivation function used in its multilayer perceptron (MLP) architecture.Multiple nonlinearities have been investigated; yet, current INRs facelimitations in capturing high-frequency components, diverse signal types, andhandling inverse problems. We have identified that these problems can begreatly alleviated by introducing a paradigm shift in INRs. We find that anarchitecture with learnable activations in initial layers can represent finedetails in the underlying signals. Specifically, we propose SL$^{2}$A-INR, ahybrid network for INR with a single-layer learnable activation function,prompting the effectiveness of traditional ReLU-based MLPs. Our method performssuperior across diverse tasks, including image representation, 3D shapereconstructions, inpainting, single image super-resolution, CT reconstruction,and novel view synthesis. Through comprehensive experiments, SL$^{2}$A-INR setsnew benchmarks in accuracy, quality, and convergence rates for INR.</description><author>Moein Heidari, Reza Rezaeian, Reza Azad, Dorit Merhof, Hamid Soltanian-Zadeh, Ilker Hacihaliloglu</author><pubDate>Wed, 18 Sep 2024 18:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10836v2</guid></item><item><title>Gender Representation and Bias in Indian Civil Service Mock Interviews</title><link>http://arxiv.org/abs/2409.12194v1</link><description>This paper makes three key contributions. First, via a substantial corpus of51,278 interview questions sourced from 888 YouTube videos of mock interviewsof Indian civil service candidates, we demonstrate stark gender bias in thebroad nature of questions asked to male and female candidates. Second, ourexperiments with large language models show a strong presence of gender bias inexplanations provided by the LLMs on the gender inference task. Finally, wepresent a novel dataset of 51,278 interview questions that can inform futuresocial science studies.</description><author>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</author><pubDate>Wed, 18 Sep 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12194v1</guid></item><item><title>Vista3D: Unravel the 3D Darkside of a Single Image</title><link>http://arxiv.org/abs/2409.12193v1</link><description>We embark on the age-old quest: unveiling the hidden dimensions of objectsfrom mere glimpses of their visible parts. To address this, we present Vista3D,a framework that realizes swift and consistent 3D generation within a mere 5minutes. At the heart of Vista3D lies a two-phase approach: the coarse phaseand the fine phase. In the coarse phase, we rapidly generate initial geometrywith Gaussian Splatting from a single image. In the fine phase, we extract aSigned Distance Function (SDF) directly from learned Gaussian Splatting,optimizing it with a differentiable isosurface representation. Furthermore, itelevates the quality of generation by using a disentangled representation withtwo independent implicit functions to capture both visible and obscured aspectsof objects. Additionally, it harmonizes gradients from 2D diffusion prior with3D-aware diffusion priors by angular diffusion prior composition. Throughextensive evaluation, we demonstrate that Vista3D effectively sustains abalance between the consistency and diversity of the generated 3D objects.Demos and code will be available at https://github.com/florinshen/Vista3D.</description><author>Qiuhong Shen, Xingyi Yang, Michael Bi Mi, Xinchao Wang</author><pubDate>Wed, 18 Sep 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12193v1</guid></item><item><title>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</title><link>http://arxiv.org/abs/2409.12192v1</link><description>Imitation learning has proven to be a powerful tool for training complexvisuomotor policies. However, current methods often require hundreds tothousands of expert demonstrations to handle high-dimensional visualobservations. A key reason for this poor data efficiency is that visualrepresentations are predominantly either pretrained on out-of-domain data ortrained directly through a behavior cloning objective. In this work, we presentDynaMo, a new in-domain, self-supervised method for learning visualrepresentations. Given a set of expert demonstrations, we jointly learn alatent inverse dynamics model and a forward dynamics model over a sequence ofimage embeddings, predicting the next frame in latent space, withoutaugmentations, contrastive sampling, or access to ground truth actions.Importantly, DynaMo does not require any out-of-domain data such as Internetdatasets or cross-embodied datasets. On a suite of six simulated and realenvironments, we show that representations learned with DynaMo significantlyimprove downstream imitation learning performance over prior self-supervisedlearning objectives, and pretrained representations. Gains from using DynaMohold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,and nearest neighbors. Finally, we ablate over key components of DynaMo andmeasure its impact on downstream policy performance. Robot videos are bestviewed at https://dynamo-ssl.github.io</description><author>Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto</author><pubDate>Wed, 18 Sep 2024 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12192v1</guid></item><item><title>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</title><link>http://arxiv.org/abs/2409.12191v1</link><description>We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VLmodels that redefines the conventional predetermined-resolution approach invisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,which enables the model to dynamically process images of varying resolutionsinto different numbers of visual tokens. This approach allows the model togenerate more efficient and accurate visual representations, closely aligningwith human perceptual processes. The model also integrates Multimodal RotaryPosition Embedding (M-RoPE), facilitating the effective fusion of positionalinformation across text, images, and videos. We employ a unified paradigm forprocessing both images and videos, enhancing the model's visual perceptioncapabilities. To explore the potential of large multimodal models, Qwen2-VLinvestigates the scaling laws for large vision-language models (LVLMs). Byscaling both the model size-with versions at 2B, 8B, and 72B parameters-and theamount of training data, the Qwen2-VL Series achieves highly competitiveperformance. Notably, the Qwen2-VL-72B model achieves results comparable toleading models such as GPT-4o and Claude3.5-Sonnet across various multimodalbenchmarks, outperforming other generalist models. Code is available at\url{https://github.com/QwenLM/Qwen2-VL}.</description><author>Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin</author><pubDate>Wed, 18 Sep 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12191v1</guid></item><item><title>Bundle Adjustment in the Eager Mode</title><link>http://arxiv.org/abs/2409.12190v1</link><description>Bundle adjustment (BA) is a critical technique in various roboticapplications, such as simultaneous localization and mapping (SLAM), augmentedreality (AR), and photogrammetry. BA optimizes parameters such as camera posesand 3D landmarks to align them with observations. With the growing importanceof deep learning in perception systems, there is an increasing need tointegrate BA with deep learning frameworks for enhanced reliability andperformance. However, widely-used C++-based BA frameworks, such as GTSAM,g$^2$o, and Ceres, lack native integration with modern deep learning librarieslike PyTorch. This limitation affects their flexibility, adaptability, ease ofdebugging, and overall implementation efficiency. To address this gap, weintroduce an eager-mode BA framework seamlessly integrated with PyPose,providing PyTorch-compatible interfaces with high efficiency. Our approachincludes GPU-accelerated, differentiable, and sparse operations designed for2nd-order optimization, Lie group and Lie algebra operations, and linearsolvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiency,achieving an average speedup of 18.5$\times$, 22$\times$, and 23$\times$compared to GTSAM, g$^2$o, and Ceres, respectively.</description><author>Zitong Zhan, Huan Xu, Zihang Fang, Xinpeng Wei, Yaoyu Hu, Chen Wang</author><pubDate>Wed, 18 Sep 2024 17:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12190v1</guid></item><item><title>Massively Multi-Person 3D Human Motion Forecasting with Scene Context</title><link>http://arxiv.org/abs/2409.12189v1</link><description>Forecasting long-term 3D human motion is challenging: the stochasticity ofhuman behavior makes it hard to generate realistic human motion from the inputsequence alone. Information on the scene environment and the motion of nearbypeople can greatly aid the generation process. We propose a scene-aware socialtransformer model (SAST) to forecast long-term (10s) human motion motion.Unlike previous models, our approach can model interactions between both widelyvarying numbers of people and objects in a scene. We combine a temporalconvolutional encoder-decoder architecture with a Transformer-based bottleneckthat allows us to efficiently combine motion and scene information. We modelthe conditional motion distribution using denoising diffusion models. Webenchmark our approach on the Humans in Kitchens dataset, which contains 1 to16 persons and 29 to 50 objects that are visible simultaneously. Our modeloutperforms other approaches in terms of realism and diversity on differentmetrics and in a user study. Code is available athttps://github.com/felixbmuller/SAST.</description><author>Felix B Mueller, Julian Tanke, Juergen Gall</author><pubDate>Wed, 18 Sep 2024 17:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12189v1</guid></item><item><title>Qwen2.5-Coder Technical Report</title><link>http://arxiv.org/abs/2409.12186v1</link><description>In this report, we introduce the Qwen2.5-Coder series, a significant upgradefrom its predecessor, CodeQwen1.5. This series includes two models:Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model,Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrainedon a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coderdemonstrates impressive code generation capabilities while retaining generalversatility. The model has been evaluated on a wide range of code-relatedtasks, achieving state-of-the-art (SOTA) performance across more than 10benchmarks, including code generation, completion, reasoning, and repair,consistently outperforming larger models of the same model size. We believethat the release of the Qwen2.5-Coder series will not only push the boundariesof research in code intelligence but also, through its permissive licensing,encourage broader adoption by developers in real-world applications.</description><author>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin</author><pubDate>Wed, 18 Sep 2024 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12186v1</guid></item><item><title>LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba</title><link>http://arxiv.org/abs/2408.02615v2</link><description>Recent Transformer-based diffusion models have shown remarkable performance,largely attributed to the ability of the self-attention mechanism to accuratelycapture both global and local contexts by computing all-pair interactions amonginput tokens. However, their quadratic complexity poses significantcomputational challenges for long-sequence inputs. Conversely, a recent statespace model called Mamba offers linear complexity by compressing a filteredglobal context into a hidden state. Despite its efficiency, compressioninevitably leads to information loss of fine-grained local dependencies amongtokens, which are crucial for effective visual generative modeling. Motivatedby these observations, we introduce Local Attentional Mamba (LaMamba) blocksthat combine the strengths of self-attention and Mamba, capturing both globalcontexts and local details with linear complexity. Leveraging the efficientU-Net architecture, our model exhibits exceptional scalability and surpassesthe performance of DiT across various model scales on ImageNet at 256x256resolution, all while utilizing substantially fewer GFLOPs and a comparablenumber of parameters. Compared to state-of-the-art diffusion models on ImageNet256x256 and 512x512, our largest model presents notable advantages, such as areduction of up to 62% GFLOPs compared to DiT-XL/2, while achieving superiorperformance with comparable or fewer parameters.</description><author>Yunxiang Fu, Chaoqi Chen, Yizhou Yu</author><pubDate>Wed, 18 Sep 2024 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02615v2</guid></item><item><title>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</title><link>http://arxiv.org/abs/2409.12183v1</link><description>Chain-of-thought (CoT) via prompting is the de facto method for elicitingreasoning capabilities from large language models (LLMs). But for what kinds oftasks is this extra ``thinking'' really helpful? To analyze this, we conducteda quantitative meta-analysis covering over 100 papers using CoT and ran our ownevaluations of 20 datasets across 14 models. Our results show that CoT givesstrong performance benefits primarily on tasks involving math or logic, withmuch smaller gains on other types of tasks. On MMLU, directly generating theanswer without CoT leads to almost identical accuracy as CoT unless thequestion or model's response contains an equals sign, indicating symbolicoperations and reasoning. Following this finding, we analyze the behavior ofCoT on these problems by separating planning and execution and comparingagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolicexecution, but it underperforms relative to using a symbolic solver. Ourresults indicate that CoT can be applied selectively, maintaining performancewhile saving inference costs. Furthermore, they suggest a need to move beyondprompt-based CoT to new paradigms that better leverage intermediate computationacross the whole range of LLM applications.</description><author>Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett</author><pubDate>Wed, 18 Sep 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12183v1</guid></item><item><title>A Controlled Study on Long Context Extension and Generalization in LLMs</title><link>http://arxiv.org/abs/2409.12181v1</link><description>Broad textual understanding and in-context learning require language modelsthat utilize full document contexts. Due to the implementation challengesassociated with directly training long-context models, many methods have beenproposed for extending models to handle long contexts. However, owing todifferences in data and model classes, it has been challenging to compare theseapproaches, leading to uncertainty as to how to evaluate long-contextperformance and whether it differs from standard evaluation. We implement acontrolled protocol for extension methods with a standardized evaluation,utilizing consistent base models and extension data. Our study yields severalinsights into long-context behavior. First, we reaffirm the critical role ofperplexity as a general-purpose performance indicator even in longer-contexttasks. Second, we find that current approximate attention methodssystematically underperform across long-context tasks. Finally, we confirm thatexact fine-tuning based methods are generally effective within the range oftheir extension, whereas extrapolation remains challenging. All codebases,models, and checkpoints will be made available open-source, promotingtransparency and facilitating further research in this critical area of AIdevelopment.</description><author>Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T. Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M. Rush</author><pubDate>Wed, 18 Sep 2024 17:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12181v1</guid></item><item><title>Finetuning Language Models to Emit Linguistic Expressions of Uncertainty</title><link>http://arxiv.org/abs/2409.12180v1</link><description>Large language models (LLMs) are increasingly employed in information-seekingand decision-making tasks. Despite their broad utility, LLMs tend to generateinformation that conflicts with real-world facts, and their persuasive stylecan make these inaccuracies appear confident and convincing. As a result,end-users struggle to consistently align the confidence expressed by LLMs withthe accuracy of their predictions, often leading to either blind trust in alloutputs or a complete disregard for their reliability. In this work, we exploresupervised finetuning on uncertainty-augmented predictions as a method todevelop models that produce linguistic expressions of uncertainty.Specifically, we measure the calibration of pre-trained models and thenfine-tune language models to generate calibrated linguistic expressions ofuncertainty. Through experiments on various question-answering datasets, wedemonstrate that LLMs are well-calibrated in assessing their predictions, andsupervised finetuning based on the model's own confidence leads towell-calibrated expressions of uncertainty, particularly for single-claimanswers.</description><author>Arslan Chaudhry, Sridhar Thiagarajan, Dilan Gorur</author><pubDate>Wed, 18 Sep 2024 17:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12180v1</guid></item><item><title>AnySkin: Plug-and-play Skin Sensing for Robotic Touch</title><link>http://arxiv.org/abs/2409.08276v2</link><description>While tactile sensing is widely accepted as an important and useful sensingmodality, its use pales in comparison to other sensory modalities like visionand proprioception. AnySkin addresses the critical challenges that impede theuse of tactile sensing -- versatility, replaceability, and data reusability.Building on the simplistic design of ReSkin, and decoupling the sensingelectronics from the sensing interface, AnySkin simplifies integration makingit as straightforward as putting on a phone case and connecting a charger.Furthermore, AnySkin is the first uncalibrated tactile-sensor withcross-instance generalizability of learned manipulation policies. To summarize,this work makes three key contributions: first, we introduce a streamlinedfabrication process and a design tool for creating an adhesive-free, durableand easily replaceable magnetic tactile sensor; second, we characterize slipdetection and policy learning with the AnySkin sensor; and third, wedemonstrate zero-shot generalization of models trained on one instance ofAnySkin to new instances, and compare it with popular existing tactilesolutions like DIGIT and ReSkin.https://any-skin.github.io/</description><author>Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto</author><pubDate>Wed, 18 Sep 2024 17:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08276v2</guid></item><item><title>Computational Dynamical Systems</title><link>http://arxiv.org/abs/2409.12179v1</link><description>We study the computational complexity theory of smooth, finite-dimensionaldynamical systems. Building off of previous work, we give definitions for whatit means for a smooth dynamical system to simulate a Turing machine. We thenshow that 'chaotic' dynamical systems (more precisely, Axiom A systems) and'integrable' dynamical systems (more generally, measure-preserving systems)cannot robustly simulate universal Turing machines, although such machines canbe robustly simulated by other kinds of dynamical systems. Subsequently, weshow that any Turing machine that can be encoded into a structurally stableone-dimensional dynamical system must have a decidable halting problem, andmoreover an explicit time complexity bound in instances where it does halt.More broadly, our work elucidates what it means for one 'machine' to simulateanother, and emphasizes the necessity of defining low-complexity 'encoders' and'decoders' to translate between the dynamics of the simulation and the systembeing simulated. We highlight how the notion of a computational dynamicalsystem leads to questions at the intersection of computational complexitytheory, dynamical systems theory, and real algebraic geometry.</description><author>Jordan Cotler, Semon Rezchikov</author><pubDate>Wed, 18 Sep 2024 17:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12179v1</guid></item><item><title>UKAN: Unbound Kolmogorov-Arnold Network Accompanied with Accelerated Library</title><link>http://arxiv.org/abs/2408.11200v2</link><description>In this work, we present a GPU-accelerated library for the underlyingcomponents of Kolmogorov-Arnold Networks (KANs), along with an algorithm toeliminate bounded grids in KANs. The GPU-accelerated library reduces thecomputational complexity of Basis Spline (B-spline) evaluation by a factor of$\mathcal{O}$(grid size) compared to existing codes, enabling batch computationfor large-scale learning. To overcome the limitations of traditional KANs, weintroduce Unbounded KANs (UKANs), which eliminate the need for a bounded gridand a fixed number of B-spline coefficients. To do so, we replace the KANparameters (B-spline coefficients) with a coefficient generator (CG) model. Theinputs to the CG model are designed based on the idea of an infinite symmetricgrid extending from negative infinity to positive infinity. The positionalencoding of grid group, a sequential collection of B-spline grid indexes, isfed into the CG model, and coefficients are consumed by the efficientimplementation (matrix representations) of B-spline functions to generateoutputs. We perform several experiments on regression, classification, andgenerative tasks, which are promising. In particular, UKAN does not requiredata normalization or a bounded domain for evaluation. Additionally, ourbenchmarking results indicate the superior memory and computational efficiencyof our library compared to existing codes.</description><author>Alireza Moradzadeh, Lukasz Wawrzyniak, Miles Macklin, Saee G. Paliwal</author><pubDate>Wed, 18 Sep 2024 17:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11200v2</guid></item><item><title>Machine Learning Approaches for Diagnostics and Prognostics of Industrial Systems Using Open Source Data from PHM Data Challenges: A Review</title><link>http://arxiv.org/abs/2312.16810v3</link><description>In the field of Prognostics and Health Management (PHM), recent years havewitnessed a significant surge in the application of machine learning (ML).Despite this growth, the field grapples with a lack of unified guidelines andsystematic approaches for effectively implementing these ML techniques andcomprehensive analysis regarding industrial open-source data across variedscenarios. To address these gaps, this paper provides a comprehensive review ofML approaches for diagnostics and prognostics of industrial systems usingopen-source datasets from PHM Data Challenge Competitions held between 2018 and2023 by PHM Society and IEEE Reliability Society and summarizes a unified MLframework. This review systematically categorizes and scrutinizes the problems,challenges, methodologies, and advancements demonstrated in these competitions,highlighting the evolving role of both conventional machine learning and deeplearning in tackling complex industrial tasks related to detection, diagnosis,assessment, and prognosis. Moreover, this paper delves into the commonchallenges in PHM data challenge competitions by emphasizing data-related andmodel-related issues and evaluating the limitations of these competitions. Thepotential solutions to address these challenges are also summarized. Finally,we identify key themes and potential directions for future research, providingopportunities and prospects for next-generation ML-PHM development in PHMdomain.</description><author>Hanqi Su, Jay Lee</author><pubDate>Wed, 18 Sep 2024 17:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16810v3</guid></item><item><title>Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized Learning</title><link>http://arxiv.org/abs/2407.20209v2</link><description>For overparameterized optimization tasks, such as the ones found in modernmachine learning, global minima are generally not unique. In order tounderstand generalization in these settings, it is vital to study to whichminimum an optimization algorithm converges. The possibility of having minimathat are unstable under the dynamics imposed by the optimization algorithmlimits the potential minima that the algorithm can find. In this paper, wecharacterize the global minima that are dynamically stable/unstable for bothdeterministic and stochastic gradient descent (SGD). In particular, weintroduce a characteristic Lyapunov exponent which depends on the localdynamics around a global minimum and rigorously prove that the sign of thisLyapunov exponent determines whether SGD can accumulate at the respectiveglobal minimum.</description><author>Dennis Chemnitz, Maximilian Engel</author><pubDate>Wed, 18 Sep 2024 17:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20209v2</guid></item><item><title>NovAScore: A New Automated Metric for Evaluating Document Level Novelty</title><link>http://arxiv.org/abs/2409.09249v2</link><description>The rapid expansion of online content has intensified the issue ofinformation redundancy, underscoring the need for solutions that can identifygenuinely new information. Despite this challenge, the research community hasseen a decline in focus on novelty detection, particularly with the rise oflarge language models (LLMs). Additionally, previous approaches have reliedheavily on human annotation, which is time-consuming, costly, and particularlychallenging when annotators must compare a target document against a vastnumber of historical documents. In this work, we introduce NovAScore (NoveltyEvaluation in Atomicity Score), an automated metric for evaluatingdocument-level novelty. NovAScore aggregates the novelty and salience scores ofatomic information, providing high interpretability and a detailed analysis ofa document's novelty. With its dynamic weight adjustment scheme, NovAScoreoffers enhanced flexibility and an additional dimension to assess both thenovelty level and the importance of information within a document. Ourexperiments show that NovAScore strongly correlates with human judgments ofnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0dataset and a 0.920 Pearson correlation on an internal human-annotated dataset.</description><author>Lin Ai, Ziwei Gong, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Ahmad Emami, Julia Hirschberg</author><pubDate>Wed, 18 Sep 2024 17:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09249v2</guid></item><item><title>You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL</title><link>http://arxiv.org/abs/2409.12172v1</link><description>While significant progress has been made on the text-to-SQL task, recentsolutions repeatedly encode the same database schema for every question,resulting in unnecessary high inference cost and often overlooking crucialdatabase knowledge. To address these issues, we propose You Only Read Once(YORO), a novel paradigm that directly internalizes database knowledge into theparametric knowledge of a text-to-SQL model during training and eliminates theneed for schema encoding during inference. YORO significantly reduces the inputtoken length by 66%-98%. Despite its shorter inputs, our empirical resultsdemonstrate YORO's competitive performances with traditional systems on threebenchmarks as well as its significant outperformance on large databases.Furthermore, YORO excels in handling questions with challenging valueretrievals such as abbreviation.</description><author>Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng</author><pubDate>Wed, 18 Sep 2024 17:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12172v1</guid></item><item><title>multiPI-TransBTS: A Multi-Path Learning Framework for Brain Tumor Image Segmentation Based on Multi-Physical Information</title><link>http://arxiv.org/abs/2409.12167v1</link><description>Brain Tumor Segmentation (BraTS) plays a critical role in clinical diagnosis,treatment planning, and monitoring the progression of brain tumors. However,due to the variability in tumor appearance, size, and intensity acrossdifferent MRI modalities, automated segmentation remains a challenging task. Inthis study, we propose a novel Transformer-based framework, multiPI-TransBTS,which integrates multi-physical information to enhance segmentation accuracy.The model leverages spatial information, semantic information, and multi-modalimaging data, addressing the inherent heterogeneity in brain tumorcharacteristics. The multiPI-TransBTS framework consists of an encoder, anAdaptive Feature Fusion (AFF) module, and a multi-source, multi-scale featuredecoder. The encoder incorporates a multi-branch architecture to separatelyextract modality-specific features from different MRI sequences. The AFF modulefuses information from multiple sources using channel-wise and element-wiseattention, ensuring effective feature recalibration. The decoder combines bothcommon and task-specific features through a Task-Specific Feature Introduction(TSFI) strategy, producing accurate segmentation outputs for Whole Tumor (WT),Tumor Core (TC), and Enhancing Tumor (ET) regions. Comprehensive evaluations onthe BraTS2019 and BraTS2020 datasets demonstrate the superiority ofmultiPI-TransBTS over the state-of-the-art methods. The model consistentlyachieves better Dice coefficients, Hausdorff distances, and Sensitivity scores,highlighting its effectiveness in addressing the BraTS challenges. Our resultsalso indicate the need for further exploration of the balance between precisionand recall in the ET segmentation task. The proposed framework represents asignificant advancement in BraTS, with potential implications for improvingclinical outcomes for brain tumor patients.</description><author>Hongjun Zhu, Jiaohang Huang, Kuo Chen, Xuehui Ying, Ying Qian</author><pubDate>Wed, 18 Sep 2024 17:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12167v1</guid></item><item><title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title><link>http://arxiv.org/abs/2409.10289v2</link><description>Empathetic response generation necessitates the integration of emotional andintentional dynamics to foster meaningful interactions. Existing researcheither neglects the intricate interplay between emotion and intent, leading tosuboptimal controllability of empathy, or resorts to large language models(LLMs), which incur significant computational overhead. In this paper, weintroduce ReflectDiffu, a lightweight and comprehensive framework forempathetic response generation. This framework incorporates emotion contagionto augment emotional expressiveness and employs an emotion-reasoning mask topinpoint critical emotional elements. Additionally, it integrates intentmimicry within reinforcement learning for refinement during diffusion. Byharnessing an intent twice reflect the mechanism ofExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotionaldecision-making into precise intent actions, thereby addressing empatheticresponse misalignments stemming from emotional misrecognition. Throughreflection, the framework maps emotional states to intents, markedly enhancingboth response empathy and flexibility. Comprehensive experiments reveal thatReflectDiffu outperforms existing models regarding relevance, controllability,and informativeness, achieving state-of-the-art results in both automatic andhuman evaluations.</description><author>Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem</author><pubDate>Wed, 18 Sep 2024 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10289v2</guid></item><item><title>TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes</title><link>http://arxiv.org/abs/2405.02762v2</link><description>In this paper, we present a new approach to bridge the domain gap betweensynthetic and real-world data for unmanned aerial vehicle (UAV)-basedperception. Our formulation is designed for dynamic scenes, consisting of smallmoving objects or human actions. We propose an extension of K-Planes NeuralRadiance Field (NeRF), wherein our algorithm stores a set of tiered featurevectors. The tiered feature vectors are generated to effectively modelconceptual information about a scene as well as an image decoder thattransforms output feature maps into RGB images. Our technique leverages theinformation amongst both static and dynamic objects within a scene and is ableto capture salient scene attributes of high altitude videos. We evaluate itsperformance on challenging datasets, including Okutama Action and UG2, andobserve considerable improvement in accuracy over state of the art neuralrendering methods.</description><author>Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</author><pubDate>Wed, 18 Sep 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02762v2</guid></item><item><title>Precise Forecasting of Sky Images Using Spatial Warping</title><link>http://arxiv.org/abs/2409.12162v1</link><description>The intermittency of solar power, due to occlusion from cloud cover, is oneof the key factors inhibiting its widespread use in both commercial andresidential settings. Hence, real-time forecasting of solar irradiance forgrid-connected photovoltaic systems is necessary to schedule and allocateresources across the grid. Ground-based imagers that capture wide field-of-viewimages of the sky are commonly used to monitor cloud movement around aparticular site in an effort to forecast solar irradiance. However, these wideFOV imagers capture a distorted image of sky image, where regions near thehorizon are heavily compressed. This hinders the ability to precisely predictcloud motion near the horizon which especially affects prediction over longertime horizons. In this work, we combat the aforementioned constraint byintroducing a deep learning method to predict a future sky image frame withhigher resolution than previous methods. Our main contribution is to derive anoptimal warping method to counter the adverse affects of clouds at the horizon,and learn a framework for future sky image prediction which better determinescloud evolution for longer time horizons.</description><author>Leron Julian, Aswin C. Sankaranarayanan</author><pubDate>Wed, 18 Sep 2024 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12162v1</guid></item><item><title>JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation</title><link>http://arxiv.org/abs/2409.12156v1</link><description>We introduce a novel method for joint expression and audio-guided talkingface generation. Recent approaches either struggle to preserve the speakeridentity or fail to produce faithful facial expressions. To address thesechallenges, we propose a NeRF-based network. Since we train our network onmonocular videos without any ground truth, it is essential to learndisentangled representations for audio and expression. We first learn audiofeatures in a self-supervised manner, given utterances from multiple subjects.By incorporating a contrastive learning technique, we ensure that the learnedaudio features are aligned to the lip motion and disentangled from the musclemotion of the rest of the face. We then devise a transformer-based architecturethat learns expression features, capturing long-range facial expressions anddisentangling them from the speech-specific mouth movements. Throughquantitative and qualitative evaluation, we demonstrate that our method cansynthesize high-fidelity talking face videos, achieving state-of-the-art facialexpression transfer along with lip synchronization to unseen audio.</description><author>Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Dimitris Samaras</author><pubDate>Wed, 18 Sep 2024 17:18:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12156v1</guid></item><item><title>Autopet III challenge: Incorporating anatomical knowledge into nnUNet for lesion segmentation in PET/CT</title><link>http://arxiv.org/abs/2409.12155v1</link><description>Lesion segmentation in PET/CT imaging is essential for precise tumorcharacterization, which supports personalized treatment planning and enhancesdiagnostic precision in oncology. However, accurate manual segmentation oflesions is time-consuming and prone to inter-observer variability. Given therising demand and clinical use of PET/CT, automated segmentation methods,particularly deep-learning-based approaches, have become increasingly morerelevant. The autoPET III Challenge focuses on advancing automated segmentationof tumor lesions in PET/CT images in a multitracer multicenter setting,addressing the clinical need for quantitative, robust, and generalizablesolutions. Building on previous challenges, the third iteration of the autoPETchallenge introduces a more diverse dataset featuring two different tracers(FDG and PSMA) from two clinical centers. To this extent, we developed aclassifier that identifies the tracer of the given PET/CT based on the MaximumIntensity Projection of the PET scan. We trained two individualnnUNet-ensembles for each tracer where anatomical labels are included as amulti-label task to enhance the model's performance. Our final submissionachieves cross-validation Dice scores of 76.90% and 61.33% for the publiclyavailable FDG and PSMA datasets, respectively. The code is available athttps://github.com/hakal104/autoPETIII/ .</description><author>Hamza Kalisch, Fabian HÃ¶rst, Ken Herrmann, Jens Kleesiek, Constantin Seibold</author><pubDate>Wed, 18 Sep 2024 17:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12155v1</guid></item><item><title>Abductive explanations of classifiers under constraints: Complexity and properties</title><link>http://arxiv.org/abs/2409.12154v1</link><description>Abductive explanations (AXp's) are widely used for understanding decisions ofclassifiers. Existing definitions are suitable when features are independent.However, we show that ignoring constraints when they exist between features maylead to an explosion in the number of redundant or superfluous AXp's. Wepropose three new types of explanations that take into account constraints andthat can be generated from the whole feature space or from a sample (such as adataset). They are based on a key notion of coverage of an explanation, the setof instances it explains. We show that coverage is powerful enough to discardredundant and superfluous AXp's. For each type, we analyse the complexity offinding an explanation and investigate its formal properties. The final resultis a catalogue of different forms of AXp's with different complexities anddifferent formal guarantees.</description><author>Martin Cooper, Leila Amgoud</author><pubDate>Wed, 18 Sep 2024 17:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12154v1</guid></item><item><title>Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference</title><link>http://arxiv.org/abs/2409.12150v1</link><description>Personalized outfit recommendation remains a complex challenge, demandingboth fashion compatibility understanding and trend awareness. This paperpresents a novel framework that harnesses the expressive power of largelanguage models (LLMs) for this task, mitigating their "black box" and staticnature through fine-tuning and direct feedback integration. We bridge the itemvisual-textual gap in items descriptions by employing image captioning with aMultimodal Large Language Model (MLLM). This enables the LLM to extract styleand color characteristics from human-curated fashion images, forming the basisfor personalized recommendations. The LLM is efficiently fine-tuned on theopen-source Polyvore dataset of curated fashion images, optimizing its abilityto recommend stylish outfits. A direct preference mechanism using negativeexamples is employed to enhance the LLM's decision-making process. This createsa self-enhancing AI feedback loop that continuously refines recommendations inline with seasonal fashion trends. Our framework is evaluated on the Polyvoredataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,and complementary item retrieval. These evaluations underline the framework'sability to generate stylish, trend-aligned outfit suggestions, continuouslyimproving through direct feedback. The evaluation results demonstrated that ourproposed framework significantly outperforms the base LLM, creating morecohesive outfits. The improved performance in these tasks underscores theproposed framework's potential to enhance the shopping experience with accuratesuggestions, proving its effectiveness over the vanilla LLM based outfitgeneration.</description><author>Najmeh Forouzandehmehr, Nima Farrokhsiar, Ramin Giahi, Evren Korpeoglu, Kannan Achan</author><pubDate>Wed, 18 Sep 2024 17:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12150v1</guid></item><item><title>MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</title><link>http://arxiv.org/abs/2409.12147v1</link><description>Large Language Models' (LLM) reasoning can be improved using test-timeaggregation strategies, i.e., generating multiple samples and voting amonggenerated samples. While these improve performance, they often reach asaturation point. Refinement offers an alternative by using LLM-generatedfeedback to improve solution quality. However, refinement introduces 3 keychallenges: (1) Excessive refinement: Uniformly refining all instances canover-correct and reduce the overall performance. (2) Inability to localize andaddress errors: LLMs have a limited ability to self-correct and struggle toidentify and correct their own mistakes. (3) Insufficient refinement: Decidinghow many iterations of refinement are needed is non-trivial, and stopping toosoon could leave errors unaddressed. To tackle these issues, we proposeMAgICoRe, which avoids excessive refinement by categorizing problem difficultyas easy or hard, solving easy problems with coarse-grained aggregation and hardones with fine-grained and iterative multi-agent refinement. To improve errorlocalization, we incorporate external step-wise reward model (RM) scores.Moreover, to ensure effective refinement, we employ a multi-agent loop withthree agents: Solver, Reviewer (which generates targeted feedback based onstep-wise RM scores), and the Refiner (which incorporates feedback). To ensuresufficient refinement, we re-evaluate updated solutions, iteratively initiatingfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5and show its effectiveness across 5 math datasets. Even one iteration ofMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by4.0% while using less than half the samples. Unlike iterative refinement withbaselines, MAgICoRe continues to improve with more iterations. Finally, ourablations highlight the importance of MAgICoRe's RMs and multi-agentcommunication.</description><author>Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Wed, 18 Sep 2024 17:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12147v1</guid></item><item><title>Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications</title><link>http://arxiv.org/abs/2401.17434v3</link><description>Hackathons have emerged as pivotal platforms in the software industry,driving both innovation and skill development for organizations and studentsalike. These events enable companies to quickly prototype new ideas whileoffering students practical, hands-on learning experiences. Over time,hackathons have transitioned from purely competitive events to valuableeducational tools, integrating theory with real-world problem-solving throughcollaboration between academia and industry. The infusion of artificialintelligence (AI) and machine learning is now reshaping hackathons, providingenhanced learning opportunities while also introducing ethical challenges. Thisstudy explores the influence of generative AI on students' technologicalchoices, focusing on a case study from the 2023 University of Iowa Hackathon.The findings offer insights into AI's role in these events, its educationalimpact, and propose strategies for integrating such technologies in futurehackathons, ensuring a balance between innovation, ethics, and educationalvalue.</description><author>Ramteja Sajja, Carlos Erazo Ramirez, Zhouyayan Li, Bekir Z. Demiray, Yusuf Sermet, Ibrahim Demir</author><pubDate>Wed, 18 Sep 2024 17:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17434v3</guid></item><item><title>Integrating AI and Learning Analytics for Data-Driven Pedagogical Decisions and Personalized Interventions in Education</title><link>http://arxiv.org/abs/2312.09548v2</link><description>This research study explores the conceptualization, development, anddeployment of an innovative learning analytics tool, leveraging OpenAI's GPT-4model to quantify student engagement, map learning progression, and evaluatediverse instructional strategies within an educational context. By analyzingcritical data points such as students' stress levels, curiosity, confusion,agitation, topic preferences, and study methods, the tool provides acomprehensive view of the learning environment. It also employs Bloom'staxonomy to assess cognitive development based on student inquiries. Inaddition to technical evaluation through synthetic data, feedback from a surveyof teaching faculty at the University of Iowa was collected to gauge perceivedbenefits and challenges. Faculty recognized the tool's potential to enhanceinstructional decision-making through real-time insights but expressed concernsabout data security and the accuracy of AI-generated insights. The studyoutlines the design, implementation, and evaluation of the tool, highlightingits contributions to educational outcomes, practical integration withinlearning management systems, and future refinements needed to address privacyand accuracy concerns. This research underscores AI's role in shapingpersonalized, data-driven education.</description><author>Ramteja Sajja, Yusuf Sermet, David Cwiertny, Ibrahim Demir</author><pubDate>Wed, 18 Sep 2024 17:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09548v2</guid></item><item><title>MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</title><link>http://arxiv.org/abs/2409.12140v1</link><description>We introduce MoRAG, a novel multi-part fusion based retrieval-augmentedgeneration strategy for text-based human motion generation. The method enhancesmotion diffusion models by leveraging additional knowledge obtained through animproved motion retrieval process. By effectively prompting large languagemodels (LLMs), we address spelling errors and rephrasing issues in motionretrieval. Our approach utilizes a multi-part retrieval strategy to improve thegeneralizability of motion retrieval across the language space. We creatediverse samples through the spatial composition of the retrieved motions.Furthermore, by utilizing low-level, part-specific motion information, we canconstruct motion samples for unseen text descriptions. Our experimentsdemonstrate that our framework can serve as a plug-and-play module, improvingthe performance of motion diffusion models. Code, pretrained models and samplevideos will be made available at: https://motion-rag.github.io/</description><author>Kalakonda Sai Shashank, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla</author><pubDate>Wed, 18 Sep 2024 17:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12140v1</guid></item><item><title>Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models</title><link>http://arxiv.org/abs/2409.12139v1</link><description>With the advent of the big data and large language model era, zero-shotpersonalized rapid customization has emerged as a significant trend. In thisreport, we introduce Takin AudioLLM, a series of techniques and models, mainlyincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed foraudiobook production. These models are capable of zero-shot speech production,generating high-quality speech that is nearly indistinguishable from real humanspeech and facilitating individuals to customize the speech content accordingto their own needs. Specifically, we first introduce Takin TTS, a neural codeclanguage model that builds upon an enhanced neural speech codec and amulti-task training framework, capable of generating high-fidelity naturalspeech in a zero-shot way. For Takin VC, we advocate an effective content andtimbre joint modeling approach to improve the speaker similarity, whileadvocating for a conditional flow matching based decoder to further enhance itsnaturalness and expressiveness. Last, we propose the Takin Morphing system withhighly decoupled and advanced timbre and prosody modeling approaches, whichenables individuals to customize speech production with their preferred timbreand prosody in a precise and controllable manner. Extensive experimentsvalidate the effectiveness and robustness of our Takin AudioLLM series models.For detailed demos, please refer to https://takinaudiollm.github.io.</description><author>EverestAI, :, Sijin Chen, Yuan Feng, Laipeng He, Tianwei He, Wendi He, Yanni Hu, Bin Lin, Yiting Lin, Pengfei Tan, Chengwei Tian, Chen Wang, Zhicheng Wang, Ruoye Xie, Jingjing Yin, Jianhao Ye, Jixun Yao, Quanlei Yan, Yuguang Yang</author><pubDate>Wed, 18 Sep 2024 17:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12139v1</guid></item><item><title>Estimating the number of reachable positions in Minishogi</title><link>http://arxiv.org/abs/2409.00129v2</link><description>To investigate the feasibility of strongly solving Minishogi (Gogo Shogi), itis necessary to know the number of its reachable positions from the initialposition. However, there currently remains a significant gap between the lowerand upper bounds of the value, since checking the legality of a Minishogiposition is difficult. In this paper, the authors estimate the number ofreachable positions by generating candidate positions using uniform randomsampling and measuring the proportion of those reachable by a series of legalmoves from the initial position. The experimental results reveal that thenumber of reachable Minishogi positions is approximately $2.38\times 10^{18}$.</description><author>Sotaro Ishii, Tetsuro Tanaka</author><pubDate>Wed, 18 Sep 2024 17:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00129v2</guid></item><item><title>GRIN: GRadient-INformed MoE</title><link>http://arxiv.org/abs/2409.12136v1</link><description>Mixture-of-Experts (MoE) models scale more effectively than dense models dueto sparse computation through expert routing, selectively activating only asmall subset of expert modules. However, sparse computation challengestraditional training practices, as discrete expert routing hinders standardbackpropagation and thus gradient-based optimization, which are the cornerstoneof deep learning. To better pursue the scaling power of MoE, we introduce GRIN(GRadient-INformed MoE training), which incorporates sparse gradient estimationfor expert routing and configures model parallelism to avoid token dropping.Applying GRIN to autoregressive language modeling, we develop a top-216$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,outperforms a 7B dense model and matches the performance of a 14B dense modeltrained on the same data. Extensive evaluations across diverse tasksdemonstrate the potential of GRIN to significantly enhance MoE efficacy,achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.</description><author>Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen</author><pubDate>Wed, 18 Sep 2024 17:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12136v1</guid></item><item><title>Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features</title><link>http://arxiv.org/abs/2409.12135v1</link><description>Temporal difference (TD) learning with linear function approximation,abbreviated as linear TD, is a classic and powerful prediction algorithm inreinforcement learning. While it is well understood that linear TD convergesalmost surely to a unique point, this convergence traditionally requires theassumption that the features used by the approximator are linearly independent.However, this linear independence assumption does not hold in many practicalscenarios. This work is the first to establish the almost sure convergence oflinear TD without requiring linearly independent features. In fact, we do notmake any assumptions on the features. We prove that the approximated valuefunction converges to a unique point and the weight iterates converge to a set.We also establish a notion of local stability of the weight iterates.Importantly, we do not need to introduce any other additional assumptions anddo not need to make any modification to the linear TD algorithm. Key to ouranalysis is a novel characterization of bounded invariant sets of the mean ODEof linear TD.</description><author>Jiuqi Wang, Shangtong Zhang</author><pubDate>Wed, 18 Sep 2024 16:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12135v1</guid></item><item><title>BERT-VBD: Vietnamese Multi-Document Summarization Framework</title><link>http://arxiv.org/abs/2409.12134v1</link><description>In tackling the challenge of Multi-Document Summarization (MDS), numerousmethods have been proposed, spanning both extractive and abstractivesummarization techniques. However, each approach has its own limitations,making it less effective to rely solely on either one. An emerging andpromising strategy involves a synergistic fusion of extractive and abstractivesummarization methods. Despite the plethora of studies in this domain, researchon the combined methodology remains scarce, particularly in the context ofVietnamese language processing. This paper presents a novel Vietnamese MDSframework leveraging a two-component pipeline architecture that integratesextractive and abstractive techniques. The first component employs anextractive approach to identify key sentences within each document. This isachieved by a modification of the pre-trained BERT network, which derivessemantically meaningful phrase embeddings using siamese and triplet networkstructures. The second component utilizes the VBD-LLaMA2-7B-50b model forabstractive summarization, ultimately generating the final summary document.Our proposed framework demonstrates a positive performance, attaining ROUGE-2scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-artbaselines.</description><author>Tuan-Cuong Vuong, Trang Mai Xuan, Thien Van Luong</author><pubDate>Wed, 18 Sep 2024 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12134v1</guid></item><item><title>Linguini: A benchmark for language-agnostic linguistic reasoning</title><link>http://arxiv.org/abs/2409.12126v1</link><description>We propose a new benchmark to measure a language model's linguistic reasoningskills without relying on pre-existing language-specific knowledge. The testcovers 894 questions grouped in 160 problems across 75 (mostly) extremelylow-resource languages, extracted from the International Linguistic Olympiadcorpus. To attain high accuracy on this benchmark, models don't need previousknowledge of the tested language, as all the information needed to solve thelinguistic puzzle is presented in the context. We find that, while all analyzedmodels rank below 25% accuracy, there is a significant gap between open andclosed models, with the best-performing proprietary model at 24.05% and thebest-performing open model at 8.84%.</description><author>Eduardo SÃ¡nchez, Belen Alastruey, Christophe Ropers, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussÃ </author><pubDate>Wed, 18 Sep 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12126v1</guid></item><item><title>Probabilistically Robust Watermarking of Neural Networks</title><link>http://arxiv.org/abs/2401.08261v2</link><description>As deep learning (DL) models are widely and effectively used in MachineLearning as a Service (MLaaS) platforms, there is a rapidly growing interest inDL watermarking techniques that can be used to confirm the ownership of aparticular model. Unfortunately, these methods usually produce watermarkssusceptible to model stealing attacks. In our research, we introduce a noveltrigger set-based watermarking approach that demonstrates resilience againstfunctionality stealing attacks, particularly those involving extraction anddistillation. Our approach does not require additional model training and canbe applied to any model architecture. The key idea of our method is to computethe trigger set, which is transferable between the source model and the set ofproxy models with a high probability. In our experimental study, we show thatif the probability of the set being transferable is reasonably high, it can beeffectively used for ownership verification of the stolen model. We evaluateour method on multiple benchmarks and show that our approach outperformscurrent state-of-the-art watermarking techniques in all considered experimentalsetups.</description><author>Mikhail Pautov, Nikita Bogdanov, Stanislav Pyatkin, Oleg Rogov, Ivan Oseledets</author><pubDate>Wed, 18 Sep 2024 16:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08261v2</guid></item><item><title>Optimal Visual Search with Highly Heuristic Decision Rules</title><link>http://arxiv.org/abs/2409.12124v1</link><description>Visual search is a fundamental natural task for humans and other animals. Weinvestigated the decision processes humans use when searching briefly presenteddisplays having well-separated potential target-object locations. Performancewas compared with the Bayesian-optimal decision process under the assumptionthat the information from the different potential target locations isstatistically independent. Surprisingly, humans performed slightly better thanoptimal, despite humans' substantial loss of sensitivity in the fovea, and theimplausibility of the human brain replicating the optimal computations. We showthat three factors can quantitatively explain these seemingly paradoxicalresults. Most importantly, simple and fixed heuristic decision rules reach nearoptimal search performance. Secondly, foveal neglect primarily affects only thecentral potential target location. Finally, spatially correlated neural noisecauses search performance to exceed that predicted for independent noise. Thesefindings have far-reaching implications for understanding visual search tasksand other identification tasks in humans and other animals.</description><author>Anqi Zhang, Wilson S. Geisler</author><pubDate>Wed, 18 Sep 2024 16:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12124v1</guid></item><item><title>Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement</title><link>http://arxiv.org/abs/2409.12122v1</link><description>In this report, we present a series of math-specific large language models:Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of theQwen2.5 series lies in integrating the philosophy of self-improvementthroughout the entire pipeline, from pre-training and post-training toinference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilizedto generate large-scale, high-quality mathematical data. (2) In thepost-training phase, we develop a reward model (RM) by conducting massivesampling from Qwen2-Math-Instruct. This RM is then applied to the iterativeevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,it's possible to iteratively train and update the RM, which in turn guides thenext round of SFT data iteration. On the final SFT model, we employ theultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.(3) Furthermore, during the inference stage, the RM is used to guide sampling,optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advancedmathematical reasoning capabilities, including Chain-of-Thought (CoT) andTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematicsdatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, andAIME24, covering a range of difficulties from grade school level to mathcompetition problems.</description><author>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang</author><pubDate>Wed, 18 Sep 2024 16:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12122v1</guid></item><item><title>Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference</title><link>http://arxiv.org/abs/2409.12117v1</link><description>Large language models (LLMs) have significantly advanced audio processingthrough audio codecs that convert audio into discrete tokens, enabling theapplication of language modeling techniques to audio data. However, audiocodecs often operate at high frame rates, resulting in slow training andinference, especially for autoregressive models. To address this challenge, wepresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec thatleverages finite scalar quantization and adversarial training with large speechlanguage models to achieve high-quality audio compression with a 1.89 kbpsbitrate and 21.5 frames per second. We demonstrate that our novel codec canmake the inference of LLM-based text-to-speech models around three times fasterwhile improving intelligibility and producing quality comparable to previousmodels.</description><author>Edresson Casanova, Ryan Langman, Paarth Neekhara, Shehzeen Hussain, Jason Li, Subhankar Ghosh, Ante JukiÄ, Sang-gil Lee</author><pubDate>Wed, 18 Sep 2024 16:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12117v1</guid></item><item><title>Stronger Baseline Models -- A Key Requirement for Aligning Machine Learning Research with Clinical Utility</title><link>http://arxiv.org/abs/2409.12116v1</link><description>Machine Learning (ML) research has increased substantially in recent years,due to the success of predictive modeling across diverse application domains.However, well-known barriers exist when attempting to deploy ML models inhigh-stakes, clinical settings, including lack of model transparency (or theinability to audit the inference process), large training data requirementswith siloed data sources, and complicated metrics for measuring model utility.In this work, we show empirically that including stronger baseline models inhealthcare ML evaluations has important downstream effects that aidpractitioners in addressing these challenges. Through a series of case studies,we find that the common practice of omitting baselines or comparing against aweak baseline model (e.g. a linear model with no optimization) obscures thevalue of ML methods proposed in the research literature. Using these insights,we propose some best practices that will enable practitioners to moreeffectively study and deploy ML models in clinical settings.</description><author>Nathan Wolfrath, Joel Wolfrath, Hengrui Hu, Anjishnu Banerjee, Anai N. Kothari</author><pubDate>Wed, 18 Sep 2024 16:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12116v1</guid></item><item><title>Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD)</title><link>http://arxiv.org/abs/2409.12112v1</link><description>This paper introduces the Pareto Data Framework, an approach for identifyingand selecting the Minimum Viable Data (MVD) required for enabling machinelearning applications on constrained platforms such as embedded systems, mobiledevices, and Internet of Things (IoT) devices. We demonstrate that strategicdata reduction can maintain high performance while significantly reducingbandwidth, energy, computation, and storage costs. The framework identifiesMinimum Viable Data (MVD) to optimize efficiency across resource-constrainedenvironments without sacrificing performance. It addresses common inefficientpractices in an IoT application such as overprovisioning of sensors andoverprecision, and oversampling of signals, proposing scalable solutions foroptimal sensor selection, signal extraction and transmission, and datarepresentation. An experimental methodology demonstrates effective acousticdata characterization after downsampling, quantization, and truncation tosimulate reduced-fidelity sensors and network and storage constraints; resultsshows that performance can be maintained up to 95\% with sample rates reducedby 75\% and bit depths and clip length reduced by 50\% which translates intosubstantial cost and resource reduction. These findings have implications onthe design and development of constrained systems. The paper also discussesbroader implications of the framework, including the potential to democratizeadvanced AI technologies across IoT applications and sectors such asagriculture, transportation, and manufacturing to improve access and multiplythe benefits of data-driven insights.</description><author>Tashfain Ahmed, Josh Siegel</author><pubDate>Wed, 18 Sep 2024 16:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12112v1</guid></item><item><title>Applications of Knowledge Distillation in Remote Sensing: A Survey</title><link>http://arxiv.org/abs/2409.12111v1</link><description>With the ever-growing complexity of models in the field of remote sensing(RS), there is an increasing demand for solutions that balance model accuracywith computational efficiency. Knowledge distillation (KD) has emerged as apowerful tool to meet this need, enabling the transfer of knowledge from large,complex models to smaller, more efficient ones without significant loss inperformance. This review article provides an extensive examination of KD andits innovative applications in RS. KD, a technique developed to transferknowledge from a complex, often cumbersome model (teacher) to a more compactand efficient model (student), has seen significant evolution and applicationacross various domains. Initially, we introduce the fundamental concepts andhistorical progression of KD methods. The advantages of employing KD arehighlighted, particularly in terms of model compression, enhanced computationalefficiency, and improved performance, which are pivotal for practicaldeployments in RS scenarios. The article provides a comprehensive taxonomy ofKD techniques, where each category is critically analyzed to demonstrate thebreadth and depth of the alternative options, and illustrates specific casestudies that showcase the practical implementation of KD methods in RS tasks,such as instance segmentation and object detection. Further, the reviewdiscusses the challenges and limitations of KD in RS, including practicalconstraints and prospective future directions, providing a comprehensiveoverview for researchers and practitioners in the field of RS. Through thisorganization, the paper not only elucidates the current state of research in KDbut also sets the stage for future research opportunities, thereby contributingsignificantly to both academic research and real-world applications.</description><author>Yassine Himeur, Nour Aburaed, Omar Elharrouss, Iraklis Varlamis, Shadi Atalla, Wathiq Mansoor, Hussain Al Ahmad</author><pubDate>Wed, 18 Sep 2024 16:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12111v1</guid></item><item><title>Model-free quantification of completeness, uncertainties, and outliers in atomistic machine learning using information theory</title><link>http://arxiv.org/abs/2404.12367v2</link><description>An accurate description of information is relevant for a range of problems inatomistic machine learning (ML), such as crafting training sets, performinguncertainty quantification (UQ), or extracting physical insights from largedatasets. However, atomistic ML often relies on unsupervised learning or modelpredictions to analyze information contents from simulation or training data.Here, we introduce a theoretical framework that provides a rigorous, model-freetool to quantify information contents in atomistic simulations. We demonstratethat the information entropy of a distribution of atom-centered environmentsexplains known heuristics in ML potential developments, from training set sizesto dataset optimality. Using this tool, we propose a model-free UQ method thatreliably predicts epistemic uncertainty and detects out-of-distributionsamples, including rare events in systems such as nucleation. This methodprovides a general tool for data-driven atomistic modeling and combines effortsin ML, simulations, and physical explainability.</description><author>Daniel Schwalbe-Koda, Sebastien Hamel, Babak Sadigh, Fei Zhou, Vincenzo Lordi</author><pubDate>Wed, 18 Sep 2024 16:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12367v2</guid></item><item><title>SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba</title><link>http://arxiv.org/abs/2409.12108v1</link><description>Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedureinitially designed for the treatment of early gastric cancer but is now widelyused for various gastrointestinal lesions. Computer-assisted Surgery systemshave played a crucial role in improving the precision and safety of ESDprocedures, however, their effectiveness is limited by the accurate recognitionof surgical phases. The intricate nature of ESD, with different lesioncharacteristics and tissue structures, presents challenges for real-timesurgical phase recognition algorithms. Existing surgical phase recognitionalgorithms struggle to efficiently capture temporal contexts in video-basedscenarios, leading to insufficient performance. To address these issues, wepropose SPRMamba, a novel Mamba-based framework for ESD surgical phaserecognition. SPRMamba leverages the strengths of Mamba for long-term temporalmodeling while introducing the Scaled Residual TranMamba block to enhance thecapture of fine-grained details, overcoming the limitations of traditionaltemporal models like Temporal Convolutional Networks and Transformers.Moreover, a Temporal Sample Strategy is introduced to accelerate theprocessing, which is essential for real-time phase recognition in clinicalsettings. Extensive testing on the ESD385 dataset and the cholecystectomyCholec80 dataset demonstrates that SPRMamba surpasses existing state-of-the-artmethods and exhibits greater robustness across various surgical phaserecognition tasks.</description><author>Xiangning Zhang, Jinnan Chen, Qingwei Zhang, Chengfeng Zhou, Zhengjie Zhang, Xiaobo Li, Dahong Qian</author><pubDate>Wed, 18 Sep 2024 16:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12108v1</guid></item><item><title>Measuring Human and AI Values based on Generative Psychometrics with Large Language Models</title><link>http://arxiv.org/abs/2409.12106v1</link><description>Human values and their measurement are long-standing interdisciplinaryinquiry. Recent advances in AI have sparked renewed interest in this area, withlarge language models (LLMs) emerging as both tools and subjects of valuemeasurement. This work introduces Generative Psychometrics for Values (GPV), anLLM-based, data-driven value measurement paradigm, theoretically grounded intext-revealed selective perceptions. We begin by fine-tuning an LLM foraccurate perception-level value measurement and verifying the capability ofLLMs to parse texts into perceptions, forming the core of the GPV pipeline.Applying GPV to human-authored blogs, we demonstrate its stability, validity,and superiority over prior psychological tools. Then, extending GPV to LLMvalue measurement, we advance the current art with 1) a psychometricmethodology that measures LLM values based on their scalable and free-formoutputs, enabling context-specific measurement; 2) a comparative analysis ofmeasurement paradigms, indicating response biases of prior methods; and 3) anattempt to bridge LLM values and their safety, revealing the predictive powerof different value systems and the impacts of various values on LLM safety.Through interdisciplinary efforts, we aim to leverage AI for next-generationpsychometrics and psychometrics for value-aligned AI.</description><author>Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song</author><pubDate>Wed, 18 Sep 2024 16:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12106v1</guid></item><item><title>FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated Long-Tailed Learning</title><link>http://arxiv.org/abs/2409.12105v1</link><description>Federated learning offers a paradigm to the challenge of preserving privacyin distributed machine learning. However, datasets distributed across eachclient in the real world are inevitably heterogeneous, and if the datasets canbe globally aggregated, they tend to be long-tailed distributed, which greatlyaffects the performance of the model. The traditional approach to federatedlearning primarily addresses the heterogeneity of data among clients, yet itfails to address the phenomenon of class-wise bias in global long-tailed data.This results in the trained model focusing on the head classes while neglectingthe equally important tail classes. Consequently, it is essential to develop amethodology that considers classes holistically. To address the above problems,we propose a new method FedLF, which introduces three modifications in thelocal training phase: adaptive logit adjustment, continuous class centredoptimization, and feature decorrelation. We compare seven state-of-the-artmethods with varying degrees of data heterogeneity and long-taileddistribution. Extensive experiments on benchmark datasets CIFAR-10-LT andCIFAR-100-LT demonstrate that our approach effectively mitigates the problem ofmodel performance degradation due to data heterogeneity and long-taileddistribution. our code is available at https://github.com/18sym/FedLF.</description><author>Xiuhua Lu, Peng Li, Xuefeng Jiang</author><pubDate>Wed, 18 Sep 2024 16:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12105v1</guid></item><item><title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title><link>http://arxiv.org/abs/2409.10997v2</link><description>Contextual question-answering models are susceptible to adversarialperturbations to input context, commonly observed in real-world scenarios.These adversarial noises are designed to degrade the performance of the modelby distorting the textual input. We introduce a unique dataset thatincorporates seven distinct types of adversarial noise into the context, eachapplied at five different intensity levels on the SQuAD dataset. To quantifythe robustness, we utilize robustness metrics providing a standardized measurefor assessing model performance across varying noise types and levels.Experiments on transformer-based question-answering models reveal robustnessvulnerabilities and important insights into the model's performance inrealistic textual input.</description><author>Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam</author><pubDate>Wed, 18 Sep 2024 16:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10997v2</guid></item><item><title>Symmetry-Enriched Learning: A Category-Theoretic Framework for Robust Machine Learning Models</title><link>http://arxiv.org/abs/2409.12100v1</link><description>This manuscript presents a novel framework that integrates higher-ordersymmetries and category theory into machine learning. We introduce newmathematical constructs, including hyper-symmetry categories and functorialrepresentations, to model complex transformations within learning algorithms.Our contributions include the design of symmetry-enriched learning models, thedevelopment of advanced optimization techniques leveraging categoricalsymmetries, and the theoretical analysis of their implications for modelrobustness, generalization, and convergence. Through rigorous proofs andpractical applications, we demonstrate that incorporating higher-dimensionalcategorical structures enhances both the theoretical foundations and practicalcapabilities of modern machine learning algorithms, opening new directions forresearch and innovation.</description><author>Ronald Katende</author><pubDate>Wed, 18 Sep 2024 16:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12100v1</guid></item><item><title>Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance</title><link>http://arxiv.org/abs/2409.12099v1</link><description>Understanding how humans process visual information is one of the crucialsteps for unraveling the underlying mechanism of brain activity. Recently, thiscuriosity has motivated the fMRI-to-image reconstruction task; given the fMRIdata from visual stimuli, it aims to reconstruct the corresponding visualstimuli. Surprisingly, leveraging powerful generative models such as the LatentDiffusion Model (LDM) has shown promising results in reconstructing complexvisual stimuli such as high-resolution natural images from vision datasets.Despite the impressive structural fidelity of these reconstructions, they oftenlack details of small objects, ambiguous shapes, and semantic nuances.Consequently, the incorporation of additional semantic knowledge, beyond merevisuals, becomes imperative. In light of this, we exploit how modern LDMseffectively incorporate multi-modal guidance (text guidance, visual guidance,and image layout) for structurally and semantically plausible imagegenerations. Specifically, inspired by the two-streams hypothesis suggestingthat perceptual and semantic information are processed in different brainregions, our framework, Brain-Streams, maps fMRI signals from these brainregions to appropriate embeddings. That is, by extracting textual guidance fromsemantic information regions and visual guidance from perceptual informationregions, Brain-Streams provides accurate multi-modal guidance to LDMs. Wevalidate the reconstruction ability of Brain-Streams both quantitatively andqualitatively on a real fMRI dataset comprising natural image stimuli and fMRIdata.</description><author>Jaehoon Joo, Taejin Jeong, Seongjae Hwang</author><pubDate>Wed, 18 Sep 2024 16:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12099v1</guid></item><item><title>VideoClusterNet: Self-Supervised and Adaptive Face Clustering For Videos</title><link>http://arxiv.org/abs/2407.12214v2</link><description>With the rise of digital media content production, the need for analyzingmovies and TV series episodes to locate the main cast of characters preciselyis gaining importance.Specifically, Video Face Clustering aims to grouptogether detected video face tracks with common facial identities. This problemis very challenging due to the large range of pose, expression, appearance, andlighting variations of a given face across video frames. Generic pre-trainedFace Identification (ID) models fail to adapt well to the video productiondomain, given its high dynamic range content and also unique cinematic style.Furthermore, traditional clustering algorithms depend on hyperparametersrequiring individual tuning across datasets. In this paper, we present a novelvideo face clustering approach that learns to adapt a generic face ID model tonew video face tracks in a fully self-supervised fashion. We also propose aparameter-free clustering algorithm that is capable of automatically adaptingto the finetuned model's embedding space for any input video. Due to the lackof comprehensive movie face clustering benchmarks, we also present afirst-of-kind movie dataset: MovieFaceCluster. Our dataset is handpicked byfilm industry professionals and contains extremely challenging face IDscenarios. Experiments show our method's effectiveness in handling difficultmainstream movie scenes on our benchmark dataset and state-of-the-artperformance on traditional TV series datasets.</description><author>Devesh Walawalkar, Pablo Garrido</author><pubDate>Wed, 18 Sep 2024 16:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12214v2</guid></item><item><title>Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval</title><link>http://arxiv.org/abs/2409.12097v1</link><description>Finding the perfect match between a job proposal and a set of freelancers isnot an easy task to perform at scale, especially in multiple languages. In thispaper, we propose a novel neural retriever architecture that tackles thisproblem in a multilingual setting. Our method encodes project descriptions andfreelancer profiles by leveraging pre-trained multilingual language models. Thelatter are used as backbone for a custom transformer architecture that aims tokeep the structure of the profiles and project. This model is trained with acontrastive loss on historical data. Thanks to several experiments, we showthat this approach effectively captures skill matching similarity andfacilitates efficient matching, outperforming traditional methods.</description><author>Warren Jouanneau, Marc Palyart, Emma Jouffroy</author><pubDate>Wed, 18 Sep 2024 16:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12097v1</guid></item><item><title>Computationally efficient reductions between some statistical models</title><link>http://arxiv.org/abs/2402.07717v2</link><description>We study the problem of approximately transforming a sample from a sourcestatistical model to a sample from a target statistical model without knowingthe parameters of the source model, and construct several computationallyefficient such reductions between canonical statistical experiments. Inparticular, we provide computationally efficient procedures that approximatelyreduce uniform, Erlang, and Laplace location models to general target families.We illustrate our methodology by establishing nonasymptotic reductions betweensome canonical high-dimensional problems, spanning mixtures of experts, phaseretrieval, and signal denoising. Notably, the reductions arestructure-preserving and can accommodate missing data. We also point to apossible application in transforming one differentially private mechanism toanother.</description><author>Mengqi Lou, Guy Bresler, Ashwin Pananjady</author><pubDate>Wed, 18 Sep 2024 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07717v2</guid></item><item><title>High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D Cine MRI using Online Learning Neural Networks</title><link>http://arxiv.org/abs/2312.09387v2</link><description>The functional analysis of the left atrium (LA) is important for evaluatingcardiac health and understanding diseases like atrial fibrillation. Cine MRI isideally placed for the detailed 3D characterization of LA motion anddeformation but is lacking appropriate acquisition and analysis tools. Here, wepropose tools for the Analysis for Left Atrial Displacements and DeformatIonsusing online learning neural Networks (Aladdin) and present a technicalfeasibility study on how Aladdin can characterize 3D LA function globally andregionally. Aladdin includes an online segmentation and image registrationnetwork, and a strain calculation pipeline tailored to the LA. We create mapsof LA Displacement Vector Field (DVF) magnitude and LA principal strain valuesfrom images of 10 healthy volunteers and 8 patients with cardiovascular disease(CVD), of which 2 had large left ventricular ejection fraction (LVEF)impairment. We additionally create an atlas of these biomarkers using the datafrom the healthy volunteers. Results showed that Aladdin can accurately trackthe LA wall across the cardiac cycle and characterize its motion anddeformation. Global LA function markers assessed with Aladdin agree well withestimates from 2D Cine MRI. A more marked active contraction phase was observedin the healthy cohort, while the CVD LVEF group showed overall reduced LAfunction. Aladdin is uniquely able to identify LA regions with abnormaldeformation metrics that may indicate focal pathology. We expect Aladdin tohave important clinical applications as it can non-invasively characterizeatrial pathophysiology. All source code and data are available at:https://github.com/cgalaz01/aladdin_cmr_la.</description><author>Christoforos Galazis, Samuel Shepperd, Emma Brouwer, Sandro QueirÃ³s, Ebraham Alskaf, Mustafa Anjari, Amedeo Chiribiri, Jack Lee, Anil A. Bharath, Marta Varela</author><pubDate>Wed, 18 Sep 2024 16:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09387v2</guid></item><item><title>EHRFL: Federated Learning Framework for Institution-Specific Model Construction using Electronic Health Records</title><link>http://arxiv.org/abs/2404.13318v2</link><description>The increasing volume of electronic health records (EHRs) across healthcareinstitutions presents the opportunity to enhance model accuracy and robustnessin clinical prediction tasks. Federated learning enables training on data frommultiple institutions while preserving patient privacy and complying toregulatory constraints. However, most federated learning research focuses onconstructing a global model for multiple clients, overlooking the practicalneed for institution-specific models. In this work, we introduce EHRFL, afederated learning framework using EHRs designed to develop a model tailored toa single healthcare institution. Our framework addresses two key challenges:(1) enabling federated learning across institutions with heterogeneous EHRsystems using text-based EHR modeling, and (2) reducing the costs associatedwith federated learning by selecting suitable participating clients usingaveraged patient embeddings, which enables optimizing the number ofparticipants without compromising model performance for the institution. Ourexperiment results on multiple open-source EHR datasets demonstrate theeffectiveness of EHRFL in addressing the two challenges, establishing it as apractical solution for institution-specific model development in federatedlearning.</description><author>Jiyoun Kim, Junu Kim, Kyunghoon Hur, Edward Choi</author><pubDate>Wed, 18 Sep 2024 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13318v2</guid></item><item><title>Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting</title><link>http://arxiv.org/abs/2408.15256v3</link><description>Past ontology requirements engineering (ORE) has primarily relied on manualmethods, such as interviews and collaborative forums, to gather userrequirements from domain experts, especially in large projects. CurrentOntoChat offers a framework for ORE that utilises large language models (LLMs)to streamline the process through four key functions: user story creation,competency question (CQ) extraction, CQ filtration and analysis, and ontologytesting support. In OntoChat, users are expected to prompt the chatbot togenerate user stories. However, preliminary evaluations revealed that theystruggle to do this effectively. To address this issue, we experimented with aresearch method called participatory prompting, which involvesresearcher-mediated interactions to help users without deep knowledge of LLMsuse the chatbot more effectively. This participatory prompting user studyproduces pre-defined prompt templates based on user queries, focusing oncreating and refining personas, goals, scenarios, sample data, and dataresources for user stories. These refined user stories will subsequently beconverted into CQs.</description><author>Yihang Zhao, Bohui Zhang, Xi Hu, Shuyin Ouyang, Jongmo Kim, Nitisha Jain, Jacopo de Berardinis, Albert MeroÃ±o-PeÃ±uela, Elena Simperl</author><pubDate>Wed, 18 Sep 2024 16:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15256v3</guid></item><item><title>IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition</title><link>http://arxiv.org/abs/2409.12092v1</link><description>Robotic assistive feeding holds significant promise for improving the qualityof life for individuals with eating disabilities. However, acquiring diversefood items under varying conditions and generalizing to unseen food presentsunique challenges. Existing methods that rely on surface-level geometricinformation (e.g., bounding box and pose) derived from visual cues (e.g.,color, shape, and texture) often lacks adaptability and robustness, especiallywhen foods share similar physical properties but differ in visual appearance.We employ imitation learning (IL) to learn a policy for food acquisition.Existing methods employ IL or Reinforcement Learning (RL) to learn a policybased on off-the-shelf image encoders such as ResNet-50. However, suchrepresentations are not robust and struggle to generalize across diverseacquisition scenarios. To address these limitations, we propose a novelapproach, IMRL (Integrated Multi-Dimensional Representation Learning), whichintegrates visual, physical, temporal, and geometric representations to enhancethe robustness and generalizability of IL for food acquisition. Our approachcaptures food types and physical properties (e.g., solid, semi-solid, granular,liquid, and mixture), models temporal dynamics of acquisition actions, andintroduces geometric information to determine optimal scooping points andassess bowl fullness. IMRL enables IL to adaptively adjust scooping strategiesbased on context, improving the robot's capability to handle diverse foodacquisition scenarios. Experiments on a real robot demonstrate our approach'srobustness and adaptability across various foods and bowl configurations,including zero-shot generalization to unseen settings. Our approach achievesimprovement up to $35\%$ in success rate compared with the best-performingbaseline.</description><author>Rui Liu, Zahiruddin Mahammad, Amisha Bhaskar, Pratap Tokekar</author><pubDate>Wed, 18 Sep 2024 16:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12092v1</guid></item><item><title>The Impact of Element Ordering on LM Agent Performance</title><link>http://arxiv.org/abs/2409.12089v1</link><description>There has been a surge of interest in language model agents that can navigatevirtual environments such as the web or desktop. To navigate such environments,agents benefit from information on the various elements (e.g., buttons, text,or images) present. It remains unclear which element attributes have thegreatest impact on agent performance, especially in environments that onlyprovide a graphical representation (i.e., pixels). Here we find that theordering in which elements are presented to the language model is surprisinglyimpactful--randomizing element ordering in a webpage degrades agent performancecomparably to removing all visible text from an agent's state representation.While a webpage provides a hierarchical ordering of elements, there is no suchordering when parsing elements directly from pixels. Moreover, as tasks becomemore challenging and models more sophisticated, our experiments suggest thatthe impact of ordering increases. Finding an effective ordering is non-trivial.We investigate the impact of various element ordering methods in web anddesktop environments. We find that dimensionality reduction provides a viableordering for pixel-only environments. We train a UI element detection model toderive elements from pixels and apply our findings to an agentbenchmark--OmniACT--where we only have access to pixels. Our method completesmore than two times as many tasks on average relative to the previousstate-of-the-art.</description><author>Wayne Chi, Ameet Talwalkar, Chris Donahue</author><pubDate>Wed, 18 Sep 2024 16:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12089v1</guid></item><item><title>Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques</title><link>http://arxiv.org/abs/2409.12087v1</link><description>This study explores the potential of utilizing administrative claims data,combined with advanced machine learning and deep learning techniques, topredict the progression of Chronic Kidney Disease (CKD) to End-Stage RenalDisease (ESRD). We analyze a comprehensive, 10-year dataset provided by a majorhealth insurance organization to develop prediction models for multipleobservation windows using traditional machine learning methods such as RandomForest and XGBoost as well as deep learning approaches such as Long Short-TermMemory (LSTM) networks. Our findings demonstrate that the LSTM model,particularly with a 24-month observation window, exhibits superior performancein predicting ESRD progression, outperforming existing models in theliterature. We further apply SHapley Additive exPlanations (SHAP) analysis toenhance interpretability, providing insights into the impact of individualfeatures on predictions at the individual patient level. This study underscoresthe value of leveraging administrative claims data for CKD management andpredicting ESRD progression.</description><author>Yubo Li, Saba Al-Sayouri, Rema Padman</author><pubDate>Wed, 18 Sep 2024 16:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12087v1</guid></item><item><title>Calibration Error for Decision Making</title><link>http://arxiv.org/abs/2404.13503v3</link><description>Calibration allows predictions to be reliably interpreted as probabilities bydecision makers. We propose a decision-theoretic calibration error, theCalibration Decision Loss (CDL), defined as the maximum improvement in decisionpayoff obtained by calibrating the predictions, where the maximum is over allpayoff-bounded decision tasks. Vanishing CDL guarantees the payoff loss frommiscalibration vanishes simultaneously for all downstream decision tasks. Weshow separations between CDL and existing calibration error metrics, includingthe most well-studied metric Expected Calibration Error (ECE). Our maintechnical contribution is a new efficient algorithm for online calibration thatachieves near-optimal $O(\frac{\log T}{\sqrt{T}})$ expected CDL, bypassing the$\Omega(T^{-0.472})$ lower bound for ECE by Qiao and Valiant (2021).</description><author>Lunjia Hu, Yifan Wu</author><pubDate>Wed, 18 Sep 2024 15:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13503v3</guid></item><item><title>Denoising diffusion models for high-resolution microscopy image restoration</title><link>http://arxiv.org/abs/2409.12078v1</link><description>Advances in microscopy imaging enable researchers to visualize structures atthe nanoscale level thereby unraveling intricate details of biologicalorganization. However, challenges such as image noise, photobleaching offluorophores, and low tolerability of biological samples to high light dosesremain, restricting temporal resolutions and experiment durations. Reducedlaser doses enable longer measurements at the cost of lower resolution andincreased noise, which hinders accurate downstream analyses. Here we train adenoising diffusion probabilistic model (DDPM) to predict high-resolutionimages by conditioning the model on low-resolution information. Additionally,the probabilistic aspect of the DDPM allows for repeated generation of imagesthat tend to further increase the signal-to-noise ratio. We show that our modelachieves a performance that is better or similar to the previouslybest-performing methods, across four highly diverse datasets. Importantly,while any of the previous methods show competitive performance for some, butnot all datasets, our method consistently achieves high performance across allfour data sets, suggesting high generalizability.</description><author>Pamela Osuna-Vargas, Maren H. Wehrheim, Lucas Zinz, Johanna Rahm, Ashwin Balakrishnan, Alexandra Kaminer, Mike Heilemann, Matthias Kaschube</author><pubDate>Wed, 18 Sep 2024 15:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12078v1</guid></item><item><title>Unsupervised Domain Adaptation Via Data Pruning</title><link>http://arxiv.org/abs/2409.12076v1</link><description>The removal of carefully-selected examples from training data has recentlyemerged as an effective way of improving the robustness of machine learningmodels. However, the best way to select these examples remains an openquestion. In this paper, we consider the problem from the perspective ofunsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDAwhereby training examples are removed to attempt to align the trainingdistribution to that of the target data. By adopting the maximum meandiscrepancy (MMD) as the criterion for alignment, the problem can be neatlyformulated and solved as an integer quadratic program. We evaluate our approachon a real-world domain shift task of bioacoustic event detection. As a methodfor UDA, we show that AdaPrune outperforms related techniques, and iscomplementary to other UDA algorithms such as CORAL. Our analysis of therelationship between the MMD and model accuracy, along with t-SNE plots,validate the proposed method as a principled and well-founded way of performingdata pruning.</description><author>Andrea Napoli, Paul White</author><pubDate>Wed, 18 Sep 2024 15:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12076v1</guid></item><item><title>Visualizing Temporal Topic Embeddings with a Compass</title><link>http://arxiv.org/abs/2409.10649v2</link><description>Dynamic topic modeling is useful at discovering the development and change inlatent topics over time. However, present methodology relies on algorithms thatseparate document and word representations. This prevents the creation of ameaningful embedding space where changes in word usage and documents can bedirectly analyzed in a temporal context. This paper proposes an expansion ofthe compass-aligned temporal Word2Vec methodology into dynamic topic modeling.Such a method allows for the direct comparison of word and document embeddingsacross time in dynamic topics. This enables the creation of visualizations thatincorporate temporal word embeddings within the context of documents into topicvisualizations. In experiments against the current state-of-the-art, ourproposed method demonstrates overall competitive performance in topic relevancyand diversity across temporal datasets of varying size. Simultaneously, itprovides insightful visualizations focused on temporal word embeddings whilemaintaining the insights provided by global topic evolution, advancing ourunderstanding of how topics evolve over time.</description><author>Daniel Palamarchuk, Lemara Williams, Brian Mayer, Thomas Danielson, Rebecca Faust, Larry Deschaine, Chris North</author><pubDate>Wed, 18 Sep 2024 15:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10649v2</guid></item><item><title>Online Refractive Camera Model Calibration in Visual Inertial Odometry</title><link>http://arxiv.org/abs/2409.12074v1</link><description>This paper presents a general refractive camera model and onlineco-estimation of odometry and the refractive index of unknown media. Thisenables operation in diverse and varying refractive fluids, given only thecamera calibration in air. The refractive index is estimated online as a statevariable of a monocular visual-inertial odometry framework in an iterativeformulation using the proposed camera model. The method was verified on datacollected using an underwater robot traversing inside a pool. The evaluationsdemonstrate convergence to the ideal refractive index for water despitesignificant perturbations in the initialization. Simultaneously, the approachenables on-par visual-inertial odometry performance in refractive media withoutprior knowledge of the refractive index or requirement of medium-specificcamera calibration.</description><author>Mohit Singh, Kostas Alexis</author><pubDate>Wed, 18 Sep 2024 15:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12074v1</guid></item><item><title>PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning</title><link>http://arxiv.org/abs/2409.12072v1</link><description>Backdoor attacks pose a significant threat to deep neural networks,particularly as recent advancements have led to increasingly subtleimplantation, making the defense more challenging. Existing defense mechanismstypically rely on an additional clean dataset as a standard reference andinvolve retraining an auxiliary model or fine-tuning the entire victim model.However, these approaches are often computationally expensive and not alwaysfeasible in practical applications. In this paper, we propose a novel andlightweight defense mechanism, termed PAD-FT, that does not require anadditional clean dataset and fine-tunes only a very small part of the model todisinfect the victim model. To achieve this, our approach first introduces asimple data purification process to identify and select the most-likely cleandata from the poisoned training dataset. The self-purified clean dataset isthen used for activation clipping and fine-tuning only the last classificationlayer of the victim model. By integrating data purification, activationclipping, and classifier fine-tuning, our mechanism PAD-FT demonstratessuperior effectiveness across multiple backdoor attack methods and datasets, asconfirmed through extensive experimental evaluation.</description><author>Yukai Xu, Yujie Gu, Kouichi Sakurai</author><pubDate>Wed, 18 Sep 2024 15:47:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12072v1</guid></item><item><title>Adaptive Step Sizes for Preconditioned Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2311.16956v2</link><description>This paper proposes a novel approach to adaptive step sizes in stochasticgradient descent (SGD) by utilizing quantities that we have identified asnumerically traceable -- the Lipschitz constant for gradients and a concept ofthe local variance in search directions. Our findings yield a nearlyhyperparameter-free algorithm for stochastic optimization, which has provableconvergence properties and exhibits truly problem adaptive behavior onclassical image classification tasks. Our framework is set in a general Hilbertspace and thus enables the potential inclusion of a preconditioner through thechoice of the inner product.</description><author>Frederik KÃ¶hne, Leonie Kreis, Anton Schiela, Roland Herzog</author><pubDate>Wed, 18 Sep 2024 15:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16956v2</guid></item><item><title>Fitting Multilevel Factor Models</title><link>http://arxiv.org/abs/2409.12067v1</link><description>We examine a special case of the multilevel factor model, with covariancegiven by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. Wedevelop a novel, fast implementation of the expectation-maximization (EM)algorithm, tailored for multilevel factor models, to maximize the likelihood ofthe observed data. This method accommodates any hierarchical structure andmaintains linear time and storage complexities per iteration. This is achievedthrough a new efficient technique for computing the inverse of the positivedefinite MLR matrix. We show that the inverse of an invertible PSD MLR matrixis also an MLR matrix with the same sparsity in factors, and we use therecursive Sherman-Morrison-Woodbury matrix identity to obtain the factors ofthe inverse. Additionally, we present an algorithm that computes the Choleskyfactorization of an expanded matrix with linear time and space complexities,yielding the covariance matrix as its Schur complement. This paper isaccompanied by an open-source package that implements the proposed methods.</description><author>Tetiana Parshakova, Trevor Hastie, Stephen Boyd</author><pubDate>Wed, 18 Sep 2024 15:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12067v1</guid></item><item><title>A geometric view on probabilistically robust learning</title><link>http://arxiv.org/abs/2305.18779v2</link><description>Although deep neural networks have achieved super-human performance on manyclassification tasks, they often exhibit a worrying lack of robustness towardsadversarially generated examples. Thus, considerable effort has been investedinto reformulating standard Risk Minimization (RM) into an adversarially robustframework. Recently, attention has shifted towards approaches which interpolatebetween the robustness offered by adversarial training and the higher cleanaccuracy and faster training times of RM. In this paper, we take a fresh andgeometric view on one such method -- Probabilistically Robust Learning (PRL).We propose a mathematical framework for understanding PRL, which allows us toidentify geometric pathologies in its original formulation and to introduce afamily of probabilistic nonlocal perimeter functionals to rectify them. Weprove existence of solutions to the original and modified problems using novelrelaxation methods and also study properties, as well as local limits, of theintroduced perimeters. We also clarify, through a suitable $\Gamma$-convergenceanalysis, the way in which the original and modified PRL models interpolatebetween risk minimization and adversarial training.</description><author>Leon Bungert, NicolÃ¡s GarcÃ­a Trillos, Matt Jacobs, Daniel McKenzie, ÄorÄe NikoliÄ, Qingsong Wang</author><pubDate>Wed, 18 Sep 2024 15:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18779v2</guid></item><item><title>Generalized Robot Learning Framework</title><link>http://arxiv.org/abs/2409.12061v1</link><description>Imitation based robot learning has recently gained significant attention inthe robotics field due to its theoretical potential for transferability andgeneralizability. However, it remains notoriously costly, both in terms ofhardware and data collection, and deploying it in real-world environmentsdemands meticulous setup of robots and precise experimental conditions. In thispaper, we present a low-cost robot learning framework that is both easilyreproducible and transferable to various robots and environments. Wedemonstrate that deployable imitation learning can be successfully applied evento industrial-grade robots, not just expensive collaborative robotic arms.Furthermore, our results show that multi-task robot learning is achievable withsimple network architectures and fewer demonstrations than previously thoughtnecessary. As the current evaluating method is almost subjective when it comesto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - anovel evaluation strategy that provides a more objective assessment ofperformance. We conduct an extensive comparison of success rates across variousself-designed tasks to validate our approach. To foster collaboration andsupport the robot learning community, we have open-sourced all relevantdatasets and model checkpoints, available at huggingface.co/ZhiChengAI.</description><author>Jiahuan Yan, Zhouyang Hong, Yu Zhao, Yu Tian, Yunxin Liu, Travis Davies, Luhui Hu</author><pubDate>Wed, 18 Sep 2024 15:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12061v1</guid></item><item><title>PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models</title><link>http://arxiv.org/abs/2409.12060v1</link><description>The task of determining whether two texts are paraphrases has long been achallenge in NLP. However, the prevailing notion of paraphrase is often quitesimplistic, offering only a limited view of the vast spectrum of paraphrasephenomena. Indeed, we find that evaluating models in a paraphrase dataset canleave uncertainty about their true semantic understanding. To alleviate this,we release paraphrasus, a benchmark designed for multi-dimensional assessmentof paraphrase detection models and finer model selection. We find thatparaphrase detection models under a fine-grained evaluation lens exhibittrade-offs that cannot be captured through a single classification dataset.</description><author>Andrianos Michail, Simon Clematide, Juri Opitz</author><pubDate>Wed, 18 Sep 2024 15:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12060v1</guid></item><item><title>Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking</title><link>http://arxiv.org/abs/2409.12059v1</link><description>Large Language Model can reasonably understand and generate human expressionsbut may lack of thorough thinking and reasoning mechanisms. Recently there havebeen several studies which enhance the thinking ability of language models butmost of them are not data-driven or training-based. In this paper, we aremotivated by the cognitive mechanism in the natural world, and design a novelmodel architecture called TaS which allows it to first consider the thoughtsand then express the response based upon the query. We design several pipelinesto annotate or generate the thought contents from prompt-response samples, thenadd language heads in a middle layer which behaves as the thinking layer. Wetrain the language model by the thoughts-augmented data and successfully letthe thinking layer automatically generate reasonable thoughts and finallyoutput more reasonable responses. Both qualitative examples and quantitativeresults validate the effectiveness and performance of TaS. Our code isavailable at https://anonymous.4open.science/r/TadE.</description><author>Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Wed, 18 Sep 2024 15:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12059v1</guid></item><item><title>Cartan moving frames and the data manifolds</title><link>http://arxiv.org/abs/2409.12057v1</link><description>The purpose of this paper is to employ the language of Cartan moving framesto study the geometry of the data manifolds and its Riemannian structure, viathe data information metric and its curvature at data points. Using thisframework and through experiments, explanations on the response of a neuralnetwork are given by pointing out the output classes that are easily reachablefrom a given input. This emphasizes how the proposed mathematical relationshipbetween the output of the network and the geometry of its inputs can beexploited as an explainable artificial intelligence tool.</description><author>Eliot Tron, Rita Fioresi, Nicolas Couellan, StÃ©phane Puechmorel</author><pubDate>Wed, 18 Sep 2024 15:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12057v1</guid></item><item><title>PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning</title><link>http://arxiv.org/abs/2305.19472v3</link><description>Procedural planning, which entails decomposing a high-level goal into asequence of temporally ordered steps, is an important yet intricate task formachines. It involves integrating common-sense knowledge to reason aboutcomplex and often contextualized situations, e.g. ``scheduling a doctor'sappointment without a phone''. While current approaches show encouragingresults using large language models (LLMs), they are hindered by drawbacks suchas costly API calls and reproducibility issues. In this paper, we advocateplanning using smaller language models. We present PlaSma, a novel two-prongedapproach to endow small language models with procedural knowledge and(constrained) language planning capabilities. More concretely, we developsymbolic procedural knowledge distillation to enhance the commonsense knowledgein small language models and an inference-time algorithm to facilitate morestructured and accurate reasoning. In addition, we introduce a new relatedtask, Replanning, that requires a revision of a plan to cope with a constrainedsituation. In both the planning and replanning settings, we show thatorders-of-magnitude smaller models (770M-11B parameters) can compete and oftensurpass their larger teacher models' capabilities. Finally, we showcasesuccessful application of PlaSma in an embodied environment, VirtualHome.</description><author>Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona J. Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi</author><pubDate>Wed, 18 Sep 2024 15:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19472v3</guid></item><item><title>Extended Deep Submodular Functions</title><link>http://arxiv.org/abs/2409.12053v1</link><description>We introduce a novel category of set functions called Extended DeepSubmodular functions (EDSFs), which are neural network-representable. EDSFsserve as an extension of Deep Submodular Functions (DSFs), inheriting crucialproperties from DSFs while addressing innate limitations. It is known that DSFscan represent a limiting subset of submodular functions. In contrast, throughan analysis of polymatroid properties, we establish that EDSFs possess thecapability to represent all monotone submodular functions, a notableenhancement compared to DSFs. Furthermore, our findings demonstrate that EDSFscan represent any monotone set function, indicating the family of EDSFs isequivalent to the family of all monotone set functions. Additionally, we provethat EDSFs maintain the concavity inherent in DSFs when the components of theinput vector are non-negative real numbers-an essential feature in certaincombinatorial optimization problems. Through extensive experiments, weillustrate that EDSFs exhibit significantly lower empirical generalizationerror than DSFs in the learning of coverage functions. This suggests that EDSFspresent a promising advancement in the representation and learning of setfunctions with improved generalization capabilities.</description><author>Seyed Mohammad Hosseini, Arash Jamshid, Seyed Mahdi Noormousavi, Mahdi Jafari Siavoshani, Naeimeh Omidvar</author><pubDate>Wed, 18 Sep 2024 15:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12053v1</guid></item><item><title>A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models</title><link>http://arxiv.org/abs/2408.14496v3</link><description>Recent advances in deep learning have completely transformed the domain ofcomputational pathology (CPath). More specifically, it has altered thediagnostic workflow of pathologists by integrating foundation models (FMs) andvision-language models (VLMs) in their assessment and decision-making process.The limitations of existing deep learning approaches in CPath can be overcomeby FMs through learning a representation space that can be adapted to a widevariety of downstream tasks without explicit supervision. Deploying VLMs allowpathology reports written in natural language be used as rich semanticinformation sources to improve existing models as well as generate predictionsin natural language form. In this survey, a holistic and systematic overview ofrecent innovations in FMs and VLMs in CPath is presented. Furthermore, thetools, datasets and training schemes for these models are summarized inaddition to categorizing them into distinct groups. This extensive surveyhighlights the current trends in CPath and its possible revolution through theuse of FMs and VLMs in the future.</description><author>Dibaloke Chanda, Milan Aryal, Nasim Yahya Soltani, Masoud Ganji</author><pubDate>Wed, 18 Sep 2024 15:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14496v3</guid></item><item><title>Comparison of Two Augmentation Methods in Improving Detection Accuracy of Hemarthrosis</title><link>http://arxiv.org/abs/2409.05225v2</link><description>With the increase of computing power, machine learning models in medicalimaging have been introduced to help in rending medical diagnosis andinspection, like hemophilia, a rare disorder in which blood cannot clotnormally. Often, one of the bottlenecks of detecting hemophilia is the lack ofdata available to train the algorithm to increase the accuracy. As a possiblesolution, this research investigated whether introducing augmented data by datasynthesis or traditional augmentation techniques can improve model accuracy,helping to diagnose the diseases. To tackle this research, features ofultrasound images were extracted by the pre-trained VGG-16, and similaritieswere compared by cosine similarity measure based on extracted features indifferent distributions among real images, synthetic images, and augmentationimages (Real vs. Real, Syn vs. Syn, Real vs. Different Batches of Syn, Real vs.Augmentation Techniques). Model testing performance was investigated usingEffientNet-B4 to recognize "blood" images with two augmentation methods. Inaddition, a gradient-weighted class activation mapping (Grad-CAM) visualizationwas used to interpret the unexpected results like loss of accuracy. Syntheticand real images do not show high similarity, with a mean similarity score of0.4737. Synthetic batch 1 dataset and images by horizontal flip are moresimilar to the original images. Classic augmentation techniques and datasynthesis can improve model accuracy, and data by traditional augmentationtechniques have a better performance than synthetic data. In addition, theGrad-CAM heatmap figured out the loss of accuracy is due to a shift in thedomain. Overall, this research found that two augmentation methods, datasynthesis and traditional augmentation techniques, both can improve accuracy toa certain extent to help to diagnose rare diseases.</description><author>Qianyu Fan</author><pubDate>Wed, 18 Sep 2024 15:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05225v2</guid></item><item><title>Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model</title><link>http://arxiv.org/abs/2409.09263v3</link><description>As Chile's electric power sector advances toward a future powered byrenewable energy, accurate forecasting of renewable generation is essential formanaging grid operations. The integration of renewable energy sources isparticularly challenging due to the operational difficulties of managing theirpower generation, which is highly variable compared to fossil fuel sources,delaying the availability of clean energy. To mitigate this, we quantify theimpact of increasing intermittent generation from wind and solar on thermalpower plants in Chile and introduce a hybrid wind speed forecasting methodologywhich combines two custom ML models for Chile. The first model is based onTiDE, an MLP-based ML model for short-term forecasts, and the second is basedon a graph neural network, GraphCast, for medium-term forecasts up to 10 days.Our hybrid approach outperforms the most accurate operational deterministicsystems by 4-21% for short-term forecasts and 5-23% for medium-term forecastsand can directly lower the impact of wind generation on thermal ramping,curtailment, and system-level emissions in Chile.</description><author>Dhruv Suri, Praneet Dutta, Flora Xue, Ines Azevedo, Ravi Jain</author><pubDate>Wed, 18 Sep 2024 15:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09263v3</guid></item><item><title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title><link>http://arxiv.org/abs/2409.12046v1</link><description>Tables, figures, and listings (TFLs) are essential tools for summarizingclinical trial data. Creation of TFLs for reporting activities is often atime-consuming task encountered routinely during the execution of clinicaltrials. This study explored the use of large language models (LLMs) to automatethe generation of TFLs through prompt engineering and few-shot transferlearning. Using public clinical trial data in ADaM format, our resultsdemonstrated that LLMs can efficiently generate TFLs with prompt instructions,showcasing their potential in this domain. Furthermore, we developed aconservational agent named Clinical Trial TFL Generation Agent: An app thatmatches user queries to predefined prompts that produce customized programs togenerate specific predefined TFLs.</description><author>Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu</author><pubDate>Wed, 18 Sep 2024 15:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12046v1</guid></item></channel></rss>