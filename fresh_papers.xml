<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 05 Mar 2024 06:00:26 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting</title><link>http://arxiv.org/abs/2401.06040v3</link><description>Traffic forecasting is the foundation for intelligent transportation systems.Spatiotemporal graph neural networks have demonstrated state-of-the-artperformance in traffic forecasting. However, these methods do not explicitlymodel some of the natural characteristics in traffic data, such as themultiscale structure that encompasses spatial and temporal variations atdifferent levels of granularity or scale. To that end, we propose aWavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combinesmultiscale analysis (MSA)-based method with Deep Learning (DL)-based method. InWavGCRN, the traffic data is decomposed into time-frequency components withDiscrete Wavelet Transformation (DWT), constructing a multi-stream inputstructure; then Graph Convolutional Recurrent networks (GCRNs) are employed asencoders for each stream, extracting spatiotemporal features in differentscales; and finally the learnable Inversed DWT and GCRN are combined as thedecoder, fusing the information from all streams for traffic metricsreconstruction and prediction. Furthermore, road-network-informed graphs anddata-driven graph learning are combined to accurately capture spatialcorrelation. The proposed method can offer well-defined interpretability,powerful learning capability, and competitive forecasting performance onreal-world traffic data sets.</description><author>Qipeng Qian, Tanwi Mallick</author><pubDate>Mon, 04 Mar 2024 15:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06040v3</guid></item><item><title>Do Language Models' Words Refer?</title><link>http://arxiv.org/abs/2308.05576v3</link><description>What do language models (LMs) do with language? Everyone agrees that they canproduce sequences of (mostly) coherent strings of English. But do thosesentences mean something, or are LMs simply babbling in a convincing simulacrumof language use? Here we will address one aspect of this broad question:whether LMs' words can refer, that is, achieve "word-to-world" connections.There is prima facie reason to think they do not since LMs do not interact withthe world in the way that ordinary language users do. Drawing on insights fromthe externalist tradition in philosophy of language, we argue that thoseappearances are misleading: even if the inputs to an LM are simply strings oftext, they are strings of text with natural histories, and that may suffice toput LMs' words into referential contact with the external world.</description><author>Matthew Mandelkern, Tal Linzen</author><pubDate>Mon, 04 Mar 2024 14:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05576v3</guid></item><item><title>DAMS-DETR: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion</title><link>http://arxiv.org/abs/2403.00326v2</link><description>Infrared-visible object detection aims to achieve robust even full-day objectdetection by fusing the complementary information of infrared and visibleimages. However, highly dynamically variable complementary characteristics andcommonly existing modality misalignment make the fusion of complementaryinformation difficult. In this paper, we propose a Dynamic AdaptiveMultispectral Detection Transformer (DAMS-DETR) based on DETR to simultaneouslyaddress these two challenges. Specifically, we propose a Modality CompetitiveQuery Selection strategy to provide useful prior information. This strategy candynamically select basic salient modality feature representation for eachobject. To effectively mine the complementary information and adapt tomisalignment situations, we propose a Multispectral Deformable Cross-attentionmodule to adaptively sample and aggregate multi-semantic level features ofinfrared and visible images for each object. In addition, we further adopt thecascade structure of DETR to better mine complementary information. Experimentson four public datasets of different scenes demonstrate significantimprovements compared to other state-of-the-art methods. The code will bereleased at https://github.com/gjj45/DAMS-DETR.</description><author>Junjie Guo, Chenqiang Gao, Fangcen Liu, Deyu Meng</author><pubDate>Mon, 04 Mar 2024 13:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00326v2</guid></item><item><title>WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset</title><link>http://arxiv.org/abs/2402.19282v3</link><description>This paper presents WanJuan-CC, a safe and high-quality open-sourced Englishwebtext dataset derived from Common Crawl data. The study addresses thechallenges of constructing large-scale pre-training datasets for languagemodels, which require vast amounts of high-quality data. A comprehensiveprocess was designed to handle Common Crawl data, including extraction,heuristic rule filtering, fuzzy deduplication, content safety filtering, anddata quality filtering. From approximately 68 billion original Englishdocuments, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens ofhigh-quality data as part of WanJuan-CC. We have open-sourced 100B Tokens fromthis dataset. The paper also provides statistical information related to dataquality, enabling users to select appropriate data according to their needs. Toevaluate the quality and utility of the dataset, we trained 1B-parameter and3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Resultsshow that WanJuan-CC performs better on validation datasets and downstreamtasks.</description><author>Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang Xu, Wei Li, Zhongyin Tu, Hang Yan, Conghui He</author><pubDate>Mon, 04 Mar 2024 12:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19282v3</guid></item><item><title>Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities</title><link>http://arxiv.org/abs/2309.16739v3</link><description>Large language models (LLMs), which have shown remarkable capabilities, arerevolutionizing AI development and potentially shaping our future. However,given their multimodality, the status quo cloud-based deployment faces somecritical challenges: 1) long response time; 2) high bandwidth costs; and 3) theviolation of data privacy. 6G mobile edge computing (MEC) systems may resolvethese pressing issues. In this article, we explore the potential of deployingLLMs at the 6G edge. We start by introducing killer applications powered bymultimodal LLMs, including robotics and healthcare, to highlight the need fordeploying LLMs in the vicinity of end users. Then, we identify the criticalchallenges for LLM deployment at the edge and envision the 6G MEC architecturefor LLMs. Furthermore, we delve into two design aspects, i.e., edge trainingand edge inference for LLMs. In both aspects, considering the inherent resourcelimitations at the edge, we discuss various cutting-edge techniques, includingsplit learning/inference, parameter-efficient fine-tuning, quantization, andparameter-sharing inference, to facilitate the efficient deployment of LLMs.This article serves as a position paper for thoroughly identifying themotivation, challenges, and pathway for empowering LLMs at the 6G edge.</description><author>Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen, Kaibin Huang</author><pubDate>Mon, 04 Mar 2024 12:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16739v3</guid></item><item><title>Correspondence-free online human motion retargeting</title><link>http://arxiv.org/abs/2302.00556v3</link><description>We present a data-driven framework for unsupervised human motion retargetingthat animates a target subject with the motion of a source subject. Our methodis correspondence-free, requiring neither spatial correspondences between thesource and target shapes nor temporal correspondences between different framesof the source motion. This allows to animate a target shape with arbitrarysequences of humans in motion, possibly captured using 4D acquisition platformsor consumer devices. Our method unifies the advantages of two existing lines ofwork, namely skeletal motion retargeting, which leverages long-term temporalcontext, and surface-based retargeting, which preserves surface details, bycombining a geometry-aware deformation model with a skeleton-aware motiontransfer approach. This allows to take into account long-term temporal contextwhile accounting for surface details. During inference, our method runs online,i.e. input can be processed in a serial way, and retargeting is performed in asingle forward pass per frame. Experiments show that including long-termtemporal context during training improves the method's accuracy for skeletalmotion and detail preservation. Furthermore, our method generalizes tounobserved motions and body shapes. We demonstrate that our method achievesstate-of-the-art results on two test datasets and that it can be used toanimate human models with the output of a multi-view acquisition platform. Codeis available at\url{https://gitlab.inria.fr/rrekikdi/human-motion-retargeting2023}.</description><author>Rim Rekik, Mathieu Marsot, Anne-Hélène Olivier, Jean-Sébastien Franco, Stefanie Wuhrer</author><pubDate>Mon, 04 Mar 2024 10:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00556v3</guid></item><item><title>Deformable One-shot Face Stylization via DINO Semantic Guidance</title><link>http://arxiv.org/abs/2403.00459v2</link><description>This paper addresses the complex issue of one-shot face stylization, focusingon the simultaneous consideration of appearance and structure, where previousmethods have fallen short. We explore deformation-aware face stylization thatdiverges from traditional single-image style reference, opting for a real-styleimage pair instead. The cornerstone of our method is the utilization of aself-supervised vision transformer, specifically DINO-ViT, to establish arobust and consistent facial structure representation across both real andstyle domains. Our stylization process begins by adapting the StyleGANgenerator to be deformation-aware through the integration of spatialtransformers (STN). We then introduce two innovative constraints for generatorfine-tuning under the guidance of DINO semantics: i) a directional deformationloss that regulates directional vectors in DINO space, and ii) a relativestructural consistency constraint based on DINO token self-similarities,ensuring diverse generation. Additionally, style-mixing is employed to alignthe color generation with the reference, minimizing inconsistentcorrespondences. This framework delivers enhanced deformability for generalone-shot face stylization, achieving notable efficiency with a fine-tuningduration of approximately 10 minutes. Extensive qualitative and quantitativecomparisons demonstrate our superiority over state-of-the-art one-shot facestylization methods. Code is available at https://github.com/zichongc/DoesFS</description><author>Yang Zhou, Zichong Chen, Hui Huang</author><pubDate>Mon, 04 Mar 2024 10:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00459v2</guid></item><item><title>ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models</title><link>http://arxiv.org/abs/2403.00510v2</link><description>Probing the memorization of large language models holds significantimportance. Previous works have established metrics for quantifyingmemorization, explored various influencing factors, such as data duplication,model size, and prompt length, and evaluated memorization by comparing modeloutputs with training corpora. However, the training corpora are of enormousscale and its pre-processing is time-consuming. To explore memorization withoutaccessing training data, we propose a novel approach, named ROME, whereinmemorization is explored by comparing disparities across memorized andnon-memorized. Specifically, models firstly categorize the selected samplesinto memorized and non-memorized groups, and then comparing the demonstrationsin the two groups from the insights of text, probability, and hidden state.Experimental findings show the disparities in factors including word length,part-of-speech, word frequency, mean and variance, just to name a few.</description><author>Bo Li, Qinghua Zhao, Lijie Wen</author><pubDate>Mon, 04 Mar 2024 06:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00510v2</guid></item><item><title>Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks</title><link>http://arxiv.org/abs/2403.00692v2</link><description>The upcoming landscape of Earth Observation missions will defined bynetworked heterogeneous nanosatellite constellations required to meet strictmission requirements, such as revisit times and spatial resolution. However,scheduling satellite communications in these satellite networks throughefficiently creating a global satellite Contact Plan (CP) is a complex task,with current solutions requiring ground-based coordination or being limited byonboard computational resources. The paper proposes a novel approach toovercome these challenges by modeling the constellations and CP as dynamicnetworks and employing graph-based techniques. The proposed method utilizes astate-of-the-art dynamic graph neural network to evaluate the performance of agiven CP and update it using a heuristic algorithm based on simulatedannealing. The trained neural network can predict the network delay with a meanabsolute error of 3.6 minutes. Simulation results show that the proposed methodcan successfully design a contact plan for large satellite networks, improvingthe delay by 29.1%, similar to a traditional approach, while performing theobjective evaluations 20x faster.</description><author>Guillem Casadesus-Vila, Joan-Adria Ruiz-de-Azua, Eduard Alarcon</author><pubDate>Mon, 04 Mar 2024 04:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00692v2</guid></item><item><title>A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement</title><link>http://arxiv.org/abs/2403.01369v1</link><description>Self-supervised learned models have been found to be very effective forcertain speech tasks such as automatic speech recognition, speakeridentification, keyword spotting and others. While the features are undeniablyuseful in speech recognition and associated tasks, their utility in speechenhancement systems is yet to be firmly established, and perhaps not properlyunderstood. In this paper, we investigate the uses of SSL representations forsingle-channel speech enhancement in challenging conditions and find that theyadd very little value for the enhancement task. Our constraints are designedaround on-device real-time speech enhancement -- model is causal, the computefootprint is small. Additionally, we focus on low SNR conditions where suchmodels struggle to provide good enhancement. In order to systematically examinehow SSL representations impact performance of such enhancement models, wepropose a variety of techniques to utilize these embeddings which includedifferent forms of knowledge-distillation and pre-training.</description><author>Ravi Shankar, Ke Tan, Buye Xu, Anurag Kumar</author><pubDate>Sun, 03 Mar 2024 02:05:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01369v1</guid></item><item><title>Improving Cross-lingual Representation for Semantic Retrieval with Code-switching</title><link>http://arxiv.org/abs/2403.01364v1</link><description>Semantic Retrieval (SR) has become an indispensable part of the FAQ system inthe task-oriented question-answering (QA) dialogue scenario. The demands for across-lingual smart-customer-service system for an e-commerce platform or someparticular business conditions have been increasing recently. Most previousstudies exploit cross-lingual pre-trained models (PTMs) for multi-lingualknowledge retrieval directly, while some others also leverage the continualpre-training before fine-tuning PTMs on the downstream tasks. However, nomatter which schema is used, the previous work ignores to inform PTMs of somefeatures of the downstream task, i.e. train their PTMs without providing anysignals related to SR. To this end, in this work, we propose an AlternativeCross-lingual PTM for SR via code-switching. We are the first to utilize thecode-switching approach for cross-lingual SR. Besides, we introduce the novelcode-switched continual pre-training instead of directly using the PTMs on theSR tasks. The experimental results show that our proposed approach consistentlyoutperforms the previous SOTA methods on SR and semantic textual similarity(STS) tasks with three business corpora and four open datasets in 20+languages.</description><author>Mieradilijiang Maimaiti, Yuanhang Zheng, Ji Zhang, Fei Huang, Yue Zhang, Wenpei Luo, Kaiyu Huang</author><pubDate>Sun, 03 Mar 2024 01:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01364v1</guid></item><item><title>Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model</title><link>http://arxiv.org/abs/2403.01362v1</link><description>Precision in identifying and differentiating micro and macro blood vessels inthe retina is crucial for the diagnosis of retinal diseases, although it posesa significant challenge. Current autoencoding-based segmentation approachesencounter limitations as they are constrained by the encoder and undergo areduction in resolution during the encoding stage. The inability to recoverlost information in the decoding phase further impedes these approaches.Consequently, their capacity to extract the retinal microvascular structure isrestricted. To address this issue, we introduce Swin-Res-Net, a specializedmodule designed to enhance the precision of retinal vessel segmentation.Swin-Res-Net utilizes the Swin transformer which uses shifted windows withdisplacement for partitioning, to reduce network complexity and acceleratemodel convergence. Additionally, the model incorporates interactive fusion witha functional module in the Res2Net architecture. The Res2Net leveragesmulti-scale techniques to enlarge the receptive field of the convolutionalkernel, enabling the extraction of additional semantic information from theimage. This combination creates a new module that enhances the localization andseparation of micro vessels in the retina. To improve the efficiency ofprocessing vascular information, we've added a module to eliminate redundantinformation between the encoding and decoding steps. Our proposed architecture produces outstanding results, either meeting orsurpassing those of other published models. The AUC reflects significantenhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wisesegmentation of retinal vessels across three widely utilized datasets:CHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperformsalternative architectures, demonstrating superior performance in both IOU andF1 measure metrics.</description><author>Rui Yang, Shunpu Zhang</author><pubDate>Sun, 03 Mar 2024 01:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01362v1</guid></item><item><title>Bandit Profit-maximization for Targeted Marketing</title><link>http://arxiv.org/abs/2403.01361v1</link><description>We study a sequential profit-maximization problem, optimizing for both priceand ancillary variables like marketing expenditures. Specifically, we aim tomaximize profit over an arbitrary sequence of multiple demand curves, eachdependent on a distinct ancillary variable, but sharing the same price. Aprototypical example is targeted marketing, where a firm (seller) wishes tosell a product over multiple markets. The firm may invest different marketingexpenditures for different markets to optimize customer acquisition, but mustmaintain the same price across all markets. Moreover, markets may haveheterogeneous demand curves, each responding to prices and marketingexpenditures differently. The firm's objective is to maximize its gross profit,the total revenue minus marketing costs. Our results are near-optimal algorithms for this class of problems in anadversarial bandit setting, where demand curves are arbitrary non-adaptivesequences, and the firm observes only noisy evaluations of chosen points on thedemand curves. We prove a regret upper bound of$\widetilde{\mathcal{O}}\big(nT^{3/4}\big)$ and a lower bound of$\Omega\big((nT)^{3/4}\big)$ for monotonic demand curves, and a regret bound of$\widetilde{\Theta}\big(nT^{2/3}\big)$ for demands curves that are monotonic inprice and concave in the ancillary variables.</description><author>Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy</author><pubDate>Sun, 03 Mar 2024 01:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01361v1</guid></item><item><title>CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain</title><link>http://arxiv.org/abs/2402.07234v2</link><description>Large Language Models (LLMs) have demonstrated significant potential andeffectiveness across multiple application domains. To assess the performance ofmainstream LLMs in public security tasks, this study aims to construct aspecialized evaluation benchmark tailored to the Chinese public securitydomain--CPSDbench. CPSDbench integrates datasets related to public securitycollected from real-world scenarios, supporting a comprehensive assessment ofLLMs across four key dimensions: text classification, information extraction,question answering, and text generation. Furthermore, this study introduces aset of innovative evaluation metrics designed to more precisely quantify theefficacy of LLMs in executing tasks related to public security. Through thein-depth analysis and evaluation conducted in this research, we not onlyenhance our understanding of the performance strengths and limitations ofexisting models in addressing public security issues but also providereferences for the future development of more accurate and customized LLMmodels targeted at applications in this field.</description><author>Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu, Qiang Cheng</author><pubDate>Sun, 03 Mar 2024 01:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07234v2</guid></item><item><title>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge</title><link>http://arxiv.org/abs/2311.05112v4</link><description>Large language models (LLMs), such as ChatGPT, have received substantialattention due to their capabilities for understanding and generating humanlanguage. While there has been a burgeoning trend in research focusing on theemployment of LLMs in supporting different medical tasks (e.g., enhancingclinical diagnostics and providing medical education), a review of theseefforts, particularly their development, practical applications, and outcomesin medicine, remains scarce. Therefore, this review aims to provide a detailedoverview of the development and deployment of LLMs in medicine, including thechallenges and opportunities they face. In terms of development, we provide adetailed introduction to the principles of existing medical LLMs, includingtheir basic model structures, number of parameters, and sources and scales ofdata used for model development. It serves as a guide for practitioners indeveloping medical LLMs tailored to their specific needs. In terms ofdeployment, we offer a comparison of the performance of different LLMs acrossvarious medical tasks, and further compare them with state-of-the-artlightweight models, aiming to provide an understanding of the advantages andlimitations of LLMs in medicine. Overall, in this review, we address thefollowing questions: 1) What are the practices for developing medical LLMs 2)How to measure the medical task performance of LLMs in a medical setting? 3)How have medical LLMs been employed in real-world practice? 4) What challengesarise from the use of medical LLMs? and 5) How to more effectively develop anddeploy medical LLMs? By answering these questions, this review aims to provideinsights into the opportunities for LLMs in medicine and serve as a practicalresource. We also maintain a regularly updated list of practical guides onmedical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.</description><author>Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton</author><pubDate>Sun, 03 Mar 2024 01:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05112v4</guid></item><item><title>a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification</title><link>http://arxiv.org/abs/2403.01355v1</link><description>Spoofing detection is today a mainstream research topic. Standard metrics canbe applied to evaluate the performance of isolated spoofing detection solutionsand others have been proposed to support their evaluation when they arecombined with speaker detection. These either have well-known deficiencies orrestrict the architectural approach to combine speaker and spoof detectors. Inthis paper, we propose an architecture-agnostic detection cost function(a-DCF). A generalisation of the original DCF used widely for the assessment ofautomatic speaker verification (ASV), the a-DCF is designed for the evaluationof spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisionsin a Bayes risk sense, with explicitly defined class priors and detection costmodel. We demonstrate the merit of the a-DCF through the benchmarkingevaluation of architecturally-heterogeneous spoofing-robust ASV solutions.</description><author>Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen, Nicholas Evans, Jean-Francois Bonastre, Itshak Lapidot</author><pubDate>Sun, 03 Mar 2024 00:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01355v1</guid></item><item><title>Improving Uncertainty Sampling with Bell Curve Weight Function</title><link>http://arxiv.org/abs/2403.01352v1</link><description>Typically, a supervised learning model is trained using passive learning byrandomly selecting unlabelled instances to annotate. This approach is effectivefor learning a model, but can be costly in cases where acquiring labelledinstances is expensive. For example, it can be time-consuming to manuallyidentify spam mails (labelled instances) from thousands of emails (unlabelledinstances) flooding an inbox during initial data collection. Generally, weanswer the above scenario with uncertainty sampling, an active learning methodthat improves the efficiency of supervised learning by using fewer labelledinstances than passive learning. Given an unlabelled data pool, uncertaintysampling queries the labels of instances where the predicted probabilities, p,fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquiredlabels are then added to the existing labelled data pool to learn a new model.Nonetheless, the performance of uncertainty sampling is susceptible to the areaof unpredictable responses (AUR) and the nature of the dataset. It is difficultto determine whether to use passive learning or uncertainty sampling withoutprior knowledge of a new dataset. To address this issue, we propose bell curvesampling, which employs a bell curve weight function to acquire new labels.With the bell curve centred at p=0.5, bell curve sampling selects instanceswhose predicted values are in the uncertainty area most of the time withoutneglecting the rest. Simulation results show that, most of the time bell curvesampling outperforms uncertainty sampling and passive learning in datasets ofdifferent natures and with AUR.</description><author>Zan-Kai Chong, Hiroyuki Ohsaki, Bok-Min Goi</author><pubDate>Sun, 03 Mar 2024 00:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01352v1</guid></item><item><title>Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition</title><link>http://arxiv.org/abs/2311.16119v3</link><description>Large Language Models (LLMs) are deployed in interactive contexts with directuser engagement, such as chatbots and writing assistants. These deployments arevulnerable to prompt injection and jailbreaking (collectively, prompt hacking),in which models are manipulated to ignore their original instructions andfollow potentially malicious ones. Although widely acknowledged as asignificant security threat, there is a dearth of large-scale resources andquantitative studies on prompt hacking. To address this lacuna, we launch aglobal prompt hacking competition, which allows for free-form human inputattacks. We elicit 600K+ adversarial prompts against three state-of-the-artLLMs. We describe the dataset, which empirically verifies that current LLMs canindeed be manipulated via prompt hacking. We also present a comprehensivetaxonomical ontology of the types of adversarial prompts.</description><author>Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber</author><pubDate>Sun, 03 Mar 2024 00:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16119v3</guid></item><item><title>SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization</title><link>http://arxiv.org/abs/2403.01348v1</link><description>Indoor localization is a critical task in many embedded applications, such asasset tracking, emergency response, and realtime navigation. In this article,we propose a novel fingerprintingbased framework for indoor localization calledSANGRIA that uses stacked autoencoder neural networks with gradient boostedtrees. Our approach is designed to overcome the device heterogeneity challengethat can create uncertainty in wireless signal measurements across embeddeddevices used for localization. We compare SANGRIA to several state-of-the-artframeworks and demonstrate 42.96% lower average localization error acrossdiverse indoor locales and heterogeneous devices.</description><author>Danish Gufran, Saideep Tiku, Sudeep Pasricha</author><pubDate>Sun, 03 Mar 2024 00:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01348v1</guid></item><item><title>Improve Cost Efficiency of Active Learning over Noisy Dataset</title><link>http://arxiv.org/abs/2403.01346v1</link><description>Active learning is a learning strategy whereby the machine learning algorithmactively identifies and labels data points to optimize its learning. Thisstrategy is particularly effective in domains where an abundance of unlabeleddata exists, but the cost of labeling these data points is prohibitivelyexpensive. In this paper, we consider cases of binary classification, whereacquiring a positive instance incurs a significantly higher cost compared tothat of negative instances. For example, in the financial industry, such as inmoney-lending businesses, a defaulted loan constitutes a positive event leadingto substantial financial loss. To address this issue, we propose a shiftednormal distribution sampling function that samples from a wider range thantypical uncertainty sampling. Our simulation underscores that our proposedsampling function limits both noisy and positive label selection, deliveringbetween 20% and 32% improved cost efficiency over different test datasets.</description><author>Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng</author><pubDate>Sat, 02 Mar 2024 23:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01346v1</guid></item><item><title>ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation</title><link>http://arxiv.org/abs/2403.01345v1</link><description>Accurate human shape recovery from a monocular RGB image is a challengingtask because humans come in different shapes and sizes and wear differentclothes. In this paper, we propose ShapeBoost, a new human shape recoveryframework that achieves pixel-level alignment even for rare body shapes andhigh accuracy for people wearing different types of clothes. Unlike previousapproaches that rely on the use of PCA-based shape coefficients, we adopt a newhuman shape parameterization that decomposes the human shape into bone lengthsand the mean width of each part slice. This part-based parameterizationtechnique achieves a balance between flexibility and validity using asemi-analytical shape reconstruction algorithm. Based on this newparameterization, a clothing-preserving data augmentation module is proposed togenerate realistic images with diverse body shapes and accurate annotations.Experimental results show that our method outperforms other state-of-the-artmethods in diverse body shape situations as well as in varied clothingsituations.</description><author>Siyuan Bian, Jiefeng Li, Jiasheng Tang, Cewu Lu</author><pubDate>Sat, 02 Mar 2024 23:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01345v1</guid></item><item><title>Mitigating the Bias in the Model for Continual Test-Time Adaptation</title><link>http://arxiv.org/abs/2403.01344v1</link><description>Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapta source pre-trained model to continually changing target domains. In the CTAsetting, a model does not know when the target domain changes, thus facing adrastic change in the distribution of streaming inputs during the test-time.The key challenge is to keep adapting the model to the continually changingtarget domains in an online manner. We find that a model shows highly biasedpredictions as it constantly adapts to the chaining distribution of the targetdata. It predicts certain classes more often than other classes, makinginaccurate over-confident predictions. This paper mitigates this issue toimprove performance in the CTA scenario. To alleviate the bias issue, we makeclass-wise exponential moving average target prototypes with reliable targetsamples and exploit them to cluster the target features class-wisely. Moreover,we aim to align the target distributions to the source distribution byanchoring the target feature to its corresponding source prototype. Withextensive experiments, our proposed method achieves noteworthy performance gainwhen applied on top of existing CTA methods without substantial adaptation timeoverhead.</description><author>Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak</author><pubDate>Sat, 02 Mar 2024 23:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01344v1</guid></item><item><title>LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems</title><link>http://arxiv.org/abs/2403.01342v1</link><description>In the rapidly evolving field of natural language processing, the translationof linguistic descriptions into mathematical formulation of optimizationproblems presents a formidable challenge, demanding intricate understanding andprocessing capabilities from Large Language Models (LLMs). This study comparesprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot andone-shot settings for this task. Our findings show GPT-4's superiorperformance, particularly in the one-shot scenario. A central part of thisresearch is the introduction of `LM4OPT,' a progressive fine-tuning frameworkfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.However, this research highlights a notable gap in the contextual understandingcapabilities of smaller models such as Llama-2-7b compared to largercounterparts, especially in processing lengthy and complex input contexts. Ourempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4surpasses the baseline performance established by previous research, achievingan F1-score of 0.63, solely based on the problem description in naturallanguage, and without relying on any additional named entity information.GPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. Thesefindings not only benchmark the current capabilities of LLMs in a novelapplication area but also lay the groundwork for future improvements inmathematical formulation of optimization problems from natural language input.</description><author>Tasnim Ahmed, Salimur Choudhury</author><pubDate>Sat, 02 Mar 2024 23:32:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01342v1</guid></item><item><title>Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization</title><link>http://arxiv.org/abs/2402.18284v2</link><description>Wide usage of ChatGPT has highlighted the potential of reinforcement learningfrom human feedback. However, its training pipeline relies on manual ranking, aresource-intensive process. To reduce labor costs, we propose a self-supervisedtext ranking approach for applying Proximal-Policy-Optimization to fine-tunelanguage models while eliminating the need for human annotators. Our methodbegins with probabilistic sampling to encourage a language model to generatediverse responses for each input. We then employ TextRank and ISODATAalgorithms to rank and cluster these responses based on their semantics.Subsequently, we construct a reward model to learn the rank and optimize ourgenerative policy. Our experimental results, conducted using two languagemodels on three tasks, demonstrate that the models trained by our methodconsiderably outperform baselines regarding BLEU, GLEU, and METEOR scores.Furthermore, our manual evaluation shows that our ranking results exhibit aremarkably high consistency with that of humans. This research significantlyreduces training costs of proximal policy-guided models and demonstrates thepotential for self-correction of language models.</description><author>Shuo Yang, Gjergji Kasneci</author><pubDate>Sat, 02 Mar 2024 23:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18284v2</guid></item><item><title>Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations</title><link>http://arxiv.org/abs/2403.01339v1</link><description>For any subgroup $G$ of the symmetric group $\mathcal{S}_n$ on $n$ symbols,we present results for the uniform $\mathcal{C}^k$ approximation of$G$-invariant functions by $G$-invariant polynomials. For the case of totallysymmetric functions ($G = \mathcal{S}_n$), we show that this gives rise to thesum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both theinner and outer functions can be chosen to be smooth, and moreover, the innerfunction can be chosen to be independent of the target function beingapproximated. In particular, we show that the embedding dimension required isindependent of the regularity of the target function, the accuracy of thedesired approximation, as well as $k$. Next, we show that a similar procedureallows us to obtain a uniform $\mathcal{C}^k$ approximation of antisymmetricfunctions as a sum of $K$ terms, where each term is a product of a smoothtotally symmetric function and a smooth antisymmetric homogeneous polynomial ofdegree at most $\binom{n}{2}$. We also provide upper and lower bounds on $K$and show that $K$ is independent of the regularity of the target function, thedesired approximation accuracy, and $k$.</description><author>Soumya Ganguly, Khoa Tran, Rahul Sarkar</author><pubDate>Sat, 02 Mar 2024 23:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01339v1</guid></item><item><title>Data Augmentation for Conversational AI</title><link>http://arxiv.org/abs/2309.04739v2</link><description>Advancements in conversational systems have revolutionized informationaccess, surpassing the limitations of single queries. However, developingdialogue systems requires a large amount of training data, which is a challengein low-resource domains and languages. Traditional data collection methods likecrowd-sourcing are labor-intensive and time-consuming, making them ineffectivein this context. Data augmentation (DA) is an affective approach to alleviatethe data scarcity problem in conversational systems. This tutorial provides acomprehensive and up-to-date overview of DA approaches in the context ofconversational systems. It highlights recent advances in conversationaugmentation, open domain and task-oriented conversation generation, anddifferent paradigms of evaluating these models. We also discuss currentchallenges and future directions in order to help researchers and practitionersto further advance the field in this area.</description><author>Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi</author><pubDate>Sat, 02 Mar 2024 23:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04739v2</guid></item><item><title>Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering</title><link>http://arxiv.org/abs/2301.12318v2</link><description>Most existing methods to detect backdoored machine learning (ML) models takeone of the two approaches: trigger inversion (aka. reverse engineer) and weightanalysis (aka. model diagnosis). In particular, the gradient-based triggerinversion is considered to be among the most effective backdoor detectiontechniques, as evidenced by the TrojAI competition, Trojan Detection Challengeand backdoorBench. However, little has been done to understand why thistechnique works so well and, more importantly, whether it raises the bar to thebackdoor attack. In this paper, we report the first attempt to answer thisquestion by analyzing the change rate of the backdoored model around itstrigger-carrying inputs. Our study shows that existing attacks tend to injectthe backdoor characterized by a low change rate around trigger-carrying inputs,which are easy to capture by gradient-based trigger inversion. In the meantime,we found that the low change rate is not necessary for a backdoor attack tosucceed: we design a new attack enhancement called \textit{Gradient Shaping}(GRASP), which follows the opposite direction of adversarial training to reducethe change rate of a backdoored model with regard to the trigger, withoutundermining its backdoor effect. Also, we provide a theoretic analysis toexplain the effectiveness of this new technique and the fundamental weakness ofgradient-based trigger inversion. Finally, we perform both theoretical andexperimental analysis, showing that the GRASP enhancement does not reduce theeffectiveness of the stealthy attacks against the backdoor detection methodsbased on weight analysis, as well as other backdoor mitigation methods withoutusing detection.</description><author>Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang</author><pubDate>Sat, 02 Mar 2024 22:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12318v2</guid></item><item><title>Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification</title><link>http://arxiv.org/abs/2301.11562v7</link><description>Variance in predictions across different trained models is a significant,under-explored source of error in fair binary classification. In practice, thevariance on some data examples is so large that decisions can be effectivelyarbitrary. To investigate this problem, we take an experimental approach andmake four overarching contributions: We: 1) Define a metric calledself-consistency, derived from variance, which we use as a proxy for measuringand reducing arbitrariness; 2) Develop an ensembling algorithm that abstainsfrom classification when a prediction would be arbitrary; 3) Conduct thelargest to-date empirical study of the role of variance (vis-a-visself-consistency and arbitrariness) in fair binary classification; and, 4)Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA)datasets easily usable for future research. Altogether, our experiments revealshocking insights about the reliability of conclusions on benchmark datasets.Most fair binary classification benchmarks are close-to-fair when taking intoaccount the amount of arbitrariness present in predictions -- before we eventry to apply any fairness interventions. This finding calls into question thepractical utility of common algorithmic fairness methods, and in turn suggeststhat we should reconsider how we choose to measure fairness in binaryclassification.</description><author>A. Feder Cooper, Katherine Lee, Madiha Zahrah Choksi, Solon Barocas, Christopher De Sa, James Grimmelmann, Jon Kleinberg, Siddhartha Sen, Baobao Zhang</author><pubDate>Sat, 02 Mar 2024 22:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11562v7</guid></item><item><title>Chaining thoughts and LLMs to learn DNA structural biophysics</title><link>http://arxiv.org/abs/2403.01332v1</link><description>The future development of an AI scientist, a tool that is capable ofintegrating a variety of experimental data and generating testable hypotheses,holds immense potential. So far, bespoke machine learning models have beencreated to specialize in singular scientific tasks, but otherwise lack theflexibility of a general purpose model. Here, we show that a general purposelarge language model, chatGPT 3.5-turbo, can be fine-tuned to learn thestructural biophysics of DNA. We find that both fine-tuning models to returnchain-of-thought responses and chaining together models fine-tuned for subtaskshave an enhanced ability to analyze and design DNA sequences and theirstructures.</description><author>Tyler D. Ross, Ashwin Gopinath</author><pubDate>Sat, 02 Mar 2024 22:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01332v1</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v2</link><description>Due to the difficulty of acquiring large-scale 3D human keypoint annotation,previous methods for 3D human pose estimation (HPE) have often relied on 2Dimage features and sequential 2D annotations. Furthermore, the training ofthese networks typically assumes the prediction of a human bounding box and theaccurate alignment of 3D point clouds with 2D images, making direct applicationin real-world scenarios challenging. In this paper, we present the 1stframework for end-to-end 3D human pose estimation, named LPFormer, which usesonly LiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: firstly, it identifies the human bounding box andextracts multi-level feature representations, and secondly, it utilizes atransformer-based network to predict human keypoints based on these features.Our method demonstrates that 3D HPE can be seamlessly integrated into a strongLiDAR perception network and benefit from the features extracted by thenetwork. Experimental results on the Waymo Open Dataset demonstrate thestate-of-the-art performance, and improvements even compared to previousmulti-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge, Hassan Foroosh</author><pubDate>Sat, 02 Mar 2024 22:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v2</guid></item><item><title>Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models</title><link>http://arxiv.org/abs/2403.01329v1</link><description>This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solverdistillation approach to improve sample efficiency of Diffusion and Flowmodels. BNS solvers are based on a family of non-stationary solvers thatprovably subsumes existing numerical ODE solvers and consequently demonstrateconsiderable improvement in sample approximation (PSNR) over these baselines.Compared to model distillation, BNS solvers benefit from a tiny parameter space($&lt;$200 parameters), fast optimization (two orders of magnitude faster),maintain diversity of samples, and in contrast to previous solver distillationapproaches nearly close the gap from standard distillation methods such asProgressive Distillation in the low-medium NFE regime. For example, BNS solverachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. Weexperimented with BNS solvers for conditional image generation, text-to-imagegeneration, and text-2-audio generation showing significant improvement insample approximation (PSNR) in all.</description><author>Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman</author><pubDate>Sat, 02 Mar 2024 22:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01329v1</guid></item><item><title>LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR Perception</title><link>http://arxiv.org/abs/2303.12194v2</link><description>There is a recent trend in the LiDAR perception field towards unifyingmultiple tasks in a single strong network with improved performance, as opposedto using separate networks for each task. In this paper, we introduce a newLiDAR multi-task learning paradigm based on the transformer. The proposedLiDARFormer utilizes cross-space global contextual feature information andexploits cross-task synergy to boost the performance of LiDAR perception tasksacross multiple large-scale datasets and benchmarks. Our noveltransformer-based framework includes a cross-space transformer module thatlearns attentive features between the 2D dense Bird's Eye View (BEV) and 3Dsparse voxel feature maps. Additionally, we propose a transformer decoder forthe segmentation task to dynamically adjust the learned features by leveragingthe categorical feature representations. Furthermore, we combine thesegmentation and detection features in a shared transformer decoder withcross-task attention layers to enhance and integrate the object-level andclass-level features. LiDARFormer is evaluated on the large-scale nuScenes andthe Waymo Open datasets for both 3D detection and semantic segmentation tasks,and it outperforms all previously published methods on both tasks. Notably,LiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for asingle model LiDAR-only method.</description><author>Zixiang Zhou, Dongqiangzi Ye, Weijia Chen, Yufei Xie, Yu Wang, Panqu Wang, Hassan Foroosh</author><pubDate>Sat, 02 Mar 2024 22:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12194v2</guid></item><item><title>DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions</title><link>http://arxiv.org/abs/2403.01326v1</link><description>Neural Architecture Search (NAS), aiming at automatically designing neuralarchitectures by machines, has been considered a key step toward automaticmachine learning. One notable NAS branch is the weight-sharing NAS, whichsignificantly improves search efficiency and allows NAS algorithms to run onordinary computers. Despite receiving high expectations, this category ofmethods suffers from low search effectiveness. By employing a generalizationboundedness tool, we demonstrate that the devil behind this drawback is theuntrustworthy architecture rating with the oversized search space of thepossible architectures. Addressing this problem, we modularize a large searchspace into blocks with small search spaces and develop a family of models withthe distilling neural architecture (DNA) techniques. These proposed models,namely a DNA family, are capable of resolving multiple dilemmas of theweight-sharing NAS, such as scalability, efficiency, and multi-modalcompatibility. Our proposed DNA models can rate all architecture candidates, asopposed to previous works that can only access a sub- search space usingheuristic algorithms. Moreover, under a certain computational complexityconstraint, our method can seek architectures with different depths and widths.Extensive experimental evaluations show that our models achievestate-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobileconvolutional network and a small vision transformer, respectively.Additionally, we provide in-depth empirical analysis and insights into neuralarchitecture ratings. Codes available: \url{https://github.com/changlin31/DNA}.</description><author>Guangrun Wang, Changlin Li, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian, Xiaodan Liang, Xiaojun Chang, Liang Lin</author><pubDate>Sat, 02 Mar 2024 22:16:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01326v1</guid></item><item><title>Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming</title><link>http://arxiv.org/abs/2305.18436v3</link><description>$K$-means clustering is a widely used machine learning method for identifyingpatterns in large datasets. Semidefinite programming (SDP) relaxations haverecently been proposed for solving the $K$-means optimization problem thatenjoy strong statistical optimality guarantees, but the prohibitive cost ofimplementing an SDP solver renders these guarantees inaccessible to practicaldatasets. By contrast, nonnegative matrix factorization (NMF) is a simpleclustering algorithm that is widely used by machine learning practitioners, butwithout a solid statistical underpinning nor rigorous guarantees. In thispaper, we describe an NMF-like algorithm that works by solving a nonnegativelow-rank restriction of the SDP relaxed $K$-means formulation using a nonconvexBurer--Monteiro factorization approach. The resulting algorithm is just assimple and scalable as state-of-the-art NMF algorithms, while also enjoying thesame strong statistical optimality guarantees as the SDP. In our experiments,we observe that our algorithm achieves substantially smaller mis-clusteringerrors compared to the existing state-of-the-art.</description><author>Yubo Zhuang, Xiaohui Chen, Yun Yang, Richard Y. Zhang</author><pubDate>Sat, 02 Mar 2024 22:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18436v3</guid></item><item><title>NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning</title><link>http://arxiv.org/abs/2403.01325v1</link><description>Neural Radiance Fields (NeRF) have garnered remarkable success in novel viewsynthesis. Nonetheless, the task of generating high-quality images for novelviews persists as a critical challenge. While the existing efforts haveexhibited commendable progress, capturing intricate details, enhancingtextures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metricswarrant further focused attention and advancement. In this work, we proposeNeRF-VPT, an innovative method for novel view synthesis to address thesechallenges. Our proposed NeRF-VPT employs a cascading view prompt tuningparadigm, wherein RGB information gained from preceding rendering outcomesserves as instructive visual prompts for subsequent rendering stages, with theaspiration that the prior knowledge embedded in the prompts can facilitate thegradual enhancement of rendered image quality. NeRF-VPT only requires samplingRGB data from previous stage renderings as priors at each training stage,without relying on extra guidance or complex techniques. Thus, our NeRF-VPT isplug-and-play and can be readily integrated into existing methods. Byconducting comparative analyses of our NeRF-VPT against several NeRF-basedapproaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,Real Forward-Facing, Replica dataset, and a user-captured dataset, wesubstantiate that our NeRF-VPT significantly elevates baseline performance andproficiently generates more high-quality novel view images than all thecompared state-of-the-art methods. Furthermore, the cascading learning ofNeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting ina significant enhancement of accuracy for sparse-view novel view synthesis. Thesource code and dataset are available at\url{https://github.com/Freedomcls/NeRF-VPT}.</description><author>Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr</author><pubDate>Sat, 02 Mar 2024 22:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01325v1</guid></item><item><title>Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond</title><link>http://arxiv.org/abs/2209.06177v4</link><description>Homophily is a graph property describing the tendency of edges to connectsimilar nodes; the opposite is called heterophily. It is often believed thatheterophilous graphs are challenging for standard message-passing graph neuralnetworks (GNNs), and much effort has been put into developing efficient methodsfor this setting. However, there is no universally agreed-upon measure ofhomophily in the literature. In this work, we show that commonly used homophilymeasures have critical drawbacks preventing the comparison of homophily levelsacross different datasets. For this, we formalize desirable properties for aproper homophily measure and verify which measures satisfy which properties. Inparticular, we show that a measure that we call adjusted homophily satisfiesmore desirable properties than other popular homophily measures while beingrarely used in graph machine learning literature. Then, we go beyond thehomophily-heterophily dichotomy and propose a new characteristic that allowsone to further distinguish different sorts of heterophily. The proposed labelinformativeness (LI) characterizes how much information a neighbor's labelprovides about a node's label. We prove that this measure satisfies importantdesirable properties. We also observe empirically that LI better agrees withGNN performance compared to homophily measures, which confirms that it is auseful characteristic of the graph structure.</description><author>Oleg Platonov, Denis Kuznedelev, Artem Babenko, Liudmila Prokhorenkova</author><pubDate>Sat, 02 Mar 2024 22:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06177v4</guid></item><item><title>Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities</title><link>http://arxiv.org/abs/2312.01050v2</link><description>Nowadays, the significance of monitoring stress levels and recognizing earlysigns of mental illness cannot be overstated. Automatic stress detection intext can proactively help manage stress and protect mental well-being. Intoday's digital era, social media platforms reflect the psychologicalwell-being and stress levels within various communities. This study focuses ondetecting and analyzing stress-related posts in Reddit academic communities.Due to online education and remote work, these communities have become centralfor academic discussions and support. We classify text as stressed or not usingnatural language processing and machine learning classifiers, with Dreaddit asour training dataset, which contains labeled data from Reddit. Next, we collectand analyze posts from various academic subreddits. We identified that the mosteffective individual feature for stress detection is the Bag of Words, pairedwith the Logistic Regression classifier, achieving a 77.78% accuracy rate andan F1 score of 0.79 on the DReaddit dataset. This combination also performsbest in stress detection on human-annotated datasets, with a 72% accuracy rate.Our key findings reveal that posts and comments in professors Redditcommunities are the most stressful, compared to other academic levels,including bachelor, graduate, and Ph.D. students. This research contributes toour understanding of the stress levels within academic communities. It can helpacademic institutions and online communities develop measures and interventionsto address this issue effectively.</description><author>Nazzere Oryngozha, Pakizar Shamoi, Ayan Igali</author><pubDate>Sat, 02 Mar 2024 21:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01050v2</guid></item><item><title>High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media</title><link>http://arxiv.org/abs/2403.01318v1</link><description>Motivated by the empirical power law of the distributions of credits (e.g.,the number of "likes") of viral posts in social media, we introduce thehigh-dimensional tail index regression and methods of estimation and inferencefor its parameters. We propose a regularized estimator, establish itsconsistency, and derive its convergence rate. To conduct inference, we proposeto debias the regularized estimate, and establish the asymptotic normality ofthe debiased estimator. Simulation studies support our theory. These methodsare applied to text analyses of viral posts in X (formerly Twitter) concerningLGBTQ+.</description><author>Yuya Sasaki, Jing Tao, Yulong Wang</author><pubDate>Sat, 02 Mar 2024 21:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01318v1</guid></item><item><title>Discovering Latent Knowledge in Language Models Without Supervision</title><link>http://arxiv.org/abs/2212.03827v2</link><description>Existing techniques for training language models can be misaligned with thetruth: if we train models with imitation learning, they may reproduce errorsthat humans make; if we train them to generate text that humans rate highly,they may output errors that human evaluators can't detect. We proposecircumventing this issue by directly finding latent knowledge inside theinternal activations of a language model in a purely unsupervised way.Specifically, we introduce a method for accurately answering yes-no questionsgiven only unlabeled model activations. It works by finding a direction inactivation space that satisfies logical consistency properties, such as that astatement and its negation have opposite truth values. We show that despiteusing no supervision and no model outputs, our method can recover diverseknowledge represented in large language models: across 6 models and 10question-answering datasets, it outperforms zero-shot accuracy by 4\% onaverage. We also find that it cuts prompt sensitivity in half and continues tomaintain high accuracy even when models are prompted to generate incorrectanswers. Our results provide an initial step toward discovering what languagemodels know, distinct from what they say, even when we don't have access toexplicit ground truth labels.</description><author>Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt</author><pubDate>Sat, 02 Mar 2024 21:33:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03827v2</guid></item><item><title>Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits</title><link>http://arxiv.org/abs/2403.01317v1</link><description>While graph neural networks (GNNs) have gained popularity for learningcircuit representations in various electronic design automation (EDA) tasks,they face challenges in scalability when applied to large graphs and exhibitlimited generalizability to new designs. These limitations make them lesspractical for addressing large-scale, complex circuit problems. In this work wepropose HOGA, a novel attention-based model for learning circuitrepresentations in a scalable and generalizable manner. HOGA first computeshop-wise features per node prior to model training. Subsequently, the hop-wisefeatures are solely used to produce node representations through a gatedself-attention module, which adaptively learns important features amongdifferent hops without involving the graph topology. As a result, HOGA isadaptive to various structures across different circuits and can be efficientlytrained in a distributed manner. To demonstrate the efficacy of HOGA, weconsider two representative EDA tasks: quality of results (QoR) prediction andfunctional reasoning. Our experimental results indicate that (1) HOGA reducesestimation error over conventional GNNs by 46.76% for predicting QoR afterlogic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs foridentifying functional blocks on unseen gate-level netlists after complextechnology mapping; (3) The training time for HOGA almost linearly decreaseswith an increase in computing resources.</description><author>Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev Jain, Zhiru Zhang</author><pubDate>Sat, 02 Mar 2024 21:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01317v1</guid></item><item><title>TUMTraf V2X Cooperative Perception Dataset</title><link>http://arxiv.org/abs/2403.01316v1</link><description>Cooperative perception offers several benefits for enhancing the capabilitiesof autonomous vehicles and improving road safety. Using roadside sensors inaddition to onboard sensors increases reliability and extends the sensor range.External sensors offer higher situational awareness for automated vehicles andprevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusionmodel, and TUMTraf-V2X, a perception dataset, for the cooperative 3D objectdetection and tracking task. Our dataset contains 2,000 labeled point cloudsand 5,000 labeled images from five roadside and four onboard sensors. Itincludes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeledeight categories and covered occlusion scenarios with challenging drivingmaneuvers, like traffic violations, near-miss events, overtaking, and U-turns.Through multiple experiments, we show that our CoopDet3D camera-LiDAR fusionmodel achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDARfusion model. Finally, we make our dataset, model, labeling tool, and dev-kitpublicly available on our website:https://tum-traffic-dataset.github.io/tumtraf-v2x.</description><author>Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, Alois C. Knoll</author><pubDate>Sat, 02 Mar 2024 21:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01316v1</guid></item><item><title>Near-optimal Per-Action Regret Bounds for Sleeping Bandits</title><link>http://arxiv.org/abs/2403.01315v1</link><description>We derive near-optimal per-action regret bounds for sleeping bandits, inwhich both the sets of available arms and their losses in every round arechosen by an adversary. In a setting with $K$ total arms and at most $A$available arms in each round over $T$ rounds, the best known upper bound is$O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleepingregrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upperbound contains an extra multiplicative factor of $K\ln{K}$. We address this gapby directly minimizing the per-action regret using generalized versions ofEXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimalbounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend ourresults to the setting of bandits with advice from sleeping experts,generalizing EXP4 along the way. This leads to new proofs for a number ofexisting adaptive and tracking regret bounds for standard non-sleeping bandits.Extending our results to the bandit version of experts that report theirconfidences leads to new bounds for the confidence regret that dependsprimarily on the sum of experts' confidences. We prove a lower bound, showingthat for any minimax optimal algorithms, there exists an action whose regret issublinear in $T$ but linear in the number of its active rounds.</description><author>Quan Nguyen, Nishant A. Mehta</author><pubDate>Sat, 02 Mar 2024 21:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01315v1</guid></item><item><title>A critical look at the evaluation of GNNs under heterophily: Are we really making progress?</title><link>http://arxiv.org/abs/2302.11640v2</link><description>Node classification is a classical graph machine learning task on which GraphNeural Networks (GNNs) have recently achieved strong results. However, it isoften believed that standard GNNs only work well for homophilous graphs, i.e.,graphs where edges tend to connect nodes of the same class. Graphs without thisproperty are called heterophilous, and it is typically assumed that specializedmethods are required to achieve strong performance on such graphs. In thiswork, we challenge this assumption. First, we show that the standard datasetsused for evaluating heterophily-specific models have serious drawbacks, makingresults obtained by using them unreliable. The most significant of thesedrawbacks is the presence of a large number of duplicate nodes in the datasetsSquirrel and Chameleon, which leads to train-test data leakage. We show thatremoving duplicate nodes strongly affects GNN performance on these datasets.Then, we propose a set of heterophilous graphs of varying properties that webelieve can serve as a better benchmark for evaluating the performance of GNNsunder heterophily. We show that standard GNNs achieve strong results on theseheterophilous graphs, almost always outperforming specialized models. Ourdatasets and the code for reproducing our experiments are available athttps://github.com/yandex-research/heterophilous-graphs</description><author>Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, Liudmila Prokhorenkova</author><pubDate>Sat, 02 Mar 2024 21:17:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11640v2</guid></item><item><title>Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System</title><link>http://arxiv.org/abs/2403.01310v1</link><description>The nutritional quality of diets has significantly deteriorated over the pasttwo to three decades, a decline often underestimated by the people. Thisdeterioration, coupled with a hectic lifestyle, has contributed to escalatinghealth concerns. Recognizing this issue, researchers at Harvard have advocatedfor a balanced nutritional plate model to promote health. Inspired by thisresearch, our paper introduces an innovative Image-Based Dietary Assessmentsystem aimed at evaluating the healthiness of meals through image analysis. Oursystem employs advanced image segmentation and classification techniques toanalyze food items on a plate, assess their proportions, and calculate mealadherence to Harvard's healthy eating recommendations. This approach leveragesmachine learning and nutritional science to empower individuals with actionableinsights for healthier eating choices. Our four-step framework involvessegmenting the image, classifying the items, conducting a nutritionalassessment based on the Harvard Healthy Eating Plate research, and offeringtailored recommendations. The prototype system has shown promising results inpromoting healthier eating habits by providing an accessible, evidence-basedtool for dietary assessment.</description><author>Assylzhan Izbassar, Pakizar Shamoi</author><pubDate>Sat, 02 Mar 2024 21:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01310v1</guid></item><item><title>PBP: Path-based Trajectory Prediction for Autonomous Driving</title><link>http://arxiv.org/abs/2309.03750v2</link><description>Trajectory prediction plays a crucial role in the autonomous driving stack byenabling autonomous vehicles to anticipate the motion of surrounding agents.Goal-based prediction models have gained traction in recent years foraddressing the multimodal nature of future trajectories. Goal-based predictionmodels simplify multimodal prediction by first predicting 2D goal locations ofagents and then predicting trajectories conditioned on each goal. However, asingle 2D goal location serves as a weak inductive bias for predicting thewhole trajectory, often leading to poor map compliance, i.e., part of thetrajectory going off-road or breaking traffic rules. In this paper, we improveupon goal-based prediction by proposing the Path-based prediction (PBP)approach. PBP predicts a discrete probability distribution over reference pathsin the HD map using the path features and predicts trajectories in thepath-relative Frenet frame. We applied the PBP trajectory decoder on top of theHiVT scene encoder and report results on the Argoverse dataset. Our experimentsshow that PBP achieves competitive performance on the standard trajectoryprediction metrics, while significantly outperforming state-of-the-artbaselines in terms of map compliance.</description><author>Sepideh Afshar, Nachiket Deo, Akshay Bhagat, Titas Chakraborty, Yunming Shao, Balarama Raju Buddharaju, Adwait Deshpande, Henggang Cui</author><pubDate>Sat, 02 Mar 2024 20:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03750v2</guid></item><item><title>VNLP: Turkish NLP Package</title><link>http://arxiv.org/abs/2403.01309v1</link><description>In this work, we present VNLP: the first dedicated, complete, open-source,well-documented, lightweight, production-ready, state-of-the-art NaturalLanguage Processing (NLP) package for the Turkish language. It contains a widevariety of tools, ranging from the simplest tasks, such as sentence splittingand text normalization, to the more advanced ones, such as text and tokenclassification models. Its token classification models are based on "ContextModel", a novel architecture that is both an encoder and an auto-regressivemodel. NLP tasks solved by VNLP models include but are not limited to SentimentAnalysis, Named Entity Recognition, Morphological Analysis \&amp; Disambiguationand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddingsand corresponding SentencePiece Unigram tokenizers. VNLP has an open-sourceGitHub repository, ReadtheDocs documentation, PyPi package for convenientinstallation, Python and command-line API and a demo page to test all thefunctionality. Consequently, our main contribution is a complete, compact,easy-to-install and easy-to-use NLP package for Turkish.</description><author>Meliksah Turker, Mehmet Erdi Ari, Aydin Han</author><pubDate>Sat, 02 Mar 2024 20:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01309v1</guid></item><item><title>VBART: The Turkish LLM</title><link>http://arxiv.org/abs/2403.01308v1</link><description>We present VBART, the first Turkish sequence-to-sequence Large LanguageModels (LLMs) pre-trained on a large corpus from scratch. VBART are compactLLMs based on good ideas leveraged from BART and mBART models and come in twosizes, Large and XLarge. Fine-tuned VBART models surpass the priorstate-of-the-art results in abstractive text summarization, title generation,text paraphrasing, question answering and question generation tasks. They allowfine-tuning for future text generation tasks and datasets, carving a new pathfor Turkish Natural Language Processing (NLP) research. Our work shows thathaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,improving existing results and providing efficient models for training andinference. Moreover, we show that our monolingual tokenizer is 7x moreefficient than OpenAI's multilingual tokenizer. Last but not least, weintroduce a method to enlarge an existing pre-trained LLM and question therelevancy of Chinchilla Scaling Law to sequence-to-sequence masked languagemodels. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB arepublicly available at huggingface.co/vngrs-ai.</description><author>Meliksah Turker, Mehmet Erdi Ari, Aydin Han</author><pubDate>Sat, 02 Mar 2024 20:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01308v1</guid></item><item><title>ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation</title><link>http://arxiv.org/abs/2403.01306v1</link><description>Web-scale training on paired text-image data is becoming increasingly centralto multimodal learning, but is challenged by the highly noisy nature ofdatasets in the wild. Standard data filtering approaches succeed in removingmismatched text-image pairs, but permit semantically related but highlyabstract or subjective text. These approaches lack the fine-grained ability toisolate the most concrete samples that provide the strongest signal forlearning in a noisy dataset. In this work, we propose a new metric, imagecaption concreteness, that evaluates caption text without an image reference tomeasure its concreteness and relevancy for use in multimodal learning. Ourapproach leverages strong foundation models for measuring visual-semanticinformation loss in multimodal representations. We demonstrate that thisstrongly correlates with human evaluation of concreteness in both single-wordand sentence-level texts. Moreover, we show that curation using ICC complementsexisting approaches: It succeeds in selecting the highest quality samples frommultimodal web-scale datasets to allow for efficient training inresource-constrained settings.</description><author>Moran Yanuka, Morris Alper, Hadar Averbuch-Elor, Raja Giryes</author><pubDate>Sat, 02 Mar 2024 20:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01306v1</guid></item><item><title>Improving the Validity of Automatically Generated Feedback via Reinforcement Learning</title><link>http://arxiv.org/abs/2403.01304v1</link><description>Automatically generating feedback via large language models (LLMs) inintelligent tutoring systems and online learning platforms has the potential toimprove the learning outcomes of many students. However, both feedbackgeneration and evaluation are challenging: feedback content has to be validespecially in subjects like math, which requires models to understand theproblem, the solution, and where the student's error lies. Feedback also has tobe pedagogically valid to reflect effective tutoring strategies, such asexplaining possible misconceptions and encouraging the student, among otherdesirable features. In this work, we address both problems of automaticallygenerating and evaluating feedback while considering both correctness andalignment. First, we propose a rubric for evaluating math feedback and showthat GPT-4 is able to effectively use it to annotate human-written andLLM-generated feedback. Second, we propose a framework for feedback generationthat optimizes both correctness and alignment using reinforcement learning(RL). Specifically, we use GPT-4's annotations to create preferences overfeedback pairs in an augmented dataset for training via direct preferenceoptimization (DPO). We show that our methods significantly increase thecorrectness and alignment of generated feedback with Llama 2, an open-sourceLLM, qualitatively analyze our generation and evaluation systems using casestudies, and outline several areas for future work.</description><author>Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan</author><pubDate>Sat, 02 Mar 2024 20:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01304v1</guid></item><item><title>Supplier Recommendation in Online Procurement</title><link>http://arxiv.org/abs/2403.01301v1</link><description>Supply chain optimization is key to a healthy and profitable business. Manycompanies use online procurement systems to agree contracts with suppliers. Itis vital that the most competitive suppliers are invited to bid for suchcontracts. In this work, we propose a recommender system to assist withsupplier discovery in road freight online procurement. Our system is able toprovide personalized supplier recommendations, taking into account customerneeds and preferences. This is a novel application of recommender systems,calling for design choices that fit the unique requirements of onlineprocurement. Our preliminary results, using real-world data, are promising.</description><author>Victor Coscrato, Derek Bridge</author><pubDate>Sat, 02 Mar 2024 19:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01301v1</guid></item><item><title>Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection</title><link>http://arxiv.org/abs/2403.01300v1</link><description>RGBT multispectral pedestrian detection has emerged as a promising solutionfor safety-critical applications that require day/night operations. However,the modality bias problem remains unsolved as multispectral pedestriandetectors learn the statistical bias in datasets. Specifically, datasets inmultispectral pedestrian detection mainly distribute between ROTO (day) andRXTO (night) data; the majority of the pedestrian labels statistically co-occurwith their thermal features. As a result, multispectral pedestrian detectorsshow poor generalization ability on examples beyond this statisticalcorrelation, such as ROTX data. To address this problem, we propose a novelCausal Mode Multiplexer (CMM) framework that effectively learns the causalitiesbetween multispectral inputs and predictions. Moreover, we construct a newdataset (ROTX-MP) to evaluate modality bias in multispectral pedestriandetection. ROTX-MP mainly includes ROTX examples not presented in previousdatasets. Extensive experiments demonstrate that our proposed CMM frameworkgeneralizes well on existing datasets (KAIST, CVC-14, FLIR) and the newROTX-MP. We will release our new dataset to the public for future research.</description><author>Taeheon Kim, Sebin Shin, Youngjoon Yu, Hak Gu Kim, Yong Man Ro</author><pubDate>Sat, 02 Mar 2024 19:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01300v1</guid></item><item><title>A Photonic Physically Unclonable Function's Resilience to Multiple-Valued Machine Learning Attacks</title><link>http://arxiv.org/abs/2403.01299v1</link><description>Physically unclonable functions (PUFs) identify integrated circuits usingnonlinearly-related challenge-response pairs (CRPs). Ideally, the relationshipbetween challenges and corresponding responses is unpredictable, even if asubset of CRPs is known. Previous work developed a photonic PUF offeringimproved security compared to non-optical counterparts. Here, we investigatethis PUF's susceptibility to Multiple-Valued-Logic-based machine learningattacks. We find that approximately 1,000 CRPs are necessary to train modelsthat predict response bits better than random chance. Given the significantchallenge of acquiring a vast number of CRPs from a photonic PUF, our resultsdemonstrate photonic PUF resilience against such attacks.</description><author>Jessie M. Henderson, Elena R. Henderson, Clayton A. Harper, Hiva Shahoei, William V. Oxford, Eric C. Larson, Duncan L. MacFarlane, Mitchell A. Thornton</author><pubDate>Sat, 02 Mar 2024 19:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01299v1</guid></item><item><title>Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review</title><link>http://arxiv.org/abs/2402.18590v2</link><description>The paper underscores the significance of Large Language Models (LLMs) inreshaping recommender systems, attributing their value to unique reasoningabilities absent in traditional recommenders. Unlike conventional systemslacking direct user interaction data, LLMs exhibit exceptional proficiency inrecommending items, showcasing their adeptness in comprehending intricacies oflanguage. This marks a fundamental paradigm shift in the realm ofrecommendations. Amidst the dynamic research landscape, researchers activelyharness the language comprehension and generation capabilities of LLMs toredefine the foundations of recommendation tasks. The investigation thoroughlyexplores the inherent strengths of LLMs within recommendation frameworks,encompassing nuanced contextual comprehension, seamless transitions acrossdiverse domains, adoption of unified approaches, holistic learning strategiesleveraging shared data reservoirs, transparent decision-making, and iterativeimprovements. Despite their transformative potential, challenges persist,including sensitivity to input prompts, occasional misinterpretations, andunforeseen recommendations, necessitating continuous refinement and evolutionin LLM-driven recommender systems.</description><author>Arpita Vats, Vinija Jain, Rahul Raja, Aman Chadha</author><pubDate>Sat, 02 Mar 2024 19:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18590v2</guid></item><item><title>A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches</title><link>http://arxiv.org/abs/2303.07196v2</link><description>Vector-based word representations help countless Natural Language Processing(NLP) tasks capture the language's semantic and syntactic regularities. In thispaper, we present the characteristics of existing word embedding approaches andanalyze them with regard to many classification tasks. We categorize themethods into two main groups - Traditional approaches mostly use matrixfactorization to produce word representations, and they are not able to capturethe semantic and syntactic regularities of the language very well. On the otherhand, Neural-network-based approaches can capture sophisticated regularities ofthe language and preserve the word relationships in the generated wordrepresentations. We report experimental results on multiple classificationtasks and highlight the scenarios where one approach performs better than therest.</description><author>Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil</author><pubDate>Sat, 02 Mar 2024 19:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07196v2</guid></item><item><title>Greed is All You Need: An Evaluation of Tokenizer Inference Methods</title><link>http://arxiv.org/abs/2403.01289v1</link><description>While subword tokenizers such as BPE and WordPiece are typically used tobuild vocabularies for NLP models, the method of decoding text into a sequenceof tokens from these vocabularies is often left unspecified, or ill-suited tothe method in which they were constructed. We provide a controlled analysis ofseven tokenizer inference methods across four different algorithms and threevocabulary sizes, performed on a novel intrinsic evaluation suite we curatedfor English, combining measures rooted in morphology, cognition, andinformation theory. We show that for the most commonly used tokenizers, greedyinference performs surprisingly well; and that SaGe, a recently-introducedcontextually-informed tokenizer, outperforms all others on morphologicalalignment.</description><author>Omri Uzan, Craig W. Schmidt, Chris Tanner, Yuval Pinter</author><pubDate>Sat, 02 Mar 2024 19:01:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01289v1</guid></item><item><title>Summary Paper: Use Case on Building Collaborative Safe Autonomous Systems-A Robotdog for Guiding Visually Impaired People</title><link>http://arxiv.org/abs/2403.01286v1</link><description>This is a summary paper of a use case of a Robotdog dedicated to guidevisually impaired people in complex environment like a smart intersection. Insuch scenarios, the Robotdog has to autonomously decide whether it is safe tocross the intersection or not in order to further guide the human. We leveragedata sharing and collaboration between the Robotdog and other autonomoussystems operating in the same environment. We propose a system architecture forautonomous systems through a separation of a collaborative decision layer, toenable collective decision making processes, where data about the environment,relevant to the Robotdog decision, together with evidences for trustworthinessabout other systems and the environment are shared.</description><author>Aman Malhotra, Selma Saidi</author><pubDate>Sat, 02 Mar 2024 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01286v1</guid></item><item><title>Neural Radiance Fields in Medical Imaging: Challenges and Next Steps</title><link>http://arxiv.org/abs/2402.17797v2</link><description>Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,offer great potential to revolutionize medical imaging by synthesizingthree-dimensional representations from the projected two-dimensional imagedata. However, they face unique challenges when applied to medicalapplications. This paper presents a comprehensive examination of applicationsof NeRFs in medical imaging, highlighting four imminent challenges, includingfundamental imaging principles, inner structure requirement, object boundarydefinition, and color density significance. We discuss current methods ondifferent organs and discuss related limitations. We also review severaldatasets and evaluation metrics and propose several promising directions forfuture research.</description><author>Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li</author><pubDate>Sat, 02 Mar 2024 18:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17797v2</guid></item><item><title>Fast Low-parameter Video Activity Localization in Collaborative Learning Environments</title><link>http://arxiv.org/abs/2403.01281v1</link><description>Research on video activity detection has primarily focused on identifyingwell-defined human activities in short video segments. The majority of theresearch on video activity recognition is focused on the development of largeparameter systems that require training on large video datasets. This paperdevelops a low-parameter, modular system with rapid inferencing capabilitiesthat can be trained entirely on limited datasets without requiring transferlearning from large-parameter systems. The system can accurately detect andassociate specific activities with the students who perform the activities inreal-life classroom videos. Additionally, the paper develops an interactiveweb-based application to visualize human activity maps over long real-lifeclassroom videos.</description><author>Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon Pattichis, Marios S. Patticis</author><pubDate>Sat, 02 Mar 2024 18:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01281v1</guid></item><item><title>Prediction of Cellular Identities from Trajectory and Cell Fate Information</title><link>http://arxiv.org/abs/2401.06182v2</link><description>Determining cell identities in imaging sequences is an important yetchallenging task. The conventional method for cell identification is via celltracking, which is complex and can be time-consuming. In this study, we proposean innovative approach to cell identification during early $\textit{C.elegans}$ embryogenesis using machine learning. Cell identification during$\textit{C. elegans}$ embryogenesis would provide insights into neuraldevelopment with implications for higher organisms including humans. Weemployed random forest, MLP, and LSTM models, and tested cell classificationaccuracy on 3D time-lapse confocal datasets spanning the first 4 hours ofembryogenesis. By leveraging a small number of spatial-temporal features ofindividual cells, including cell trajectory and cell fate information, ourmodels achieve an accuracy of over 91%, even with limited data. We alsodetermine the most important feature contributions and can interpret thesefeatures in the context of biological knowledge. Our research demonstrates thesuccess of predicting cell identities in time-lapse imaging sequences directlyfrom simple spatio-temporal features.</description><author>Baiyang Dai, Jiamin Yang, Hari Shroff, Patrick La Riviere</author><pubDate>Sat, 02 Mar 2024 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06182v2</guid></item><item><title>Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection</title><link>http://arxiv.org/abs/2304.14614v3</link><description>Multi-sensor fusion (MSF) is widely used in autonomous vehicles (AVs) forperception, particularly for 3D object detection with camera and LiDAR sensors.The purpose of fusion is to capitalize on the advantages of each modality whileminimizing its weaknesses. Advanced deep neural network (DNN)-based fusiontechniques have demonstrated the exceptional and industry-leading performance.Due to the redundant information in multiple modalities, MSF is also recognizedas a general defence strategy against adversarial attacks. In this paper, weattack fusion models from the camera modality that is considered to be oflesser importance in fusion but is more affordable for attackers. We argue thatthe weakest link of fusion models depends on their most vulnerable modality,and propose an attack framework that targets advanced camera-LiDAR fusion-based3D object detection models through camera-only adversarial attacks. Ourapproach employs a two-stage optimization-based strategy that first thoroughlyevaluates vulnerable image areas under adversarial attacks, and then appliesdedicated attack strategies for different fusion models to generate deployablepatches. The evaluations with six advanced camera-LiDAR fusion models and onecamera-only model indicate that our attacks successfully compromise all ofthem. Our approach can either decrease the mean average precision (mAP) ofdetection performance from 0.824 to 0.353, or degrade the detection score of atarget object from 0.728 to 0.156, demonstrating the efficacy of our proposedattack framework. Code is available.</description><author>Zhiyuan Cheng, Hongjun Choi, James Liang, Shiwei Feng, Guanhong Tao, Dongfang Liu, Michael Zuzak, Xiangyu Zhang</author><pubDate>Sat, 02 Mar 2024 17:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14614v3</guid></item><item><title>Optimal Integrated Task and Path Planning and Its Application to Multi-Robot Pickup and Delivery</title><link>http://arxiv.org/abs/2403.01277v1</link><description>We propose a generic multi-robot planning mechanism that combines an optimaltask planner and an optimal path planner to provide a scalable solution forcomplex multi-robot planning problems. The Integrated planner, through theinteraction of the task planner and the path planner, produces optimalcollision-free trajectories for the robots. We illustrate our general algorithmon an object pick-and-drop planning problem in a warehouse scenario where agroup of robots is entrusted with moving objects from one location to anotherin the workspace. We solve the task planning problem by reducing it into anSMT-solving problem and employing the highly advanced SMT solver Z3 to solveit. To generate collision-free movement of the robots, we extend thestate-of-the-art algorithm Conflict Based Search with Precedence Constraintswith several domain-specific constraints. We evaluate our integrated task andpath planner extensively on various instances of the object pick-and-dropplanning problem and compare its performance with a state-of-the-artmulti-robot classical planner. Experimental results demonstrate that ourplanning mechanism can deal with complex planning problems and outperforms astate-of-the-art classical planner both in terms of computation time and thequality of the generated plan.</description><author>Aman Aryan, Manan Modi, Indranil Saha, Rupak Majumdar, Swarup Mohalik</author><pubDate>Sat, 02 Mar 2024 17:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01277v1</guid></item><item><title>NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention</title><link>http://arxiv.org/abs/2403.01273v1</link><description>Large language model inference on Central Processing Units (CPU) ischallenging due to the vast quantities of expensive Multiply-Add (MAD) matrixoperations in the attention computations. In this paper, we argue that there isa rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,which allow for ultra-low-latency lookups in batch. We leverage this uniquecapability of CPUs to propose NoMAD-Attention, an efficient attention algorithmthat replaces MAD operations with in-register lookups. Through hardware-awarealgorithmic designs, NoMAD-Attention achieves the computation of attentionscores using repeated fast accesses to SIMD registers despite their highlylimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-basedLLMs without model finetuning. Empirical evaluations demonstrate thatNoMAD-Attention maintains the quality of the original LLMs well, and speeds upthe 4-bit quantized LLaMA-7B-based model by up to 2$\times$ at 16k contextlength. Our results are reproducible athttps://github.com/tonyzhang617/nomad-dist.</description><author>Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali Shrivastava</author><pubDate>Sat, 02 Mar 2024 17:29:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01273v1</guid></item><item><title>Can a Confident Prior Replace a Cold Posterior?</title><link>http://arxiv.org/abs/2403.01272v1</link><description>Benchmark datasets used for image classification tend to have very low levelsof label noise. When Bayesian neural networks are trained on these datasets,they often underfit, misrepresenting the aleatoric uncertainty of the data. Acommon solution is to cool the posterior, which improves fit to the trainingdata but is challenging to interpret from a Bayesian perspective. We explorewhether posterior tempering can be replaced by a confidence-inducing priordistribution. First, we introduce a "DirClip" prior that is practical to sampleand nearly matches the performance of a cold posterior. Second, we introduce a"confidence prior" that directly approximates a cold likelihood in the limit ofdecreasing temperature but cannot be easily sampled. Lastly, we provide severalgeneral insights into confidence-inducing priors, such as when they mightdiverge and how fine-tuning can mitigate numerical instability.</description><author>Martin Marek, Brooks Paige, Pavel Izmailov</author><pubDate>Sat, 02 Mar 2024 17:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01272v1</guid></item><item><title>Online Learning under Budget and ROI Constraints via Weak Adaptivity</title><link>http://arxiv.org/abs/2302.01203v3</link><description>We study online learning problems in which a decision maker has to make asequence of costly decisions, with the goal of maximizing their expected rewardwhile adhering to budget and return-on-investment (ROI) constraints. Existingprimal-dual algorithms designed for constrained online learning problems underadversarial inputs rely on two fundamental assumptions. First, the decisionmaker must know beforehand the value of parameters related to the degree ofstrict feasibility of the problem (i.e. Slater parameters). Second, a strictlyfeasible solution to the offline optimization problem must exist at each round.Both requirements are unrealistic for practical applications such as bidding inonline ad auctions. In this paper, we show how such assumptions can becircumvented by endowing standard primal-dual templates with weakly adaptiveregret minimizers. This results in a ``dual-balancing'' framework which ensuresthat dual variables stay sufficiently small, even in the absence of knowledgeabout Slater's parameter. We prove the first best-of-both-worlds no-regretguarantees which hold in absence of the two aforementioned assumptions, understochastic and adversarial inputs. Finally, we show how to instantiate theframework to optimally bid in various mechanisms of practical relevance, suchas first- and second-price auctions.</description><author>Matteo Castiglioni, Andrea Celli, Christian Kroer</author><pubDate>Sat, 02 Mar 2024 17:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01203v3</guid></item><item><title>Translating Legalese: Enhancing Public Understanding of Court Opinions with Legal Summarizers</title><link>http://arxiv.org/abs/2311.06534v2</link><description>Judicial opinions are written to be persuasive and could build public trustin court decisions, yet they can be difficult for non-experts to understand. Wepresent a pipeline for using an AI assistant to generate simplified summariesof judicial opinions. Compared to existing expert-written summaries, theseAI-generated simple summaries are more accessible to the public and more easilyunderstood by non-experts. We show in a survey experiment that the AI summarieshelp respondents understand the key features of a ruling, and have higherperceived quality, especially for respondents with less formal education.</description><author>Elliott Ash, Aniket Kesari, Suresh Naidu, Lena Song, Dominik Stammbach</author><pubDate>Sat, 02 Mar 2024 17:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06534v2</guid></item><item><title>A comprehensive cross-language framework for harmful content detection with the aid of sentiment analysis</title><link>http://arxiv.org/abs/2403.01270v1</link><description>In today's digital world, social media plays a significant role infacilitating communication and content sharing. However, the exponential risein user-generated content has led to challenges in maintaining a respectfulonline environment. In some cases, users have taken advantage of anonymity inorder to use harmful language, which can negatively affect the user experienceand pose serious social problems. Recognizing the limitations of manualmoderation, automatic detection systems have been developed to tackle thisproblem. Nevertheless, several obstacles persist, including the absence of auniversal definition for harmful language, inadequate datasets acrosslanguages, the need for detailed annotation guideline, and most importantly, acomprehensive framework. This study aims to address these challenges byintroducing, for the first time, a detailed framework adaptable to anylanguage. This framework encompasses various aspects of harmful languagedetection. A key component of the framework is the development of a general anddetailed annotation guideline. Additionally, the integration of sentimentanalysis represents a novel approach to enhancing harmful language detection.Also, a definition of harmful language based on the review of different relatedconcepts is presented. To demonstrate the effectiveness of the proposedframework, its implementation in a challenging low-resource language isconducted. We collected a Persian dataset and applied the annotation guidelinefor harmful detection and sentiment analysis. Next, we present baselineexperiments utilizing machine and deep learning methods to set benchmarks.Results prove the framework's high performance, achieving an accuracy of 99.4%in offensive language detection and 66.2% in sentiment analysis.</description><author>Mohammad Dehghani</author><pubDate>Sat, 02 Mar 2024 17:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01270v1</guid></item><item><title>Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach</title><link>http://arxiv.org/abs/2403.01268v1</link><description>Federated Learning (FL) trains a black-box and high-dimensional model amongdifferent clients by exchanging parameters instead of direct data sharing,which mitigates the privacy leak incurred by machine learning. However, FLstill suffers from membership inference attacks (MIA) or data reconstructionattacks (DRA). In particular, an attacker can extract the information fromlocal datasets by constructing DRA, which cannot be effectively throttled byexisting techniques, e.g., Differential Privacy (DP). In this paper, we aim to ensure a strong privacy guarantee for FL under DRA.We prove that reconstruction errors under DRA are constrained by theinformation acquired by an attacker, which means that constraining thetransmitted information can effectively throttle DRA. To quantify theinformation leakage incurred by FL, we establish a channel model, which dependson the upper bound of joint mutual information between the local dataset andmultiple transmitted parameters. Moreover, the channel model indicates that thetransmitted information can be constrained through data space operation, whichcan improve training efficiency and the model accuracy under constrainedinformation. According to the channel model, we propose algorithms to constrainthe information transmitted in a single round of local training. With a limitednumber of training rounds, the algorithms ensure that the total amount oftransmitted information is limited. Furthermore, our channel model can beapplied to various privacy-enhancing techniques (such as DP) to enhance privacyguarantees against DRA. Extensive experiments with real-world datasets validatethe effectiveness of our methods.</description><author>Qi Tan, Qi Li, Yi Zhao, Zhuotao Liu, Xiaobing Guo, Ke Xu</author><pubDate>Sat, 02 Mar 2024 17:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01268v1</guid></item><item><title>Decodable and Sample Invariant Continuous Object Encoder</title><link>http://arxiv.org/abs/2311.00187v3</link><description>We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of acontinuous object (e.g. a function), HDFE produces an explicit vectorrepresentation of the given object, invariant to the sample distribution anddensity. Sample distribution and density invariance enables HDFE toconsistently encode continuous objects regardless of their sampling, andtherefore allows neural networks to receive continuous objects as inputs formachine learning tasks, such as classification and regression. Besides, HDFEdoes not require any training and is proved to map the object into an organizedembedding space, which facilitates the training of the downstream tasks. Inaddition, the encoding is decodable, which enables neural networks to regresscontinuous objects by regressing their encodings. Therefore, HDFE serves as aninterface for processing continuous objects. We apply HDFE to function-to-function mapping, where vanilla HDFE achievescompetitive performance as the state-of-the-art algorithm. We apply HDFE topoint cloud surface normal estimation, where a simple replacement from PointNetto HDFE leads to immediate 12% and 15% error reductions in two benchmarks. Inaddition, by integrating HDFE into the PointNet-based SOTA network, we improvethe SOTA baseline by 2.5% and 1.7% in the same benchmarks.</description><author>Dehao Yuan, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos</author><pubDate>Sat, 02 Mar 2024 17:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00187v3</guid></item><item><title>Dissecting Language Models: Machine Unlearning via Selective Pruning</title><link>http://arxiv.org/abs/2403.01267v1</link><description>Understanding and shaping the behaviour of Large Language Models (LLMs) isincreasingly important as applications become more powerful and more frequentlyadopted. This paper introduces a machine unlearning method specificallydesigned for LLMs. We introduce a selective pruning method for LLMs thatremoves neurons based on their relative importance on a targeted capabilitycompared to overall network performance. This approach is a compute- anddata-efficient method for identifying and removing neurons that enable specificbehaviours. Our findings reveal that both feed-forward and attention neurons inLLMs are specialized; that is, for specific tasks, certain neurons are morecrucial than others.</description><author>Nicholas Pochinkov, Nandi Schoots</author><pubDate>Sat, 02 Mar 2024 17:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01267v1</guid></item><item><title>Using Forwards-Backwards Models to Approximate MDP Homomorphisms</title><link>http://arxiv.org/abs/2209.06356v3</link><description>Reinforcement learning agents must painstakingly learn through trial anderror what sets of state-action pairs are value equivalent -- requiring anoften prohibitively large amount of environment experience. MDP homomorphismshave been proposed that reduce the MDP of an environment to an abstract MDP,enabling better sample efficiency. Consequently, impressive improvements havebeen achieved when a suitable homomorphism can be constructed a priori --usually by exploiting a practitioner's knowledge of environment symmetries. Wepropose a novel approach to constructing homomorphisms in discrete actionspaces, which uses a learnt model of environment dynamics to infer whichstate-action pairs lead to the same state -- which can reduce the size of thestate-action space by a factor as large as the cardinality of the originalaction space. In MinAtar, we report an almost 4x improvement over a value-basedoff-policy baseline in the low sample limit, when averaging over all games andoptimizers.</description><author>Augustine N. Mavor-Parker, Matthew J. Sargent, Christian Pehle, Andrea Banino, Lewis D. Griffin, Caswell Barry</author><pubDate>Sat, 02 Mar 2024 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06356v3</guid></item><item><title>Single-image camera calibration with model-free distortion correction</title><link>http://arxiv.org/abs/2403.01263v1</link><description>Camera calibration is a process of paramount importance in computer visionapplications that require accurate quantitative measurements. The popularmethod developed by Zhang relies on the use of a large number of images of aplanar grid of fiducial points captured in multiple poses. Although flexibleand easy to implement, Zhang's method has some limitations. The simultaneousoptimization of the entire parameter set, including the coefficients of apredefined distortion model, may result in poor distortion correction at theimage boundaries or in miscalculation of the intrinsic parameters, even with areasonably small reprojection error. Indeed, applications involving imagestitching (e.g. multi-camera systems) require accurate mapping of distortion upto the outermost regions of the image. Moreover, intrinsic parameters affectthe accuracy of camera pose estimation, which is fundamental for applicationssuch as vision servoing in robot navigation and automated assembly. This paperproposes a method for estimating the complete set of calibration parametersfrom a single image of a planar speckle pattern covering the entire sensor. Thecorrespondence between image points and physical points on the calibrationtarget is obtained using Digital Image Correlation. The effective focal lengthand the extrinsic parameters are calculated separately after a prior evaluationof the principal point. At the end of the procedure, a dense and uniformmodel-free distortion map is obtained over the entire image. Synthetic datawith different noise levels were used to test the feasibility of the proposedmethod and to compare its metrological performance with Zhang's method.Real-world tests demonstrate the potential of the developed method to revealaspects of the image formation that are hidden by averaging over multipleimages.</description><author>Katia Genovese</author><pubDate>Sat, 02 Mar 2024 16:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01263v1</guid></item><item><title>Seeing the World through Your Eyes</title><link>http://arxiv.org/abs/2306.09348v2</link><description>The reflective nature of the human eye is an underappreciated source ofinformation about what the world around us looks like. By imaging the eyes of amoving person, we can collect multiple views of a scene outside the camera'sdirect line of sight through the reflections in the eyes. In this paper, wereconstruct a 3D scene beyond the camera's line of sight using portrait imagescontaining eye reflections. This task is challenging due to 1) the difficultyof accurately estimating eye poses and 2) the entangled appearance of the eyeiris and the scene reflections. Our method jointly refines the cornea poses,the radiance field depicting the scene, and the observer's eye iris texture. Wefurther propose a simple regularization prior on the iris texture pattern toimprove reconstruction quality. Through various experiments on synthetic andreal-world captures featuring people with varied eye colors, we demonstrate thefeasibility of our approach to recover 3D scenes using eye reflections.</description><author>Hadi Alzayer, Kevin Zhang, Brandon Feng, Christopher Metzler, Jia-Bin Huang</author><pubDate>Sat, 02 Mar 2024 16:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09348v2</guid></item><item><title>When No-Rejection Learning is Consistent for Regression with Rejection</title><link>http://arxiv.org/abs/2307.02932v3</link><description>Learning with rejection has been a prototypical model for studying thehuman-AI interaction on prediction tasks. Upon the arrival of a sampleinstance, the model first uses a rejector to decide whether to accept and usethe AI predictor to make a prediction or reject and defer the sample to humans.Learning such a model changes the structure of the original loss function andoften results in undesirable non-convexity and inconsistency issues. For theclassification with rejection problem, several works develop consistentsurrogate losses for the joint learning of the predictor and the rejector,while there have been fewer works for the regression counterpart. This paperstudies the regression with rejection (RwR) problem and investigates ano-rejection learning strategy that uses all the data to learn the predictor.We first establish the consistency for such a strategy under the weakrealizability condition. Then for the case without the weak realizability, weshow that the excessive risk can also be upper bounded with the sum of twoparts: prediction error and calibration error. Lastly, we demonstrate theadvantage of such a proposed learning strategy with empirical evidence.</description><author>Xiaocheng Li, Shang Liu, Chunlin Sun, Hanzhao Wang</author><pubDate>Sat, 02 Mar 2024 16:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02932v3</guid></item><item><title>MultIOD: Rehearsal-free Multihead Incremental Object Detector</title><link>http://arxiv.org/abs/2309.05334v2</link><description>Class-Incremental learning (CIL) refers to the ability of artificial agentsto integrate new classes as they appear in a stream. It is particularlyinteresting in evolving environments where agents have limited access to memoryand computational resources. The main challenge of incremental learning iscatastrophic forgetting, the inability of neural networks to retain pastknowledge when learning a new one. Unfortunately, most existingclass-incremental methods for object detection are applied to two-stagealgorithms such as Faster-RCNN, and rely on rehearsal memory to retain pastknowledge. We argue that those are not realistic, and more effort should bededicated to anchor-free and rehearsal-free object detection. In this context,we propose MultIOD, a class-incremental object detector based on CenterNet. Ourmain contributions are: (1) we propose a multihead feature pyramid andmultihead detection architecture to efficiently separate class representations,(2) we employ transfer learning between classes learned initially and thoselearned incrementally to tackle catastrophic forgetting, and (3) we use aclass-wise non-max-suppression as a post-processing technique to removeredundant boxes. Results show that our method outperforms a range ofstate-of-the-art methods on two Pascal VOC datasets, while reducing memoryfootprint by more than half.</description><author>Eden Belouadah, Arnaud Dapogny, Kevin Bailly</author><pubDate>Sat, 02 Mar 2024 16:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05334v2</guid></item><item><title>A Truly Joint Neural Architecture for Segmentation and Parsing</title><link>http://arxiv.org/abs/2402.02564v2</link><description>Contemporary multilingual dependency parsers can parse a diverse set oflanguages, but for Morphologically Rich Languages (MRLs), performance isattested to be lower than other languages. The key challenge is that, due tohigh morphological complexity and ambiguity of the space-delimited inputtokens, the linguistic units that act as nodes in the tree are not known inadvance. Pre-neural dependency parsers for MRLs subscribed to the jointmorpho-syntactic hypothesis, stating that morphological segmentation andsyntactic parsing should be solved jointly, rather than as a pipeline wheresegmentation precedes parsing. However, neural state-of-the-art parsers to dateuse a strict pipeline. In this paper we introduce a joint neural architecturewhere a lattice-based representation preserving all morphological ambiguity ofthe input is provided to an arc-factored model, which then solves themorphological segmentation and syntactic parsing tasks at once. Our experimentson Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-artperformance on parsing, tagging and segmentation of the Hebrew section of UD,using a single model. This proposed architecture is LLM-based and languageagnostic, providing a solid foundation for MRLs to obtain further performanceimprovements and bridge the gap with other languages.</description><author>Danit Yshaayahu Levi, Reut Tsarfaty</author><pubDate>Sat, 02 Mar 2024 16:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02564v2</guid></item><item><title>Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey</title><link>http://arxiv.org/abs/2403.01255v1</link><description>Recent advancements in deep learning (DL) have posed a significant challengefor automatic speech recognition (ASR). ASR relies on extensive trainingdatasets, including confidential ones, and demands substantial computationaland storage resources. Enabling adaptive systems improves ASR performance indynamic environments. DL techniques assume training and testing data originatefrom the same domain, which is not always true. Advanced DL techniques likedeep transfer learning (DTL), federated learning (FL), and reinforcementlearning (RL) address these issues. DTL allows high-performance models usingsmall yet related datasets, FL enables training on confidential data withoutdataset possession, and RL optimizes decision-making in dynamic environments,reducing computation costs. This survey offers a comprehensive review of DTL,FL, and RL-based ASR frameworks, aiming to provide insights into the latestdevelopments and aid researchers and professionals in understanding the currentchallenges. Additionally, transformers, which are advanced DL techniquesheavily used in proposed ASR frameworks, are considered in this survey fortheir ability to capture extensive dependencies in the input ASR sequence. Thepaper starts by presenting the background of DTL, FL, RL, and Transformers andthen adopts a well-designed taxonomy to outline the state-of-the-artapproaches. Subsequently, a critical analysis is conducted to identify thestrengths and weaknesses of each framework. Additionally, a comparative studyis presented to highlight the existing challenges, paving the way for futureresearch opportunities.</description><author>Hamza Kheddar, Mustapha Hemis, Yassine Himeur</author><pubDate>Sat, 02 Mar 2024 16:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01255v1</guid></item><item><title>Accelerating Greedy Coordinate Gradient via Probe Sampling</title><link>http://arxiv.org/abs/2403.01251v1</link><description>Safety of Large Language Models (LLMs) has become a central issue given theirrapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shownto be effective in constructing prompts containing adversarial suffixes tobreak the presumingly safe LLMs, but the optimization of GCG is time-consumingand limits its practicality. To reduce the time cost of GCG and enable morecomprehensive studies of LLM safety, in this work, we study a new algorithmcalled $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the coreof the algorithm is a mechanism that dynamically determines how similar asmaller draft model's predictions are to the target model's predictions forprompt candidates. When the target model is similar to the draft model, we relyheavily on the draft model to filter out a large number of potential promptcandidates to reduce the computation time. Probe sampling achieves up to $5.6$times speedup using Llama2-7b and leads to equal or improved attack successrate (ASR) on the AdvBench.</description><author>Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi, Anirudh Goyal, Michael Shieh</author><pubDate>Sat, 02 Mar 2024 16:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01251v1</guid></item><item><title>SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code</title><link>http://arxiv.org/abs/2403.01248v1</link><description>This paper introduces SceneCraft, a Large Language Model (LLM) Agentconverting text descriptions into Blender-executable Python scripts whichrender complex scenes with up to a hundred 3D assets. This process requirescomplex spatial planning and arrangement. We tackle these challenges through acombination of advanced abstraction, strategic planning, and library learning.SceneCraft first models a scene graph as a blueprint, detailing the spatialrelationships among assets in the scene. SceneCraft then writes Python scriptsbased on this graph, translating relationships into numerical constraints forasset layout. Next, SceneCraft leverages the perceptual strengths ofvision-language foundation models like GPT-V to analyze rendered images anditeratively refine the scene. On top of this process, SceneCraft features alibrary learning mechanism that compiles common script functions into areusable library, facilitating continuous self-improvement without expensiveLLM parameter tuning. Our evaluation demonstrates that SceneCraft surpassesexisting LLM-based agents in rendering complex scenes, as shown by itsadherence to constraints and favorable human assessments. We also showcase thebroader application potential of SceneCraft by reconstructing detailed 3Dscenes from the Sintel movie and guiding a video generative model withgenerated scenes as intermediary control signal.</description><author>Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi</author><pubDate>Sat, 02 Mar 2024 16:16:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01248v1</guid></item><item><title>Dual Graph Attention based Disentanglement Multiple Instance Learning for Brain Age Estimation</title><link>http://arxiv.org/abs/2403.01246v1</link><description>Deep learning techniques have demonstrated great potential for accuratelyestimating brain age by analyzing Magnetic Resonance Imaging (MRI) data fromhealthy individuals. However, current methods for brain age estimation oftendirectly utilize whole input images, overlooking two important considerations:1) the heterogeneous nature of brain aging, where different brain regions maydegenerate at different rates, and 2) the existence of age-independentredundancies in brain structure. To overcome these limitations, we propose aDual Graph Attention based Disentanglement Multi-instance Learning (DGA-DMIL)framework for improving brain age estimation. Specifically, the 3D MRI data,treated as a bag of instances, is fed into a 2D convolutional neural networkbackbone, to capture the unique aging patterns in MRI. A dual graph attentionaggregator is then proposed to learn the backbone features by exploiting theintra- and inter-instance relationships. Furthermore, a disentanglement branchis introduced to separate age-related features from age-independent structuralrepresentations to ameliorate the interference of redundant information on ageprediction. To verify the effectiveness of the proposed framework, we evaluateit on two datasets, UK Biobank and ADNI, containing a total of 35,388 healthyindividuals. Our proposed model demonstrates exceptional accuracy in estimatingbrain age, achieving a remarkable mean absolute error of 2.12 years in the UKBiobank. The results establish our approach as state-of-the-art compared toother competing brain age estimation models. In addition, the instancecontribution scores identify the varied importance of brain areas for agingprediction, which provides deeper insights into the understanding of brainaging.</description><author>Fanzhe Yan, Gang Yang, Yu Li, Aiping Liu, Xun Chen</author><pubDate>Sat, 02 Mar 2024 16:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01246v1</guid></item><item><title>AcME-AD: Accelerated Model Explanations for Anomaly Detection</title><link>http://arxiv.org/abs/2403.01245v1</link><description>Pursuing fast and robust interpretability in Anomaly Detection is crucial,especially due to its significance in practical applications. TraditionalAnomaly Detection methods excel in outlier identification but are oftenblack-boxes, providing scant insights into their decision-making process. Thislack of transparency compromises their reliability and hampers their adoptionin scenarios where comprehending the reasons behind anomaly detection is vital.At the same time, getting explanations quickly is paramount in practicalscenarios. To bridge this gap, we present AcME-AD, a novel approach rooted inExplainable Artificial Intelligence principles, designed to clarify AnomalyDetection models for tabular data. AcME-AD transcends the constraints ofmodel-specific or resource-heavy explainability techniques by delivering amodel-agnostic, efficient solution for interoperability. It offers localfeature importance scores and a what-if analysis tool, shedding light on thefactors contributing to each anomaly, thus aiding root cause analysis anddecision-making. This paper elucidates AcME-AD's foundation, its benefits overexisting methods, and validates its effectiveness with tests on both syntheticand real datasets. AcME-AD's implementation and experiment replication code isaccessible in a public repository.</description><author>Valentina Zaccaria, David Dandolo, Chiara Masiero, Gian Antonio Susto</author><pubDate>Sat, 02 Mar 2024 16:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01245v1</guid></item><item><title>Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal</title><link>http://arxiv.org/abs/2403.01244v1</link><description>Large language models (LLMs) suffer from catastrophic forgetting duringcontinual learning. Conventional rehearsal-based methods rely on previoustraining data to retain the model's ability, which may not be feasible inreal-world applications. When conducting continual learning based on apublicly-released LLM checkpoint, the availability of the original trainingdata may be non-existent. To address this challenge, we propose a frameworkcalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate syntheticinstances for rehearsal. Concretely, we first employ the base LLM forin-context learning to generate synthetic instances. Subsequently, we utilizethe latest LLM to refine the instance outputs based on the synthetic inputs,preserving its acquired ability. Finally, we select diverse high-qualitysynthetic instances for rehearsal in future stages. Experimental resultsdemonstrate that SSR achieves superior or comparable performance compared toconventional rehearsal-based approaches while being more data-efficient.Besides, SSR effectively preserves the generalization capabilities of LLMs ingeneral domains.</description><author>Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, Jinsong Su</author><pubDate>Sat, 02 Mar 2024 16:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01244v1</guid></item><item><title>Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning</title><link>http://arxiv.org/abs/2403.01242v1</link><description>Electric automation systems offer convenience and efficiency in controllingelectrical circuits and devices. Traditionally, these systems rely onpredefined commands for control, limiting flexibility and adaptability. In thispaper, we propose a novel approach to augment automation by introducingintent-based user instruction classification using machine learning techniques.Our system represents user instructions as intents, allowing for dynamiccontrol of electrical circuits without relying on predefined commands. Througha machine learning model trained on a labeled dataset of user instructions, oursystem classifies intents from user input, enabling a more intuitive andadaptable control scheme. We present the design and implementation of ourintent-based electric automation system, detailing the development of themachine learning model for intent classification. Experimental resultsdemonstrate the effectiveness of our approach in enhancing user experience andexpanding the capabilities of electric automation systems. Our work contributesto the advancement of smart technologies by providing a more seamlessinteraction between users and their environments.</description><author>Lochan Basyal, Bijay Gaudel</author><pubDate>Sat, 02 Mar 2024 16:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01242v1</guid></item><item><title>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact</title><link>http://arxiv.org/abs/2403.01241v1</link><description>Large language models (LLMs) excel in natural language processing but demandintensive computation. To mitigate this, various quantization methods have beenexplored, yet they compromise LLM performance. This paper unveils a previouslyoverlooked type of outlier in LLMs. Such outliers are found to allocate most ofthe attention scores on initial tokens of input, termed as pivot tokens, whichis crucial to the performance of quantized LLMs. Given that, we proposeIntactKV to generate the KV cache of pivot tokens losslessly from thefull-precision model. The approach is simple and easy to combine with existingquantization solutions. Besides, IntactKV can be calibrated as additional LLMparameters to boost the quantized LLMs further. Mathematical analysis alsoproves that IntactKV effectively reduces the upper bound of quantization error.Empirical results show that IntactKV brings consistent improvement and achieveslossless weight-only INT4 quantization on various downstream tasks, leading tothe new state-of-the-art for LLM quantization.</description><author>Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan</author><pubDate>Sat, 02 Mar 2024 16:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01241v1</guid></item><item><title>Safe POMDP Online Planning via Shielding</title><link>http://arxiv.org/abs/2309.10216v2</link><description>Partially observable Markov decision processes (POMDPs) have been widely usedin many robotic applications for sequential decision-making under uncertainty.POMDP online planning algorithms such as Partially Observable Monte-CarloPlanning (POMCP) can solve very large POMDPs with the goal of maximizing theexpected return. But the resulting policies cannot provide safety guaranteeswhich are imperative for real-world safety-critical tasks (e.g., autonomousdriving). In this work, we consider safety requirements represented asalmost-sure reach-avoid specifications (i.e., the probability to reach a set ofgoal states is one and the probability to reach a set of unsafe states iszero). We compute shields that restrict unsafe actions which would violate thealmost-sure reach-avoid specifications. We then integrate these shields intothe POMCP algorithm for safe POMDP online planning. We propose four distinctshielding methods, differing in how the shields are computed and integrated,including factored variants designed to improve scalability. Experimentalresults on a set of benchmark domains demonstrate that the proposed shieldingmethods successfully guarantee safety (unlike the baseline POMCP withoutshielding) on large POMDPs, with negligible impact on the runtime for onlineplanning.</description><author>Shili Sheng, David Parker, Lu Feng</author><pubDate>Sat, 02 Mar 2024 15:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10216v2</guid></item><item><title>On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving</title><link>http://arxiv.org/abs/2403.01238v1</link><description>End-to-end motion planning models equipped with deep neural networks haveshown great potential for enabling full autonomous driving. However, theoversized neural networks render them impractical for deployment onresource-constrained systems, which unavoidably requires more computationaltime and resources during reference.To handle this, knowledge distillationoffers a promising approach that compresses models by enabling a smallerstudent model to learn from a larger teacher model. Nevertheless, how to applyknowledge distillation to compress motion planners has not been explored sofar. In this paper, we propose PlanKD, the first knowledge distillationframework tailored for compressing end-to-end motion planners. First,considering that driving scenes are inherently complex, often containingplanning-irrelevant or even noisy information, transferring such information isnot beneficial for the student planner. Thus, we design an informationbottleneck based strategy to only distill planning-relevant information, ratherthan transfer all information indiscriminately. Second, different waypoints inan output planned trajectory may hold varying degrees of importance for motionplanning, where a slight deviation in certain crucial waypoints might lead to acollision. Therefore, we devise a safety-aware waypoint-attentive distillationmodule that assigns adaptive weights to different waypoints based on theimportance, to encourage the student to accurately mimic more crucialwaypoints, thereby improving overall safety. Experiments demonstrate that ourPlanKD can boost the performance of smaller planners by a large margin, andsignificantly reduce their reference time.</description><author>Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang</author><pubDate>Sat, 02 Mar 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01238v1</guid></item><item><title>NeRF Solves Undersampled MRI Reconstruction</title><link>http://arxiv.org/abs/2402.13226v2</link><description>This article presents a novel undersampled magnetic resonance imaging (MRI)technique that leverages the concept of Neural Radiance Field (NeRF). Withradial undersampling, the corresponding imaging problem can be reformulatedinto an image modeling task from sparse-view rendered data; therefore, a highdimensional MR image is obtainable from undersampled k-space data by takingadvantage of implicit neural representation. A multi-layer perceptron, which isdesigned to output an image intensity from a spatial coordinate, learns the MRphysics-driven rendering relation between given measurement data and desiredimage. Effective undersampling strategies for high-quality neuralrepresentation are investigated. The proposed method serves two benefits: (i)The learning is based fully on single undersampled k-space data, not a bunch ofmeasured data and target image sets. It can be used potentially for diagnosticMR imaging, such as fetal MRI, where data acquisition is relatively rare orlimited against diversity of clinical images while undersampled reconstructionis highly demanded. (ii) A reconstructed MR image is a scan-specificrepresentation highly adaptive to the given k-space measurement. Numerousexperiments validate the feasibility and capability of the proposed approach.</description><author>Tae Jun Jang, Chang Min Hyun</author><pubDate>Sat, 02 Mar 2024 15:46:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13226v2</guid></item><item><title>Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings</title><link>http://arxiv.org/abs/2403.01234v1</link><description>Exploring molecular spaces is crucial for advancing our understanding ofchemical properties and reactions, leading to groundbreaking innovations inmaterials science, medicine, and energy. This paper explores an approach foractive learning in molecular discovery using Deep Kernel Learning (DKL), anovel approach surpassing the limits of classical Variational Autoencoders(VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, whichanalyze molecular structures based on similarity, revealing limitations due tosparse regularities in latent spaces. DKL, however, offers a more holisticperspective by correlating structure with properties, creating latent spacesthat prioritize molecular functionality. This is achieved by recalculatingembedding vectors iteratively, aligning with the experimental availability oftarget properties. The resulting latent spaces are not only better organizedbut also exhibit unique characteristics such as concentrated maximarepresenting molecular functionalities and a correlation between predictiveuncertainty and error. Additionally, the formation of exclusion regions aroundcertain compounds indicates unexplored areas with potential for groundbreakingfunctionalities. This study underscores DKL's potential in molecular research,offering new avenues for understanding and discovering molecularfunctionalities beyond classical VAE limitations.</description><author>Ayana Ghosh, Maxim Ziatdinov and, Sergei V. Kalinin</author><pubDate>Sat, 02 Mar 2024 15:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01234v1</guid></item><item><title>Polynormer: Polynomial-Expressive Graph Transformer in Linear Time</title><link>http://arxiv.org/abs/2403.01232v1</link><description>Graph transformers (GTs) have emerged as a promising architecture that istheoretically more expressive than message-passing graph neural networks(GNNs). However, typical GT models have at least quadratic complexity and thuscannot scale to large graphs. While there are several linear GTs recentlyproposed, they still lag behind GNN counterparts on several popular graphdatasets, which poses a critical concern on their practical expressivity. Tobalance the trade-off between expressivity and scalability of GTs, we proposePolynormer, a polynomial-expressive GT model with linear complexity. Polynormeris built upon a novel base model that learns a high-degree polynomial on inputfeatures. To enable the base model permutation equivariant, we integrate itwith graph topology and node features separately, resulting in local and globalequivariant attention models. Consequently, Polynormer adopts a linearlocal-to-global attention scheme to learn high-degree equivariant polynomialswhose coefficients are controlled by attention scores. Polynormer has beenevaluated on $13$ homophilic and heterophilic datasets, including large graphswith millions of nodes. Our extensive experiment results show that Polynormeroutperforms state-of-the-art GNN and GT baselines on most datasets, evenwithout the use of nonlinear activation functions.</description><author>Chenhui Deng, Zichao Yue, Zhiru Zhang</author><pubDate>Sat, 02 Mar 2024 15:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01232v1</guid></item><item><title>Benchmarking Segmentation Models with Mask-Preserved Attribute Editing</title><link>http://arxiv.org/abs/2403.01231v1</link><description>When deploying segmentation models in practice, it is critical to evaluatetheir behaviors in varied and complex scenes. Different from the previousevaluation paradigms only in consideration of global attribute variations (e.g.adverse weather), we investigate both local and global attribute variations forrobustness evaluation. To achieve this, we construct a mask-preserved attributeediting pipeline to edit visual attributes of real images with precise controlof structural information. Therefore, the original segmentation labels can bereused for the edited images. Using our pipeline, we construct a benchmarkcovering both object and image attributes (e.g. color, material, pattern,style). We evaluate a broad variety of semantic segmentation models, spanningfrom conventional close-set models to recent open-vocabulary large models ontheir robustness to different types of variations. We find that both local andglobal attribute variations affect segmentation performances, and thesensitivity of models diverges across different variation types. We argue thatlocal attributes have the same importance as global attributes, and should beconsidered in the robustness evaluation of segmentation models. Code:https://github.com/PRIS-CV/Pascal-EA.</description><author>Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo</author><pubDate>Sat, 02 Mar 2024 15:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01231v1</guid></item><item><title>REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild</title><link>http://arxiv.org/abs/2403.01229v1</link><description>Recognizing speaking in humans is a central task towards understanding socialinteractions. Ideally, speaking would be detected from individual voicerecordings, as done previously for meeting scenarios. However, individual voicerecordings are hard to obtain in the wild, especially in crowded minglingscenarios due to cost, logistics, and privacy concerns. As an alternative,machine learning models trained on video and wearable sensor data make itpossible to recognize speech by detecting its related gestures in anunobtrusive, privacy-preserving way. These models themselves should ideally betrained using labels obtained from the speech signal. However, existingmingling datasets do not contain high quality audio recordings. Instead,speaking status annotations have often been inferred by human annotators fromvideo, without validation of this approach against audio-based ground truth. Inthis paper we revisit no-audio speaking status estimation by presenting thefirst publicly available multimodal dataset with high-quality individual speechrecordings of 33 subjects in a professional networking event. We present threebaselines for no-audio speaking status segmentation: a) from video, b) frombody acceleration (chest-worn accelerometer), c) from body pose tracks. In allcases we predict a 20Hz binary speaking status signal extracted from the audio,a time resolution not available in previous datasets. In addition to providingthe signals and ground truth necessary to evaluate a wide range of speakingstatus detection methods, the availability of audio in REWIND makes it suitablefor cross-modality studies not feasible with previous mingling datasets.Finally, our flexible data consent setup creates new challenges for multimodalsystems under missing modalities.</description><author>Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung</author><pubDate>Sat, 02 Mar 2024 15:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01229v1</guid></item><item><title>Top-Down Framework for Weakly-supervised Grounded Image Captioning</title><link>http://arxiv.org/abs/2306.07490v3</link><description>Weakly-supervised grounded image captioning (WSGIC) aims to generate thecaption and ground (localize) predicted object words in the input image withoutusing bounding box supervision. Recent two-stage solutions mostly apply abottom-up pipeline: (1) encode the input image into multiple region featuresusing an object detector; (2) leverage region features for captioning andgrounding. However, utilizing independent proposals produced by objectdetectors tends to make the subsequent grounded captioner overfitted in findingthe correct object words, overlooking the relation between objects, andselecting incompatible proposal regions for grounding. To address these issues,we propose a one-stage weakly-supervised grounded captioner that directly takesthe RGB image as input to perform captioning and grounding at the top-downimage level. Specifically, we encode the image into visual tokenrepresentations and propose a Recurrent Grounding Module (RGM) in the decoderto obtain precise Visual Language Attention Maps (VLAMs), which recognize thespatial locations of the objects. In addition, we explicitly inject a relationmodule into our one-stage framework to encourage relation understanding throughmulti-label classification. This relation semantics served as contextualinformation facilitating the prediction of relation and object words in thecaption. We observe that the relation semantic not only assists the groundedcaptioner in generating a more accurate caption but also improves the groundingperformance. We validate the effectiveness of our proposed method on twochallenging datasets (Flick30k Entities captioning and MSCOCO captioning). Theexperimental results demonstrate that our method achieves state-of-the-artgrounding performance.</description><author>Chen Cai, Suchen Wang, Kim-hui Yap, Yi Wang</author><pubDate>Sat, 02 Mar 2024 15:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07490v3</guid></item><item><title>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</title><link>http://arxiv.org/abs/2308.09729v5</link><description>Large language models (LLMs) have achieved remarkable performance in naturallanguage understanding and generation tasks. However, they often suffer fromlimitations such as difficulty in incorporating new knowledge, generatinghallucinations, and explaining their reasoning process. To address thesechallenges, we propose a novel prompting pipeline, named \method, thatleverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.Our method enables LLMs to comprehend KG inputs and infer with a combination ofimplicit and external knowledge. Moreover, our method elicits the mind map ofLLMs, which reveals their reasoning pathways based on the ontology ofknowledge. We evaluate our method on diverse question \&amp; answering tasks,especially in medical domains, and show significant improvements overbaselines. We also introduce a new hallucination evaluation benchmark andanalyze the effects of different components of our method. Our resultsdemonstrate the effectiveness and robustness of our method in merging knowledgefrom LLMs and KGs for combined inference. To reproduce our results and extendthe framework further, we make our codebase available athttps://github.com/wyl-willing/MindMap.</description><author>Yilin Wen, Zifeng Wang, Jimeng Sun</author><pubDate>Sat, 02 Mar 2024 15:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09729v5</guid></item><item><title>Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation</title><link>http://arxiv.org/abs/2402.18919v2</link><description>While standard Empirical Risk Minimization (ERM) training is proven effectivefor image classification on in-distribution data, it fails to perform well onout-of-distribution samples. One of the main sources of distribution shift forimage classification is the compositional nature of images. Specifically, inaddition to the main object or component(s) determining the label, some otherimage components usually exist, which may lead to the shift of inputdistribution between train and test environments. More importantly, thesecomponents may have spurious correlations with the label. To address thisissue, we propose Decompose-and-Compose (DaC), which improves robustness tocorrelation shift by a compositional approach based on combining elements ofimages. Based on our observations, models trained with ERM usually highlyattend to either the causal components or the components having a high spuriouscorrelation with the label (especially in datapoints on which models have ahigh confidence). In fact, according to the amount of spurious correlation andthe easiness of classification based on the causal or non-causal components,the model usually attends to one of these more (on samples with highconfidence). Following this, we first try to identify the causal components ofimages using class activation maps of models trained with ERM. Afterward, weintervene on images by combining them and retraining the model on the augmenteddata, including the counterfactual ones. Along with its high interpretability,this work proposes a group-balancing method by intervening on images withoutrequiring group labels or information regarding the spurious features duringtraining. The method has an overall better worst group accuracy compared toprevious methods with the same amount of supervision on the group labels incorrelation shift.</description><author>Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast, Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah</author><pubDate>Sat, 02 Mar 2024 14:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18919v2</guid></item><item><title>Approximate Nash Equilibrium Learning for n-Player Markov Games in Dynamic Pricing</title><link>http://arxiv.org/abs/2207.06492v3</link><description>We investigate Nash equilibrium learning in a competitive Markov Game (MG)environment, where multiple agents compete, and multiple Nash equilibria canexist. In particular, for an oligopolistic dynamic pricing environment, exactNash equilibria are difficult to obtain due to the curse-of-dimensionality. Wedevelop a new model-free method to find approximate Nash equilibria.Gradient-free black box optimization is then applied to estimate $\epsilon$,the maximum reward advantage of an agent unilaterally deviating from any jointpolicy, and to also estimate the $\epsilon$-minimizing policy for any givenstate. The policy-$\epsilon$ correspondence and the state to$\epsilon$-minimizing policy are represented by neural networks, the latterbeing the Nash Policy Net. During batch update, we perform Nash Q learning onthe system, by adjusting the action probabilities using the Nash Policy Net. Wedemonstrate that an approximate Nash equilibrium can be learned, particularlyin the dynamic pricing domain where exact solutions are often intractable.</description><author>Larkin Liu</author><pubDate>Sat, 02 Mar 2024 14:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.06492v3</guid></item><item><title>DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction</title><link>http://arxiv.org/abs/2403.01226v1</link><description>Audio-visual saliency prediction can draw support from diverse modalitycomplements, but further performance enhancement is still challenged bycustomized architectures as well as task-specific loss functions. In recentstudies, denoising diffusion models have shown more promising in unifying taskframeworks owing to their inherent ability of generalization. Following thismotivation, a novel Diffusion architecture for generalized audio-visualSaliency prediction (DiffSal) is proposed in this work, which formulates theprediction problem as a conditional generative task of the saliency map byutilizing input audio and video as the conditions. Based on the spatio-temporalaudio-visual features, an extra network Saliency-UNet is designed to performmulti-modal attention modulation for progressive refinement of the ground-truthsaliency map from the noisy map. Extensive experiments demonstrate that theproposed DiffSal can achieve excellent performance across six challengingaudio-visual benchmarks, with an average relative improvement of 6.3\% over theprevious state-of-the-art results by six metrics.</description><author>Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha</author><pubDate>Sat, 02 Mar 2024 14:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01226v1</guid></item><item><title>G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment</title><link>http://arxiv.org/abs/2402.18122v2</link><description>Despite numerous completed studies, achieving high fidelity talking facegeneration with highly synchronized lip movements corresponding to arbitraryaudio remains a significant challenge in the field. The shortcomings ofpublished studies continue to confuse many researchers. This paper introducesG4G, a generic framework for high fidelity talking face generation withfine-grained intra-modal alignment. G4G can reenact the high fidelity oforiginal video while producing highly synchronized lip movements regardless ofgiven audio tones or volumes. The key to G4G's success is the use of a diagonalmatrix to enhance the ordinary alignment of audio-image intra-modal features,which significantly increases the comparative learning between positive andnegative samples. Additionally, a multi-scaled supervision module is introducedto comprehensively reenact the perceptional fidelity of original video acrossthe facial region while emphasizing the synchronization of lip movements andthe input audio. A fusion network is then used to further fuse the facialregion and the rest. Our experimental results demonstrate significantachievements in reenactment of original video quality as well as highlysynchronized talking lips. G4G is an outperforming generic framework that canproduce talking videos competitively closer to ground truth level than currentstate-of-the-art methods.</description><author>Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</author><pubDate>Sat, 02 Mar 2024 14:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18122v2</guid></item><item><title>FaNS: a Facet-based Narrative Similarity Metric</title><link>http://arxiv.org/abs/2309.04823v2</link><description>Similar Narrative Retrieval is a crucial task since narratives are essentialfor explaining and understanding events, and multiple related narratives oftenhelp to create a holistic view of the event of interest. To accurately identifysemantically similar narratives, this paper proposes a novel narrativesimilarity metric called Facet-based Narrative Similarity (FaNS), based on theclassic 5W1H facets (Who, What, When, Where, Why, and How), which are extractedby leveraging the state-of-the-art Large Language Models (LLMs). Unlikeexisting similarity metrics that only focus on overall lexical/semantic match,FaNS provides a more granular matching along six different facets independentlyand then combines them. To evaluate FaNS, we created a comprehensive dataset bycollecting narratives from AllSides, a third-party news portal. Experimentalresults demonstrate that the FaNS metric exhibits a higher correlation (37\%higher) than traditional text similarity metrics that directly measure thelexical/semantic match between narratives, demonstrating its effectiveness incomparing the finer details between a pair of narratives.</description><author>Mousumi Akter, Shubhra Kanti Karmaker Santu</author><pubDate>Sat, 02 Mar 2024 14:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04823v2</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v5</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-contextlearning and opens up new avenues for addressing complex reasoning challenges.Our code is released athttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.</description><author>Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Sat, 02 Mar 2024 14:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v5</guid></item><item><title>Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions</title><link>http://arxiv.org/abs/2403.01222v1</link><description>Emotions are a central aspect of communication. Consequently, emotionanalysis (EA) is a rapidly growing field in natural language processing (NLP).However, there is no consensus on scope, direction, or methods. In this paper,we conduct a thorough review of 154 relevant NLP publications from the lastdecade. Based on this review, we address four different questions: (1) How areEA tasks defined in NLP? (2) What are the most prominent emotion frameworks andwhich emotions are modeled? (3) Is the subjectivity of emotions considered interms of demographics and cultural factors? and (4) What are the primary NLPapplications for EA? We take stock of trends in EA and tasks, emotionframeworks used, existing datasets, methods, and applications. We then discussfour lacunae: (1) the absence of demographic and cultural aspects does notaccount for the variation in how emotions are perceived, but instead assumesthey are universally experienced in the same manner; (2) the poor fit ofemotion categories from the two main emotion theories to the task; (3) the lackof standardized EA terminology hinders gap identification, comparison, andfuture goals; and (4) the absence of interdisciplinary research isolates EAfrom insights in other fields. Our work will enable more focused research intoEA and a more holistic approach to modeling emotions in NLP.</description><author>Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy</author><pubDate>Sat, 02 Mar 2024 14:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01222v1</guid></item><item><title>A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations</title><link>http://arxiv.org/abs/2403.01221v1</link><description>Counterfactual explanations constitute among the most popular methods foranalyzing the predictions of black-box systems since they can recommendcost-efficient and actionable changes to the input to turn an undesiredsystem's output into a desired output. While most of the existingcounterfactual methods explain a single instance, several real-world use cases,such as customer satisfaction, require the identification of a singlecounterfactual that can satisfy multiple instances (e.g. customers)simultaneously. In this work, we propose a flexible two-stage algorithm forfinding groups of instances along with cost-efficient multi-instancecounterfactual explanations. This is motivated by the fact that in mostprevious works the aspect of finding such groups is not addressed.</description><author>André Artelt, Andreas Gregoriades</author><pubDate>Sat, 02 Mar 2024 14:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01221v1</guid></item></channel></rss>