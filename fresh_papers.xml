<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 06 Jan 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models</title><link>http://arxiv.org/abs/2501.01428v1</link><description>In recent years, 2D Vision-Language Models (VLMs) have made significantstrides in image-text understanding tasks. However, their performance in 3Dspatial comprehension, which is critical for embodied intelligence, remainslimited. Recent advances have leveraged 3D point clouds and multi-view imagesas inputs, yielding promising results. However, we propose exploring a purelyvision-based solution inspired by human perception, which merely relies onvisual cues for 3D spatial understanding. This paper empirically investigatesthe limitations of VLMs in 3D spatial knowledge, revealing that their primaryshortcoming lies in the lack of global-local correspondence between the sceneand individual frames. To address this, we introduce GPT4Scene, a novel visualprompting paradigm in VLM training and inference that helps build theglobal-local relationship, significantly improving the 3D spatial understandingof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)image from the video and marks consistent object IDs across both frames and theBEV image. The model then inputs the concatenated BEV image and video frameswith markers. In zero-shot evaluations, GPT4Scene improves performance overclosed-source VLMs like GPT-4o. Additionally, we prepare a processed videodataset consisting of 165K text annotation to fine-tune open-source VLMs,achieving state-of-the-art performance on all 3D understanding tasks.Surprisingly, after training with the GPT4Scene paradigm, VLMs consistentlyimprove during inference, even without visual prompting and BEV image asexplicit correspondence. It demonstrates that the proposed paradigm helps VLMsdevelop an intrinsic ability to understand 3D scenes, which paves the way for anoninvasive approach to extending pre-trained VLMs for 3D scene understanding.</description><author>Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, Hengshuang Zhao</author><pubDate>Thu, 02 Jan 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01428v1</guid></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>http://arxiv.org/abs/2501.01427v1</link><description>Despite significant advancements in video generation, inserting a givenobject into videos remains a challenging task. The difficulty lies inpreserving the appearance details of the reference object and accuratelymodeling coherent motions at the same time. In this paper, we proposeVideoAnydoor, a zero-shot video object insertion framework with high-fidelitydetail preservation and precise motion control. Starting from a text-to-videomodel, we utilize an ID extractor to inject the global identity and leverage abox sequence to control the overall motion. To preserve the detailed appearanceand meanwhile support fine-grained motion control, we design a pixel warper. Ittakes the reference image with arbitrary key-points and the correspondingkey-point trajectories as inputs. It warps the pixel details according to thetrajectories and fuses the warped features with the diffusion U-Net, thusimproving detail preservation and supporting users in manipulating the motiontrajectories. In addition, we propose a training strategy involving both videosand static images with a reweight reconstruction loss to enhance insertionquality. VideoAnydoor demonstrates significant superiority over existingmethods and naturally supports various downstream applications (e.g., talkinghead generation, video virtual try-on, multi-region editing) withouttask-specific fine-tuning.</description><author>Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, Hengshuang Zhao</author><pubDate>Thu, 02 Jan 2025 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01427v1</guid></item><item><title>Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions</title><link>http://arxiv.org/abs/2501.01425v1</link><description>Controlling the movements of dynamic objects and the camera within generatedvideos is a meaningful yet challenging task. Due to the lack of datasets withcomprehensive motion annotations, existing algorithms can not simultaneouslycontrol the motions of both camera and objects, resulting in limitedcontrollability over generated contents. To address this issue and facilitatethe research in this field, we introduce a Synthetic Dataset for Free-FormMotion Control (SynFMC). The proposed SynFMC dataset includes diverse objectsand environments and covers various motion patterns according to specificrules, simulating common and complex real-world scenarios. The complete 6D poseinformation facilitates models learning to disentangle the motion effects fromobjects and the camera in a video. To validate the effectiveness andgeneralization of SynFMC, we further propose a method, Free-Form Motion Control(FMC). FMC enables independent or simultaneous control of object and cameramovements, producing high-fidelity videos. Moreover, it is compatible withvarious personalized text-to-image (T2I) models for different content styles.Extensive experiments demonstrate that the proposed FMC outperforms previousmethods across multiple scenarios.</description><author>Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, Dacheng Tao</author><pubDate>Thu, 02 Jan 2025 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01425v1</guid></item><item><title>Unifying Specialized Visual Encoders for Video Language Models</title><link>http://arxiv.org/abs/2501.01426v1</link><description>The recent advent of Large Language Models (LLMs) has ushered sophisticatedreasoning capabilities into the realm of video through Video Large LanguageModels (VideoLLMs). However, VideoLLMs currently rely on a single visionencoder for all of their visual processing, which limits the amount and type ofvisual information that can be conveyed to the LLM. Our method, MERV,Multi-Encoder Representation of Videos, instead leverages multiple frozenvisual encoders to create a unified representation of a video, providing theVideoLLM with a comprehensive set of specialized visual knowledge.Spatio-temporally aligning the features from each encoder allows us to tackle awider range of open-ended and multiple-choice video understanding questions andoutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracythan Video-LLaVA across the standard suite video understanding benchmarks,while also having a better Video-ChatGPT score. We also improve upon SeViLA,the previous best on zero-shot Perception Test accuracy, by 2.2%. MERVintroduces minimal extra parameters and trains faster than equivalentsingle-encoder methods while parallelizing the visual processing. Finally, weprovide qualitative evidence that MERV successfully captures domain knowledgefrom each of its encoders. Our results offer promising directions in utilizingmultiple vision encoders for comprehensive video understanding.</description><author>Jihoon Chung, Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou, Olga Russakovsky</author><pubDate>Thu, 02 Jan 2025 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01426v1</guid></item><item><title>Object-level Visual Prompts for Compositional Image Generation</title><link>http://arxiv.org/abs/2501.01424v1</link><description>We introduce a method for composing object-level visual prompts within atext-to-image diffusion model. Our approach addresses the task of generatingsemantically coherent compositions across diverse scenes and styles, similar tothe versatility and expressiveness offered by text prompts. A key challenge inthis task is to preserve the identity of the objects depicted in the inputvisual prompts, while also generating diverse compositions across differentimages. To address this challenge, we introduce a new KV-mixed cross-attentionmechanism, in which keys and values are learned from distinct visualrepresentations. The keys are derived from an encoder with a small bottleneckfor layout control, whereas the values come from a larger bottleneck encoderthat captures fine-grained appearance details. By mixing keys and values fromthese complementary sources, our model preserves the identity of the visualprompts while supporting flexible variations in object arrangement, pose, andcomposition. During inference, we further propose object-level compositionalguidance to improve the method's identity preservation and layout correctness.Results show that our technique produces diverse scene compositions thatpreserve the unique characteristics of each visual prompt, expanding thecreative potential of text-to-image generation.</description><author>Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman</author><pubDate>Thu, 02 Jan 2025 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01424v1</guid></item><item><title>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</title><link>http://arxiv.org/abs/2501.01423v1</link><description>Latent diffusion models with Transformer architectures excel at generatinghigh-fidelity images. However, recent studies reveal an optimization dilemma inthis two-stage design: while increasing the per-token feature dimension invisual tokenizers improves reconstruction quality, it requires substantiallylarger diffusion models and more training iterations to achieve comparablegeneration performance. Consequently, existing systems often settle forsub-optimal solutions, either producing visual artifacts due to informationloss within tokenizers or failing to converge fully due to expensivecomputation costs. We argue that this dilemma stems from the inherentdifficulty in learning unconstrained high-dimensional latent spaces. To addressthis, we propose aligning the latent space with pre-trained vision foundationmodels when training the visual tokenizers. Our proposed VA-VAE (Visionfoundation model Aligned Variational AutoEncoder) significantly expands thereconstruction-generation frontier of latent diffusion models, enabling fasterconvergence of Diffusion Transformers (DiT) in high-dimensional latent spaces.To exploit the full potential of VA-VAE, we build an enhanced DiT baseline withimproved training strategies and architecture designs, termed LightningDiT. Theintegrated system achieves state-of-the-art (SOTA) performance on ImageNet256x256 generation with an FID score of 1.35 while demonstrating remarkabletraining efficiency by reaching an FID score of 2.11 in just 64epochs--representing an over 21 times convergence speedup compared to theoriginal DiT. Models and codes are available at:https://github.com/hustvl/LightningDiT.</description><author>Jingfeng Yao, Xinggang Wang</author><pubDate>Thu, 02 Jan 2025 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01423v1</guid></item><item><title>Multi-Modal Video Feature Extraction for Popularity Prediction</title><link>http://arxiv.org/abs/2501.01422v1</link><description>This work aims to predict the popularity of short videos using the videosthemselves and their related features. Popularity is measured by four keyengagement metrics: view count, like count, comment count, and share count.This study employs video classification models with different architectures andtraining methods as backbone networks to extract video modality features.Meanwhile, the cleaned video captions are incorporated into a carefullydesigned prompt framework, along with the video, as input for video-to-textgeneration models, which generate detailed text-based video contentunderstanding. These texts are then encoded into vectors using a pre-trainedBERT model. Based on the six sets of vectors mentioned above, a neural networkis trained for each of the four prediction metrics. Moreover, the studyconducts data mining and feature engineering based on the video and tabulardata, constructing practical features such as the total frequency of hashtagappearances, the total frequency of mention appearances, video duration, framecount, frame rate, and total time online. Multiple machine learning models aretrained, and the most stable model, XGBoost, is selected. Finally, thepredictions from the neural network and XGBoost models are averaged to obtainthe final result.</description><author>Haixu Liu, Wenning Wang, Haoxiang Zheng, Penghao Jiang, Qirui Wang, Ruiqing Yan, Qiuzhuang Sun</author><pubDate>Thu, 02 Jan 2025 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01422v1</guid></item><item><title>GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</title><link>http://arxiv.org/abs/2404.14403v2</link><description>The success of image generative models has enabled us to build methods thatcan edit images based on text or other user input. However, these methods arebespoke, imprecise, require additional information, or are limited to only 2Dimage edits. We present GeoDiffuser, a zero-shot optimization-based method thatunifies common 2D and 3D image-based object editing capabilities into a singlemethod. Our key insight is to view image editing operations as geometrictransformations. We show that these transformations can be directlyincorporated into the attention layers in diffusion models to implicitlyperform editing operations. Our training-free optimization method uses anobjective function that seeks to preserve object style but generate plausibleimages, for instance with accurate lighting and shadows. It also inpaintsdisoccluded parts of the image where the object was originally located. Given anatural image and user input, we segment the foreground object using SAM andestimate a corresponding transform which is used by our optimization approachfor editing. GeoDiffuser can perform common 2D and 3D edits like objecttranslation, 3D rotation, and removal. We present quantitative results,including a perceptual study, that shows how our approach is better thanexisting methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html formore information.</description><author>Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar</author><pubDate>Thu, 02 Jan 2025 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14403v2</guid></item><item><title>R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization</title><link>http://arxiv.org/abs/2501.01421v1</link><description>Learning-based visual localization methods that use scene coordinateregression (SCR) offer the advantage of smaller map sizes. However, on datasetswith complex illumination changes or image-level ambiguities, it remains a lessrobust alternative to feature matching methods. This work aims to close thegap. We introduce a covisibility graph-based global encoding learning and dataaugmentation strategy, along with a depth-adjusted reprojection loss tofacilitate implicit triangulation. Additionally, we revisit the networkarchitecture and local feature extraction module. Our method achievesstate-of-the-art on challenging large-scale datasets without relying on networkensembles or 3D supervision. On Aachen Day-Night, we are 10$\times$ moreaccurate than previous SCR methods with similar map sizes and require at least5$\times$ smaller map sizes than any other SCR method while still deliveringsuperior accuracy. Code will be available at: https://github.com/cvg/scrstudio .</description><author>Xudong Jiang, Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Marc Pollefeys</author><pubDate>Thu, 02 Jan 2025 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01421v1</guid></item><item><title>A Multi-task Supervised Compression Model for Split Computing</title><link>http://arxiv.org/abs/2501.01420v1</link><description>Split computing ($\neq$ split learning) is a promising approach to deeplearning models for resource-constrained edge computing systems, where weaksensor (mobile) devices are wirelessly connected to stronger edge serversthrough channels with limited communication capacity. State-of-theart work onsplit computing presents methods for single tasks such as image classification,object detection, or semantic segmentation. The application of existing methodsto multitask problems degrades model accuracy and/or significantly increaseruntime latency. In this study, we propose Ladon, the first multi-task-headsupervised compression model for multi-task split computing. Experimentalresults show that the multi-task supervised compression model eitheroutperformed or rivaled strong lightweight baseline models in terms ofpredictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasetswhile learning compressed representations at its early layers. Furthermore, ourmodels reduced end-to-end latency (by up to 95.4%) and energy consumption ofmobile devices (by up to 88.2%) in multi-task split computing scenarios.</description><author>Yoshitomo Matsubara, Matteo Mendula, Marco Levorato</author><pubDate>Thu, 02 Jan 2025 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01420v1</guid></item><item><title>Hierarchical Alignment-enhanced Adaptive Grounding Network for Generalized Referring Expression Comprehension</title><link>http://arxiv.org/abs/2501.01416v1</link><description>In this work, we address the challenging task of Generalized ReferringExpression Comprehension (GREC). Compared to the classic Referring ExpressionComprehension (REC) that focuses on single-target expressions, GREC extends thescope to a more practical setting by further encompassing no-target andmulti-target expressions. Existing REC methods face challenges in handling thecomplex cases encountered in GREC, primarily due to their fixed output andlimitations in multi-modal representations. To address these issues, we proposea Hierarchical Alignment-enhanced Adaptive Grounding Network (HieA2G) for GREC,which can flexibly deal with various types of referring expressions. First, aHierarchical Multi-modal Semantic Alignment (HMSA) module is proposed toincorporate three levels of alignments, including word-object, phrase-object,and text-image alignment. It enables hierarchical cross-modal interactionsacross multiple levels to achieve comprehensive and robust multi-modalunderstanding, greatly enhancing grounding ability for complex cases. Then, toaddress the varying number of target objects in GREC, we introduce an AdaptiveGrounding Counter (AGC) to dynamically determine the number of output targets.Additionally, an auxiliary contrastive loss is employed in AGC to enhanceobject-counting ability by pulling in multi-modal features with the samecounting and pushing away those with different counting. Extensive experimentalresults show that HieA2G achieves new state-of-the-art performance on thechallenging GREC task and also the other 4 tasks, including REC, PhraseGrounding, Referring Expression Segmentation (RES), and Generalized ReferringExpression Segmentation (GRES), demonstrating the remarkable superiority andgeneralizability of the proposed HieA2G.</description><author>Yaxian Wang, Henghui Ding, Shuting He, Xudong Jiang, Bifan Wei, Jun Liu</author><pubDate>Thu, 02 Jan 2025 18:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01416v1</guid></item><item><title>Deep Discrete Encoders: Identifiable Deep Generative Models for Rich Data with Discrete Latent Layers</title><link>http://arxiv.org/abs/2501.01414v1</link><description>In the era of generative AI, deep generative models (DGMs) with latentrepresentations have gained tremendous popularity. Despite their impressiveempirical performance, the statistical properties of these models remainunderexplored. DGMs are often overparametrized, non-identifiable, anduninterpretable black boxes, raising serious concerns when deploying them inhigh-stakes applications. Motivated by this, we propose an interpretable deepgenerative modeling framework for rich data types with discrete latent layers,called Deep Discrete Encoders (DDEs). A DDE is a directed graphical model withmultiple binary latent layers. Theoretically, we propose transparentidentifiability conditions for DDEs, which imply progressively smaller sizes ofthe latent layers as they go deeper. Identifiability ensures consistentparameter estimation and inspires an interpretable design of the deeparchitecture. Computationally, we propose a scalable estimation pipeline of alayerwise nonlinear spectral initialization followed by a penalized stochasticapproximation EM algorithm. This procedure can efficiently estimate models withexponentially many latent components. Extensive simulation studies validate ourtheoretical results and demonstrate the proposed algorithms' excellentperformance. We apply DDEs to three diverse real datasets for hierarchicaltopic modeling, image representation learning, response time modeling ineducational testing, and obtain interpretable findings.</description><author>Seunghyun Lee, Yuqi Gu</author><pubDate>Thu, 02 Jan 2025 18:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01414v1</guid></item><item><title>On Unifying Video Generation and Camera Pose Estimation</title><link>http://arxiv.org/abs/2501.01409v1</link><description>Inspired by the emergent 3D capabilities in image generators, we explorewhether video generators similarly exhibit 3D awareness. Usingstructure-from-motion (SfM) as a benchmark for 3D tasks, we investigate ifintermediate features from OpenSora, a video generation model, can supportcamera pose estimation. We first examine native 3D awareness in videogeneration features by routing raw intermediate outputs to SfM-predictionmodules like DUSt3R. Then, we explore the impact of fine-tuning on camera poseestimation to enhance 3D awareness. Results indicate that while video generatorfeatures have limited inherent 3D awareness, task-specific supervisionsignificantly boosts their accuracy for camera pose estimation, resulting incompetitive performance. The proposed unified model, named JOG3R, producescamera pose estimates with competitive quality without degrading videogeneration quality.</description><author>Chun-Hao Paul Huang, Jae Shin Yoon, Hyeonho Jeong, Niloy Mitra, Duygu Ceylan</author><pubDate>Thu, 02 Jan 2025 18:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01409v1</guid></item><item><title>Nested Attention: Semantic-aware Attention Values for Concept Personalization</title><link>http://arxiv.org/abs/2501.01407v1</link><description>Personalizing text-to-image models to generate images of specific subjectsacross diverse scenes and styles is a rapidly advancing field. Currentapproaches often face challenges in maintaining a balance between identitypreservation and alignment with the input text prompt. Some methods rely on asingle textual token to represent a subject, which limits expressiveness, whileothers employ richer representations but disrupt the model's prior, diminishingprompt alignment. In this work, we introduce Nested Attention, a novelmechanism that injects a rich and expressive image representation into themodel's existing cross-attention layers. Our key idea is to generatequery-dependent subject values, derived from nested attention layers that learnto select relevant subject features for each region in the generated image. Weintegrate these nested layers into an encoder-based personalization method, andshow that they enable high identity preservation while adhering to input textprompts. Our approach is general and can be trained on various domains.Additionally, its prior preservation allows us to combine multiple personalizedsubjects from different domains in a single image.</description><author>Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or</author><pubDate>Thu, 02 Jan 2025 18:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01407v1</guid></item><item><title>nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images Segmentation</title><link>http://arxiv.org/abs/2501.01406v1</link><description>This paper provides a novel 3D medical image segmentation model structurecalled nnY-Net. This name comes from the fact that our model adds across-attention module at the bottom of the U-net structure to form a Ystructure. We integrate the advantages of the two latest SOTA models, MedNeXtand SwinUNETR, and use Swin Transformer as the encoder and ConvNeXt as thedecoder to innovatively design the Swin-NeXt structure. Our model uses thelowest-level feature map of the encoder as Key and Value and uses patientfeatures such as pathology and treatment information as Query to calculate theattention weights in a Cross Attention module. Moreover, we simplify some pre-and post-processing as well as data enhancement methods in 3D imagesegmentation based on the dynUnet and nnU-net frameworks. We integrate ourproposed Swin-NeXt with Cross-Attention framework into this framework. Last, weconstruct a DiceFocalCELoss to improve the training efficiency for the unevendata convergence of voxel classification.</description><author>Haixu Liu, Zerui Tao, Wenzhen Dong, Qiuzhuang Sun</author><pubDate>Thu, 02 Jan 2025 18:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01406v1</guid></item><item><title>MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes</title><link>http://arxiv.org/abs/2412.19260v2</link><description>Several studies showed that Large Language Models (LLMs) can answer medicalquestions correctly, even outperforming the average human score in some medicalexams. However, to our knowledge, no study has been conducted to assess theability of language models to validate existing or generated medical text forcorrectness and consistency. In this paper, we introduce MEDEC(https://github.com/abachaa/MEDEC), the first publicly available benchmark formedical error detection and correction in clinical notes, covering five typesof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and CausalOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notesfrom three US hospital systems that were not previously seen by any LLM. Thedataset has been used for the MEDIQA-CORR shared task to evaluate seventeenparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe thedata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting andcorrecting medical errors requiring both medical knowledge and reasoningcapabilities. We also conducted a comparative study where two medical doctorsperformed the same task on the MEDEC test set. The results showed that MEDEC isa sufficiently challenging benchmark to assess the ability of models tovalidate existing or generated notes and to correct medical errors. We alsofound that although recent LLMs have a good performance in error detection andcorrection, they are still outperformed by medical doctors in these tasks. Wediscuss the potential factors behind this gap, the insights from ourexperiments, the limitations of current evaluation metrics, and share potentialpointers for future research.</description><author>Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin</author><pubDate>Thu, 02 Jan 2025 18:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19260v2</guid></item><item><title>Sparsely Multimodal Data Fusion</title><link>http://arxiv.org/abs/2403.20280v2</link><description>Multimodal data fusion is essential for applications requiring theintegration of diverse data sources, especially in the presence of incompleteor sparsely available modalities. This paper presents a comparative study ofthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,and Everything at Once (EAO), to evaluate their performance on sparselymultimodal data. MCA introduces fusion embeddings for all combinations of inputmodalities and uses attention masking to create distinct attention channels,enabling flexible and efficient data fusion. Experiments on two datasets withfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperformsZorro across ranking, recall, regression, and classification tasks andoutperforms EAO across regression and classification tasks. MCA achievessuperior performance by maintaining robust uniformity across unimodal andfusion embeddings. While EAO performs best in ranking metrics due to itsapproach of forming fusion embeddings post-inference, it underperforms indownstream tasks requiring multimodal interactions. These results highlight theimportance of contrasting all modality combinations in constructing embeddingspaces and offers insights into the design of multimodal architectures forreal-world applications with incomplete data.</description><author>Josiah Bjorgaard</author><pubDate>Thu, 02 Jan 2025 18:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20280v2</guid></item><item><title>Best Transition Matrix Esitimation or Best Label Noise Robustness Classifier? Two Possible Methods to Enhance the Performance of T-revision</title><link>http://arxiv.org/abs/2501.01402v1</link><description>Label noise refers to incorrect labels in a dataset caused by human errors orcollection defects, which is common in real-world applications and cansignificantly reduce the accuracy of models. This report explores how toestimate noise transition matrices and construct deep learning classifiers thatare robust against label noise. In cases where the transition matrix is known,we apply forward correction and importance reweighting methods to correct theimpact of label noise using the transition matrix. When the transition matrixis unknown or inaccurate, we use the anchor point assumption and T-Revisionseries methods to estimate or correct the noise matrix. In this study, wefurther improved the T-Revision method by developing T-Revision-Alpha andT-Revision-Softmax to enhance stability and robustness. Additionally, wedesigned and implemented two baseline classifiers, a Multi-Layer Perceptron(MLP) and ResNet-18, based on the cross-entropy loss function. We compared theperformance of these methods on predicting clean labels and estimatingtransition matrices using the FashionMINIST dataset with known noise transitionmatrices. For the CIFAR-10 dataset, where the noise transition matrix isunknown, we estimated the noise matrix and evaluated the ability of the methodsto predict clean labels.</description><author>Haixu Liu, Zerui Tao, Naihui Zhang, Sixing Liu</author><pubDate>Thu, 02 Jan 2025 18:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01402v1</guid></item><item><title>A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models</title><link>http://arxiv.org/abs/2501.01394v1</link><description>Transformer-based models for time series forecasting (TSF) have attractedsignificant attention in recent years due to their effectiveness andversatility. However, these models often require extensive hyperparameteroptimization (HPO) to achieve the best possible performance, and a unifiedpipeline for HPO in transformer-based TSF remains lacking. In this paper, wepresent one such pipeline and conduct extensive experiments on severalstate-of-the-art (SOTA) transformer-based TSF models. These experiments areconducted on standard benchmark datasets to evaluate and compare theperformance of different models, generating practical insights and examples.Our pipeline is generalizable beyond transformer-based architectures and can beapplied to other SOTA models, such as Mamba and TimeMixer, as demonstrated inour experiments. The goal of this work is to provide valuable guidance to bothindustry practitioners and academic researchers in efficiently identifyingoptimal hyperparameters suited to their specific domain applications. The codeand complete experimental results are available on GitHub.</description><author>Jingjing Xu, Caesar Wu, Yuan-Fang Li, Grégoire Danoy, Pascal Bouvry</author><pubDate>Thu, 02 Jan 2025 18:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01394v1</guid></item><item><title>Familiarity-Based Open-Set Recognition Under Adversarial Attacks</title><link>http://arxiv.org/abs/2311.05006v2</link><description>Open-set recognition (OSR), the identification of novel categories, can be acritical component when deploying classification models in real-worldapplications. Recent work has shown that familiarity-based scoring rules suchas the Maximum Softmax Probability (MSP) or the Maximum Logit Score (MLS) arestrong baselines when the closed-set accuracy is high. However, one of thepotential weaknesses of familiarity-based OSR are adversarial attacks. Here, westudy gradient-based adversarial attacks on familiarity scores for both typesof attacks, False Familiarity and False Novelty attacks, and evaluate theireffectiveness in informed and uninformed settings on TinyImageNet. Furthermore,we explore how novel and familiar samples react to adversarial attacks andformulate the adversarial reaction score as an alternative OSR scoring rule,which shows a high correlation with the MLS familiarity score.</description><author>Philip Enevoldsen, Christian Gundersen, Nico Lang, Serge Belongie, Christian Igel</author><pubDate>Thu, 02 Jan 2025 18:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05006v2</guid></item><item><title>Learning 3D Garment Animation from Trajectories of A Piece of Cloth</title><link>http://arxiv.org/abs/2501.01393v1</link><description>Garment animation is ubiquitous in various applications, such as virtualreality, gaming, and film producing. Recently, learning-based approaches obtaincompelling performance in animating diverse garments under versatile scenarios.Nevertheless, to mimic the deformations of the observed garments, data-drivenmethods require large scale of garment data, which are both resource-wiseexpensive and time-consuming. In addition, forcing models to match the dynamicsof observed garment animation may hinder the potentials to generalize to unseencases. In this paper, instead of using garment-wise supervised-learning weadopt a disentangled scheme to learn how to animate observed garments: 1).learning constitutive behaviors from the observed cloth; 2). dynamicallyanimate various garments constrained by the learned constitutive laws.Specifically, we propose Energy Unit network (EUNet) to model the constitutiverelations in the format of energy. Without the priors from analytical physicsmodels and differentiable simulation engines, EUNet is able to directly capturethe constitutive behaviors from the observed piece of cloth and uniformlydescribes the change of energy caused by deformations, such as stretching andbending. We further apply the pre-trained EUNet to animate various garmentsbased on energy optimizations. The disentangled scheme alleviates the need ofgarment data and enables us to utilize the dynamics of a piece of cloth foranimating garments. Experiments show that while EUNet effectively delivers theenergy gradients due to the deformations, models constrained by EUNet achievemore stable and physically plausible performance comparing with those trainedin garment-wise supervised manner. Code is available athttps://github.com/ftbabi/EUNet_NeurIPS2024.git .</description><author>Yidi Shao, Chen Change Loy, Bo Dai</author><pubDate>Thu, 02 Jan 2025 18:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01393v1</guid></item><item><title>ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer</title><link>http://arxiv.org/abs/2501.01392v1</link><description>Prostate cancer, a growing global health concern, necessitates precisediagnostic tools, with Magnetic Resonance Imaging (MRI) offeringhigh-resolution soft tissue imaging that significantly enhances diagnosticaccuracy. Recent advancements in explainable AI and representation learninghave significantly improved prostate cancer diagnosis by enabling automated andprecise lesion classification. However, existing explainable AI methods,particularly those based on frameworks like generative adversarial networks(GANs), are predominantly developed for natural image generation, and theirapplication to medical imaging often leads to suboptimal performance due to theunique characteristics and complexity of medical image. To address thesechallenges, our paper introduces three key contributions. First, we proposeProjectedEx, a generative framework that provides interpretable,multi-attribute explanations, effectively linking medical image features toclassifier decisions. Second, we enhance the encoder module by incorporatingfeature pyramids, which enables multiscale feedback to refine the latent spaceand improves the quality of generated explanations. Additionally, we conductcomprehensive experiments on both the generator and classifier, demonstratingthe clinical relevance and effectiveness of ProjectedEx in enhancinginterpretability and supporting the adoption of AI in medical settings. Codewill be released at https://github.com/Richardqiyi/ProjectedEx</description><author>Xuyin Qi, Zeyu Zhang, Aaron Berliano Handoko, Huazhan Zheng, Mingxi Chen, Ta Duc Huy, Vu Minh Hieu Phan, Lei Zhang, Linqi Cheng, Shiyu Jiang, Zhiwei Zhang, Zhibin Liao, Yang Zhao, Minh-Son To</author><pubDate>Thu, 02 Jan 2025 18:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01392v1</guid></item><item><title>Accurate RNA 3D structure prediction using a language model-based deep learning approach</title><link>http://arxiv.org/abs/2207.01586v3</link><description>Accurate prediction of RNA three-dimensional (3D) structure remains anunsolved challenge. Determining RNA 3D structures is crucial for understandingtheir functions and informing RNA-targeting drug development and syntheticbiology design. The structural flexibility of RNA, which leads to scarcity ofexperimentally determined data, complicates computational prediction efforts.Here, we present RhoFold+, an RNA language model-based deep learning methodthat accurately predicts 3D structures of single-chain RNAs from sequences. Byintegrating an RNA language model pre-trained on ~23.7 million RNA sequencesand leveraging techniques to address data scarcity, RhoFold+ offers a fullyautomated end-to-end pipeline for RNA 3D structure prediction. Retrospectiveevaluations on RNA-Puzzles and CASP15 natural RNA targets demonstrateRhoFold+'s superiority over existing methods, including human expert groups.Its efficacy and generalizability are further validated through cross-familyand cross-type assessments, as well as time-censored benchmarks. Additionally,RhoFold+ predicts RNA secondary structures and inter-helical angles, providingempirically verifiable features that broaden its applicability to RNA structureand function studies.</description><author>Tao Shen, Zhihang Hu, Siqi Sun, Di Liu, Felix Wong, Jiuming Wang, Jiayang Chen, Yixuan Wang, Liang Hong, Jin Xiao, Liangzhen Zheng, Tejas Krishnamoorthi, Irwin King, Sheng Wang, Peng Yin, James J. Collins, Yu Li</author><pubDate>Thu, 02 Jan 2025 18:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01586v3</guid></item><item><title>OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios</title><link>http://arxiv.org/abs/2501.01384v1</link><description>With the rapid development of large language models, researchers have createdincreasingly advanced spoken dialogue systems that can naturally converse withhumans. However, these systems still struggle to handle the full complexity ofreal-world conversations, including audio events, musical contexts, andemotional expressions, mainly because current dialogue datasets are constrainedin both scale and scenario diversity. In this paper, we propose leveragingsynthetic data to enhance the dialogue models across diverse scenarios. Weintroduce ShareChatX, the first comprehensive, large-scale dataset for spokendialogue that spans diverse scenarios. Based on this dataset, we introduceOmniChat, a multi-turn dialogue system with a heterogeneous feature fusionmodule, designed to optimize feature selection in different dialogue contexts.In addition, we explored critical aspects of training dialogue systems usingsynthetic data. Through comprehensive experimentation, we determined the idealbalance between synthetic and real data, achieving state-of-the-art results onthe real-world dialogue dataset DailyTalk. We also highlight the crucialimportance of synthetic data in tackling diverse, complex dialogue scenarios,especially those involving audio and music. For more details, please visit ourdemo page at \url{https://sharechatx.github.io/}.</description><author>Xize Cheng, Dongjie Fu, Xiaoda Yang, Minghui Fang, Ruofan Hu, Jingyu Lu, Bai Jionghao, Zehan Wang, Shengpeng Ji, Rongjie Huang, Linjun Li, Yu Chen, Tao Jin, Zhou Zhao</author><pubDate>Thu, 02 Jan 2025 17:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01384v1</guid></item><item><title>Text2Data: Low-Resource Data Generation with Textual Control</title><link>http://arxiv.org/abs/2402.10941v2</link><description>Natural language serves as a common and straightforward signal for humans tointeract seamlessly with machines. Recognizing the importance of thisinterface, the machine learning community is investing considerable effort ingenerating data that is semantically coherent with textual instructions. Whilestrides have been made in text-to-data generation spanning image editing, audiosynthesis, video creation, and beyond, low-resource areas characterized byexpensive annotations or complex data structures, such as molecules, motiondynamics, and time series, often lack textual labels. This deficiency impedessupervised learning, thereby constraining the application of advancedgenerative models for text-to-data tasks. In response to these challenges inthe low-resource scenario, we propose Text2Data, a novel approach that utilizesunlabeled data to understand the underlying data distribution through anunsupervised diffusion model. Subsequently, it undergoes controllablefinetuning via a novel constraint optimization-based learning objective thatensures controllability and effectively counteracts catastrophic forgetting.Comprehensive experiments demonstrate that Text2Data is able to achieveenhanced performance regarding controllability across various modalities,including molecules, motions and time series, when compared to existingbaselines.</description><author>Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese</author><pubDate>Thu, 02 Jan 2025 17:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10941v2</guid></item><item><title>Des-q: a quantum algorithm to provably speedup retraining of decision trees</title><link>http://arxiv.org/abs/2309.09976v5</link><description>Decision trees are widely adopted machine learning models due to theirsimplicity and explainability. However, as training data size grows, standardmethods become increasingly slow, scaling polynomially with the number oftraining examples. In this work, we introduce Des-q, a novel quantum algorithmto construct and retrain decision trees for regression and binaryclassification tasks. Assuming the data stream produces small, periodicincrements of new training examples, Des-q significantly reduces the treeretraining time. Des-q achieves a logarithmic complexity in the combined totalnumber of old and new examples, even accounting for the time needed to load thenew samples into quantum-accessible memory. Our approach to grow the tree fromany given node involves performing piecewise linear splits to generate multiplehyperplanes, thus partitioning the input feature space into distinct regions.To determine the suitable anchor points for these splits, we develop anefficient quantum-supervised clustering method, building upon the q-meansalgorithm introduced by Kerenidis et al. We benchmark the simulated version ofDes-q against the state-of-the-art classical methods on multiple data sets andobserve that our algorithm exhibits similar performance to the state-of-the-artdecision trees while significantly speeding up the periodic tree retraining.</description><author>Niraj Kumar, Romina Yalovetzky, Changhao Li, Pierre Minssen, Marco Pistoia</author><pubDate>Thu, 02 Jan 2025 17:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09976v5</guid></item><item><title>Training Medical Large Vision-Language Models with Abnormal-Aware Feedback</title><link>http://arxiv.org/abs/2501.01377v1</link><description>Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulateextensive medical knowledge, demonstrate excellent capabilities inunderstanding medical images and responding to human queries based on theseimages. However, there remain challenges in visual localization in medicalimages, which is crucial for abnormality detection and interpretation. Toaddress these issues, we propose a novel UMed-LVLM designed with UnveilingMedical abnormalities. Specifically, we collect a Medical AbnormalitiesUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLMtraining. To collect MAU dataset, we propose a prompt method utilizing theGPT-4V to generate diagnoses based on identified abnormal areas in medicalimages. Moreover, the two-stage training method includes Abnormal-AwareInstruction Tuning and Abnormal-Aware Rewarding, comprising AbnormalLocalization Rewarding and Vision Relevance Rewarding. Experimental resultsdemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying andunderstanding medical abnormality. In addition, this work shows that enhancingthe abnormality detection capabilities of Med-LVLMs significantly improvestheir understanding of medical images and generalization capability.</description><author>Yucheng Zhou, Lingran Song, Jianbing Shen</author><pubDate>Thu, 02 Jan 2025 17:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01377v1</guid></item><item><title>Iris Recognition for Infants</title><link>http://arxiv.org/abs/2501.01375v1</link><description>Non-invasive, efficient, physical token-less, accurate and stableidentification methods for newborns may prevent baby swapping at birth, limitbaby abductions and improve post-natal health monitoring across geographies,within the context of both the formal (i.e., hospitals) and informal (i.e.,humanitarian and fragile settings) health sectors. This paper explores thefeasibility of application iris recognition to build biometric identifiers for4-6 week old infants. We (a) collected near infrared (NIR) iris images from 17infants using a specially-designed NIR iris sensor; (b) evaluated six irisrecognition methods to assess readiness of the state-of-the-art irisrecognition to be applied to newborns and infants; (c) proposed a newsegmentation model that correctly detects iris texture within infants irisimages, and coupled it with several iris texture encoding approaches to offer,to the first of our knowledge, a fully-operational infant iris recognitionsystem; and, (d) trained a StyleGAN-based model to synthesize iris imagesmimicking samples acquired from infants to deliver to the research communityprivacy-safe infant iris images. The proposed system, incorporating thespecially-designed iris sensor and segmenter, and applied to the collectedinfant iris samples, achieved Equal Error Rate (EER) of 3\% and Area Under ROCCurve (AUC) of 99\%, compared to EER$\geq$20\% and AUC$\leq$88\% obtained forstate of the art adult iris recognition systems. This suggests that it may befeasible to design methods that succesfully extract biometric features frominfant irises.</description><author>Rasel Ahmed Bhuiyan, Mateusz Trokielewicz, Piotr Maciejewicz, Sherri Bucher, Adam Czajka</author><pubDate>Thu, 02 Jan 2025 17:34:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01375v1</guid></item><item><title>Task Singular Vectors: Reducing Task Interference in Model Merging</title><link>http://arxiv.org/abs/2412.00081v2</link><description>Task Arithmetic has emerged as a simple yet effective method to merge modelswithout additional training. However, by treating entire networks as flatparameter vectors, it overlooks key structural information and is susceptibleto task interference. In this paper, we study task vectors at the layer level,focusing on task layer matrices and their singular value decomposition. Inparticular, we concentrate on the resulting singular vectors, which we refer toas Task Singular Vectors (TSV). Recognizing that layer task matrices are oftenlow-rank, we propose TSV-Compress (TSV-C), a simple procedure that compressesthem to 10% of their original size while retaining 99% of accuracy. We furtherleverage this low-rank space to define a new measure of task interference basedon the interaction of singular vectors from different tasks. Building on thesefindings, we introduce TSV-Merge (TSV-M), a novel model merging approach thatcombines compression with interference reduction, significantly outperformingexisting methods.</description><author>Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, Emanuele Rodolà</author><pubDate>Thu, 02 Jan 2025 17:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00081v2</guid></item><item><title>ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI</title><link>http://arxiv.org/abs/2501.01372v1</link><description>Background: Late Gadolinium Enhancement (LGE) imaging is the gold standardfor assessing myocardial fibrosis and scarring, with left ventricular (LV) LGEextent predicting major adverse cardiac events (MACE). Despite its importance,routine LGE-based LV scar quantification is hindered by labor-intensive manualsegmentation and inter-observer variability. Methods: We propose ScarNet, ahybrid model combining a transformer-based encoder from the Medical SegmentAnything Model (MedSAM) with a convolution-based U-Net decoder, enhanced bytailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathypatients with expert segmentations of myocardial and scar boundaries and testedon 184 separate patients. Results: ScarNet achieved robust scar segmentation in184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944),significantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) andnnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lowerbias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias:-13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlosimulations with noise perturbations, ScarNet achieved significantly higherscar Dice (0.892 \pm 0.053, CoV = 5.9%) than MedSAM (0.048 \pm 0.112, CoV =233.3%) and nnU-Net (0.615 \pm 0.537, CoV = 28.7%). Conclusion: ScarNetoutperformed MedSAM and nnU-Net in accurately segmenting myocardial and scarboundaries in LGE images. The model exhibited robust performance across diverseimage qualities and scar patterns.</description><author>Neda Tavakoli, Amir Ali Rahsepar, Brandon C. Benefield, Daming Shen, Santiago López-Tapia, Florian Schiffers, Jeffrey J. Goldberger, Christine M. Albert, Edwin Wu, Aggelos K. Katsaggelos, Daniel C. Lee, Daniel Kim</author><pubDate>Thu, 02 Jan 2025 17:30:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01372v1</guid></item><item><title>CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering</title><link>http://arxiv.org/abs/2501.01371v1</link><description>Recent Vision-Language Models (VLMs) have demonstrated remarkablecapabilities in visual understanding and reasoning, and in particular onmultiple-choice Visual Question Answering (VQA). Still, these models can makedistinctly unnatural errors, for example, providing (wrong) answers tounanswerable VQA questions, such as questions asking about objects that do notappear in the image. To address this issue, we propose CLIP-UP: CLIP-basedUnanswerable Problem detection, a novel lightweight method for equipping VLMswith the ability to withhold answers to unanswerable questions. By leveragingCLIP to extract question-image alignment information, CLIP-UP requires onlyefficient training of a few additional layers, while keeping the original VLMs'weights unchanged. Tested across LLaVA models, CLIP-UP achievesstate-of-the-art results on the MM-UPD benchmark for assessing unanswerabilityin multiple-choice VQA, while preserving the original performance on othertasks.</description><author>Ben Vardi, Oron Nir, Ariel Shamir</author><pubDate>Thu, 02 Jan 2025 17:30:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01371v1</guid></item><item><title>Embedding-based Approaches to Hyperpartisan News Detection</title><link>http://arxiv.org/abs/2501.01370v1</link><description>In this paper, we describe our systems in which the objective is to determinewhether a given news article could be considered as hyperpartisan.Hyperpartisan news is news that takes an extremely polarized politicalstandpoint with an intention of creating political divide among the public. Weattempted several approaches, including n-grams, sentiment analysis, as well assentence and document representation using pre-tained ELMo. Our best systemusing pre-trained ELMo with Bidirectional LSTM achieved an accuracy of 83%through 10-fold cross-validation without much hyperparameter tuning.</description><author>Karthik Mohan, Pengyu Chen</author><pubDate>Thu, 02 Jan 2025 17:29:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01370v1</guid></item><item><title>In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before An Ongoing Trajectory Terminates</title><link>http://arxiv.org/abs/2410.15612v3</link><description>Inverse reinforcement learning (IRL) aims to learn a reward function and acorresponding policy that best fit the demonstrated trajectories of an expert.However, current IRL works cannot learn incrementally from an ongoingtrajectory because they have to wait to collect at least one completetrajectory to learn. To bridge the gap, this paper considers the problem oflearning a reward function and a corresponding policy while observing theinitial state-action pair of an ongoing trajectory and keeping updating thelearned reward and policy when new state-action pairs of the ongoing trajectoryare observed. We formulate this problem as an online bi-level optimizationproblem where the upper level dynamically adjusts the learned reward accordingto the newly observed state-action pairs with the help of a meta-regularizationterm, and the lower level learns the corresponding policy. We propose a novelalgorithm to solve this problem and guarantee that the algorithm achievessub-linear local regret $O(\sqrt{T}+\log T+\sqrt{T}\log T)$. If the rewardfunction is linear, we prove that the proposed algorithm achieves sub-linearregret $O(\log T)$. Experiments are used to validate the proposed algorithm.</description><author>Shicheng Liu, Minghui Zhu</author><pubDate>Thu, 02 Jan 2025 17:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15612v3</guid></item><item><title>The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation</title><link>http://arxiv.org/abs/2412.14323v2</link><description>Translating between languages with drastically different grammaticalconventions poses challenges, not just for human interpreters but also formachine translation systems. In this work, we specifically target thetranslation challenges posed by attributive nouns in Chinese, which frequentlycause ambiguities in English translation. By manually inserting the omittedparticle X ('DE'). In news article titles from the Penn Chinese DiscourseTreebank, we developed a targeted dataset to fine-tune Hugging Face Chinese toEnglish translation models, specifically improving how this critical functionword is handled. This focused approach not only complements the broaderstrategies suggested by previous studies but also offers a practicalenhancement by specifically addressing a common error type in Chinese-Englishtranslation.</description><author>Lisa Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos</author><pubDate>Thu, 02 Jan 2025 17:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14323v2</guid></item><item><title>Test-time Controllable Image Generation by Explicit Spatial Constraint Enforcement</title><link>http://arxiv.org/abs/2501.01368v1</link><description>Recent text-to-image generation favors various forms of spatial conditions,e.g., masks, bounding boxes, and key points. However, the majority of the priorart requires form-specific annotations to fine-tune the original model, leadingto poor test-time generalizability. Meanwhile, existing training-free methodswork well only with simplified prompts and spatial conditions. In this work, wepropose a novel yet generic test-time controllable generation method that aimsat natural text prompts and complex conditions. Specifically, we decouplespatial conditions into semantic and geometric conditions and then enforcetheir consistency during the image-generation process individually. As for theformer, we target bridging the gap between the semantic condition and textprompts, as well as the gap between such condition and the attention map fromdiffusion models. To achieve this, we propose to first complete the promptw.r.t. semantic condition, and then remove the negative impact of distractingprompt words by measuring their statistics in attention maps as well asdistances in word space w.r.t. this condition. To further cope with the complexgeometric conditions, we introduce a geometric transform module, in whichRegion-of-Interests will be identified in attention maps and further used totranslate category-wise latents w.r.t. geometric condition. More importantly,we propose a diffusion-based latents-refill method to explicitly remove theimpact of latents at the RoI, reducing the artifacts on generated images.Experiments on Coco-stuff dataset showcase 30$\%$ relative boost compared toSOTA training-free methods on layout consistency evaluation metrics.</description><author>Z. Zhang, B. Liu, J. Bao, L. Chen, S. Zhu, J. Yu</author><pubDate>Thu, 02 Jan 2025 17:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01368v1</guid></item><item><title>Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation</title><link>http://arxiv.org/abs/2501.01367v1</link><description>People have a variety of preferences for how robots behave. To understand andreason about these preferences, robots aim to learn a reward function thatdescribes how aligned robot behaviors are with a user's preferences. Goodrepresentations of a robot's behavior can significantly reduce the time andeffort required for a user to teach the robot their preferences. Specifyingthese representations -- what "features" of the robot's behavior matter tousers -- remains a difficult problem; Features learned from raw data lacksemantic meaning and features learned from user data require users to engage intedious labeling processes. Our key insight is that users tasked withcustomizing a robot are intrinsically motivated to produce labels throughexploratory search; they explore behaviors that they find interesting andignore behaviors that are irrelevant. To harness this novel data source ofexploratory actions, we propose contrastive learning from exploratory actions(CLEA) to learn trajectory features that are aligned with features that userscare about. We learned CLEA features from exploratory actions users performedin an open-ended signal design activity (N=25) with a Kuri robot, and evaluatedCLEA features through a second user study with a different set of users (N=42).CLEA features outperformed self-supervised features when eliciting userpreferences over four metrics: completeness, simplicity, minimality, andexplainability.</description><author>Nathaniel Dennler, Stefanos Nikolaidis, Maja Matarić</author><pubDate>Thu, 02 Jan 2025 17:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01367v1</guid></item><item><title>SegKAN: High-Resolution Medical Image Segmentation with Long-Distance Dependencies</title><link>http://arxiv.org/abs/2412.19990v2</link><description>Hepatic vessels in computed tomography scans often suffer from imagefragmentation and noise interference, making it difficult to maintain vesselintegrity and posing significant challenges for vessel segmentation. To addressthis issue, we propose an innovative model: SegKAN. First, we improve theconventional embedding module by adopting a novel convolutional networkstructure for image embedding, which smooths out image noise and preventsissues such as gradient explosion in subsequent stages. Next, we transform thespatial relationships between Patch blocks into temporal relationships to solvethe problem of capturing positional relationships between Patch blocks intraditional Vision Transformer models. We conducted experiments on a Hepaticvessel dataset, and compared to the existing state-of-the-art model, the Dicescore improved by 1.78%. These results demonstrate that the proposed newstructure effectively enhances the segmentation performance of high-resolutionextended objects. Code will be available at https://github.com/goblin327/SegKAN</description><author>Shengbo Tan, Rundong Xue, Shipeng Luo, Zeyu Zhang, Xinran Wang, Lei Zhang, Daji Ergu, Zhang Yi, Yang Zhao, Ying Cai</author><pubDate>Thu, 02 Jan 2025 17:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19990v2</guid></item><item><title>From Models to Systems: A Comprehensive Fairness Framework for Compositional Recommender Systems</title><link>http://arxiv.org/abs/2412.04655v2</link><description>Fairness research in machine learning often centers on ensuring equitableperformance of individual models. However, real-world recommendation systemsare built on multiple models and even multiple stages, from candidate retrievalto scoring and serving, which raises challenges for responsible development anddeployment. This system-level view, as highlighted by regulations like the EUAI Act, necessitates moving beyond auditing individual models as independententities. We propose a holistic framework for modeling system-level fairness,focusing on the end-utility delivered to diverse user groups, and considerinteractions between components such as retrieval and scoring models. Weprovide formal insights on the limitations of focusing solely on model-levelfairness and highlight the need for alternative tools that account forheterogeneity in user preferences. To mitigate system-level disparities, weadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimizeutility and equity. We empirically demonstrate the effectiveness of ourproposed framework on synthetic and real datasets, underscoring the need for asystem-level framework.</description><author>Brian Hsu, Cyrus DiCiccio, Natesh Sivasubramoniapillai, Hongseok Namkoong</author><pubDate>Thu, 02 Jan 2025 17:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04655v2</guid></item><item><title>ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding</title><link>http://arxiv.org/abs/2501.01366v1</link><description>3D visual grounding (3DVG) involves localizing entities in a 3D scenereferred to by natural language text. Such models are useful for embodied AIand scene retrieval applications, which involve searching for objects orpatterns using natural language descriptions. While recent works have focusedon LLM-based scaling of 3DVG datasets, these datasets do not capture the fullrange of potential prompts which could be specified in the English language. Toensure that we are scaling up and testing against a useful and representativeset of prompts, we propose a framework for linguistically analyzing 3DVGprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), adiagnostic dataset for evaluating visual grounding methods against a diverseset of language patterns. We evaluate existing open-vocabulary 3DVG methods todemonstrate that these methods are not yet proficient in understanding andidentifying the targets of more challenging, out-of-distribution prompts,toward real-world applications.</description><author>Austin T. Wang, ZeMing Gong, Angel X. Chang</author><pubDate>Thu, 02 Jan 2025 17:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01366v1</guid></item><item><title>Perception-guided Jailbreak against Text-to-Image Models</title><link>http://arxiv.org/abs/2408.10848v3</link><description>In recent years, Text-to-Image (T2I) models have garnered significantattention due to their remarkable advancements. However, security concerns haveemerged due to their potential to generate inappropriate or Not-Safe-For-Work(NSFW) images. In this paper, inspired by the observation that texts withdifferent semantics can lead to similar human perceptions, we propose anLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-boxjailbreak method that requires no specific T2I model (model-free) and generateshighly natural attack prompts. Specifically, we propose identifying a safephrase that is similar in human perception yet inconsistent in text semanticswith the target unsafe word and using it as a substitution. The experimentsconducted on six open-source models and commercial online services withthousands of prompts have verified the effectiveness of PGJ.</description><author>Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao, Geguang Pu, Yang Liu</author><pubDate>Thu, 02 Jan 2025 17:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10848v3</guid></item><item><title>Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach</title><link>http://arxiv.org/abs/2402.02954v3</link><description>A recent theory shows that a multi-player decentralized partially observableMarkov decision process can be transformed into an equivalent single-playergame, enabling the application of \citeauthor{bellman}'s principle ofoptimality to solve the single-player game by breaking it down intosingle-stage subgames. However, this approach entangles the decision variablesof all players at each single-stage subgame, resulting in backups with adouble-exponential complexity. This paper demonstrates how to disentangle thesedecision variables while maintaining optimality under hierarchical informationsharing, a prominent management style in our society. To achieve this, we applythe principle of optimality to solve any single-stage subgame by breaking itdown further into smaller subgames, enabling us to make single-player decisionsat a time. Our approach reveals that extensive-form games always exist withsolutions to a single-stage subgame, significantly reducing time complexity.Our experimental results show that the algorithms leveraging these findings canscale up to much larger multi-player games without compromising optimality.</description><author>Johan Peralez, Aurélien Delage, Olivier Buffet, Jilles S. Dibangoye</author><pubDate>Thu, 02 Jan 2025 17:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02954v3</guid></item><item><title>SwitchLoRA: Switched Low-Rank Adaptation Can Learn Full-Rank Information</title><link>http://arxiv.org/abs/2406.06564v3</link><description>In the training of large language models, parameter-efficient techniques suchas LoRA optimize memory usage and reduce communication overhead and memoryusage during the fine-tuning phase. However, applying such techniques directlyduring the pre-training phase results in poor performance, primarily becausethe premature implementation of low-rank training significantly reduces modelaccuracy. Existing methods like ReLoRA and GaLore have attempted to addressthis challenge by updating the low-rank subspace. However, they still fallshort of achieving the accuracy of full-rank training. Specifically, ReLoRArestricts the frequency of updates to preserve optimizer states consistency,hindering its ability to closely approximate full-rank training behavior.Meanwhile, GaLore relies on Singular Value Decomposition (SVD) to approximatethe full-rank space, which introduces accuracy loss during the approximationprocess. In this paper, we introduce SwitchLoRA, a parameter-efficient trainingtechnique that frequently and smoothly replaces the trainable parameters ofLoRA adapters with alternative parameters. SwitchLoRA updates the low-ranksubspace incrementally, targeting only a few dimensions at a time to minimizethe impact on optimizer states. This allows a higher update frequency, therebyenhancing accuracy by enabling the updated parameters to more closely mimicfull-rank behavior during the pre-training phase. Our results demonstrate thatSwitchLoRA actually surpasses full-rank training, reducing perplexity from15.23 to 15.01 on the LLaMA 1.3B model, while also cutting communicationoverhead by 54\% and memory usage by 13\%. Furthermore, after full fine-tuningthe SwitchLoRA pre-trained model and the full-rank pre-trained model on theGLUE benchmark, the SwitchLoRA pre-trained model showed an average accuracygain of about 1\% over the full-rank pre-trained model.</description><author>Kaiye Zhou, Shucheng Wang, Jun Xu</author><pubDate>Thu, 02 Jan 2025 17:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06564v3</guid></item><item><title>Variational autoencoders with latent high-dimensional steady geometric flows for dynamics</title><link>http://arxiv.org/abs/2410.10137v4</link><description>We develop Riemannian approaches to variational autoencoders (VAEs) forPDE-type ambient data with regularizing geometric latent dynamics, which werefer to as VAE-DLM, or VAEs with dynamical latent manifolds. We redevelop theVAE framework such that manifold geometries, subject to our geometric flow,embedded in Euclidean space are learned in the intermediary latent spacedeveloped by encoders and decoders. By tailoring the geometric flow in whichthe latent space evolves, we induce latent geometric properties of ourchoosing, which are reflected in empirical performance. We reformulate thetraditional evidence lower bound (ELBO) loss with a considerate choice ofprior. We develop a linear geometric flow with a steady-state regularizingterm. This flow requires only automatic differentiation of one time derivative,and can be solved in moderately high dimensions in a physics-informed approach,allowing more expressive latent representations. We discuss how this flow canbe formulated as a gradient flow, and maintains entropy away from metricsingularity. This, along with an eigenvalue penalization condition, helpsensure the manifold is sufficiently large in measure, nondegenerate, and acanonical geometry, which contribute to a robust representation. Our methodsfocus on the modified multi-layer perceptron architecture with tanh activationsfor the manifold encoder-decoder. We demonstrate, on our datasets of interest,our methods perform at least as well as the traditional VAE, and oftentimesbetter. Our methods can outperform this and a VAE endowed with our proposedarchitecture, frequently reducing out-of-distribution (OOD) error between 15%to 35% on select datasets. We highlight our method on ambient PDEs whosesolutions maintain minimal variation in late times. We provide empiricaljustification towards how we can improve robust learning for external dynamicswith VAEs.</description><author>Andrew Gracyk</author><pubDate>Thu, 02 Jan 2025 17:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10137v4</guid></item><item><title>Rethinking Relation Extraction: Beyond Shortcuts to Generalization with a Debiased Benchmark</title><link>http://arxiv.org/abs/2501.01349v1</link><description>Benchmarks are crucial for evaluating machine learning algorithm performance,facilitating comparison and identifying superior solutions. However, biaseswithin datasets can lead models to learn shortcut patterns, resulting ininaccurate assessments and hindering real-world applicability. This paperaddresses the issue of entity bias in relation extraction tasks, where modelstend to rely on entity mentions rather than context. We propose a debiasedrelation extraction benchmark DREB that breaks the pseudo-correlation betweenentity mentions and relation types through entity replacement. DREB utilizesBias Evaluator and PPL Evaluator to ensure low bias and high naturalness,providing a reliable and accurate assessment of model generalization in entitybias scenarios. To establish a new baseline on DREB, we introduce MixDebias, adebiasing method combining data-level and model training-level techniques.MixDebias effectively improves model performance on DREB while maintainingperformance on the original dataset. Extensive experiments demonstrate theeffectiveness and robustness of MixDebias compared to existing methods,highlighting its potential for improving the generalization ability of relationextraction models. We will release DREB and MixDebias publicly.</description><author>Liang He, Yougang Chu, Zhen Wu, Jianbing Zhang, Xinyu Dai, Jiajun Chen</author><pubDate>Thu, 02 Jan 2025 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01349v1</guid></item><item><title>A Closer Look at Deep Learning Methods on Tabular Datasets</title><link>http://arxiv.org/abs/2407.00956v2</link><description>Tabular data is prevalent across diverse domains in machine learning. Whileclassical methods like tree-based models have long been effective, Deep NeuralNetwork (DNN)-based methods have recently demonstrated promising performance.However, the diverse characteristics of methods and the inherent heterogeneityof tabular datasets make understanding and interpreting tabular methods bothchallenging and prone to unstable observations. In this paper, we conductin-depth evaluations and comprehensive analyses of tabular methods, with aparticular focus on DNN-based models, using a benchmark of over 300 tabulardatasets spanning a wide range of task types, sizes, and domains. First, weperform an extensive comparison of 32 state-of-the-art deep and tree-basedmethods, evaluating their average performance across multiple criteria.Although method ranks vary across datasets, we empirically find thattop-performing methods tend to concentrate within a small subset of tabularmodels, regardless of the criteria used. Next, we investigate whether thetraining dynamics of deep tabular models can be predicted based on datasetproperties. This approach not only offers insights into the behavior of deeptabular methods but also identifies a core set of "meta-features" that reflectdataset heterogeneity. The other subset includes datasets where method ranksare consistent with the overall benchmark, acting as a reliable probe forfurther tabular analysis.</description><author>Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, De-Chuan Zhan</author><pubDate>Thu, 02 Jan 2025 16:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00956v2</guid></item><item><title>AdaptVC: High Quality Voice Conversion with Adaptive Learning</title><link>http://arxiv.org/abs/2501.01347v1</link><description>The goal of voice conversion is to transform the speech of a source speakerto sound like that of a reference speaker while preserving the originalcontent. A key challenge is to extract disentangled linguistic content from thesource and voice style from the reference. While existing approaches leveragevarious methods to isolate the two, a generalization still requires furtherattention, especially for robustness in zero-shot scenarios. In this paper, weachieve successful disentanglement of content and speaker features by tuningself-supervised speech features with adapters. The adapters are trained todynamically encode nuanced features from rich self-supervised features, and thedecoder fuses them to produce speech that accurately resembles the referencewith minimal loss of content. Moreover, we leverage a conditional flow matchingdecoder with cross-attention speaker conditioning to further boost thesynthesis quality and efficiency. Subjective and objective evaluations in azero-shot scenario demonstrate that the proposed method outperforms existingmodels in speech quality and similarity to the reference speech.</description><author>Jaehun Kim, Ji-Hoon Kim, Yeunju Choi, Tan Dat Nguyen, Seongkyu Mun, Joon Son Chung</author><pubDate>Thu, 02 Jan 2025 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01347v1</guid></item><item><title>Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability</title><link>http://arxiv.org/abs/2501.01346v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in processing both visual and textual information. However, thecritical challenge of alignment between visual and linguistic representationsis not fully understood. This survey presents a comprehensive examination ofalignment and misalignment in LVLMs through an explainability lens. We firstexamine the fundamentals of alignment, exploring its representational andbehavioral aspects, training methodologies, and theoretical foundations. Wethen analyze misalignment phenomena across three semantic levels: object,attribute, and relational misalignment. Our investigation reveals thatmisalignment emerges from challenges at multiple levels: the data level, themodel level, and the inference level. We provide a comprehensive review ofexisting mitigation strategies, categorizing them into parameter-frozen andparameter-tuning approaches. Finally, we outline promising future researchdirections, emphasizing the need for standardized evaluation protocols andin-depth explainability studies.</description><author>Dong Shu, Haiyan Zhao, Jingyu Hu, Weiru Liu, Lu Cheng, Mengnan Du</author><pubDate>Thu, 02 Jan 2025 16:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01346v1</guid></item><item><title>Machine Learning for Modeling Wireless Radio Metrics with Crowdsourced Data and Local Environment Features</title><link>http://arxiv.org/abs/2501.01344v1</link><description>This paper presents a suite of machine learning models, CRC-ML-Radio Metrics,designed for modeling RSRP, RSRQ, and RSSI wireless radio metrics in 4Genvironments. These models utilize crowdsourced data with local environmentalfeatures to enhance prediction accuracy across both indoor at elevation andoutdoor urban settings. They achieve RMSE performance of 9.76 to 11.69 dB forRSRP, 2.90 to 3.23 dB for RSRQ, and 9.50 to 10.36 dB for RSSI, evaluated onover 300,000 data points in the Toronto, Montreal, and Vancouver areas. Theseresults demonstrate the robustness and adaptability of the models, supportingprecise network planning and quality of service optimization in complexCanadian urban environments.</description><author>Yifeng Qiu, Alexis Bose</author><pubDate>Thu, 02 Jan 2025 16:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01344v1</guid></item><item><title>DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring</title><link>http://arxiv.org/abs/2501.01342v1</link><description>Effective process monitoring is increasingly vital in industrial automationfor ensuring operational safety, necessitating both high accuracy andefficiency. Although Transformers have demonstrated success in various fields,their canonical form based on the self-attention mechanism is inadequate forprocess monitoring due to two primary limitations: (1) the step-wisecorrelations captured by self-attention mechanism are difficult to capturediscriminative patterns in monitoring logs due to the lacking semantics of eachstep, thus compromising accuracy; (2) the quadratic computational complexity ofself-attention hampers efficiency. To address these issues, we proposeDeepFilter, a Transformer-style framework for process monitoring. The coreinnovation is an efficient filtering layer that excel capturing long-term andperiodic patterns with reduced complexity. Equipping with the global filteringlayer, DeepFilter enhances both accuracy and efficiency, meeting the stringentdemands of process monitoring. Experimental results on real-world processmonitoring datasets validate DeepFilter's superiority in terms of accuracy andefficiency compared to existing state-of-the-art models.</description><author>Hao Wang, Zhichao Chen, Licheng Pan, Xiaoyu Jiang, Yichen Song, Qunshan He, Xinggao Liu</author><pubDate>Thu, 02 Jan 2025 16:47:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01342v1</guid></item><item><title>Simultaneous Latent State Estimation and Latent Linear Dynamics Discovery from Image Observations</title><link>http://arxiv.org/abs/2501.01339v1</link><description>The problem of state estimation has a long history with many successfulalgorithms that allow analytical derivation or approximation of posteriorfiltering distribution given the noisy observations. This report tries toconclude previous works to resolve the problem of latent state estimation givenimage-based observations and also suggests a new solution to this problem.</description><author>Nikita Kostin</author><pubDate>Thu, 02 Jan 2025 16:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01339v1</guid></item><item><title>Commutative Evolution Laws in Holographic Cellular Automata: AdS/CFT, Near-Extremal D3-Branes, and a Deep Learning Approach</title><link>http://arxiv.org/abs/2012.06441v8</link><description>According to 't Hooft, restoring Poincar\'e invariance in a holographiccellular automaton (CA) requires two distinct evolution laws that commute. Weexplore how this is realized in the AdS/CFT framework, assuming commutativityas a fundamental principle--much like general covariance once did--for encodingcurvature. In our setup, physical processes in a given spacetime are encoded ina CA; to preserve Poincar\'e symmetry, the spacetime curvature must effectivelyvanish, so we consider a near-extremal black D3-brane solution, in which boththe stretched horizon and the conformal boundary are approximated by Minkowskispace. AdS/CFT implies a spatial evolution law connecting these hypersurfaces.Commutativity means the final state does not depend on the order of timeevolution on each hypersurface and spatial evolution between them, forcing thetime evolution law on the horizon and boundary to coincide. To satisfy allthese conditions, we aim to demonstrate that the spatial evolution lawinevitably encapsulates the curvature of the bulk, including quantum effects.For a computational model, we compactify the hyperplanes to tori, reducing thedegrees of freedom to a finite number; taking these tori to infinite size thenrestores Poincar\'e symmetry. We propose a deep learning algorithm that, givena known time evolution law and commutativity, deduces the corresponding spatialevolution law.</description><author>Hyunju Go</author><pubDate>Thu, 02 Jan 2025 16:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.06441v8</guid></item><item><title>Aligning Large Language Models for Faithful Integrity Against Opposing Argument</title><link>http://arxiv.org/abs/2501.01336v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities incomplex reasoning tasks. However, they can be easily misled by unfaithfularguments during conversations, even when their original statements arecorrect. To this end, we investigate the problem of maintaining faithfulintegrity in LLMs. This involves ensuring that LLMs adhere to their faithfulstatements in the face of opposing arguments and are able to correct theirincorrect statements when presented with faithful arguments. In this work, wepropose a novel framework, named Alignment for Faithful Integrity withConfidence Estimation (AFICE), which aims to align the LLM responses withfaithful integrity. Specifically, AFICE first designs a Bilateral ConfidenceEstimation (BCE) approach for estimating the uncertainty of each responsegenerated by the LLM given a specific context, which simultaneously estimatethe model's confidence to the question based on the internal states duringdecoding as well as to the answer based on cumulative probability ratios. Withthe BCE, we construct a conversational preference dataset composed of context,original statement, and argument, which is adopted for aligning the LLM forfaithful integrity using Direct Preference Optimization (DPO). Extensiveexperimental results on a wide range of benchmarks demonstrate significantimprovements in the LLM's ability to maintain faithful responses whenencountering opposing arguments, ensuring both the practical utility andtrustworthiness of LLMs in complex interactive settings. Code and data will bereleased via https://github.com/zhaoy777/AFICE.git</description><author>Yong Zhao, Yang Deng, See-Kiong Ng, Tat-Seng Chua</author><pubDate>Thu, 02 Jan 2025 16:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01336v1</guid></item><item><title>CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models</title><link>http://arxiv.org/abs/2501.01335v1</link><description>Numerous studies have investigated methods for jailbreaking Large LanguageModels (LLMs) to generate harmful content. Typically, these methods areevaluated using datasets of malicious prompts designed to bypass securitypolicies established by LLM providers. However, the generally broad scope andopen-ended nature of existing datasets can complicate the assessment ofjailbreaking effectiveness, particularly in specific domains, notablycybersecurity. To address this issue, we present and publicly releaseCySecBench, a comprehensive dataset containing 12662 prompts specificallydesigned to evaluate jailbreaking techniques in the cybersecurity domain. Thedataset is organized into 10 distinct attack-type categories, featuringclose-ended prompts to enable a more consistent and accurate assessment ofjailbreaking attempts. Furthermore, we detail our methodology for datasetgeneration and filtration, which can be adapted to create similar datasets inother domains. To demonstrate the utility of CySecBench, we propose andevaluate a jailbreaking approach based on prompt obfuscation. Our experimentalresults show that this method successfully elicits harmful content fromcommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPTand 88% with Gemini; in contrast, Claude demonstrated greater resilience with ajailbreaking SR of 17%. Compared to existing benchmark approaches, our methodshows superior performance, highlighting the value of domain-specificevaluation datasets for assessing LLM security measures. Moreover, whenevaluated using prompts from a widely used dataset (i.e., AdvBench), itachieved an SR of 78.5%, higher than the state-of-the-art methods.</description><author>Johan Wahréus, Ahmed Mohamed Hussain, Panos Papadimitratos</author><pubDate>Thu, 02 Jan 2025 16:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01335v1</guid></item><item><title>Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension</title><link>http://arxiv.org/abs/2501.01332v1</link><description>Understanding how large language models (LLMs) acquire, retain, and applyknowledge remains an open challenge. This paper introduces a novel framework,K-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctnessand confidence. The framework defines six categories of knowledge, ranging fromhighly confident correctness to confidently held misconceptions, enabling anuanced evaluation of model comprehension beyond binary accuracy. Using thisframework, we demonstrate how techniques like chain-of-thought prompting andreinforcement learning with human feedback fundamentally alter the knowledgestructures of internal (pre-trained) and external (context-dependent) knowledgein LLMs. CoT particularly enhances base model performance and shows synergisticbenefits when applied to aligned LLMs. Moreover, our layer-wise analysisreveals that higher layers in LLMs encode more high-confidence knowledge, whilelow-confidence knowledge tends to emerge in middle-to-lower layers.</description><author>Yanbo Fang, Ruixiang Tang</author><pubDate>Thu, 02 Jan 2025 16:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01332v1</guid></item><item><title>BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages</title><link>http://arxiv.org/abs/2412.04351v2</link><description>This paper focuses on developing translation models and related applicationsfor 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj,Bodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada,Kangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili,Malayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi,Sanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu,Telugu, and Urdu. Achieving this requires parallel and other types of corporafor all 36 * 36 language pairs, addressing challenges like script variations,phonetic differences, and syntactic diversity. For instance, languages likeKashmiri and Sindhi, which use multiple scripts, demand script normalizationfor alignment, while low-resource languages such as Khasi and Santali requiresynthetic data augmentation to ensure sufficient coverage and quality. To address these challenges, this work proposes strategies for corpuscreation by leveraging existing resources, developing parallel datasets,generating domain-specific corpora, and utilizing synthetic data techniques.Additionally, it evaluates machine translation across various dimensions,including standard and discourse-level translation, domain-specifictranslation, reference-based and reference-free evaluation, error analysis, andautomatic post-editing. By integrating these elements, the study establishes acomprehensive framework to improve machine translation quality and enablebetter cross-lingual communication in India's linguistically diverse ecosystem.</description><author>Vandan Mujadia, Dipti Misra Sharma</author><pubDate>Thu, 02 Jan 2025 16:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04351v2</guid></item><item><title>Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation</title><link>http://arxiv.org/abs/2412.21117v2</link><description>In this work, we introduce Prometheus, a 3D-aware latent diffusion model fortext-to-3D generation at both object and scene levels in seconds. We formulate3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussiangeneration within the latent diffusion paradigm. To ensure generalizability, webuild our model upon pre-trained text-to-image generation model with onlyminimal adjustments, and further train it using a large number of images fromboth single-view and multi-view datasets. Furthermore, we introduce an RGB-Dlatent space into 3D Gaussian generation to disentangle appearance and geometryinformation, enabling efficient feed-forward generation of 3D Gaussians withbetter fidelity and geometry. Extensive experimental results demonstrate theeffectiveness of our method in both feed-forward 3D Gaussian reconstruction andtext-to-3D generation. Project page:https://freemty.github.io/project-prometheus/</description><author>Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, Yiyi Liao</author><pubDate>Thu, 02 Jan 2025 16:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21117v2</guid></item><item><title>The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation</title><link>http://arxiv.org/abs/2501.01329v1</link><description>Test cases are essential for validating the reliability and quality ofsoftware applications. Recent studies have demonstrated the capability of LargeLanguage Models (LLMs) to generate useful test cases for given source code.However, the existing work primarily relies on human-written plain prompts,which often leads to suboptimal results since the performance of LLMs can behighly influenced by the prompts. Moreover, these approaches use the sameprompt for all LLMs, overlooking the fact that different LLMs might be bestsuited to different prompts. Given the wide variety of possible promptformulations, automatically discovering the optimal prompt for each LLMpresents a significant challenge. Although there are methods on automatedprompt optimization in the natural language processing field, they are hard toproduce effective prompts for the test case generation task. First, the methodsiteratively optimize prompts by simply combining and mutating existing oneswithout proper guidance, resulting in prompts that lack diversity and tend torepeat the same errors in the generated test cases. Second, the prompts aregenerally lack of domain contextual knowledge, limiting LLMs' performance inthe task.</description><author>Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiaoqian Jiao, Chun Yong Chong, Shan Gao, Michael Lyu</author><pubDate>Thu, 02 Jan 2025 16:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01329v1</guid></item><item><title>Domain-invariant feature learning in brain MR imaging for content-based image retrieval</title><link>http://arxiv.org/abs/2501.01326v1</link><description>When conducting large-scale studies that collect brain MR images frommultiple facilities, the impact of differences in imaging equipment andprotocols at each site cannot be ignored, and this domain gap has become asignificant issue in recent years. In this study, we propose a newlow-dimensional representation (LDR) acquisition method called style encoderadversarial domain adaptation (SE-ADA) to realize content-based image retrieval(CBIR) of brain MR images. SE-ADA reduces domain differences while preservingpathological features by separating domain-specific information from LDR andminimizing domain differences using adversarial learning. In evaluation experiments comparing SE-ADA with recent domain harmonizationmethods on eight public brain MR datasets (ADNI1/2/3, OASIS1/2/3/4, PPMI),SE-ADA effectively removed domain information while preserving key aspects ofthe original brain structure and demonstrated the highest disease searchaccuracy.</description><author>Shuya Tobari, Shuhei Tomoshige, Hayato Muraki, Kenichi Oishi, Hitoshi Iyatomi</author><pubDate>Thu, 02 Jan 2025 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01326v1</guid></item><item><title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title><link>http://arxiv.org/abs/2501.01324v1</link><description>In this work, we develop a scalable approach for a flexible latent factormodel for high-dimensional dynamical systems. Each latent factor process hasits own correlation and variance parameters, and the orthogonal factor loadingmatrix can be either fixed or estimated. We utilize an orthogonal factorloading matrix that avoids computing the inversion of the posterior covariancematrix at each time of the Kalman filter, and derive closed-form expressions inan expectation-maximization algorithm for parameter estimation, whichsubstantially reduces the computational complexity without approximation. Ourstudy is motivated by inversely estimating slow slip events from geodetic data,such as continuous GPS measurements. Extensive simulated studies illustratehigher accuracy and scalability of our approach compared to alternatives. Byapplying our method to geodetic measurements in the Cascadia region, ourestimated slip better agrees with independently measured seismic data of tremorevents. The substantial acceleration from our method enables the use of massivenoisy data for geological hazard quantification and other applications.</description><author>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</author><pubDate>Thu, 02 Jan 2025 16:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01324v1</guid></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>http://arxiv.org/abs/2501.01320v1</link><description>Video restoration poses non-trivial challenges in maintaining fidelity whilerecovering temporally consistent details from unknown degradations in the wild.Despite recent advances in diffusion-based restoration, these methods oftenface limitations in generation capability and sampling efficiency. In thiswork, we present SeedVR, a diffusion transformer designed to handle real-worldvideo restoration with arbitrary length and resolution. The core design ofSeedVR lies in the shifted window attention that facilitates effectiverestoration on long video sequences. SeedVR further supports variable-sizedwindows near the boundary of both spatial and temporal dimensions, overcomingthe resolution constraints of traditional window attention. Equipped withcontemporary practices, including causal video autoencoder, mixed image andvideo training, and progressive training, SeedVR achieves highly-competitiveperformance on both synthetic and real-world benchmarks, as well asAI-generated videos. Extensive experiments demonstrate SeedVR's superiorityover existing methods for generic video restoration.</description><author>Jianyi Wang, Zhijie Lin, Meng Wei, Yang Zhao, Ceyuan Yang, Chen Change Loy, Lu Jiang</author><pubDate>Thu, 02 Jan 2025 16:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01320v1</guid></item><item><title>Understanding Difficult-to-learn Examples in Contrastive Learning: A Theoretical Framework for Spectral Contrastive Learning</title><link>http://arxiv.org/abs/2501.01317v1</link><description>Unsupervised contrastive learning has shown significant performanceimprovements in recent years, often approaching or even rivaling supervisedlearning in various tasks. However, its learning mechanism is fundamentallydifferent from that of supervised learning. Previous works have shown thatdifficult-to-learn examples (well-recognized in supervised learning as examplesaround the decision boundary), which are essential in supervised learning,contribute minimally in unsupervised settings. In this paper, perhapssurprisingly, we find that the direct removal of difficult-to-learn examples,although reduces the sample size, can boost the downstream classificationperformance of contrastive learning. To uncover the reasons behind this, wedevelop a theoretical framework modeling the similarity between different pairsof samples. Guided by this theoretical framework, we conduct a thoroughtheoretical analysis revealing that the presence of difficult-to-learn examplesnegatively affects the generalization of contrastive learning. Furthermore, wedemonstrate that the removal of these examples, and techniques such as margintuning and temperature scaling can enhance its generalization bounds, therebyimproving performance. Empirically, we propose a simple and efficient mechanismfor selecting difficult-to-learn examples and validate the effectiveness of theaforementioned methods, which substantiates the reliability of our proposedtheoretical framework.</description><author>Yi-Ge Zhang, Jingyi Cui, Qiran Li, Yisen Wang</author><pubDate>Thu, 02 Jan 2025 16:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01317v1</guid></item><item><title>Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls</title><link>http://arxiv.org/abs/2412.15023v2</link><description>Sound designers and Foley artists usually sonorize a scene, such as from amovie or video game, by manually annotating and sonorizing each action ofinterest in the video. In our case, the intent is to leave full creativecontrol to sound designers with a tool that allows them to bypass the morerepetitive parts of their work, thus being able to focus on the creativeaspects of sound production. We achieve this presenting Stable-V2A, a two-stagemodel consisting of: an RMS-Mapper that estimates an envelope representative ofthe audio characteristics associated with the input video; and Stable-Foley, adiffusion model based on Stable Audio Open that generates audio semanticallyand temporally aligned with the target video. Temporal alignment is guaranteedby the use of the envelope as a ControlNet input, while semantic alignment isachieved through the use of sound representations chosen by the designer ascross-attention conditioning of the diffusion process. We train and test ourmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. Inaddition, to test our model on a case study of interest, we introduce WalkingThe Maps, a dataset of videos extracted from video games depicting animatedcharacters walking in different locations. Samples and code available on ourdemo page at https://ispamm.github.io/Stable-V2A.</description><author>Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello</author><pubDate>Thu, 02 Jan 2025 16:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15023v2</guid></item><item><title>A Survey of Controllable Learning: Methods and Applications in Information Retrieval</title><link>http://arxiv.org/abs/2407.06083v2</link><description>Controllability has become a crucial aspect of trustworthy machine learning,enabling learners to meet predefined targets and adapt dynamically at test timewithout requiring retraining as the targets shift. We provide a formaldefinition of controllable learning (CL), and discuss its applications ininformation retrieval (IR) where information needs are often complex anddynamic. The survey categorizes CL according to what is controllable (e.g.,multiple objectives, user portrait, scenario adaptation), who controls (usersor platforms), how control is implemented (e.g., rule-based method, Paretooptimization, hypernetwork and others), and where to implement control (e.g.,pre-processing, in-processing, post-processing methods). Then, we identifychallenges faced by CL across training, evaluation, task setting, anddeployment in online environments. Additionally, we outline promisingdirections for CL in theoretical analysis, efficient computation, empoweringlarge language models, application scenarios and evaluation frameworks.</description><author>Chenglei Shen, Xiao Zhang, Teng Shi, Changshuo Zhang, Guofu Xie, Jun Xu</author><pubDate>Thu, 02 Jan 2025 16:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06083v2</guid></item><item><title>Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions</title><link>http://arxiv.org/abs/2210.07996v5</link><description>We study the classical Network Revenue Management (NRM) problem withaccept/reject decisions and $T$ IID arrivals. We consider a distributional formwhere each arrival must fall under a finite number of possible categories, eachwith a deterministic resource consumption vector, but a random valuedistributed continuously over an interval. We develop an online algorithm thatachieves $O(\log^2 T)$ regret under this model, with the only (necessary)assumption being that the probability densities are bounded away from 0. Wederive a second result that achieves $O(\log T)$ regret under an additionalassumption of second-order growth. To our knowledge, these are the firstresults achieving logarithmic-level regret in an NRM model with continuousvalues that do not require any kind of "non-degeneracy" assumptions. Ourresults are achieved via new techniques including a new method of boundingmyopic regret, a "semi-fluid" relaxation of the offline allocation, and animproved bound on the "dual convergence".</description><author>Jiashuo Jiang, Will Ma, Jiawei Zhang</author><pubDate>Thu, 02 Jan 2025 16:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07996v5</guid></item><item><title>Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics</title><link>http://arxiv.org/abs/2409.16756v3</link><description>Explainable AI (XAI) is a rapidly growing domain with a myriad of proposedmethods as well as metrics aiming to evaluate their efficacy. However, currentstudies are often of limited scope, examining only a handful of XAI methods andignoring underlying design parameters for performance, such as the modelarchitecture or the nature of input data. Moreover, they often rely on one or afew metrics and neglect thorough validation, increasing the risk of selectionbias and ignoring discrepancies among metrics. These shortcomings leavepractitioners confused about which method to choose for their problem. Inresponse, we introduce LATEC, a large-scale benchmark that critically evaluates17 prominent XAI methods using 20 distinct metrics. We systematicallyincorporate vital design parameters like varied architectures and diverse inputmodalities, resulting in 7,560 examined combinations. Through LATEC, weshowcase the high risk of conflicting metrics leading to unreliable rankingsand consequently propose a more robust evaluation scheme. Further, wecomprehensively evaluate various XAI methods to assist practitioners inselecting appropriate methods aligning with their needs. Curiously, theemerging top-performing method, Expected Gradients, is not examined in anyrelevant related study. LATEC reinforces its role in future XAI research bypublicly releasing all 326k saliency maps and 378k metric scores as a(meta-)evaluation dataset. The benchmark is hosted at:https://github.com/IML-DKFZ/latec.</description><author>Lukas Klein, Carsten T. Lüth, Udo Schlegel, Till J. Bungert, Mennatallah El-Assady, Paul F. Jäger</author><pubDate>Thu, 02 Jan 2025 15:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16756v3</guid></item><item><title>Learning Spectral Methods by Transformers</title><link>http://arxiv.org/abs/2501.01312v1</link><description>Transformers demonstrate significant advantages as the building block ofmodern LLMs. In this work, we study the capacities of Transformers inperforming unsupervised learning. We show that multi-layered Transformers,given a sufficiently large set of pre-training instances, are able to learn thealgorithms themselves and perform statistical estimation tasks given newinstances. This learning paradigm is distinct from the in-context learningsetup and is similar to the learning procedure of human brains where skills arelearned through past experience. Theoretically, we prove that pre-trainedTransformers can learn the spectral methods and use the classification ofbi-class Gaussian mixture model as an example. Our proof is constructive usingalgorithmic design techniques. Our results are built upon the similarities ofmulti-layered Transformer architecture with the iterative recovery algorithmsused in practice. Empirically, we verify the strong capacity of themulti-layered (pre-trained) Transformer on unsupervised learning through thelens of both the PCA and the Clustering tasks performed on the synthetic andreal-world datasets.</description><author>Yihan He, Yuan Cao, Hong-Yu Chen, Dennis Wu, Jianqing Fan, Han Liu</author><pubDate>Thu, 02 Jan 2025 15:53:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01312v1</guid></item><item><title>Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers</title><link>http://arxiv.org/abs/2501.01311v1</link><description>In this study, we introduce the Multi-Head Explainer (MHEX), a versatile andmodular framework that enhances both the explainability and accuracy ofConvolutional Neural Networks (CNNs) and Transformer-based models. MHEXconsists of three core components: an Attention Gate that dynamicallyhighlights task-relevant features, Deep Supervision that guides early layers tocapture fine-grained details pertinent to the target class, and an EquivalentMatrix that unifies refined local and global representations to generatecomprehensive saliency maps. Our approach demonstrates superior compatibility,enabling effortless integration into existing residual networks like ResNet andTransformer architectures such as BERT with minimal modifications. Extensiveexperiments on benchmark datasets in medical imaging and text classificationshow that MHEX not only improves classification accuracy but also produceshighly interpretable and detailed saliency scores.</description><author>Bohang Sun, Pietro Liò</author><pubDate>Thu, 02 Jan 2025 15:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01311v1</guid></item><item><title>Upper Bounds for Learning in Reproducing Kernel Hilbert Spaces for Non IID Samples</title><link>http://arxiv.org/abs/2410.08361v2</link><description>In this paper, we study a Markov chain-based stochastic gradient algorithm ingeneral Hilbert spaces, aiming to approximate the optimal solution of aquadratic loss function. We establish probabilistic upper bounds on itsconvergence. We further extend these results to an online regularized learningalgorithm in reproducing kernel Hilbert spaces, where the samples are drawnalong a Markov chain trajectory hence the samples are of the non i.i.d. type.</description><author>Priyanka Roy, Susanne Saminger-Platz</author><pubDate>Thu, 02 Jan 2025 15:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08361v2</guid></item><item><title>Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking</title><link>http://arxiv.org/abs/2501.01306v1</link><description>Large language models (LLMs) demonstrate exceptional capabilities, yet stillface the hallucination issue. Typical text generation approaches adopt anauto-regressive generation without deliberate reasoning, which often results inuntrustworthy and factually inaccurate responses. In this paper, we proposeHaluSearch, a novel framework that incorporates tree search-based algorithms(e.g. MCTS) to enable an explicit slow thinking generation process formitigating hallucinations of LLMs during inference. Specifically, HaluSearchframes text generation as a step-by-step reasoning process, using aself-evaluation reward model to score each generation step and guide the treesearch towards the most reliable generation pathway for fully exploiting theinternal knowledge of LLMs. To balance efficiency and quality, we introduce ahierarchical thinking system switch mechanism inspired by the dual processtheory in cognitive science, which dynamically alternates between fast and slowthinking modes at both the instance and step levels, adapting to the complexityof questions and reasoning states. We conduct extensive experiments on bothEnglish and Chinese datasets and the results show that our approachsignificantly outperforms baseline approaches.</description><author>Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen</author><pubDate>Thu, 02 Jan 2025 15:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01306v1</guid></item><item><title>Amortized Bayesian Experimental Design for Decision-Making</title><link>http://arxiv.org/abs/2411.02064v2</link><description>Many critical decisions, such as personalized medical diagnoses and productpricing, are made based on insights gained from designing, observing, andanalyzing a series of experiments. This highlights the crucial role ofexperimental design, which goes beyond merely collecting information on systemparameters as in traditional Bayesian experimental design (BED), but also playsa key part in facilitating downstream decision-making. Most recent BED methodsuse an amortized policy network to rapidly design experiments. However, theinformation gathered through these methods is suboptimal for down-the-linedecision-making, as the experiments are not inherently designed with downstreamobjectives in mind. In this paper, we present an amortized decision-aware BEDframework that prioritizes maximizing downstream decision utility. We introducea novel architecture, the Transformer Neural Decision Process (TNDP), capableof instantly proposing the next experimental design, whilst inferring thedownstream decision, thus effectively amortizing both tasks within a unifiedworkflow. We demonstrate the performance of our method across several tasks,showing that it can deliver informative designs and facilitate accuratedecision-making.</description><author>Daolang Huang, Yujia Guo, Luigi Acerbi, Samuel Kaski</author><pubDate>Thu, 02 Jan 2025 15:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02064v2</guid></item><item><title>Large Language Models for Mental Health Diagnostic Assessments: Exploring The Potential of Large Language Models for Assisting with Mental Health Diagnostic Assessments -- The Depression and Anxiety Case</title><link>http://arxiv.org/abs/2501.01305v1</link><description>Large language models (LLMs) are increasingly attracting the attention ofhealthcare professionals for their potential to assist in diagnosticassessments, which could alleviate the strain on the healthcare system causedby a high patient load and a shortage of providers. For LLMs to be effective insupporting diagnostic assessments, it is essential that they closely replicatethe standard diagnostic procedures used by clinicians. In this paper, wespecifically examine the diagnostic assessment processes described in thePatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) andthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalizedanxiety disorder (GAD). We investigate various prompting and fine-tuningtechniques to guide both proprietary and open-source LLMs in adhering to theseprocesses, and we evaluate the agreement between LLM-generated diagnosticoutcomes and expert-validated ground truth. For fine-tuning, we utilize theMentalllama and Llama models, while for prompting, we experiment withproprietary models like GPT-3.5 and GPT-4o, as well as open-source models suchas llama-3.1-8b and mixtral-8x7b.</description><author>Kaushik Roy, Harshul Surana, Darssan Eswaramoorthi, Yuxin Zi, Vedant Palit, Ritvik Garimella, Amit Sheth</author><pubDate>Thu, 02 Jan 2025 15:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01305v1</guid></item><item><title>Citations and Trust in LLM Generated Responses</title><link>http://arxiv.org/abs/2501.01303v1</link><description>Question answering systems are rapidly advancing, but their opaque nature mayimpact user trust. We explored trust through an anti-monitoring framework,where trust is predicted to be correlated with presence of citations andinversely related to checking citations. We tested this hypothesis with a livequestion-answering experiment that presented text responses generated using acommercial Chatbot along with varying citations (zero, one, or five), bothrelevant and random, and recorded if participants checked the citations andtheir self-reported trust in the generated responses. We found a significantincrease in trust when citations were present, a result that held true evenwhen the citations were random; we also found a significant decrease in trustwhen participants checked the citations. These results highlight the importanceof citations in enhancing trust in AI-generated content.</description><author>Yifan Ding, Matthew Facciani, Amrit Poudel, Ellen Joyce, Salvador Aguinaga, Balaji Veeramani, Sanmitra Bhattacharya, Tim Weninger</author><pubDate>Thu, 02 Jan 2025 15:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01303v1</guid></item><item><title>MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension</title><link>http://arxiv.org/abs/2409.13609v3</link><description>Referring Expression Comprehension (REC), which aims to ground a local visualregion via natural language, is a task that heavily relies on multimodalalignment. Most existing methods utilize powerful pre-trained models totransfer visual/linguistic knowledge by full fine-tuning. However, fullfine-tuning the entire backbone not only breaks the rich prior knowledgeembedded in the pre-training, but also incurs significant computational costs.Motivated by the recent emergence of Parameter-Efficient Transfer Learning(PETL) methods, we aim to solve the REC task in an effective and efficientmanner. Directly applying these PETL methods to the REC task is inappropriate,as they lack the specific-domain abilities for precise local visual perceptionand visual-language alignment. Therefore, we propose a novel framework ofMultimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.Specifically, MaPPER comprises Dynamic Prior Adapters guided by an alignedprior, and Local Convolution Adapters to extract precise local semantics forbetter visual perception. Moreover, the Prior-Guided Text module is proposed tofurther utilize the prior for facilitating the cross-modal alignment.Experimental results on three widely-used benchmarks demonstrate that MaPPERachieves the best accuracy compared to the full fine-tuning and other PETLmethods with only 1.41% tunable backbone parameters. Our code is available athttps://github.com/liuting20/MaPPER.</description><author>Ting Liu, Zunnan Xu, Yue Hu, Liangtao Shi, Zhiqiang Wang, Quanjun Yin</author><pubDate>Thu, 02 Jan 2025 15:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13609v3</guid></item><item><title>Boosting Memory Efficiency in Transfer Learning for High-Resolution Medical Image Classification</title><link>http://arxiv.org/abs/2408.02426v2</link><description>The success of large-scale pre-trained models has established fine-tuning asa standard method for achieving significant improvements in downstream tasks.However, fine-tuning the entire parameter set of a pre-trained model is costly.Parameter-efficient transfer learning (PETL) has recently emerged as acost-effective alternative for adapting pre-trained models to downstream tasks.Despite its advantages, the increasing model size and input resolution presentchallenges for PETL, as the training memory consumption is not reduced aseffectively as the parameter usage. In this paper, we introduce Fine-grainedPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medicalimage classification, which significantly reduces the training memoryconsumption compared to other PETL methods. FPT+ performs transfer learning bytraining a lightweight side network and accessing pre-trained knowledge from alarge pre-trained model (LPM) through fine-grained prompts and fusion modules.Specifically, we freeze the LPM of interest and construct a learnablelightweight side network. The frozen LPM processes high-resolution images toextract fine-grained features, while the side network employs correspondingdown-sampled low-resolution images to minimize the memory usage. To enable theside network to leverage pre-trained knowledge, we propose fine-grained promptsand fusion modules, which collaborate to summarize information through theLPM's intermediate activations. We evaluate FPT+ on eight medical imagedatasets of varying sizes, modalities, and complexities. Experimental resultsdemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of thelearnable parameters and 3.18% of the memory required for fine-tuning an entireViT-B model. Our code is available https://github.com/YijinHuang/FPT.</description><author>Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang</author><pubDate>Thu, 02 Jan 2025 15:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02426v2</guid></item><item><title>Derivatives and residual distribution of regularized M-estimators with application to adaptive tuning</title><link>http://arxiv.org/abs/2107.05143v2</link><description>This paper studies M-estimators with gradient-Lipschitz loss functionregularized with convex penalty in linear models with Gaussian design matrixand arbitrary noise distribution. A practical example is the robust M-estimatorconstructed with the Huber loss and the Elastic-Net penalty and the noisedistribution has heavy-tails. Our main contributions are three-fold. (i) Weprovide general formulae for the derivatives of regularized M-estimators$\hat\beta(y,X)$ where differentiation is taken with respect to both $y$ and$X$; this reveals a simple differentiability structure shared by all convexregularized M-estimators. (ii) Using these derivatives, we characterize thedistribution of the residual $r_i = y_i-x_i^\top\hat\beta$ in the intermediatehigh-dimensional regime where dimension and sample size are of the same order.(iii) Motivated by the distribution of the residuals, we propose a noveladaptive criterion to select tuning parameters of regularized M-estimators. Thecriterion approximates the out-of-sample error up to an additive constantindependent of the estimator, so that minimizing the criterion provides a proxyfor minimizing the out-of-sample error. The proposed adaptive criterion doesnot require the knowledge of the noise distribution or of the covariance of thedesign. Simulated data confirms the theoretical findings, regarding both thedistribution of the residuals and the success of the criterion as a proxy ofthe out-of-sample error. Finally our results reveal new relationships betweenthe derivatives of $\hat\beta(y,X)$ and the effective degrees of freedom of theM-estimator, which are of independent interest.</description><author>Pierre C Bellec, Yiwei Shen</author><pubDate>Thu, 02 Jan 2025 15:19:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.05143v2</guid></item><item><title>LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite Networks</title><link>http://arxiv.org/abs/2501.01293v1</link><description>Recently, the increasing deployment of LEO satellite systems has enabledvarious space analytics (e.g., crop and climate monitoring), which heavilyrelies on the advancements in deep learning (DL). However, the intermittentconnectivity between LEO satellites and ground station (GS) significantlyhinders the timely transmission of raw data to GS for centralized learning,while the scaled-up DL models hamper distributed learning onresource-constrained LEO satellites. Though split learning (SL) can be apotential solution to these problems by partitioning a model and offloadingprimary training workload to GS, the labor-intensive labeling process remainsan obstacle, with intermittent connectivity and data heterogeneity being otherchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SLdesign tailored for satellite networks to combat these challenges. LeveragingSS learning to handle (labeled) data scarcity, we construct an auxiliary modelto tackle the training failure of the satellite-GS non-contact time. Moreover,we propose a pseudo-labeling algorithm to rectify data imbalances acrosssatellites. Lastly, an adaptive activation interpolation scheme is devised toprevent the overfitting of server-side sub-model training at GS. Extensiveexperiments with real-world LEO satellite traces (e.g., Starlink) demonstratethat our LEO-Split framework achieves superior performance compared tostate-ofthe-art benchmarks.</description><author>Zheng Lin, Yuxin Zhang, Zhe Chen, Zihan Fang, Cong Wu, Xianhao Chen, Yue Gao, Jun Luo</author><pubDate>Thu, 02 Jan 2025 15:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01293v1</guid></item><item><title>Change Detection-Based Procedures for Piecewise Stationary MABs: A Modular Approach</title><link>http://arxiv.org/abs/2501.01291v1</link><description>Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationaryenvironments, where the reward distributions associated with the arms do notchange with time. In many applications, however, the environment is moreaccurately modeled as being nonstationary. In this work, piecewise stationaryMAB (PS-MAB) environments are investigated, in which the reward distributionsassociated with a subset of the arms change at some change-points and remainstationary between change-points. Our focus is on the asymptotic analysis ofPS-MABs, for which practical algorithms based on change detection (CD) havebeen previously proposed. Our goal is to modularize the design and analysis ofsuch CD-based Bandit (CDB) procedures. To this end, we identify therequirements for stationary bandit algorithms and change detectors in a CDBprocedure that are needed for the modularization. We assume that the rewardsare sub-Gaussian. Under this assumption and a condition on the separation ofthe change-points, we show that the analysis of CDB procedures can indeed bemodularized, so that regret bounds can be obtained in a unified manner forvarious combinations of change detectors and bandit algorithms. Through thisanalysis, we develop new modular CDB procedures that are order-optimal. Wecompare the performance of our modular CDB procedures with various othermethods in simulations.</description><author>Yu-Han Huang, Argyrios Gerogiannis, Subhonmesh Bose, Venugopal V. Veeravalli</author><pubDate>Thu, 02 Jan 2025 15:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01291v1</guid></item><item><title>λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</title><link>http://arxiv.org/abs/2412.05313v2</link><description>Efficiently learning and executing long-horizon mobile manipulation (MoMa)tasks is crucial for advancing robotics in household and workplace settings.However, current MoMa models are data-inefficient, underscoring the need forimproved models that require realistic-sized benchmarks to evaluate theirefficiency, which do not exist. To address this, we introduce the LAMBDA({\lambda}) benchmark (Long-horizon Actions for Mobile-manipulationBenchmarking of Directed Activities), which evaluates the data efficiency ofmodels on language-conditioned, long-horizon, multi-room, multi-floor,pick-and-place tasks using a dataset of manageable size, more feasible forcollection. The benchmark includes 571 human-collected demonstrations thatprovide realism and diversity in simulated and real-world settings. Unlikeplanner-generated data, these trajectories offer natural variability andreplay-verifiability, ensuring robust learning and evaluation. We benchmarkseveral models, including learning-based models and a neuro-symbolic modularapproach combining foundation models with task and motion planning.Learning-based models show suboptimal success rates, even when leveragingpretrained weights, underscoring significant data inefficiencies. However, theneuro-symbolic approach performs significantly better while being more dataefficient. Findings highlight the need for more data-efficient learning-basedMoMa approaches. {\lambda} addresses this gap by serving as a key benchmark forevaluating the data efficiency of those future models in handling householdrobotics tasks.</description><author>Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Xinyu Liu, Stefanie Tellex</author><pubDate>Thu, 02 Jan 2025 15:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05313v2</guid></item><item><title>ToolComp: A Multi-Tool Reasoning &amp; Process Supervision Benchmark</title><link>http://arxiv.org/abs/2501.01290v1</link><description>Despite recent advances in AI, the development of systems capable ofexecuting complex, multi-step reasoning tasks involving multiple tools remainsa significant challenge. Current benchmarks fall short in capturing thereal-world complexity of tool-use reasoning, where verifying the correctness ofnot only the final answer but also the intermediate steps is important forevaluation, development, and identifying failures during inference time. Tobridge this gap, we introduce ToolComp, a comprehensive benchmark designed toevaluate multi-step tool-use reasoning. ToolComp is developed through acollaboration between models and human annotators, featuringhuman-edited/verified prompts, final answers, and process supervision labels,allowing for the evaluation of both final outcomes and intermediate reasoning.Evaluation across six different model families demonstrates the challengingnature of our dataset, with the majority of models achieving less than 50%accuracy. Additionally, we generate synthetic training data to compare theperformance of outcome-supervised reward models (ORMs) with process-supervisedreward models (PRMs) to assess their ability to improve complex tool-usereasoning as evaluated by ToolComp. Our results show that PRMs generalizesignificantly better than ORMs, achieving a 19% and 11% improvement in rank@1accuracy for ranking base and fine-tuned model trajectories, respectively.These findings highlight the critical role of process supervision in both theevaluation and training of AI models, paving the way for more robust andcapable systems in complex, multi-step tool-use tasks.</description><author>Vaskar Nath, Pranav Raja, Claire Yoon, Sean Hendryx</author><pubDate>Thu, 02 Jan 2025 15:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01290v1</guid></item><item><title>SAP: Corrective Machine Unlearning with Scaled Activation Projection for Label Noise Robustness</title><link>http://arxiv.org/abs/2403.08618v2</link><description>Label corruption, where training samples are mislabeled due to non-expertannotation or adversarial attacks, significantly degrades model performance.Acquiring large, perfectly labeled datasets is costly, and retraining modelsfrom scratch is computationally expensive. To address this, we introduce ScaledActivation Projection (SAP), a novel SVD (Singular Value Decomposition)-basedcorrective machine unlearning algorithm. SAP mitigates label noise byidentifying a small subset of trusted samples using cross-entropy loss andprojecting model weights onto a clean activation space estimated using SVD onthese trusted samples. This process suppresses the noise introduced inactivations due to the mislabeled samples. In our experiments, we demonstrateSAP's effectiveness on synthetic noise with different settings and real-worldlabel noise. SAP applied to the CIFAR dataset with 25% synthetic corruptionshow upto 6% generalization improvements. Additionally, SAP can improve thegeneralization over noise robust training approaches on CIFAR dataset by ~3.2%on average. Further, we observe generalization improvements of 2.31% for aVision Transformer model trained on naturally corrupted Clothing1M.</description><author>Sangamesh Kodge, Deepak Ravikumar, Gobinda Saha, Kaushik Roy</author><pubDate>Thu, 02 Jan 2025 15:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08618v2</guid></item><item><title>OCTAMamba: A State-Space Model Approach for Precision OCTA Vasculature Segmentation</title><link>http://arxiv.org/abs/2409.08000v2</link><description>Optical Coherence Tomography Angiography (OCTA) is a crucial imagingtechnique for visualizing retinal vasculature and diagnosing eye diseases suchas diabetic retinopathy and glaucoma. However, precise segmentation of OCTAvasculature remains challenging due to the multi-scale vessel structures andnoise from poor image quality and eye lesions. In this study, we proposedOCTAMamba, a novel U-shaped network based on the Mamba architecture, designedto segment vasculature in OCTA accurately. OCTAMamba integrates a Quad StreamEfficient Mining Embedding Module for local feature extraction, a Multi-ScaleDilated Asymmetric Convolution Module to capture multi-scale vasculature, and aFocused Feature Recalibration Module to filter noise and highlight targetareas. Our method achieves efficient global modeling and local featureextraction while maintaining linear complexity, making it suitable forlow-computation medical applications. Extensive experiments on the OCTA 3M,OCTA 6M, and ROSSA datasets demonstrated that OCTAMamba outperformsstate-of-the-art methods, providing a new reference for efficient OCTAsegmentation. Code is available at https://github.com/zs1314/OCTAMamba</description><author>Shun Zou, Zhuo Zhang, Guangwei Gao</author><pubDate>Thu, 02 Jan 2025 15:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08000v2</guid></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>http://arxiv.org/abs/2412.21079v2</link><description>As a verified need, consistent editing across in-the-wild images remains atechnical challenge arising from various unmanageable factors, like objectposes, lighting conditions, and photography environments. Edicho steps in witha training-free solution based on diffusion models, featuring a fundamentaldesign principle of using explicit image correspondence to direct editing.Specifically, the key components include an attention manipulation module and acarefully refined classifier-free guidance (CFG) denoising strategy, both ofwhich take into account the pre-estimated correspondence. Such aninference-time algorithm enjoys a plug-and-play nature and is compatible tomost diffusion-based editing methods, such as ControlNet and BrushNet.Extensive results demonstrate the efficacy of Edicho in consistent cross-imageediting under diverse settings. We will release the code to facilitate futurestudies.</description><author>Qingyan Bai, Hao Ouyang, Yinghao Xu, Qiuyu Wang, Ceyuan Yang, Ka Leong Cheng, Yujun Shen, Qifeng Chen</author><pubDate>Thu, 02 Jan 2025 15:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21079v2</guid></item><item><title>Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions</title><link>http://arxiv.org/abs/2309.01115v3</link><description>This study presents a general analytical framework using DBSCAN clusteringand penalized regression models to address multifactor problems with structuralcomplexity and multicollinearity issues, such as carbon emission issue. Theframework leverages DBSCAN for unsupervised learning to objectively clusterfeatures. Meanwhile, penalized regression considers model complexity controland high dimensional feature selection to identify dominant influencingfactors. Applying this framework to analyze energy consumption data for 46industries from 2000 to 2019 identified 16 categories in the sample of China.We quantitatively assessed emission characteristics and drivers for each. Theresults demonstrate the framework's analytical approach can identify primaryemission sources by category, providing quantitative references fordecision-making. Overall, this framework can evaluate complex regional issueslike carbon emissions to support policymaking. This research preliminarilyvalidated its application value in identifying opportunities for emissionreduction worldwide.</description><author>Xuanming Zhang, Xiaoxue Wang, Yonghang Chen</author><pubDate>Thu, 02 Jan 2025 14:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01115v3</guid></item><item><title>Optimized Relay Lens Design For High-Resolution Image Transmission In Military Target Detection Systems</title><link>http://arxiv.org/abs/2501.01287v1</link><description>The design and performance analysis of relay lenses that providehigh-performance image transmission for target acquisition and tracking inmilitary optical systems. Relay lenses are critical components for clear andlossless image transmission over long distances. In this study, the opticalperformance of a relay lens system designed and optimized using ZEMAX softwareis investigated in detail. The analysis focuses on important optical propertiessuch as modulation transfer function (MTF), spot diagrams, Seidel diagram,field curvature and distortion. The results show that the lens has significantpotential in military applications for target detection and tracking with highresolution and low aberration.</description><author>Burak Celik, Kivanc Dogan, Ezgi Taskin, Ayhan Akbal, Ahmet Orhan</author><pubDate>Thu, 02 Jan 2025 14:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01287v1</guid></item><item><title>NeutraSum: A Language Model can help a Balanced Media Diet by Neutralizing News Summaries</title><link>http://arxiv.org/abs/2501.01284v1</link><description>Media bias in news articles arises from the political polarisation of mediaoutlets, which can reinforce societal stereotypes and beliefs. Reporting on thesame event often varies significantly between outlets, reflecting theirpolitical leanings through polarised language and focus. Although previousstudies have attempted to generate bias-free summaries from multiperspectivenews articles, they have not effectively addressed the challenge of mitigatinginherent media bias. To address this gap, we propose \textbf{NeutraSum}, anovel framework that integrates two neutrality losses to adjust the semanticspace of generated summaries, thus minimising media bias. These losses,designed to balance the semantic distances across polarised inputs and ensurealignment with expert-written summaries, guide the generation of neutral andfactually rich summaries. To evaluate media bias, we employ the politicalcompass test, which maps political leanings based on economic and socialdimensions. Experimental results on the Allsides dataset demonstrate thatNeutraSum not only improves summarisation performance but also achievessignificant reductions in media bias, offering a promising approach for neutralnews summarisation.</description><author>Xi Luo, Junjie Liu, Sirong Wu, Yuhui Deng</author><pubDate>Thu, 02 Jan 2025 14:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01284v1</guid></item><item><title>CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries</title><link>http://arxiv.org/abs/2501.01282v1</link><description>Vision-language models (VLMs) have advanced human-AI interaction but strugglewith cultural understanding, often misinterpreting symbols, gestures, andartifacts due to biases in predominantly Western-centric training data. In thispaper, we construct CultureVerse, a large-scale multimodal benchmark covering19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3question types, with the aim of characterizing and improving VLMs'multicultural understanding capabilities. Then, we propose CultureVLM, a seriesof VLMs fine-tuned on our dataset to achieve significant performanceimprovement in cultural understanding. Our evaluation of 16 models revealssignificant disparities, with a stronger performance in Western concepts andweaker results in African and Asian contexts. Fine-tuning on our CultureVerseenhances cultural perception, demonstrating cross-cultural, cross-continent,and cross-dataset generalization without sacrificing performance on models'general VLM benchmarks. We further present insights on cultural generalizationand forgetting. We hope that this work could lay the foundation for moreequitable and culturally aware multimodal AI systems.</description><author>Shudong Liu, Yiqiao Jin, Cheng Li, Derek F. Wong, Qingsong Wen, Lichao Sun, Haipeng Chen, Xing Xie, Jindong Wang</author><pubDate>Thu, 02 Jan 2025 14:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01282v1</guid></item><item><title>Athanor: Local Search over Abstract Constraint Specifications</title><link>http://arxiv.org/abs/2410.05937v2</link><description>Local search is a common method for solving combinatorial optimisationproblems. We focus on general-purpose local search solvers that accept as inputa constraint model - a declarative description of a problem consisting of a setof decision variables under a set of constraints. Existing approaches typicallytake as input models written in solver-independent constraint modellinglanguages like MiniZinc. The Athanor solver we describe herein differs in thatit begins from a specification of a problem in the abstract constraintspecification language Essence, which allows problems to be described withoutcommitment to low-level modelling decisions through its support for a rich setof abstract types. The advantage of proceeding from Essence is that thestructure apparent in a concise, abstract specification of a problem can beexploited to generate high quality neighbourhoods automatically, avoiding thedifficult task of identifying that structure in an equivalent constraint model.Based on the twin benefits of neighbourhoods derived from high level types andthe scalability derived by searching directly over those types, our empiricalresults demonstrate strong performance in practice relative to existingsolution methods.</description><author>Saad Attieh, Nguyen Dang, Christopher Jefferson, Ian Miguel, Peter Nightingale</author><pubDate>Thu, 02 Jan 2025 14:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05937v2</guid></item><item><title>RiTTA: Modeling Event Relations in Text-to-Audio Generation</title><link>http://arxiv.org/abs/2412.15922v2</link><description>Despite significant advancements in Text-to-Audio (TTA) generation modelsachieving high-fidelity audio with fine-grained context understanding, theystruggle to model the relations between audio events described in the inputtext. However, previous TTA methods have not systematically explored audioevent relation modeling, nor have they proposed frameworks to enhance thiscapability. In this work, we systematically study audio event relation modelingin TTA generation models. We first establish a benchmark for this task by: 1.proposing a comprehensive relation corpus covering all potential relations inreal-world scenarios; 2. introducing a new audio event corpus encompassingcommonly heard audios; and 3. proposing new evaluation metrics to assess audioevent relation modeling from various perspectives. Furthermore, we propose afinetuning framework to enhance existing TTA models ability to model audioevents relation. Code is available at: https://github.com/yuhanghe01/RiTTA</description><author>Yuhang He, Yash Jain, Xubo Liu, Andrew Markham, Vibhav Vineet</author><pubDate>Thu, 02 Jan 2025 14:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15922v2</guid></item><item><title>Predictive Model Development to Identify Failed Healing in Patients after Non-Union Fracture Surgery</title><link>http://arxiv.org/abs/2404.11760v3</link><description>Bone non-union is among the most severe complications associated with traumasurgery, occurring in 10-30% of cases after long bone fractures. Treatingnon-unions requires a high level of surgical expertise and often involvesmultiple revision surgeries, sometimes even leading to amputation. Thus, moreaccurate prognosis is crucial for patient well-being. Recent advances inmachine learning (ML) hold promise for developing models to predict non-unionhealing, even when working with smaller datasets, a commonly encounteredchallenge in clinical domains. To demonstrate the effectiveness of ML inidentifying candidates at risk of failed non-union healing, we applied three MLmodels (logistic regression, support vector machine, and XGBoost) to theclinical dataset TRUFFLE, which includes 797 patients with long bone non-union.The models provided prediction results with 70% sensitivity, and thespecificities of 66% (XGBoost), 49% (support vector machine), and 43% (logisticregression). These findings offer valuable clinical insights because theyenable early identification of patients at risk of failed non-union healingafter the initial surgical revision treatment protocol.</description><author>Cedric Donié, Marie K. Reumann, Tony Hartung, Benedikt J. Braun, Tina Histing, Satoshi Endo, Sandra Hirche</author><pubDate>Thu, 02 Jan 2025 14:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11760v3</guid></item><item><title>Marketing Mix Modeling in Lemonade</title><link>http://arxiv.org/abs/2501.01276v1</link><description>Marketing mix modeling (MMM) is a widely used method to assess theeffectiveness of marketing campaigns and optimize marketing strategies.Bayesian MMM is an advanced approach that allows for the incorporation of priorinformation, uncertainty quantification, and probabilistic predictions (1). Inthis paper, we describe the process of building a Bayesian MMM model for theonline insurance company Lemonade. We first collected data on Lemonade'smarketing activities, such as online advertising, social media, and brandmarketing, as well as performance data. We then used a Bayesian framework toestimate the contribution of each marketing channel on total performance, whileaccounting for various factors such as seasonality, market trends, andmacroeconomic indicators. To validate the model, we compared its predictionswith the actual performance data from A/B-testing and sliding window holdoutdata (2). The results showed that the predicted contribution of each marketingchannel is aligned with A/B test performance and is actionable. Furthermore, weconducted several scenario analyses using convex optimization to test thesensitivity of the model to different assumptions and to evaluate the impact ofchanges in the marketing mix on sales. The insights gained from the modelallowed Lemonade to adjust their marketing strategy and allocate their budgetmore effectively. Our case study demonstrates the benefits of using BayesianMMM for marketing attribution and optimization in a data-driven company likeLemonade. The approach is flexible, interpretable, and can provide valuableinsights for decision-making.</description><author>Roy Ravid</author><pubDate>Thu, 02 Jan 2025 14:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01276v1</guid></item><item><title>HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking</title><link>http://arxiv.org/abs/2501.01275v1</link><description>The evolution of Advanced Driver Assistance Systems (ADAS) has increased theneed for robust and generalizable algorithms for multi-object tracking.Traditional statistical model-based tracking methods rely on predefined motionmodels and assumptions about system noise distributions. Althoughcomputationally efficient, they often lack adaptability to varying trafficscenarios and require extensive manual design and parameter tuning. To addressthese issues, we propose a novel 3D multi-object tracking approach forvehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) withina tracking-by-detection paradigm. In particular, it learns the transitionresidual and Kalman gain directly from data, which eliminates the need formanual motion and stochastic parameter modeling. Validated on the real-worldKITTI dataset, HybridTrack achieves 82.08% HOTA accuracy, significantlyoutperforming state-of-the-art methods. We also evaluate our method underdifferent configurations, achieving the fastest processing speed of 112 FPS.Consequently, HybridTrack eliminates the dependency on scene-specific designswhile improving performance and maintaining real-time efficiency. The code willbe publicly available at the time of publishing:https://github.com/leandro-svg/HybridTrack.git.</description><author>Leandro Di Bella, Yangxintong Lyu, Bruno Cornelis, Adrian Munteanu</author><pubDate>Thu, 02 Jan 2025 14:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01275v1</guid></item><item><title>Does a Large Language Model Really Speak in Human-Like Language?</title><link>http://arxiv.org/abs/2501.01273v1</link><description>Large Language Models (LLMs) have recently emerged, attracting considerableattention due to their ability to generate highly natural, human-like text.This study compares the latent community structures of LLM-generated text andhuman-written text within a hypothesis testing procedure. Specifically, weanalyze three text sets: original human-written texts ($\mathcal{O}$), theirLLM-paraphrased versions ($\mathcal{G}$), and a twice-paraphrased set($\mathcal{S}$) derived from $\mathcal{G}$. Our analysis addresses two keyquestions: (1) Is the difference in latent community structures between$\mathcal{O}$ and $\mathcal{G}$ the same as that between $\mathcal{G}$ and$\mathcal{S}$? (2) Does $\mathcal{G}$ become more similar to $\mathcal{O}$ asthe LLM parameter controlling text variability is adjusted? The first questionis based on the assumption that if LLM-generated text truly resembles humanlanguage, then the gap between the pair ($\mathcal{O}$, $\mathcal{G}$) shouldbe similar to that between the pair ($\mathcal{G}$, $\mathcal{S}$), as bothpairs consist of an original text and its paraphrase. The second questionexamines whether the degree of similarity between LLM-generated and human textvaries with changes in the breadth of text generation. To address thesequestions, we propose a statistical hypothesis testing framework that leveragesthe fact that each text has corresponding parts across all datasets due totheir paraphrasing relationship. This relationship enables the mapping of onedataset's relative position to another, allowing two datasets to be mapped to athird dataset. As a result, both mapped datasets can be quantified with respectto the space characterized by the third dataset, facilitating a directcomparison between them. Our results indicate that GPT-generated text remainsdistinct from human-authored text.</description><author>Mose Park, Yunjin Choi, Jong-June Jeon</author><pubDate>Thu, 02 Jan 2025 14:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01273v1</guid></item><item><title>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</title><link>http://arxiv.org/abs/2311.05546v4</link><description>Multi-Agent Reinforcement Learning is becoming increasingly more important intimes of autonomous driving and other smart industrial applications.Simultaneously a promising new approach to Reinforcement Learning arises usingthe inherent properties of quantum mechanics, reducing the trainable parametersof a model significantly. However, gradient-based Multi-Agent QuantumReinforcement Learning methods often have to struggle with barren plateaus,holding them back from matching the performance of classical approaches. Whilegradient free Quantum Reinforcement Learning methods may alleviate some ofthese challenges, they too are not immune to the difficulties posed by barrenplateaus. We build upon an existing approach for gradient free QuantumReinforcement Learning and propose three genetic variations with VariationalQuantum Circuits for Multi-Agent Reinforcement Learning using evolutionaryoptimization. We evaluate our genetic variations in the Coin Game environmentand also compare them to classical approaches. We showed that our VariationalQuantum Circuit approaches perform significantly better compared to a neuralnetwork with a similar amount of trainable parameters. Compared to the largerneural network, our approaches archive similar results using $97.88\%$ lessparameters.</description><author>Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien</author><pubDate>Thu, 02 Jan 2025 14:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05546v4</guid></item><item><title>PIMAEX: Multi-Agent Exploration through Peer Incentivization</title><link>http://arxiv.org/abs/2501.01266v1</link><description>While exploration in single-agent reinforcement learning has been studiedextensively in recent years, considerably less work has focused on itscounterpart in multi-agent reinforcement learning. To address this issue, thiswork proposes a peer-incentivized reward function inspired by previous researchon intrinsic curiosity and influence-based rewards. The \textit{PIMAEX} reward,short for Peer-Incentivized Multi-Agent Exploration, aims to improveexploration in the multi-agent setting by encouraging agents to exert influenceover each other to increase the likelihood of encountering novel states. Weevaluate the \textit{PIMAEX} reward in conjunction with\textit{PIMAEX-Communication}, a multi-agent training algorithm that employs acommunication channel for agents to influence one another. The evaluation isconducted in the \textit{Consume/Explore} environment, a partially observableenvironment with deceptive rewards, specifically designed to challenge theexploration vs.\ exploitation dilemma and the credit-assignment problem. Theresults empirically demonstrate that agents using the \textit{PIMAEX} rewardwith \textit{PIMAEX-Communication} outperform those that do not.</description><author>Michael Kölle, Johannes Tochtermann, Julian Schönberger, Gerhard Stenzel, Philipp Altmann, Claudia Linnhoff-Popien</author><pubDate>Thu, 02 Jan 2025 14:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01266v1</guid></item><item><title>Tensor-Based Foundations of Ordinary Least Squares and Neural Network Regression Models</title><link>http://arxiv.org/abs/2411.12873v3</link><description>This article introduces a novel approach to the mathematical development ofOrdinary Least Squares and Neural Network regression models, diverging fromtraditional methods in current Machine Learning literature. By leveragingTensor Analysis and fundamental matrix computations, the theoreticalfoundations of both models are meticulously detailed and extended to theircomplete algorithmic forms. The study culminates in the presentation of threealgorithms, including a streamlined version of the Backpropagation Algorithmfor Neural Networks, illustrating the benefits of this new mathematicalapproach.</description><author>Roberto Dias Algarte</author><pubDate>Thu, 02 Jan 2025 14:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12873v3</guid></item><item><title>ProgCo: Program Helps Self-Correction of Large Language Models</title><link>http://arxiv.org/abs/2501.01264v1</link><description>Self-Correction aims to enable large language models (LLMs) to self-verifyand self-refine their initial responses without external feedback. However,LLMs often fail to effectively self-verify and generate correct feedback,further misleading refinement and leading to the failure of self-correction,especially in complex reasoning tasks. In this paper, we propose Program-drivenSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achievescomplex verification logic and extensive validation through self-generated,self-executing verification pseudo-programs. Then, program-driven refinement(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinementon both responses and verification programs to mitigate misleading of incorrectfeedback in complex reasoning tasks. Experiments on three instruction-followingand mathematical benchmarks indicate that ProgCo achieves effectiveself-correction, and can be further enhance performance when combined with realprogram tools.</description><author>Xiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng</author><pubDate>Thu, 02 Jan 2025 13:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01264v1</guid></item><item><title>Stealthy Backdoor Attack to Real-world Models in Android Apps</title><link>http://arxiv.org/abs/2501.01263v1</link><description>Powered by their superior performance, deep neural networks (DNNs) have foundwidespread applications across various domains. Many deep learning (DL) modelsare now embedded in mobile apps, making them more accessible to end usersthrough on-device DL. However, deploying on-device DL to users' smartphonessimultaneously introduces several security threats. One primary threat isbackdoor attacks. Extensive research has explored backdoor attacks for severalyears and has proposed numerous attack approaches. However, few studies haveinvestigated backdoor attacks on DL models deployed in the real world, or theyhave shown obvious deficiencies in effectiveness and stealthiness. In thiswork, we explore more effective and stealthy backdoor attacks on real-world DLmodels extracted from mobile apps. Our main justification is that imperceptibleand sample-specific backdoor triggers generated by DNN-based steganography canenhance the efficacy of backdoor attacks on real-world models. We first confirmthe effectiveness of steganography-based backdoor attacks on fourstate-of-the-art DNN models. Subsequently, we systematically evaluate andanalyze the stealthiness of the attacks to ensure they are difficult toperceive. Finally, we implement the backdoor attacks on real-world models andcompare our approach with three baseline methods. We collect 38,387 mobileapps, extract 89 DL models from them, and analyze these models to obtain theprerequisite model information for the attacks. After identifying the targetmodels, our approach achieves an average of 12.50% higher attack success ratethan DeepPayload while better maintaining the normal performance of the models.Extensive experimental results demonstrate that our method enables moreeffective, robust, and stealthy backdoor attacks on real-world models.</description><author>Jiali Wei, Ming Fan, Xicheng Zhang, Wenjing Jiao, Haijun Wang, Ting Liu</author><pubDate>Thu, 02 Jan 2025 13:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01263v1</guid></item><item><title>Detail Matters: Mamba-Inspired Joint Unfolding Network for Snapshot Spectral Compressive Imaging</title><link>http://arxiv.org/abs/2501.01262v1</link><description>In the coded aperture snapshot spectral imaging system, Deep UnfoldingNetworks (DUNs) have made impressive progress in recovering 3D hyperspectralimages (HSIs) from a single 2D measurement. However, the inherent nonlinear andill-posed characteristics of HSI reconstruction still pose challenges toexisting methods in terms of accuracy and stability. To address this issue, wepropose a Mamba-inspired Joint Unfolding Network (MiJUN), which integratesphysics-embedded DUNs with learning-based HSI imaging. Firstly, leveraging theconcept of trapezoid discretization to expand the representation space ofunfolding networks, we introduce an accelerated unfolding network scheme. Thisapproach can be interpreted as a generalized accelerated half-quadraticsplitting with a second-order differential equation, which reduces the relianceon initial optimization stages and addresses challenges related to long-rangeinteractions. Crucially, within the Mamba framework, we restructure theMamba-inspired global-to-local attention mechanism by incorporating a selectivestate space model and an attention mechanism. This effectively reinterpretsMamba as a variant of the Transformer} architecture, improving its adaptabilityand efficiency. Furthermore, we refine the scanning strategy with Mamba byintegrating the tensor mode-$k$ unfolding into the Mamba network. This approachemphasizes the low-rank properties of tensors along various modes, whileconveniently facilitating 12 scanning directions. Numerical and visualcomparisons on both simulation and real datasets demonstrate the superiority ofour proposed MiJUN, and achieving overwhelming detail representation.</description><author>Mengjie Qin, Yuchao Feng, Zongliang Wu, Yulun Zhang, Xin Yuan</author><pubDate>Thu, 02 Jan 2025 13:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01262v1</guid></item><item><title>Detecting Financial Bots on the Ethereum Blockchain</title><link>http://arxiv.org/abs/2403.19530v2</link><description>The integration of bots in Distributed Ledger Technologies (DLTs) fostersefficiency and automation. However, their use is also associated with predatorytrading and market manipulation, and can pose threats to system integrity. Itis therefore essential to understand the extent of bot deployment in DLTs;despite this, current detection systems are predominantly rule-based and lackflexibility. In this study, we present a novel approach that utilizes machinelearning for the detection of financial bots on the Ethereum platform. First,we systematize existing scientific literature and collect anecdotal evidence toestablish a taxonomy for financial bots, comprising 7 categories and 24subcategories. Next, we create a ground-truth dataset consisting of 133 humanand 137 bot addresses. Third, we employ both unsupervised and supervisedmachine learning algorithms to detect bots deployed on Ethereum. Thehighest-performing clustering algorithm is a Gaussian Mixture Model with anaverage cluster purity of 82.6%, while the highest-performing model for binaryclassification is a Random Forest with an accuracy of 83%. Our machinelearning-based detection mechanism contributes to understanding the Ethereumecosystem dynamics by providing additional insights into the current botlandscape.</description><author>Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer</author><pubDate>Thu, 02 Jan 2025 13:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19530v2</guid></item><item><title>Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents</title><link>http://arxiv.org/abs/2407.01887v3</link><description>In-context reinforcement learning (ICRL) is a frontier paradigm for solvingreinforcement learning problems in the foundation model era. While ICRLcapabilities have been demonstrated in transformers through task-specifictraining, the potential of Large Language Models (LLMs) out-of-the-box remainslargely unexplored. Recent findings highlight that LLMs often face challengeswhen dealing with numerical contexts, and limited attention has been paid toevaluating their performance through preference feedback generated by theenvironment. This paper is the first to investigate LLMs as in-contextdecision-makers under the problem of Dueling Bandits (DB), a statelesspreference-based reinforcement learning setting that extends the classicMulti-Armed Bandit (MAB) model by querying for preference feedback. We compareGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against ninewell-established DB algorithms. Our results reveal that our top-performing LLM,GPT-4 Turbo, has the zero-shot relative decision-making ability to achievesurprisingly low weak regret across all the DB environment instances by quicklyincluding the best arm in duels. However, an optimality gap exists between LLMsand classic DB algorithms in terms of strong regret. LLMs struggle to convergeand consistently exploit even when explicitly prompted to do so, and aresensitive to prompt variations. To bridge this gap, we propose an agentic flowframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integratesoff-the-shelf DB algorithms with LLM agents through fine-grained adaptiveinterplay. We show that LEAD has theoretical guarantees inherited from classicDB algorithms on both weak and strong regret. We validate its efficacy androbustness even with noisy and adversarial prompts. The design of our frameworksheds light on how to enhance the trustworthiness of LLMs used for in-contextdecision-making.</description><author>Fanzeng Xia, Hao Liu, Yisong Yue, Tongxin Li</author><pubDate>Thu, 02 Jan 2025 13:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01887v3</guid></item></channel></rss>