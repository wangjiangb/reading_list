<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 02 Sep 2024 01:01:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model Transformer for Multimodal Aspect-based Sentiment Analysis</title><link>http://arxiv.org/abs/2408.15379v2</link><description>Multimodal aspect-based sentiment analysis (MABSA) enhances sentimentdetection by combining text with other data types like images. However, despitesetting significant benchmarks, attention mechanisms exhibit limitations inefficiently modelling long-range dependencies between aspect and opiniontargets within the text. They also face challenges in capturing global-contextdependencies for visual representations. To this end, we proposeKolmogorov-Arnold Networks (KANs) and Selective State Space model (Mamba)transformer (DualKanbaFormer), a novel architecture to address the aboveissues. We leverage the power of Mamba to capture global context dependencies,Multi-head Attention (MHA) to capture local context dependencies, and KANs tocapture non-linear modelling patterns for both textual representations (textualKanbaFormer) and visual representations (visual KanbaFormer). Furthermore, wefuse the textual KanbaFormer and visual KanbaFomer with a gated fusion layer tocapture the inter-modality dynamics. According to extensive experimentalresults, our model outperforms some state-of-the-art (SOTA) studies on twopublic datasets.</description><author>Adamu Lawan, Juhua Pu, Haruna Yunusa, Muhammad Lawan, Aliyu Umar, Adamu Sani Yahya</author><pubDate>Fri, 30 Aug 2024 16:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15379v2</guid></item><item><title>A Permuted Autoregressive Approach to Word-Level Recognition for Urdu Digital Text</title><link>http://arxiv.org/abs/2408.15119v3</link><description>This research paper introduces a novel word-level Optical CharacterRecognition (OCR) model specifically designed for digital Urdu text, leveragingtransformer-based architectures and attention mechanisms to address thedistinct challenges of Urdu script recognition, including its diverse textstyles, fonts, and variations. The model employs a permuted autoregressivesequence (PARSeq) architecture, which enhances its performance by enablingcontext-aware inference and iterative refinement through the training ofmultiple token permutations. This method allows the model to adeptly managecharacter reordering and overlapping characters, commonly encountered in Urduscript. Trained on a dataset comprising approximately 160,000 Urdu text images,the model demonstrates a high level of accuracy in capturing the intricacies ofUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handlingcertain text variations, the model exhibits superior accuracy and effectivenessin practical applications. Future work will focus on refining the model throughadvanced data augmentation techniques and the integration of context-awarelanguage models to further enhance its performance and robustness in Urdu textrecognition.</description><author>Ahmed Mustafa, Muhammad Tahir Rafique, Muhammad Ijlal Baig, Hasan Sajid, Muhammad Jawad Khan, Karam Dad Kallu</author><pubDate>Fri, 30 Aug 2024 15:29:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15119v3</guid></item><item><title>OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset</title><link>http://arxiv.org/abs/2406.11933v3</link><description>Masked Image Modeling (MIM) has become an essential method for buildingfoundational visual models in remote sensing (RS). However, the limitations insize and diversity of existing RS datasets restrict the ability of MIM methodsto learn generalizable representations. Additionally, conventional MIMtechniques, which require reconstructing all tokens, introduce unnecessarycomputational overhead. To address these issues, we present a new pre-trainingpipeline for RS models, featuring the creation of a large-scale RS dataset andan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4Mby collecting publicly available RS datasets and processing them throughexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million opticalimages covering various RS tasks, such as object detection and pixelsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-trainingmethod that dynamically encodes and reconstructs semantically rich patchtokens, thereby reducing the inefficiencies of traditional MIM models caused byredundant background pixels in RS images. Extensive experiments demonstratethat OpticalRS-4M significantly improves classification, detection, andsegmentation performance, while SelectiveMAE increases training efficiency over2 times. This highlights the effectiveness and scalability of our pipeline indeveloping RS foundational models.</description><author>Fengxiang Wang, Hongzhen Wang, Di Wang, Zonghao Guo, Zhenyu Zhong, Long Lan, Jing Zhang, Zhiyuan Liu, Maosong Sun</author><pubDate>Fri, 30 Aug 2024 15:08:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11933v3</guid></item><item><title>Docling Technical Report</title><link>http://arxiv.org/abs/2408.09869v3</link><description>This technical report introduces Docling, an easy to use, self-contained,MIT-licensed open-source package for PDF document conversion. It is powered bystate-of-the-art specialized AI models for layout analysis (DocLayNet) andtable structure recognition (TableFormer), and runs efficiently on commodityhardware in a small resource budget. The code interface allows for easyextensibility and addition of new features and models.</description><author>Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, Lokesh Mishra, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar</author><pubDate>Fri, 30 Aug 2024 15:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09869v3</guid></item><item><title>Temporal Ensemble Logic</title><link>http://arxiv.org/abs/2408.14443v2</link><description>We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modallogic for linear-time temporal reasoning. TEL includes primitive temporalconstructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimesbefore $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''$\varphi_t$. TEL has been motivated from the requirement for rigor andreproducibility for cohort specification and discovery in clinical andpopulation health research, to fill a gap in formalizing temporal reasoning inbiomedicine. Existing logical frameworks such as linear temporal logic are toorestrictive to express temporal and sequential properties in biomedicine, ortoo permissive in semantic constructs, such as in Halpern-Shoham logic, toserve this purpose. In this paper, we first introduce TEL in a general set up,with discrete and dense time as special cases. We then focus on the theoreticaldevelopment of discrete TEL on the temporal domain of positive integers$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rmTEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadicsecond order logic, characterized by B\"{u}chi automata. We present its formalsemantics, a proof system, and provide a proof for the undecidability of thesatisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial resultson expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,followed by application outlook and discussions.</description><author>Guo-Qiang Zhang</author><pubDate>Fri, 30 Aug 2024 14:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14443v2</guid></item><item><title>Revisiting 360 Depth Estimation with PanoGabor: A New Fusion Perspective</title><link>http://arxiv.org/abs/2408.16227v2</link><description>Depth estimation from a monocular 360 image is important to the perception ofthe entire 3D environment. However, the inherent distortion and large field ofview (FoV) in 360 images pose great challenges for this task. To this end,existing mainstream solutions typically introduce additional perspective-based360 representations (\textit{e.g.}, Cubemap) to achieve effective featureextraction. Nevertheless, regardless of the introduced representations, theyeventually need to be unified into the equirectangular projection (ERP) formatfor the subsequent depth estimation, which inevitably reintroduces thetroublesome distortions. In this work, we propose an oriented distortion-awareGabor Fusion framework (PGFuse) to address the above challenges. First, weintroduce Gabor filters that analyze texture in the frequency domain, therebyextending the receptive fields and enhancing depth cues. To address thereintroduced distortions, we design a linear latitude-aware distortionrepresentation method to generate customized, distortion-aware Gabor filters(PanoGabor filters). Furthermore, we design a channel-wise and spatial-wiseunidirectional fusion module (CS-UFM) that integrates the proposed PanoGaborfilters to unify other representations into the ERP format, deliveringeffective and distortion-free features. Considering the orientation sensitivityof the Gabor transform, we introduce a spherical gradient constraint tostabilize this sensitivity. Experimental results on three popular indoor 360benchmarks demonstrate the superiority of the proposed PGFuse to existingstate-of-the-art solutions. Code can be available upon acceptance.</description><author>Zhijie Shen, Chunyu Lin, Lang Nie, Kang Liao</author><pubDate>Fri, 30 Aug 2024 13:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16227v2</guid></item><item><title>Addressing the challenges of loop detection in agricultural environments</title><link>http://arxiv.org/abs/2408.15761v2</link><description>While visual SLAM systems are well studied and achieve impressive results inindoor and urban settings, natural, outdoor and open-field environments aremuch less explored and still present relevant research challenges. Visualnavigation and local mapping have shown a relatively good performance inopen-field environments. However, globally consistent mapping and long-termlocalization still depend on the robustness of loop detection and closure, forwhich the literature is scarce. In this work we propose a novel method to pavethe way towards robust loop detection in open fields, particularly inagricultural settings, based on local feature search and stereo geometricrefinement, with a final stage of relative pose estimation. Our methodconsistently achieves good loop detections, with a median error of 15cm. We aimto characterize open fields as a novel environment for loop detection,understanding the limitations and problems that arise when dealing with them.</description><author>Nicolás Soncini, Javier Civera, Taihú Pire</author><pubDate>Fri, 30 Aug 2024 13:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15761v2</guid></item><item><title>Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)</title><link>http://arxiv.org/abs/2408.15874v2</link><description>Outlier detection algorithms typically assign an outlier score to eachobservation in a dataset, indicating the degree to which an observation is anoutlier. However, these scores are often not comparable across algorithms andcan be difficult for humans to interpret. Statistical scaling addresses thisproblem by transforming outlier scores into outlier probabilities without usingground-truth labels, thereby improving interpretability and comparabilityacross algorithms. However, the quality of this transformation can be differentfor outliers and inliers. Missing outliers in scenarios where they are ofparticular interest - such as healthcare, finance, or engineering - can becostly or dangerous. Thus, ensuring good probabilities for outliers isessential. This paper argues that statistical scaling, as commonly used in theliterature, does not produce equally good probabilities for outliers as forinliers. Therefore, we propose robust statistical scaling, which uses robustestimators to improve the probabilities for outliers. We evaluate severalvariants of our method against other outlier score transformations forreal-world datasets and outlier detection algorithms, where it can improve theprobabilities for outliers.</description><author>Philipp Röchner, Henrique O. Marques, Ricardo J. G. B. Campello, Arthur Zimek, Franz Rothlauf</author><pubDate>Fri, 30 Aug 2024 11:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15874v2</guid></item><item><title>Evidential Deep Partial Multi-View Classification With Discount Fusion</title><link>http://arxiv.org/abs/2408.13123v3</link><description>Incomplete multi-view data classification poses significant challenges due tothe common issue of missing views in real-world scenarios. Despiteadvancements, existing methods often fail to provide reliable predictions,largely due to the uncertainty of missing views and the inconsistent quality ofimputed data. To tackle these problems, we propose a novel framework calledEvidential Deep Partial Multi-View Classification (EDP-MVC). Initially, we useK-means imputation to address missing views, creating a complete set ofmulti-view data. However, the potential conflicts and uncertainties within thisimputed data can affect the reliability of downstream inferences. To managethis, we introduce a Conflict-Aware Evidential Fusion Network (CAEFN), whichdynamically adjusts based on the reliability of the evidence, ensuringtrustworthy discount fusion and producing reliable inference outcomes.Comprehensive experiments on various benchmark datasets reveal EDP-MVC not onlymatches but often surpasses the performance of state-of-the-art methods.</description><author>Haojian Huang, Zhe Liu, Sukumar Letchmunan, Muhammet Deveci, Mingwei Lin, Weizhong Wang</author><pubDate>Fri, 30 Aug 2024 09:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13123v3</guid></item><item><title>A mathematical framework of intelligence and consciousness based on Riemannian Geometry</title><link>http://arxiv.org/abs/2407.11024v3</link><description>Understanding intelligence is a central pursuit in neuroscience, cognitivescience, and artificial intelligence. Intelligence encompasses learning,problem-solving, creativity, and even consciousness. Recent advancements ingeometric analysis have revealed new insights into high-dimensional informationrepresentation and organisation, exposing intrinsic data structures and dynamicprocesses within neural and artificial systems. However, a comprehensiveframework that unifies the static and dynamic aspects of intelligence is stilllacking. This manuscript proposes a mathematical framework based on Riemanniangeometry to describe the structure and dynamics of intelligence andconsciousness. Intelligence elements are conceptualised as tokens embedded in ahigh-dimensional space. The learned token embeddings capture theinterconnections of tokens across various scenarios and tasks, formingmanifolds in the intelligence space. Thought flow is depicted as the sequentialactivation of tokens along geodesics within these manifolds. During thenavigation of geodesics, consciousness, as a self-referential process,perceives the thought flow, evaluates it against predictions, and providesfeedback through prediction errors, adjusting the geodesic: non-zero predictionerrors, such as learning, lead to the restructuring of the curved manifolds,thus changing the geodesic of thought flow. This dynamic interaction integratesnew information, evolves the geometry and facilitates learning. The geometry ofintelligence guides consciousness, and consciousness structures the geometry ofintelligence. By integrating geometric concepts, this proposed theory offers aunified, mathematically framework for describing the structure and dynamics ofintelligence and consciousness. Applicable to biological and artificialintelligence, this framework may pave the way for future research and empiricalvalidation.</description><author>Meng Lu</author><pubDate>Fri, 30 Aug 2024 07:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11024v3</guid></item><item><title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</title><link>http://arxiv.org/abs/2408.15545v2</link><description>Scientific literature understanding is crucial for extracting targetedinformation and garnering insights, thereby significantly advancing scientificdiscovery. Despite the remarkable success of Large Language Models (LLMs), theyface challenges in scientific literature understanding, primarily due to (1) alack of scientific knowledge and (2) unfamiliarity with specialized scientifictasks. To develop an LLM specialized in scientific literature understanding, wepropose a hybrid strategy that integrates continual pre-training (CPT) andsupervised fine-tuning (SFT), to simultaneously infuse scientific domainknowledge and enhance instruction-following capabilities for domain-specifictasks.cIn this process, we identify two key challenges: (1) constructinghigh-quality CPT corpora, and (2) generating diverse SFT instructions. Weaddress these challenges through a meticulous pipeline, including PDF textextraction, parsing content error correction, quality filtering, and syntheticinstruction creation. Applying this strategy, we present a suite of LLMs:SciLitLLM, specialized in scientific literature understanding. These modelsdemonstrate promising performance on scientific literature understandingbenchmarks. Our contributions are threefold: (1) We present an effective framework thatintegrates CPT and SFT to adapt LLMs to scientific literature understanding,which can also be easily adapted to other domains. (2) We propose an LLM-basedsynthesis method to generate diverse and high-quality scientific instructions,resulting in a new instruction set -- SciLitIns -- for supervised fine-tuningin less-represented scientific domains. (3) SciLitLLM achieves promisingperformance improvements on scientific literature understanding benchmarks.</description><author>Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai</author><pubDate>Fri, 30 Aug 2024 06:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15545v2</guid></item><item><title>PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods</title><link>http://arxiv.org/abs/2407.06985v4</link><description>In domain-specific applications, GPT-4, augmented with precise prompts orRetrieval-Augmented Generation (RAG), shows notable potential but faces thecritical tri-lemma of performance, cost, and data privacy. High performancerequires sophisticated processing techniques, yet managing multiple agentswithin a complex workflow often proves costly and challenging. To address this,we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework.This systematizes domain-specific tasks by integrating precise questiondecomposition, advanced information retrieval, comprehensive summarization, andrigorous self-assessment. Given the concerns of cost and data privacy,enterprises are shifting from proprietary models like GPT-4 to custom models,striking a balance between cost, security, and performance. We developedindustrial practices leveraging online data and user feedback for efficientmodel tuning. This study provides best practice guidelines for applyingmulti-agent systems in domain-specific problem-solving and implementingeffective agent tuning strategies. Our empirical studies, particularly in thefinancial question-answering domain, demonstrate that our approach achieves95.0% of GPT-4's performance, while effectively managing costs and ensuringdata privacy.</description><author>Yiying Wang, Xiaojing Li, Binzhu Wang, Yueyang Zhou, Yingru Lin, Han Ji, Hong Chen, Jinshi Zhang, Fei Yu, Zewei Zhao, Song Jin, Renji Gong, Wanqing Xu</author><pubDate>Fri, 30 Aug 2024 06:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06985v4</guid></item><item><title>Identification of Prognostic Biomarkers for Stage III Non-Small Cell Lung Carcinoma in Female Nonsmokers Using Machine Learning</title><link>http://arxiv.org/abs/2408.16068v2</link><description>Lung cancer remains a leading cause of cancer-related deaths globally, withnon-small cell lung cancer (NSCLC) being the most common subtype. This studyaimed to identify key biomarkers associated with stage III NSCLC in non-smokingfemales using gene expression profiling from the GDS3837 dataset. UtilizingXGBoost, a machine learning algorithm, the analysis achieved a strongpredictive performance with an AUC score of 0.835. The top biomarkersidentified - CCAAT enhancer binding protein alpha (C/EBP-alpha), lactatedehydrogenase A4 (LDHA), UNC-45 myosin chaperone B (UNC-45B), checkpoint kinase1 (CHK1), and hypoxia-inducible factor 1 subunit alpha (HIF-1-alpha) - havebeen validated in the literature as being significantly linked to lung cancer.These findings highlight the potential of these biomarkers for early diagnosisand personalized therapy, emphasizing the value of integrating machine learningwith molecular profiling in cancer research.</description><author>Huili Zheng, Qimin Zhang, Yiru Gong, Zheyan Liu, Shaohan Chen</author><pubDate>Fri, 30 Aug 2024 03:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16068v2</guid></item><item><title>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</title><link>http://arxiv.org/abs/2408.16725v2</link><description>Recent advances in language models have achieved significant progress.GPT-4o, as a new milestone, has enabled real-time conversations with humans,demonstrating near-human natural fluency. Such human-computer interactionnecessitates models with the capability to perform reasoning directly with theaudio modality and generate output in streaming. However, this remains beyondthe reach of current academic models, as they typically depend on extra TTSsystems for speech synthesis, resulting in undesirable latency. This paperintroduces the Mini-Omni, an audio-based end-to-end conversational model,capable of real-time speech interaction. To achieve this capability, we proposea text-instructed speech generation method, along with batch-parallelstrategies during inference to further boost the performance. Our method alsohelps to retain the original model's language capabilities with minimaldegradation, enabling other works to establish real-time interactioncapabilities. We call this training method "Any Model Can Talk". We alsointroduce the VoiceAssistant-400K dataset to fine-tune models optimized forspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,open-source model for real-time speech interaction, offering valuable potentialfor future research.</description><author>Zhifei Xie, Changqiao Wu</author><pubDate>Fri, 30 Aug 2024 02:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16725v2</guid></item><item><title>LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models</title><link>http://arxiv.org/abs/2408.16224v2</link><description>Recent advances in large vision-language models (VLMs) typically employvision encoders based on the Vision Transformer (ViT) architecture. Thedivision of the images into patches by ViT results in a fragmented perception,thereby hindering the visual understanding capabilities of VLMs. In this paper,we propose an innovative enhancement to address this limitation by introducinga Scene Graph Expression (SGE) module in VLMs. This module extracts andstructurally expresses the complex semantic information within images, therebyimproving the foundational perception and understanding abilities of VLMs.Extensive experiments demonstrate that integrating our SGE module significantlyenhances the VLM's performance in vision-language tasks, indicating itseffectiveness in preserving intricate semantic details and facilitating bettervisual understanding.</description><author>Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng</author><pubDate>Fri, 30 Aug 2024 02:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16224v2</guid></item><item><title>Differentiable Edge-based OPC</title><link>http://arxiv.org/abs/2408.08969v3</link><description>Optical proximity correction (OPC) is crucial for pushing the boundaries ofsemiconductor manufacturing and enabling the continued scaling of integratedcircuits. While pixel-based OPC, termed as inverse lithography technology(ILT), has gained research interest due to its flexibility and precision. Itscomplexity and intricate features can lead to challenges in mask writing,increased defects, and higher costs, hence hindering widespread industrialadoption. In this paper, we propose DiffOPC, a differentiable OPC frameworkthat enjoys the virtue of both edge-based OPC and ILT. By employing a maskrule-aware gradient-based optimization approach, DiffOPC efficiently guidesmask edge segment movement during mask optimization, minimizing wafer error bypropagating true gradients from the cost function back to the mask edges. Ourapproach achieves lower edge placement error while reducing manufacturing costby half compared to state-of-the-art OPC techniques, bridging the gap betweenthe high accuracy of pixel-based OPC and the practicality required forindustrial adoption, thus offering a promising solution for advancedsemiconductor manufacturing.</description><author>Guojin Chen, Haoyu Yang, Haoxing Ren, Bei Yu, David Z. Pan</author><pubDate>Fri, 30 Aug 2024 02:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08969v3</guid></item><item><title>Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks</title><link>http://arxiv.org/abs/2408.16757v2</link><description>Detecting test-time distribution shift has emerged as a key capability forsafely deployed machine learning models, with the question being tackled undervarious guises in recent years. In this paper, we aim to provide a consolidatedview of the two largest sub-fields within the community: out-of-distribution(OOD) detection and open-set recognition (OSR). In particular, we aim toprovide rigorous empirical analysis of different methods across settings andprovide actionable takeaways for practitioners and researchers. Concretely, wemake the following contributions: (i) We perform rigorous cross-evaluationbetween state-of-the-art methods in the OOD detection and OSR settings andidentify a strong correlation between the performances of methods for them;(ii) We propose a new, large-scale benchmark setting which we suggest betterdisentangles the problem tackled by OOD detection and OSR, re-evaluatingstate-of-the-art OOD detection and OSR methods in this setting; (iii) Wesurprisingly find that the best performing method on standard benchmarks(Outlier Exposure) struggles when tested at scale, while scoring rules whichare sensitive to the deep feature magnitude consistently show promise; and (iv)We conduct empirical analysis to explain these phenomena and highlightdirections for future research. Code:https://github.com/Visual-AI/Dissect-OOD-OSR</description><author>Hongjun Wang, Sagar Vaze, Kai Han</author><pubDate>Fri, 30 Aug 2024 02:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16757v2</guid></item><item><title>Towards Graph Prompt Learning: A Survey and Beyond</title><link>http://arxiv.org/abs/2408.14520v2</link><description>Large-scale "pre-train and prompt learning" paradigms have demonstratedremarkable adaptability, enabling broad applications across diverse domainssuch as question answering, image recognition, and multimodal retrieval. Thisapproach fully leverages the potential of large-scale pre-trained models,reducing downstream data requirements and computational costs while enhancingmodel applicability across various tasks. Graphs, as versatile data structuresthat capture relationships between entities, play pivotal roles in fields suchas social network analysis, recommender systems, and biological graphs. Despitethe success of pre-train and prompt learning paradigms in Natural LanguageProcessing (NLP) and Computer Vision (CV), their application in graph domainsremains nascent. In graph-structured data, not only do the node and edgefeatures often have disparate distributions, but the topological structuresalso differ significantly. This diversity in graph data can lead toincompatible patterns or gaps between pre-training and fine-tuning ondownstream graphs. We aim to bridge this gap by summarizing methods foralleviating these disparities. This includes exploring prompt designmethodologies, comparing related techniques, assessing application scenariosand datasets, and identifying unresolved problems and challenges. This surveycategorizes over 100 relevant works in this field, summarizing general designprinciples and the latest applications, including text-attributed graphs,molecules, proteins, and recommendation systems. Through this extensive review,we provide a foundational understanding of graph prompt learning, aiming toimpact not only the graph mining community but also the broader ArtificialGeneral Intelligence (AGI) community.</description><author>Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou</author><pubDate>Fri, 30 Aug 2024 01:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14520v2</guid></item><item><title>A global AI community requires language-diverse publishing</title><link>http://arxiv.org/abs/2408.14772v2</link><description>In this provocation, we discuss the English dominance of the AI researchcommunity, arguing that the requirement for English language publishing upholdsand reinforces broader regimes of extraction in AI. While large language modelsand machine translation have been celebrated as a way to break down barriers,we regard their use as a symptom of linguistic exclusion of scientists andpotential readers. We propose alternative futures for a healthier publishingculture, organized around three themes: administering conferences in thelanguages of the country in which they are held, instructing peer reviewers notto adjudicate the language appropriateness of papers, and offeringopportunities to publish and present in multiple languages. We welcome newtranslations of this piece. Please contact the authors if you would like tocontribute one.</description><author>Haley Lepp, Parth Sarin</author><pubDate>Thu, 29 Aug 2024 19:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14772v2</guid></item><item><title>3D Whole-body Grasp Synthesis with Directional Controllability</title><link>http://arxiv.org/abs/2408.16770v1</link><description>Synthesizing 3D whole-bodies that realistically grasp objects is useful foranimation, mixed reality, and robotics. This is challenging, because the handsand body need to look natural w.r.t. each other, the grasped object, as well asthe local scene (i.e., a receptacle supporting the object). Only recent worktackles this, with a divide-and-conquer approach; it first generates a"guiding" right-hand grasp, and then searches for bodies that match this.However, the guiding-hand synthesis lacks controllability and receptacleawareness, so it likely has an implausible direction (i.e., a body can't matchthis without penetrating the receptacle) and needs corrections through majorpost-processing. Moreover, the body search needs exhaustive sampling and isexpensive. These are strong limitations. We tackle these with a novel methodcalled CWGrasp. Our key idea is that performing geometry-based reasoning "earlyon," instead of "too late," provides rich "control" signals for inference. Tothis end, CWGrasp first samples a plausible reaching-direction vector (usedlater for both the arm and hand) from a probabilistic model built viaraycasting from the object and collision checking. Then, it generates areaching body with a desired arm direction, as well as a "guiding" graspinghand with a desired palm direction that complies with the arm's one.Eventually, CWGrasp refines the body to match the "guiding" hand, whileplausibly contacting the scene. Notably, generating already-compatible "parts"greatly simplifies the "whole." Moreover, CWGrasp uniquely tackles both right-and left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.CWGrasp outperforms baselines, at lower runtime and budget, while allcomponents help performance. Code and models will be released.</description><author>Georgios Paschalidis, Romana Wilschut, Dimitrije Antić, Omid Taheri, Dimitrios Tzionas</author><pubDate>Thu, 29 Aug 2024 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16770v1</guid></item><item><title>PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning</title><link>http://arxiv.org/abs/2408.16769v1</link><description>Medical vision-language models (Med-VLMs) trained on large datasets ofmedical image-text pairs and later fine-tuned for specific tasks have emergedas a mainstream paradigm in medical image analysis. However, recent studieshave highlighted the susceptibility of these Med-VLMs to adversarial attacks,raising concerns about their safety and robustness. Randomized smoothing is awell-known technique for turning any classifier into a model that iscertifiably robust to adversarial perturbations. However, this approachrequires retraining the Med-VLM-based classifier so that it classifies wellunder Gaussian noise, which is often infeasible in practice. In this paper, wepropose a novel framework called PromptSmooth to achieve efficient certifiedrobustness of Med-VLMs by leveraging the concept of prompt learning. Given anypre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise bylearning textual prompts in a zero-shot or few-shot manner, achieving adelicate balance between accuracy and robustness, while minimizing thecomputational overhead. Moreover, PromptSmooth requires only a single model tohandle multiple noise levels, which substantially reduces the computationalcost compared to traditional methods that rely on training a separate model foreach noise level. Comprehensive experiments based on three Med-VLMs and acrosssix downstream datasets of various imaging modalities demonstrate the efficacyof PromptSmooth. Our code and models are available athttps://github.com/nhussein/promptsmooth.</description><author>Noor Hussein, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</author><pubDate>Thu, 29 Aug 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16769v1</guid></item><item><title>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</title><link>http://arxiv.org/abs/2408.16768v1</link><description>We introduce SAM2Point, a preliminary exploration adapting Segment AnythingModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Pointinterprets any 3D data as a series of multi-directional videos, and leveragesSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.Our framework supports various prompt types, including 3D points, boxes, andmasks, and can generalize across diverse scenarios, such as 3D objects, indoorscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlightthe robust generalization capabilities of SAM2Point. To our best knowledge, wepresent the most faithful implementation of SAM in 3D, which may serve as astarting point for future research in promptable 3D segmentation. Online Demo:https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:https://github.com/ZiyuGuo99/SAM2Point .</description><author>Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng</author><pubDate>Thu, 29 Aug 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16768v1</guid></item><item><title>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</title><link>http://arxiv.org/abs/2408.16767v1</link><description>Advancements in 3D scene reconstruction have transformed 2D images from thereal world into 3D models, producing realistic 3D results from hundreds ofinput photos. Despite great success in dense-view reconstruction scenarios,rendering a detailed scene from insufficient captured views is still anill-posed optimization problem, often resulting in artifacts and distortions inunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstructionparadigm that reframes the ambiguous reconstruction challenge as a temporalgeneration task. The key insight is to unleash the strong generative prior oflarge pre-trained video diffusion models for sparse-view reconstruction.However, 3D view consistency struggles to be accurately preserved in directlygenerated video frames from pre-trained models. To address this, given limitedinput views, the proposed ReconX first constructs a global point cloud andencodes it into a contextual space as the 3D structure condition. Guided by thecondition, the video diffusion model then synthesizes video frames that areboth detail-preserved and exhibit a high degree of 3D consistency, ensuring thecoherence of the scene from various perspectives. Finally, we recover the 3Dscene from the generated video through a confidence-aware 3D Gaussian Splattingoptimization scheme. Extensive experiments on various real-world datasets showthe superiority of our ReconX over state-of-the-art methods in terms of qualityand generalizability.</description><author>Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan</author><pubDate>Thu, 29 Aug 2024 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16767v1</guid></item><item><title>CSGO: Content-Style Composition in Text-to-Image Generation</title><link>http://arxiv.org/abs/2408.16766v1</link><description>The diffusion model has shown exceptional capabilities in controlled imagegeneration, which has further fueled interest in image style transfer. Existingworks mainly focus on training free-based methods (e.g., image inversion) dueto the scarcity of specific data. In this study, we present a data constructionpipeline for content-style-stylized image triplets that generates andautomatically cleanses stylized data triplets. Based on this pipeline, weconstruct a dataset IMAGStyle, the first large-scale style transfer datasetcontaining 210k image triplets, available for the community to explore andresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer modelbased on end-to-end training, which explicitly decouples content and stylefeatures employing independent feature injection. The unified CSGO implementsimage-driven style transfer, text-driven stylized synthesis, and textediting-driven stylized synthesis. Extensive experiments demonstrate theeffectiveness of our approach in enhancing style control capabilities in imagegeneration. Additional visualization and access to the source code can belocated on the project page: \url{https://csgo-gen.github.io/}.</description><author>Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, Zechao Li</author><pubDate>Thu, 29 Aug 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16766v1</guid></item><item><title>A Score-Based Density Formula, with Applications in Diffusion Generative Models</title><link>http://arxiv.org/abs/2408.16765v1</link><description>Score-based generative models (SGMs) have revolutionized the field ofgenerative modeling, achieving unprecedented success in generating realisticand diverse content. Despite empirical advances, the theoretical basis for whyoptimizing the evidence lower bound (ELBO) on the log-likelihood is effectivefor training diffusion generative models, such as DDPMs, remains largelyunexplored. In this paper, we address this question by establishing a densityformula for a continuous-time diffusion process, which can be viewed as thecontinuous-time limit of the forward process in an SGM. This formula revealsthe connection between the target density and the score function associatedwith each step of the forward process. Building on this, we demonstrate thatthe minimizer of the optimization objective for training DDPMs nearly coincideswith that of the true objective, providing a theoretical foundation foroptimizing DDPMs using the ELBO. Furthermore, we offer new insights into therole of score-matching regularization in training GANs, the use of ELBO indiffusion classifiers, and the recently proposed diffusion loss.</description><author>Gen Li, Yuling Yan</author><pubDate>Thu, 29 Aug 2024 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16765v1</guid></item><item><title>Batched Stochastic Bandit for Nondegenerate Functions</title><link>http://arxiv.org/abs/2405.05733v2</link><description>This paper studies batched bandit learning problems for nondegeneratefunctions. We introduce an algorithm that solves the batched bandit problem fornondegenerate functions near-optimally. More specifically, we introduce analgorithm, called Geometric Narrowing (GN), whose regret bound is of order$\widetilde{{\mathcal{O}}} ( A_{+}^d \sqrt{T} )$. In addition, GN only needs$\mathcal{O} (\log \log T)$ batches to achieve this regret. We also providelower bound analysis for this problem. More specifically, we prove that oversome (compact) doubling metric space of doubling dimension $d$: 1. For anypolicy $\pi$, there exists a problem instance on which $\pi$ admits a regret oforder ${\Omega} ( A_-^d \sqrt{T})$; 2. No policy can achieve a regret of order$ A_-^d \sqrt{T} $ over all problem instances, using less than $ \Omega ( \log\log T ) $ rounds of communications. Our lower bound analysis shows that the GNalgorithm achieves near optimal regret with minimal number of batches.</description><author>Yu Liu, Yunlu Shu, Tianyu Wang</author><pubDate>Thu, 29 Aug 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05733v2</guid></item><item><title>UV-free Texture Generation with Denoising and Geodesic Heat Diffusions</title><link>http://arxiv.org/abs/2408.16762v1</link><description>Seams, distortions, wasted UV space, vertex-duplication, and varyingresolution over the surface are the most prominent issues of the standardUV-based texturing of meshes. These issues are particularly acute whenautomatic UV-unwrapping techniques are used. For this reason, instead ofgenerating textures in automatically generated UV-planes like moststate-of-the-art methods, we propose to represent textures as colouredpoint-clouds whose colours are generated by a denoising diffusion probabilisticmodel constrained to operate on the surface of 3D objects. Our sampling andresolution agnostic generative model heavily relies on heat diffusion over thesurface of the meshes for spatial communication between points. To enableprocessing of arbitrarily sampled point-cloud textures and ensure long-distancetexture consistency we introduce a fast re-sampling of the mesh spectralproperties used during the heat diffusion and introduce a novelheat-diffusion-based self-attention mechanism. Our code and pre-trained modelsare available at github.com/simofoti/UV3-TeD.</description><author>Simone Foti, Stefanos Zafeiriou, Tolga Birdal</author><pubDate>Thu, 29 Aug 2024 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16762v1</guid></item><item><title>OmniRe: Omni Urban Scene Reconstruction</title><link>http://arxiv.org/abs/2408.16760v1</link><description>We introduce OmniRe, a holistic approach for efficiently reconstructinghigh-fidelity dynamic urban scenes from on-device logs. Recent methods formodeling driving sequences using neural radiance fields or Gaussian Splattinghave demonstrated the potential of reconstructing challenging dynamic scenes,but often overlook pedestrians and other non-vehicle dynamic actors, hinderinga complete pipeline for dynamic urban scene reconstruction. To that end, wepropose a comprehensive 3DGS framework for driving scenes, named OmniRe, thatallows for accurate, full-length reconstruction of diverse dynamic objects in adriving log. OmniRe builds dynamic neural scene graphs based on Gaussianrepresentations and constructs multiple local canonical spaces that modelvarious dynamic actors, including vehicles, pedestrians, and cyclists, amongmany others. This capability is unmatched by existing methods. OmniRe allows usto holistically reconstruct different objects present in the scene,subsequently enabling the simulation of reconstructed scenarios with all actorsparticipating in real-time (~60Hz). Extensive evaluations on the Waymo datasetshow that our approach outperforms prior state-of-the-art methodsquantitatively and qualitatively by a large margin. We believe our work fills acritical gap in driving reconstruction.</description><author>Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang</author><pubDate>Thu, 29 Aug 2024 17:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16760v1</guid></item><item><title>VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation</title><link>http://arxiv.org/abs/2407.10972v2</link><description>In the realm of vision models, the primary mode of representation is usingpixels to rasterize the visual world. Yet this is not always the best or uniqueway to represent visual content, especially for designers and artists whodepict the world using geometry primitives such as polygons. Vector graphics(VG), on the other hand, offer a textual representation of visual content,which can be more concise and powerful for content like cartoons, sketches andscientific figures. Recent studies have shown promising results on processingvector graphics with capable Large Language Models (LLMs). However, such worksfocus solely on qualitative results, understanding, or a specific type ofvector graphics. We propose VGBench, a comprehensive benchmark for LLMs onhandling vector graphics through diverse aspects, including (a) both visualunderstanding and generation, (b) evaluation of various vector graphicsformats, (c) diverse question types, (d) wide range of prompting techniques,(e) under multiple LLMs and (f) comparison with VLMs on rasterizedrepresentations. Evaluating on our collected 4279 understanding and 5845generation samples, we find that LLMs show strong capability on both aspectswhile exhibiting less desirable performance on low-level formats (SVG). Bothdata and evaluation pipeline will be open-sourced at https://vgbench.github.io.</description><author>Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee</author><pubDate>Thu, 29 Aug 2024 17:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10972v2</guid></item><item><title>Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks</title><link>http://arxiv.org/abs/2408.16757v1</link><description>Detecting test-time distribution shift has emerged as a key capability forsafely deployed machine learning models, with the question being tackled undervarious guises in recent years. In this paper, we aim to provide a consolidatedview of the two largest sub-fields within the community: out-of-distribution(OOD) detection and open-set recognition (OSR). In particular, we aim toprovide rigorous empirical analysis of different methods across settings andprovide actionable takeaways for practitioners and researchers. Concretely, wemake the following contributions: (i) We perform rigorous cross-evaluationbetween state-of-the-art methods in the OOD detection and OSR settings andidentify a strong correlation between the performances of methods for them;(ii) We propose a new, large-scale benchmark setting which we suggest betterdisentangles the problem tackled by OOD detection and OSR, re-evaluatingstate-of-the-art OOD detection and OSR methods in this setting; (iii) Wesurprisingly find that the best performing method on standard benchmarks(Outlier Exposure) struggles when tested at scale, while scoring rules whichare sensitive to the deep feature magnitude consistently show promise; and (iv)We conduct empirical analysis to explain these phenomena and highlightdirections for future research. Code:\url{https://github.com/Visual-AI/Dissect-OOD-OSR}</description><author>Hongjun Wang, Sagar Vaze, Kai Han</author><pubDate>Thu, 29 Aug 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16757v1</guid></item><item><title>How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2408.16756v1</link><description>The rapid evolution of large language models (LLMs) has transformed thecompetitive landscape in natural language processing (NLP), particularly forEnglish and other data-rich languages. However, underrepresented languages likeCantonese, spoken by over 85 million people, face significant development gaps,which is particularly concerning given the economic significance of theGuangdong-Hong Kong-Macau Greater Bay Area, and in substantialCantonese-speaking populations in places like Singapore and North America.Despite its wide use, Cantonese has scant representation in NLP research,especially compared to other languages from similarly developed regions. Tobridge these gaps, we outline current Cantonese NLP methods and introduce newbenchmarks designed to evaluate LLM performance in factual generation,mathematical logic, complex reasoning, and general knowledge in Cantonese,which aim to advance open-source Cantonese LLM technology. We also proposefuture research directions and recommended models to enhance Cantonese LLMdevelopment.</description><author>Jiyue Jiang, Liheng Chen, Pengan Chen, Sheng Wang, Qinghang Bao, Lingpeng Kong, Yu Li, Chuan Wu</author><pubDate>Thu, 29 Aug 2024 17:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16756v1</guid></item><item><title>Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2408.16753v1</link><description>Reinforcement learning is used to align language models with human preferencesignals after first pre-training the model to predict the next token of textwithin a large corpus using likelihood maximization. Before being deployed in aspecific domain, models are often further fine-tuned on task specific data.Since human preferences are often unavailable for the last step, it isperformed using likelihood maximization as that is the typical default method.However, reinforcement learning has other advantages besides facilitatingalignment to a human derived reward function. For one, whereas likelihoodmaximization is a form of imitation learning in which the model is trained onwhat to do under ideal conditions, reinforcement learning is not limited todemonstrating actions just for optimally reached states and trains a model whatto do under a range of scenarios as it explores the policy space. In addition,it also trains a model what not to do, suppressing competitive but pooractions. This work develops a framework for last-mile fine-tuning usingreinforcement learning and tests whether it garners performance gains. Theexperiments center on abstractive summarization, but the framework is generaland broadly applicable. Use of the procedure produced significantly betterresults than likelihood maximization when comparing raw predictions. For thespecific data tested, the gap could be bridged by employing post-processing ofthe maximum likelihood outputs. Nonetheless, the framework offers a new avenuefor model optimization in situations where post-processing may be lessstraightforward or effective, and it can be extended to include more complexclasses of undesirable outputs to penalize and train against, such ashallucinations.</description><author>Alec Solway</author><pubDate>Thu, 29 Aug 2024 17:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16753v1</guid></item><item><title>GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models</title><link>http://arxiv.org/abs/2408.11817v2</link><description>Large multimodal models (LMMs) have exhibited proficiencies across manyvisual tasks. Although numerous well-known benchmarks exist to evaluate modelperformance, they increasingly have insufficient headroom. As such, there is apressing need for a new generation of benchmarks challenging enough for thenext generation of LMMs. One area that LMMs show potential is graph analysis,specifically, the tasks an analyst might typically perform when interpretingfigures such as estimating the mean, intercepts or correlations of functionsand data series. In this work, we introduce GRAB, a graph analysis benchmark,fit for current and future frontier LMMs. Our benchmark is entirely synthetic,ensuring high-quality, noise-free questions. GRAB is comprised of 2170questions, covering four tasks and 23 graph properties. We evaluate 20 LMMs onGRAB, finding it to be a challenging benchmark, with the highest performingmodel attaining a score of just 21.7%. Finally, we conduct various ablations toinvestigate where the models succeed and struggle. We release GRAB to encourageprogress in this important, growing domain.</description><author>Jonathan Roberts, Kai Han, Samuel Albanie</author><pubDate>Thu, 29 Aug 2024 17:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11817v2</guid></item><item><title>Conditional score-based diffusion models for solving inverse problems in mechanics</title><link>http://arxiv.org/abs/2406.13154v3</link><description>We propose a framework to perform Bayesian inference using conditionalscore-based diffusion models to solve a class of inverse problems in mechanicsinvolving the inference of a specimen's spatially varying material propertiesfrom noisy measurements of its mechanical response to loading. Conditionalscore-based diffusion models are generative models that learn to approximatethe score function of a conditional distribution using samples from the jointdistribution. More specifically, the score functions corresponding to multiplerealizations of the measurement are approximated using a single neural network,the so-called score network, which is subsequently used to sample the posteriordistribution using an appropriate Markov chain Monte Carlo scheme based onLangevin dynamics. Training the score network only requires simulating theforward model. Hence, the proposed approach can accommodate black-box forwardmodels and complex measurement noise. Moreover, once the score network has beentrained, it can be re-used to solve the inverse problem for differentrealizations of the measurements. We demonstrate the efficacy of the proposedapproach on a suite of high-dimensional inverse problems in mechanics thatinvolve inferring heterogeneous material properties from noisy measurements.Some examples we consider involve synthetic data, while others include datacollected from actual elastography experiments. Further, our applicationsdemonstrate that the proposed approach can handle different measurementmodalities, complex patterns in the inferred quantities, non-Gaussian andnon-additive noise models, and nonlinear black-box forward models. The resultsshow that the proposed framework can solve large-scale physics-based inverseproblems efficiently.</description><author>Agnimitra Dasgupta, Harisankar Ramaswamy, Javier Murgoitio-Esandi, Ken Foo, Runze Li, Qifa Zhou, Brendan Kennedy, Assad Oberai</author><pubDate>Thu, 29 Aug 2024 17:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13154v3</guid></item><item><title>A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models</title><link>http://arxiv.org/abs/2408.16751v1</link><description>Beyond maximum likelihood estimation (MLE), the standard objective of alanguage model (LM) that optimizes good examples probabilities, many studieshave explored ways that also penalize bad examples for enhancing the quality ofoutput distribution, including unlikelihood training, exponential maximizingaverage treatment effect (ExMATE), and direct preference optimization (DPO). Tosystematically compare these methods and further provide a unified recipe forLM optimization, in this paper, we present a unique angle of gradient analysisof loss functions that simultaneously reward good examples and penalize badones in LMs. Through both mathematical results and experiments onCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functionalcharacteristics among these methods. We find that ExMATE serves as a superiorsurrogate for MLE, and that combining DPO with ExMATE instead of MLE furtherenhances both the statistical (5-7%) and generative (+18% win rate)performance.</description><author>Yi-Lin Tuan, William Yang Wang</author><pubDate>Thu, 29 Aug 2024 17:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16751v1</guid></item><item><title>Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge</title><link>http://arxiv.org/abs/2408.16749v1</link><description>The United States has experienced a significant increase in violentextremism, prompting the need for automated tools to detect and limit thespread of extremist ideology online. This study evaluates the performance ofBidirectional Encoder Representations from Transformers (BERT) and GenerativePre-Trained Transformers (GPT) in detecting and classifying online domesticextremist posts. We collected social media posts containing "far-right" and"far-left" ideological keywords and manually labeled them as extremist ornon-extremist. Extremist posts were further classified into one or more of fivecontributing elements of extremism based on a working definitional framework.The BERT model's performance was evaluated based on training data size andknowledge transfer between categories. We also compared the performance of GPT3.5 and GPT 4 models using different prompts: na\"ive, layperson-definition,role-playing, and professional-definition. Results showed that the bestperforming GPT models outperformed the best performing BERT models, with moredetailed prompts generally yielding better results. However, overly complexprompts may impair performance. Different versions of GPT have uniquesensitives to what they consider extremist. GPT 3.5 performed better atclassifying far-left extremist posts, while GPT 4 performed better atclassifying far-right extremist posts. Large language models, represented byGPT models, hold significant potential for online extremism classificationtasks, surpassing traditional BERT models in a zero-shot setting. Futureresearch should explore human-computer interactions in optimizing GPT modelsfor extremist detection and classification tasks to develop more efficient(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)methods for identifying extremist content.</description><author>Beidi Dong, Jin R. Lee, Ziwei Zhu, Balassubramanian Srinivasan</author><pubDate>Thu, 29 Aug 2024 17:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16749v1</guid></item><item><title>Theoretical and Methodological Framework for Studying Texts Produced by Large Language Models</title><link>http://arxiv.org/abs/2408.16740v1</link><description>This paper addresses the conceptual, methodological and technical challengesin studying large language models (LLMs) and the texts they produce from aquantitative linguistics perspective. It builds on a theoretical framework thatdistinguishes between the LLM as a substrate and the entities the modelsimulates. The paper advocates for a strictly non-anthropomorphic approach tomodels while cautiously applying methodologies used in studying humanlinguistic behavior to the simulated entities. While natural languageprocessing researchers focus on the models themselves, their architecture,evaluation, and methods for improving performance, we as quantitative linguistsshould strive to build a robust theory concerning the characteristics of textsproduced by LLMs, how they differ from human-produced texts, and the propertiesof simulated entities. Additionally, we should explore the potential of LLMs asan instrument for studying human culture, of which language is an integralpart.</description><author>Jiří Milička</author><pubDate>Thu, 29 Aug 2024 17:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16740v1</guid></item><item><title>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</title><link>http://arxiv.org/abs/2408.16737v1</link><description>Training on high-quality synthetic data from strong language models (LMs) isa common strategy to improve the reasoning performance of LMs. In this work, werevisit whether this strategy is compute-optimal under a fixed inference budget(e.g., FLOPs). To do so, we investigate the trade-offs between generatingsynthetic data using a stronger but more expensive (SE) model versus a weakerbut cheaper (WC) model. We evaluate the generated data across three keymetrics: coverage, diversity, and false positive rate, and show that the datafrom WC models may have higher coverage and diversity, but also exhibit higherfalse positive rates. We then finetune LMs on data from SE and WC models indifferent settings: knowledge distillation, self-improvement, and a novelweak-to-strong improvement setup where a weaker LM teaches reasoning to astronger LM. Our findings reveal that models finetuned on WC-generated dataconsistently outperform those trained on SE-generated data across multiplebenchmarks and multiple choices of WC and SE models. These results challengethe prevailing practice of relying on SE models for synthetic data generation,suggesting that WC may be the compute-optimal approach for training advanced LMreasoners.</description><author>Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi</author><pubDate>Thu, 29 Aug 2024 17:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16737v1</guid></item><item><title>FilFL: Client Filtering for Optimized Client Participation in Federated Learning</title><link>http://arxiv.org/abs/2302.06599v3</link><description>Federated learning, an emerging machine learning paradigm, enables clients tocollaboratively train a model without exchanging local data. Clientsparticipating in the training process significantly impact the convergencerate, learning efficiency, and model generalization. We propose a novelapproach, client filtering, to improve model generalization and optimize clientparticipation and training. The proposed method periodically filters availableclients to identify a subset that maximizes a combinatorial objective functionwith an efficient greedy filtering algorithm. Thus, the clients are assessed asa combination rather than individually. We theoretically analyze theconvergence of federated learning with client filtering in heterogeneoussettings and evaluate its performance across diverse vision and language tasks,including realistic scenarios with time-varying client availability. Ourempirical results demonstrate several benefits of our approach, includingimproved learning efficiency, faster convergence, and up to 10% higher testaccuracy than training without client filtering.</description><author>Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini, Marco Canini</author><pubDate>Thu, 29 Aug 2024 17:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06599v3</guid></item><item><title>Learning to Prompt Your Domain for Vision-Language Models</title><link>http://arxiv.org/abs/2310.03103v5</link><description>Prompt learning has recently become a very efficient transfer learningparadigm for Contrastive Language Image Pretraining (CLIP) models. Comparedwith fine-tuning the entire encoder, prompt learning can obtain highlycompetitive results by optimizing only a small number of parameters, whichpresents considerably exciting benefits for federated learning applicationsthat prioritizes communication efficiency. However, in this work, we identifythat directly transferring prompt learning approaches into federated learningdoes not yield favorable results since the model often suffers fromconsiderable domain gaps across different clients. To address this issue, wepropose ADAPT, a novel domain-aware prompt learning approach that facilitatesboth intra- and inter-domain prompts across federated participants. The basicidea of ADAPT is that the prompted CLIP should detect the input image's domaincorrespondence and before making the prediction of its category. Extensiveexperiments of ADAPT demonstrate its significant efficiency and effectivenessin federated learning. For example, by learning and sharing only 0.08Mparameters, our ADAPT attains a 68.4% average accuracy over six domains in theDomainNet dataset, which improves the original CLIP by a large margin of 14.8%.</description><author>Guoyizhe Wei, Feng Wang, Anshul Shah, Rama Chellappa</author><pubDate>Thu, 29 Aug 2024 17:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03103v5</guid></item><item><title>VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation</title><link>http://arxiv.org/abs/2408.16730v1</link><description>A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) isthat while increasing the number of vision tokens generally enhances visualunderstanding, it also significantly raises memory and computational costs,especially in long-term, dense video frame streaming scenarios. Althoughlearnable approaches like Q-Former and Perceiver Resampler have been developedto reduce the vision token burden, they overlook the context causally modeledby LLMs (i.e., key-value cache), potentially leading to missed visual cues whenaddressing user queries. In this paper, we introduce a novel approach to reducevision compute by leveraging redundant vision tokens "skipping layers" ratherthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, isinspired by mixture-of-depths LLMs and addresses the challenge of numerousvision tokens in long-term or streaming video. Specifically, for eachtransformer layer, we learn to skip the computation for a high proportion(e.g., 80\%) of vision tokens, passing them directly to the next layer. Thisapproach significantly enhances model efficiency, achieving approximately\textasciitilde42\% time and \textasciitilde30\% memory savings for the entiretraining. Moreover, our method reduces the computation in the context and avoiddecreasing the vision tokens, thus preserving or even improving performancecompared to the vanilla model. We conduct extensive experiments to demonstratethe effectiveness of VideoLLM-MoD, showing its state-of-the-art results onmultiple benchmarks, including narration, forecasting, and summarization tasksin COIN, Ego4D, and Ego-Exo4D datasets.</description><author>Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou</author><pubDate>Thu, 29 Aug 2024 17:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16730v1</guid></item><item><title>Evaluation Framework for Feedback Generation Methods in Skeletal Movement Assessment</title><link>http://arxiv.org/abs/2404.09359v4</link><description>The application of machine-learning solutions to movement assessment fromskeleton videos has attracted significant research attention in recent years.This advancement has made rehabilitation at home more accessible, utilizingmovement assessment algorithms that can operate on affordable equipment forhuman pose detection and analysis from 2D or 3D videos. While the primaryobjective of automatic assessment tasks is to score movements, the automaticgeneration of feedback highlighting key movement issues has the potential tosignificantly enhance and accelerate the rehabilitation process. While numerousresearch works exist in the field of automatic movement assessment, only ahandful address feedback generation. In this study, we propose terminology andcriteria for the classification, evaluation, and comparison of feedbackgeneration solutions. We discuss the challenges associated with each feedbackgeneration approach and use our proposed criteria to classify existingsolutions. To our knowledge, this is the first work that formulates feedbackgeneration in skeletal movement assessment.</description><author>Tal Hakim</author><pubDate>Thu, 29 Aug 2024 17:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09359v4</guid></item><item><title>Prediction-Feedback DETR for Temporal Action Detection</title><link>http://arxiv.org/abs/2408.16729v1</link><description>Temporal Action Detection (TAD) is fundamental yet challenging for real-worldvideo applications. Leveraging the unique benefits of transformers, variousDETR-based approaches have been adopted in TAD. However, it has recently beenidentified that the attention collapse in self-attention causes the performancedegradation of DETR for TAD. Building upon previous research, this paper newlyaddresses the attention collapse problem in cross-attention within DETR-basedTAD methods. Moreover, our findings reveal that cross-attention exhibitspatterns distinct from predictions, indicating a short-cut phenomenon. Toresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),which utilizes predictions to restore the collapse and align the cross- andself-attention with predictions. Specifically, we devise novelprediction-feedback objectives using guidance from the relations of thepredictions. As a result, Pred-DETR significantly alleviates the collapse andachieves state-of-the-art performance among DETR-based methods on variouschallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, andFineAction.</description><author>Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo</author><pubDate>Thu, 29 Aug 2024 17:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16729v1</guid></item><item><title>Adaptive Log-Euclidean Metrics for SPD Matrix Learning</title><link>http://arxiv.org/abs/2303.15477v5</link><description>Symmetric Positive Definite (SPD) matrices have received wide attention inmachine learning due to their intrinsic capacity to encode underlyingstructural correlation in data. Many successful Riemannian metrics have beenproposed to reflect the non-Euclidean geometry of SPD manifolds. However, mostexisting metric tensors are fixed, which might lead to sub-optimal performancefor SPD matrix learning, especially for deep SPD neural networks. To remedythis limitation, we leverage the commonly encountered pullback techniques andpropose Adaptive Log-Euclidean Metrics (ALEMs), which extend the widely usedLog-Euclidean Metric (LEM). Compared with the previous Riemannian metrics, ourmetrics contain learnable parameters, which can better adapt to the complexdynamics of Riemannian neural networks with minor extra computations. We alsopresent a complete theoretical analysis to support our ALEMs, includingalgebraic and Riemannian properties. The experimental and theoretical resultsdemonstrate the merit of the proposed metrics in improving the performance ofSPD neural networks. The efficacy of our metrics is further showcased on a setof recently developed Riemannian building blocks, including Riemannian batchnormalization, Riemannian Residual blocks, and Riemannian classifiers.</description><author>Ziheng Chen, Yue Song, Tianyang Xu, Zhiwu Huang, Xiao-Jun Wu, Nicu Sebe</author><pubDate>Thu, 29 Aug 2024 17:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15477v5</guid></item><item><title>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</title><link>http://arxiv.org/abs/2408.16725v1</link><description>Recent advances in language models have achieved significant progress.GPT-4o, as a new milestone, has enabled real-time conversations with humans,demonstrating near-human natural fluency. Such human-computer interactionnecessitates models with the capability to perform reasoning directly with theaudio modality and generate output in streaming. However, this remains beyondthe reach of current academic models, as they typically depend on extra TTSsystems for speech synthesis, resulting in undesirable latency. This paperintroduces the Mini-Omni, an audio-based end-to-end conversational model,capable of real-time speech interaction. To achieve this capability, we proposea text-instructed speech generation method, along with batch-parallelstrategies during inference to further boost the performance. Our method alsohelps to retain the original model's language capabilities with minimaldegradation, enabling other works to establish real-time interactioncapabilities. We call this training method "Any Model Can Talk". We alsointroduce the VoiceAssistant-400K dataset to fine-tune models optimized forspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,open-source model for real-time speech interaction, offering valuable potentialfor future research.</description><author>Zhifei Xie, Changqiao Wu</author><pubDate>Thu, 29 Aug 2024 17:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16725v1</guid></item><item><title>OpticalRS-4M: Scaling Efficient Masked Autoencoder Learning on Large Remote Sensing Dataset</title><link>http://arxiv.org/abs/2406.11933v2</link><description>Masked Image Modeling (MIM) has become an essential method for buildingfoundational visual models in remote sensing (RS). However, the limitations insize and diversity of existing RS datasets restrict the ability of MIM methodsto learn generalizable representations. Additionally, conventional MIMtechniques, which require reconstructing all tokens, introduce unnecessarycomputational overhead. To address these issues, we present a new pre-trainingpipeline for RS models, featuring the creation of a large-scale RS dataset andan efficient MIM approach. We curated a high-quality dataset named OpticalRS-4Mby collecting publicly available RS datasets and processing them throughexclusion, slicing, and deduplication. OpticalRS-4M comprises 4 million opticalimages covering various RS tasks, such as object detection and pixelsegmentation. To enhance efficiency, we propose SelectiveMAE, a pre-trainingmethod that dynamically encodes and reconstructs semantically rich patchtokens, thereby reducing the inefficiencies of traditional MIM models caused byredundant background pixels in RS images. Extensive experiments demonstratethat OpticalRS-4M significantly improves classification, detection, andsegmentation performance, while SelectiveMAE increases training efficiency over2 times. This highlights the effectiveness and scalability of our pipeline indeveloping RS foundational models.</description><author>Fengxiang Wang, Hongzhen Wang, Di Wang, Zonghao Guo, Zhenyu Zhong, Long Lan, Jing Zhang, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 29 Aug 2024 17:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11933v2</guid></item><item><title>Normalized mutual information is a biased measure for classification and community detection</title><link>http://arxiv.org/abs/2307.01282v2</link><description>Normalized mutual information is widely used as a similarity measure forevaluating the performance of clustering and classification algorithms. In thispaper, we argue that results returned by the normalized mutual information arebiased for two reasons: first, because they ignore the information content ofthe contingency table and, second, because their symmetric normalizationintroduces spurious dependence on algorithm output. We introduce a modifiedversion of the mutual information that remedies both of these shortcomings. Asa practical demonstration of the importance of using an unbiased measure, weperform extensive numerical tests on a basket of popular algorithms for networkcommunity detection and show that one's conclusions about which algorithm isbest are significantly affected by the biases in the traditional mutualinformation.</description><author>Maximilian Jerdee, Alec Kirkley, M. E. J. Newman</author><pubDate>Thu, 29 Aug 2024 17:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01282v2</guid></item><item><title>H-SGANet: Hybrid Sparse Graph Attention Network for Deformable Medical Image Registration</title><link>http://arxiv.org/abs/2408.16719v1</link><description>The integration of Convolutional Neural Network (ConvNet) and Transformer hasemerged as a strong candidate for image registration, leveraging the strengthsof both models and a large parameter space. However, this hybrid model,treating brain MRI volumes as grid or sequence structures, faces challenges inaccurately representing anatomical connectivity, diverse brain regions, andvital connections contributing to the brain's internal architecture. Concernsalso arise regarding the computational expense and GPU memory usage associatedwith this model. To tackle these issues, a lightweight hybrid sparse graphattention network (H-SGANet) has been developed. This network incorporates acentral mechanism, Sparse Graph Attention (SGA), based on a Vision Graph NeuralNetwork (ViG) with predetermined anatomical connections. The SGA module expandsthe model's receptive field and seamlessly integrates into the network. Tofurther amplify the advantages of the hybrid network, the SeparableSelf-Attention (SSA) is employed as an enhanced token mixer, integrated withdepth-wise convolution to constitute SSAFormer. This strategic integration isdesigned to more effectively extract long-range dependencies. As a hybridConvNet-ViG-Transformer model, H-SGANet offers threefold benefits forvolumetric medical image registration. It optimizes fixed and moving imagesconcurrently through a hybrid feature fusion layer and an end-to-end learningframework. Compared to VoxelMorph, a model with a similar parameter count,H-SGANet demonstrates significant performance enhancements of 3.5% and 1.5% inDice score on the OASIS dataset and LPBA40 dataset, respectively.</description><author>Yufeng Zhou, Wenming Cao</author><pubDate>Thu, 29 Aug 2024 17:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16719v1</guid></item><item><title>A GREAT Architecture for Edge-Based Graph Problems Like TSP</title><link>http://arxiv.org/abs/2408.16717v1</link><description>In the last years, many neural network-based approaches have been proposed totackle combinatorial optimization problems such as routing problems. Many ofthese approaches are based on graph neural networks (GNNs) or relatedtransformers, operating on the Euclidean coordinates representing the routingproblems. However, GNNs are inherently not well suited to operate on densegraphs, such as in routing problems. Furthermore, models operating on Euclideancoordinates cannot be applied to non-Euclidean versions of routing problemsthat are often found in real-world settings. To overcome these limitations, wepropose a novel GNN-related edge-based neural model called Graph Edge AttentionNetwork (GREAT). We evaluate the performance of GREAT in theedge-classification task to predict optimal edges in the Traveling SalesmanProblem (TSP). We can use such a trained GREAT model to produce sparse TSPgraph instances, keeping only the edges GREAT finds promising. Compared toother, non-learning-based methods to sparsify TSP graphs, GREAT can producevery sparse graphs while keeping most of the optimal edges. Furthermore, webuild a reinforcement learning-based GREAT framework which we apply toEuclidean and non-Euclidean asymmetric TSP. This framework achievesstate-of-the-art results.</description><author>Attila Lischka, Jiaming Wu, Morteza Haghir Chehreghani, Balázs Kulcsár</author><pubDate>Thu, 29 Aug 2024 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16717v1</guid></item><item><title>Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning</title><link>http://arxiv.org/abs/2405.09536v2</link><description>Gradient boosting is a sequential ensemble method that fits a new weakerlearner to pseudo residuals at each iteration. We propose Wasserstein gradientboosting, a novel extension of gradient boosting that fits a new weak learnerto alternative pseudo residuals that are Wasserstein gradients of lossfunctionals of probability distributions assigned at each input. It solvesdistribution-valued supervised learning, where the output values of thetraining dataset are probability distributions for each input. Inclassification and regression, a model typically returns, for each input, apoint estimate of a parameter of a noise distribution specified for a responsevariable, such as the class probability parameter of a categorical distributionspecified for a response label. A main application of Wasserstein gradientboosting in this paper is tree-based evidential learning, which returns adistributional estimate of the response parameter for each input. Weempirically demonstrate the superior performance of the probabilisticprediction by Wasserstein gradient boosting in comparison with existinguncertainty quantification methods.</description><author>Takuo Matsubara</author><pubDate>Thu, 29 Aug 2024 17:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09536v2</guid></item><item><title>Enhanced forecasting of stock prices based on variational mode decomposition, PatchTST, and adaptive scale-weighted layer</title><link>http://arxiv.org/abs/2408.16707v1</link><description>The significant fluctuations in stock index prices in recent years highlightthe critical need for accurate forecasting to guide investment and financialstrategies. This study introduces a novel composite forecasting framework thatintegrates variational mode decomposition (VMD), PatchTST, and adaptivescale-weighted layer (ASWL) to address these challenges. Utilizing datasets offour major stock indices--SP500, DJI, SSEC, and FTSE--from 2000 to 2024, theproposed method first decomposes the raw price series into intrinsic modefunctions (IMFs) using VMD. Each IMF is then modeled with PatchTST to capturetemporal patterns effectively. The ASWL module is applied to incorporate scaleinformation, enhancing prediction accuracy. The final forecast is derived byaggregating predictions from all IMFs. The VMD-PatchTST-ASWL frameworkdemonstrates significant improvements in forecasting accuracy compared totraditional models, showing robust performance across different indices. Thisinnovative approach provides a powerful tool for stock index price forecasting,with potential applications in various financial analysis and investmentdecision-making contexts.</description><author>Xiaorui Xue, Shaofang Li, Xiaonan Wang</author><pubDate>Thu, 29 Aug 2024 17:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16707v1</guid></item><item><title>Awes, Laws, and Flaws From Today's LLM Research</title><link>http://arxiv.org/abs/2408.15409v2</link><description>We perform a critical examination of the scientific methodology behindcontemporary large language model (LLM) research. For this we assess over 2,000research works based on criteria typical of what is considered good research(e.g. presence of statistical tests and reproducibility) and cross-validate itwith arguments that are at the centre of controversy (e.g., claims of emergentbehaviour, the use of LLMs as evaluators). We find multiple trends, such asdeclines in claims of emergent behaviour and ethics disclaimers; the rise ofLLMs as evaluators in spite of a lack of consensus from the community abouttheir useability; and an increase of claims of LLM reasoning abilities,typically without leveraging human evaluation. This paper underscores the needfor more scrutiny and rigour by and from this field to live up to thefundamentals of a responsible scientific method that is ethical, reproducible,systematic, and open to criticism.</description><author>Adrian de Wynter</author><pubDate>Thu, 29 Aug 2024 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15409v2</guid></item><item><title>One-Shot Learning Meets Depth Diffusion in Multi-Object Videos</title><link>http://arxiv.org/abs/2408.16704v1</link><description>Creating editable videos that depict complex interactions between multipleobjects in various artistic styles has long been a challenging task infilmmaking. Progress is often hampered by the scarcity of data sets thatcontain paired text descriptions and corresponding videos that showcase theseinteractions. This paper introduces a novel depth-conditioning approach thatsignificantly advances this field by enabling the generation of coherent anddiverse videos from just a single text-video pair using a pre-traineddepth-aware Text-to-Image (T2I) model. Our method fine-tunes the pre-trainedmodel to capture continuous motion by employing custom-designed spatial andtemporal attention mechanisms. During inference, we use the DDIM inversion toprovide structural guidance for video generation. This innovative techniqueallows for continuously controllable depth in videos, facilitating thegeneration of multiobject interactions while maintaining the concept generationand compositional strengths of the original T2I model across various artisticstyles, such as photorealism, animation, and impressionism.</description><author>Anisha Jain</author><pubDate>Thu, 29 Aug 2024 16:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16704v1</guid></item><item><title>CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common Crawl</title><link>http://arxiv.org/abs/2405.11039v3</link><description>The Common Crawl (CC) corpus is the largest open web crawl dataset containing9.5+ petabytes of data captured since 2008. The dataset is instrumental intraining large language models, and as such it has been studied for(un)desirable content, and distilled for smaller, domain-specific datasets.However, to our knowledge, no research has been dedicated to using CC as asource of annotated geospatial data. In this paper, we introduce an efficientpipeline to extract annotated user-generated tracks from GPX files found in CC,and the resulting multimodal dataset with 1,416 pairings of human-writtendescriptions and MultiLineString vector data from the 6 most recent CCreleases. The dataset can be used to study people's outdoor activity patterns,the way people talk about their outdoor experiences, as well as for developingtrajectory generation or track annotation models, or for various other problemsin place of synthetically generated routes. Our reproducible code is availableon GitHub: https://github.com/ilyankou/cc-gpx</description><author>Ilya Ilyankou, Meihui Wang, Stefano Cavazzi, James Haworth</author><pubDate>Thu, 29 Aug 2024 16:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11039v3</guid></item><item><title>GradBias: Unveiling Word Influence on Bias in Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2408.16700v1</link><description>Recent progress in Text-to-Image (T2I) generative models has enabledhigh-quality image generation. As performance and accessibility increase, thesemodels are gaining significant attraction and popularity: ensuring theirfairness and safety is a priority to prevent the dissemination and perpetuationof biases. However, existing studies in bias detection focus on closed sets ofpredefined biases (e.g., gender, ethnicity). In this paper, we propose ageneral framework to identify, quantify, and explain biases in an open setsetting, i.e. without requiring a predefined set. This pipeline leverages aLarge Language Model (LLM) to propose biases starting from a set of captions.Next, these captions are used by the target generative model for generating aset of images. Finally, Vision Question Answering (VQA) is leveraged for biasevaluation. We show two variations of this framework: OpenBias and GradBias.OpenBias detects and quantifies biases, while GradBias determines thecontribution of individual prompt words on biases. OpenBias effectively detectsboth well-known and novel biases related to people, objects, and animals andhighly aligns with existing closed-set bias detection methods and humanjudgment. GradBias shows that neutral words can significantly influence biasesand it outperforms several baselines, including state-of-the-art foundationmodels. Code available here: https://github.com/Moreno98/GradBias.</description><author>Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Xingqian Xu, Humphrey Shi, Nicu Sebe</author><pubDate>Thu, 29 Aug 2024 16:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16700v1</guid></item><item><title>Quantifying Geospatial in the Common Crawl Corpus</title><link>http://arxiv.org/abs/2406.04952v2</link><description>Large language models (LLMs) exhibit emerging geospatial capabilities,stemming from their pre-training on vast unlabelled text datasets that areoften derived from the Common Crawl (CC) corpus. However, the geospatialcontent within CC remains largely unexplored, impacting our understanding ofLLMs' spatial reasoning. This paper investigates the prevalence of geospatialdata in recent Common Crawl releases using Gemini 1.5, a powerful languagemodel. By analyzing a sample of documents and manually revising the results, weestimate that 18.7% of web documents in CC contain geospatial information suchas coordinates and addresses. We find little difference in prevalence betweenEnlgish- and non-English-language documents. Our findings provide quantitativeinsights into the nature and extent of geospatial data in CC, and lay thegroundwork for future studies of geospatial biases of LLMs.</description><author>Ilya Ilyankou, Meihui Wang, Stefano Cavazzi, James Haworth</author><pubDate>Thu, 29 Aug 2024 16:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04952v2</guid></item><item><title>GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</title><link>http://arxiv.org/abs/2403.05527v3</link><description>Key-value (KV) caching has become the de-facto to accelerate generation speedfor large language models (LLMs) inference. However, the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem, significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods, however, often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step, resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge, wepropose GEAR, an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error, and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques, GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives, GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement, while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</description><author>Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao</author><pubDate>Thu, 29 Aug 2024 16:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05527v3</guid></item><item><title>SympGNNs: Symplectic Graph Neural Networks for identifiying high-dimensional Hamiltonian systems and node classification</title><link>http://arxiv.org/abs/2408.16698v1</link><description>Existing neural network models to learn Hamiltonian systems, such asSympNets, although accurate in low-dimensions, struggle to learn the correctdynamics for high-dimensional many-body systems. Herein, we introduceSymplectic Graph Neural Networks (SympGNNs) that can effectively handle systemidentification in high-dimensional Hamiltonian systems, as well as nodeclassification. SympGNNs combines symplectic maps with permutationequivariance, a property of graph neural networks. Specifically, we propose twovariants of SympGNNs: i) G-SympGNN and ii) LA-SympGNN, arising from differentparameterizations of the kinetic and potential energy. We demonstrate thecapabilities of SympGNN on two physical examples: a 40-particle coupledHarmonic oscillator, and a 2000-particle molecular dynamics simulation in atwo-dimensional Lennard-Jones potential. Furthermore, we demonstrate theperformance of SympGNN in the node classification task, achieving accuracycomparable to the state-of-the-art. We also empirically show that SympGNN canovercome the oversmoothing and heterophily problems, two key challenges in thefield of graph neural networks.</description><author>Alan John Varghese, Zhen Zhang, George Em Karniadakis</author><pubDate>Thu, 29 Aug 2024 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16698v1</guid></item><item><title>Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix Multiplication</title><link>http://arxiv.org/abs/2406.10166v2</link><description>Sparse matrix-matrix multiplication (SpGEMM) is a critical operation innumerous fields, including scientific computing, graph analytics, and deeplearning. These applications exploit the sparsity of matrices to reduce storageand computational demands. However, the irregular structure of sparse matricesposes significant challenges for performance optimization. Traditional hardwareaccelerators are tailored for specific sparsity patterns with fixed dataflowschemes - inner, outer, and row-wise but often perform suboptimally when theactual sparsity deviates from these predetermined patterns. As the use ofSpGEMM expands across various domains, each with distinct sparsitycharacteristics, the demand for hardware accelerators that can efficientlyhandle a range of sparsity patterns is increasing. This paper presents amachine learning based approach for adaptively selecting the most appropriatedataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employingdecision trees and deep reinforcement learning, we explore the potential ofthese techniques to surpass heuristic-based methods in identifying optimaldataflow schemes. We evaluate our models by comparing their performance withthat of a heuristic, highlighting the strengths and weaknesses of eachapproach. Our findings suggest that using machine learning for dynamic dataflowselection in hardware accelerators can provide upto 28 times gains.</description><author>Sanjali Yadav, Bahar Asgari</author><pubDate>Thu, 29 Aug 2024 16:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10166v2</guid></item><item><title>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</title><link>http://arxiv.org/abs/2310.12000v2</link><description>Latent Gaussian process (GP) models are flexible probabilistic non-parametricfunction models. Vecchia approximations are accurate approximations for GPs toovercome computational bottlenecks for large data, and the Laplaceapproximation is a fast method with asymptotic convergence guarantees toapproximate marginal likelihoods and posterior predictive distributions fornon-Gaussian likelihoods. Unfortunately, the computational complexity ofcombined Vecchia-Laplace approximations grows faster than linearly in thesample size when used in combination with direct solver methods such as theCholesky decomposition. Computations with Vecchia-Laplace approximations canthus become prohibitively slow precisely when the approximations are usuallythe most accurate, i.e., on large data sets. In this article, we presentiterative methods to overcome this drawback. Among other things, we introduceand analyze several preconditioners, derive new convergence results, andpropose novel methods for accurately approximating predictive variances. Weanalyze our proposed methods theoretically and in experiments with simulatedand real-world data. In particular, we obtain a speed-up of an order ofmagnitude compared to Cholesky-based calculations and a threefold increase inprediction accuracy in terms of the continuous ranked probability scorecompared to a state-of-the-art method on a large satellite data set. Allmethods are implemented in a free C++ software library with high-level Pythonand R packages.</description><author>Pascal Kündig, Fabio Sigrist</author><pubDate>Thu, 29 Aug 2024 16:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12000v2</guid></item><item><title>Methods for Recovering Conditional Independence Graphs: A Survey</title><link>http://arxiv.org/abs/2211.06829v3</link><description>Conditional Independence (CI) graphs are a type of probabilistic graphicalmodels that are primarily used to gain insights about feature relationships.Each edge represents the partial correlation between the connected featureswhich gives information about their direct dependence. In this survey, we listout different methods and study the advances in techniques developed to recoverCI graphs. We cover traditional optimization methods as well as recentlydeveloped deep learning architectures along with their recommendedimplementations. To facilitate wider adoption, we include preliminaries thatconsolidate associated operations, for example techniques to obtain covariancematrix for mixed datatypes.</description><author>Harsh Shrivastava, Urszula Chajewska</author><pubDate>Thu, 29 Aug 2024 16:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06829v3</guid></item><item><title>Generic Objects as Pose Probes for Few-Shot View Synthesis</title><link>http://arxiv.org/abs/2408.16690v1</link><description>Radiance fields including NeRFs and 3D Gaussians demonstrate great potentialin high-fidelity rendering and scene reconstruction, while they require asubstantial number of posed images as inputs. COLMAP is frequently employed forpreprocessing to estimate poses, while it necessitates a large number offeature matches to operate effectively, and it struggles with scenescharacterized by sparse features, large baselines between images, or a limitednumber of input images. We aim to tackle few-view NeRF reconstruction usingonly 3 to 6 unposed scene images. Traditional methods often use calibrationboards but they are not common in images. We propose a novel idea of utilizingeveryday objects, commonly found in both images and real life, as "poseprobes". The probe object is automatically segmented by SAM, whose shape isinitialized from a cube. We apply a dual-branch volume rendering optimization(object NeRF and scene NeRF) to constrain the pose optimization and jointlyrefine the geometry. Specifically, object poses of two views are firstestimated by PnP matching in an SDF representation, which serves as initialposes. PnP matching, requiring only a few features, is suitable forfeature-sparse scenes. Additional views are incrementally incorporated torefine poses from preceding views. In experiments, PoseProbe achievesstate-of-the-art performance in both pose estimation and novel view synthesisacross multiple datasets. We demonstrate its effectiveness, particularly infew-view and large-baseline scenes where COLMAP struggles. In ablations, usingdifferent objects in a scene yields comparable performance.</description><author>Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</author><pubDate>Thu, 29 Aug 2024 16:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16690v1</guid></item><item><title>CW-CNN &amp; CW-AN: Convolutional Networks and Attention Networks for CW-Complexes</title><link>http://arxiv.org/abs/2408.16686v1</link><description>We present a novel framework for learning on CW-complex structured datapoints. Recent advances have discussed CW-complexes as ideal learningrepresentations for problems in cheminformatics. However, there is a lack ofavailable machine learning methods suitable for learning on CW-complexes. Inthis paper we develop notions of convolution and attention that are welldefined for CW-complexes. These notions enable us to create the first neuralnetwork that can receive a CW-complex as input. We illustrate and interpretthis framework in the context of supervised prediction.</description><author>Rahul Khorana</author><pubDate>Thu, 29 Aug 2024 16:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16686v1</guid></item><item><title>PartFormer: Awakening Latent Diverse Representation from Vision Transformer for Object Re-Identification</title><link>http://arxiv.org/abs/2408.16684v1</link><description>Extracting robust feature representation is critical for objectre-identification to accurately identify objects across non-overlappingcameras. Although having a strong representation ability, the VisionTransformer (ViT) tends to overfit on most distinct regions of training data,limiting its generalizability and attention to holistic object features.Meanwhile, due to the structural difference between CNN and ViT, fine-grainedstrategies that effectively address this issue in CNN do not continue to besuccessful in ViT. To address this issue, by observing the latent diverserepresentation hidden behind the multi-head attention, we present PartFormer,an innovative adaptation of ViT designed to overcome the granularitylimitations in object Re-ID tasks. The PartFormer integrates a HeadDisentangling Block (HDB) that awakens the diverse representation of multi-headself-attention without the typical loss of feature richness induced byconcatenation and FFN layers post-attention. To avoid the homogenization ofattention heads and promote robust part-based feature learning, two headdiversity constraints are imposed: attention diversity constraint andcorrelation diversity constraint. These constraints enable the model to exploitdiverse and discriminative feature representations from different attentionheads. Comprehensive experiments on various object Re-ID benchmarks demonstratethe superiority of the PartFormer. Specifically, our framework significantlyoutperforms state-of-the-art by 2.4\% mAP scores on the most challenging MSMT17dataset.</description><author>Lei Tan, Pingyang Dai, Jie Chen, Liujuan Cao, Yongjian Wu, Rongrong Ji</author><pubDate>Thu, 29 Aug 2024 16:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16684v1</guid></item><item><title>A Catalog of Fairness-Aware Practices in Machine Learning Engineering</title><link>http://arxiv.org/abs/2408.16683v1</link><description>Machine learning's widespread adoption in decision-making processes raisesconcerns about fairness, particularly regarding the treatment of sensitivefeatures and potential discrimination against minorities. The softwareengineering community has responded by developing fairness-oriented metrics,empirical studies, and approaches. However, there remains a gap inunderstanding and categorizing practices for engineering fairness throughoutthe machine learning lifecycle. This paper presents a novel catalog ofpractices for addressing fairness in machine learning derived from a systematicmapping study. The study identifies and categorizes 28 practices from existingliterature, mapping them onto different stages of the machine learninglifecycle. From this catalog, the authors extract actionable items andimplications for both researchers and practitioners in software engineering.This work aims to provide a comprehensive resource for integrating fairnessconsiderations into the development and deployment of machine learning systems,enhancing their reliability, accountability, and credibility.</description><author>Gianmario Voria, Giulia Sellitto, Carmine Ferrara, Francesco Abate, Andrea De Lucia, Filomena Ferrucci, Gemma Catolino, Fabio Palomba</author><pubDate>Thu, 29 Aug 2024 16:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16683v1</guid></item><item><title>Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity</title><link>http://arxiv.org/abs/2408.16673v1</link><description>Large language models rely on Supervised Fine-Tuning (SFT) to specialize indownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but itoften leads to overfitting and limited output diversity due to its aggressiveupdates to the data distribution. This paper aim to address these issues byintroducing the maximum entropy principle, which favors models with flatterdistributions that still effectively capture the data. Specifically, we developa new distribution matching method called GEM, which solves reverseKullback-Leibler divergence minimization with an entropy regularizer. For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.First, when applied to the UltraFeedback dataset to develop generalinstruction-following abilities, GEM exhibits reduced overfitting, evidenced bylower perplexity and better performance on the IFEval benchmark. Furthermore,GEM enhances output diversity, leading to performance gains of up to 7 pointson math reasoning and code generation tasks using best-of-n sampling, evenwithout domain-specific data. Second, when fine-tuning with domain-specificdatasets for math reasoning and code generation, GEM also shows lessoverfitting and improvements of up to 10 points compared with CE.</description><author>Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo</author><pubDate>Thu, 29 Aug 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16673v1</guid></item><item><title>Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever</title><link>http://arxiv.org/abs/2408.16672v1</link><description>Multi-vector dense models, such as ColBERT, have proven highly effective ininformation retrieval. ColBERT's late interaction scoring approximates thejoint query-document attention seen in cross-encoders while maintaininginference efficiency closer to traditional dense retrieval models, thanks toits bi-encoder architecture and recent optimizations in indexing and search. Inthis paper, we introduce several improvements to the ColBERT model architectureand training pipeline, leveraging techniques successful in the more establishedsingle-vector embedding model paradigm, particularly those suited forheterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstratesstrong performance across a range of English and multilingual retrieval tasks,while also cutting storage requirements by up to 50% compared to previousmodels.</description><author>Rohan Jha, Bo Wang, Michael Günther, Saba Sturua, Mohammad Kalim Akram, Han Xiao</author><pubDate>Thu, 29 Aug 2024 16:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16672v1</guid></item><item><title>Iterative Graph Alignment</title><link>http://arxiv.org/abs/2408.16667v1</link><description>By compressing diverse narratives, LLMs go beyond memorization, achievingintelligence by capturing generalizable causal relationships. However, theysuffer from local 'representation gaps' due to insufficient training datadiversity, limiting their real-world utility, especially in tasks requiringstrict alignment to rules. Traditional alignment methods relying on heavy humanannotations are inefficient and unscalable. Recent self-alignment techniquesalso fall short, as they often depend on self-selection based prompting andmemorization-based learning. To address these issues, we introduce IterativeGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. Ateacher model (VLM) employs Iterative Graph Prompting (IGP) to create logicalgraphs and reference answers. The student model (LLM) identifies localknowledge gaps by attempting to align its responses with these references,collaborating with helper models to generate diverse answers. These alignedresponses are then used for iterative supervised fine-tuning (SFT). Ourevaluations across five rule-based scenarios demonstrate IGP's effectiveness,with a 73.12\% alignment improvement in Claude Sonnet 3.5, andLlama3-8B-Instruct achieving an 86.20\% improvement, outperforming ClaudeSonnet 3.5 in rule-based alignment.</description><author>Fangyuan Yu, Hardeep Singh Arora, Matt Johnson</author><pubDate>Thu, 29 Aug 2024 16:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16667v1</guid></item><item><title>Manipulate-Anything: Automating Real-World Robots using Vision-Language Models</title><link>http://arxiv.org/abs/2406.18915v3</link><description>Large-scale endeavors like and widespread community efforts such asOpen-X-Embodiment have contributed to growing the scale of robot demonstrationdata. However, there is still an opportunity to improve the quality, quantity,and diversity of robot demonstration data. Although vision-language models havebeen shown to automatically generate demonstration data, their utility has beenlimited to environments with privileged state information, they requirehand-designed skills, and are limited to interactions with few objectinstances. We propose Manipulate-Anything, a scalable automated generationmethod for real-world robotic manipulation. Unlike prior work, our method canoperate in real-world environments without any privileged state information,hand-designed skills, and can manipulate any static object. We evaluate ourmethod using two setups. First, Manipulate-Anything successfully generatestrajectories for all 7 real-world and 14 simulation tasks, significantlyoutperforming existing methods like VoxPoser. Second, Manipulate-Anything'sdemonstrations can train more robust behavior cloning policies than trainingwith human demonstrations, or from data generated by VoxPoser, Scaling-up, andCode-As-Policies. We believe Manipulate-Anything can be a scalable method forboth generating data for robotics and solving novel tasks in a zero-shotsetting. Project page: https://robot-ma.github.io/.</description><author>Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna</author><pubDate>Thu, 29 Aug 2024 16:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18915v3</guid></item><item><title>Space3D-Bench: Spatial 3D Question Answering Benchmark</title><link>http://arxiv.org/abs/2408.16662v1</link><description>Answering questions about the spatial properties of the environment poseschallenges for existing language and vision foundation models due to a lack ofunderstanding of the 3D world notably in terms of relationships betweenobjects. To push the field forward, multiple 3D Q&amp;A datasets were proposedwhich, overall, provide a variety of questions, but they individually focus onparticular aspects of 3D reasoning or are limited in terms of data modalities.To address this, we present Space3D-Bench - a collection of 1000 generalspatial questions and answers related to scenes of the Replica dataset whichoffers a variety of data modalities: point clouds, posed RGB-D images,navigation meshes and 3D object detections. To ensure that the questions covera wide range of 3D objectives, we propose an indoor spatial questions taxonomyinspired by geographic information systems and use it to balance the datasetaccordingly. Moreover, we provide an assessment system that grades naturallanguage responses based on predefined ground-truth answers by leveraging aVision Language Model's comprehension of both text and images to compare theresponses with ground-truth textual information or relevant visual data.Finally, we introduce a baseline called RAG3D-Chat integrating the worldunderstanding of foundation models with rich context retrieval, achieving anaccuracy of 67% on the proposed dataset.</description><author>Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, Marc Pollefeys</author><pubDate>Thu, 29 Aug 2024 16:05:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16662v1</guid></item><item><title>Eigen-Cluster VIS: Improving Weakly-supervised Video Instance Segmentation by Leveraging Spatio-temporal Consistency</title><link>http://arxiv.org/abs/2408.16661v1</link><description>The performance of Video Instance Segmentation (VIS) methods has improvedsignificantly with the advent of transformer networks. However, these networksoften face challenges in training due to the high annotation cost. To addressthis, unsupervised and weakly-supervised methods have been developed to reducethe dependency on annotations. This work introduces a novel weakly-supervisedmethod called Eigen-cluster VIS that, without requiring any mask annotations,achieves competitive accuracy compared to other VIS approaches. This method isbased on two key innovations: a Temporal Eigenvalue Loss (TEL) and a clip-levelQuality Cluster Coefficient (QCC). The TEL ensures temporal coherence byleveraging the eigenvalues of the Laplacian matrix derived from graph adjacencymatrices. By minimizing the mean absolute error (MAE) between the eigenvaluesof adjacent frames, this loss function promotes smooth transitions and stablesegmentation boundaries over time, reducing temporal discontinuities andimproving overall segmentation quality. The QCC employs the K-means method toensure the quality of spatio-temporal clusters without relying on ground truthmasks. Using the Davies-Bouldin score, the QCC provides an unsupervised measureof feature discrimination, allowing the model to self-evaluate and adapt tovarying object distributions, enhancing robustness during the testing phase.These enhancements are computationally efficient and straightforward, offeringsignificant performance gains without additional annotated data. The proposedEigen-Cluster VIS method is evaluated on the YouTube-VIS 2019/2021 and OVISdatasets, demonstrating that it effectively narrows the performance gap betweenthe fully-supervised and weakly-supervised VIS approaches. The code isavailable on: https://github.com/farnooshar/EigenClusterVIS</description><author>Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei</author><pubDate>Thu, 29 Aug 2024 16:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16661v1</guid></item><item><title>Post-processing fairness with minimal changes</title><link>http://arxiv.org/abs/2408.15096v2</link><description>In this paper, we introduce a novel post-processing algorithm that is bothmodel-agnostic and does not require the sensitive attribute at test time. Inaddition, our algorithm is explicitly designed to enforce minimal changesbetween biased and debiased predictions; a property that, while highlydesirable, is rarely prioritized as an explicit objective in fairnessliterature. Our approach leverages a multiplicative factor applied to the logitvalue of probability scores produced by a black-box classifier. We demonstratethe efficacy of our method through empirical evaluations, comparing itsperformance against other four debiasing algorithms on two widely used datasetsin fairness research.</description><author>Federico Di Gennaro, Thibault Laugel, Vincent Grari, Xavier Renard, Marcin Detyniecki</author><pubDate>Thu, 29 Aug 2024 15:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15096v2</guid></item><item><title>Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition</title><link>http://arxiv.org/abs/2407.04559v2</link><description>Visual storytelling consists in generating a natural language story given atemporally ordered sequence of images. This task is not only challenging formodels, but also very difficult to evaluate with automatic metrics since thereis no consensus about what makes a story 'good'. In this paper, we introduce anovel method that measures story quality in terms of human likeness regardingthree key aspects highlighted in previous work: visual grounding, coherence,and repetitiveness. We then use this method to evaluate the stories generatedby several models, showing that the foundation model LLaVA obtains the bestresult, but only slightly so compared to TAPM, a 50-times smaller visualstorytelling model. Upgrading the visual and language components of TAPMresults in a model that yields competitive performance with a relatively lownumber of parameters. Finally, we carry out a human evaluation study, whoseresults suggest that a 'good' story may require more than a human-like level ofvisual grounding, coherence, and repetition.</description><author>Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle</author><pubDate>Thu, 29 Aug 2024 15:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04559v2</guid></item><item><title>Optimal Parallelization of Boosting</title><link>http://arxiv.org/abs/2408.16653v1</link><description>Recent works on the parallel complexity of Boosting have established stronglower bounds on the tradeoff between the number of training rounds $p$ and thetotal parallel work per round $t$. These works have also presented highlynon-trivial parallel algorithms that shed light on different regions of thistradeoff. Despite these advancements, a significant gap persists between thetheoretical lower bounds and the performance of these algorithms across much ofthe tradeoff space. In this work, we essentially close this gap by providingboth improved lower bounds on the parallel complexity of weak-to-stronglearners, and a parallel Boosting algorithm whose performance matches thesebounds across the entire $p$ vs.~$t$ compromise spectrum, up to logarithmicfactors. Ultimately, this work settles the true parallel complexity of Boostingalgorithms that are nearly sample-optimal.</description><author>Arthur da Cunha, Mikael Møller Høgsgaard, Kasper Green Larsen</author><pubDate>Thu, 29 Aug 2024 15:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16653v1</guid></item><item><title>Towards Efficient Modelling of String Dynamics: A Comparison of State Space and Koopman based Deep Learning Methods</title><link>http://arxiv.org/abs/2408.16650v1</link><description>This paper presents an examination of State Space Models (SSM) andKoopman-based deep learning methods for modelling the dynamics of both linearand non-linear stiff strings. Through experiments with datasets generated underdifferent initial conditions and sample rates, we assess the capacity of thesemodels to accurately model the complex behaviours observed in string dynamics.Our findings indicate that our proposed Koopman-based model performs as well asor better than other existing approaches in non-linear cases for long-sequencemodelling. We inform the design of these architectures with the structure of theproblems at hand. Although challenges remain in extending model predictionsbeyond the training horizon (i.e., extrapolation), the focus of ourinvestigation lies in the models' ability to generalise across differentinitial conditions within the training time interval. This research contributesinsights into the physical modelling of dynamical systems (in particular thoseaddressing musical acoustics) by offering a comparative overview of these andprevious methods and introducing innovative strategies for model improvement.Our results highlight the efficacy of these models in simulating non-lineardynamics and emphasise their wide-ranging applicability in accurately modellingdynamical systems over extended sequences.</description><author>Rodrigo Diaz, Carlos De La Vega Martin, Mark Sandler</author><pubDate>Thu, 29 Aug 2024 15:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16650v1</guid></item><item><title>Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2408.15886v2</link><description>In recent years, the evolution of machine learning techniques hassignificantly impacted the field of intrusion detection, particularly withinthe context of the Internet of Things (IoT). As IoT networks expand, the needfor robust security measures to counteract potential threats has becomeincreasingly critical. This paper introduces a hybrid Intrusion DetectionSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)with the XGBoost algorithm. Our proposed IDS leverages the unique capabilitiesof KANs, which utilize learnable activation functions to model complexrelationships within data, alongside the powerful ensemble learning techniquesof XGBoost, known for its high performance in classification tasks. This hybridapproach not only enhances the detection accuracy but also improves theinterpretability of the model, making it suitable for dynamic and intricate IoTenvironments. Experimental evaluations demonstrate that our hybrid IDS achievesan impressive detection accuracy exceeding 99% in distinguishing between benignand malicious activities. Additionally, we were able to achieve F1 scores,precision, and recall that exceeded 98%. Furthermore, we conduct a comparativeanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessingperformance metrics such as Precision, Recall, and F1-score. The resultsunderscore the efficacy of integrating KANs with XGBoost, highlighting thepotential of this innovative approach to significantly strengthen the securityframework of IoT networks.</description><author>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</author><pubDate>Thu, 29 Aug 2024 15:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15886v2</guid></item><item><title>Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination</title><link>http://arxiv.org/abs/2405.00846v3</link><description>Despite the impressive recent advances in learning-based robot control,ensuring robustness to out-of-distribution conditions remains an openchallenge. Safety filters can, in principle, keep arbitrary control policiesfrom incurring catastrophic failures by overriding unsafe actions, but existingsolutions for complex (e.g., legged) robot dynamics do not span the full motionenvelope and instead rely on local, reduced-order models. These filters tend tooverly restrict agility and can still fail when perturbed away from nominalconditions. This paper presents the gameplay filter, a new class of predictivesafety filter that continually plays out hypothetical matches between itssimulation-trained safety strategy and a virtual adversary co-trained to invokeworst-case events and sim-to-real error, and precludes actions that would causeit to fail down the line. We demonstrate the scalability and robustness of theapproach with a first-of-its-kind full-order safety filter for (36-D)quadrupedal dynamics. Physical experiments on two different quadruped platformsdemonstrate the superior zero-shot effectiveness of the gameplay filter underlarge perturbations such as tugging and unmodeled terrain.</description><author>Duy P. Nguyen, Kai-Chieh Hsu, Wenhao Yu, Jie Tan, Jaime F. Fisac</author><pubDate>Thu, 29 Aug 2024 15:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00846v3</guid></item><item><title>DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving</title><link>http://arxiv.org/abs/2408.16647v1</link><description>The advancement of autonomous driving technologies necessitates increasinglysophisticated methods for understanding and predicting real-world scenarios.Vision language models (VLMs) are emerging as revolutionary tools withsignificant potential to influence autonomous driving. In this paper, wepropose the DriveGenVLM framework to generate driving videos and use VLMs tounderstand them. To achieve this, we employ a video generation frameworkgrounded in denoising diffusion probabilistic models (DDPM) aimed at predictingreal-world video sequences. We then explore the adequacy of our generatedvideos for use in VLMs by employing a pre-trained model known as EfficientIn-context Learning on Egocentric Videos (EILEV). The diffusion model istrained with the Waymo open dataset and evaluated using the Fr\'echet VideoDistance (FVD) score to ensure the quality and realism of the generated videos.Corresponding narrations are provided by EILEV for these generated videos,which may be beneficial in the autonomous driving domain. These narrations canenhance traffic scene understanding, aid in navigation, and improve planningcapabilities. The integration of video generation with VLMs in the DriveGenVLMframework represents a significant step forward in leveraging advanced AImodels to address complex challenges in autonomous driving.</description><author>Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, Zhaobin Mo</author><pubDate>Thu, 29 Aug 2024 15:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16647v1</guid></item><item><title>SODAWideNet++: Combining Attention and Convolutions for Salient Object Detection</title><link>http://arxiv.org/abs/2408.16645v1</link><description>Salient Object Detection (SOD) has traditionally relied on feature refinementmodules that utilize the features of an ImageNet pre-trained backbone. However,this approach limits the possibility of pre-training the entire network becauseof the distinct nature of SOD and image classification. Additionally, thearchitecture of these backbones originally built for Image classification issub-optimal for a dense prediction task like SOD. To address these issues, wepropose a novel encoder-decoder-style neural network called SODAWideNet++ thatis designed explicitly for SOD. Inspired by the vision transformers ability toattain a global receptive field from the initial stages, we introduce theAttention Guided Long Range Feature Extraction (AGLRFE) module, which combineslarge dilated convolutions and self-attention. Specifically, we use attentionfeatures to guide long-range information extracted by multiple dilatedconvolutions, thus taking advantage of the inductive biases of a convolutionoperation and the input dependency brought by self-attention. In contrast tothe current paradigm of ImageNet pre-training, we modify 118K annotated imagesfrom the COCO semantic segmentation dataset by binarizing the annotations topre-train the proposed model end-to-end. Further, we supervise the backgroundpredictions along with the foreground to push our model to generate accuratesaliency predictions. SODAWideNet++ performs competitively on five differentdatasets while only containing 35% of the trainable parameters compared to thestate-of-the-art models. The code and pre-computed saliency maps are providedat https://github.com/VimsLab/SODAWideNetPlusPlus.</description><author>Rohit Venkata Sai Dulam, Chandra Kambhamettu</author><pubDate>Thu, 29 Aug 2024 15:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16645v1</guid></item><item><title>3D Pose-Based Temporal Action Segmentation for Figure Skating: A Fine-Grained and Jump Procedure-Aware Annotation Approach</title><link>http://arxiv.org/abs/2408.16638v1</link><description>Understanding human actions from videos is essential in many domains,including sports. In figure skating, technical judgments are performed bywatching skaters' 3D movements, and its part of the judging procedure can beregarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figureskating that automatically assign temporal semantics to video are activelyresearched. However, there is a lack of datasets and effective methods for TAStasks requiring 3D pose data. In this study, we first created the FS-Jump3Ddataset of complex and dynamic figure skating jumps using optical markerlessmotion capture. We also propose a new fine-grained figure skating jump TASdataset annotation method with which TAS models can learn jump procedures. Inthe experimental results, we validated the usefulness of 3D pose features asinput and the fine-grained dataset for the TAS model in figure skating.FS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.</description><author>Ryota Tanaka, Tomohiro Suzuki, Keisuke Fujii</author><pubDate>Thu, 29 Aug 2024 15:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16638v1</guid></item><item><title>Generalization of Hamiltonian algorithms</title><link>http://arxiv.org/abs/2405.14469v2</link><description>The paper proves generalization results for a class of stochastic learningalgorithms. The method applies whenever the algorithm generates an absolutelycontinuous distribution relative to some a-priori measure and the Radon Nikodymderivative has subgaussian concentration. Applications are bounds for the Gibbsalgorithm and randomizations of stable deterministic algorithms as well asPAC-Bayesian bounds with data-dependent priors.</description><author>Andreas Maurer</author><pubDate>Thu, 29 Aug 2024 15:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14469v2</guid></item><item><title>RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model</title><link>http://arxiv.org/abs/2408.16634v1</link><description>The increasing sophistication of text-to-image generative models has led tocomplex challenges in defining and enforcing copyright infringement criteriaand protection. Existing methods, such as watermarking and datasetdeduplication, fail to provide comprehensive solutions due to the lack ofstandardized metrics and the inherent complexity of addressing copyrightinfringement in diffusion models. To deal with these challenges, we propose aReinforcement Learning-based Copyright Protection(RLCP) method forText-to-Image Diffusion Model, which minimizes the generation ofcopyright-infringing content while maintaining the quality of themodel-generated dataset. Our approach begins with the introduction of a novelcopyright metric grounded in copyright law and court precedents oninfringement. We then utilize the Denoising Diffusion Policy Optimization(DDPO) framework to guide the model through a multi-step decision-makingprocess, optimizing it using a reward function that incorporates our proposedcopyright metric. Additionally, we employ KL divergence as a regularizationterm to mitigate some failure modes and stabilize RL fine-tuning. Experimentsconducted on 3 mixed datasets of copyright and non-copyright images demonstratethat our approach significantly reduces copyright infringement risk whilemaintaining image quality.</description><author>Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</author><pubDate>Thu, 29 Aug 2024 15:39:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16634v1</guid></item><item><title>Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning</title><link>http://arxiv.org/abs/2408.16633v1</link><description>With the rapid growth of global e-commerce, the demand for automation in thelogistics industry is increasing. This study focuses on automated pickingsystems in warehouses, utilizing deep learning and reinforcement learningtechnologies to enhance picking efficiency and accuracy while reducing systemfailure rates. Through empirical analysis, we demonstrate the effectiveness ofthese technologies in improving robot picking performance and adaptability tocomplex environments. The results show that the integrated machine learningmodel significantly outperforms traditional methods, effectively addressing thechallenges of peak order processing, reducing operational errors, and improvingoverall logistics efficiency. Additionally, by analyzing environmental factors,this study further optimizes system design to ensure efficient and stableoperation under variable conditions. This research not only provides innovativesolutions for logistics automation but also offers a theoretical and empiricalfoundation for future technological development and application.</description><author>Keqin Li, Jin Wang, Xubo Wu, Xirui Peng, Runmian Chang, Xiaoyu Deng, Yiwen Kang, Yue Yang, Fanghao Ni, Bo Hong</author><pubDate>Thu, 29 Aug 2024 15:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16633v1</guid></item><item><title>Maelstrom Networks</title><link>http://arxiv.org/abs/2408.16632v1</link><description>Artificial Neural Networks has struggled to devise a way to incorporateworking memory into neural networks. While the ``long term'' memory can be seenas the learned weights, the working memory consists likely more of dynamicalactivity, that is missing from feed-forward models. Current state of the artmodels such as transformers tend to ``solve'' this by ignoring working memoryentirely and simply process the sequence as an entire piece of data; howeverthis means the network cannot process the sequence in an online fashion, andleads to an immense explosion in memory requirements. Here, inspired by acombination of controls, reservoir computing, deep learning, and recurrentneural networks, we offer an alternative paradigm that combines the strength ofrecurrent networks, with the pattern matching capability of feed-forward neuralnetworks, which we call the \textit{Maelstrom Networks} paradigm. This paradigmleaves the recurrent component - the \textit{Maelstrom} - unlearned, andoffloads the learning to a powerful feed-forward network. This allows thenetwork to leverage the strength of feed-forward training without unrolling thenetwork, and allows for the memory to be implemented in new neuromorphichardware. It endows a neural network with a sequential memory that takesadvantage of the inductive bias that data is organized causally in the temporaldomain, and imbues the network with a state that represents the agent's``self'', moving through the environment. This could also lead the way tocontinual learning, with the network modularized and ``'protected'' fromoverwrites that come with new data. In addition to aiding in solving theseperformance problems that plague current non-temporal deep networks, this alsocould finally lead towards endowing artificial networks with a sense of``self''.</description><author>Matthew Evanusa, Cornelia Fermüller, Yiannis Aloimonos</author><pubDate>Thu, 29 Aug 2024 15:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16632v1</guid></item><item><title>LLMs generate structurally realistic social networks but overestimate political homophily</title><link>http://arxiv.org/abs/2408.16629v1</link><description>Generating social networks is essential for many applications, such asepidemic modeling and social simulations. Prior approaches either involve deeplearning models, which require many observed networks for training, or stylizedmodels, which are limited in their realism and flexibility. In contrast, LLMsoffer the potential for zero-shot and flexible network generation. However, twokey questions are: (1) are LLM's generated networks realistic, and (2) what arerisks of bias, given the importance of demographics in forming social ties? Toanswer these questions, we develop three prompting methods for networkgeneration and compare the generated networks to real social networks. We findthat more realistic networks are generated with "local" methods, where the LLMconstructs relations for one persona at a time, compared to "global" methodsthat construct the entire network at once. We also find that the generatednetworks match real networks on many characteristics, including density,clustering, community structure, and degree. However, we find that LLMsemphasize political homophily over all other types of homophily andoverestimate political homophily relative to real-world measures.</description><author>Serina Chang, Alicja Chaszczewicz, Emma Wang, Maya Josifovska, Emma Pierson, Jure Leskovec</author><pubDate>Thu, 29 Aug 2024 15:36:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16629v1</guid></item><item><title>Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes</title><link>http://arxiv.org/abs/2405.20743v2</link><description>Trajectory forecasting is crucial for video surveillance analytics, as itenables the anticipation of future movements for a set of agents, e.g.basketball players engaged in intricate interactions with long-term intentions.Deep generative models offer a natural learning approach for trajectoryforecasting, yet they encounter difficulties in achieving an optimal balancebetween sampling fidelity and diversity. We address this challenge byleveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize adiscrete latent space to tackle the issue of posterior collapse. Specifically,we introduce an instance-based codebook that allows tailored latentrepresentations for each example. In a nutshell, the rows of the codebook aredynamically adjusted to reflect contextual information (i.e., past motionpatterns extracted from the observed trajectories). In this way, thediscretization process gains flexibility, leading to improved reconstructions.Notably, instance-level dynamics are injected into the codebook throughlow-rank updates, which restrict the customization of the codebook to a lowerdimension space. The resulting discrete space serves as the basis of thesubsequent step, which regards the training of a diffusion-based predictivemodel. We show that such a two-fold framework, augmented with instance-leveldiscretization, leads to accurate and diverse forecasts, yieldingstate-of-the-art performance on three established benchmarks.</description><author>Riccardo Benaglia, Angelo Porrello, Pietro Buzzega, Simone Calderara, Rita Cucchiara</author><pubDate>Thu, 29 Aug 2024 15:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20743v2</guid></item><item><title>Turbulence Strength $C_n^2$ Estimation from Video using Physics-based Deep Learning</title><link>http://arxiv.org/abs/2408.16623v1</link><description>Images captured from a long distance suffer from dynamic image distortion dueto turbulent flow of air cells with random temperatures, and thus refractiveindices. This phenomenon, known as image dancing, is commonly characterized byits refractive-index structure constant $C_n^2$ as a measure of the turbulencestrength. For many applications such as atmospheric forecast model,long-range/astronomy imaging, and aviation safety, optical communicationtechnology, $C_n^2$ estimation is critical for accurately sensing the turbulentenvironment. Previous methods for $C_n^2$ estimation include estimation frommeteorological data (temperature, relative humidity, wind shear, etc.) forsingle-point measurements, two-ended pathlength measurements from opticalscintillometer for path-averaged $C_n^2$, and more recently estimating $C_n^2$from passive video cameras for low cost and hardware complexity. In this paper,we present a comparative analysis of classical image gradient methods for$C_n^2$ estimation and modern deep learning-based methods leveragingconvolutional neural networks. To enable this, we collect a dataset of videocapture along with reference scintillometer measurements for ground truth, andwe release this unique dataset to the scientific community. We observe thatdeep learning methods can achieve higher accuracy when trained on similar data,but suffer from generalization errors to other, unseen imagery as compared toclassical methods. To overcome this trade-off, we present a novel physics-basednetwork architecture that combines learned convolutional layers with adifferentiable image gradient method that maintains high accuracy while beinggeneralizable across image datasets.</description><author>Ripon Kumar Saha, Esen Salcin, Jihoo Kim, Joseph Smith, Suren Jayasuriya</author><pubDate>Thu, 29 Aug 2024 15:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16623v1</guid></item><item><title>Sparse Signal Reconstruction for Overdispersed Low-photon Count Biomedical Imaging Using $\ell_p$ Total Variation</title><link>http://arxiv.org/abs/2408.16622v1</link><description>The negative binomial model, which generalizes the Poisson distributionmodel, can be found in applications involving low-photon signal recovery,including medical imaging. Recent studies have explored several regularizationterms for the negative binomial model, such as the $\ell_p$ quasi-norm with $0&lt; p &lt; 1$, $\ell_1$ norm, and the total variation (TV) quasi-seminorm forpromoting sparsity in signal recovery. These penalty terms have been shown toimprove image reconstruction outcomes. In this paper, we investigate the$\ell_p$ quasi-seminorm, both isotropic and anisotropic $\ell_p$ TVquasi-seminorms, within the framework of the negative binomial statisticalmodel. This problem can be formulated as an optimization problem, which wesolve using a gradient-based approach. We present comparisons between thenegative binomial and Poisson statistical models using the $\ell_p$ TVquasi-seminorm as well as common penalty terms. Our experimental resultshighlight the efficacy of the proposed method.</description><author>Yu Lu, Roummel F. Marcia</author><pubDate>Thu, 29 Aug 2024 15:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16622v1</guid></item><item><title>Verification of Geometric Robustness of Neural Networks via Piecewise Linear Approximation and Lipschitz Optimisation</title><link>http://arxiv.org/abs/2408.13140v2</link><description>We address the problem of verifying neural networks against geometrictransformations of the input image, including rotation, scaling, shearing, andtranslation. The proposed method computes provably sound piecewise linearconstraints for the pixel values by using sampling and linear approximations incombination with branch-and-bound Lipschitz optimisation. The method obtainsprovably tighter over-approximations of the perturbation region than thepresent state-of-the-art. We report results from experiments on a comprehensiveset of verification benchmarks on MNIST and CIFAR10. We show that our proposedimplementation resolves up to 32% more verification cases than presentapproaches.</description><author>Ben Batten, Yang Zheng, Alessandro De Palma, Panagiotis Kouvaros, Alessio Lomuscio</author><pubDate>Thu, 29 Aug 2024 15:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13140v2</guid></item><item><title>Towards Infusing Auxiliary Knowledge for Distracted Driver Detection</title><link>http://arxiv.org/abs/2408.16621v1</link><description>Distracted driving is a leading cause of road accidents globally.Identification of distracted driving involves reliably detecting andclassifying various forms of driver distraction (e.g., texting, eating, orusing in-car devices) from in-vehicle camera feeds to enhance road safety. Thistask is challenging due to the need for robust models that can generalize to adiverse set of driver behaviors without requiring extensive annotated datasets.In this paper, we propose KiD3, a novel method for distracted driver detection(DDD) by infusing auxiliary knowledge about semantic relations between entitiesin a scene and the structural configuration of the driver's pose. Specifically,we construct a unified framework that integrates the scene graphs, and driverpose information with the visual cues in video frames to create a holisticrepresentation of the driver's actions.Our results indicate that KiD3 achievesa 13.64% accuracy improvement over the vision-only baseline by incorporatingsuch auxiliary knowledge with visual information.</description><author>Ishwar B Balappanawar, Ashmit Chamoli, Ruwan Wickramarachchi, Aditya Mishra, Ponnurangam Kumaraguru, Amit P. Sheth</author><pubDate>Thu, 29 Aug 2024 15:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16621v1</guid></item><item><title>Hyperdimensional Vector Tsetlin Machines with Applications to Sequence Learning and Generation</title><link>http://arxiv.org/abs/2408.16620v1</link><description>We construct a two-layered model for learning and generating sequential datathat is both computationally fast and competitive with vanilla Tsetlinmachines, adding numerous advantages. Through the use of hyperdimensionalvector computing (HVC) algebras and Tsetlin machine clause structures, wedemonstrate that the combination of both inherits the generality of dataencoding and decoding of HVC with the fast interpretable nature of Tsetlinmachines to yield a powerful machine learning model. We apply the approach intwo areas, namely in forecasting, generating new sequences, and classification.For the latter, we derive results for the entire UCR Time Series Archive andcompare with the standard benchmarks to see how well the method competes intime series classification.</description><author>Christian D. Blakely</author><pubDate>Thu, 29 Aug 2024 15:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16620v1</guid></item><item><title>Blending Low and High-Level Semantics of Time Series for Better Masked Time Series Generation</title><link>http://arxiv.org/abs/2408.16613v1</link><description>State-of-the-art approaches in time series generation (TSG), such asTimeVQVAE, utilize vector quantization-based tokenization to effectively modelcomplex distributions of time series. These approaches first learn to transformtime series into a sequence of discrete latent vectors, and then a prior modelis learned to model the sequence. The discrete latent vectors, however, onlycapture low-level semantics (\textit{e.g.,} shapes). We hypothesize thathigher-fidelity time series can be generated by training a prior model on moreinformative discrete latent vectors that contain both low and high-levelsemantics (\textit{e.g.,} characteristic dynamics). In this paper, we introducea novel framework, termed NC-VQVAE, to integrate self-supervised learning intothose TSG methods to derive a discrete latent space where low and high-levelsemantics are captured. Our experimental results demonstrate that NC-VQVAEresults in a considerable improvement in the quality of synthetic samples.</description><author>Johan Vik Mathisen, Erlend Lokna, Daesoo Lee, Erlend Aune</author><pubDate>Thu, 29 Aug 2024 15:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16613v1</guid></item><item><title>Data Quality Monitoring through Transfer Learning on Anomaly Detection for the Hadron Calorimeters</title><link>http://arxiv.org/abs/2408.16612v1</link><description>The proliferation of sensors brings an immense volume of spatio-temporal (ST)data in many domains for various purposes, including monitoring, diagnostics,and prognostics applications. Data curation is a time-consuming process for alarge volume of data, making it challenging and expensive to deploy dataanalytics platforms in new environments. Transfer learning (TL) mechanismspromise to mitigate data sparsity and model complexity by utilizing pre-trainedmodels for a new task. Despite the triumph of TL in fields like computer visionand natural language processing, efforts on complex ST models for anomalydetection (AD) applications are limited. In this study, we present thepotential of TL within the context of AD for the Hadron Calorimeter of theCompact Muon Solenoid experiment at CERN. We have transferred the ST AD modelstrained on data collected from one part of a calorimeter to another. We haveinvestigated different configurations of TL on semi-supervised autoencoders ofthe ST AD models -- transferring convolutional, graph, and recurrent neuralnetworks of both the encoder and decoder networks. The experiment resultsdemonstrate that TL effectively enhances the model learning accuracy on atarget subdetector. The TL achieves promising data reconstruction and ADperformance while substantially reducing the trainable parameters of the ADmodels. It also improves robustness against anomaly contamination in thetraining data sets of the semi-supervised AD models.</description><author>Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, Pavel Parygin, David Yu, Jay Dittmann, The CMS-HCAL Collaboration</author><pubDate>Thu, 29 Aug 2024 15:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16612v1</guid></item><item><title>Subspace Representation Learning for Sparse Linear Arrays to Localize More Sources than Sensors: A Deep Learning Methodology</title><link>http://arxiv.org/abs/2408.16605v1</link><description>Localizing more sources than sensors with a sparse linear array (SLA) haslong relied on minimizing a distance between two covariance matrices and recentalgorithms often utilize semidefinite programming (SDP). Although deep neuralnetwork (DNN)-based methods offer new alternatives, they still depend oncovariance matrix fitting. In this paper, we develop a novel methodology thatestimates the co-array subspaces from a sample covariance for SLAs. Ourmethodology trains a DNN to learn signal and noise subspace representationsthat are invariant to the selection of bases. To learn such representations, wepropose loss functions that gauge the separation between the desired and theestimated subspace. In particular, we propose losses that measure the length ofthe shortest path between subspaces viewed on a union of Grassmannians, andprove that it is possible for a DNN to approximate signal subspaces. Thecomputation of learning subspaces of different dimensions is accelerated by anew batch sampling strategy called consistent rank sampling. The methodology isrobust to array imperfections due to its geometry-agnostic and data-drivennature. In addition, we propose a fully end-to-end gridless approach thatdirectly learns angles to study the possibility of bypassing subspace methods.Numerical results show that learning such subspace representations is morebeneficial than learning covariances or angles. It outperforms conventionalSDP-based methods such as the sparse and parametric approach (SPA) and existingDNN-based covariance reconstruction methods for a wide range of signal-to-noiseratios (SNRs), snapshots, and source numbers for both perfect and imperfectarrays.</description><author>Kuan-Lin Chen, Bhaskar D. Rao</author><pubDate>Thu, 29 Aug 2024 15:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16605v1</guid></item><item><title>Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express</title><link>http://arxiv.org/abs/2408.14698v2</link><description>As user content and queries become increasingly multi-modal, the need foreffective multi-modal search systems has grown. Traditional search systemsoften rely on textual and metadata annotations for indexed images, whilemulti-modal embeddings like CLIP enable direct search using text and imageembeddings. However, embedding-based approaches face challenges in integratingcontextual features such as user locale and recency. Building a scalablemulti-modal search system requires fine-tuning several components. This paperpresents a multi-modal search architecture and a series of AB tests thatoptimize embeddings and multi-modal technologies in Adobe Express templatesearch. We address considerations such as embedding model selection, the rolesof embeddings in matching and ranking, and the balance between dense and sparseembeddings. Our iterative approach demonstrates how utilizing sparse, dense,and contextual features enhances short and long query search, significantlyreduces null rates (over 70\%), and increases click-through rates (CTR). Ourfindings provide insights into developing robust multi-modal search systems,thereby enhancing relevance for complex queries.</description><author>Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan</author><pubDate>Thu, 29 Aug 2024 15:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14698v2</guid></item><item><title>Examination of Code generated by Large Language Models</title><link>http://arxiv.org/abs/2408.16601v1</link><description>Large language models (LLMs), such as ChatGPT and Copilot, are transformingsoftware development by automating code generation and, arguably, enable rapidprototyping, support education, and boost productivity. Therefore, correctnessand quality of the generated code should be on par with manually written code.To assess the current state of LLMs in generating correct code of high quality,we conducted controlled experiments with ChatGPT and Copilot: we let the LLMsgenerate simple algorithms in Java and Python along with the corresponding unittests and assessed the correctness and the quality (coverage) of the generated(test) codes. We observed significant differences between the LLMs, between thelanguages, between algorithm and test codes, and over time. The present paperreports these results together with the experimental methods allowing repeatedand comparable assessments for more algorithms, languages, and LLMs over time.</description><author>Robin Beer, Alexander Feix, Tim Guttzeit, Tamara Muras, Vincent Müller, Maurice Rauscher, Florian Schäffler, Welf Löwe</author><pubDate>Thu, 29 Aug 2024 15:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16601v1</guid></item><item><title>On the Efficacy of Text-Based Input Modalities for Action Anticipation</title><link>http://arxiv.org/abs/2401.12972v3</link><description>Anticipating future actions is a highly challenging task due to the diversityand scale of potential future actions; yet, information from differentmodalities help narrow down plausible action choices. Each modality can providediverse and often complementary context for the model to learn from. Whileprevious multi-modal methods leverage information from modalities such as videoand audio, we primarily explore how text descriptions of actions and objectscan also lead to more accurate action anticipation by providing additionalcontextual cues, e.g., about the environment and its contents. We propose aMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformerarchitecture that jointly learns from multi-modal features and textdescriptions of actions and objects. We train our model in two stages, wherethe model first learns to align video clips with descriptions of futureactions, and is subsequently fine-tuned to predict future actions. Compared toexisting methods, M-CAT has the advantage of learning additional context fromtwo types of text inputs: rich descriptions of future actions duringpre-training, and, text descriptions for detected objects and actions duringmodality feature fusion. Through extensive experimental evaluation, wedemonstrate that our model outperforms previous methods on the EpicKitchensdatasets, and show that using simple text descriptions of actions and objectsaid in more effective action anticipation. In addition, we examine the impactof object and action information obtained via text, and perform extensiveablations.</description><author>Apoorva Beedu, Harish Haresamudram, Karan Samel, Irfan Essa</author><pubDate>Thu, 29 Aug 2024 15:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12972v3</guid></item><item><title>sEMG-Driven Physics-Informed Gated Recurrent Networks for Modeling Upper Limb Multi-Joint Movement Dynamics</title><link>http://arxiv.org/abs/2408.16599v1</link><description>Exoskeletons and rehabilitation systems offer great potential for enhancinghuman strength and recovery through advanced human-machine interfaces (HMIs)that adapt to movement dynamics. However, the real-time application ofphysics-informed neural networks (PINNs) is limited by their reliance on fixedinput lengths and surrogate models. This study introduces a novelphysics-informed Gated Recurrent Network (PiGRN) designed to predictmulti-joint torques using surface electromyography (sEMG) data. The PiGRN modelemploys a Gated Recurrent Unit (GRU) to convert time-series sEMG inputs intomulti-joint kinematics and external loads, which are then integrated into anequation of motion to ensure consistency with physical laws. Experimentalvalidation with sEMG data from five participants performing elbowflexion-extension tasks showed that the PiGRN model accurately predicted jointtorques for 10 unfamiliar movements, with RMSE values between 4.02\% and11.40\% and correlation coefficients ranging from 0.87 to 0.98. These findingshighlight the PiGRN's potential for real-time exoskeleton and rehabilitationapplications. Future research will explore more diverse datasets, improvemusculoskeletal models, and investigate unsupervised learning methods.</description><author>Rajnish Kumar, Anand Gupta, Suriya Prakash Muthukrishnan, Lalan Kumar, Sitikantha Roy</author><pubDate>Thu, 29 Aug 2024 15:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16599v1</guid></item><item><title>High-Dimensional Sparse Data Low-rank Representation via Accelerated Asynchronous Parallel Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2408.16592v1</link><description>Data characterized by high dimensionality and sparsity are commonly used todescribe real-world node interactions. Low-rank representation (LR) can maphigh-dimensional sparse (HDS) data to low-dimensional feature spaces and infernode interactions via modeling data latent associations. Unfortunately,existing optimization algorithms for LR models are computationally inefficientand slowly convergent on large-scale datasets. To address this issue, thispaper proposes an Accelerated Asynchronous Parallel Stochastic Gradient DescentA2PSGD for High-Dimensional Sparse Data Low-rank Representation with threefold-ideas: a) establishing a lock-free scheduler to simultaneously respond toscheduling requests from multiple threads; b) introducing a greedyalgorithm-based load balancing strategy for balancing the computational loadamong threads; c) incorporating Nesterov's accelerated gradient into thelearning scheme to accelerate model convergence. Empirical studies show thatA2PSGD outperforms existing optimization algorithms for HDS data LR in bothaccuracy and training time.</description><author>Qicong Hu, Hao Wu</author><pubDate>Thu, 29 Aug 2024 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16592v1</guid></item><item><title>CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</title><link>http://arxiv.org/abs/2408.16589v1</link><description>We demonstrate that carefully adjusting the tokenizer of the Whisper speechrecognition model significantly improves the precision of word-level timestampswhen applying dynamic time warping to the decoder's cross-attention scores. Wefine-tune the model to produce more verbatim speech transcriptions and employseveral techniques to increase robustness against multiple speakers andbackground noise. These adjustments achieve state-of-the-art performance onbenchmarks for verbatim speech transcription, word segmentation, and the timeddetection of filler events, and can further mitigate transcriptionhallucinations. The code is available openhttps://github.com/nyrahealth/CrisperWhisper.</description><author>Laurin Wagner, Bernhard Thallinger, Mario Zusag</author><pubDate>Thu, 29 Aug 2024 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16589v1</guid></item></channel></rss>