<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 13 Oct 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts</title><link>http://arxiv.org/abs/2410.08211v1</link><description>Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) arerenowned for their versatility, as they can be applied to diverse applicationsin a zero-shot setup. However, when these models are used in specific domains,their performance often falls short due to domain gaps or theunder-representation of these domains in the training data. While fine-tuningVLP models on custom datasets with human-annotated labels can address thisissue, annotating even a small-scale dataset (e.g., 100k samples) can be anexpensive endeavor, often requiring expert annotators if the task is complex.To address these challenges, we propose LatteCLIP, an unsupervised method forfine-tuning CLIP models on classification with known class names in customdomains, without relying on human annotations. Our method leverages LargeMultimodal Models (LMMs) to generate expressive textual descriptions for bothindividual images and groups of images. These provide additional contextualinformation to guide the fine-tuning process in the custom domains. SinceLMM-generated descriptions are prone to hallucination or missing details, weintroduce a novel strategy to distill only the useful information and stabilizethe training. Specifically, we learn rich per-class prototype representationsfrom noisy generated texts and dual pseudo-labels. Our experiments on 10domain-specific datasets show that LatteCLIP outperforms pre-trained zero-shotmethods by an average improvement of +4.74 points in top-1 accuracy and otherstate-of-the-art unsupervised methods by +3.45 points.</description><author>Anh-Quan Cao, Maximilian Jaritz, Matthieu Guillaumin, Raoul de Charette, Loris Bazzani</author><pubDate>Thu, 10 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08211v1</guid></item><item><title>PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection</title><link>http://arxiv.org/abs/2410.08210v1</link><description>Single point supervised oriented object detection has gained attention andmade initial progress within the community. Diverse from those approachesrelying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBBhas shown promise due to its prior-free feature. In this paper, we proposePointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotatedboxes from points without relying on any other prior. Specifically, we firstgenerate a Class Probability Map (CPM) by training the network with non-uniformpositive and negative sampling. We show that the CPM is able to learn theapproximate object regions and their contours. Then, Principal ComponentAnalysis (PCA) is applied to accurately estimate the orientation and theboundary of objects. By further incorporating a separation mechanism, weresolve the confusion caused by the overlapping on the CPM, enabling itsoperation in high-density scenarios. Extensive comparisons demonstrate that ourmethod achieves a training speed 15.58x faster and an accuracy improvement of11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to theprevious state-of-the-art, PointOBB. This significantly advances the cuttingedge of single point supervised oriented detection in the modular track.</description><author>Botao Ren, Xue Yang, Yi Yu, Junwei Luo, Zhidong Deng</author><pubDate>Thu, 10 Oct 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08210v1</guid></item><item><title>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</title><link>http://arxiv.org/abs/2410.08209v1</link><description>Current large multimodal models (LMMs) face challenges in grounding, whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision, we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding, we introduce an "attend-and-segment" method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMMutilizing a diffusion-based visual encoder, as opposed to the standard CLIPvisual encoder, and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata, our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.Notably, we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision, outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</description><author>Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Thu, 10 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08209v1</guid></item><item><title>SPA: 3D Spatial-Awareness Enables Effective Embodied Representation</title><link>http://arxiv.org/abs/2410.08208v1</link><description>In this paper, we introduce SPA, a novel representation learning frameworkthat emphasizes the importance of 3D spatial awareness in embodied AI. Ourapproach leverages differentiable neural rendering on multi-view images toendow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.We present the most comprehensive evaluation of embodied representationlearning to date, covering 268 tasks across 8 simulators with diverse policiesin both single-task and language-conditioned multi-task scenarios. The resultsare compelling: SPA consistently outperforms more than 10 state-of-the-artrepresentation methods, including those specifically designed for embodied AI,vision-centric tasks, and multi-modal applications, while using less trainingdata. Furthermore, we conduct a series of real-world experiments to confirm itseffectiveness in practical scenarios. These results highlight the critical roleof 3D spatial awareness for embodied representation learning. Our strongestmodel takes more than 6000 GPU hours to train and we are committed toopen-sourcing all code and model weights to foster future research in embodiedrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.</description><author>Haoyi Zhu, Honghui Yang, Yating Wang, Jiange Yang, Limin Wang, Tong He</author><pubDate>Thu, 10 Oct 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08208v1</guid></item><item><title>DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models</title><link>http://arxiv.org/abs/2410.08207v1</link><description>Discrete diffusion models have achieved success in tasks like imagegeneration and masked language modeling but face limitations in controlledcontent editing. We introduce DICE (Discrete Inversion for ControllableEditing), the first approach to enable precise inversion for discrete diffusionmodels, including multinomial diffusion and masked generative models. Byrecording noise sequences and masking patterns during the reverse diffusionprocess, DICE enables accurate reconstruction and flexible editing of discretedata without the need for predefined masks or attention manipulation. Wedemonstrate the effectiveness of DICE across both image and text domains,evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our resultsshow that DICE preserves high data fidelity while enhancing editingcapabilities, offering new opportunities for fine-grained content manipulationin discrete spaces. For project webpage, seehttps://hexiaoxiao-cs.github.io/DICE/.</description><author>Xiaoxiao He, Ligong Han, Quan Dao, Song Wen, Minhao Bai, Di Liu, Han Zhang, Martin Renqiang Min, Felix Juefei-Xu, Chaowei Tan, Bo Liu, Kang Li, Hongdong Li, Junzhou Huang, Faez Ahmed, Akash Srivastava, Dimitris Metaxas</author><pubDate>Thu, 10 Oct 2024 17:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08207v1</guid></item><item><title>Interactive4D: Interactive 4D LiDAR Segmentation</title><link>http://arxiv.org/abs/2410.08206v1</link><description>Interactive segmentation has an important role in facilitating the annotationprocess of future LiDAR datasets. Existing approaches sequentially segmentindividual objects at each LiDAR scan, repeating the process throughout theentire sequence, which is redundant and ineffective. In this work, we proposeinteractive 4D segmentation, a new paradigm that allows segmenting multipleobjects on multiple LiDAR scans simultaneously, and Interactive4D, the firstinteractive 4D segmentation model that segments multiple objects onsuperimposed consecutive LiDAR scans in a single iteration by utilizing thesequential nature of LiDAR data. While performing interactive segmentation, ourmodel leverages the entire space-time volume, leading to more efficientsegmentation. Operating on the 4D volume, it directly provides consistentinstance IDs over time and also simplifies tracking annotations. Moreover, weshow that click simulations are crucial for successful model training on LiDARpoint clouds. To this end, we design a click simulation strategy that is bettersuited for the characteristics of LiDAR data. To demonstrate its accuracy andeffectiveness, we evaluate Interactive4D on multiple LiDAR datasets, whereInteractive4D achieves a new state-of-the-art by a large margin. Uponacceptance, we will publicly release the code and models athttps://vision.rwth-aachen.de/Interactive4D.</description><author>Ilya Fradlin, Idil Esen Zulfikar, Kadir Yilmaz, Theodora Kontogianni, Bastian Leibe</author><pubDate>Thu, 10 Oct 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08206v1</guid></item><item><title>Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training</title><link>http://arxiv.org/abs/2410.08202v1</link><description>The rapid advancement of Large Language Models (LLMs) has led to an influx ofefforts to extend their capabilities to multimodal tasks. Among them, growingattention has been focused on monolithic Multimodal Large Language Models(MLLMs) that integrate visual encoding and language decoding into a single LLM.Despite the structural simplicity and deployment-friendliness, training amonolithic MLLM with promising performance still remains challenging. Inparticular, the popular approaches adopt continuous pre-training to extend apre-trained LLM to a monolithic MLLM, which suffers from catastrophicforgetting and leads to performance degeneration. In this paper, we aim toovercome this limitation from the perspective of delta tuning. Specifically,our core idea is to embed visual parameters into a pre-trained LLM, therebyincrementally learning visual knowledge from massive data via delta tuning,i.e., freezing the LLM when optimizing the visual parameters. Based on thisprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlesslyintegrates a set of visual experts via a multimodal mixture-of-expertsstructure. Moreover, we propose an innovative pre-training strategy to maximizethe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training(EViP). In particular, EViP is designed as a progressive learning process forvisual experts, which aims to fully exploit the visual knowledge from noisydata to high-quality data. To validate our approach, we conduct extensiveexperiments on 16 benchmarks. Experimental results not only validate thesuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, butalso confirm its better deployment efficiency, with first token latency reducedby up to 67%.</description><author>Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, Xizhou Zhu</author><pubDate>Thu, 10 Oct 2024 17:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08202v1</guid></item><item><title>Sparse Repellency for Shielded Generation in Text-to-image Diffusion Models</title><link>http://arxiv.org/abs/2410.06025v2</link><description>The increased adoption of diffusion models in text-to-image generation hastriggered concerns on their reliability. Such models are now closelyscrutinized under the lens of various metrics, notably calibration, fairness,or compute efficiency. We focus in this work on two issues that arise whendeploying these models: a lack of diversity when prompting images, and atendency to recreate images from the training set. To solve both problems, wepropose a method that coaxes the sampled trajectories of pretrained diffusionmodels to land on images that fall outside of a reference set. We achieve thisby adding repellency terms to the diffusion SDE throughout the generationtrajectory, which are triggered whenever the path is expected to land tooclosely to an image in the shielded reference set. Our method is sparse in thesense that these repellency terms are zero and inactive most of the time, andeven more so towards the end of the generation trajectory. Our method, namedSPELL for sparse repellency, can be used either with a static reference setthat contains protected images, or dynamically, by updating the set at eachtimestep with the expected images concurrently generated within a batch. Weshow that adding SPELL to popular diffusion models improves their diversitywhile impacting their FID only marginally, and performs comparatively betterthan other recent training-free diversity methods. We also demonstrate howSPELL can ensure a shielded generation away from a very large set of protectedimages by considering all 1.2M images from ImageNet as the protected set.</description><author>Michael Kirchhof, James Thornton, Pierre Ablin, Louis Béthune, Eugene Ndiaye, Marco Cuturi</author><pubDate>Thu, 10 Oct 2024 17:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06025v2</guid></item><item><title>Efficient Dictionary Learning with Switch Sparse Autoencoders</title><link>http://arxiv.org/abs/2410.08201v1</link><description>Sparse autoencoders (SAEs) are a recent technique for decomposing neuralnetwork activations into human-interpretable features. However, in order forSAEs to identify all features represented in frontier models, it will benecessary to scale them up to very high width, posing a computationalchallenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAEarchitecture aimed at reducing the compute cost of training SAEs. Inspired bysparse mixture of experts models, Switch SAEs route activation vectors betweensmaller "expert" SAEs, enabling SAEs to efficiently scale to many morefeatures. We present experiments comparing Switch SAEs with other SAEarchitectures, and find that Switch SAEs deliver a substantial Paretoimprovement in the reconstruction vs. sparsity frontier for a given fixedtraining compute budget. We also study the geometry of features across experts,analyze features duplicated across experts, and verify that Switch SAE featuresare as interpretable as features found by other SAE architectures.</description><author>Anish Mudide, Joshua Engels, Eric J. Michaud, Max Tegmark, Christian Schroeder de Witt</author><pubDate>Thu, 10 Oct 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08201v1</guid></item><item><title>Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity</title><link>http://arxiv.org/abs/2410.08198v1</link><description>Adam outperforms SGD when training language models. Yet this advantage is notwell-understood theoretically -- previous convergence analysis for Adam and SGDmainly focuses on the number of steps $T$ and is already minimax-optimal innon-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In this work, weargue that the exploitation of nice $\ell_\infty$-geometry is the key advantageof Adam over SGD. More specifically, we give a new convergence analysis forAdam under novel assumptions that loss is smooth under $\ell_\infty$-geometryrather than the more common $\ell_2$-geometry, which yields a much betterempirical smoothness constant for GPT-2 and ResNet models. Our experimentsconfirm that Adam performs much worse when the favorable $\ell_\infty$-geometryis changed while SGD provably remains unaffected. We also extend theconvergence analysis to blockwise Adam under novel blockwise smoothnessassumptions.</description><author>Shuo Xie, Mohamad Amin Mohamadi, Zhiyuan Li</author><pubDate>Thu, 10 Oct 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08198v1</guid></item><item><title>Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models</title><link>http://arxiv.org/abs/2404.07983v2</link><description>Contrastive vision-language models (VLMs), like CLIP, have gained popularityfor their versatile applicability to various downstream tasks. Despite theirsuccesses in some tasks, like zero-shot object recognition, they performsurprisingly poor on other tasks, like attribute recognition. Previous work hasattributed these challenges to the modality gap, a separation of image and textin the shared representation space, and to a bias towards objects over otherfactors, such as attributes. In this analysis paper, we investigate bothphenomena thoroughly. We evaluated off-the-shelf VLMs and find that while thegap's influence on performance is typically overshadowed by other factors, wefind indications that closing the gap indeed leads to improvements. Moreover,we find that, contrary to intuition, only few embedding dimensions drive thegap and that the embedding spaces are differently organized. To allow for aclean study of object bias, we introduce a definition and a correspondingmeasure of it. Equipped with this tool, we find that object bias does not leadto worse performance on other concepts, such as attributes per se. However, whydo both phenomena, modality gap and object bias, emerge in the first place? Toanswer this fundamental question and uncover some of the inner workings ofcontrastive VLMs, we conducted experiments that allowed us to control theamount of shared information between the modalities. These experiments revealedthat the driving factor behind both the modality gap and the object bias, is aninformation imbalance between images and captions, and unveiled an intriguingconnection between the modality gap and entropy of the logits.</description><author>Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, Thomas Brox</author><pubDate>Thu, 10 Oct 2024 17:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07983v2</guid></item><item><title>From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</title><link>http://arxiv.org/abs/2410.08197v1</link><description>Tool learning enables Large Language Models (LLMs) to interact with externalenvironments by invoking tools, serving as an effective strategy to mitigatethe limitations inherent in their pre-training data. In this process, tooldocumentation plays a crucial role by providing usage instructions for LLMs,thereby facilitating effective tool utilization. This paper concentrates on thecritical challenge of bridging the comprehension gap between LLMs and externaltools due to the inadequacies and inaccuracies inherent in existinghuman-centric tool documentation. We propose a novel framework, DRAFT, aimed atDynamically Refining tool documentation through the Analysis of Feedback andTrails emanating from LLMs' interactions with external tools. This methodologypivots on an innovative trial-and-error approach, consisting of three distinctlearning phases: experience gathering, learning from experience, anddocumentation rewriting, to iteratively enhance the tool documentation. Thisprocess is further optimized by implementing a diversity-promoting explorationstrategy to ensure explorative diversity and a tool-adaptive terminationmechanism to prevent overfitting while enhancing efficiency. Extensiveexperiments on multiple datasets demonstrate that DRAFT's iterative,feedback-based refinement significantly ameliorates documentation quality,fostering a deeper comprehension and more effective utilization of tools byLLMs. Notably, our analysis reveals that the tool documentation refined via ourapproach demonstrates robust cross-model generalization capabilities.</description><author>Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen</author><pubDate>Thu, 10 Oct 2024 17:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08197v1</guid></item><item><title>MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</title><link>http://arxiv.org/abs/2410.08196v1</link><description>Code has been shown to be effective in enhancing the mathematical reasoningabilities of large language models due to its precision and accuracy. Previousworks involving continued mathematical pretraining often include code thatutilizes math-related packages, which are primarily designed for fields such asengineering, machine learning, signal processing, or module testing, ratherthan being directly focused on mathematical reasoning. In this paper, weintroduce a novel method for generating mathematical code accompanied withcorresponding reasoning steps for continued pretraining. Our approach beginswith the construction of a high-quality mathematical continued pretrainingdataset by incorporating math-related web data, code using mathematicalpackages, math textbooks, and synthetic data. Next, we construct reasoningsteps by extracting LaTeX expressions, the conditions needed for theexpressions, and the results of the expressions from the previously collecteddataset. Based on this extracted information, we generate corresponding code toaccurately capture the mathematical reasoning process. Appending the generatedcode to each reasoning step results in data consisting of paired naturallanguage reasoning steps and their corresponding code. Combining this data withthe original dataset results in a 19.2B-token high-performing mathematicalpretraining corpus, which we name MathCode-Pile. Training several popular basemodels with this corpus significantly improves their mathematical abilities,leading to the creation of the MathCoder2 family of models. All of our dataprocessing and training code is open-sourced, ensuring full transparency andeasy reproducibility of the entire data collection and training pipeline. Thecode is released at https://github.com/mathllm/MathCoder2 .</description><author>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li</author><pubDate>Thu, 10 Oct 2024 17:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08196v1</guid></item><item><title>Features are fate: a theory of transfer learning in high-dimensional regression</title><link>http://arxiv.org/abs/2410.08194v1</link><description>With the emergence of large-scale pre-trained neural networks, methods toadapt such "foundation" models to data-limited downstream tasks have become anecessity. Fine-tuning, preference optimization, and transfer learning have allbeen successfully employed for these purposes when the target task closelyresembles the source task, but a precise theoretical understanding of "tasksimilarity" is still lacking. While conventional wisdom suggests that simplemeasures of similarity between source and target distributions, such as$\phi$-divergences or integral probability metrics, can directly predict thesuccess of transfer, we prove the surprising fact that, in general, this is notthe case. We adopt, instead, a feature-centric viewpoint on transfer learningand establish a number of theoretical results that demonstrate that when thetarget task is well represented by the feature space of the pre-trained model,transfer learning outperforms training from scratch. We study deep linearnetworks as a minimal model of transfer learning in which we can analyticallycharacterize the transferability phase diagram as a function of the targetdataset size and the feature space overlap. For this model, we establishrigorously that when the feature space overlap between the source and targettasks is sufficiently strong, both linear transfer and fine-tuning improveperformance, especially in the low data limit. These results build on anemerging understanding of feature learning dynamics in deep linear networks,and we demonstrate numerically that the rigorous results we derive for thelinear case also apply to nonlinear networks.</description><author>Javan Tahir, Surya Ganguli, Grant M. Rotskoff</author><pubDate>Thu, 10 Oct 2024 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08194v1</guid></item><item><title>GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment</title><link>http://arxiv.org/abs/2410.08193v1</link><description>Large Language Models (LLMs) exhibit impressive capabilities but requirecareful alignment with human preferences. Traditional training-time methodsfinetune LLMs using human preference datasets but incur significant trainingcosts and require repeated training to handle diverse user preferences.Test-time alignment methods address this by using reward models (RMs) to guidefrozen LLMs without retraining. However, existing test-time approaches rely ontrajectory-level RMs which are designed to evaluate complete responses, makingthem unsuitable for autoregressive text generation that requires computingnext-token rewards from partial responses. To address this, we introduceGenARM, a test-time alignment approach that leverages the Autoregressive RewardModel--a novel reward parametrization designed to predict next-token rewardsfor efficient and effective autoregressive generation. Theoretically, wedemonstrate that this parametrization can provably guide frozen LLMs toward anydistribution achievable by traditional RMs within the KL-regularizedreinforcement learning framework. Experimental results show that GenARMsignificantly outperforms prior test-time alignment baselines and matches theperformance of training-time methods. Additionally, GenARM enables efficientweak-to-strong guidance, aligning larger LLMs with smaller RMs without the highcosts of training larger models. Furthermore, GenARM supports multi-objectivealignment, allowing real-time trade-offs between preference dimensions andcatering to diverse user preferences without retraining.</description><author>Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh</author><pubDate>Thu, 10 Oct 2024 17:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08193v1</guid></item><item><title>HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation</title><link>http://arxiv.org/abs/2410.08192v1</link><description>Recent advancements in text-to-image diffusion models have shown remarkablecreative capabilities with textual prompts, but generating personalizedinstances based on specific subjects, known as subject-driven generation,remains challenging. To tackle this issue, we present a new hybrid frameworkcalled HybridBooth, which merges the benefits of optimization-based anddirect-regression methods. HybridBooth operates in two stages: the WordEmbedding Probe, which generates a robust initial word embedding using afine-tuned encoder, and the Word Embedding Refinement, which further adapts theencoder to specific subject images by optimizing key parameters. This approachallows for effective and fast inversion of visual concepts into textualembedding, even from a single image, while maintaining the model'sgeneralization capabilities.</description><author>Shanyan Guan, Yanhao Ge, Ying Tai, Jian Yang, Wei Li, Mingyu You</author><pubDate>Thu, 10 Oct 2024 17:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08192v1</guid></item><item><title>Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators</title><link>http://arxiv.org/abs/2402.13984v2</link><description>Machine learning force fields (MLFFs) are an attractive alternative toab-initio methods for molecular dynamics (MD) simulations. However, they canproduce unstable simulations, limiting their ability to model phenomenaoccurring over longer timescales and compromising the quality of estimatedobservables. To address these challenges, we present Stability-Aware BoltzmannEstimator (StABlE) Training, a multi-modal training procedure which leveragesjoint supervision from reference quantum-mechanical calculations and systemobservables. StABlE Training iteratively runs many MD simulations in parallelto seek out unstable regions, and corrects the instabilities via supervisionwith a reference observable. We achieve efficient end-to-end automaticdifferentiation through MD simulations using our Boltzmann Estimator, ageneralization of implicit differentiation techniques to a broader class ofstochastic algorithms. Unlike existing techniques based on active learning, ourapproach requires no additional ab-initio energy and forces calculations tocorrect instabilities. We demonstrate our methodology across organic molecules,tetrapeptides, and condensed phase systems, using three modern MLFFarchitectures. StABlE-trained models achieve significant improvements insimulation stability, data efficiency, and agreement with referenceobservables. By incorporating observables into the training process alongsidefirst-principles calculations, StABlE Training can be viewed as a generalsemi-empirical framework applicable across MLFF architectures and systems. Thismakes it a powerful tool for training stable and accurate MLFFs, particularlyin the absence of large reference datasets.</description><author>Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan</author><pubDate>Thu, 10 Oct 2024 17:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13984v2</guid></item><item><title>Poison-splat: Computation Cost Attack on 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2410.08190v1</link><description>3D Gaussian splatting (3DGS), known for its groundbreaking performance andefficiency, has become a dominant 3D representation and brought progress tomany 3D vision tasks. However, in this work, we reveal a significant securityvulnerability that has been largely overlooked in 3DGS: the computation cost oftraining 3DGS could be maliciously tampered by poisoning the input data. Bydeveloping an attack named Poison-splat, we reveal a novel attack surface wherethe adversary can poison the input images to drastically increase thecomputation memory and time needed for 3DGS training, pushing the algorithmtowards its worst computation complexity. In extreme cases, the attack can evenconsume all allocable memory, leading to a Denial-of-Service (DoS) thatdisrupts servers, resulting in practical damages to real-world 3DGS servicevendors. Such a computation cost attack is achieved by addressing a bi-leveloptimization problem through three tailored strategies: attack objectiveapproximation, proxy model rendering, and optional constrained optimization.These strategies not only ensure the effectiveness of our attack but also makeit difficult to defend with simple defensive measures. We hope the revelationof this novel attack surface can spark attention to this crucial yet overlookedvulnerability of 3DGS systems.</description><author>Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan</author><pubDate>Thu, 10 Oct 2024 17:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08190v1</guid></item><item><title>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</title><link>http://arxiv.org/abs/2410.08189v1</link><description>In this paper, we propose a new framework for zero-shot object navigation.Existing zero-shot object navigation methods prompt LLM with the text ofspatially closed objects, which lacks enough scene context for in-depthreasoning. To better preserve the information of environment and fully exploitthe reasoning ability of LLM, we propose to represent the observed scene with3D scene graph. The scene graph encodes the relationships between objects,groups and rooms with a LLM-friendly structure, for which we design ahierarchical chain-of-thought prompt to help LLM reason the goal locationaccording to scene context by traversing the nodes and edges. Moreover, benefitfrom the scene graph representation, we further design a re-perceptionmechanism to empower the object navigation framework with the ability tocorrect perception error. We conduct extensive experiments on MP3D, HM3D andRoboTHOR environments, where SG-Nav surpasses previous state-of-the-artzero-shot methods by more than 10% SR on all benchmarks, while the decisionprocess is explainable. To the best of our knowledge, SG-Nav is the firstzero-shot method that achieves even higher performance than supervised objectnavigation methods on the challenging MP3D benchmark.</description><author>Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 10 Oct 2024 17:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08189v1</guid></item><item><title>DifFRelight: Diffusion-Based Facial Performance Relighting</title><link>http://arxiv.org/abs/2410.08188v1</link><description>We present a novel framework for free-viewpoint facial performance relightingusing diffusion-based image-to-image translation. Leveraging a subject-specificdataset containing diverse facial expressions captured under various lightingconditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, wetrain a diffusion model for precise lighting control, enabling high-fidelityrelit facial images from flat-lit inputs. Our framework includesspatially-aligned conditioning of flat-lit captures and random noise, alongwith integrated lighting information for global control, utilizing priorknowledge from the pre-trained Stable Diffusion model. This model is thenapplied to dynamic facial performances captured in a consistent flat-litenvironment and reconstructed for novel-view synthesis using a scalable dynamic3D Gaussian Splatting method to maintain quality and consistency in the relitresults. In addition, we introduce unified lighting control by integrating anovel area lighting representation with directional lighting, allowing forjoint adjustments in light size and direction. We also enable high dynamicrange imaging (HDRI) composition using multiple directional lights to producedynamic sequences under complex lighting conditions. Our evaluationsdemonstrate the models efficiency in achieving precise lighting control andgeneralizing across various facial expressions while preserving detailedfeatures such as skintexture andhair. The model accurately reproduces complexlighting effects like eye reflections, subsurface scattering, self-shadowing,and translucency, advancing photorealism within our framework.</description><author>Mingming He, Pascal Clausen, Ahmet Levent Taşel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, Paul Debevec</author><pubDate>Thu, 10 Oct 2024 17:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08188v1</guid></item><item><title>Scaling Laws For Diffusion Transformers</title><link>http://arxiv.org/abs/2410.08184v1</link><description>Diffusion transformers (DiT) have already achieved appealing synthesis andscaling properties in content recreation, e.g., image and video generation.However, scaling laws of DiT are less explored, which usually offer precisepredictions regarding optimal model size and data requirements given a specificcompute budget. Therefore, experiments across a broad range of compute budgets,from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling lawsin DiT for the first time. Concretely, the loss of pretraining DiT also followsa power-law relationship with the involved compute. Based on the scaling law,we can not only determine the optimal model size and required data but alsoaccurately predict the text-to-image generation loss given a model with 1Bparameters and a compute budget of 1e21 FLOPs. Additionally, we alsodemonstrate that the trend of pre-training loss matches the generationperformances (e.g., FID), even across various datasets, which complements themapping from compute to synthesis quality and thus provides a predictablebenchmark that assesses model performance and data quality at a reduced cost.</description><author>Zhengyang Liang, Hao He, Ceyuan Yang, Bo Dai</author><pubDate>Thu, 10 Oct 2024 17:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08184v1</guid></item><item><title>MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models</title><link>http://arxiv.org/abs/2410.08182v1</link><description>Existing multimodal retrieval benchmarks primarily focus on evaluatingwhether models can retrieve and utilize external textual knowledge for questionanswering. However, there are scenarios where retrieving visual information iseither more beneficial or easier to access than textual data. In this paper, weintroduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, inwhich we systematically identify and categorize scenarios where visuallyaugmented knowledge is better than textual knowledge, for instance, more imagesfrom varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353human-annotated multiple-choice questions across 9 distinct scenarios. WithMRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary largevision-language models (LVLMs). Our results show that all LVLMs exhibit greaterimprovements when augmented with images compared to textual knowledge,confirming that MRAG-Bench is vision-centric. Additionally, we conductextensive analysis with MRAG-Bench, which offers valuable insights intoretrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faceschallenges in effectively leveraging retrieved knowledge, achieving only a5.82% improvement with ground-truth information, in contrast to a 33.16%improvement observed in human participants. These findings highlight theimportance of MRAG-Bench in encouraging the community to enhance LVLMs' abilityto utilize retrieved visual knowledge more effectively.</description><author>Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, Nanyun Peng</author><pubDate>Thu, 10 Oct 2024 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08182v1</guid></item><item><title>RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS Generative Model from a Single Image</title><link>http://arxiv.org/abs/2410.08181v1</link><description>The generation of high-quality 3D car assets is essential for variousapplications, including video games, autonomous driving, and virtual reality.Current 3D generation methods utilizing NeRF or 3D-GS as representations for 3Dobjects, generate a Lambertian object under fixed lighting and lack separatedmodelings for material and global illumination. As a result, the generatedassets are unsuitable for relighting under varying lighting conditions,limiting their applicability in downstream tasks. To address this challenge, wepropose a novel relightable 3D object generative framework that automates thecreation of 3D car assets, enabling the swift and accurate reconstruction of avehicle's geometry, texture, and material properties from a single input image.Our approach begins with introducing a large-scale synthetic car datasetcomprising over 1,000 high-precision 3D vehicle models. We represent 3D objectsusing global illumination and relightable 3D Gaussian primitives integratingwith BRDF parameters. Building on this representation, we introduce afeed-forward model that takes images as input and outputs both relightable 3DGaussians and global illumination parameters. Experimental results demonstratethat our method produces photorealistic 3D car assets that can be seamlesslyintegrated into road scenes with different illuminations, which offerssubstantial practical benefits for industrial applications.</description><author>Xiaoxue Chen, Jv Zheng, Hao Huang, Haoran Xu, Weihao Gu, Kangliang Chen, He xiang, Huan-ang Gao, Hao Zhao, Guyue Zhou, Yaqin Zhang</author><pubDate>Thu, 10 Oct 2024 17:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08181v1</guid></item><item><title>TANet: Triplet Attention Network for All-In-One Adverse Weather Image Restoration</title><link>http://arxiv.org/abs/2410.08177v1</link><description>Adverse weather image restoration aims to remove unwanted degraded artifacts,such as haze, rain, and snow, caused by adverse weather conditions. Existingmethods achieve remarkable results for addressing single-weather conditions.However, they face challenges when encountering unpredictable weatherconditions, which often happen in real-world scenarios. Although differentweather conditions exhibit different degradation patterns, they share commoncharacteristics that are highly related and complementary, such as occlusionscaused by degradation patterns, color distortion, and contrast attenuation dueto the scattering of atmospheric particles. Therefore, we focus on leveragingcommon knowledge across multiple weather conditions to restore images in aunified manner. In this paper, we propose a Triplet Attention Network (TANet)to efficiently and effectively address all-in-one adverse weather imagerestoration. TANet consists of Triplet Attention Block (TAB) that incorporatesthree types of attention mechanisms: Local Pixel-wise Attention (LPA) andGlobal Strip-wise Attention (GSA) to address occlusions caused by non-uniformdegradation patterns, and Global Distribution Attention (GDA) to address colordistortion and contrast attenuation caused by atmospheric phenomena. Byleveraging common knowledge shared across different weather conditions, TANetsuccessfully addresses multiple weather conditions in a unified manner.Experimental results show that TANet efficiently and effectively achievesstate-of-the-art performance in all-in-one adverse weather image restoration.The source code is available at https://github.com/xhuachris/TANet-ACCV-2024.</description><author>Hsing-Hua Wang, Fu-Jen Tsai, Yen-Yu Lin, Chia-Wen Lin</author><pubDate>Thu, 10 Oct 2024 17:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08177v1</guid></item><item><title>$\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases</title><link>http://arxiv.org/abs/2406.06887v3</link><description>Preference learning provides a promising solution to address the limitationsof supervised fine-tuning (SFT) for code language models, where the model isnot explicitly trained to differentiate between correct and incorrect code.Recent findings demonstrate that on-policy data is the key to successfulpreference learning, where the preference data is collected using the samepolicy LM being trained. Inspired by this, we propose PLUM, an on-policy$\textbf{P}$reference $\textbf{L}$earning framework A$\textbf{u}$gmented withtest cases for code L$\textbf{M}$ s. The framework operates in three keystages: (1) automatic generation of test cases from natural languageinstructions, (2) creation of a preference data by evaluating candidate codesolutions sampled from the policy, which can then be used to (3) train thepolicy LM. PLUM levitates the need to train reward models, allowing for largescale on-policy and online preference data collation. PLUM is evaluated on bothstandard benchmarks (HumanEval, MBPP) and more challenging ones(LiveCodeBench), delivering substantial improvements over original SFT'edmodels and other execution-feedback-driven approaches. We show PLUM's benefitsare consistent across various widely-used code LMs even they have beenwell-trained with SFT. For example, PLUM increases pass rates by up to 4.8% onaverage on standard benchmarks and 11.8% on LiveCodeBench, demonstrating itseffectiveness and generalizability. We also demonstrate the benefits ofon-policy and online preference learning by comprehensive experimentation.</description><author>Dylan Zhang, Shizhe Diao, Xueyan Zou, Hao Peng</author><pubDate>Thu, 10 Oct 2024 17:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06887v3</guid></item><item><title>Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models</title><link>http://arxiv.org/abs/2410.08174v1</link><description>Multimodal Large Language Models (MLLMs) exhibit promising advancementsacross various tasks, yet they still encounter significant trustworthinessissues. Prior studies apply Split Conformal Prediction (SCP) in languagemodeling to construct prediction sets with statistical guarantees. However,these methods typically rely on internal model logits or are restricted tomultiple-choice settings, which hampers their generalizability and adaptabilityin dynamic, open-ended environments. In this paper, we introduce TRON, atwo-step framework for risk control and assessment, applicable to any MLLM thatsupports sampling in both open-ended and closed-ended scenarios. TRON comprisestwo main components: (1) a novel conformal score to sample response sets ofminimum size, and (2) a nonconformity score to identify high-quality responsesbased on self-consistency theory, controlling the error rates by two specificrisk levels. Furthermore, we investigate semantic redundancy in prediction setswithin open-ended contexts for the first time, leading to a promisingevaluation metric for MLLMs based on average set size. Our comprehensiveexperiments across four Video Question-Answering (VideoQA) datasets utilizingeight MLLMs show that TRON achieves desired error rates bounded by twouser-specified risk levels. Additionally, deduplicated prediction sets maintainadaptiveness while being more efficient and stable for risk assessment underdifferent risk levels.</description><author>Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng</author><pubDate>Thu, 10 Oct 2024 17:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08174v1</guid></item><item><title>On the Evaluation of Generative Robotic Simulations</title><link>http://arxiv.org/abs/2410.08172v1</link><description>Due to the difficulty of acquiring extensive real-world data, robotsimulation has become crucial for parallel training and sim-to-real transfer,highlighting the importance of scalable simulated robotic tasks. Foundationmodels have demonstrated impressive capacities in autonomously generatingfeasible robotic tasks. However, this new paradigm underscores the challenge ofadequately evaluating these autonomously generated tasks. To address this, wepropose a comprehensive evaluation framework tailored to generativesimulations. Our framework segments evaluation into three core aspects:quality, diversity, and generalization. For single-task quality, we evaluatethe realism of the generated task and the completeness of the generatedtrajectories using large language models and vision-language models. In termsof diversity, we measure both task and data diversity through text similarityof task descriptions and world model loss trained on collected tasktrajectories. For task-level generalization, we assess the zero-shotgeneralization ability on unseen tasks of a policy trained with multiplegenerated tasks. Experiments conducted on three representative task generationpipelines demonstrate that the results from our framework are highly consistentwith human evaluations, confirming the feasibility and validity of ourapproach. The findings reveal that while metrics of quality and diversity canbe achieved through certain methods, no single approach excels across allmetrics, suggesting a need for greater focus on balancing these differentmetrics. Additionally, our analysis further highlights the common challenge oflow generalization capability faced by current works. Our anonymous website:https://sites.google.com/view/evaltasks.</description><author>Feng Chen, Botian Xu, Pu Hua, Peiqi Duan, Yanchao Yang, Yi Ma, Huazhe Xu</author><pubDate>Thu, 10 Oct 2024 17:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08172v1</guid></item><item><title>ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion</title><link>http://arxiv.org/abs/2410.08168v1</link><description>We present ZeroComp, an effective zero-shot 3D object compositing approachthat does not require paired composite-scene images during training. Our methodleverages ControlNet to condition from intrinsic images and combines it with aStable Diffusion model to utilize its scene priors, together operating as aneffective rendering engine. During training, ZeroComp uses intrinsic imagesbased on geometry, albedo, and masked shading, all without the need for pairedimages of scenes with and without composite objects. Once trained, itseamlessly integrates virtual 3D objects into scenes, adjusting shading tocreate realistic composites. We developed a high-quality evaluation dataset anddemonstrate that ZeroComp outperforms methods using explicit lightingestimations and generative techniques in quantitative and human perceptionbenchmarks. Additionally, ZeroComp extends to real and outdoor imagecompositing, even when trained solely on synthetic indoor data, showcasing itseffectiveness in image compositing.</description><author>Zitian Zhang, Frédéric Fortier-Chouinard, Mathieu Garon, Anand Bhattad, Jean-François Lalonde</author><pubDate>Thu, 10 Oct 2024 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08168v1</guid></item><item><title>Visual Scratchpads: Enabling Global Reasoning in Vision</title><link>http://arxiv.org/abs/2410.08165v1</link><description>Modern vision models have achieved remarkable success in benchmarks wherelocal features provide critical information about the target. There is now agrowing interest in solving tasks that require more global reasoning, wherelocal features offer no significant information. These tasks are reminiscent ofthe connectivity tasks discussed by Minsky and Papert in 1969, which exposedthe limitations of the perceptron model and contributed to the first AI winter.In this paper, we revisit such tasks by introducing four global visualbenchmarks involving path findings and mazes. We show that: (1) althoughtoday's large vision models largely surpass the expressivity limitations of theearly models, they still struggle with the learning efficiency; we put forwardthe "globality degree" notion to understand this limitation; (2) we thendemonstrate that the picture changes and global reasoning becomes feasible withthe introduction of "visual scratchpads"; similarly to the text scratchpads andchain-of-thoughts used in language models, visual scratchpads help break downglobal tasks into simpler ones; (3) we finally show that some scratchpads arebetter than others, in particular, "inductive scratchpads" that take stepsrelying on less information afford better out-of-distribution generalizationand succeed for smaller model sizes.</description><author>Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe</author><pubDate>Thu, 10 Oct 2024 17:44:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08165v1</guid></item><item><title>Asynchronous Graph Generator</title><link>http://arxiv.org/abs/2309.17335v3</link><description>We introduce the asynchronous graph generator (AGG), a novel graph attentionnetwork for imputation and prediction of multi-channel time series. Free fromrecurrent components or assumptions about temporal/spatial regularity, AGGencodes measurements, timestamps and channel-specific features directly in thenodes via learnable embeddings. Through an attention mechanism, theseembeddings allow for discovering expressive relationships among the variablesof interest in the form of a homogeneous graph. Once trained, AGG performsimputation by \emph{conditional attention generation}, i.e., by creating a newnode conditioned on given timestamps and channel specification. The proposedAGG is compared to related methods in the literature and its performance isanalysed from a data augmentation perspective. Our experiments reveal that AGGachieved state-of-the-art results in time series imputation, classification andprediction for the benchmark datasets \emph{Beijing Air Quality},\emph{PhysioNet ICU 2012} and \emph{UCI localisation}, outperforming otherrecent attention-based networks.</description><author>Christopher P. Ley, Felipe Tobar</author><pubDate>Thu, 10 Oct 2024 17:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17335v3</guid></item><item><title>Agent S: An Open Agentic Framework that Uses Computers Like a Human</title><link>http://arxiv.org/abs/2410.08164v1</link><description>We present Agent S, an open agentic framework that enables autonomousinteraction with computers through a Graphical User Interface (GUI), aimed attransforming human-computer interaction by automating complex, multi-steptasks. Agent S aims to address three key challenges in automating computertasks: acquiring domain-specific knowledge, planning over long task horizons,and handling dynamic, non-uniform interfaces. To this end, Agent S introducesexperience-augmented hierarchical planning, which learns from externalknowledge search and internal experience retrieval at multiple levels,facilitating efficient task planning and subtask execution. In addition, itemploys an Agent-Computer Interface (ACI) to better elicit the reasoning andcontrol capabilities of GUI agents based on Multimodal Large Language Models(MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms thebaseline by 9.37% on success rate (an 83.6% relative improvement) and achievesa new state-of-the-art. Comprehensive analysis highlights the effectiveness ofindividual components and provides insights for future improvements.Furthermore, Agent S demonstrates broad generalizability to different operatingsystems on a newly-released WindowsAgentArena benchmark. Code available athttps://github.com/simular-ai/Agent-S.</description><author>Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric Wang</author><pubDate>Thu, 10 Oct 2024 17:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08164v1</guid></item><item><title>The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading</title><link>http://arxiv.org/abs/2410.08162v1</link><description>The effect of surprisal on processing difficulty has been a central topic ofinvestigation in psycholinguistics. Here, we use eyetracking data to examinethree language processing regimes that are common in daily life but have notbeen addressed with respect to this question: information seeking, repeatedprocessing, and the combination of the two. Using standard regime-agnosticsurprisal estimates we find that the prediction of surprisal theory regardingthe presence of a linear effect of surprisal on processing times, extends tothese regimes. However, when using surprisal estimates from regime-specificcontexts that match the contexts and tasks given to humans, we find that ininformation seeking, such estimates do not improve the predictive power ofprocessing times compared to standard surprisals. Further, regime-specificcontexts yield near zero surprisal estimates with no predictive power forprocessing times in repeated reading. These findings point to misalignments oftask and memory representations between humans and current language models, andquestion the extent to which such models can be used for estimating cognitivelyrelevant quantities. We further discuss theoretical challenges posed by theseresults.</description><author>Keren Gruteke Klein, Yoav Meiri, Omer Shubi, Yevgeni Berzak</author><pubDate>Thu, 10 Oct 2024 17:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08162v1</guid></item><item><title>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation</title><link>http://arxiv.org/abs/2410.08159v1</link><description>Diffusion models have become the dominant approach for visual generation.They are trained by denoising a Markovian process that gradually adds noise tothe input. We argue that the Markovian property limits the models ability tofully utilize the generation trajectory, leading to inefficiencies duringtraining and inference. In this paper, we propose DART, a transformer-basedmodel that unifies autoregressive (AR) and diffusion within a non-Markovianframework. DART iteratively denoises image patches spatially and spectrallyusing an AR model with the same architecture as standard language models. DARTdoes not rely on image quantization, enabling more effective image modelingwhile maintaining flexibility. Furthermore, DART seamlessly trains with bothtext and image data in a unified model. Our approach demonstrates competitiveperformance on class-conditioned and text-to-image generation tasks, offering ascalable, efficient alternative to traditional diffusion models. Through thisunified framework, DART sets a new benchmark for scalable, high-quality imagesynthesis.</description><author>Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai</author><pubDate>Thu, 10 Oct 2024 17:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08159v1</guid></item><item><title>RayEmb: Arbitrary Landmark Detection in X-Ray Images Using Ray Embedding Subspace</title><link>http://arxiv.org/abs/2410.08152v1</link><description>Intra-operative 2D-3D registration of X-ray images with pre-operativelyacquired CT scans is a crucial procedure in orthopedic surgeries. Anatomicallandmarks pre-annotated in the CT volume can be detected in X-ray images toestablish 2D-3D correspondences, which are then utilized for registration.However, registration often fails in certain view angles due to poor landmarkvisibility. We propose a novel method to address this issue by detectingarbitrary landmark points in X-ray images. Our approach represents 3D points asdistinct subspaces, formed by feature vectors (referred to as ray embeddings)corresponding to intersecting rays. Establishing 2D-3D correspondences thenbecomes a task of finding ray embeddings that are close to a given subspace,essentially performing an intersection test. Unlike conventional methods forlandmark estimation, our approach eliminates the need for manually annotatingfixed landmarks. We trained our model using the synthetic images generated fromCTPelvic1K CLINIC dataset, which contains 103 CT volumes, and evaluated it onthe DeepFluoro dataset, comprising real X-ray images. Experimental resultsdemonstrate the superiority of our method over conventional methods. The codeis available at https://github.com/Pragyanstha/rayemb.</description><author>Pragyan Shrestha, Chun Xie, Yuichi Yoshii, Itaru Kitahara</author><pubDate>Thu, 10 Oct 2024 17:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08152v1</guid></item><item><title>Progressive Autoregressive Video Diffusion Models</title><link>http://arxiv.org/abs/2410.08151v1</link><description>Current frontier video diffusion models have demonstrated remarkable resultsat generating high-quality videos. However, they can only generate short videoclips, normally around 10 seconds or 240 frames, due to computation limitationsduring training. In this work, we show that existing models can be naturallyextended to autoregressive video diffusion models without changing thearchitectures. Our key idea is to assign the latent frames with progressivelyincreasing noise levels rather than a single noise level, which allows forfine-grained condition among the latents and large overlaps between theattention windows. Such progressive video denoising allows our models toautoregressively generate video frames without quality degradation or abruptscene changes. We present state-of-the-art results on long video generation at1 minute (1440 frames at 24 FPS). Videos from this paper are available athttps://desaixie.github.io/pa-vdm/.</description><author>Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, Yang Zhou</author><pubDate>Thu, 10 Oct 2024 17:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08151v1</guid></item><item><title>OpenDAS: Open-Vocabulary Domain Adaptation for Segmentation</title><link>http://arxiv.org/abs/2405.20141v2</link><description>Recently, Vision-Language Models (VLMs) have advanced segmentation techniquesby shifting from the traditional segmentation of a closed-set of predefinedobject classes to open-vocabulary segmentation (OVS), allowing users to segmentnovel classes and concepts unseen during training of the segmentation model.However, this flexibility comes with a trade-off: fully-supervised closed-setmethods still outperform OVS methods on base classes, that is on classes onwhich they have been explicitly trained. This is due to the lack ofpixel-aligned training masks for VLMs (which are trained on image-captionpairs), and the absence of domain-specific knowledge, such as autonomousdriving. Therefore, we propose the task of open-vocabulary domain adaptation toinfuse domain-specific knowledge into VLMs while preserving theiropen-vocabulary nature. By doing so, we achieve improved performance in baseand novel classes. Existing VLM adaptation methods improve performance on base(training) queries, but fail to fully preserve the open-set capabilities ofVLMs on novel queries. To address this shortcoming, we combineparameter-efficient prompt tuning with a triplet-loss-based training strategythat uses auxiliary negative queries. Notably, our approach is the onlyparameter-efficient method that consistently surpasses the original VLM onnovel classes. Our adapted VLMs can seamlessly be integrated into existing OVSpipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2Dsegmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices foropen-vocabulary 3D instance segmentation without other changes.</description><author>Gonca Yilmaz, Songyou Peng, Marc Pollefeys, Francis Engelmann, Hermann Blum</author><pubDate>Thu, 10 Oct 2024 17:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20141v2</guid></item><item><title>Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</title><link>http://arxiv.org/abs/2410.08146v1</link><description>A promising approach for improving reasoning in large language models is touse process reward models (PRMs). PRMs provide feedback at each step of amulti-step reasoning trace, potentially improving credit assignment overoutcome reward models (ORMs) that only provide feedback at the final step.However, collecting dense, per-step human labels is not scalable, and trainingPRMs from automatically-labeled data has thus far led to limited gains. Toimprove a base policy by running search against a PRM or using it as denserewards for reinforcement learning (RL), we ask: "How should we design processrewards?". Our key insight is that, to be effective, the process reward for astep should measure progress: a change in the likelihood of producing a correctresponse in the future, before and after taking the step, corresponding to thenotion of step-level advantages in RL. Crucially, this progress should bemeasured under a prover policy distinct from the base policy. We theoreticallycharacterize the set of good provers and our results show that optimizingprocess rewards from such provers improves exploration during test-time searchand online RL. In fact, our characterization shows that weak prover policiescan substantially improve a stronger base policy, which we also observeempirically. We validate our claims by training process advantage verifiers(PAVs) to predict progress under such provers, and show that compared to ORMs,test-time search against PAVs is $&gt;8\%$ more accurate, and $1.5-5\times$ morecompute-efficient. Online RL with dense rewards from PAVs enables one of thefirst results with $5-6\times$ gain in sample efficiency, and $&gt;6\%$ gain inaccuracy, over ORMs.</description><author>Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar</author><pubDate>Thu, 10 Oct 2024 17:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08146v1</guid></item><item><title>Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs</title><link>http://arxiv.org/abs/2410.08145v1</link><description>This paper explores the problem of commonsense-level vision-knowledgeconflict in Multimodal Large Language Models (MLLMs), where visual informationcontradicts model's internal commonsense knowledge (see Figure 1). To studythis issue, we introduce an automated pipeline, augmented withhuman-in-the-loop quality control, to establish a benchmark aimed at simulatingand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafteda diagnostic benchmark comprising 374 original images and 1,122 high-qualityquestion-answer (QA) pairs. This benchmark covers two types of conflict targetand three question difficulty levels, providing a thorough assessment tool.Through this benchmark, we evaluate the conflict-resolution capabilities ofnine representative MLLMs across various model families and find a noticeableover-reliance on textual queries. Drawing on these findings, we propose a novelprompting strategy, "Focus-on-Vision" (FoV), which markedly enhances MLLMs'ability to favor visual data over conflicting textual knowledge. Our detailedanalysis and the newly proposed strategy significantly advance theunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The dataand code are made publicly available.</description><author>Xiaoyuan Liu, Wenxuan Wang, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Pinjia He, Zhaopeng Tu</author><pubDate>Thu, 10 Oct 2024 17:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08145v1</guid></item><item><title>DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory</title><link>http://arxiv.org/abs/2410.08143v1</link><description>Large language models (LLMs) have achieved reasonable quality improvements inmachine translation (MT). However, most current research on MT-LLMs still facessignificant challenges in maintaining translation consistency and accuracy whenprocessing entire documents. In this paper, we introduce DelTA, aDocument-levEL Translation Agent designed to overcome these limitations. DelTAfeatures a multi-level memory structure that stores information across variousgranularities and spans, including Proper Noun Records, Bilingual Summary,Long-Term Memory, and Short-Term Memory, which are continuously retrieved andupdated by auxiliary LLM-based components. Experimental results indicate thatDelTA significantly outperforms strong baselines in terms of translationconsistency and quality across four open/closed-source LLMs and tworepresentative document translation datasets, achieving an increase inconsistency scores by up to 4.58 percentage points and in COMET scores by up to3.16 points on average. DelTA employs a sentence-by-sentence translationstrategy, ensuring no sentence omissions and offering a memory-efficientsolution compared to the mainstream method. Furthermore, DelTA improves pronountranslation accuracy, and the summary component of the agent also shows promiseas a tool for query-based summarization tasks. We release our code and data athttps://github.com/YutongWang1216/DocMTAgent.</description><author>Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang</author><pubDate>Thu, 10 Oct 2024 17:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08143v1</guid></item><item><title>Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</title><link>http://arxiv.org/abs/2410.04587v2</link><description>Large language models have demonstrated impressive value in performing asautonomous agents when equipped with external tools and API calls. Nonetheless,effectively harnessing their potential for executing complex tasks cruciallyrelies on enhancements in their function calling capabilities. This paperidentifies a critical gap in existing function calling models, whereperformance varies significantly across benchmarks, often due to being misledby specific naming conventions. To address such an issue, we introduce Hammer,a novel family of foundation models specifically engineered for on-devicefunction calling. Hammer employs an augmented dataset that enhances models'sensitivity to irrelevant functions and incorporates function maskingtechniques to minimize misleading. Our empirical evaluations reveal that Hammernot only outperforms larger models but also demonstrates robust generalizationacross diverse benchmarks, achieving sota results. Our open sourcecontributions include a specialized dataset for irrelevance detection, a tuningframework for enhanced generalization, and the Hammer models, establishing anew standard for function calling performance.</description><author>Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang</author><pubDate>Thu, 10 Oct 2024 17:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04587v2</guid></item><item><title>PaliGemma: A versatile 3B VLM for transfer</title><link>http://arxiv.org/abs/2407.07726v2</link><description>PaliGemma is an open Vision-Language Model (VLM) that is based on theSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained tobe a versatile and broadly knowledgeable base model that is effective totransfer. It achieves strong performance on a wide variety of open-world tasks.We evaluate PaliGemma on almost 40 diverse tasks including standard VLMbenchmarks, but also more specialized tasks such as remote-sensing andsegmentation.</description><author>Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiaohua Zhai</author><pubDate>Thu, 10 Oct 2024 17:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07726v2</guid></item><item><title>Theia: Distilling Diverse Vision Foundation Models for Robot Learning</title><link>http://arxiv.org/abs/2407.20179v2</link><description>Vision-based robot policy learning, which maps visual inputs to actions,necessitates a holistic understanding of diverse visual tasks beyondsingle-task needs like classification or segmentation. Inspired by this, weintroduce Theia, a vision foundation model for robot learning that distillsmultiple off-the-shelf vision foundation models trained on varied vision tasks.Theia's rich visual representations encode diverse visual knowledge, enhancingdownstream robot learning. Extensive experiments demonstrate that Theiaoutperforms its teacher models and prior robot learning models using lesstraining data and smaller model sizes. Additionally, we quantify the quality ofpre-trained visual representations and hypothesize that higher entropy infeature norm distributions leads to improved robot learning performance. Code,models, and demo are available at https://theia.theaiinstitute.com.</description><author>Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, Laura Herlant</author><pubDate>Thu, 10 Oct 2024 17:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20179v2</guid></item><item><title>6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering</title><link>http://arxiv.org/abs/2410.04974v2</link><description>Novel view synthesis has advanced significantly with the development ofneural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,achieving high quality without compromising real-time rendering remainschallenging, particularly for physically-based ray tracing with view-dependenteffects. Recently, N-dimensional Gaussians (N-DG) introduced a 6Dspatial-angular representation to better incorporate view-dependent effects,but the Gaussian representation and control scheme are sub-optimal. In thispaper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),which enhances color and opacity representations and leverages the additionaldirectional information in the 6D space for optimized Gaussian control. Ourapproach is fully compatible with the 3DGS framework and significantly improvesreal-time radiance field rendering by better modeling view-dependent effectsand fine details. Experiments demonstrate that 6DGS significantly outperforms3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reductionof 66.5% Gaussian points compared to 3DGS. The project page is:https://gaozhongpai.github.io/6dgs/</description><author>Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu</author><pubDate>Thu, 10 Oct 2024 17:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04974v2</guid></item><item><title>Mini-batch Coresets for Memory-efficient Training of Large Language Models</title><link>http://arxiv.org/abs/2407.19580v2</link><description>Training with larger mini-batches improves the convergence rate and can yieldsuperior performance. However, training with large mini-batches becomesprohibitive for Large Language Models (LLMs), due to the large GPU memoryrequirement. To address this problem, an effective approach is finding smallmini-batch coresets that closely match the gradient of larger mini-batches.However, this approach becomes infeasible and ineffective for LLMs, due to thehighly imbalanced nature of the sources in language data, use of the Adamoptimizer, and the very large gradient dimensionality of LLMs. In this work, weaddress the above challenges by proposing Coresets for Training LLMs (CoLM).First, we show that mini-batch coresets found by gradient matching do notcontain representative examples of the small sources w.h.p., and thus includingall examples of the small sources in the mini-batch coresets is crucial foroptimal performance. Second, we normalize the gradients by their historicalexponential to find mini-batch coresets for training with Adam. Finally, weleverage zeroth-order methods to find smooth gradient of the last V -projectionmatrix and sparsify it to keep the dimensions with the largest normalizedgradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr withLoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces thememory requirement of fine-tuning by 2x and even outperforms training with 4xlarger mini-batches. Notably, CoLM easily stack with existing memory-efficienttraining methods, such as LoRA.</description><author>Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman</author><pubDate>Thu, 10 Oct 2024 17:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19580v2</guid></item><item><title>AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models</title><link>http://arxiv.org/abs/2403.00953v2</link><description>Rare diseases affect millions worldwide but often face limited research focusdue to their low prevalence. This results in prolonged diagnoses and a lack ofapproved therapies. Recent advancements in Large Language Models (LLMs) haveshown promise in automating the extraction of medical information, offeringpotential to improve medical diagnosis and management. However, most LLMs lackprofessional medical knowledge, especially concerning rare diseases, andstruggle to handle the latest rare disease information. They also cannoteffectively manage rare disease data and are not directly suitable fordiagnosis and management tasks. Our objective is to create an end-to-end systemcalled AutoRD, which automates the extraction of information from medical textsabout rare diseases, focusing on entities and their relations. AutoRDintegrates up-to-date structured knowledge and demonstrates superiorperformance in rare disease extraction tasks. We conduct various experiments toevaluate AutoRD's performance, aiming to surpass common LLMs and traditionalmethods.</description><author>Lang Cao, Jimeng Sun, Adam Cross</author><pubDate>Thu, 10 Oct 2024 17:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00953v2</guid></item><item><title>Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction</title><link>http://arxiv.org/abs/2410.08134v1</link><description>Generative modeling of discrete data underlies important applicationsspanning text-based agents like ChatGPT to the design of the very buildingblocks of life in protein sequences. However, application domains need to exertcontrol over the generated data by steering the generative process - typicallyvia RLHF - to satisfy a specified property, reward, or affinity metric. In thispaper, we study the problem of steering Masked Diffusion Models (MDMs), arecent class of discrete diffusion models that offer a compelling alternativeto traditional autoregressive models. We introduce Discrete Denoising PosteriorPrediction (DDPP), a novel framework that casts the task of steeringpre-trained MDMs as a problem of probabilistic inference by learning to samplefrom a target Bayesian posterior. Our DDPP framework leads to a family of threenovel objectives that are all simulation-free, and thus scalable while applyingto general non-differentiable reward functions. Empirically, we instantiateDDPP by steering MDMs to perform class-conditional pixel-level image modeling,RLHF-based alignment of MDMs using text-based rewards, and finetuning proteinlanguage models to generate more diverse secondary structures and shorterproteins. We substantiate our designs via wet-lab validation, where we observetransient expression of reward-optimized protein sequences.</description><author>Jarrid Rector-Brooks, Mohsin Hasan, Zhangzhi Peng, Zachary Quinn, Chenghao Liu, Sarthak Mittal, Nouha Dziri, Michael Bronstein, Yoshua Bengio, Pranam Chatterjee, Alexander Tong, Avishek Joey Bose</author><pubDate>Thu, 10 Oct 2024 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08134v1</guid></item><item><title>Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks</title><link>http://arxiv.org/abs/2410.08133v1</link><description>Current LLM benchmarks focus on evaluating models' memory of facts andsemantic relations, primarily assessing semantic aspects of long-term memory.However, in humans, long-term memory also includes episodic memory, which linksmemories to their contexts, such as the time and place they occurred. Theability to contextualize memories is crucial for many cognitive tasks andeveryday functions. This form of memory has not been evaluated in LLMs withexisting benchmarks. To address the gap in evaluating memory in LLMs, weintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used tostudy episodic memory in cognitive psychology. SORT requires LLMs to recall thecorrect order of text segments, and provides a general framework that is botheasily extendable and does not require any additional annotations. We presentan initial evaluation dataset, Book-SORT, comprising 36k pairs of segmentsextracted from 9 books recently added to the public domain. Based on a humanexperiment with 155 participants, we show that humans can recall sequence orderbased on long-term memory of a book. We find that models can perform the taskwith high accuracy when relevant text is given in-context during the SORTevaluation. However, when presented with the book text only during training,LLMs' performance on SORT falls short. By allowing to evaluate more aspects ofmemory, we believe that SORT will aid in the emerging development ofmemory-augmented models.</description><author>Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman, Sebastian Michelmann, Alexander Huth, Mariya Toneva</author><pubDate>Thu, 10 Oct 2024 17:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08133v1</guid></item><item><title>Control, Transport and Sampling: Towards Better Loss Design</title><link>http://arxiv.org/abs/2405.13731v2</link><description>Leveraging connections between diffusion-based sampling, optimal transport,and stochastic optimal control through their shared links to the Schr\"odingerbridge problem, we propose novel objective functions that can be used totransport $\nu$ to $\mu$, consequently sample from the target $\mu$, viaoptimally controlled dynamics. We highlight the importance of the pathwiseperspective and the role various optimality conditions on the path measure canplay for the design of valid training losses, the careful choice of which offernumerical advantages in implementation. Basing the formalism on Schr\"odingerbridge comes with the additional practical capability of baking in inductivebias when it comes to Neural Network training.</description><author>Qijia Jiang, David Nabergoj</author><pubDate>Thu, 10 Oct 2024 17:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13731v2</guid></item><item><title>Deconstructing equivariant representations in molecular systems</title><link>http://arxiv.org/abs/2410.08131v1</link><description>Recent equivariant models have shown significant progress in not justchemical property prediction, but as surrogates for dynamical simulations ofmolecules and materials. Many of the top performing models in this category arebuilt within the framework of tensor products, which preserves equivariance byrestricting interactions and transformations to those that are allowed bysymmetry selection rules. Despite being a core part of the modeling process,there has not yet been much attention into understanding what informationpersists in these equivariant representations, and their general behavioroutside of benchmark metrics. In this work, we report on a set of experimentsusing a simple equivariant graph convolution model on the QM9 dataset, focusingon correlating quantitative performance with the resulting molecular graphembeddings. Our key finding is that, for a scalar prediction task, many of theirreducible representations are simply ignored during training -- specificallythose pertaining to vector ($l=1$) and tensor quantities ($l=2$) -- an issuethat does not necessarily make itself evident in the test metric. Weempirically show that removing some unused orders of spherical harmonicsimproves model performance, correlating with improved latent space structure.We provide a number of recommendations for future experiments to try andimprove efficiency and utilization of equivariant features based on theseobservations.</description><author>Kin Long Kelvin Lee, Mikhail Galkin, Santiago Miret</author><pubDate>Thu, 10 Oct 2024 17:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08131v1</guid></item><item><title>Think Beyond Size: Dynamic Prompting for More Effective Reasoning</title><link>http://arxiv.org/abs/2410.08130v1</link><description>This paper presents Dynamic Prompting, a novel framework aimed at improvingthe reasoning capabilities of Large Language Models (LLMs). In contrast toconventional static prompting methods, Dynamic Prompting enables the adaptivemodification of prompt sequences and step counts based on real-time taskcomplexity and model performance. This dynamic adaptation facilitates moreefficient problem-solving, particularly in smaller models, by reducinghallucinations and repetitive cycles. Our empirical evaluations demonstratethat Dynamic Prompting allows smaller LLMs to perform competitively with muchlarger models, thereby challenging the conventional emphasis on model size asthe primary determinant of reasoning efficacy.</description><author>Kamesh R</author><pubDate>Thu, 10 Oct 2024 17:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08130v1</guid></item><item><title>Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency</title><link>http://arxiv.org/abs/2410.08129v1</link><description>3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, bothfor inverse rendering as well as real-time exploration of scenes. In theseapplications, coherence across camera frames and multiple views is crucial, beit for robust convergence of a scene reconstruction or for artifact-freefly-throughs. Recent work started mitigating artifacts that break multi-viewcoherence, including popping artifacts due to inconsistent transparency sortingand perspective-correct outlines of (2D) splats. At the same time, real-timerequirements forced such implementations to accept compromises in howtransparency of large assemblies of 3D Gaussians is resolved, in turn breakingcoherence in other ways. In our work, we aim at achieving maximum coherence, byrendering fully perspective-correct 3D Gaussians while using a high-qualityapproximation of accurate blending, hybrid transparency, on a per-pixel level,in order to retain real-time frame rates. Our fast and perspectively accurateapproach for evaluation of 3D Gaussians does not require matrix inversions,thereby ensuring numerical stability and eliminating the need for specialhandling of degenerate splats, and the hybrid transparency formulation forblending maintains similar quality as fully resolved per-pixel transparenciesat a fraction of the rendering costs. We further show that each of these twocomponents can be independently integrated into Gaussian splatting systems. Incombination, they achieve up to 2$\times$ higher frame rates, 2$\times$ fasteroptimization, and equal or better image quality with fewer rendering artifactscompared to traditional 3DGS on common benchmarks.</description><author>Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor</author><pubDate>Thu, 10 Oct 2024 17:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08129v1</guid></item><item><title>Mars: Situated Inductive Reasoning in an Open-World Environment</title><link>http://arxiv.org/abs/2410.08126v1</link><description>Large Language Models (LLMs) trained on massive corpora have shown remarkablesuccess in knowledge-intensive tasks. Yet, most of them rely on pre-storedknowledge. Inducing new general knowledge from a specific environment andperforming reasoning with the acquired knowledge -- \textit{situated inductivereasoning}, is crucial and challenging for machine intelligence. In this paper,we design Mars, an interactive environment devised for situated inductivereasoning. It introduces counter-commonsense game mechanisms by modifyingterrain, survival setting and task dependency while adhering to certainprinciples. In Mars, agents need to actively interact with their surroundings,derive useful rules and perform decision-making tasks in specific contexts. Weconduct experiments on various RL-based and LLM-based methods, finding thatthey all struggle on this challenging situated inductive reasoning benchmark.Furthermore, we explore \textit{Induction from Reflection}, where we instructagents to perform inductive reasoning from history trajectory. The superiorperformance underscores the importance of inductive reasoning in Mars. ThroughMars, we aim to galvanize advancements in situated inductive reasoning and setthe stage for developing the next generation of AI systems that can reason inan adaptive and context-sensitive way.</description><author>Xiaojuan Tang, Jiaqi Li, Yitao Liang, Song-chun Zhu, Muhan Zhang, Zilong Zheng</author><pubDate>Thu, 10 Oct 2024 17:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08126v1</guid></item><item><title>Generalizing Stochastic Smoothing for Differentiation and Gradient Estimation</title><link>http://arxiv.org/abs/2410.08125v1</link><description>We deal with the problem of gradient estimation for stochastic differentiablerelaxations of algorithms, operators, simulators, and other non-differentiablefunctions. Stochastic smoothing conventionally perturbs the input of anon-differentiable function with a differentiable density distribution withfull support, smoothing it and enabling gradient estimation. Our theory startsat first principles to derive stochastic smoothing with reduced assumptions,without requiring a differentiable density nor full support, and we present ageneral framework for relaxation and gradient estimation of non-differentiableblack-box functions $f:\mathbb{R}^n\to\mathbb{R}^m$. We develop variancereduction for gradient estimation from 3 orthogonal perspectives. Empirically,we benchmark 6 distributions and up to 24 variance reduction strategies fordifferentiable sorting and ranking, differentiable shortest-paths on graphs,differentiable rendering for pose estimation, as well as differentiable cryo-ETsimulations.</description><author>Felix Petersen, Christian Borgelt, Aashwin Mishra, Stefano Ermon</author><pubDate>Thu, 10 Oct 2024 17:10:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08125v1</guid></item><item><title>The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD</title><link>http://arxiv.org/abs/2410.06186v2</link><description>We propose a simple heuristic privacy analysis of noisy clipped stochasticgradient descent (DP-SGD) in the setting where only the last iterate isreleased and the intermediate iterates remain hidden. Namely, our heuristicassumes a linear structure for the model. We show experimentally that our heuristic is predictive of the outcome ofprivacy auditing applied to various training procedures. Thus it can be usedprior to training as a rough estimate of the final privacy leakage. We alsoprobe the limitations of our heuristic by providing some artificialcounterexamples where it underestimates the privacy leakage. The standard composition-based privacy analysis of DP-SGD effectively assumesthat the adversary has access to all intermediate iterates, which is oftenunrealistic. However, this analysis remains the state of the art in practice.While our heuristic does not replace a rigorous privacy analysis, itillustrates the large gap between the best theoretical upper bounds and theprivacy auditing lower bounds and sets a target for further work to improve thetheoretical privacy analyses. We also empirically support our heuristic andshow existing privacy auditing attacks are bounded by our heuristic analysis inboth vision and language tasks.</description><author>Thomas Steinke, Milad Nasr, Arun Ganesh, Borja Balle, Christopher A. Choquette-Choo, Matthew Jagielski, Jamie Hayes, Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis</author><pubDate>Thu, 10 Oct 2024 17:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06186v2</guid></item><item><title>RISE-SDF: a Relightable Information-Shared Signed Distance Field for Glossy Object Inverse Rendering</title><link>http://arxiv.org/abs/2409.20140v3</link><description>In this paper, we propose a novel end-to-end relightable neural inverserendering system that achieves high-quality reconstruction of geometry andmaterial properties, thus enabling high-quality relighting. The cornerstone ofour method is a two-stage approach for learning a better factorization of sceneparameters. In the first stage, we develop a reflection-aware radiance fieldusing a neural signed distance field (SDF) as the geometry representation anddeploy an MLP (multilayer perceptron) to estimate indirect illumination. In thesecond stage, we introduce a novel information-sharing network structure tojointly learn the radiance field and the physically based factorization of thescene. For the physically based factorization, to reduce the noise caused byMonte Carlo sampling, we apply a split-sum approximation with a simplifiedDisney BRDF and cube mipmap as the environment light representation. In therelighting phase, to enhance the quality of indirect illumination, we propose asecond split-sum algorithm to trace secondary rays under the split-sumrendering framework. Furthermore, there is no dataset or protocol available toquantitatively evaluate the inverse rendering performance for glossy objects.To assess the quality of material reconstruction and relighting, we havecreated a new dataset with ground truth BRDF parameters and relighting results.Our experiments demonstrate that our algorithm achieves state-of-the-artperformance in inverse rendering and relighting, with particularly strongresults in the reconstruction of highly reflective objects.</description><author>Deheng Zhang, Jingyu Wang, Shaofei Wang, Marko Mihajlovic, Sergey Prokudin, Hendrik P. A. Lensch, Siyu Tang</author><pubDate>Thu, 10 Oct 2024 17:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20140v3</guid></item><item><title>Heterogeneous Graph Auto-Encoder for CreditCard Fraud Detection</title><link>http://arxiv.org/abs/2410.08121v1</link><description>The digital revolution has significantly impacted financial transactions,leading to a notable increase in credit card usage. However, this conveniencecomes with a trade-off: a substantial rise in fraudulent activities.Traditional machine learning methods for fraud detection often struggle tocapture the inherent interconnectedness within financial data. This paperproposes a novel approach for credit card fraud detection that leverages GraphNeural Networks (GNNs) with attention mechanisms applied to heterogeneous graphrepresentations of financial data. Unlike homogeneous graphs, heterogeneousgraphs capture intricate relationships between various entities in thefinancial ecosystem, such as cardholders, merchants, and transactions,providing a richer and more comprehensive data representation for fraudanalysis. To address the inherent class imbalance in fraud data, where genuinetransactions significantly outnumber fraudulent ones, the proposed approachintegrates an autoencoder. This autoencoder, trained on genuine transactions,learns a latent representation and flags deviations during reconstruction aspotential fraud. This research investigates two key questions: (1) Howeffectively can a GNN with an attention mechanism detect and prevent creditcard fraud when applied to a heterogeneous graph? (2) How does the efficacy ofthe autoencoder with attention approach compare to traditional methods? Theresults are promising, demonstrating that the proposed model outperformsbenchmark algorithms such as Graph Sage and FI-GRL, achieving a superior AUC-PRof 0.89 and an F1-score of 0.81. This research significantly advances frauddetection systems and the overall security of financial transactions byleveraging GNNs with attention mechanisms and addressing class imbalancethrough an autoencoder.</description><author>Moirangthem Tiken Singh, Rabinder Kumar Prasad, Gurumayum Robert Michael, N K Kaphungkui, N. Hemarjit Singh</author><pubDate>Thu, 10 Oct 2024 17:05:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08121v1</guid></item><item><title>Q-VLM: Post-training Quantization for Large Vision-Language Models</title><link>http://arxiv.org/abs/2410.08119v1</link><description>In this paper, we propose a post-training quantization framework of largevision-language models (LVLMs) for efficient multi-modal inference.Conventional quantization methods sequentially search the layer-wise roundingfunctions by minimizing activation discretization errors, which fails toacquire optimal quantization strategy without considering cross-layerdependency. On the contrary, we mine the cross-layer dependency thatsignificantly influences discretization errors of the entire vision-languagemodel, and embed this dependency into optimal quantization strategy searchingwith low search cost. Specifically, we observe the strong correlation betweenthe activation entropy and the cross-layer dependency concerning outputdiscretization errors. Therefore, we employ the entropy as the proxy topartition blocks optimally, which aims to achieve satisfying trade-offs betweendiscretization errors and the search cost. Moreover, we optimize the visualencoder to disentangle the cross-layer dependency for fine-graineddecomposition of search space, so that the search cost is further reducedwithout harming the quantization accuracy. Experimental results demonstratethat our method compresses the memory by 2.78x and increase generate speed by1.44x about 13B LLaVA model without performance degradation on diversemulti-modal reasoning tasks. Code is available athttps://github.com/ChangyuanWang17/QVLM.</description><author>Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 10 Oct 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08119v1</guid></item><item><title>Medical Image Quality Assessment based on Probability of Necessity and Sufficiency</title><link>http://arxiv.org/abs/2410.08118v1</link><description>Medical image quality assessment (MIQA) is essential for reliable medicalimage analysis. While deep learning has shown promise in this field, currentmodels could be misled by spurious correlations learned from data and strugglewith out-of-distribution (OOD) scenarios. To that end, we propose an MIQAframework based on a concept from causal inference: Probability of Necessityand Sufficiency (PNS). PNS measures how likely a set of features is to be bothnecessary (always present for an outcome) and sufficient (capable ofguaranteeing an outcome) for a particular result. Our approach leverages thisconcept by learning hidden features from medical images with high PNS valuesfor quality prediction. This encourages models to capture more essentialpredictive information, enhancing their robustness to OOD scenarios. Weevaluate our framework on an Anterior Segment Optical Coherence Tomography(AS-OCT) dataset for the MIQA task and experimental results demonstrate theeffectiveness of our framework.</description><author>Boyu Chen, Ameenat L. Solebo, Weiye Bao, Paul Taylor</author><pubDate>Thu, 10 Oct 2024 17:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08118v1</guid></item><item><title>On Barycenter Computation: Semi-Unbalanced Optimal Transport-based Method on Gaussians</title><link>http://arxiv.org/abs/2410.08117v1</link><description>We explore a robust version of the barycenter problem among $n$ centeredGaussian probability measures, termed Semi-Unbalanced Optimal Transport(SUOT)-based Barycenter, wherein the barycenter remains fixed while the othersare relaxed using Kullback-Leibler divergence. We develop optimizationalgorithms on Bures-Wasserstein manifold, named the Exact Geodesic GradientDescent and Hybrid Gradient Descent algorithms. While the Exact GeodesicGradient Descent method is based on computing the exact closed form of thefirst-order derivative of the objective function of the barycenter along ageodesic on the Bures manifold, the Hybrid Gradient Descent method utilizesoptimizer components when solving the SUOT problem to replace outlier measuresbefore applying the Riemannian Gradient Descent. We establish the theoreticalconvergence guarantees for both methods and demonstrate that the Exact GeodesicGradient Descent algorithm attains a dimension-free convergence rate. Finally,we conduct experiments to compare the normal Wasserstein Barycenter with oursand perform an ablation study.</description><author>Ngoc-Hai Nguyen, Dung Le, Hoang-Phi Nguyen, Tung Pham, Nhat Ho</author><pubDate>Thu, 10 Oct 2024 17:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08117v1</guid></item><item><title>Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System</title><link>http://arxiv.org/abs/2410.08115v1</link><description>Large Language Model (LLM) based multi-agent systems (MAS) show remarkablepotential in collaborative problem-solving, yet they still face criticalchallenges: low communication efficiency, poor scalability, and a lack ofeffective parameter-updating optimization methods. We present Optima, a novelframework that addresses these issues by significantly enhancing bothcommunication efficiency and task effectiveness in LLM-based MAS through LLMtraining. Optima employs an iterative generate, rank, select, and trainparadigm with a reward function balancing task performance, token efficiency,and communication readability. We explore various RL algorithms, includingSupervised Fine-Tuning, Direct Preference Optimization, and their hybridapproaches, providing insights into their effectiveness-efficiency trade-offs.We integrate Monte Carlo Tree Search-inspired techniques for DPO datageneration, treating conversation turns as tree nodes to explore diverseinteraction paths. Evaluated on common multi-agent tasks, includinginformation-asymmetric question answering and complex reasoning, Optima showsconsistent and substantial improvements over single-agent baselines and vanillaMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than10\% tokens on tasks requiring heavy information exchange. Moreover, Optima'sefficiency gains open new possibilities for leveraging inference-compute moreeffectively, leading to improved inference-time scaling laws. By addressingfundamental challenges in LLM-based MAS, Optima shows the potential towardsscalable, efficient, and effective MAS(https://chenweize1998.github.io/optima-project-page).</description><author>Weize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 10 Oct 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08115v1</guid></item><item><title>Parameter-Efficient Fine-Tuning in Spectral Domain for Point Cloud Learning</title><link>http://arxiv.org/abs/2410.08114v1</link><description>Recently, leveraging pre-training techniques to enhance point cloud modelshas become a hot research topic. However, existing approaches typically requirefull fine-tuning of pre-trained models to achieve satisfied performance ondownstream tasks, accompanying storage-intensive and computationally demanding.To address this issue, we propose a novel Parameter-Efficient Fine-Tuning(PEFT) method for point cloud, called PointGST (Point cloud Graph SpectralTuning). PointGST freezes the pre-trained model and introduces a lightweight,trainable Point Cloud Spectral Adapter (PCSA) to fine-tune parameters in thespectral domain. The core idea is built on two observations: 1) The innertokens from frozen models might present confusion in the spatial domain; 2)Task-specific intrinsic information is important for transferring the generalknowledge to the downstream task. Specifically, PointGST transfers the pointtokens from the spatial domain to the spectral domain, effectivelyde-correlating confusion among tokens via using orthogonal components forseparating. Moreover, the generated spectral basis involves intrinsicinformation about the downstream point clouds, enabling more targeted tuning.As a result, PointGST facilitates the efficient transfer of general knowledgeto downstream tasks while significantly reducing training costs. Extensiveexperiments on challenging point cloud datasets across various tasksdemonstrate that PointGST not only outperforms its fully fine-tuningcounterpart but also significantly reduces trainable parameters, making it apromising solution for efficient point cloud learning. It improves upon a solidbaseline by +2.28%, 1.16%, and 2.78%, resulting in 99.48%, 97.76%, and 96.18%on the ScanObjNN OBJ BG, OBJ OBLY, and PB T50 RS datasets, respectively. Thisadvancement establishes a new state-of-the-art, using only 0.67% of thetrainable parameters.</description><author>Dingkang Liang, Tianrui Feng, Xin Zhou, Yumeng Zhang, Zhikang Zou, Xiang Bai</author><pubDate>Thu, 10 Oct 2024 17:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08114v1</guid></item><item><title>Robust AI-Generated Text Detection by Restricted Embeddings</title><link>http://arxiv.org/abs/2410.08113v1</link><description>Growing amount and quality of AI-generated texts makes detecting such contentmore difficult. In most real-world scenarios, the domain (style and topic) ofgenerated data and the generator model are not known in advance. In this work,we focus on the robustness of classifier-based detectors of AI-generated text,namely their ability to transfer to unseen generators or semantic domains. Weinvestigate the geometry of the embedding space of Transformer-based textencoders and show that clearing out harmful linear subspaces helps to train arobust classifier, ignoring domain-specific spurious features. We investigateseveral subspace decomposition and feature selection strategies and achievesignificant improvements over state of the art methods in cross-domain andcross-generator transfer. Our best approaches for head-wise andcoordinate-based subspace removal increase the mean out-of-distribution (OOD)classification score by up to 9% and 14% in particular setups for RoBERTa andBERT embeddings respectively. We release our code and data:https://github.com/SilverSolver/RobustATD</description><author>Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, Serguei Barannikov, Sergey Nikolenko, Irina Piontkovskaya</author><pubDate>Thu, 10 Oct 2024 16:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08113v1</guid></item><item><title>Active Fourier Auditor for Estimating Distributional Properties of ML Models</title><link>http://arxiv.org/abs/2410.08111v1</link><description>With the pervasive deployment of Machine Learning (ML) models in real-worldapplications, verifying and auditing properties of ML models have become acentral concern. In this work, we focus on three properties: robustness,individual fairness, and group fairness. We discuss two approaches for auditingML model properties: estimation with and without reconstruction of the targetmodel under audit. Though the first approach is studied in the literature, thesecond approach remains unexplored. For this purpose, we develop a newframework that quantifies different properties in terms of the Fouriercoefficients of the ML model under audit but does not parametricallyreconstruct it. We propose the Active Fourier Auditor (AFA), which queriessample points according to the Fourier coefficients of the ML model, andfurther estimates the properties. We derive high probability error bounds onAFA's estimates, along with the worst-case lower bounds on the samplecomplexity to audit them. Numerically we demonstrate on multiple datasets andmodels that AFA is more accurate and sample-efficient to estimate theproperties of interest than the baselines.</description><author>Ayoub Ajarra, Bishwamittra Ghosh, Debabrota Basu</author><pubDate>Thu, 10 Oct 2024 16:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08111v1</guid></item><item><title>A Closer Look at Machine Unlearning for Large Language Models</title><link>http://arxiv.org/abs/2410.08109v1</link><description>Large language models (LLMs) may memorize sensitive or copyrighted content,raising privacy and legal concerns. Due to the high cost of retraining fromscratch, researchers attempt to employ machine unlearning to remove specificcontent from LLMs while preserving the overall performance. In this paper, wediscuss several issues in machine unlearning for LLMs and provide our insightson possible approaches. To address the issue of inadequate evaluation of modeloutputs after unlearning, we introduce three additional metrics to evaluatetoken diversity, sentence semantics, and factual correctness. We thencategorize unlearning methods into untargeted and targeted, and discuss theirissues respectively. Specifically, the behavior that untargeted unlearningattempts to approximate is unpredictable and may involve hallucinations, andexisting regularization is insufficient for targeted unlearning. To alleviatethese issues, we propose using the objective of maximizing entropy (ME) foruntargeted unlearning and incorporate answer preservation (AP) loss asregularization for targeted unlearning. Experimental results across threescenarios, i.e., fictitious unlearning, continual unlearning, and real-worldunlearning, demonstrate the effectiveness of our approaches. The code isavailable at https://github.com/sail-sg/closer-look-LLM-unlearning.</description><author>Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin</author><pubDate>Thu, 10 Oct 2024 16:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08109v1</guid></item><item><title>Identifying and Addressing Delusions for Target-Directed Decision-Making</title><link>http://arxiv.org/abs/2410.07096v2</link><description>We are interested in target-directed agents, which produce targets duringdecision-time planning, to guide their behaviors and achieve bettergeneralization during evaluation. Improper training of these agents can resultin delusions: the agent may come to hold false beliefs about the targets, whichcannot be properly rejected, leading to unwanted behaviors and damagingout-of-distribution generalization. We identify different types of delusions byusing intuitive examples in carefully controlled environments, and investigatetheir causes. We demonstrate how delusions can be addressed for agents trainedby hindsight relabeling, a mainstream approach in for training target-directedRL agents. We validate empirically the effectiveness of the proposed solutionsin correcting delusional behaviors and improving out-of-distributiongeneralization.</description><author>Mingde Zhao, Tristan Sylvain, Doina Precup, Yoshua Bengio</author><pubDate>Thu, 10 Oct 2024 16:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07096v2</guid></item><item><title>Reference-based Metrics Disprove Themselves in Question Generation</title><link>http://arxiv.org/abs/2403.12242v3</link><description>Reference-based metrics such as BLEU and BERTScore are widely used toevaluate question generation (QG). In this study, on QG benchmarks such asSQuAD and HotpotQA, we find that using human-written references cannotguarantee the effectiveness of the reference-based metrics. Most QG benchmarkshave only one reference; we replicate the annotation process and collectanother reference. A good metric is expected to grade a human-validatedquestion no worse than generated questions. However, the results ofreference-based metrics on our newly collected reference disproved the metricsthemselves. We propose a reference-free metric consisted of multi-dimensionalcriteria such as naturalness, answerability, and complexity, utilizing largelanguage models. These criteria are not constrained to the syntactic orsemantic of a single reference question, and the metric does not require adiverse set of references. Experiments reveal that our metric accuratelydistinguishes between high-quality questions and flawed ones, and achievesstate-of-the-art alignment with human judgment.</description><author>Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang</author><pubDate>Thu, 10 Oct 2024 16:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12242v3</guid></item><item><title>IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera</title><link>http://arxiv.org/abs/2410.08107v1</link><description>Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) fornovel view synthesis have achieved remarkable progress with frame-based camera(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a noveltype of bio-inspired visual sensor, i.e. event camera, has demonstratedadvantages in high temporal resolution, high dynamic range, low powerconsumption and low latency. Due to its unique asynchronous and irregular datacapturing process, limited work has been proposed to apply neuralrepresentation or 3D Gaussian splatting for an event camera. In this work, wepresent IncEventGS, an incremental 3D Gaussian Splatting reconstructionalgorithm with a single event camera. To recover the 3D scene representationincrementally, we exploit the tracking and mapping paradigm of conventionalSLAM pipelines for IncEventGS. Given the incoming event stream, the trackerfirstly estimates an initial camera motion based on prior reconstructed 3D-GSscene representation. The mapper then jointly refines both the 3D scenerepresentation and camera motion based on the previously estimated motiontrajectory from the tracker. The experimental results demonstrate thatIncEventGS delivers superior performance compared to prior NeRF-based methodsand other related baselines, even we do not have the ground-truth camera poses.Furthermore, our method can also deliver better performance compared tostate-of-the-art event visual odometry methods in terms of camera motionestimation. Code is publicly available at:https://github.com/wu-cvgl/IncEventGS.</description><author>Jian Huang, Chengrui Dong, Peidong Liu</author><pubDate>Thu, 10 Oct 2024 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08107v1</guid></item><item><title>What Makes Large Language Models Reason in (Multi-Turn) Code Generation?</title><link>http://arxiv.org/abs/2410.08105v1</link><description>Prompting techniques such as chain-of-thought have established themselves asa popular vehicle for improving the outputs of large language models (LLMs).For code generation, however, their exact mechanics and efficacy areunder-explored. We thus investigate the effects of a wide range of promptingstrategies with a focus on automatic re-prompting over multiple turns andcomputational requirements. After systematically decomposing reasoning,instruction, and execution feedback prompts, we conduct an extensive gridsearch on the competitive programming benchmarks CodeContests and TACO formultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).Our study reveals strategies that consistently improve performance across allmodels with small and large sampling budgets. We then show how finetuning withsuch an optimal configuration allows models to internalize the inducedreasoning process and obtain improvements in performance and scalability formulti-turn code generation.</description><author>Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin Negrevergne, Gabriel Synnaeve</author><pubDate>Thu, 10 Oct 2024 16:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08105v1</guid></item><item><title>Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures</title><link>http://arxiv.org/abs/2410.06672v2</link><description>The hypothesis of Universality in interpretability suggests that differentneural networks may converge to implement similar algorithms on similar tasks.In this work, we investigate two mainstream architectures for languagemodeling, namely Transformers and Mambas, to explore the extent of theirmechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolateinterpretable features from these models and show that most features aresimilar in these two models. We also validate the correlation between featuresimilarity and Universality. We then delve into the circuit-level analysis ofMamba models and find that the induction circuits in Mamba are structurallyanalogous to those in Transformers. We also identify a nuanced difference wecall \emph{Off-by-One motif}: The information of one token is written into theSSM state in its next position. Whilst interaction between tokens inTransformers does not exhibit such trend.</description><author>Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu</author><pubDate>Thu, 10 Oct 2024 16:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06672v2</guid></item><item><title>Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining</title><link>http://arxiv.org/abs/2410.08102v1</link><description>Efficient data selection is crucial to accelerate the pretraining of largelanguage models (LLMs). While various methods have been proposed to enhancedata efficiency, limited research has addressed the inherent conflicts betweenthese approaches to achieve optimal data selection for LLM pretraining. Totackle this problem, we propose a novel multi-agent collaborative dataselection mechanism. In this framework, each data selection method serves as anindependent agent, and an agent console is designed to dynamically integratethe information from all agents throughout the LLM training process. We conductextensive empirical studies to evaluate our multi-agent framework. Theexperimental results demonstrate that our approach significantly improves dataefficiency, accelerates convergence in LLM training, and achieves an averageperformance gain of 10.5% across multiple language model benchmarks compared tothe state-of-the-art methods.</description><author>Tianyi Bai, Ling Yang, Zhen Hao Wong, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Qiu Jiantao, Wentao Zhang, Binhang Yuan, Conghui He</author><pubDate>Thu, 10 Oct 2024 16:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08102v1</guid></item><item><title>CrackSegDiff: Diffusion Probability Model-based Multi-modal Crack Segmentation</title><link>http://arxiv.org/abs/2410.08100v1</link><description>Integrating grayscale and depth data in road inspection robots could enhancethe accuracy, reliability, and comprehensiveness of road condition assessments,leading to improved maintenance strategies and safer infrastructure. However,these data sources are often compromised by significant background noise fromthe pavement. Recent advancements in Diffusion Probabilistic Models (DPM) havedemonstrated remarkable success in image segmentation tasks, showcasing potentdenoising capabilities, as evidenced in studies like SegDiff\cite{amit2021segdiff}. Despite these advancements, current DPM-basedsegmentors do not fully capitalize on the potential of original image data. Inthis paper, we propose a novel DPM-based approach for crack segmentation, namedCrackSegDiff, which uniquely fuses grayscale and range/depth images. Thismethod enhances the reverse diffusion process by intensifying the interactionbetween local feature extraction via DPM and global feature extraction. Unliketraditional methods that utilize Transformers for global features, our approachemploys Vm-unet \cite{ruan2024vm} to efficiently capture long-range informationof the original data. The integration of features is further refined throughtwo innovative modules: the Channel Fusion Module (CFM) and the Shallow FeatureCompensation Module (SFCM). Our experimental evaluation on the three-classcrack image segmentation tasks within the FIND dataset demonstrates thatCrackSegDiff outperforms state-of-the-art methods, particularly excelling inthe detection of shallow cracks. Code is available athttps://github.com/sky-visionX/CrackSegDiff.</description><author>Xiaoyan Jiang, Licheng Jiang, Anjie Wang, Kaiying Zhu, Yongbin Gao</author><pubDate>Thu, 10 Oct 2024 16:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08100v1</guid></item><item><title>A Generative AI Technique for Synthesizing a Digital Twin for U.S. Residential Solar Adoption and Generation</title><link>http://arxiv.org/abs/2410.08098v1</link><description>Residential rooftop solar adoption is considered crucial for reducing carbonemissions. The lack of photovoltaic (PV) data at a finer resolution (e.g.,household, hourly levels) poses a significant roadblock to informeddecision-making. We discuss a novel methodology to generate a highly granular,residential-scale realistic dataset for rooftop solar adoption across thecontiguous United States. The data-driven methodology consists of: (i)integrated machine learning models to identify PV adopters, (ii) methods toaugment the data using explainable AI techniques to glean insights about keyfeatures and their interactions, and (iii) methods to generate household-levelhourly solar energy output using an analytical model. The resulting syntheticdatasets are validated using real-world data and can serve as a digital twinfor modeling downstream tasks. Finally, a policy-based case study utilizing thedigital twin for Virginia demonstrated increased rooftop solar adoption withthe 30\% Federal Solar Investment Tax Credit, especially inLow-to-Moderate-Income communities.</description><author>Aparna Kishore, Swapna Thorve, Madhav Marathe</author><pubDate>Thu, 10 Oct 2024 16:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08098v1</guid></item><item><title>Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary</title><link>http://arxiv.org/abs/2402.00236v4</link><description>This study reports an unintuitive finding that positional encoding enhanceslearning of recurrent neural networks (RNNs). Positional encoding is ahigh-dimensional representation of time indices on input data. Most famously,positional encoding complements the capabilities of Transformer neuralnetworks, which lack an inherent mechanism for representing the data order. Bycontrast, RNNs can encode the temporal information of data points on their own,rendering their use of positional encoding seemingly redundant/unnecessary.Nonetheless, investigations through synthetic benchmarks reveal an advantage ofcoupling positional encoding and RNNs, especially for handling a largevocabulary that yields low-frequency tokens. Further scrutinization unveilsthat these low-frequency tokens destabilizes the gradients of vanilla RNNs, andthe positional encoding resolves this instability. These results shed a newlight on the utility of positional encoding beyond its canonical role as atimekeeper for Transformers.</description><author>Takashi Morita</author><pubDate>Thu, 10 Oct 2024 16:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00236v4</guid></item><item><title>Universal In-Context Approximation By Prompting Fully Recurrent Models</title><link>http://arxiv.org/abs/2406.01424v2</link><description>Zero-shot and in-context learning enable solving tasks without modelfine-tuning, making them essential for developing generative model solutions.Therefore, it is crucial to understand whether a pretrained model can beprompted to approximate any function, i.e., whether it is a universalin-context approximator. While it was recently shown that transformer models dopossess this property, these results rely on their attention mechanism. Hence,these findings do not apply to fully recurrent architectures like RNNs, LSTMs,and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs,Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin canalso serve as universal in-context approximators. To streamline our argument,we introduce a programming language called LSRL that compiles to these fullyrecurrent architectures. LSRL may be of independent interest for furtherstudies of fully recurrent models, such as constructing interpretabilitybenchmarks. We also study the role of multiplicative gating and observe thatarchitectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) canimplement certain operations more stably, making them more viable candidatesfor practical in-context universal approximation.</description><author>Aleksandar Petrov, Tom A. Lamb, Alasdair Paren, Philip H. S. Torr, Adel Bibi</author><pubDate>Thu, 10 Oct 2024 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01424v2</guid></item><item><title>SAKA: An Intelligent Platform for Semi-automated Knowledge Graph Construction and Application</title><link>http://arxiv.org/abs/2410.08094v1</link><description>Knowledge graph (KG) technology is extensively utilized in many areas, andmany companies offer applications based on KG. Nonetheless, the majority of KGplatforms necessitate expertise and tremendous time and effort of users toconstruct KG records manually, which poses great difficulties for ordinarypeople to use. Additionally, audio data is abundant and holds valuableinformation, but it is challenging to transform it into a KG. What's more, theplatforms usually do not leverage the full potential of the KGs constructed byusers. In this paper, we propose an intelligent and user-friendly platform forSemi-automated KG Construction and Application (SAKA) to address the problemsaforementioned. Primarily, users can semi-automatically construct KGs fromstructured data of numerous areas by interacting with the platform, based onwhich multi-versions of KG can be stored, viewed, managed, and updated.Moreover, we propose an Audio-based KG Information Extraction (AGIE) method toestablish KGs from audio data. Lastly, the platform creates a semanticparsing-based knowledge base question answering (KBQA) system based on theuser-created KGs. We prove the feasibility of the semi-automatic KGconstruction method on the SAKA platform.</description><author>Hanrong Zhang, Xinyue Wang, Jiabao Pan, Hongwei Wang</author><pubDate>Thu, 10 Oct 2024 16:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08094v1</guid></item><item><title>Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond</title><link>http://arxiv.org/abs/2410.07158v2</link><description>In recent years, training data attribution (TDA) methods have emerged as apromising direction for the interpretability of neural networks. While researcharound TDA is thriving, limited effort has been dedicated to the evaluation ofattributions. Similar to the development of evaluation metrics for traditionalfeature attribution approaches, several standalone metrics have been proposedto evaluate the quality of TDA methods across various contexts. However, thelack of a unified framework that allows for systematic comparison limits trustin TDA methods and stunts their widespread adoption. To address this researchgap, we introduce Quanda, a Python toolkit designed to facilitate theevaluation of TDA methods. Beyond offering a comprehensive set of evaluationmetrics, Quanda provides a uniform interface for seamless integration withexisting TDA implementations across different repositories, thus enablingsystematic benchmarking. The toolkit is user-friendly, thoroughly tested,well-documented, and available as an open-source library on PyPi and underhttps://github.com/dilyabareeva/quanda.</description><author>Dilyara Bareeva, Galip Ümit Yolcu, Anna Hedström, Niklas Schmolenski, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Thu, 10 Oct 2024 16:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07158v2</guid></item><item><title>UW-SDF: Exploiting Hybrid Geometric Priors for Neural SDF Reconstruction from Underwater Multi-view Monocular Images</title><link>http://arxiv.org/abs/2410.08092v1</link><description>Due to the unique characteristics of underwater environments, accurate 3Dreconstruction of underwater objects poses a challenging problem in tasks suchas underwater exploration and mapping. Traditional methods that rely onmultiple sensor data for 3D reconstruction are time-consuming and facechallenges in data acquisition in underwater scenarios. We propose UW-SDF, aframework for reconstructing target objects from multi-view underwater imagesbased on neural SDF. We introduce hybrid geometric priors to optimize thereconstruction process, markedly enhancing the quality and efficiency of neuralSDF reconstruction. Additionally, to address the challenge of segmentationconsistency in multi-view images, we propose a novel few-shot multi-view targetsegmentation strategy using the general-purpose segmentation model (SAM),enabling rapid automatic segmentation of unseen objects. Through extensivequalitative and quantitative experiments on diverse datasets, we demonstratethat our proposed method outperforms the traditional underwater 3Dreconstruction method and other neural rendering approaches in the field ofunderwater 3D reconstruction.</description><author>Zeyu Chen, Jingyi Tang, Gu Wang, Shengquan Li, Xinghui Li, Xiangyang Ji, Xiu Li</author><pubDate>Thu, 10 Oct 2024 16:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08092v1</guid></item><item><title>Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation</title><link>http://arxiv.org/abs/2410.08091v1</link><description>Despite alleviating the dependence on dense annotations inherent to fullysupervised methods, weakly supervised point cloud semantic segmentation suffersfrom inadequate supervision signals. In response to this challenge, weintroduce a novel perspective that imparts auxiliary constraints by regulatingthe feature space under weak supervision. Our initial investigation identifieswhich distributions accurately characterize the feature space, subsequentlyleveraging this priori to guide the alignment of the weakly supervisedembeddings. Specifically, we analyze the superiority of the mixture of vonMises-Fisher distributions (moVMF) among several common distributioncandidates. Accordingly, we develop a Distribution Guidance Network (DGNet),which comprises a weakly supervised learning branch and a distributionalignment branch. Leveraging reliable clustering initialization derived fromthe weakly supervised learning branch, the distribution alignment branchalternately updates the parameters of the moVMF and the network, ensuringalignment with the moVMF-defined latent space. Extensive experiments validatethe rationality and effectiveness of our distribution choice and networkdesign. Consequently, DGNet achieves state-of-the-art performance undermultiple datasets and various weakly supervised settings.</description><author>Zhiyi Pan, Wei Gao, Shan Liu, Ge Li</author><pubDate>Thu, 10 Oct 2024 16:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08091v1</guid></item><item><title>Noether's razor: Learning Conserved Quantities</title><link>http://arxiv.org/abs/2410.08087v1</link><description>Symmetries have proven useful in machine learning models, improvinggeneralisation and overall performance. At the same time, recent advancementsin learning dynamical systems rely on modelling the underlying Hamiltonian toguarantee the conservation of energy. These approaches can be connected via aseminal result in mathematical physics: Noether's theorem, which states thatsymmetries in a dynamical system correspond to conserved quantities. This workuses Noether's theorem to parameterise symmetries as learnable conservedquantities. We then allow conserved quantities and associated symmetries to belearned directly from train data through approximate Bayesian model selection,jointly with the regular training procedure. As training objective, we derive avariational lower bound to the marginal likelihood. The objective automaticallyembodies an Occam's Razor effect that avoids collapse of conservation laws tothe trivial constant, without the need to manually add and tune additionalregularisers. We demonstrate a proof-of-principle on $n$-harmonic oscillatorsand $n$-body systems. We find that our method correctly identifies the correctconserved quantities and U($n$) and SE($n$) symmetry groups, improving overallperformance and predictive accuracy on test data.</description><author>Tycho F. A. van der Ouderaa, Mark van der Wilk, Pim de Haan</author><pubDate>Thu, 10 Oct 2024 16:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08087v1</guid></item><item><title>Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering</title><link>http://arxiv.org/abs/2410.08085v1</link><description>Recent works integrating Knowledge Graphs (KGs) have led to promisingimprovements in enhancing reasoning accuracy of Large Language Models (LLMs).However, current benchmarks mainly focus on closed tasks, leaving a gap in theassessment of more complex, real-world scenarios. This gap has also obscuredthe evaluation of KGs' potential to mitigate the problem of hallucination inLLMs. To fill the gap, we introduce OKGQA, a new benchmark specificallydesigned to assess LLMs enhanced with KGs under open-ended, real-world questionanswering scenarios. OKGQA is designed to closely reflect the complexities ofpractical applications using questions from different types, and incorporatesspecific metrics to measure both the reduction in hallucinations and theenhancement in reasoning capabilities. To consider the scenario in which KGsmay have varying levels of mistakes, we further propose another experimentsetting OKGQA-P to assess model performance when the semantics and structure ofKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explorewhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)conduct a comparative analysis to shed light on methods and future directionsfor leveraging KGs to reduce LLMs' hallucination. We believe that this studycan facilitate a more complete performance comparison and encourage continuousimprovement in integrating KGs with LLMs.</description><author>Yuan Sui, Bryan Hooi</author><pubDate>Thu, 10 Oct 2024 16:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08085v1</guid></item><item><title>Differentiability in Unrolled Training of Neural Physics Simulators on Transient Dynamics</title><link>http://arxiv.org/abs/2402.12971v2</link><description>Unrolling training trajectories over time strongly influences the inferenceaccuracy of neural network-augmented physics simulators. We analyze this inthree variants of training neural time-steppers. In addition to one-step setupsand fully differentiable unrolling, we include a third, less widely usedvariant: unrolling without temporal gradients. Comparing networks trained withthese three modalities disentangles the two dominant effects of unrolling,training distribution shift and long-term gradients. We present detailed studyacross physical systems, network sizes and architectures, training setups, andtest scenarios. It also encompasses two simulation modes: In prediction setups,we rely solely on neural networks to compute a trajectory. In contrast,correction setups include a numerical solver that is supported by a neuralnetwork. Spanning these variations, our study provides the empirical basis forour main findings: Non-differentiable but unrolled training with a numericalsolver in a correction setup can yield substantial improvements over a fullydifferentiable prediction setup not utilizing this solver. The accuracy ofmodels trained in a fully differentiable setup differs compared to theirnon-differentiable counterparts. Differentiable ones perform best in acomparison among correction networks as well as among prediction setups. Forboth, the accuracy of non-differentiable unrolling comes close. Furthermore, weshow that these behaviors are invariant to the physical system, the networkarchitecture and size, and the numerical scheme. These results motivateintegrating non-differentiable numerical simulators into training setups evenif full differentiability is unavailable. We show the convergence rate ofcommon architectures to be low compared to numerical algorithms. This motivatescorrection setups combining neural and numerical parts which utilize benefitsof both.</description><author>Bjoern List, Li-Wei Chen, Kartik Bali, Nils Thuerey</author><pubDate>Thu, 10 Oct 2024 16:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12971v2</guid></item><item><title>ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human with Animatable Garments</title><link>http://arxiv.org/abs/2410.08082v1</link><description>In this paper, we highlight a critical yet often overlooked factor in most 3Dhuman tasks, namely modeling humans with complex garments. It is known that theparameterized formulation of SMPL is able to fit human skin; while complexgarments, e.g., hand-held objects and loose-fitting garments, are difficult toget modeled within the unified framework, since their movements are usuallydecoupled with the human body. To enhance the capability of SMPL skeleton inresponse to this situation, we propose a modular growth strategy that enablesthe joint tree of the skeleton to expand adaptively. Specifically, our method,called ToMiE, consists of parent joints localization and external jointsoptimization. For parent joints localization, we employ a gradient-basedapproach guided by both LBS blending weights and motion kernels. Once theexternal joints are obtained, we proceed to optimize their transformations inSE(3) across different frames, enabling rendering and explicit animation. ToMiEmanages to outperform other methods across various cases with garments, notonly in rendering quality but also by offering free animation of grown joints,thereby enhancing the expressive ability of SMPL skeleton for a broader rangeof applications.</description><author>Yifan Zhan, Qingtian Zhu, Muyao Niu, Mingze Ma, Jiancheng Zhao, Zhihang Zhong, Xiao Sun, Yu Qiao, Yinqiang Zheng</author><pubDate>Thu, 10 Oct 2024 16:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08082v1</guid></item><item><title>Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning</title><link>http://arxiv.org/abs/2410.08081v1</link><description>Packing, initially utilized in the pre-training phase, is an optimizationtechnique designed to maximize hardware resource efficiency by combiningdifferent training sequences to fit the model's maximum input length. Althoughit has demonstrated effectiveness during pre-training, there remains a lack ofcomprehensive analysis for the supervised fine-tuning (SFT) stage on thefollowing points: (1) whether packing can effectively enhance trainingefficiency while maintaining performance, (2) the suitable size of the modeland dataset for fine-tuning with the packing method, and (3) whether packingunrelated or related training samples might cause the model to eitherexcessively disregard or over-rely on the context. In this paper, we perform extensive comparisons between SFT methods usingpadding and packing, covering SFT datasets ranging from 69K to 1.2M and modelsfrom 8B to 70B. This provides the first comprehensive analysis of theadvantages and limitations of packing versus padding, as well as practicalconsiderations for implementing packing in various training scenarios. Ouranalysis covers various benchmarks, including knowledge, reasoning, and coding,as well as GPT-based evaluations, time efficiency, and other fine-tuningparameters. We also open-source our code for fine-tuning and evaluation andprovide checkpoints fine-tuned on datasets of different sizes, aiming toadvance future research on packing methods. Code is available at:https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.</description><author>Shuhe Wang, Guoyin Wang, Jiwei Li, Eduard Hovy, Chen Guo</author><pubDate>Thu, 10 Oct 2024 16:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08081v1</guid></item><item><title>Paramanu: A Family of Novel Efficient Generative Foundation Language Models for Indian Languages</title><link>http://arxiv.org/abs/2401.18034v2</link><description>We present "Paramanu", a family of novel language models (LM) for Indianlanguages, consisting of auto-regressive monolingual, bilingual, andmultilingual models pretrained from scratch. Currently, it covers 10 languages(Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil,Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu). The modelsare pretrained on a single GPU with context size of 1024 and vary in size from13.29 million (M) to 367.5 M parameters. We proposed a RoPE embedding scalingmethod that enables us to pretrain language models from scratch at largersequence length context size than typical GPU memory permits. We alsointroduced a novel efficient Indic tokenizer, "mBharat", using a combination ofBPE and Unigram, achieving the least fertility score and the ability totokenize unseen languages in both the same script &amp; Roman script. We alsoproposed and performed language-specific tokenization for multilingual models &amp;domain-specific tokenization for monolingual models. To address the "curse ofmultilinguality" in our mParamanu model, we pretrained on comparable corporabased on typological grouping within the same script. Our findings show alanguage transfer phenomenon from low-resource to high-resource languageswithin languages of the same script &amp; typology. Human evaluations foropen-ended text generation demonstrated that Paramanu models outperformedseveral LLMs, despite being 20 to 64 times smaller. We createdinstruction-tuning datasets &amp; instruction-tuned our models on 23,000instructions in respective languages. Comparisons with multilingual LLMs acrossvarious benchmarks for natural language (NL) understanding, NL inference, &amp;reading comprehension highlight the advantages of our models; leads to theconclusion that high quality generative LM are possible without high amount ofcompute power &amp; enormous number of parameters.</description><author>Mitodru Niyogi, Arnab Bhattacharya</author><pubDate>Thu, 10 Oct 2024 16:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18034v2</guid></item><item><title>Multimodal Optimization with k-Cluster Big Bang-Big Crunch Algorithm and Postprocessing Methods for Identification and Quantification of Optima</title><link>http://arxiv.org/abs/2401.06153v2</link><description>Multimodal optimization is often encountered in engineering problems,especially when different and alternative solutions are sought. Evolutionaryalgorithms can efficiently tackle multimodal optimization thanks to theirfeatures such as the concept of population, exploration/exploitation, and beingsuitable for parallel computation. This paper investigates whether a less-knownoptimizer, the Big Bang-Big Crunch (BBBC) algorithm, is suitable for multimodaloptimization. We extended BBBC and propose k-BBBC, a clustering-basedmulti-modal optimizer. Additionally, we introduce two post-processing methodsto (i) identify the local optima in a set of retrieved solutions (i.e., apopulation), and (ii) quantify the number of correctly retrieved optima againstthe expected ones (i.e., success rate). Our results show that k-BBBC performswell even with problems having a large number of optima (tested on $379$optima) and high dimensionality (tested on $32$ decision variables), but itbecomes computationally too expensive for problems with many local optima(i.e., in the CEC'2013 benchmark set). Compared to other multimodaloptimization methods, it outperforms them in terms of accuracy (in both searchand objective space) and success rate (number of correctly retrieved optima)when tested on basic multimodal functions, especially when elitism is applied;however, it requires knowing the number of optima of a problem, which makes itsperformance decrease when tested on niching competition test CEC'2013. Lastly,we validated our proposed post-processing methods by comparing their successrate to the actual one: results suggest that these methods can be used toevaluate the performance of a multimodal optimization algorithm by correctlyidentifying optima and providing an indication of success -- without the needto know where the optima are located in the search space.</description><author>Kemal Erdem Yenin, Reha Oguz Sayin, Kuzey Arar, Kadir Kaan Atalay, Fabio Stroppa</author><pubDate>Thu, 10 Oct 2024 16:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06153v2</guid></item><item><title>miniCodeProps: a Minimal Benchmark for Proving Code Properties</title><link>http://arxiv.org/abs/2406.11915v2</link><description>AI agents have shown initial promise in automating mathematical theoremproving in proof assistants such as Lean. The same proof assistants can be usedto verify the correctness of code by pairing code with specifications andproofs that the specifications hold. Automating the writing of code,specifications, and proofs could lower the cost of verification, or,ambitiously, enable an AI agent to output safe, provably correct code. However,it remains unclear whether current neural theorem provers can automaticallyverify even relatively simple programs. We present miniCodeProps, a benchmarkof 201 program specifications in the Lean proof assistant, aimed at thesubproblem of automatically generating a proof for a provided program andspecification. miniCodeProps contains specifications about simple,self-contained programs (e.g., lists, natural numbers, binary trees) withvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient tobreak current LLM-based provers, with state-of-the-art methods showing promiseon the easy properties in miniCodeProps, yet failing to prove nearly all of themedium and hard properties. We publicly release miniCodeProps as a benchmarkfor furthering automated theorem proving in the context of formally verifiedcode.</description><author>Evan Lohn, Sean Welleck</author><pubDate>Thu, 10 Oct 2024 16:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11915v2</guid></item><item><title>CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images</title><link>http://arxiv.org/abs/2305.05314v3</link><description>The visual examination of tissue biopsy sections is fundamental for cancerdiagnosis, with pathologists analyzing sections at multiple magnifications todiscern tumor cells and their subtypes. However, existing attention-basedmultiple instance learning (MIL) models used for analyzing Whole Slide Images(WSIs) in cancer diagnostics often overlook the contextual information of tumorand neighboring tiles, leading to misclassifications. To address this, wepropose the Context-Aware Multiple Instance Learning (CAMIL) architecture.CAMIL incorporates neighbor-constrained attention to consider dependenciesamong tiles within a WSI and integrates contextual constraints as priorknowledge into the MIL model. We evaluated CAMIL on subtyping non-small celllung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17)metastasis, achieving test AUCs of 97.5\%, 95.9\%, and 88.1\%, respectively,outperforming other state-of-the-art methods. Additionally, CAMIL enhancesmodel interpretability by identifying regions of high diagnostic value.</description><author>Olga Fourkioti, Matt De Vries, Chen Jin, Daniel C. Alexander, Chris Bakal</author><pubDate>Thu, 10 Oct 2024 16:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05314v3</guid></item><item><title>Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</title><link>http://arxiv.org/abs/2410.08074v1</link><description>Text-to-image diffusion models rely on massive, web-scale datasets. Trainingthem from scratch is computationally expensive, and as a result, developersoften prefer to make incremental updates to existing models. These updatesoften compose fine-tuning steps (to learn new concepts or improve modelperformance) with "unlearning" steps (to "forget" existing concepts, such ascopyrighted works or explicit content). In this work, we demonstrate a criticaland previously unknown vulnerability that arises in this paradigm: even underbenign, non-adversarial conditions, fine-tuning a text-to-image diffusion modelon seemingly unrelated images can cause it to "relearn" concepts that werepreviously "unlearned." We comprehensively investigate the causes and scope ofthis phenomenon, which we term concept resurgence, by performing a series ofexperiments which compose "mass concept erasure" (the current state of the artfor unlearning in text-to-image diffusion models (Lu et al., 2024)) withsubsequent fine-tuning of Stable Diffusion v1.4. Our findings underscore thefragility of composing incremental model updates, and raise serious newconcerns about current approaches to ensuring the safety and alignment oftext-to-image diffusion models.</description><author>Vinith M. Suriyakumar, Rohan Alur, Ayush Sekhari, Manish Raghavan, Ashia C. Wilson</author><pubDate>Thu, 10 Oct 2024 16:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08074v1</guid></item><item><title>Injective flows for star-like manifolds</title><link>http://arxiv.org/abs/2406.09116v2</link><description>Normalizing Flows (NFs) are powerful and efficient models for densityestimation. When modeling densities on manifolds, NFs can be generalized toinjective flows but the Jacobian determinant becomes computationallyprohibitive. Current approaches either consider bounds on the log-likelihood orrely on some approximations of the Jacobian determinant. In contrast, wepropose injective flows for star-like manifolds and show that for suchmanifolds we can compute the Jacobian determinant exactly and efficiently, withthe same cost as NFs. This aspect is particularly relevant for variationalinference settings, where no samples are available and only some unnormalizedtarget is known. Among many, we showcase the relevance of modeling densities onstar-like manifolds in two settings. Firstly, we introduce a novel ObjectiveBayesian approach for penalized likelihood models by interpreting level-sets ofthe penalty as star-like manifolds. Secondly, we consider probabilistic mixingmodels and introduce a general method for variational inference by defining theposterior of mixture weights on the probability simplex.</description><author>Marcello Massimo Negri, Jonathan Aellen, Volker Roth</author><pubDate>Thu, 10 Oct 2024 16:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09116v2</guid></item><item><title>PromptFix: You Prompt and We Fix the Photo</title><link>http://arxiv.org/abs/2405.16785v2</link><description>Diffusion models equipped with language models demonstrate excellentcontrollability in image generation tasks, allowing image processing to adhereto human instructions. However, the lack of diverse instruction-following datahampers the development of models that effectively recognize and executeuser-customized instructions, particularly in low-level tasks. Moreover, thestochastic nature of the diffusion process leads to deficiencies in imagegeneration or editing tasks that require the detailed preservation of thegenerated images. To address these limitations, we propose PromptFix, acomprehensive framework that enables diffusion models to follow humaninstructions to perform a wide variety of image-processing tasks. First, weconstruct a large-scale instruction-following dataset that covers comprehensiveimage-processing tasks, including low-level tasks, image editing, and objectcreation. Next, we propose a high-frequency guidance sampling method toexplicitly control the denoising process and preserve high-frequency details inunprocessed areas. Finally, we design an auxiliary prompting adapter, utilizingVision-Language Models (VLMs) to enhance text prompts and improve the model'stask generalization. Experimental results show that PromptFix outperformsprevious methods in various image-processing tasks. Our proposed model alsoachieves comparable inference efficiency with these baseline models andexhibits superior zero-shot capabilities in blind restoration and combinationtasks. The dataset and code are available athttps://www.yongshengyu.com/PromptFix-Page.</description><author>Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, Jiebo Luo</author><pubDate>Thu, 10 Oct 2024 16:09:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16785v2</guid></item><item><title>Morphing Tokens Draw Strong Masked Image Models</title><link>http://arxiv.org/abs/2401.00254v3</link><description>Masked image modeling (MIM) has emerged as a promising approach for trainingVision Transformers (ViTs). The essence of MIM lies in the token-wiseprediction of masked tokens, which aims to predict targets tokenized fromimages or generated by pre-trained models like vision-language models. Whileusing tokenizers or pre-trained models are plausible MIM targets, they oftenoffer spatially inconsistent targets even for neighboring tokens, complicatingmodels to learn unified and discriminative representations. Our pilot studyidentifies spatial inconsistencies and suggests that resolving them canaccelerate representation learning. Building upon this insight, we introduce anovel self-supervision signal called Dynamic Token Morphing (DTM), whichdynamically aggregates contextually related tokens to yield contextualizedtargets, thereby mitigating spatial inconsistency. DTM is compatible withvarious SSL frameworks; we showcase improved MIM results by employing DTM,barely introducing extra training costs. Our method facilitates training byusing consistent targets, resulting in 1) faster training and 2) reducedlosses. Experiments on ImageNet-1K and ADE20K demonstrate the superiority ofour method compared with state-of-the-art, complex MIM methods. Furthermore,the comparative evaluation of the iNaturalists and fine-grained visualclassification datasets further validates the transferability of our method onvarious downstream tasks. Code is available at https://github.com/naver-ai/dtm</description><author>Taekyung Kim, Byeongho Heo, Dongyoon Han</author><pubDate>Thu, 10 Oct 2024 16:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00254v3</guid></item><item><title>DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization</title><link>http://arxiv.org/abs/2302.06961v5</link><description>Accurate fovea localization is essential for analyzing retinal diseases toprevent irreversible vision loss. While current deep learning-based methodsoutperform traditional ones, they still face challenges such as the lack oflocal anatomical landmarks around the fovea, the inability to robustly handlediseased retinal images, and the variations in image conditions. In this paper,we propose a novel transformer-based architecture called DualStreamFoveaNet(DSFN) for multi-cue fusion. This architecture explicitly incorporateslong-range connections and global features using retina and vesseldistributions for robust fovea localization. We introduce a spatial attentionmechanism in the dual-stream encoder to extract and fuse self-learnedanatomical information, focusing more on features distributed along bloodvessels and significantly reducing computational costs by decreasing tokennumbers. Our extensive experiments show that the proposed architecture achievesstate-of-the-art performance on two public datasets and one large-scale privatedataset. Furthermore, we demonstrate that the DSFN is more robust on bothnormal and diseased retina images and has better generalization capacity incross-dataset experiments.</description><author>Sifan Song, Jinfeng Wang, Zilong Wang, Hongxing Wang, Jionglong Su, Xiaowei Ding, Kang Dang</author><pubDate>Thu, 10 Oct 2024 16:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06961v5</guid></item><item><title>Gaussian Process Thompson Sampling via Rootfinding</title><link>http://arxiv.org/abs/2410.08071v1</link><description>Thompson sampling (TS) is a simple, effective stochastic policy in Bayesiandecision making. It samples the posterior belief about the reward profile andoptimizes the sample to obtain a candidate decision. In continuousoptimization, the posterior of the objective function is often a Gaussianprocess (GP), whose sample paths have numerous local optima, making theirglobal optimization challenging. In this work, we introduce an efficient globaloptimization strategy for GP-TS that carefully selects starting points forgradient-based multi-start optimizers. It identifies all local optima of theprior sample via univariate global rootfinding, and optimizes the posteriorsample using a differentiable, decoupled representation. We demonstrateremarkable improvement in the global optimization of GP posterior samples,especially in high dimensions. This leads to dramatic improvements in theoverall performance of Bayesian optimization using GP-TS acquisition functions,surprisingly outperforming alternatives like GP-UCB and EI.</description><author>Taiwo A. Adebiyi, Bach Do, Ruda Zhang</author><pubDate>Thu, 10 Oct 2024 16:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08071v1</guid></item><item><title>Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models</title><link>http://arxiv.org/abs/2405.02162v3</link><description>In the field of robotics and computer vision, efficient and accurate semanticmapping remains a significant challenge due to the growing demand forintelligent machines that can comprehend and interact with complexenvironments. Conventional panoptic mapping methods, however, are limited bypredefined semantic classes, thus making them ineffective for handling novel orunforeseen objects. In response to this limitation, we introduce the UnifiedPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances infoundation models to enable real-time, on-demand label generation using naturallanguage prompts. By incorporating a dynamic labeling strategy into traditionalpanoptic mapping techniques, UPPM provides significant improvements inadaptability and versatility while maintaining high performance levels in mapreconstruction. We demonstrate our approach on real-world and simulateddatasets. Results show that UPPM can accurately reconstruct scenes and segmentobjects while generating rich semantic labels through natural languageinteractions. A series of ablation experiments validated the advantages offoundation model-based labeling over fixed label sets.</description><author>Mohamad Al Mdfaa, Raghad Salameh, Sergey Zagoruyko, Gonzalo Ferrer</author><pubDate>Thu, 10 Oct 2024 16:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02162v3</guid></item><item><title>Sparse Attention Decomposition Applied to Circuit Tracing</title><link>http://arxiv.org/abs/2410.00340v2</link><description>Many papers have shown that attention heads work in conjunction with eachother to perform complex tasks. It's frequently assumed that communicationbetween attention heads is via the addition of specific features to tokenresiduals. In this work we seek to isolate and identify the features used toeffect communication and coordination among attention heads in GPT-2 small. Ourkey leverage on the problem is to show that these features are very oftensparsely coded in the singular vectors of attention head matrices. Wecharacterize the dimensionality and occurrence of these signals across theattention heads in GPT-2 small when used for the Indirect Object Identification(IOI) task. The sparse encoding of signals, as provided by attention headsingular vectors, allows for efficient separation of signals from the residualbackground and straightforward identification of communication paths betweenattention heads. We explore the effectiveness of this approach by tracingportions of the circuits used in the IOI task. Our traces reveal considerabledetail not present in previous studies, shedding light on the nature ofredundant paths present in GPT-2. And our traces go beyond previous work byidentifying features used to communicate between attention heads whenperforming IOI.</description><author>Gabriel Franco, Mark Crovella</author><pubDate>Thu, 10 Oct 2024 16:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00340v2</guid></item><item><title>Unlearning-based Neural Interpretations</title><link>http://arxiv.org/abs/2410.08069v1</link><description>Gradient-based interpretations often require an anchor point of comparison toavoid saturation in computing feature importance. We show that currentbaselines defined using static functions--constant mapping, averaging orblurring--inject harmful colour, texture or frequency assumptions that deviatefrom model behaviour. This leads to accumulation of irregular gradients,resulting in attribution maps that are biased, fragile and manipulable.Departing from the static approach, we propose UNI to compute an (un)learnable,debiased and adaptive baseline by perturbing the input towards an unlearningdirection of steepest ascent. Our method discovers reliable baselines andsucceeds in erasing salient features, which in turn locally smooths thehigh-curvature decision boundaries. Our analyses point to unlearning as apromising avenue for generating faithful, efficient and robust interpretations.</description><author>Ching Lam Choi, Alexandre Duplessis, Serge Belongie</author><pubDate>Thu, 10 Oct 2024 16:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08069v1</guid></item><item><title>Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2410.08068v1</link><description>Large Language Models (LLMs) exhibit impressive performance across variousdomains but still struggle with arithmetic reasoning tasks. Recent work showsthe effectiveness of prompt design methods in enhancing reasoning capabilities.However, these approaches overlook crucial requirements for prior knowledge ofspecific concepts, theorems, and tricks to tackle most arithmetic reasoningproblems successfully. To address this issue, we propose a novel and effectiveTeaching-Inspired Integrated Framework, which emulates the instructionalprocess of a teacher guiding students. This method equips LLMs with essentialconcepts, relevant theorems, and similar problems with analogous solutionapproaches, facilitating the enhancement of reasoning abilities. Additionally,we introduce two new Chinese datasets, MathMC and MathToF, both with detailedexplanations and answers. Experiments are conducted on nine benchmarks whichdemonstrates that our approach improves the reasoning accuracy of LLMs. WithGPT-4 and our framework, we achieve new state-of-the-art performance on fourmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code areavailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting.</description><author>Wenting Tan, Dongxiao Chen, Jieting Xue, Zihao Wang, Taijie Chen</author><pubDate>Thu, 10 Oct 2024 16:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08068v1</guid></item><item><title>DAPE: Data-Adaptive Positional Encoding for Length Extrapolation</title><link>http://arxiv.org/abs/2405.14722v5</link><description>Positional encoding plays a crucial role in transformers, significantlyimpacting model performance and length generalization. Prior research hasintroduced absolute positional encoding (APE) and relative positional encoding(RPE) to distinguish token positions in given sequences. However, both APE andRPE remain fixed after model training regardless of input data, limiting theiradaptability and flexibility. Hence, we expect that the desired positionalencoding should be data-adaptive and can be dynamically adjusted with the givenattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)method, which dynamically and semantically adjusts based on input context andlearned fixed priors. Experimental validation on real-world datasets (Arxiv,Books3, and CHE) demonstrates that DAPE enhances model performances in terms oftrained length and length generalization, where the improvements arestatistically significant. The model visualization suggests that our model cankeep both local and anti-local information. Finally, we successfully train themodel on sequence length 128 and achieve better performance at evaluationsequence length 8192, compared with other static positional encoding methods,revealing the benefit of the adaptive positional encoding method.</description><author>Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li</author><pubDate>Thu, 10 Oct 2024 16:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14722v5</guid></item><item><title>Reward-Augmented Data Enhances Direct Preference Alignment of LLMs</title><link>http://arxiv.org/abs/2410.08067v1</link><description>Preference alignment in Large Language Models (LLMs) has significantlyimproved their ability to adhere to human instructions and intentions. However,existing direct alignment algorithms primarily focus on relative preferencesand often overlook the qualitative aspects of responses. Striving to maximizethe implicit reward gap between the chosen and the slightly inferior rejectedresponses can cause overfitting and unnecessary unlearning of the high-qualityrejected responses. The unawareness of the reward scores also drives the LLM toindiscriminately favor the low-quality chosen responses and fail to generalizeto responses with the highest rewards, which are sparse in data. To overcomethese shortcomings, our study introduces reward-conditioned LLM policies thatdiscern and learn from the entire spectrum of response quality within thedataset, helping extrapolate to more optimal regions. We propose an effectiveyet simple data relabeling method that conditions the preference pairs onquality scores to construct a reward-augmented dataset. This dataset is easilyintegrated with existing direct alignment algorithms and is applicable to anypreference dataset. The experimental results across instruction-followingbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate thatour approach consistently boosts the performance of DPO by a considerablemargin across diverse models. Additionally, our method improves the averageaccuracy on various academic benchmarks. When applying our method to on-policydata, the resulting DPO model achieves SOTA results on AlpacaEval. Throughablation studies, we demonstrate that our method not only maximizes the utilityof preference data but also mitigates the issue of unlearning, demonstratingits broad effectiveness beyond mere dataset expansion. Our code is available athttps://github.com/shenao-zhang/reward-augmented-preference.</description><author>Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang</author><pubDate>Thu, 10 Oct 2024 16:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08067v1</guid></item><item><title>Reversible Decoupling Network for Single Image Reflection Removal</title><link>http://arxiv.org/abs/2410.08063v1</link><description>Recent deep-learning-based approaches to single-image reflection removal haveshown promising advances, primarily for two reasons: 1) the utilization ofrecognition-pretrained features as inputs, and 2) the design of dual-streaminteraction networks. However, according to the Information Bottleneckprinciple, high-level semantic clues tend to be compressed or discarded duringlayer-by-layer propagation. Additionally, interactions in dual-stream networksfollow a fixed pattern across different layers, limiting overall performance.To address these limitations, we propose a novel architecture called ReversibleDecoupling Network (RDNet), which employs a reversible encoder to securevaluable information while flexibly decoupling transmission- andreflection-relevant features during the forward pass. Furthermore, we customizea transmission-rate-aware prompt generator to dynamically calibrate features,further boosting performance. Extensive experiments demonstrate the superiorityof RDNet over existing SOTA methods on five widely-adopted benchmark datasets.Our code will be made publicly available.</description><author>Hao Zhao, Mingjia Li, Qiming Hu, Xiaojie Guo</author><pubDate>Thu, 10 Oct 2024 15:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08063v1</guid></item></channel></rss>