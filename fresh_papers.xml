<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 13 Aug 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Iterative Reweighted Least Squares Networks With Convergence Guarantees for Solving Inverse Imaging Problems</title><link>http://arxiv.org/abs/2308.05745v1</link><description>In this work we present a novel optimization strategy for imagereconstruction tasks under analysis-based image regularization, which promotessparse and/or low-rank solutions in some learned transform domain. Weparameterize such regularizers using potential functions that correspond toweighted extensions of the $\ell_p^p$-vector and $\mathcal{S}_p^p$Schatten-matrix quasi-norms with $0 &lt; p \le 1$. Our proposed minimizationstrategy extends the Iteratively Reweighted Least Squares (IRLS) method,typically used for synthesis-based $\ell_p$ and $\mathcal{S}_p$ norm andanalysis-based $\ell_1$ and nuclear norm regularization. We prove that undermild conditions our minimization algorithm converges linearly to a stationarypoint, and we provide an upper bound for its convergence rate. Further, toselect the parameters of the regularizers that deliver the best results for theproblem at hand, we propose to learn them from training data by formulating thesupervised learning process as a stochastic bilevel optimization problem. Weshow that thanks to the convergence guarantees of our proposed minimizationstrategy, such optimization can be successfully performed with amemory-efficient implicit back-propagation scheme. We implement our learnedIRLS variants as recurrent networks and assess their performance on thechallenging image reconstruction tasks of non-blind deblurring,super-resolution and demosaicking. The comparisons against other existinglearned reconstruction approaches demonstrate that our overall method is verycompetitive and in many cases outperforms existing unrolled networks, whosenumber of parameters is orders of magnitude higher than in our case.</description><author>Iaroslav Koshelev, Stamatios Lefkimmiatis</author><pubDate>Thu, 10 Aug 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05745v1</guid></item><item><title>PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs</title><link>http://arxiv.org/abs/2308.05744v1</link><description>In this paper, we develop a new method to automatically convert 2D linedrawings from three orthographic views into 3D CAD models. Existing methods forthis problem reconstruct 3D models by back-projecting the 2D observations into3D space while maintaining explicit correspondence between the input andoutput. Such methods are sensitive to errors and noises in the input, thusoften fail in practice where the input drawings created by human designers areimperfect. To overcome this difficulty, we leverage the attention mechanism ina Transformer-based sequence generation model to learn flexible mappingsbetween the input and output. Further, we design shape programs which aresuitable for generating the objects of interest to boost the reconstructionaccuracy and facilitate CAD modeling applications. Experiments on a newbenchmark dataset show that our method significantly outperforms existing oneswhen the inputs are noisy or incomplete.</description><author>Wentao Hu, Jia Zheng, Zixin Zhang, Xiaojun Yuan, Jian Yin, Zihan Zhou</author><pubDate>Thu, 10 Aug 2023 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05744v1</guid></item><item><title>Neural Progressive Meshes</title><link>http://arxiv.org/abs/2308.05741v1</link><description>The recent proliferation of 3D content that can be consumed on hand-helddevices necessitates efficient tools for transmitting large geometric data,e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose achallenge to storage as well as transmission bandwidth, and level-of-detailtechniques are often used to transmit an asset using an appropriate bandwidthbudget. It is especially desirable for these methods to transmit dataprogressively, improving the quality of the geometry with more data. Our keyinsight is that the geometric details of 3D meshes often exhibit similar localpatterns even across different shapes, and thus can be effectively representedwith a shared learned generative space. We learn this space using asubdivision-based encoder-decoder architecture trained in advance on a largecollection of surfaces. We further observe that additional residual featurescan be transmitted progressively between intermediate levels of subdivisionthat enable the client to control the tradeoff between bandwidth cost andquality of reconstruction, providing a neural progressive mesh representation.We evaluate our method on a diverse set of complex 3D shapes and demonstratethat it outperforms baselines in terms of compression ratio and reconstructionquality.</description><author>Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</author><pubDate>Thu, 10 Aug 2023 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05741v1</guid></item><item><title>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</title><link>http://arxiv.org/abs/2308.05739v1</link><description>Gradient-based optimization is now ubiquitous across graphics, butunfortunately can not be applied to problems with undefined or zero gradients.To circumvent this issue, the loss function can be manually replaced by a"surrogate" that has similar minima but is differentiable. Our proposedframework, ZeroGrads, automates this process by learning a neural approximationof the objective function, the surrogate, which in turn can be used todifferentiate through arbitrary black-box graphics pipelines. We train thesurrogate on an actively smoothed version of the objective and encouragelocality, focusing the surrogate's capacity on what matters at the currenttraining episode. The fitting is performed online, alongside the parameteroptimization, and self-supervised, without pre-computed data or pre-trainedmodels. As sampling the objective is expensive (it requires a full rendering orsimulator run), we devise an efficient sampling scheme that allows fortractable run-times and competitive performance at little overhead. Wedemonstrate optimizing diverse non-convex, non-differentiable black-boxproblems in graphics, such as visibility in rendering, discrete parameterspaces in procedural modelling or optimal control in physics-driven animation.In contrast to more traditional algorithms, our approach scales well to higherdimensions, which we demonstrate on problems with up to 35k interlinkedvariables.</description><author>Michael Fischer, Tobias Ritschel</author><pubDate>Thu, 10 Aug 2023 18:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05739v1</guid></item><item><title>Follow Anything: Open-set detection, tracking, and following in real-time</title><link>http://arxiv.org/abs/2308.05737v1</link><description>Tracking and following objects of interest is critical to several roboticsuse cases, ranging from industrial automation to logistics and warehousing, tohealthcare and security. In this paper, we present a robotic system to detect,track, and follow any object in real-time. Our approach, dubbed ``followanything'' (FAn), is an open-vocabulary and multimodal model -- it is notrestricted to concepts seen at training time and can be applied to novelclasses at inference time using text, images, or click queries. Leveraging richvisual descriptors from large-scale pre-trained models (foundation models), FAncan detect and segment objects by matching multimodal queries (text, images,clicks) against an input image sequence. These detected and segmented objectsare tracked across image frames, all while accounting for occlusion and objectre-emergence. We demonstrate FAn on a real-world robotic system (a micro aerialvehicle) and report its ability to seamlessly follow the objects of interest ina real-time control loop. FAn can be deployed on a laptop with a lightweight(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. Toenable rapid adoption, deployment, and extensibility, we open-source all ourcode on our project webpage at https://github.com/alaamaalouf/FollowAnything .We also encourage the reader the watch our 5-minutes explainer video in thishttps://www.youtube.com/watch?v=6Mgt3EPytrw .</description><author>Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</author><pubDate>Thu, 10 Aug 2023 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05737v1</guid></item><item><title>MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction</title><link>http://arxiv.org/abs/2308.05736v1</link><description>High-definition (HD) map provides abundant and precise static environmentalinformation of the driving scene, serving as a fundamental and indispensablecomponent for planning in autonomous driving system. In this paper, we present\textbf{Map} \textbf{TR}ansformer, an end-to-end framework for onlinevectorized HD map construction. We propose a unified permutation-equivalentmodeling approach, \ie, modeling map element as a point set with a group ofequivalent permutations, which accurately describes the shape of map elementand stabilizes the learning process. We design a hierarchical query embeddingscheme to flexibly encode structured map information and perform hierarchicalbipartite matching for map element learning. To speed up convergence, wefurther introduce auxiliary one-to-many matching and dense supervision. Theproposed method well copes with various map elements with arbitrary shapes. Itruns at real-time inference speed and achieves state-of-the-art performance onboth nuScenes and Argoverse2 datasets. Abundant qualitative results show stableand robust map construction quality in complex and various driving scenes. Codeand more demos are available at \url{https://github.com/hustvl/MapTR} forfacilitating further studies and applications.</description><author>Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu, Chang Huang, Xinggang Wang</author><pubDate>Thu, 10 Aug 2023 18:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05736v1</guid></item><item><title>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</title><link>http://arxiv.org/abs/2308.05734v1</link><description>Although audio generation shares commonalities across different types ofaudio, such as speech, music, and sound effects, designing models for each typerequires careful consideration of specific objectives and biases that cansignificantly differ from those of other types. To bring us closer to a unifiedperspective of audio generation, this paper proposes a framework that utilizesthe same learning method for speech, music, and sound effect generation. Ourframework introduces a general representation of audio, called language ofaudio (LOA). Any audio can be translated into LOA based on AudioMAE, aself-supervised pre-trained representation learning model. In the generationprocess, we translate any modalities into LOA by using a GPT-2 model, and weperform self-supervised audio generation learning with a latent diffusion modelconditioned on LOA. The proposed framework naturally brings advantages such asin-context learning abilities and reusable self-supervised pretrained AudioMAEand latent diffusion models. Experiments on the major benchmarks oftext-to-audio, text-to-music, and text-to-speech demonstrate newstate-of-the-art or competitive performance to previous approaches. Our demoand code are available at https://audioldm.github.io/audioldm2.</description><author>Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</author><pubDate>Thu, 10 Aug 2023 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05734v1</guid></item><item><title>FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models</title><link>http://arxiv.org/abs/2308.05733v1</link><description>3D scene reconstruction is a long-standing vision task. Existing approachescan be categorized into geometry-based and learning-based methods. The formerleverages multi-view geometry but can face catastrophic failures due to thereliance on accurate pixel correspondence across views. The latter wasproffered to mitigate these issues by learning 2D or 3D representationdirectly. However, without a large-scale video or 3D training data, it canhardly generalize to diverse real-world scenarios due to the presence of tensof millions or even billions of optimization parameters in the deep network.Recently, robust monocular depth estimation models trained with large-scaledatasets have been proven to possess weak 3D geometry prior, but they areinsufficient for reconstruction due to the unknown camera parameters, theaffine-invariant property, and inter-frame inconsistency. Here, we propose anovel test-time optimization approach that can transfer the robustness ofaffine-invariant depth models such as LeReS to challenging diverse scenes whileensuring inter-frame consistency, with only dozens of parameters to optimizeper video frame. Specifically, our approach involves freezing the pre-trainedaffine-invariant depth model's depth predictions, rectifying them by optimizingthe unknown scale-shift values with a geometric consistency alignment module,and employing the resulting scale-consistent depth maps to robustly obtaincamera poses and achieve dense scene reconstruction, even in low-textureregions. Experiments show that our method achieves state-of-the-artcross-dataset reconstruction on five zero-shot testing datasets.</description><author>Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Cheng, Feng Zhao</author><pubDate>Thu, 10 Aug 2023 18:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05733v1</guid></item><item><title>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</title><link>http://arxiv.org/abs/2308.05732v1</link><description>Time-dependent partial differential equations (PDEs) are ubiquitous inscience and engineering. Recently, mostly due to the high computational cost oftraditional solution techniques, deep neural network based surrogates havegained increased interest. The practical utility of such neural PDE solversrelies on their ability to provide accurate, stable predictions over long timehorizons, which is a notoriously hard problem. In this work, we present alarge-scale analysis of common temporal rollout strategies, identifying theneglect of non-dominant spatial frequency information, often associated withhigh frequencies in PDE solutions, as the primary pitfall limiting stable,accurate rollout performance. Based on these insights, we draw inspiration fromrecent advances in diffusion models to introduce PDE-Refiner; a novel modelclass that enables more accurate modeling of all frequency components via amultistep refinement process. We validate PDE-Refiner on challenging benchmarksof complex fluid dynamics, demonstrating stable and accurate rollouts thatconsistently outperform state-of-the-art models, including neural, numerical,and hybrid neural-numerical architectures. We further demonstrate thatPDE-Refiner greatly enhances data efficiency, since the denoising objectiveimplicitly induces a novel form of spectral data augmentation. Finally,PDE-Refiner's connection to diffusion models enables an accurate and efficientassessment of the model's predictive uncertainty, allowing us to estimate whenthe surrogate becomes inaccurate.</description><author>Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter</author><pubDate>Thu, 10 Aug 2023 18:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05732v1</guid></item><item><title>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</title><link>http://arxiv.org/abs/2308.05731v1</link><description>Automated driving has the potential to revolutionize personal, public, andfreight mobility. Besides the enormous challenge of perception, i.e. accuratelyperceiving the environment using available sensor data, automated drivingcomprises planning a safe, comfortable, and efficient motion trajectory. Topromote safety and progress, many works rely on modules that predict the futuremotion of surrounding traffic. Modular automated driving systems commonlyhandle prediction and planning as sequential separate tasks. While thisaccounts for the influence of surrounding traffic on the ego-vehicle, it failsto anticipate the reactions of traffic participants to the ego-vehicle'sbehavior. Recent works suggest that integrating prediction and planning in aninterdependent joint step is necessary to achieve safe, efficient, andcomfortable driving. While various models implement such integrated systems, acomprehensive overview and theoretical understanding of different principlesare lacking. We systematically review state-of-the-art deep learning-basedprediction, planning, and integrated prediction and planning models. Differentfacets of the integration ranging from model architecture and model design tobehavioral aspects are considered and related to each other. Moreover, wediscuss the implications, strengths, and limitations of different integrationmethods. By pointing out research gaps, describing relevant future challenges,and highlighting trends in the research field, we identify promising directionsfor future research.</description><author>Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</author><pubDate>Thu, 10 Aug 2023 18:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05731v1</guid></item><item><title>InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing</title><link>http://arxiv.org/abs/2308.00135v3</link><description>Large text-to-image diffusion models have achieved remarkable success ingenerating diverse, high-quality images. Additionally, these models have beensuccessfully leveraged to edit input images by just changing the text prompt.But when these models are applied to videos, the main challenge is to ensuretemporal consistency and coherence across frames. In this paper, we proposeInFusion, a framework for zero-shot text-based video editing leveraging largepre-trained image diffusion models. Our framework specifically supports editingof multiple concepts with pixel-level control over diverse concepts mentionedin the editing prompt. Specifically, we inject the difference in featuresobtained with source and edit prompts from U-Net residual blocks of decoderlayers. When these are combined with injected attention features, it becomesfeasible to query the source contents and scale edited concepts along with theinjection of unedited parts. The editing is further controlled in afine-grained manner with mask extraction and attention fusion, which cut theedited part from the source and paste it into the denoising pipeline for theediting prompt. Our framework is a low-cost alternative to one-shot tunedmodels for editing since it does not require training. We demonstrated complexconcept editing with a generalised image model (Stable Diffusion v1.5) usingLoRA. Adaptation is compatible with all the existing image diffusiontechniques. Extensive experimental results demonstrate the effectiveness ofexisting methods in rendering high-quality and temporally consistent videos.</description><author>Anant Khandelwal</author><pubDate>Thu, 10 Aug 2023 18:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00135v3</guid></item><item><title>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</title><link>http://arxiv.org/abs/2308.05725v1</link><description>Recent work has shown that it is possible to resynthesize high-quality speechbased, not on text, but on low bitrate discrete units that have been learned ina self-supervised fashion and can therefore capture expressive aspects ofspeech that are hard to transcribe (prosody, voice styles, non-verbalvocalization). The adoption of these methods is still limited by the fact thatmost speech synthesis datasets are read, severely limiting spontaneity andexpressivity. Here, we introduce Expresso, a high-quality expressive speechdataset for textless speech synthesis that includes both read speech andimprovised dialogues rendered in 26 spontaneous expressive styles. Weillustrate the challenges and potentials of this dataset with an expressiveresynthesis benchmark where the task is to encode the input in low-bitrateunits and resynthesize it in a target voice while preserving content and style.We evaluate resynthesis quality with automatic metrics for differentself-supervised discrete encoders, and explore tradeoffs between quality,bitrate and invariance to speaker and style. All the dataset, evaluationmetrics and baseline models are open source</description><author>Tu Anh Nguyen, Wei-Ning Hsu, Antony D'Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</author><pubDate>Thu, 10 Aug 2023 18:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05725v1</guid></item><item><title>Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions</title><link>http://arxiv.org/abs/2308.05724v1</link><description>Deep learning training training algorithms are a huge success in recent yearsin many fields including speech, text,image video etc. Deeper and deeper layersare proposed with huge success with resnet structures having around 152 layers.Shallow convolution neural networks(CNN's) are still an active research, wheresome phenomena are still unexplained. Activation functions used in the networkare of utmost importance, as they provide non linearity to the networks. Relu'sare the most commonly used activation function.We show a complex piece-wiselinear(PWL) activation in the hidden layer. We show that these PWL activationswork much better than relu activations in our networks for convolution neuralnetworks and multilayer perceptrons. Result comparison in PyTorch for shallowand deep CNNs are given to further strengthen our case.</description><author>Chinmay Rane, Kanishka Tyagi, Michael Manry</author><pubDate>Thu, 10 Aug 2023 18:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05724v1</guid></item><item><title>Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction</title><link>http://arxiv.org/abs/2308.05721v1</link><description>CNNs and Transformers have their own advantages and both have been widelyused for dense prediction in multi-task learning (MTL). Most of the currentstudies on MTL solely rely on CNN or Transformer. In this work, we present anovel MTL model by combining both merits of deformable CNN and query-basedTransformer with shared gating for multi-task learning of dense prediction.This combination may offer a simple and efficient solution owing to itspowerful and flexible task-specific learning and advantages of lower cost, lesscomplexity and smaller parameters than the traditional MTL methods. Weintroduce deformable mixer Transformer with gating (DeMTG), a simple andeffective encoder-decoder architecture up-to-date that incorporates theconvolution and attention mechanism in a unified network for MTL. It isexquisitely designed to use advantages of each block, and provide deformableand comprehensive features for all tasks from local and global perspective.First, the deformable mixer encoder contains two types of operators: thechannel-aware mixing operator leveraged to allow communication among differentchannels, and the spatial-aware deformable operator with deformable convolutionapplied to efficiently sample more informative spatial locations. Second, thetask-aware gating transformer decoder is used to perform the task-specificpredictions, in which task interaction block integrated with self-attention isapplied to capture task interaction features, and the task query blockintegrated with gating attention is leveraged to select correspondingtask-specific features. Further, the experiment results demonstrate that theproposed DeMTG uses fewer GFLOPs and significantly outperforms currentTransformer-based and CNN-based competitive models on a variety of metrics onthree dense prediction datasets. Our code and models are available athttps://github.com/yangyangxu0/DeMTG.</description><author>Yangyang Xu, Yibo Yang, Bernard Ghanemm, Lefei Zhang</author><pubDate>Thu, 10 Aug 2023 18:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05721v1</guid></item><item><title>Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse</title><link>http://arxiv.org/abs/2308.03880v2</link><description>Online violence against children has increased globally recently, demandingurgent attention. Competent authorities manually analyze abuse complaints tocomprehend crime dynamics and identify patterns. However, the manual analysisof these complaints presents a challenge because it exposes analysts to harmfulcontent during the review process. Given these challenges, we present a novelsolution, an automated tool designed to analyze children's sexual abuse reportscomprehensively. By automating the analysis process, our tool significantlyreduces the risk of exposure to harmful content by categorizing the reports onthree dimensions: Subject, Degree of Criminality, and Damage. Furthermore,leveraging our multidisciplinary team's expertise, we introduce a novelapproach to annotate the collected data, enabling a more in-depth analysis ofthe reports. This approach improves the comprehension of fundamental patternsand trends, enabling law enforcement agencies and policymakers to createfocused strategies in the fight against children's violence.</description><author>Juanita Puentes, Angela Castillo, Wilmar Osejo, Yuly Calderón, Viviana Quintero, Lina Saldarriaga, Diana Agudelo, Pablo Arbeláez</author><pubDate>Thu, 10 Aug 2023 18:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03880v2</guid></item><item><title>RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks</title><link>http://arxiv.org/abs/2301.10822v2</link><description>The state-of-the-art predictive maintenance (PdM) techniques have shown greatsuccess in reducing maintenance costs and downtime of complicated machineswhile increasing overall productivity through extensive utilization ofInternet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors andDL algorithms are both prone to cyber-attacks. For instance, DL algorithms areknown for their susceptibility to adversarial examples. Such adversarialattacks are vastly under-explored in the PdM domain. This is because theadversarial attacks in the computer vision domain for classification taskscannot be directly applied to the PdM domain for multivariate time series (MTS)regression tasks. In this work, we propose an end-to-end methodology to designadversarially robust PdM systems by extensively analyzing the effect ofdifferent types of adversarial attacks and proposing a novel adversarialdefense technique for DL-enabled PdM models. First, we propose novel MTSProjected Gradient Descent (PGD) and MTS PGD with random restarts (PGD_r)attacks. Then, we evaluate the impact of MTS PGD and PGD_r along with MTS FastGradient Sign Method (FGSM) and MTS Basic Iterative Method (BIM) on LongShort-Term Memory (LSTM), Gated Recurrent Unit (GRU), Convolutional NeuralNetwork (CNN), and Bi-directional LSTM based PdM system. Our results usingNASA's turbofan engine dataset show that adversarial attacks can cause a severedefect (up to 11X) in the RUL prediction, outperforming the effectiveness ofthe state-of-the-art PdM attacks by 3X. Furthermore, we present a novelapproximate adversarial training method to defend against adversarial attacks.We observe that approximate adversarial training can significantly improve therobustness of PdM models (up to 54X) and outperforms the state-of-the-art PdMdefense methods by offering 3X more robustness.</description><author>Ayesha Siddique, Ripan Kumar Kundu, Gautam Raj Mode, Khaza Anuarul Hoque</author><pubDate>Thu, 10 Aug 2023 18:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10822v2</guid></item><item><title>Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection</title><link>http://arxiv.org/abs/2303.14961v3</link><description>As the use of machine learning continues to expand, the importance ofensuring its safety cannot be overstated. A key concern in this regard is theability to identify whether a given sample is from the training distribution,or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries canmanipulate OOD samples in ways that lead a classifier to make a confidentprediction. In this study, we present a novel approach for certifying therobustness of OOD detection within a $\ell_2$-norm around the input, regardlessof network architecture and without the need for specific components oradditional training. Further, we improve current techniques for detectingadversarial attacks on OOD samples, while providing high levels of certifiedand adversarial robustness on in-distribution samples. The average of all OODdetection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$relative to previous approaches.</description><author>Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann</author><pubDate>Thu, 10 Aug 2023 18:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14961v3</guid></item><item><title>Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems</title><link>http://arxiv.org/abs/2308.05713v1</link><description>This report describes a test of the large language model GPT-4 with theWolfram Alpha and the Code Interpreter plug-ins on 105 original problems inscience and math, at the high school and college levels, carried out inJune-August 2023. Our tests suggest that the plug-ins significantly enhanceGPT's ability to solve these problems. Having said that, there are still often"interface" failures; that is, GPT often has trouble formulating problems in away that elicits useful answers from the plug-ins. Fixing these interfacefailures seems like a central challenge in making GPT a reliable tool forcollege-level calculation problems.</description><author>Ernest Davis, Scott Aaronson</author><pubDate>Thu, 10 Aug 2023 18:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05713v1</guid></item><item><title>A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control</title><link>http://arxiv.org/abs/2308.05711v1</link><description>Reinforcement learning (RL) is a promising approach for optimizing HVACcontrol. RL offers a framework for improving system performance, reducingenergy consumption, and enhancing cost efficiency. We benchmark two popularclassical and deep RL methods (Q-Learning and Deep-Q-Networks) across multipleHVAC environments and explore the practical consideration of modelhyper-parameter selection and reward tuning. The findings provide insight forconfiguring RL agents in HVAC systems, promoting energy-efficient andcost-effective operation.</description><author>Marshall Wang, John Willes, Thomas Jiralerspong, Matin Moezzi</author><pubDate>Thu, 10 Aug 2023 18:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05711v1</guid></item><item><title>STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced Audio-Visual Diarization</title><link>http://arxiv.org/abs/2306.10608v2</link><description>This report introduces our novel method named STHG for the Audio-VisualDiarization task of the Ego4D Challenge 2023. Our key innovation is that wemodel all the speakers in a video using a single, unified heterogeneous graphlearning framework. Unlike previous approaches that require a separatecomponent solely for the camera wearer, STHG can jointly detect the speechactivities of all people including the camera wearer. Our final method obtains61.1% DER on the test set of Ego4D, which significantly outperforms all thebaselines as well as last year's winner. Our submission achieved 1st place inthe Ego4D Challenge 2023. We additionally demonstrate that applying theoff-the-shelf speech recognition system to the diarized speech segments by STHGproduces a competitive performance on the Speech Transcription task of thischallenge.</description><author>Kyle Min</author><pubDate>Thu, 10 Aug 2023 18:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10608v2</guid></item><item><title>Intel Labs at Ego4D Challenge 2022: A Better Baseline for Audio-Visual Diarization</title><link>http://arxiv.org/abs/2210.07764v2</link><description>This report describes our approach for the Audio-Visual Diarization (AVD)task of the Ego4D Challenge 2022. Specifically, we present multiple technicalimprovements over the official baselines. First, we improve the detectionperformance of the camera wearer's voice activity by modifying the trainingscheme of its model. Second, we discover that an off-the-shelf voice activitydetection model can effectively remove false positives when it is appliedsolely to the camera wearer's voice activities. Lastly, we show that betteractive speaker detection leads to a better AVD outcome. Our final methodobtains 65.9% DER on the test set of Ego4D, which significantly outperforms allthe baselines. Our submission achieved 1st place in the Ego4D Challenge 2022.</description><author>Kyle Min</author><pubDate>Thu, 10 Aug 2023 18:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07764v2</guid></item><item><title>Shadow Datasets, New challenging datasets for Causal Representation Learning</title><link>http://arxiv.org/abs/2308.05707v1</link><description>Discovering causal relations among semantic factors is an emergent topic inrepresentation learning. Most causal representation learning (CRL) methods arefully supervised, which is impractical due to costly labeling. To resolve thisrestriction, weakly supervised CRL methods were introduced. To evaluate CRLperformance, four existing datasets, Pendulum, Flow, CelebA(BEARD) andCelebA(SMILE), are utilized. However, existing CRL datasets are limited tosimple graphs with few generative factors. Thus we propose two new datasetswith a larger number of diverse generative factors and more sophisticatedcausal graphs. In addition, current real datasets, CelebA(BEARD) andCelebA(SMILE), the originally proposed causal graphs are not aligned with thedataset distributions. Thus, we propose modifications to them.</description><author>Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</author><pubDate>Thu, 10 Aug 2023 18:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05707v1</guid></item><item><title>Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving</title><link>http://arxiv.org/abs/2308.05701v1</link><description>In recent years there have been remarkable advancements in autonomousdriving. While autonomous vehicles demonstrate high performance in closed-setconditions, they encounter difficulties when confronted with unexpectedsituations. At the same time, world models emerged in the field of model-basedreinforcement learning as a way to enable agents to predict the futuredepending on potential actions. This led to outstanding results in sparsereward and complex control tasks. This work provides an overview of how worldmodels can be leveraged to perform anomaly detection in the domain ofautonomous driving. We provide a characterization of world models and relateindividual components to previous works in anomaly detection to facilitatefurther research in the field.</description><author>Daniel Bogdoll, Lukas Bosch, Tim Joseph, Helen Gremmelmaier, Yitian Yang, J. Marius Zöllner</author><pubDate>Thu, 10 Aug 2023 18:04:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05701v1</guid></item><item><title>AI-GOMS: Large AI-Driven Global Ocean Modeling System</title><link>http://arxiv.org/abs/2308.03152v2</link><description>Ocean modeling is a powerful tool for simulating the physical, chemical, andbiological processes of the ocean, which is the foundation for marine scienceresearch and operational oceanography. Modern numerical ocean modeling mainlyconsists of governing equations and numerical algorithms. Nonlinearinstability, computational expense, low reusability efficiency and highcoupling costs have gradually become the main bottlenecks for the furtherdevelopment of numerical ocean modeling. Recently, artificialintelligence-based modeling in scientific computing has shown revolutionarypotential for digital twins and scientific simulations, but the bottlenecks ofnumerical ocean modeling have not been further solved. Here, we presentAI-GOMS, a large AI-driven global ocean modeling system, for accurate andefficient global ocean daily prediction. AI-GOMS consists of a backbone modelwith the Fourier-based Masked Autoencoder structure for basic ocean variableprediction and lightweight fine-tuning models incorporating regionaldownscaling, wave decoding, and biochemistry coupling modules. AI-GOMS hasachieved the best performance in 30 days of prediction for the global oceanbasic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyondthe good performance in statistical metrics, AI-GOMS realizes the simulation ofmesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution andocean stratification in the tropical Pacific Ocean. AI-GOMS provides a newbackbone-downstream paradigm for Earth system modeling, which makes the systemtransferable, scalable and reusable.</description><author>Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomeng Huang</author><pubDate>Thu, 10 Aug 2023 18:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03152v2</guid></item><item><title>SSLRec: A Self-Supervised Learning Library for Recommendation</title><link>http://arxiv.org/abs/2308.05697v1</link><description>Self-supervised learning (SSL) has gained significant interest in recentyears as a solution to address the challenges posed by sparse and noisy data inrecommender systems. Despite the growing number of SSL algorithms designed toprovide state-of-the-art performance in various recommendation scenarios (e.g.,graph collaborative filtering, sequential recommendation, socialrecommendation, KG-enhanced recommendation), there is still a lack of unifiedframeworks that integrate recommendation algorithms across different domains.Such a framework could serve as the cornerstone for self-supervisedrecommendation algorithms, unifying the validation of existing methods anddriving the design of new ones. To address this gap, we introduce SSLRec, anovel benchmark platform that provides a standardized, flexible, andcomprehensive framework for evaluating various SSL-enhanced recommenders. TheSSLRec library features a modular architecture that allows users to easilyevaluate state-of-the-art models and a complete set of data augmentation andself-supervised toolkits to help create SSL recommendation models with specificneeds. Furthermore, SSLRec simplifies the process of training and evaluatingdifferent recommendation models with consistent and fair settings. Our SSLRecplatform covers a comprehensive set of state-of-the-art SSL-enhancedrecommendation models across different scenarios, enabling researchers toevaluate these cutting-edge models and drive further innovation in the field.Our implemented SSLRec framework is available at the source code repositoryhttps://github.com/HKUDS/SSLRec.</description><author>Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, Chao Huang</author><pubDate>Thu, 10 Aug 2023 17:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05697v1</guid></item><item><title>A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment</title><link>http://arxiv.org/abs/2308.05696v1</link><description>Training large language models (LLMs) with open-domain instruction data hasyielded remarkable success in aligning to end tasks and user preferences.Extensive research has highlighted that enhancing the quality and diversity ofinstruction data consistently improves performance. However, the impact of datacomplexity, as a crucial metric, remains relatively unexplored in threeaspects: (1) scaling law, where the sustainability of performance improvementswith increasing complexity is uncertain, (2) additional tokens, whether theimprovement brought by complexity comes from introducing more training tokens,and (3) curriculum tuning, where the potential advantages of incorporatinginstructions ranging from easy to difficult are not yet fully understood. Inthis paper, we propose \textit{tree-instruct} to systematically enhance thecomplexity of instruction data in a controllable manner. This approach adds aspecified number of nodes into the instruction semantic tree, yielding newinstruction data based on the modified tree. By adjusting the number of addednodes, we can control the difficulty level in the modified instruction data.Our preliminary experiments reveal the following insights: (1) Increasingcomplexity consistently leads to sustained performance improvements. Forinstance, using 1,000 instruction data and 10 nodes resulted in a substantial24\% increase in win rate. (2) Under the same token budget, a few complexinstructions outperform diverse yet simple instructions. (3) Curriculuminstruction tuning might not yield the anticipated results; focusing onincreasing complexity appears to be the key.</description><author>Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, Nevin L. Zhang</author><pubDate>Thu, 10 Aug 2023 17:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05696v1</guid></item><item><title>Masked Diffusion as Self-supervised Representation Learner</title><link>http://arxiv.org/abs/2308.05695v1</link><description>Denoising diffusion probabilistic models have recently demonstratedstate-of-the-art generative performance and been used as strong pixel-levelrepresentation learners. This paper decomposes the interrelation between thegenerative capability and representation learning ability inherent in diffusionmodels. We present masked diffusion model (MDM), a scalable self-supervisedrepresentation learner that substitutes the conventional additive Gaussiannoise of traditional diffusion with a masking mechanism. Our proposed approachconvincingly surpasses prior benchmarks, demonstrating remarkable advancementsin both medical and natural image semantic segmentation tasks, particularlywithin the context of few-shot scenario.</description><author>Zixuan Pan, Jianxu Chen, Yiyu Shi</author><pubDate>Thu, 10 Aug 2023 17:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05695v1</guid></item><item><title>Synthesizing Mixed-type Electronic Health Records using Diffusion Models</title><link>http://arxiv.org/abs/2302.14679v2</link><description>Electronic Health Records (EHRs) contain sensitive patient information, whichpresents privacy concerns when sharing such data. Synthetic data generation isa promising solution to mitigate these risks, often relying on deep generativemodels such as Generative Adversarial Networks (GANs). However, recent studieshave shown that diffusion models offer several advantages over GANs, such asgeneration of more realistic synthetic data and stable training in generatingdata modalities, including image, text, and sound. In this work, we investigatethe potential of diffusion models for generating realistic mixed-type tabularEHRs, comparing TabDDPM model with existing methods on four datasets in termsof data quality, utility, privacy, and augmentation. Our experimentsdemonstrate that TabDDPM outperforms the state-of-the-art models across allevaluation metrics, except for privacy, which confirms the trade-off betweenprivacy and utility.</description><author>Taha Ceritli, Ghadeer O. Ghosheh, Vinod Kumar Chauhan, Tingting Zhu, Andrew P. Creagh, David A. Clifton</author><pubDate>Thu, 10 Aug 2023 17:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14679v2</guid></item><item><title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title><link>http://arxiv.org/abs/2308.05681v1</link><description>Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.</description><author>Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</author><pubDate>Thu, 10 Aug 2023 17:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05681v1</guid></item><item><title>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</title><link>http://arxiv.org/abs/2308.05680v1</link><description>The task of retrieving already debunked narratives aims to detect storiesthat have already been fact-checked. The successful detection of claims thathave already been debunked not only reduces the manual efforts of professionalfact-checkers but can also contribute to slowing the spread of misinformation.Mainly due to the lack of readily available data, this is an understudiedproblem, particularly when considering the cross-lingual task, i.e. theretrieval of fact-checking articles in a language different from the languageof the online post being checked. This paper fills this gap by (i) creating anovel dataset to enable research on cross-lingual retrieval of already debunkednarratives, using tweets as queries to a database of fact-checking articles;(ii) presenting an extensive experiment to benchmark fine-tuned andoff-the-shelf multilingual pre-trained Transformer models for this task; and(iii) proposing a novel multistage framework that divides this cross-lingualdebunk retrieval task into refinement and re-ranking stages. Results show thatthe task of cross-lingual retrieval of already debunked narratives ischallenging and off-the-shelf Transformer models fail to outperform a stronglexical-based baseline (BM25). Nevertheless, our multistage retrieval frameworkis robust, outperforming BM25 in most scenarios and enabling cross-domain andzero-shot learning, without significantly harming the model's performance.</description><author>Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</author><pubDate>Thu, 10 Aug 2023 17:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05680v1</guid></item><item><title>Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission</title><link>http://arxiv.org/abs/2305.11122v3</link><description>Autonomous experimentation has emerged as an efficient approach to acceleratethe pace of materials discovery. Although instruments for autonomous synthesishave become popular in molecular and polymer science, solution processing ofhybrid materials and nanoparticles, examples of autonomous tools for physicalvapor deposition are scarce yet important for the semiconductor industry. Here,we report the design and implementation of an autonomous workflow for sputterdeposition of thin films with controlled composition, leveraging a highlyautomated sputtering reactor custom-controlled by Python, optical emissionspectroscopy (OES), and a Bayesian optimization algorithm. We modeled filmcomposition, measured by x-ray fluorescence, as a linear function of emissionlines monitored during the co-sputtering from elemental Zn and Ti targets inN$_2$ atmosphere. A Bayesian control algorithm, informed by OES, navigates thespace of sputtering power to fabricate films with user-defined composition, byminimizing the absolute error between desired and measured emission signals. Wevalidated our approach by autonomously fabricating Zn$_x$Ti$_{1-x}$N$_y$ filmswith deviations from the targeted cation composition within relative 3.5 %,even for 15 nm thin films, demonstrating that the proposed approach canreliably synthesize thin films with specific composition and minimal humaninterference. Moreover, the proposed method can be extended to more difficultsynthesis experiments where plasma intensity depends non-linearly on pressure,or the elemental sticking coefficients strongly depend on the substratetemperature.</description><author>Davi M. Febba, Kevin R. Talley, Kendal Johnson, Stephen Schaefer, Sage R. Bauers, John S. Mangum, Rebecca W. Smaha, Andriy Zakutayev</author><pubDate>Thu, 10 Aug 2023 17:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11122v3</guid></item><item><title>Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience</title><link>http://arxiv.org/abs/2308.03712v2</link><description>This paper asks whether current self-supervised learning methods, ifsufficiently scaled up, would be able to reach human-level visual objectrecognition capabilities with the same type and amount of visual experiencehumans learn from. Previous work on this question only considered the scalingof data size. Here, we consider the simultaneous scaling of data size, modelsize, and image resolution. We perform a scaling experiment with visiontransformers up to 633M parameters in size (ViT-H/14) trained with up to 5Khours of human-like video data (long, continuous, mostly egocentric videos)with image resolutions of up to 476x476 pixels. The efficiency of maskedautoencoders (MAEs) as a self-supervised learning algorithm makes it possibleto run this scaling experiment on an unassuming academic budget. We find thatit is feasible to reach human-level object recognition capacity at sub-humanscales of model size, data size, and image size, if these factors are scaled upsimultaneously. To give a concrete example, we estimate that a 2.5B parameterViT model trained with 20K hours (2.3 years) of human-like video data with aspatial resolution of 952x952 pixels should be able to reach roughlyhuman-level accuracy on ImageNet. Human-level competence is thus achievable fora fundamental perceptual capability from human-like perceptual experience(human-like in both amount and type) with extremely generic learning algorithmsand architectures and without any substantive inductive biases.</description><author>A. Emin Orhan</author><pubDate>Thu, 10 Aug 2023 17:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03712v2</guid></item><item><title>Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions</title><link>http://arxiv.org/abs/2308.02312v3</link><description>Over the last decade, Q&amp;A platforms have played a crucial role in howprogrammers seek help online. The emergence of ChatGPT, however, is causing ashift in this pattern. Despite ChatGPT's popularity, there hasn't been athorough investigation into the quality and usability of its responses tosoftware engineering queries. To address this gap, we undertook a comprehensiveanalysis of ChatGPT's replies to 517 questions from Stack Overflow (SO). Weassessed the correctness, consistency, comprehensiveness, and conciseness ofthese responses. Additionally, we conducted an extensive linguistic analysisand a user study to gain insights into the linguistic and human aspects ofChatGPT's answers. Our examination revealed that 52% of ChatGPT's answerscontain inaccuracies and 77% are verbose. Nevertheless, users still preferChatGPT's responses 39.34% of the time due to their comprehensiveness andarticulate language style. These findings underscore the need for meticulouserror correction in ChatGPT while also raising awareness among users about thepotential risks associated with seemingly accurate answers.</description><author>Samia Kabir, David N. Udo-Imeh, Bonan Kou, Tianyi Zhang</author><pubDate>Thu, 10 Aug 2023 17:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02312v3</guid></item><item><title>2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds</title><link>http://arxiv.org/abs/2308.05667v1</link><description>The commonly adopted detect-then-match approach to registration findsdifficulties in the cross-modality cases due to the incompatible keypointdetection and inconsistent feature description. We propose, 2D3D-MATR, adetection-free method for accurate and robust registration between images andpoint clouds. Our method adopts a coarse-to-fine pipeline where it firstcomputes coarse correspondences between downsampled patches of the input imageand the point cloud and then extends them to form dense correspondences betweenpixels and points within the patch region. The coarse-level patch matching isbased on transformer which jointly learns global contextual constraints withself-attention and cross-modality correlations with cross-attention. To resolvethe scale ambiguity in patch matching, we construct a multi-scale pyramid foreach image patch and learn to find for each point patch the best matching imagepatch at a proper resolution level. Extensive experiments on two publicbenchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-artP2-Net by around $20$ percentage points on inlier ratio and over $10$ points onregistration recall. Our code and models are available at\url{https://github.com/minhaolee/2D3DMATR}.</description><author>Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chengyang Zhu, Kai Xu</author><pubDate>Thu, 10 Aug 2023 17:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05667v1</guid></item><item><title>Width and Depth Limits Commute in Residual Networks</title><link>http://arxiv.org/abs/2302.00453v2</link><description>We show that taking the width and depth to infinity in a deep neural networkwith skip connections, when branches are scaled by $1/\sqrt{depth}$ (the onlynontrivial scaling), result in the same covariance structure no matter how thatlimit is taken. This explains why the standard infinite-width-then-depthapproach provides practical insights even for networks with depth of the sameorder as width. We also demonstrate that the pre-activations, in this case,have Gaussian distributions which has direct applications in Bayesian deeplearning. We conduct extensive simulations that show an excellent match withour theoretical findings.</description><author>Soufiane Hayou, Greg Yang</author><pubDate>Thu, 10 Aug 2023 17:09:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00453v2</guid></item><item><title>Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data</title><link>http://arxiv.org/abs/2308.05665v1</link><description>Modern transportation planning relies heavily on accurate predictions ofperson and vehicle trips. However, traditional planning models often fail toaccount for the intricacies and dynamics of travel behavior, leading toless-than-optimal accuracy in these predictions. This study explores thepotential of deep learning techniques to transform the way we approach trippredictions, and ultimately, transportation planning. Utilizing a comprehensivedataset from the National Household Travel Survey (NHTS), we developed andtrained a deep learning model for predicting person and vehicle trips. Theproposed model leverages the vast amount of information in the NHTS data,capturing complex, non-linear relationships that were previously overlooked bytraditional models. As a result, our deep learning model achieved an impressiveaccuracy of 98% for person trip prediction and 96% for vehicle trip estimation.This represents a significant improvement over the performances of traditionaltransportation planning models, thereby demonstrating the power of deeplearning in this domain. The implications of this study extend beyond just moreaccurate predictions. By enhancing the accuracy and reliability of tripprediction models, planners can formulate more effective, data-driventransportation policies, infrastructure, and services. As such, our researchunderscores the need for the transportation planning field to embrace advancedtechniques like deep learning. The detailed methodology, along with a thoroughdiscussion of the results and their implications, are presented in thesubsequent sections of this paper.</description><author>Kojo Adu-Gyamfi, Sharma Anuj</author><pubDate>Thu, 10 Aug 2023 17:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05665v1</guid></item><item><title>Open-vocabulary Object Segmentation with Diffusion Models</title><link>http://arxiv.org/abs/2301.05221v2</link><description>The goal of this paper is to extract the visual-language correspondence froma pre-trained text-to-image diffusion model, in the form of segmentation map,i.e., simultaneously generating images and segmentation masks for thecorresponding visual entities described in the text prompt. We make thefollowing contributions: (i) we pair the existing Stable Diffusion model with anovel grounding module, that can be trained to align the visual and textualembedding space of the diffusion model with only a small number of objectcategories; (ii) we establish an automatic pipeline for constructing a dataset,that consists of {image, segmentation mask, text prompt} triplets, to train theproposed grounding module; (iii) we evaluate the performance of open-vocabularygrounding on images generated from the text-to-image diffusion model and showthat the module can well segment the objects of categories beyond seen ones attraining time; (iv) we adopt the augmented diffusion model to build a syntheticsemantic segmentation dataset, and show that, training a standard segmentationmodel on such dataset demonstrates competitive performance on the zero-shotsegmentation(ZS3) benchmark, which opens up new opportunities for adopting thepowerful diffusion model for discriminative tasks.</description><author>Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 10 Aug 2023 17:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05221v2</guid></item><item><title>GUNNEL: Guided Mixup Augmentation and Multi-View Fusion for Aquatic Animal Segmentation</title><link>http://arxiv.org/abs/2112.06193v3</link><description>Recent years have witnessed great advances in object segmentation research.In addition to generic objects, aquatic animals have attracted researchattention. Deep learning-based methods are widely used for aquatic animalsegmentation and have achieved promising performance. However, there is a lackof challenging datasets for benchmarking. In this work, we build a new datasetdubbed Aquatic Animal Species. We also devise a novel GUided mixup augmeNtatioNand multi-modEl fusion for aquatic animaL segmentation (GUNNEL) that leveragesthe advantages of multiple segmentation models to effectively segment aquaticanimals and improves the training performance by synthesizing hard samples.Extensive experiments demonstrated the superiority of our proposed frameworkover existing state-of-the-art instance segmentation methods. The code isavailable at https://github.com/lmquan2000/mask-mixup. The dataset is availableat https://doi.org/10.5281/zenodo.8208877 .</description><author>Minh-Quan Le, Trung-Nghia Le, Tam V. Nguyen, Isao Echizen, Minh-Triet Tran</author><pubDate>Thu, 10 Aug 2023 17:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.06193v3</guid></item><item><title>AD-CLIP: Adapting Domains in Prompt Space Using CLIP</title><link>http://arxiv.org/abs/2308.05659v1</link><description>Although deep learning models have shown impressive performance on supervisedlearning tasks, they often struggle to generalize well when the training(source) and test (target) domains differ. Unsupervised domain adaptation (DA)has emerged as a popular solution to this problem. However, current DAtechniques rely on visual backbones, which may lack semantic richness. Despitethe potential of large-scale vision-language foundation models like CLIP, theireffectiveness for DA has yet to be fully explored. To address this gap, weintroduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP thataims to solve the DA problem in the prompt space. We leverage the frozen visionbackbone of CLIP to extract both image style (domain) and content information,which we apply to learn prompt tokens. Our prompts are designed to bedomain-invariant and class-generalizable, by conditioning prompt learning onimage style and content features simultaneously. We use standard supervisedcontrastive learning in the source domain, while proposing an entropyminimization strategy to align domains in the embedding space given the targetdomain data. We also consider a scenario where only target domain samples areavailable during testing, without any source domain data, and propose across-domain style mapping network to hallucinate domain-agnostic tokens. Ourextensive experiments on three benchmark DA datasets demonstrate theeffectiveness of AD-CLIP compared to existing literature.</description><author>Mainak Singha, Harsh Pal, Ankit Jha, Biplab Banerjee</author><pubDate>Thu, 10 Aug 2023 16:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05659v1</guid></item><item><title>Automatic Extraction of Relevant Road Infrastructure using Connected vehicle data and Deep Learning Model</title><link>http://arxiv.org/abs/2308.05658v1</link><description>In today's rapidly evolving urban landscapes, efficient and accurate mappingof road infrastructure is critical for optimizing transportation systems,enhancing road safety, and improving the overall mobility experience fordrivers and commuters. Yet, a formidable bottleneck obstructs progress - thelaborious and time-intensive manual identification of intersections. Simplyconsidering the shear number of intersections that need to be identified, andthe labor hours required per intersection, the need for an automated solutionbecomes undeniable. To address this challenge, we propose a novel approach thatleverages connected vehicle data and cutting-edge deep learning techniques. Byemploying geohashing to segment vehicle trajectories and then generating imagerepresentations of road segments, we utilize the YOLOv5 (You Only Look Onceversion 5) algorithm for accurate classification of both straight road segmentsand intersections. Experimental results demonstrate an impressive overallclassification accuracy of 95%, with straight roads achieving a remarkable 97%F1 score and intersections reaching a 90% F1 score. This approach not onlysaves time and resources but also enables more frequent updates and acomprehensive understanding of the road network. Our research showcases thepotential impact on traffic management, urban planning, and autonomous vehiclenavigation systems. The fusion of connected vehicle data and deep learningmodels holds promise for a transformative shift in road infrastructure mapping,propelling us towards a smarter, safer, and more connected transportationecosystem.</description><author>Adu-Gyamfi Kojo, Kandiboina Raghupathi, Ravichandra-Mouli Varsha, Knickerbocker Skylar, Hans Zachary N, Hawkins, Neal R, Sharma Anuj</author><pubDate>Thu, 10 Aug 2023 16:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05658v1</guid></item><item><title>Attention-based 3D CNN with Multi-layer Features for Alzheimer's Disease Diagnosis using Brain Images</title><link>http://arxiv.org/abs/2308.05655v1</link><description>Structural MRI and PET imaging play an important role in the diagnosis ofAlzheimer's disease (AD), showing the morphological changes and glucosemetabolism changes in the brain respectively. The manifestations in the brainimage of some cognitive impairment patients are relatively inconspicuous, forexample, it still has difficulties in achieving accurate diagnosis through sMRIin clinical practice. With the emergence of deep learning, convolutional neuralnetwork (CNN) has become a valuable method in AD-aided diagnosis, but some CNNmethods cannot effectively learn the features of brain image, making thediagnosis of AD still presents some challenges. In this work, we propose anend-to-end 3D CNN framework for AD diagnosis based on ResNet, which integratesmulti-layer features obtained under the effect of the attention mechanism tobetter capture subtle differences in brain images. The attention maps showedour model can focus on key brain regions related to the disease diagnosis. Ourmethod was verified in ablation experiments with two modality images on 792subjects from the ADNI database, where AD diagnostic accuracies of 89.71% and91.18% were achieved based on sMRI and PET respectively, and also outperformedsome state-of-the-art methods.</description><author>Yanteng Zhang, Qizhi Teng, Xiaohai He, Tong Niu, Lipei Zhang, Yan Liu, Chao Ren</author><pubDate>Thu, 10 Aug 2023 16:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05655v1</guid></item><item><title>Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models</title><link>http://arxiv.org/abs/2305.03829v4</link><description>Image-based precision medicine aims to personalize treatment decisions basedon an individual's unique imaging features so as to improve their clinicaloutcome. Machine learning frameworks that integrate uncertainty estimation aspart of their treatment recommendations would be safer and more reliable.However, little work has been done in adapting uncertainty estimationtechniques and validation metrics for precision medicine. In this paper, we useBayesian deep learning for estimating the posterior distribution over factualand counterfactual outcomes on several treatments. This allows for estimatingthe uncertainty for each treatment option and for the individual treatmenteffects (ITE) between any two treatments. We train and evaluate this model topredict future new and enlarging T2 lesion counts on a large, multi-centerdataset of MR brain images of patients with multiple sclerosis, exposed toseveral treatments during randomized controlled trials. We evaluate thecorrelation of the uncertainty estimate with the factual error, and, given thelack of ground truth counterfactual outcomes, demonstrate how uncertainty forthe ITE prediction relates to bounds on the ITE error. Lastly, we demonstratehow knowledge of uncertainty could modify clinical decision-making to improveindividual patient and clinical trial outcomes.</description><author>Joshua Durso-Finley, Jean-Pierre Falet, Raghav Mehta, Douglas L. Arnold, Nick Pawlowski, Tal Arbel</author><pubDate>Thu, 10 Aug 2023 16:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03829v4</guid></item><item><title>Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization</title><link>http://arxiv.org/abs/2308.05648v1</link><description>Video moment localization aims to retrieve the target segment of an untrimmedvideo according to the natural language query. Weakly supervised methods gainsattention recently, as the precise temporal location of the target segment isnot always available. However, one of the greatest challenges encountered bythe weakly supervised method is implied in the mismatch between the video andlanguage induced by the coarse temporal annotations. To refine thevision-language alignment, recent works contrast the cross-modalitysimilarities driven by reconstructing masked queries between positive andnegative video proposals. However, the reconstruction may be influenced by thelatent spurious correlation between the unmasked and the masked parts, whichdistorts the restoring process and further degrades the efficacy of contrastivelearning since the masked words are not completely reconstructed from thecross-modality knowledge. In this paper, we discover and mitigate this spuriouscorrelation through a novel proposed counterfactual cross-modality reasoningmethod. Specifically, we first formulate query reconstruction as an aggregatedcausal effect of cross-modality and query knowledge. Then by introducingcounterfactual cross-modality knowledge into this aggregation, the spuriousimpact of the unmasked part contributing to the reconstruction is explicitlymodeled. Finally, by suppressing the unimodal effect of masked query, we canrectify the reconstructions of video proposals to perform reasonablecontrastive learning. Extensive experimental evaluations demonstrate theeffectiveness of our proposed method. The code is available at\href{https://github.com/sLdZ0306/CCR}{https://github.com/sLdZ0306/CCR}.</description><author>Zezhong Lv, Bing Su, Ji-Rong Wen</author><pubDate>Thu, 10 Aug 2023 16:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05648v1</guid></item><item><title>AST-MHSA : Code Summarization using Multi-Head Self-Attention</title><link>http://arxiv.org/abs/2308.05646v1</link><description>Code summarization aims to generate concise natural language descriptions forsource code. The prevailing approaches adopt transformer-based encoder-decoderarchitectures, where the Abstract Syntax Tree (AST) of the source code isutilized for encoding structural information. However, ASTs are much longerthan the corresponding source code, and existing methods ignore this sizeconstraint by directly feeding the entire linearized AST into the encoders.This simplistic approach makes it challenging to extract truly valuabledependency relations from the overlong input sequence and leads to significantcomputational overhead due to self-attention applied to all nodes in the AST. To address this issue effectively and efficiently, we present a model,AST-MHSA that uses multi-head attention to extract the important semanticinformation from the AST. The model consists of two main components: an encoderand a decoder. The encoder takes as input the abstract syntax tree (AST) of thecode and generates a sequence of hidden states. The decoder then takes thesehidden states as input and generates a natural language summary of the code. The multi-head attention mechanism allows the model to learn differentrepresentations of the input code, which can be combined to generate a morecomprehensive summary. The model is trained on a dataset of code and summaries,and the parameters of the model are optimized to minimize the loss between thegenerated summaries and the ground-truth summaries.</description><author>Yeshwanth Nagaraj, Ujjwal Gupta</author><pubDate>Thu, 10 Aug 2023 16:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05646v1</guid></item><item><title>Selective inference using randomized group lasso estimators for general models</title><link>http://arxiv.org/abs/2306.13829v2</link><description>Selective inference methods are developed for group lasso estimators for usewith a wide class of distributions and loss functions. The method includes theuse of exponential family distributions, as well as quasi-likelihood modelingfor overdispersed count data, for example, and allows for categorical orgrouped covariates as well as continuous covariates. A randomizedgroup-regularized optimization problem is studied. The added randomizationallows us to construct a post-selection likelihood which we show to be adequatefor selective inference when conditioning on the event of the selection of thegrouped covariates. This likelihood also provides a selective point estimator,accounting for the selection by the group lasso. Confidence regions for theregression parameters in the selected model take the form of Wald-type regionsand are shown to have bounded volume. The selective inference method forgrouped lasso is illustrated on data from the national health and nutritionexamination survey while simulations showcase its behaviour and favorablecomparison with other methods.</description><author>Yiling Huang, Sarah Pirenne, Snigdha Panigrahi, Gerda Claeskens</author><pubDate>Thu, 10 Aug 2023 16:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13829v2</guid></item><item><title>A Comparative Visual Analytics Framework for Evaluating Evolutionary Processes in Multi-objective Optimization</title><link>http://arxiv.org/abs/2308.05640v1</link><description>Evolutionary multi-objective optimization (EMO) algorithms have beendemonstrated to be effective in solving multi-criteria decision-makingproblems. In real-world applications, analysts often employ several algorithmsconcurrently and compare their solution sets to gain insight into thecharacteristics of different algorithms and explore a broader range of feasiblesolutions. However, EMO algorithms are typically treated as black boxes,leading to difficulties in performing detailed analysis and comparisons betweenthe internal evolutionary processes. Inspired by the successful application ofvisual analytics tools in explainable AI, we argue that interactivevisualization can significantly enhance the comparative analysis betweenmultiple EMO algorithms. In this paper, we present a visual analytics frameworkthat enables the exploration and comparison of evolutionary processes in EMOalgorithms. Guided by a literature review and expert interviews, the proposedframework addresses various analytical tasks and establishes a multi-facetedvisualization design to support the comparative analysis of intermediategenerations in the evolution as well as solution sets. We demonstrate theeffectiveness of our framework through case studies on benchmarking andreal-world multi-objective optimization problems to elucidate how analysts canleverage our framework to inspect and compare diverse algorithms.</description><author>Yansong Huang, Zherui Zhang, Ao Jiao, Yuxin Ma, Ran Cheng</author><pubDate>Thu, 10 Aug 2023 16:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05640v1</guid></item><item><title>VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</title><link>http://arxiv.org/abs/2112.02399v3</link><description>Contrastive Language-Image Pre-training (CLIP) has drawn increasing attentionrecently for its transferable visual representation learning. However, due tothe semantic gap within datasets, CLIP's pre-trained image-text alignmentbecomes sub-optimal on downstream tasks, which severely harms its transferringperformance. To better adapt the cross-modality embedding space, we propose toenhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guidetextual features of different categories to adaptively explore informativeregions on the image and aggregate visual features by attention mechanisms. Inthis way, the texts become visual-guided, namely, more semantically correlatedwith downstream images, which greatly benefits the category-wise matchingprocess. In few-shot settings, we evaluate our VT-CLIP on 11 well-knownclassification datasets to demonstrate its effectiveness.</description><author>Longtian Qiu, Renrui Zhang, Ziyu Guo, Ziyao Zeng, Zilu Guo, Yafeng Li, Guangnan Zhang</author><pubDate>Thu, 10 Aug 2023 16:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.02399v3</guid></item><item><title>From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks</title><link>http://arxiv.org/abs/2307.02279v2</link><description>The connection between Residual Neural Networks (ResNets) and continuous-timecontrol systems (known as NeurODEs) has led to a mathematical analysis ofneural networks which has provided interesting results of both theoretical andpractical significance. However, by construction, NeurODEs have been limited todescribing constant-width layers, making them unsuitable for modeling deeplearning architectures with layers of variable width. In this paper, we proposea continuous-time Autoencoder, which we call AutoencODE, based on amodification of the controlled field that drives the dynamics. This adaptationenables the extension of the mean-field control framework originally devisedfor conventional NeurODEs. In this setting, we tackle the case of low Tikhonovregularization, resulting in potentially non-convex cost landscapes. While theglobal results obtained for high Tikhonov regularization may not hold globally,we show that many of them can be recovered in regions where the loss functionis locally convex. Inspired by our theoretical findings, we develop a trainingmethod tailored to this specific type of Autoencoders with residualconnections, and we validate our approach through numerical experimentsconducted on various examples.</description><author>Cristina Cipriani, Massimo Fornasier, Alessandro Scagliotti</author><pubDate>Thu, 10 Aug 2023 16:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02279v2</guid></item><item><title>A Homomorphic Encryption Framework for Privacy-Preserving Spiking Neural Networks</title><link>http://arxiv.org/abs/2308.05636v1</link><description>Machine learning (ML) is widely used today, especially through deep neuralnetworks (DNNs), however, increasing computational load and resourcerequirements have led to cloud-based solutions. To address this problem, a newgeneration of networks called Spiking Neural Networks (SNN) has emerged, whichmimic the behavior of the human brain to improve efficiency and reduce energyconsumption. These networks often process large amounts of sensitiveinformation, such as confidential data, and thus privacy issues arise.Homomorphic encryption (HE) offers a solution, allowing calculations to beperformed on encrypted data without decrypting it. This research comparestraditional DNNs and SNNs using the Brakerski/Fan-Vercauteren (BFV) encryptionscheme. The LeNet-5 model, a widely-used convolutional architecture, is usedfor both DNN and SNN models based on the LeNet-5 architecture, and the networksare trained and compared using the FashionMNIST dataset. The results show thatSNNs using HE achieve up to 40% higher accuracy than DNNs for low values of theplaintext modulus t, although their execution time is longer due to theirtime-coding nature with multiple time-steps.</description><author>Farzad Nikfam, Raffaele Casaburi, Alberto Marchisio, Maurizio Martina, Muhammad Shafique</author><pubDate>Thu, 10 Aug 2023 16:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05636v1</guid></item><item><title>IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer</title><link>http://arxiv.org/abs/2308.05633v1</link><description>Automated medical report generation has become increasingly important inmedical analysis. It can produce computer-aided diagnosis descriptions and thussignificantly alleviate the doctors' work. Inspired by the huge success ofneural machine translation and image captioning, various deep learning methodshave been proposed for medical report generation. However, due to the inherentproperties of medical data, including data imbalance and the length andcorrelation between report sequences, the generated reports by existing methodsmay exhibit linguistic fluency but lack adequate clinical accuracy. In thiswork, we propose an image-to-indicator hierarchical transformer (IIHT)framework for medical report generation. It consists of three modules, i.e., aclassifier module, an indicator expansion module and a generator module. Theclassifier module first extracts image features from the input medical imagesand produces disease-related indicators with their corresponding states. Thedisease-related indicators are subsequently utilised as input for the indicatorexpansion module, incorporating the "data-text-data" strategy. Thetransformer-based generator then leverages these extracted features along withimage features as auxiliary information to generate final reports. Furthermore,the proposed IIHT method is feasible for radiologists to modify diseaseindicators in real-world scenarios and integrate the operations into theindicator expansion module for fluent and accurate medical report generation.Extensive experiments and comparisons with state-of-the-art methods undervarious evaluation metrics demonstrate the great performance of the proposedmethod.</description><author>Keqiang Fan, Xiaohao Cai, Mahesan Niranjan</author><pubDate>Thu, 10 Aug 2023 16:22:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05633v1</guid></item><item><title>Deep learning-based Crop Row Detection for Infield Navigation of Agri-Robots</title><link>http://arxiv.org/abs/2209.04278v2</link><description>Autonomous navigation in agricultural environments is challenged by varyingfield conditions that arise in arable fields. State-of-the-art solutions forautonomous navigation in such environments require expensive hardware such asRTK-GNSS. This paper presents a robust crop row detection algorithm thatwithstands such field variations using inexpensive cameras. Existing datasetsfor crop row detection does not represent all the possible field variations. Adataset of sugar beet images was created representing 11 field variationscomprised of multiple grow stages, light levels, varying weed densities, curvedcrop rows and discontinuous crop rows. The proposed pipeline segments the croprows using a deep learning-based method and employs the predicted segmentationmask for extraction of the central crop using a novel central crop rowselection algorithm. The novel crop row detection algorithm was tested for croprow detection performance and the capability of visual servoing along a croprow. The visual servoing-based navigation was tested on a realistic simulationscenario with the real ground and plant textures. Our algorithm demonstratedrobust vision-based crop row detection in challenging field conditionsoutperforming the baseline.</description><author>Rajitha de Silva, Grzegorz Cielniak, Gang Wang, Junfeng Gao</author><pubDate>Thu, 10 Aug 2023 16:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04278v2</guid></item><item><title>ReLU and Addition-based Gated RNN</title><link>http://arxiv.org/abs/2308.05629v1</link><description>We replace the multiplication and sigmoid function of the conventionalrecurrent gate with addition and ReLU activation. This mechanism is designed tomaintain long-term memory for sequence processing but at a reducedcomputational cost, thereby opening up for more efficient execution or largermodels on restricted hardware. Recurrent Neural Networks (RNNs) with gatingmechanisms such as LSTM and GRU have been widely successful in learning fromsequential data due to their ability to capture long-term dependencies.Conventionally, the update based on current inputs and the previous statehistory is each multiplied with dynamic weights and combined to compute thenext state. However, multiplication can be computationally expensive,especially for certain hardware architectures or alternative arithmetic systemssuch as homomorphic encryption. It is demonstrated that the novel gatingmechanism can capture long-term dependencies for a standard synthetic sequencelearning task while significantly reducing computational costs such thatexecution time is reduced by half on CPU and by one-third under encryption.Experimental results on handwritten text recognition tasks furthermore showthat the proposed architecture can be trained to achieve comparable accuracy toconventional GRU and LSTM baselines. The gating mechanism introduced in thispaper may enable privacy-preserving AI applications operating under homomorphicencryption by avoiding the multiplication of encrypted variables. It can alsosupport quantization in (unencrypted) plaintext applications, with thepotential for substantial performance gains since the addition-basedformulation can avoid the expansion to double precision often required formultiplication.</description><author>Rickard Brännvall, Henrik Forsgren, Fredrik Sandin, Marcus Liwicki</author><pubDate>Thu, 10 Aug 2023 16:18:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05629v1</guid></item><item><title>Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data</title><link>http://arxiv.org/abs/2304.05874v2</link><description>Graph neural network (GNN) models are increasingly being used for theclassification of electroencephalography (EEG) data. However, GNN-baseddiagnosis of neurological disorders, such as Alzheimer's disease (AD), remainsa relatively unexplored area of research. Previous studies have relied onfunctional connectivity methods to infer brain graph structures and used simpleGNN architectures for the diagnosis of AD. In this work, we propose a noveladaptive gated graph convolutional network (AGGCN) that can provide explainablepredictions. AGGCN adaptively learns graph structures by combiningconvolution-based node feature enhancement with a well-known correlation-basedmeasure of functional connectivity. Furthermore, the gated graph convolutioncan dynamically weigh the contribution of various spatial scales. The proposedmodel achieves high accuracy in both eyes-closed and eyes-open conditions,indicating the stability of learned representations. Finally, we demonstratethat the proposed AGGCN model generates consistent explanations of itspredictions that might be relevant for further study of AD-related alterationsof brain networks.</description><author>Dominik Klepl, Fei He, Min Wu, Daniel J. Blackburn, Ptolemaios G. Sarrigiannis</author><pubDate>Thu, 10 Aug 2023 16:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05874v2</guid></item><item><title>Normalized Gradients for All</title><link>http://arxiv.org/abs/2308.05621v1</link><description>In this short note, I show how to adapt to H\"{o}lder smoothness usingnormalized gradients in a black-box way. Moreover, the bound will depend on anovel notion of local H\"{o}lder smoothness. The main idea directly comes fromLevy [2017].</description><author>Francesco Orabona</author><pubDate>Thu, 10 Aug 2023 16:10:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05621v1</guid></item><item><title>Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance</title><link>http://arxiv.org/abs/2308.05619v1</link><description>As data shift or new data become available, updating clinical machinelearning models may be necessary to maintain or improve performance over time.However, updating a model can introduce compatibility issues when the behaviorof the updated model does not align with user expectations, resulting in pooruser-model team performance. Existing compatibility measures depend on modeldecision thresholds, limiting their applicability in settings where models areused to generate rankings based on estimated risk. To address this limitation,we propose a novel rank-based compatibility measure, $C^R$, and a new lossfunction that aims to optimize discriminative performance while encouraginggood compatibility. Applied to a case study in mortality risk stratificationleveraging data from MIMIC, our approach yields more compatible models whilemaintaining discriminative performance compared to existing model selectiontechniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:$0.005$, $0.035$). This work provides new tools to analyze and update riskstratification models used in clinical care.</description><author>Erkin Ötleş, Brian T. Denton, Jenna Wiens</author><pubDate>Thu, 10 Aug 2023 16:08:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05619v1</guid></item><item><title>A Neural Network Based Choice Model for Assortment Optimization</title><link>http://arxiv.org/abs/2308.05617v1</link><description>Discrete-choice models are used in economics, marketing and revenuemanagement to predict customer purchase probabilities, say as a function ofprices and other features of the offered assortment. While they have been shownto be expressive, capturing customer heterogeneity and behaviour, they are alsohard to estimate, often based on many unobservables like utilities; andmoreover, they still fail to capture many salient features of customerbehaviour. A natural question then, given their success in other contexts, isif neural networks can eliminate the necessity of carefully building acontext-dependent customer behaviour model and hand-coding and tuning theestimation. It is unclear however how one would incorporate assortment effectsinto such a neural network, and also how one would optimize the assortment withsuch a black-box generative model of choice probabilities. In this paper weinvestigate first whether a single neural network architecture can predictpurchase probabilities for datasets from various contexts and generated undervarious models and assumptions. Next, we develop an assortment optimizationformulation that is solvable by off-the-shelf integer programming solvers. Wecompare against a variety of benchmark discrete-choice models on simulated aswell as real-world datasets, developing training tricks along the way to makethe neural network prediction and subsequent optimization robust and comparablein performance to the alternates.</description><author>Hanzhao Wang, Zhongze Cai, Xiaocheng Li, Kalyan Talluri</author><pubDate>Thu, 10 Aug 2023 16:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05617v1</guid></item><item><title>From Random Search to Bandit Learning in Metric Measure Spaces</title><link>http://arxiv.org/abs/2305.11509v4</link><description>Random Search is one of the most widely-used method for HyperparameterOptimization, and is critical to the success of deep learning models. Despiteits astonishing performance, little non-heuristic theory has been developed todescribe the underlying working mechanism. This paper gives a theoreticalaccounting of Random Search. We introduce the concept of \emph{scatteringdimension} that describes the landscape of the underlying function, andquantifies the performance of random search. We show that, when the environmentis noise-free, the output of random search converges to the optimal value inprobability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scatteringdimension of the underlying function. When the observed function values arecorrupted by bounded $iid$ noise, the output of random search converges to theoptimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on theprinciples of random search, we introduce an algorithm, called BLiN-MOS, forLipschitz bandits in doubling metric spaces that are also endowed with aprobability measure, and show that BLiN-MOS achieves a regret rate of order $\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$is the zooming dimension of the problem instance.</description><author>Chuying Han, Yasong Feng, Tianyu Wang</author><pubDate>Thu, 10 Aug 2023 16:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11509v4</guid></item><item><title>A Smart Robotic System for Industrial Plant Supervision</title><link>http://arxiv.org/abs/2308.05612v1</link><description>In today's chemical production plants, human field operators perform frequentchecks on the plant's integrity to guarantee high safety standards, and thusare possibly the first to encounter dangerous operating conditions. Toalleviate their tasks of failure detection and monitoring by audio, visual, andolfactory perceptions, we present a robotic system that consists of anautonomously navigating robot integrated with various sensors and dataprocessing. We aim to resemble the human sensing and interpretationcapabilities of sight, smell, and hearing, for providing automated inspection.We evaluate our system extensively at a wastewater facility in full workingconditions. Our results demonstrate that the system is able to robustlynavigate a plant and to provide useful information about critical operatingconditions.</description><author>D. Adriana Gómez-Rosal, Max Bergau, Georg K. J. Fischer, Andreas Wachaja, Johannes Gräter, Matthias Odenweller, Uwe Piechottka, Fabian Hoeflinger, Nikhil Gosala, Niklas Wetzel, Daniel Büscher, Abhinav Valada, Wolfram Burgard</author><pubDate>Thu, 10 Aug 2023 15:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05612v1</guid></item><item><title>Deep Multiview Clustering by Contrasting Cluster Assignments</title><link>http://arxiv.org/abs/2304.10769v4</link><description>Multiview clustering (MVC) aims to reveal the underlying structure ofmultiview data by categorizing data samples into clusters. Deep learning-basedmethods exhibit strong feature learning capabilities on large-scale datasets.For most existing deep MVC methods, exploring the invariant representations ofmultiple views is still an intractable problem. In this paper, we propose across-view contrastive learning (CVCL) method that learns view-invariantrepresentations and produces clustering results by contrasting the clusterassignments among multiple views. Specifically, we first employ deepautoencoders to extract view-dependent features in the pretraining stage. Then,a cluster-level CVCL strategy is presented to explore consistent semantic labelinformation among the multiple views in the fine-tuning stage. Thus, theproposed CVCL method is able to produce more discriminative cluster assignmentsby virtue of this learning strategy. Moreover, we provide a theoreticalanalysis of soft cluster assignment alignment. Extensive experimental resultsobtained on several datasets demonstrate that the proposed CVCL methodoutperforms several state-of-the-art approaches.</description><author>Jie Chen, Hua Mao, Wai Lok Woo, Xi Peng</author><pubDate>Thu, 10 Aug 2023 15:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10769v4</guid></item><item><title>Forecasting Irregularly Sampled Time Series using Graphs</title><link>http://arxiv.org/abs/2305.12932v2</link><description>Forecasting irregularly sampled time series with missing values is a crucialtask for numerous real-world applications such as healthcare, astronomy, andclimate sciences. State-of-the-art approaches to this problem rely on OrdinaryDifferential Equations (ODEs) which are known to be slow and often requireadditional features to handle missing values. To address this issue, we proposea novel model using Graphs for Forecasting Irregularly Sampled Time Series withmissing values which we call GraFITi. GraFITi first converts the time series toa Sparsity Structure Graph which is a sparse bipartite graph, and thenreformulates the forecasting problem as the edge weight prediction task in thegraph. It uses the power of Graph Neural Networks to learn the graph andpredict the target edge weights. GraFITi has been tested on 3 real-world and 1synthetic irregularly sampled time series dataset with missing values andcompared with various state-of-the-art models. The experimental resultsdemonstrate that GraFITi improves the forecasting accuracy by up to 17% andreduces the run time up to 5 times compared to the state-of-the-art forecastingmodels.</description><author>Vijaya Krishna Yalavarthi, Kiran Madhusudhanan, Randolf Sholz, Nourhan Ahmed, Johannes Burchert, Shayan Jawed, Stefan Born, Lars Schmidt-Thieme</author><pubDate>Thu, 10 Aug 2023 15:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12932v2</guid></item><item><title>LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition</title><link>http://arxiv.org/abs/2308.05609v1</link><description>Biomedical Natural Language Processing (NLP) tends to become cumbersome formost researchers, frequently due to the amount and heterogeneity of text to beprocessed. To address this challenge, the industry is continuously developinghighly efficient tools and creating more flexible engineering solutions. Thiswork presents the integration between industry data engineering solutions forefficient data processing and academic systems developed for Named EntityRecognition (LasigeUnicage\_NER) and Relation Extraction (BiOnt). Our designreflects an integration of those components with external knowledge in the formof additional training data from other datasets and biomedical ontologies. Weused this pipeline in the 2022 LitCoin NLP Challenge, where our teamLasigeUnicage was awarded the 7th Prize out of approximately 200 participatingteams, reflecting a successful collaboration between the academia (LASIGE) andthe industry (Unicage). The software supporting this work is available at\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.</description><author>Pedro Ruas, Diana F. Sousa, André Neves, Carlos Cruz, Francisco M. Couto</author><pubDate>Thu, 10 Aug 2023 15:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05609v1</guid></item><item><title>Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network</title><link>http://arxiv.org/abs/2308.05605v1</link><description>Monocular depth estimation is known as an ill-posed task in which objects ina 2D image usually do not contain sufficient information to predict theirdepth. Thus, it acts differently from other tasks (e.g., classification andsegmentation) in many ways. In this paper, we find that self-supervisedmonocular depth estimation shows a direction sensitivity and environmentaldependency in the feature representation. But the current backbones borrowedfrom other tasks pay less attention to handling different types ofenvironmental information, limiting the overall depth accuracy. To bridge thisgap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN),which improves the depth feature representation in two aspects. First, wepropose a direction-aware module, which can learn to adjust the featureextraction in each direction, facilitating the encoding of different types ofinformation. Secondly, we design a new cumulative convolution to improve theefficiency for aggregating important environmental information. Experimentsshow that our method achieves significant improvements on three widely usedbenchmarks, KITTI, Cityscapes, and Make3D, setting a new state-of-the-artperformance on the popular benchmarks with all three types of self-supervision.</description><author>Wencheng Han, Junbo Yin, Jianbing Shen</author><pubDate>Thu, 10 Aug 2023 15:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05605v1</guid></item><item><title>Online learning techniques for prediction of temporal tabular datasets with regime changes</title><link>http://arxiv.org/abs/2301.00790v4</link><description>The application of deep learning to non-stationary temporal datasets can leadto overfitted models that underperform under regime changes. In this work, wepropose a modular machine learning pipeline for ranking predictions on temporalpanel datasets which is robust under regime changes. The modularity of thepipeline allows the use of different models, including Gradient BoostingDecision Trees (GBDTs) and Neural Networks, with and without featureengineering. We evaluate our framework on financial data for stock portfolioprediction, and find that GBDT models with dropout display high performance,robustness and generalisability with reduced complexity and computational cost.We then demonstrate how online learning techniques, which require no retrainingof models, can be used post-prediction to enhance the results. First, we showthat dynamic feature projection improves robustness by reducing drawdown inregime changes. Second, we demonstrate that dynamical model ensembling based onselection of models with good recent performance leads to improved Sharpe andCalmar ratios of out-of-sample predictions. We also evaluate the robustness ofour pipeline across different data splits and random seeds with goodreproducibility.</description><author>Thomas Wong, Mauricio Barahona</author><pubDate>Thu, 10 Aug 2023 15:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00790v4</guid></item><item><title>GPT-4 Can't Reason</title><link>http://arxiv.org/abs/2308.03762v2</link><description>GPT-4 was released in March 2023 to wide acclaim, marking a very substantialimprovement across the board over GPT-3.5 (OpenAI's previously best model,which had powered the initial release of ChatGPT). However, despite thegenuinely impressive improvement, there are good reasons to be highly skepticalof GPT-4's ability to reason. This position paper discusses the nature ofreasoning; criticizes the current formulation of reasoning problems in the NLPcommunity, as well as the way in which LLM reasoning performance is currentlyevaluated; introduces a small collection of 21 diverse reasoning problems; andperforms a detailed qualitative evaluation of GPT-4's performance on thoseproblems. Based on this analysis, the paper concludes that, despite itsoccasional flashes of analytical brilliance, GPT-4 at present is utterlyincapable of reasoning.</description><author>Konstantine Arkoudas</author><pubDate>Thu, 10 Aug 2023 15:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03762v2</guid></item><item><title>Object Goal Navigation with Recursive Implicit Maps</title><link>http://arxiv.org/abs/2308.05602v1</link><description>Object goal navigation aims to navigate an agent to locations of a givenobject category in unseen environments. Classical methods explicitly build mapsof environments and require extensive engineering while lacking semanticinformation for object-oriented exploration. On the other hand, end-to-endlearning methods alleviate manual map design and predict actions using implicitrepresentations. Such methods, however, lack an explicit notion of geometry andmay have limited ability to encode navigation history. In this work, we proposean implicit spatial map for object goal navigation. Our implicit map isrecursively updated with new observations at each step using a transformer. Toencourage spatial reasoning, we introduce auxiliary tasks and train our modelto reconstruct explicit maps as well as to predict visual features, semanticlabels and actions. Our method significantly outperforms the state of the arton the challenging MP3D dataset and generalizes well to the HM3D dataset. Wesuccessfully deploy our model on a real robot and achieve encouraging objectgoal navigation results in real scenes using only a few real-worlddemonstrations. Code, trained models and videos are available at\url{https://www.di.ens.fr/willow/research/onav_rim/}.</description><author>Shizhe Chen, Thomas Chabal, Ivan Laptev, Cordelia Schmid</author><pubDate>Thu, 10 Aug 2023 15:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05602v1</guid></item><item><title>Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction</title><link>http://arxiv.org/abs/2308.05601v1</link><description>Inter-city highway transportation is significant for urban life. As one ofthe key functions in intelligent transportation system (ITS), trafficevaluation always plays significant role nowadays, and daily traffic flowprediction still faces challenges at network-wide toll stations. On the onehand, the data imbalance in practice among various locations deteriorates theperformance of prediction. On the other hand, complex correlativespatio-temporal factors cannot be comprehensively employed in long-termduration. In this paper, a prediction method is proposed for daily traffic flowin highway domain through spatio-temporal deep learning. In our method, datanormalization strategy is used to deal with data imbalance, due to long-taildistribution of traffic flow at network-wide toll stations. And then, based ongraph convolutional network, we construct networks in distinct semantics tocapture spatio-temporal features. Beside that, meteorology and calendarfeatures are used by our model in the full connection stage to extra externalcharacteristics of traffic flow. By extensive experiments and case studies inone Chinese provincial highway, our method shows clear improvement inpredictive accuracy than baselines and practical benefits in business.</description><author>Weilong Ding, Tianpu Zhang, Jianwu Wang, Zhuofeng Zhao</author><pubDate>Thu, 10 Aug 2023 15:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05601v1</guid></item><item><title>NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</title><link>http://arxiv.org/abs/2308.05600v1</link><description>Deep neural network (DNN) deployment has been confined to larger hardwaredevices due to their expensive computational requirements. This challenge hasrecently reached another scale with the emergence of large language models(LLMs). In order to reduce both their memory footprint and latency, a promisingtechnique is quantization. It consists in converting floating pointrepresentations to low bit-width fixed point representations, usually byassuming a uniform mapping onto a regular grid. This process, referred to inthe literature as uniform quantization, may however be ill-suited as most DNNweights and activations follow a bell-shaped distribution. This is even worseon LLMs whose weight distributions are known to exhibit large, high impact,outlier values. In this work, we propose an improvement over the most commonlyadopted way to tackle this limitation in deep learning models quantization,namely, non-uniform quantization. NUPES leverages automorphisms to preserve thescalar multiplications. Such transformations are derived from power functions.However, the optimization of the exponent parameter and weight values remains achallenging and novel problem which could not be solved with previous posttraining optimization techniques which only learn to round up or down weightvalues in order to preserve the predictive function. We circumvent thislimitation with a new paradigm: learning new quantized weights over the entirequantized space. Similarly, we enable the optimization of the power exponent,i.e. the optimization of the quantization operator itself during training byalleviating all the numerical instabilities. The resulting predictive functionis compatible with integer-only low-bit inference. We show the ability of themethod to achieve state-of-the-art compression rates in both, data-free anddata-driven configurations.</description><author>Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</author><pubDate>Thu, 10 Aug 2023 15:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05600v1</guid></item><item><title>You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content</title><link>http://arxiv.org/abs/2308.05596v1</link><description>The spread of toxic content online is an important problem that has adverseeffects on user experience online and in our society at large. Motivated by theimportance and impact of the problem, research focuses on developing solutionsto detect toxic content, usually leveraging machine learning (ML) modelstrained on human-annotated datasets. While these efforts are important, thesemodels usually do not generalize well and they can not cope with new trends(e.g., the emergence of new toxic terms). Currently, we are witnessing a shiftin the approach to tackling societal issues online, particularly leveraginglarge language models (LLMs) like GPT-3 or T5 that are trained on vast corporaand have strong generalizability. In this work, we investigate how we can useLLMs and prompt learning to tackle the problem of toxic content, particularlyfocusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection,and 3) Detoxification. We perform an extensive evaluation over five modelarchitectures and eight datasets demonstrating that LLMs with prompt learningcan achieve similar or even better performance compared to models trained onthese specific tasks. We find that prompt learning achieves around 10\%improvement in the toxicity classification task compared to the baselines,while for the toxic span detection task we find better performance to the bestbaseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for thedetoxification task, we find that prompt learning can successfully reduce theaverage toxicity score (from 0.775 to 0.213) while preserving semantic meaning.</description><author>Xinlei He, Savvas Zannettou, Yun Shen, Yang Zhang</author><pubDate>Thu, 10 Aug 2023 15:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05596v1</guid></item><item><title>Test-Time Selection for Robust Skin Lesion Analysis</title><link>http://arxiv.org/abs/2308.05595v1</link><description>Skin lesion analysis models are biased by artifacts placed during imageacquisition, which influence model predictions despite carrying no clinicalinformation. Solutions that address this problem by regularizing models toprevent learning those spurious features achieve only partial success, andexisting test-time debiasing techniques are inappropriate for skin lesionanalysis due to either making unrealistic assumptions on the distribution oftest data or requiring laborious annotation from medical practitioners. Wepropose TTS (Test-Time Selection), a human-in-the-loop method that leveragespositive (e.g., lesion area) and negative (e.g., artifacts) keypoints in testsamples. TTS effectively steers models away from exploiting spuriousartifact-related correlations without retraining, and with less annotationrequirements. Our solution is robust to a varying availability of annotations,and different levels of bias. We showcase on the ISIC2019 dataset (for which werelease a subset of annotated images) how our model could be deployed in thereal-world for mitigating bias.</description><author>Alceu Bissoto, Catarina Barata, Eduardo Valle, Sandra Avila</author><pubDate>Thu, 10 Aug 2023 15:08:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05595v1</guid></item><item><title>Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories</title><link>http://arxiv.org/abs/2308.05011v2</link><description>With the increasing volume of astronomical data generated by modern surveytelescopes, automated pipelines and machine learning techniques have becomecrucial for analyzing and extracting knowledge from these datasets. Anomalydetection, i.e. the task of identifying irregular or unexpected patterns in thedata, is a complex challenge in astronomy. In this paper, we proposeMulti-Class Deep Support Vector Data Description (MCDSVDD), an extension of thestate-of-the-art anomaly detection algorithm One-Class Deep SVDD, specificallydesigned to handle different inlier categories with distinct datadistributions. MCDSVDD uses a neural network to map the data into hyperspheres,where each hypersphere represents a specific inlier category. The distance ofeach sample from the centers of these hyperspheres determines the anomalyscore. We evaluate the effectiveness of MCDSVDD by comparing its performancewith several anomaly detection algorithms on a large dataset of astronomicallight-curves obtained from the Zwicky Transient Facility. Our resultsdemonstrate the efficacy of MCDSVDD in detecting anomalous sources whileleveraging the presence of different inlier categories. The code and the dataneeded to reproduce our results are publicly available athttps://github.com/mperezcarrasco/AnomalyALeRCE.</description><author>Manuel Pérez-Carrasco, Guillermo Cabrera-Vives, Lorena Hernández-García, Francisco Forster, Paula Sánchez-Sáez, Alejandra Muñoz Arancibia, Nicolás Astorga, Franz Bauer, Amelia Bayo, Martina Cádiz-Leyton, Marcio Catelan</author><pubDate>Thu, 10 Aug 2023 15:08:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05011v2</guid></item><item><title>Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length</title><link>http://arxiv.org/abs/2308.05585v1</link><description>The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role inshaping the impact of large language models (LLMs), contributing significantlyto controlling output toxicity and selecting output styles, particularly asLLMs often harbor misleading content, highlighting the urgency to align themwith human values for secure AI systems. The RLHF, characterized by complexity,instability, and sensitivity to hyperparameters, makes the evaluation of thereward model for complex tasks challenging, thereby further complicating theuse of Proximal Policy Optimization (PPO). In this paper, we introduce a simpletask designed to employ Gloden as a reward model that validates theeffectiveness of PPO and inspires it, primarily explaining the task ofutilizing PPO to manipulate the tokenizer length of the output generated by themodel. Experiments confirm that PPO is not only effective in manipulating theoutput tokenizer length to a certain extent in this type of task but alsoexhibits facilitated training once the influence of the reward model effect isexcluded, making it an exciting development.</description><author>Miao Fan, Chen Hu, Shuchang Zhou</author><pubDate>Thu, 10 Aug 2023 14:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05585v1</guid></item><item><title>Generative Diffusion Models for Radio Wireless Channel Modelling and Sampling</title><link>http://arxiv.org/abs/2308.05583v1</link><description>Channel modelling is essential to designing modern wireless communicationsystems. The increasing complexity of channel modelling and the cost ofcollecting high-quality wireless channel data have become major challenges. Inthis paper, we propose a diffusion model based channel sampling approach forrapidly synthesizing channel realizations from limited data. We use a diffusionmodel with a U Net based architecture operating in the frequency space domain.To evaluate how well the proposed model reproduces the true distribution ofchannels in the training dataset, two evaluation metrics are used: $i)$ theapproximate $2$-Wasserstein distance between real and generated distributionsof the normalized power spectrum in the antenna and frequency domains and $ii)$precision and recall metric for distributions. We show that, compared toexisting GAN based approaches which suffer from mode collapse and unstabletraining, our diffusion based approach trains stably and generates diverse andhigh-fidelity samples from the true channel distribution. We also show that wecan pretrain the model on a simulated urban macro-cellular channel dataset andfine-tune it on a smaller, out-of-distribution urban micro-cellular dataset,therefore showing that it is feasible to model real world channels usinglimited data with this approach.</description><author>Ushnish Sengupta, Chinkuo Jao, Alberto Bernacchia, Sattar Vakili, Da-shan Shiu</author><pubDate>Thu, 10 Aug 2023 14:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05583v1</guid></item><item><title>Category Feature Transformer for Semantic Segmentation</title><link>http://arxiv.org/abs/2308.05581v1</link><description>Aggregation of multi-stage features has been revealed to play a significantrole in semantic segmentation. Unlike previous methods employing point-wisesummation or concatenation for feature aggregation, this study proposes theCategory Feature Transformer (CFT) that explores the flow of category embeddingand transformation among multi-stage features through the prevalent multi-headattention mechanism. CFT learns unified feature embeddings for individualsemantic categories from high-level features during each aggregation processand dynamically broadcasts them to high-resolution features. Integrating theproposed CFT into a typical feature pyramid structure exhibits superiorperformance over a broad range of backbone networks. We conduct extensiveexperiments on popular semantic segmentation benchmarks. Specifically, theproposed CFT obtains a compelling 55.1% mIoU with greatly reduced modelparameters and computations on the challenging ADE20K dataset.</description><author>Quan Tang, Chuanjian Liu, Fagui Liu, Yifan Liu, Jun Jiang, Bowen Zhang, Kai Han, Yunhe Wang</author><pubDate>Thu, 10 Aug 2023 14:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05581v1</guid></item><item><title>Do Language Models Refer?</title><link>http://arxiv.org/abs/2308.05576v1</link><description>What do language models (LMs) do with language? Everyone agrees that theyproduce sequences of (mostly) coherent sentences. But are they saying anythingwith those strings or simply babbling in a convincing simulacrum of languageuse? This is a vague question, and there are many ways of making it precise.Here we will address one aspect of the question, namely, whether LMs' wordsrefer: that is, whether the outputs of LMs achieve "word-to-world" connections.There is prima facie reason to think they do not since LMs do not interact withthe world in the way that ordinary language users do. Drawing on insights fromthe externalist tradition in philosophy of language, we argue that appearancesare misleading and that there is good reason to think that LMs can refer.</description><author>Matthew Mandelkern, Tal Linzen</author><pubDate>Thu, 10 Aug 2023 14:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05576v1</guid></item><item><title>Symmetry Defense Against XGBoost Adversarial Perturbation Attacks</title><link>http://arxiv.org/abs/2308.05575v1</link><description>We examine whether symmetry can be used to defend tree-based ensembleclassifiers such as gradient-boosting decision trees (GBDTs) againstadversarial perturbation attacks. The idea is based on a recent symmetrydefense for convolutional neural network classifiers (CNNs) that utilizes CNNs'lack of invariance with respect to symmetries. CNNs lack invariance becausethey can classify a symmetric sample, such as a horizontally flipped image,differently from the original sample. CNNs' lack of invariance also means thatCNNs can classify symmetric adversarial samples differently from the incorrectclassification of adversarial samples. Using CNNs' lack of invariance, therecent CNN symmetry defense has shown that the classification of symmetricadversarial samples reverts to the correct sample classification. In order toapply the same symmetry defense to GBDTs, we examine GBDT invariance and arethe first to show that GBDTs also lack invariance with respect to symmetries.We apply and evaluate the GBDT symmetry defense for nine datasets against sixperturbation attacks with a threat model that ranges from zero-knowledge toperfect-knowledge adversaries. Using the feature inversion symmetry againstzero-knowledge adversaries, we achieve up to 100% accuracy on adversarialsamples even when default and robust classifiers have 0% accuracy. Using thefeature inversion and horizontal flip symmetries against perfect-knowledgeadversaries, we achieve up to over 95% accuracy on adversarial samples for theGBDT classifier of the F-MNIST dataset even when default and robust classifiershave 0% accuracy.</description><author>Blerta Lindqvist</author><pubDate>Thu, 10 Aug 2023 14:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05575v1</guid></item><item><title>Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages</title><link>http://arxiv.org/abs/2308.05574v1</link><description>Current research in zero-shot translation is plagued by several issues suchas high compute requirements, increased training time and off targettranslations. Proposed remedies often come at the cost of additional data orcompute requirements. Pivot based neural machine translation is preferred overa single-encoder model for most settings despite the increased training andevaluation time. In this work, we overcome the shortcomings of zero-shottranslation by taking advantage of transliteration and linguistic similarity.We build a single encoder-decoder neural machine translation system forDravidian-Dravidian multilingual translation and perform zero-shot translation.We compare the data vs zero-shot accuracy tradeoff and evaluate the performanceof our vanilla method against the current state of the art pivot based method.We also test the theory that morphologically rich languages require largevocabularies by restricting the vocabulary using an optimal transport basedtechnique. Our model manages to achieves scores within 3 BLEU of large-scalepivot-based models when it is trained on 50\% of the language directions.</description><author>Danish Ebadulla, Rahul Raman, S. Natarajan, Hridhay Kiran Shetty, Ashish Harish Shenoy</author><pubDate>Thu, 10 Aug 2023 14:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05574v1</guid></item><item><title>Deep incremental learning models for financial temporal tabular datasets with distribution shifts</title><link>http://arxiv.org/abs/2303.07925v7</link><description>We present a robust deep incremental learning framework for regression taskson financial temporal tabular datasets which is built upon the incremental useof commonly available tabular and time series prediction models to adapt todistributional shifts typical of financial datasets. The framework uses asimple basic building block (decision trees) to build self-similar models ofany required complexity to deliver robust performance under adverse situationssuch as regime changes, fat-tailed distributions, and low signal-to-noiseratios. As a detailed study, we demonstrate our scheme using XGBoost modelstrained on the Numerai dataset and show that a two layer deep ensemble ofXGBoost models over different model snapshots delivers high quality predictionsunder different market regimes. We also show that the performance of XGBoostmodels with different number of boosting rounds in three scenarios (small,standard and large) is monotonically increasing with respect to model size andconverges towards the generalisation upper bound. We also evaluate therobustness of the model under variability of different hyperparameters, such asmodel complexity and data sampling settings. Our model has low hardwarerequirements as no specialised neural architectures are used and each basemodel can be independently trained in parallel.</description><author>Thomas Wong, Mauricio Barahona</author><pubDate>Thu, 10 Aug 2023 14:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07925v7</guid></item><item><title>C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT</title><link>http://arxiv.org/abs/2308.05567v1</link><description>Large language models (LLMs), such as ChatGPT, have demonstrated outstandingperformance in various fields, particularly in natural language understandingand generation tasks. In complex application scenarios, users tend to engage inmulti-turn conversations with ChatGPT to keep contextual information and obtaincomprehensive responses. However, human forgetting and model contextualforgetting remain prominent issues in multi-turn conversation scenarios, whichchallenge the users' conversation comprehension and contextual continuity forChatGPT. To address these challenges, we propose an interactive conversationvisualization system called C5, which includes Global View, Topic View, andContext-associated Q\&amp;A View. The Global View uses the GitLog diagram metaphorto represent the conversation structure, presenting the trend of conversationevolution and supporting the exploration of locally salient features. The TopicView is designed to display all the question and answer nodes and theirrelationships within a topic using the structure of a knowledge graph, therebydisplay the relevance and evolution of conversations. The Context-associatedQ\&amp;A View consists of three linked views, which allow users to exploreindividual conversations deeply while providing specific contextual informationwhen posing questions. The usefulness and effectiveness of C5 were evaluatedthrough a case study and a user study.</description><author>Pan Liang, Danwei Ye, Zihao Zhu, Yunchao Wang, Wang Xia, Ronghua Liang, Guodao Sun</author><pubDate>Thu, 10 Aug 2023 14:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05567v1</guid></item><item><title>AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting</title><link>http://arxiv.org/abs/2308.05566v1</link><description>We introduce AutoGluon-TimeSeries - an open-source AutoML library forprobabilistic time series forecasting. Focused on ease of use and robustness,AutoGluon-TimeSeries enables users to generate accurate point and quantileforecasts with just 3 lines of Python code. Built on the design philosophy ofAutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecastingmodels to deliver high accuracy within a short training time.AutoGluon-TimeSeries combines both conventional statistical models,machine-learning based forecasting approaches, and ensembling techniques. Inour evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstratesstrong empirical performance, outperforming a range of forecasting methods interms of both point and quantile forecast accuracy, and often even improvingupon the best-in-hindsight combination of prior methods.</description><author>Oleksandr Shchur, Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, Yuyang Wang</author><pubDate>Thu, 10 Aug 2023 14:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05566v1</guid></item><item><title>Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns</title><link>http://arxiv.org/abs/2308.05564v1</link><description>Large skew-t factor copula models are attractive for the modeling offinancial data because they allow for asymmetric and extreme tail dependence.We show that the copula implicit in the skew-t distribution of Azzalini andCapitanio (2003) allows for a higher level of pairwise asymmetric dependencethan two popular alternative skew-t copulas. Estimation of this copula in highdimensions is challenging, and we propose a fast and accurate Bayesianvariational inference (VI) approach to do so. The method uses a conditionallyGaussian generative representation of the skew-t distribution to define anaugmented posterior that can be approximated accurately. A fast stochasticgradient ascent algorithm is used to solve the variational optimization. Thenew methodology is used to estimate copula models for intraday returns from2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneityin asymmetric dependence over equity pairs, in addition to the variability inpairwise correlations. We show that intraday predictive densities from theskew-t copula are more accurate than from some other copula models, whileportfolio selection strategies based on the estimated pairwise taildependencies improve performance relative to the benchmark index.</description><author>Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</author><pubDate>Thu, 10 Aug 2023 14:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05564v1</guid></item><item><title>Recent Advancements In The Field Of Deepfake Detection</title><link>http://arxiv.org/abs/2308.05563v1</link><description>A deepfake is a photo or video of a person whose image has been digitallyaltered or partially replaced with an image of someone else. Deepfakes have thepotential to cause a variety of problems and are often used maliciously. Acommon usage is altering videos of prominent political figures and celebrities.These deepfakes can portray them making offensive, problematic, and/or untruestatements. Current deepfakes can be very realistic, and when used in this way,can spread panic and even influence elections and political opinions. There aremany deepfake detection strategies currently in use but finding the mostcomprehensive and universal method is critical. So, in this survey we willaddress the problems of malicious deepfake creation and the lack of universaldeepfake detection methods. Our objective is to survey and analyze a variety ofcurrent methods and advances in the field of deepfake detection.</description><author>Natalie Krueger, Dr. Mounika Vanamala, Dr. Rushit Dave</author><pubDate>Thu, 10 Aug 2023 14:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05563v1</guid></item><item><title>Cross-Domain Product Representation Learning for Rich-Content E-Commerce</title><link>http://arxiv.org/abs/2308.05550v1</link><description>The proliferation of short video and live-streaming platforms hasrevolutionized how consumers engage in online shopping. Instead of browsingproduct pages, consumers are now turning to rich-content e-commerce, where theycan purchase products through dynamic and interactive media like short videosand live streams. This emerging form of online shopping has introducedtechnical challenges, as products may be presented differently across variousmedia domains. Therefore, a unified product representation is essential forachieving cross-domain product recognition to ensure an optimal user searchexperience and effective product recommendations. Despite the urgent industrialneed for a unified cross-domain product representation, previous studies havepredominantly focused only on product pages without taking into account shortvideos and live streams. To fill the gap in the rich-content e-commerce area,in this paper, we introduce a large-scale cRoss-dOmain Product Ecognitiondataset, called ROPE. ROPE covers a wide range of product categories andcontains over 180,000 products, corresponding to millions of short videos andlive streams. It is the first dataset to cover product pages, short videos, andlive streams simultaneously, providing the basis for establishing a unifiedproduct representation across different media domains. Furthermore, we proposea Cross-dOmain Product rEpresentation framework, namely COPE, which unifiesproduct representations in different domains through multimodal learningincluding text and vision. Extensive experiments on downstream tasksdemonstrate the effectiveness of COPE in learning a joint feature space for allproduct domains.</description><author>Xuehan Bai, Yan Li, Yanhua Cheng, Wenjie Yang, Quan Chen, Han Li</author><pubDate>Thu, 10 Aug 2023 14:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05550v1</guid></item><item><title>A hybrid deep-learning-metaheuristic framework for bi-level network design problems</title><link>http://arxiv.org/abs/2303.06024v3</link><description>This study proposes a hybrid deep-learning-metaheuristic framework with abi-level architecture for road network design problems (NDPs). We train a graphneural network (GNN) to approximate the solution of the user equilibrium (UE)traffic assignment problem and use inferences made by the trained model tocalculate fitness function evaluations of a genetic algorithm (GA) toapproximate solutions for NDPs. Using three test networks, two NDP variants andan exact solver as benchmark, we show that on average, our proposed frameworkcan provide solutions within 1.5% gap of the best results in less than 0.5% ofthe time used by the exact solution procedure. Our framework can be utilizedwithin an expert system for infrastructure planning to determine the bestinfrastructure planning and management decisions under different scenarios.Given the flexibility of the framework, it can easily be adapted to many otherdecision problems that can be modeled as bi-level problems on graphs. Moreover,we foreseen interesting future research directions, thus we also put forward abrief research agenda for this topic. The key observation from our researchthat can shape future research is that the fitness function evaluation timeusing the inferences made by the GNN model was in the order of milliseconds,which points to an opportunity and a need for novel heuristics that 1) can copewell with noisy fitness function values provided by deep learning models, and2) can use the significantly enlarged efficiency of the evaluation step toexplore the search space effectively (rather than efficiently). This opens anew avenue for a modern class of metaheuristics that are crafted for use withAI-powered predictors.</description><author>Bahman Madadi, Goncalo Homem de Almeida Correia</author><pubDate>Thu, 10 Aug 2023 14:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06024v3</guid></item><item><title>Learning (With) Distributed Optimization</title><link>http://arxiv.org/abs/2308.05548v1</link><description>This paper provides an overview of the historical progression of distributedoptimization techniques, tracing their development from early duality-basedmethods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergenceof the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN)algorithm. The initial focus on Lagrangian relaxation for convex problems anddecomposition strategies led to the refinement of methods like the AlternatingDirection Method of Multipliers (ADMM). The resurgence of interest indistributed optimization in the late 2000s, particularly in machine learningand imaging, demonstrated ADMM's practical efficacy and its unifying potential.This overview also highlights the emergence of the proximal center method andits applications in diverse domains. Furthermore, the paper underscores thedistinctive features of ALADIN, which offers convergence guarantees fornon-convex scenarios without introducing auxiliary variables, differentiatingit from traditional augmentation techniques. In essence, this work encapsulatesthe historical trajectory of distributed optimization and underscores thepromising prospects of ALADIN in addressing non-convex optimization challenges.</description><author>Aadharsh Aadhithya A, Abinesh S, Akshaya J, Jayanth M, Vishnu Radhakrishnan, Sowmya V, Soman K. P</author><pubDate>Thu, 10 Aug 2023 13:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05548v1</guid></item><item><title>Enhancing AUV Autonomy With Model Predictive Path Integral Control</title><link>http://arxiv.org/abs/2308.05547v1</link><description>Autonomous underwater vehicles (AUVs) play a crucial role in surveying marineenvironments, carrying out underwater inspection tasks, and ocean exploration.However, in order to ensure that the AUV is able to carry out its missionsuccessfully, a control system capable of adapting to changing environmentalconditions is required. Furthermore, to ensure the robotic platform's safeoperation, the onboard controller should be able to operate under certainconstraints. In this work, we investigate the feasibility of Model PredictivePath Integral Control (MPPI) for the control of an AUV. We utilise a non-linearmodel of the AUV to propagate the samples of the MPPI, which allow us tocompute the control action in real time. We provide a detailed evaluation ofthe effect of the main hyperparameters on the performance of the MPPIcontroller. Furthermore, we compared the performance of the proposed methodwith a classical PID and Cascade PID approach, demonstrating the superiority ofour proposed controller. Finally, we present results where environmentalconstraints are added and show how MPPI can handle them by simply incorporatingthose constraints in the cost function.</description><author>Pierre Nicolay, Yvan Petillot, Mykhaylo Marfeychuk, Sen Wang, Ignacio Carlucho</author><pubDate>Thu, 10 Aug 2023 13:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05547v1</guid></item><item><title>Deep Richardson-Lucy Deconvolution for Low-Light Image Deblurring</title><link>http://arxiv.org/abs/2308.05543v1</link><description>Images taken under the low-light condition often contain blur and saturatedpixels at the same time. Deblurring images with saturated pixels is quitechallenging. Because of the limited dynamic range, the saturated pixels areusually clipped in the imaging process and thus cannot be modeled by the linearblur model. Previous methods use manually designed smooth functions toapproximate the clipping procedure. Their deblurring processes often requireempirically defined parameters, which may not be the optimal choices fordifferent images. In this paper, we develop a data-driven approach to model thesaturated pixels by a learned latent map. Based on the new model, the non-blinddeblurring task can be formulated into a maximum a posterior (MAP) problem,which can be effectively solved by iteratively computing the latent map and thelatent image. Specifically, the latent map is computed by learning from a mapestimation network (MEN), and the latent image estimation process isimplemented by a Richardson-Lucy (RL)-based updating scheme. To estimatehigh-quality deblurred images without amplified artifacts, we develop a priorestimation network (PEN) to obtain prior information, which is furtherintegrated into the RL scheme. Experimental results demonstrate that theproposed method performs favorably against state-of-the-art algorithms bothquantitatively and qualitatively on synthetic and real-world images.</description><author>Liang Chen, Jiawei Zhang, Zhenhua Li, Yunxuan Wei, Faming Fang, Jimmy Ren, Jinshan Pan</author><pubDate>Thu, 10 Aug 2023 13:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05543v1</guid></item><item><title>Symmetry Defense Against CNN Adversarial Perturbation Attacks</title><link>http://arxiv.org/abs/2210.04087v3</link><description>This paper uses symmetry to make Convolutional Neural Network classifiers(CNNs) robust against adversarial perturbation attacks. Such attacks addperturbation to original images to generate adversarial images that foolclassifiers such as road sign classifiers of autonomous vehicles. Althoughsymmetry is a pervasive aspect of the natural world, CNNs are unable to handlesymmetry well. For example, a CNN can classify an image differently from itsmirror image. For an adversarial image that misclassifies with a wrong label$l_w$, CNN inability to handle symmetry means that a symmetric adversarialimage can classify differently from the wrong label $l_w$. Further than that,we find that the classification of a symmetric adversarial image reverts to thecorrect label. To classify an image when adversaries are unaware of thedefense, we apply symmetry to the image and use the classification label of thesymmetric image. To classify an image when adversaries are aware of thedefense, we use mirror symmetry and pixel inversion symmetry to form a symmetrygroup. We apply all the group symmetries to the image and decide on the outputlabel based on the agreement of any two of the classification labels of thesymmetry images. Adaptive attacks fail because they need to rely on lossfunctions that use conflicting CNN output values for symmetric images. Withoutattack knowledge, the proposed symmetry defense succeeds against bothgradient-based and random-search attacks, with up to near-default accuraciesfor ImageNet. The defense even improves the classification accuracy of originalimages.</description><author>Blerta Lindqvist</author><pubDate>Thu, 10 Aug 2023 13:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.04087v3</guid></item><item><title>Robust Asymmetric Loss for Multi-Label Long-Tailed Learning</title><link>http://arxiv.org/abs/2308.05542v1</link><description>In real medical data, training samples typically show long-taileddistributions with multiple labels. Class distribution of the medical data hasa long-tailed shape, in which the incidence of different diseases is quitevaried, and at the same time, it is not unusual for images taken fromsymptomatic patients to be multi-label diseases. Therefore, in this paper, weconcurrently address these two issues by putting forth a robust asymmetric losson the polynomial function. Since our loss tackles both long-tailed andmulti-label classification problems simultaneously, it leads to a complexdesign of the loss function with a large number of hyper-parameters. Although amodel can be highly fine-tuned due to a large number of hyper-parameters, it isdifficult to optimize all hyper-parameters at the same time, and there might bea risk of overfitting a model. Therefore, we regularize the loss function usingthe Hill loss approach, which is beneficial to be less sensitive against thenumerous hyper-parameters so that it reduces the risk of overfitting the model.For this reason, the proposed loss is a generic method that can be applied tomost medical image classification tasks and does not make the training processmore time-consuming. We demonstrate that the proposed robust asymmetric lossperforms favorably against the long-tailed with multi-label medical imageclassification in addition to the various long-tailed single-label datasets.Notably, our method achieves Top-5 results on the CXR-LT dataset of the ICCVCVAMD 2023 competition. We opensource our implementation of the robustasymmetric loss in the public repository: https://github.com/kalelpark/RAL.</description><author>Wongi Park, Inhyuk Park, Sungeun Kim, Jongbin Ryu</author><pubDate>Thu, 10 Aug 2023 13:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05542v1</guid></item><item><title>Functional Neural Networks: Shift invariant models for functional data with applications to EEG classification</title><link>http://arxiv.org/abs/2301.05869v2</link><description>It is desirable for statistical models to detect signals of interestindependently of their position. If the data is generated by some smoothprocess, this additional structure should be taken into account. We introduce anew class of neural networks that are shift invariant and preserve smoothnessof the data: functional neural networks (FNNs). For this, we use methods fromfunctional data analysis (FDA) to extend multi-layer perceptrons andconvolutional neural networks to functional data. We propose different modelarchitectures, show that the models outperform a benchmark model from FDA interms of accuracy and successfully use FNNs to classify electroencephalography(EEG) data.</description><author>Florian Heinrichs, Mavin Heim, Corinna Weber</author><pubDate>Thu, 10 Aug 2023 13:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05869v2</guid></item><item><title>Forward-Forward Training of an Optical Neural Network</title><link>http://arxiv.org/abs/2305.19170v2</link><description>Neural networks (NN) have demonstrated remarkable capabilities in varioustasks, but their computation-intensive nature demands faster and moreenergy-efficient hardware implementations. Optics-based platforms, usingtechnologies such as silicon photonics and spatial light modulators, offerpromising avenues for achieving this goal. However, training multiple trainablelayers in tandem with these physical systems poses challenges, as they aredifficult to fully characterize and describe with differentiable functions,hindering the use of error backpropagation algorithm. The recently introducedForward-Forward Algorithm (FFA) eliminates the need for perfectcharacterization of the learning system and shows promise for efficienttraining with large numbers of programmable parameters. The FFA does notrequire backpropagating an error signal to update the weights, rather theweights are updated by only sending information in one direction. The localloss function for each set of trainable weights enables low-power analoghardware implementations without resorting to metaheuristic algorithms orreinforcement learning. In this paper, we present an experiment utilizingmultimode nonlinear wave propagation in an optical fiber demonstrating thefeasibility of the FFA approach using an optical system. The results show thatincorporating optical transforms in multilayer NN architectures trained withthe FFA, can lead to performance improvements, even with a relatively smallnumber of trainable weights. The proposed method offers a new path to thechallenge of training optical NNs and provides insights into leveragingphysical transformations for enhancing NN performance.</description><author>Ilker Oguz, Junjie Ke, Qifei Wang, Feng Yang, Mustafa Yildirim, Niyazi Ulas Dinc, Jih-Liang Hsieh, Christophe Moser, Demetri Psaltis</author><pubDate>Thu, 10 Aug 2023 13:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19170v2</guid></item><item><title>Is there progress in activity progress prediction?</title><link>http://arxiv.org/abs/2308.05533v1</link><description>Activity progress prediction aims to estimate what percentage of an activityhas been completed. Currently this is done with machine learning approaches,trained and evaluated on complicated and realistic video datasets. The videosin these datasets vary drastically in length and appearance. And some of theactivities have unanticipated developments, making activity progressiondifficult to estimate. In this work, we examine the results obtained byexisting progress prediction methods on these datasets. We find that currentprogress prediction methods seem not to extract useful visual information forthe progress prediction task. Therefore, these methods fail to exceed simpleframe-counting baselines. We design a precisely controlled dataset for activityprogress prediction and on this synthetic dataset we show that the consideredmethods can make use of the visual information, when this directly relates tothe progress prediction. We conclude that the progress prediction task isill-posed on the currently used real-world datasets. Moreover, to fairlymeasure activity progression we advise to consider a, simple but effective,frame-counting baseline.</description><author>Frans de Boer, Jan C. van Gemert, Jouke Dijkstra, Silvia L. Pintea</author><pubDate>Thu, 10 Aug 2023 13:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05533v1</guid></item><item><title>Conditional Generative Models for Learning Stochastic Processes</title><link>http://arxiv.org/abs/2304.10382v4</link><description>A framework to learn a multi-modal distribution is proposed, denoted as theConditional Quantum Generative Adversarial Network (C-qGAN). The neural networkstructure is strictly within a quantum circuit and, as a consequence, is shownto represent a more efficient state preparation procedure than current methods.This methodology has the potential to speed-up algorithms, such as Monte Carloanalysis. In particular, after demonstrating the effectiveness of the networkin the learning task, the technique is applied to price Asian optionderivatives, providing the foundation for further research on otherpath-dependent options.</description><author>Salvatore Certo, Anh Pham, Nicolas Robles, Andrew Vlasic</author><pubDate>Thu, 10 Aug 2023 13:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10382v4</guid></item><item><title>Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI</title><link>http://arxiv.org/abs/2308.05525v1</link><description>The ability to cope accurately and fast with Out-Of-Distribution (OOD)samples is crucial in real-world safety demanding applications. In this work wefirst study the interplay between critical points of 3D point clouds and OODsamples. Our findings are that common corruptions and outliers are ofteninterpreted as critical points. We generalize the notion of critical pointsinto importance measures. We show that training a classification network basedonly on less important points dramatically improves robustness, at a cost ofminor performance loss on the clean set. We observe that normalized entropy ishighly informative for corruption analysis. An adaptive threshold based onnormalized entropy is suggested for selecting the set of uncritical points. Ourproposed importance measure is extremely fast to compute. We show it can beused for a variety of applications, such as Explainable AI (XAI), OutlierRemoval, Uncertainty Estimation, Robust Classification and Adversarial Defense.We reach SOTA results on the two latter tasks.</description><author>Meir Yossef Levi, Guy Gilboa</author><pubDate>Thu, 10 Aug 2023 13:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05525v1</guid></item><item><title>Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning</title><link>http://arxiv.org/abs/2308.05522v1</link><description>Retrosynthesis consists of breaking down a chemical compound recursivelystep-by-step into molecular precursors until a set of commercially availablemolecules is found with the goal to provide a synthesis route. Its two primaryresearch directions, single-step retrosynthesis prediction, which models thechemical reaction logic, and multi-step synthesis planning, which tries to findthe correct sequence of reactions, are inherently intertwined. Still, thisconnection is not reflected in contemporary research. In this work, we combinethese two major research directions by applying multiple single-stepretrosynthesis models within multi-step synthesis planning and analyzing theirimpact using public and proprietary reaction data. We find a disconnectionbetween high single-step performance and potential route-finding success,suggesting that single-step models must be evaluated within synthesis planningin the future. Furthermore, we show that the commonly used single-stepretrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluationtask does not represent model performance and scalability on larger and morediverse datasets. For multi-step synthesis planning, we show that the choice ofthe single-step model can improve the overall success rate of synthesisplanning by up to +28% compared to the commonly used baseline model. Finally,we show that each single-step model finds unique synthesis routes, and differsin aspects such as route-finding success, the number of found synthesis routes,and chemical validity, making the combination of single-step retrosynthesisprediction and multi-step synthesis planning a crucial aspect when developingfuture methods.</description><author>Paula Torren-Peraire, Alan Kai Hassen, Samuel Genheden, Jonas Verhoeven, Djork-Arne Clevert, Mike Preuss, Igor Tetko</author><pubDate>Thu, 10 Aug 2023 13:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05522v1</guid></item><item><title>Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU</title><link>http://arxiv.org/abs/2308.05515v1</link><description>The ability of robots to autonomously navigate through 3D environmentsdepends on their comprehension of spatial concepts, ranging from low-levelgeometry to high-level semantics, such as objects, places, and buildings. Toenable such comprehension, 3D scene graphs have emerged as a robust tool forrepresenting the environment as a layered graph of concepts and theirrelationships. However, building these representations using monocular visionsystems in real-time remains a difficult task that has not been explored indepth. This paper puts forth a real-time spatial perception system Mono-Hydra,combining a monocular camera and an IMU sensor setup, focusing on indoorscenarios. However, the proposed approach is adaptable to outdoor applications,offering flexibility in its potential uses. The system employs a suite of deeplearning algorithms to derive depth and semantics. It uses a robocentricvisual-inertial odometry (VIO) algorithm based on square-root information,thereby ensuring consistent visual odometry with an IMU and a monocular camera.This system achieves sub-20 cm error in real-time processing at 15 fps,enabling real-time 3D scene graph construction using a laptop GPU (NVIDIA3080). This enhances decision-making efficiency and effectiveness in simplecamera setups, augmenting robotic system agility. We make Mono-Hydra publiclyavailable at: https://github.com/UAV-Centre-ITC/Mono_Hydra</description><author>U. V. B. L. Udugama, G. Vosselman, F. Nex</author><pubDate>Thu, 10 Aug 2023 12:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05515v1</guid></item><item><title>Decoupled Diffusion Models with Explicit Transition Probability</title><link>http://arxiv.org/abs/2306.13720v4</link><description>Recent diffusion probabilistic models (DPMs) have shown remarkable abilitiesof generated content, however, they often suffer from complex forwardprocesses, resulting in inefficient solutions for the reversed process andprolonged sampling times. In this paper, we aim to address the aforementionedchallenges by focusing on the diffusion process itself that we propose todecouple the intricate diffusion process into two comparatively simpler processto improve the generative efficacy and speed. In particular, we present a noveldiffusion paradigm named DDM (Decoupled Diffusion Models) based on the Itodiffusion process, in which the image distribution is approximated by anexplicit transition probability while the noise path is controlled by thestandard Wiener process. We find that decoupling the diffusion process reducesthe learning difficulty and the explicit transition probability improves thegenerative speed significantly. We prove a new training objective for DPM,which enables the model to learn to predict the noise and image componentsseparately. Moreover, given the novel forward diffusion equation, we derive thereverse denoising formula of DDM that naturally supports fewer steps ofgeneration without ordinary differential equation (ODE) based accelerators. Ourexperiments demonstrate that DDM outperforms previous DPMs by a large margin infewer function evaluations setting and gets comparable performances in longfunction evaluations setting. We also show that our framework can be applied toimage-conditioned generation and high-resolution image synthesis, and that itcan generate high-quality images with only 10 function evaluations.</description><author>Yuhang Huang, Zheng Qin, Xinwang Liu, Kai Xu</author><pubDate>Thu, 10 Aug 2023 12:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13720v4</guid></item><item><title>BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion</title><link>http://arxiv.org/abs/2307.10816v3</link><description>Recent text-to-image diffusion models have demonstrated an astonishingcapacity to generate high-quality images. However, researchers mainly studiedthe way of synthesizing images with only text prompts. While some works haveexplored using other modalities as conditions, considerable paired data, e.g.,box/mask-image pairs, and fine-tuning time are required for nurturing models.As such paired data is time-consuming and labor-intensive to acquire andrestricted to a closed set, this potentially becomes the bottleneck forapplications in an open world. This paper focuses on the simplest form ofuser-provided conditions, e.g., box or scribble. To mitigate the aforementionedproblem, we propose a training-free method to control objects and contexts inthe synthesized images adhering to the given spatial conditions. Specifically,three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,are designed and seamlessly integrated into the denoising step of diffusionmodels, requiring no additional training and massive annotated layout data.Extensive results show that the proposed constraints can control what and whereto present in the images while retaining the ability of the Stable Diffusionmodel to synthesize with high fidelity and diverse concept coverage. The codeis publicly available at https://github.com/Sierkinhane/BoxDiff.</description><author>Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, Mike Zheng Shou</author><pubDate>Thu, 10 Aug 2023 12:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10816v3</guid></item><item><title>On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem</title><link>http://arxiv.org/abs/2308.05509v1</link><description>This paper is devoted to studying the optimal expressive power of ReLU deepneural networks (DNNs) and its application in approximation via the KolmogorovSuperposition Theorem. We first constructively prove that any continuouspiecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can berepresented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer.Subsequently, we demonstrate that this construction is optimal regarding theparameter count of the DNNs, achieved through investigating the shatteringcapacity of ReLU DNNs. Moreover, by invoking the Kolmogorov SuperpositionTheorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrarywidth and depth when dealing with continuous functions in high-dimensionalspaces.</description><author>Juncai He</author><pubDate>Thu, 10 Aug 2023 12:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05509v1</guid></item><item><title>Multi-domain Recommendation with Embedding Disentangling and Domain Alignment</title><link>http://arxiv.org/abs/2308.05508v1</link><description>Multi-domain recommendation (MDR) aims to provide recommendations fordifferent domains (e.g., types of products) with overlapping users/items and iscommon for platforms such as Amazon, Facebook, and LinkedIn that host multipleservices. Existing MDR models face two challenges: First, it is difficult todisentangle knowledge that generalizes across domains (e.g., a user likes cheapitems) and knowledge specific to a single domain (e.g., a user likes blueclothing but not blue cars). Second, they have limited ability to transferknowledge across domains with small overlaps. We propose a new MDR method namedEDDA with two key components, i.e., embedding disentangling recommender anddomain alignment, to tackle the two challenges respectively. In particular, theembedding disentangling recommender separates both the model and embedding forthe inter-domain part and the intra-domain part, while most existing MDRmethods only focus on model-level disentangling. The domain alignment leveragesrandom walks from graph processing to identify similar user/item pairs fromdifferent domains and encourages similar user/item pairs to have similarembeddings, enhancing knowledge transfer. We compare EDDA with 12state-of-the-art baselines on 3 real datasets. The results show that EDDAconsistently outperforms the baselines on all datasets and domains. Alldatasets and codes are available at https://github.com/Stevenn9981/EDDA.</description><author>Wentao Ning, Xiao Yan, Weiwen Liu, Reynold Cheng, Rui Zhang, Bo Tang</author><pubDate>Thu, 10 Aug 2023 12:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05508v1</guid></item><item><title>EFX Allocations Exist for Binary Valuations</title><link>http://arxiv.org/abs/2308.05503v1</link><description>We study the fair division problem and the existence of allocationssatisfying the fairness criterion envy-freeness up to any item (EFX). Theexistence of EFX allocations is a major open problem in the fair divisionliterature. We consider binary valuations where the marginal gain of the valueby receiving an extra item is either $0$ or $1$. Babaioff et al. [2021] provedthat EFX allocations always exist for binary and submodular valuations. In thispaper, by using completely different techniques, we extend this existenceresult to general binary valuations that are not necessarily submodular, and wepresent a polynomial time algorithm for computing an EFX allocation.</description><author>Xiaolin Bu, Jiaxin Song, Ziqi Yu</author><pubDate>Thu, 10 Aug 2023 12:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05503v1</guid></item></channel></rss>