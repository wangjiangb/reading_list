<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 16 Jun 2024 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding</title><link>http://arxiv.org/abs/2406.09418v1</link><description>Building on the advances of language models, Large Multimodal Models (LMMs)have contributed significant improvements in video understanding. While thecurrent video LMMs utilize advanced Large Language Models (LLMs), they rely oneither image or video encoders to process visual inputs, each of which has itsown limitations. Image encoders excel at capturing rich spatial details fromframe sequences but lack explicit temporal context, which can be important invideos with intricate action sequences. On the other hand, video encodersprovide temporal context but are often limited by computational constraintsthat lead to processing only sparse frames at lower resolutions, resulting inreduced contextual and spatial understanding. To this end, we introduceVideoGPT+, which combines the complementary benefits of the image encoder (fordetailed spatial understanding) and the video encoder (for global temporalcontext modeling). The model processes videos by dividing them into smallersegments and applies an adaptive pooling strategy on features extracted by bothimage and video encoders. Our architecture showcases improved performanceacross multiple video benchmarks, including VCGBench, MVBench and Zero-shotquestion-answering. Further, we develop 112K video-instruction set using anovel semi-automatic annotation pipeline which further improves the modelperformance. Additionally, to comprehensively evaluate video LMMs, we presentVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,science, gaming, and surveillance videos. This benchmark with 4,354question-answer pairs evaluates the generalization of existing LMMs on densevideo captioning, spatial and temporal understanding, and complex reasoning,ensuring comprehensive assessment across diverse video types and dynamics.Code: https://github.com/mbzuai-oryx/VideoGPT-plus.</description><author>Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Khan</author><pubDate>Thu, 13 Jun 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09418v1</guid></item><item><title>An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels</title><link>http://arxiv.org/abs/2406.09415v1</link><description>This work does not introduce a new method. Instead, we present an interestingfinding that questions the necessity of the inductive bias -- locality inmodern computer vision architectures. Concretely, we find that vanillaTransformers can operate by directly treating each individual pixel as a tokenand achieve highly performant results. This is substantially different from thepopular design in Vision Transformer, which maintains the inductive bias fromConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as atoken). We mainly showcase the effectiveness of pixels-as-tokens across threewell-studied tasks in computer vision: supervised learning for objectclassification, self-supervised learning via masked autoencoding, and imagegeneration with diffusion models. Although directly operating on individualpixels is less computationally practical, we believe the community must beaware of this surprising piece of knowledge when devising the next generationof neural architectures for computer vision.</description><author>Duy-Kien Nguyen, Mahmoud Assran, Unnat Jain, Martin R. Oswald, Cees G. M. Snoek, Xinlei Chen</author><pubDate>Thu, 13 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09415v1</guid></item><item><title>Rethinking Score Distillation as a Bridge Between Image Distributions</title><link>http://arxiv.org/abs/2406.09417v1</link><description>Score distillation sampling (SDS) has proven to be an important tool,enabling the use of large-scale diffusion priors for tasks operating indata-poor domains. Unfortunately, SDS has a number of characteristic artifactsthat limit its usefulness in general-purpose applications. In this paper, wemake progress toward understanding the behavior of SDS and its variants byviewing them as solving an optimal-cost transport path from a sourcedistribution to a target distribution. Under this new interpretation, thesemethods seek to transport corrupted images (source) to the natural imagedistribution (target). We argue that current methods' characteristic artifactsare caused by (1) linear approximation of the optimal path and (2) poorestimates of the source distribution. We show that calibrating the textconditioning of the source distribution can produce high-quality generation andtranslation results with little extra overhead. Our method can be easilyapplied across many domains, matching or beating the performance of specializedmethods. We demonstrate its utility in text-to-2D, text-based NeRFoptimization, translating paintings to real images, optical illusiongeneration, and 3D sketch-to-real. We compare our method to existing approachesfor score distillation sampling and show that it can produce high-frequencydetails with realistic colors.</description><author>David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</author><pubDate>Thu, 13 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09417v1</guid></item><item><title>Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2406.09416v1</link><description>This paper presents innovative enhancements to diffusion models byintegrating a novel multi-resolution network and time-dependent layernormalization. Diffusion models have gained prominence for their effectivenessin high-fidelity image generation. While conventional approaches rely onconvolutional U-Net architectures, recent Transformer-based designs havedemonstrated superior performance and scalability. However, Transformerarchitectures, which tokenize input data (via "patchification"), face atrade-off between visual fidelity and computational complexity due to thequadratic nature of self-attention operations concerning token length. Whilelarger patch sizes enable attention computation efficiency, they struggle tocapture fine-grained visual details, leading to image distortions. To addressthis challenge, we propose augmenting the Diffusion model with theMulti-Resolution network (DiMR), a framework that refines features acrossmultiple resolutions, progressively enhancing detail from low to highresolution. Additionally, we introduce Time-Dependent Layer Normalization(TD-LN), a parameter-efficient approach that incorporates time-dependentparameters into layer normalization to inject time information and achievesuperior performance. Our method's efficacy is demonstrated on theclass-conditional ImageNet generation benchmark, where DiMR-XL variantsoutperform prior diffusion models, setting new state-of-the-art FID scores of1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:https://qihao067.github.io/projects/DiMR</description><author>Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Thu, 13 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09416v1</guid></item><item><title>Depth Anything V2</title><link>http://arxiv.org/abs/2406.09414v1</link><description>This work presents Depth Anything V2. Without pursuing fancy techniques, weaim to reveal crucial findings to pave the way towards building a powerfulmonocular depth estimation model. Notably, compared with V1, this versionproduces much finer and more robust depth predictions through three keypractices: 1) replacing all labeled real images with synthetic images, 2)scaling up the capacity of our teacher model, and 3) teaching student modelsvia the bridge of large-scale pseudo-labeled real images. Compared with thelatest models built on Stable Diffusion, our models are significantly moreefficient (more than 10x faster) and more accurate. We offer models ofdifferent scales (ranging from 25M to 1.3B params) to support extensivescenarios. Benefiting from their strong generalization capability, we fine-tunethem with metric depth labels to obtain our metric depth models. In addition toour models, considering the limited diversity and frequent noise in currenttest sets, we construct a versatile evaluation benchmark with preciseannotations and diverse scenes to facilitate future research.</description><author>Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao</author><pubDate>Thu, 13 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09414v1</guid></item><item><title>Interpreting the Weight Space of Customized Diffusion Models</title><link>http://arxiv.org/abs/2406.09413v1</link><description>We investigate the space of weights spanned by a large collection ofcustomized diffusion models. We populate this space by creating a dataset ofover 60,000 models, each of which is a base model fine-tuned to insert adifferent person's visual identity. We model the underlying manifold of theseweights as a subspace, which we term weights2weights. We demonstrate threeimmediate applications of this space -- sampling, editing, and inversion.First, as each point in the space corresponds to an identity, sampling a set ofweights from it results in a model encoding a novel identity. Next, we findlinear directions in this space corresponding to semantic edits of the identity(e.g., adding a beard). These edits persist in appearance across generatedsamples. Finally, we show that inverting a single image into this spacereconstructs a realistic identity, even if the input image is out ofdistribution (e.g., a painting). Our results indicate that the weight space offine-tuned diffusion models behaves as an interpretable latent space ofidentities.</description><author>Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei A. Efros, Kfir Aberman</author><pubDate>Thu, 13 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09413v1</guid></item><item><title>Explore the Limits of Omni-modal Pretraining at Scale</title><link>http://arxiv.org/abs/2406.09412v1</link><description>We propose to build omni-modal intelligence, which is capable ofunderstanding any modality and learning universal representations. In specific,we propose a scalable pretraining paradigm, named Multimodal Context (MiCo),which can scale up the numbers of modalities and amount of data, together withthe model parameters, in the pretraining process. With MiCo, the pretrainedmodels show significant emergent abilities in multimodal learning, which areevaluated on the following tasks: i) single-modality perception benchmarks of10 different modalities, ii) 25 cross-modality understanding tasks ofretrieval, question-answering, captioning, and iii) 18 multimodal largelanguage model benchmarks. Our models establish 37 new records forstate-of-the-art performance. We hope that our research could contribute to thedevelopment of omni-modal intelligence. Code and Models are athttps://github.com/invictus717/MiCo</description><author>Yiyuan Zhang, Handong Li, Jing Liu, Xiangyu Yue</author><pubDate>Thu, 13 Jun 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09412v1</guid></item><item><title>MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding</title><link>http://arxiv.org/abs/2406.09411v1</link><description>We introduce MuirBench, a comprehensive benchmark that focuses on robustmulti-image understanding capabilities of multimodal LLMs. MuirBench consistsof 12 diverse multi-image tasks (e.g., scene understanding, ordering) thatinvolve 10 categories of multi-image relations (e.g., multiview, temporalrelations). Comprising 11,264 images and 2,600 multiple-choice questions,MuirBench is created in a pairwise manner, where each standard instance ispaired with an unanswerable variant that has minimal semantic differences, inorder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, ourresults reveal that even the best-performing models like GPT-4o and Gemini Profind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.Open-source multimodal LLMs trained on single images can hardly generalize tomulti-image questions, hovering below 33.3% in accuracy. These resultshighlight the importance of MuirBench in encouraging the community to developmultimodal LLMs that can look beyond a single image, suggesting potentialpathways for future improvements.</description><author>Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen</author><pubDate>Thu, 13 Jun 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09411v1</guid></item><item><title>Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach</title><link>http://arxiv.org/abs/2406.09410v1</link><description>Scene graph generation (SGG) in satellite imagery (SAI) benefits promotingintelligent understanding of geospatial scenarios from perception to cognition.In SAI, objects exhibit great variations in scales and aspect ratios, and thereexist rich relationships between objects (even between spatially disjointobjects), which makes it necessary to holistically conduct SGG in large-sizevery-high-resolution (VHR) SAI. However, the lack of SGG datasets withlarge-size VHR SAI has constrained the advancement of SGG in SAI. Due to thecomplexity of large-size VHR SAI, mining triplets &lt;subject, relationship,object&gt; in large-size VHR SAI heavily relies on long-range contextualreasoning. Consequently, SGG models designed for small-size natural imagery arenot directly applicable to large-size VHR SAI. To address the scarcity ofdatasets, this paper constructs a large-scale dataset for SGG in large-size VHRSAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, namedRSG, encompassing over 210,000 objects and more than 400,000 triplets. Torealize SGG in large-size VHR SAI, we propose a context-aware cascade cognition(CAC) framework to understand SAI at three levels: object detection (OBD), pairpruning and relationship prediction. As a fundamental prerequisite for SGG inlarge-size SAI, a holistic multi-class object detection network (HOD-Net) thatcan flexibly integrate multi-scale contexts is proposed. With the considerationthat there exist a huge amount of object pairs in large-size SAI but only aminority of object pairs contain meaningful relationships, we design a pairproposal generation (PPG) network via adversarial reconstruction to selecthigh-value pairs. Furthermore, a relationship prediction network withcontext-aware messaging (RPCM) is proposed to predict the relationship types ofthese pairs.</description><author>Yansheng Li, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Youming Deng, Wenbin Wang, Xian Sun, Haifeng Li, Bo Dang, Yongjun Zhang, Yi Yu, Junchi Yan</author><pubDate>Thu, 13 Jun 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09410v1</guid></item><item><title>CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras</title><link>http://arxiv.org/abs/2406.09409v1</link><description>Point-spread-function (PSF) engineering is a well-established computationalimaging technique that uses phase masks and other optical elements to embedextra information (e.g., depth) into the images captured by conventional CMOSimage sensors. To date, however, PSF-engineering has not been applied toneuromorphic event cameras; a powerful new image sensing technology thatresponds to changes in the log-intensity of light. This paper establishes theoretical limits (Cram\'er Rao bounds) on 3D pointlocalization and tracking with PSF-engineered event cameras. Using thesebounds, we first demonstrate that existing Fisher phase masks are alreadynear-optimal for localizing static flashing point sources (e.g., blinkingfluorescent molecules). We then demonstrate that existing designs aresub-optimal for tracking moving point sources and proceed to use our theory todesign optimal phase masks and binary amplitude masks for this task. Toovercome the non-convexity of the design problem, we leverage novel implicitneural representation based parameterizations of the phase and amplitude masks.We demonstrate the efficacy of our designs through extensive simulations. Wealso validate our method with a simple prototype.</description><author>Sachin Shah, Matthew Albert Chan, Haoming Cai, Jingxi Chen, Sakshum Kulshrestha, Chahat Deep Singh, Yiannis Aloimonos, Christopher Metzler</author><pubDate>Thu, 13 Jun 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09409v1</guid></item><item><title>Data Attribution for Text-to-Image Models by Unlearning Synthesized Images</title><link>http://arxiv.org/abs/2406.09408v1</link><description>The goal of data attribution for text-to-image models is to identify thetraining images that most influence the generation of a new image. We candefine "influence" by saying that, for a given output, if a model is retrainedfrom scratch without that output's most influential images, the model shouldthen fail to generate that output image. Unfortunately, directly searching forthese influential images is computationally infeasible, since it would requirerepeatedly retraining from scratch. We propose a new approach that efficientlyidentifies highly-influential images. Specifically, we simulate unlearning thesynthesized image, proposing a method to increase the training loss on theoutput image, without catastrophic forgetting of other, unrelated concepts.Then, we find training images that are forgotten by proxy, identifying oneswith significant loss deviations after the unlearning process, and label theseas influential. We evaluate our method with a computationally intensive but"gold-standard" retraining from scratch and demonstrate our method's advantagesover previous methods.</description><author>Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang</author><pubDate>Thu, 13 Jun 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09408v1</guid></item><item><title>Towards Evaluating the Robustness of Visual State Space Models</title><link>http://arxiv.org/abs/2406.09407v1</link><description>Vision State Space Models (VSSMs), a novel architecture that combines thestrengths of recurrent neural networks and latent variable models, havedemonstrated remarkable performance in visual perception tasks by efficientlycapturing long-range dependencies and modeling complex visual dynamics.However, their robustness under natural and adversarial perturbations remains acritical concern. In this work, we present a comprehensive evaluation of VSSMs'robustness under various perturbation scenarios, including occlusions, imagestructure, common corruptions, and adversarial attacks, and compare theirperformance to well-established architectures such as transformers andConvolutional Neural Networks. Furthermore, we investigate the resilience ofVSSMs to object-background compositional changes on sophisticated benchmarksdesigned to test model performance in complex visual scenes. We also assesstheir robustness on object detection and segmentation tasks using corrupteddatasets that mimic real-world scenarios. To gain a deeper understanding ofVSSMs' adversarial robustness, we conduct a frequency analysis of adversarialattacks, evaluating their performance against low-frequency and high-frequencyperturbations. Our findings highlight the strengths and limitations of VSSMs inhandling complex visual corruptions, offering valuable insights for futureresearch and improvements in this promising field. Our code and models will beavailable at https://github.com/HashmatShadab/MambaRobustness.</description><author>Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Thu, 13 Jun 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09407v1</guid></item><item><title>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</title><link>http://arxiv.org/abs/2406.09406v1</link><description>Current multimodal and multitask foundation models like 4M or UnifiedIO showpromising results, but in practice their out-of-the-box abilities to acceptdiverse inputs and perform diverse tasks are limited by the (usually rathersmall) number of modalities and tasks they are trained on. In this paper, weexpand upon the capabilities of them by training a single model on tens ofhighly diverse modalities and by performing co-training on large-scalemultimodal datasets and text corpora. This includes training on severalsemantic and geometric modalities, feature maps from recent state of the artmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAMand 4DHumans, and a range of new modalities that allow for novel ways tointeract with the model and steer the generation, for example image metadata orcolor palettes. A crucial step in this process is performing discretetokenization on various modalities, whether they are image-like, neural networkfeature maps, vectors, structured data like instance segmentation or humanposes, or data that can be represented as text. Through this, we expand on theout-of-the-box capabilities of multimodal models and specifically show thepossibility of training one model to solve at least 3x more tasks/modalitiesthan existing ones and doing so without a loss in performance. This enablesmore fine-grained and controllable multimodal generation capabilities andallows us to study the distillation of models trained on diverse data andobjectives into a unified model. We successfully scale the training to a threebillion parameter model using tens of modalities and different datasets. Theresulting models and training code are open sourced at 4m.epfl.ch.</description><author>Roman Bachmann, Oğuzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, Amir Zamir</author><pubDate>Thu, 13 Jun 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09406v1</guid></item><item><title>Why Warmup the Learning Rate? Underlying Mechanisms and Improvements</title><link>http://arxiv.org/abs/2406.09405v1</link><description>It is common in deep learning to warm up the learning rate $\eta$, often by alinear schedule between $\eta_{\text{init}} = 0$ and a predetermined target$\eta_{\text{trgt}}$. In this paper, we show through systematic experimentsusing SGD and Adam that the overwhelming benefit of warmup arises from allowingthe network to tolerate larger $\eta_{\text{trgt}}$ by forcing the network tomore well-conditioned areas of the loss landscape. The ability to handle larger$\eta_{\text{trgt}}$ makes hyperparameter tuning more robust while improvingthe final performance. We uncover different regimes of operation during thewarmup period, depending on whether training starts off in a progressivesharpening or sharpness reduction phase, which in turn depends on theinitialization and parameterization. Using these insights, we show how$\eta_{\text{init}}$ can be properly chosen by utilizing the loss catapultmechanism, which saves on the number of warmup steps, in some cases completelyeliminating the need for warmup. We also suggest an initialization for thevariance in Adam which provides benefits similar to warmup.</description><author>Dayal Singh Kalra, Maissam Barkeshli</author><pubDate>Thu, 13 Jun 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09405v1</guid></item><item><title>ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing</title><link>http://arxiv.org/abs/2406.09404v1</link><description>This paper proposes ConsistDreamer - a novel framework that lifts 2Ddiffusion models with 3D awareness and 3D consistency, thus enablinghigh-fidelity instruction-guided scene editing. To overcome the fundamentallimitation of missing 3D consistency in 2D diffusion models, our key insight isto introduce three synergetic strategies that augment the input of the 2Ddiffusion model to become 3D-aware and to explicitly enforce 3D consistencyduring the training process. Specifically, we design surrounding views ascontext-rich input for the 2D diffusion model, and generate 3D-consistent,structured noise instead of image-independent noise. Moreover, we introduceself-supervised consistency-enforcing training within the per-scene editingprocedure. Extensive evaluation shows that our ConsistDreamer achievesstate-of-the-art performance for instruction-guided scene editing acrossvarious scenes and editing instructions, particularly in complicatedlarge-scale indoor scenes from ScanNet++, with significantly improved sharpnessand fine-grained textures. Notably, ConsistDreamer stands as the first workcapable of successfully editing complex (e.g., plaid/checkered) patterns. Ourproject page is at immortalco.github.io/ConsistDreamer.</description><author>Jun-Kun Chen, Samuel Rota Bulò, Norman Müller, Lorenzo Porzi, Peter Kontschieder, Yu-Xiong Wang</author><pubDate>Thu, 13 Jun 2024 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09404v1</guid></item><item><title>Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models</title><link>http://arxiv.org/abs/2406.09403v1</link><description>Humans draw to facilitate reasoning: we draw auxiliary lines when solvinggeometry problems; we mark and circle when reasoning on maps; we use sketchesto amplify our ideas and relieve our limited-capacity working memory. However,such actions are missing in current multimodal language models (LMs). Currentchain-of-thought and tool-use paradigms only use text as intermediate reasoningsteps. In this work, we introduce Sketchpad, a framework that gives multimodalLMs a visual sketchpad and tools to draw on the sketchpad. The LM conductsplanning and reasoning according to the visual artifacts it has drawn.Different from prior work, which uses text-to-image models to enable LMs todraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which iscloser to human sketching and better facilitates reasoning. Sketchpad can alsouse specialist vision models during the sketching process (e.g., draw boundingboxes with object detection models, draw masks with segmentation models), tofurther enhance visual perception and reasoning. We experiment with a widerange of math tasks (including geometry, functions, graphs, and chess) andcomplex visual reasoning tasks. Sketchpad substantially improves performance onall tasks over strong base models with no sketching, yielding an average gainof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets anew state of the art on all tasks, including V*Bench (80.3%), BLINK spatialreasoning (83.9%), and visual correspondence (80.8%). All codes and data are inhttps://visualsketchpad.github.io/.</description><author>Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna</author><pubDate>Thu, 13 Jun 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09403v1</guid></item><item><title>MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations</title><link>http://arxiv.org/abs/2406.09401v1</link><description>With the emergence of LLMs and their integration with other data modalities,multi-modal 3D perception attracts more attention due to its connectivity tothe physical world and makes rapid progress. However, limited by existingdatasets, previous works mainly focus on understanding object properties orinter-object spatial relationships in a 3D scene. To tackle this problem, thispaper builds the first largest ever multi-modal 3D scene dataset and benchmarkwith hierarchical grounded language annotations, MMScan. It is constructedbased on a top-down logic, from region to object level, from a single target tointer-target relationships, covering holistic aspects of spatial and attributeunderstanding. The overall pipeline incorporates powerful VLMs via carefullydesigned prompts to initialize the annotations efficiently and further involvehumans' correction in the loop to ensure the annotations are natural, correct,and comprehensive. Built upon existing 3D scanning data, the resultingmulti-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objectsand 7.7k regions as well as over 3.04M diverse samples for 3D visual groundingand question-answering benchmarks. We evaluate representative baselines on ourbenchmarks, analyze their capabilities in different aspects, and showcase thekey problems to be addressed in the future. Furthermore, we use thishigh-quality dataset to train state-of-the-art 3D visual grounding and LLMs andobtain remarkable performance improvement both on existing benchmarks andin-the-wild evaluation. Codes, datasets, and benchmarks will be available athttps://github.com/OpenRobotLab/EmbodiedScan.</description><author>Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang</author><pubDate>Thu, 13 Jun 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09401v1</guid></item><item><title>Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion</title><link>http://arxiv.org/abs/2406.09402v1</link><description>This paper proposes Instruct 4D-to-4D that achieves 4D awareness andspatial-temporal consistency for 2D diffusion models to generate high-qualityinstruction-guided dynamic scene editing results. Traditional applications of2D diffusion models in dynamic scene editing often result in inconsistency,primarily due to their inherent frame-by-frame editing methodology. Addressingthe complexities of extending instruction-guided editing to 4D, our key insightis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:achieving temporal consistency in video editing and applying these edits to thepseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)model with an anchor-aware attention module for batch processing and consistentediting. Additionally, we integrate optical flow-guided appearance propagationin a sliding window fashion for more precise frame-to-frame editing andincorporate depth-based projection to manage the extensive data of pseudo-3Dscenes, followed by iterative editing to achieve convergence. We extensivelyevaluate our approach in various scenes and editing instructions, anddemonstrate that it achieves spatially and temporally consistent editingresults, with significantly enhanced detail and sharpness over the prior art.Notably, Instruct 4D-to-4D is general and applicable to both monocular andchallenging multi-camera scenes. Code and more results are available atimmortalco.github.io/Instruct-4D-to-4D.</description><author>Linzhan Mou, Jun-Kun Chen, Yu-Xiong Wang</author><pubDate>Thu, 13 Jun 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09402v1</guid></item><item><title>Yo'LLaVA: Your Personalized Language and Vision Assistant</title><link>http://arxiv.org/abs/2406.09400v1</link><description>Large Multimodal Models (LMMs) have shown remarkable capabilities across avariety of tasks (e.g., image captioning, visual question answering). Whilebroad, their knowledge remains generic (e.g., recognizing a dog), and they areunable to handle personalized subjects (e.g., recognizing a user's pet dog).Human reasoning, in contrast, typically operates within the context of specificsubjects in our surroundings. For example, one might ask, "What should I buyfor my dog's birthday?"; as opposed to a generic inquiry about "What should Ibuy for a dog's birthday?". Similarly, when looking at a friend's image, theinterest lies in seeing their activities (e.g., "my friend is holding a cat"),rather than merely observing generic human actions (e.g., "a man is holding acat"). In this paper, we introduce the novel task of personalizing LMMs, sothat they can have conversations about a specific subject. We propose Yo'LLaVA,which learns to embed a personalized subject into a set of latent tokens givena handful of example images of the subject. Our qualitative and quantitativeanalyses reveal that Yo'LLaVA can learn the concept more efficiently usingfewer tokens and more effectively encode the visual attributes compared tostrong prompting baselines (e.g., LLaVA).</description><author>Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, Yong Jae Lee</author><pubDate>Thu, 13 Jun 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09400v1</guid></item><item><title>OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation</title><link>http://arxiv.org/abs/2406.09399v1</link><description>Tokenizer, serving as a translator to map the intricate visual data into acompact latent space, lies at the core of visual generative models. Based onthe finding that existing tokenizers are tailored to image or video inputs,this paper presents OmniTokenizer, a transformer-based tokenizer for jointimage and video tokenization. OmniTokenizer is designed with a spatial-temporaldecoupled architecture, which integrates window and causal attention forspatial and temporal modeling. To exploit the complementary nature of image andvideo data, we further propose a progressive training strategy, whereOmniTokenizer is first trained on image data on a fixed resolution to developthe spatial encoding capacity and then jointly trained on image and video dataon multiple resolutions to learn the temporal dynamics. OmniTokenizer, for thefirst time, handles both image and video inputs within a unified framework andproves the possibility of realizing their synergy. Extensive experimentsdemonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstructionperformance on various image and video datasets, e.g., 1.11 reconstruction FIDon ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTAmethods by 13% and 26%, respectively. Additionally, we also show that whenintegrated with OmniTokenizer, both language model-based approaches anddiffusion models can realize advanced visual synthesis performance,underscoring the superiority and versatility of our method. Code is availableat https://github.com/FoundationVision/OmniTokenizer.</description><author>Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Thu, 13 Jun 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09399v1</guid></item><item><title>Real-Time Deepfake Detection in the Real-World</title><link>http://arxiv.org/abs/2406.09398v1</link><description>Recent improvements in generative AI made synthesizing fake images easy; asthey can be used to cause harm, it is crucial to develop accurate techniques toidentify them. This paper introduces "Locally Aware Deepfake DetectionAlgorithm" (LaDeDa), that accepts a single 9x9 image patch and outputs itsdeepfake score. The image deepfake score is the pooled score of its patches.With merely patch-level information, LaDeDa significantly improves over thestate-of-the-art, achieving around 99% mAP on current benchmarks. Owing to thepatch-level structure of LaDeDa, we hypothesize that the generation artifactscan be detected by a simple model. We therefore distill LaDeDa intoTiny-LaDeDa, a highly efficient model consisting of only 4 convolutionallayers. Remarkably, Tiny-LaDeDa has 375x fewer FLOPs and is 10,000x moreparameter-efficient than LaDeDa, allowing it to run efficiently on edge deviceswith a minor decrease in accuracy. These almost-perfect scores raise thequestion: is the task of deepfake detection close to being solved? Perhapssurprisingly, our investigation reveals that current training protocols preventmethods from generalizing to real-world deepfakes extracted from social media.To address this issue, we introduce WildRF, a new deepfake detection datasetcurated from several popular social networks. Our method achieves the topperformance of 93.7% mAP on WildRF, however the large gap from perfect accuracyshows that reliable real-world deepfake detection is still unsolved.</description><author>Bar Cavia, Eliahu Horwitz, Tal Reiss, Yedid Hoshen</author><pubDate>Thu, 13 Jun 2024 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09398v1</guid></item><item><title>Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms</title><link>http://arxiv.org/abs/2406.09397v1</link><description>Modern vision models are trained on very large noisy datasets. While thesemodels acquire strong capabilities, they may not follow the user's intent tooutput the desired results in certain aspects, e.g., visual aesthetic,preferred style, and responsibility. In this paper, we target the realm ofvisual aesthetics and aim to align vision models with human aesthetic standardsin a retrieval system. Advanced retrieval systems usually adopt a cascade ofaesthetic models as re-rankers or filters, which are limited to low-levelfeatures like saturation and perform poorly when stylistic, cultural orknowledge contexts are involved. We find that utilizing the reasoning abilityof large language models (LLMs) to rephrase the search query and extend theaesthetic expectations can make up for this shortcoming. Based on the abovefindings, we propose a preference-based reinforcement learning method thatfine-tunes the vision models to distill the knowledge from both LLMs reasoningand the aesthetic models to better align the vision models with humanaesthetics. Meanwhile, with rare benchmarks designed for evaluating retrievalsystems, we leverage large multi-modality model (LMM) to evaluate the aestheticperformance with their strong abilities. As aesthetic assessment is one of themost subjective tasks, to validate the robustness of LMM, we further propose anovel dataset named HPIR to benchmark the alignment with human aesthetics.Experiments demonstrate that our method significantly enhances the aestheticbehaviors of the vision models, under several metrics. We believe the proposedalgorithm can be a general practice for aligning vision models with humanvalues.</description><author>Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo</author><pubDate>Thu, 13 Jun 2024 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09397v1</guid></item><item><title>Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA</title><link>http://arxiv.org/abs/2406.09396v1</link><description>Long-form videos that span across wide temporal intervals are highlyinformation redundant and contain multiple distinct events or entities that areoften loosely-related. Therefore, when performing long-form video questionanswering (LVQA),all information necessary to generate a correct response canoften be contained within a small subset of frames. Recent literature explorethe use of large language models (LLMs) in LVQA benchmarks, achievingexceptional performance, while relying on vision language models (VLMs) toconvert all visual content within videos into natural language. Such VLMs oftenindependently caption a large number of frames uniformly sampled from longvideos, which is not efficient and can mostly be redundant. Questioning thesedecision choices, we explore optimal strategies for key-frame selection andsequence-aware captioning, that can significantly reduce these redundancies. Wepropose two novel approaches that improve each of aspects, namely HierarchicalKeyframe Selector and Sequential Visual LLM. Our resulting framework termedLVNet achieves state-of-the-art performance across three benchmark LVQAdatasets. Our code will be released publicly.</description><author>Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, Michael S. Ryoo</author><pubDate>Thu, 13 Jun 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09396v1</guid></item><item><title>HyperFields: Towards Zero-Shot Generation of NeRFs from Text</title><link>http://arxiv.org/abs/2310.17075v3</link><description>We introduce HyperFields, a method for generating text-conditioned NeuralRadiance Fields (NeRFs) with a single forward pass and (optionally) somefine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learnsa smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRFdistillation training, which distills scenes encoded in individual NeRFs intoone dynamic hypernetwork. These techniques enable a single network to fit overa hundred unique scenes. We further demonstrate that HyperFields learns a moregeneral map between text and NeRFs, and consequently is capable of predictingnovel in-distribution and out-of-distribution scenes -- either zero-shot orwith a few finetuning steps. Finetuning HyperFields benefits from acceleratedconvergence thanks to the learned general map, and is capable of synthesizingnovel scenes 5 to 10 times faster than existing neural optimization-basedmethods. Our ablation experiments show that both the dynamic architecture andNeRF distillation are critical to the expressivity of HyperFields.</description><author>Sudarshan Babu, Richard Liu, Avery Zhou, Michael Maire, Greg Shakhnarovich, Rana Hanocka</author><pubDate>Thu, 13 Jun 2024 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17075v3</guid></item><item><title>Modeling Ambient Scene Dynamics for Free-view Synthesis</title><link>http://arxiv.org/abs/2406.09395v1</link><description>We introduce a novel method for dynamic free-view synthesis of an ambientscenes from a monocular capture bringing a immersive quality to the viewingexperience. Our method builds upon the recent advancements in 3D GaussianSplatting (3DGS) that can faithfully reconstruct complex static scenes.Previous attempts to extend 3DGS to represent dynamics have been confined tobounded scenes or require multi-camera captures, and often fail to generalizeto unseen motions, limiting their practical application. Our approach overcomesthese constraints by leveraging the periodicity of ambient motions to learn themotion trajectory model, coupled with careful regularization. We also proposeimportant practical strategies to improve the visual quality of the baseline3DGS static reconstructions and to improve memory efficiency critical forGPU-memory intensive learning. We demonstrate high-quality photorealistic novelview synthesis of several ambient natural scenes with intricate textures andfine structural elements.</description><author>Meng-Li Shih, Jia-Bin Huang, Changil Kim, Rajvi Shah, Johannes Kopf, Chen Gao</author><pubDate>Thu, 13 Jun 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09395v1</guid></item><item><title>WonderWorld: Interactive 3D Scene Generation from a Single Image</title><link>http://arxiv.org/abs/2406.09394v1</link><description>We present WonderWorld, a novel framework for \emph{interactive} 3D sceneextrapolation that enables users to explore and shape virtual environmentsbased on a single input image and user-specified text. While significantimprovements have been made to the visual quality of scene generation, existingmethods are run offline, taking tens of minutes to hours to generate a scene.By leveraging Fast Gaussian Surfels and a guided diffusion-based depthestimation method, WonderWorld generates geometrically consistent extrapolationwhile significantly reducing computational time. Our framework generatesconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,enabling real-time user interaction and exploration. We demonstrate thepotential of WonderWorld for applications in virtual reality, gaming, andcreative design, where users can quickly generate and navigate immersive,potentially infinite virtual worlds from a single image. Our approachrepresents a significant advancement in interactive 3D scene generation,opening up new possibilities for user-driven content creation and explorationin virtual environments. We will release full code and software forreproducibility. Project website: https://WonderWorld-2024.github.io/</description><author>Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu</author><pubDate>Thu, 13 Jun 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09394v1</guid></item><item><title>Improving Autoregressive Training with Dynamic Oracles</title><link>http://arxiv.org/abs/2406.09393v1</link><description>Many tasks within NLP can be framed as sequential decision problems, rangingfrom sequence tagging to text generation. However, for many tasks, the standardtraining methods, including maximum likelihood (teacher forcing) and scheduledsampling, suffer from exposure bias and a mismatch between metrics employedduring training and inference. DAgger provides a solution to mitigate theseproblems, yet it requires a metric-specific dynamic oracle algorithm, whichdoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. Inthis paper, we develop these novel dynamic oracles and show they maintainDAgger's no-regret guarantee for decomposable metrics like span-based F1. Weevaluate the algorithm's performance on named entity recognition (NER), textsummarization, and machine translation (MT). While DAgger with dynamic oracleyields less favorable results in our MT experiments, it outperforms thebaseline techniques in NER and text summarization.</description><author>Jianing Yang, Harshine Visvanathan, Yilin Wang, Xinyi Hu, Matthew Gormley</author><pubDate>Thu, 13 Jun 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09393v1</guid></item><item><title>A More Practical Approach to Machine Unlearning</title><link>http://arxiv.org/abs/2406.09391v1</link><description>Machine learning models often incorporate vast amounts of data, raisingsignificant privacy concerns. Machine unlearning, the ability to remove theinfluence of specific data points from a trained model, addresses theseconcerns. This paper explores practical methods for implementing machineunlearning, focusing on a first-epoch gradient-ascent approach. Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epochgradient unlearning is more effective than multi-epoch gradients. 2.Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effectiveunlearning. Gradients from the output layers (11 and 12) have no impact.Efficient unlearning can be achieved using only the embedding layer, halvingspace complexity. 3. Influence Functions &amp; Scoring: Techniques like HessianVector Product and the dot product of activations and tensors are used forquantifying unlearning. 4. Gradient Ascent Considerations: Calibration isnecessary to avoid overexposing the model to specific data points duringunlearning, which could prematurely terminate the process. 5. Fuzzy Matchingvs. Iterative Unlearning: Fuzzy matching techniques shift the model to a newoptimum, while iterative unlearning provides a more complete modality. Our empirical evaluation confirms that first-epoch gradient ascent formachine unlearning is more effective than whole-model gradient ascent. Theseresults highlight the potential of machine unlearning for enhancing dataprivacy and compliance with regulations such as GDPR and CCPA. The studyunderscores the importance of formal methods to comprehensively evaluate theunlearning process.</description><author>David Zagardo</author><pubDate>Thu, 13 Jun 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09391v1</guid></item><item><title>LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living</title><link>http://arxiv.org/abs/2406.09390v1</link><description>Large Language Vision Models (LLVMs) have demonstrated effectiveness inprocessing internet videos, yet they struggle with the visually perplexingdynamics present in Activities of Daily Living (ADL) due to limited pertinentdatasets and models tailored to relevant cues. To this end, we propose aframework for curating ADL multiview datasets to fine-tune LLVMs, resulting inthe creation of ADL-X, comprising 100K RGB video-instruction pairs, languagedescriptions, 3D skeletons, and action-conditioned object trajectories. Weintroduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevantobject trajectories to understand the intricate spatiotemporal relationshipswithin ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifyingLLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDALconsistently achieves state-of-the-art performance across all ADL evaluationmetrics. Qualitative analysis reveals LLAVIDAL's temporal reasoningcapabilities in understanding ADL. The link to the dataset is provided at:https://adl-x.github.io/</description><author>Rajatsubhra Chakraborty, Arkaprava Sinha, Dominick Reilly, Manish Kumar Govind, Pu Wang, Francois Bremond, Srijan Das</author><pubDate>Thu, 13 Jun 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09390v1</guid></item><item><title>Sagiri: Low Dynamic Range Image Enhancement with Generative Diffusion Prior</title><link>http://arxiv.org/abs/2406.09389v1</link><description>Capturing High Dynamic Range (HDR) scenery using 8-bit cameras often suffersfrom over-/underexposure, loss of fine details due to low bit-depthcompression, skewed color distributions, and strong noise in dark areas.Traditional LDR image enhancement methods primarily focus on color mapping,which enhances the visual representation by expanding the image's color rangeand adjusting the brightness. However, these approaches fail to effectivelyrestore content in dynamic range extremes, which are regions with pixel valuesclose to 0 or 255. To address the full scope of challenges in HDR imaging andsurpass the limitations of current models, we propose a novel two-stageapproach. The first stage maps the color and brightness to an appropriate rangewhile keeping the existing details, and the second stage utilizes a diffusionprior to generate content in dynamic range extremes lost during capture. Thisgenerative refinement module can also be used as a plug-and-play module toenhance and complement existing LDR enhancement models. The proposed methodmarkedly improves the quality and details of LDR images, demonstrating superiorperformance through rigorous experimental validation. The project page is athttps://sagiri0208.github.io</description><author>Baiang Li, Sizhuo Ma, Yanhong Zeng, Xiaogang Xu, Youqing Fang, Zhao Zhang, Jian Wang, Kai Chen</author><pubDate>Thu, 13 Jun 2024 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09389v1</guid></item><item><title>Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition</title><link>http://arxiv.org/abs/2406.09388v1</link><description>Vision and language models (VLMs) such as CLIP have showcased remarkablezero-shot recognition abilities yet face challenges in visio-linguisticcompositionality, particularly in linguistic comprehension and fine-grainedimage-text alignment. This paper explores the intricate relationship betweencompositionality and recognition -- two pivotal aspects of VLM capability. Weconduct a comprehensive evaluation of existing VLMs, covering both pre-trainingapproaches aimed at recognition and the fine-tuning methods designed to improvecompositionality. Our evaluation employs 12 benchmarks for compositionality,along with 21 zero-shot classification and two retrieval benchmarks forrecognition. In our analysis from 274 CLIP model checkpoints, we revealpatterns and trade-offs that emerge between compositional understanding andrecognition accuracy. Ultimately, this necessitates strategic efforts towardsdeveloping models that improve both capabilities, as well as the meticulousformulation of benchmarks for compositionality. We open our evaluationframework at https://github.com/ytaek-oh/vl_compo.</description><author>Youngtaek Oh, Pyunghwan Ahn, Jinhyung Kim, Gwangmo Song, Soonyoung Lee, In So Kweon, Junmo Kim</author><pubDate>Thu, 13 Jun 2024 18:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09388v1</guid></item><item><title>SimGen: Simulator-conditioned Driving Scene Generation</title><link>http://arxiv.org/abs/2406.09386v1</link><description>Controllable synthetic data generation can substantially lower the annotationcost of training data in autonomous driving research and development. Priorworks use diffusion models to generate driving images conditioned on the 3Dobject layout. However, those models are trained on small-scale datasets likenuScenes, which lack appearance and layout diversity. Moreover, the trainedmodels can only generate images based on the real-world layout data from thevalidation set of the same dataset, where overfitting might happen. In thiswork, we introduce a simulator-conditioned scene generation framework calledSimGen that can learn to generate diverse driving scenes by mixing data fromthe simulator and the real world. It uses a novel cascade diffusion pipeline toaddress challenging sim-to-real gaps and multi-condition conflicts. A drivingvideo dataset DIVA is collected to enhance the generative diversity of SimGen,which contains over 147.5 hours of real-world driving videos from 73 locationsworldwide and simulated driving data from the MetaDrive simulator. SimGenachieves superior generation quality and diversity while preservingcontrollability based on the text prompt and the layout pulled from asimulator. We further demonstrate the improvements brought by SimGen forsynthetic data augmentation on the BEV detection and segmentation task andshowcase its capability in safety-critical data generation. Code, data, andmodels will be made available.</description><author>Yunsong Zhou, Michael Simon, Zhenghao Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, Bolei Zhou</author><pubDate>Thu, 13 Jun 2024 18:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09386v1</guid></item><item><title>Oblivious subspace embeddings for compressed Tucker decompositions</title><link>http://arxiv.org/abs/2406.09387v1</link><description>Emphasis in the tensor literature on random embeddings (tools forlow-distortion dimension reduction) for the canonical polyadic (CP) tensordecomposition has left analogous results for the more expressive Tuckerdecomposition comparatively lacking. This work establishes generalJohnson-Lindenstrauss (JL) type guarantees for the estimation of Tuckerdecompositions when an oblivious random embedding is applied along each mode.When these embeddings are drawn from a JL-optimal family, the decomposition canbe estimated within $\varepsilon$ relative error under restrictions on theembedding dimension that are in line with recent CP results. We implement ahigher-order orthogonal iteration (HOOI) decomposition algorithm with randomembeddings to demonstrate the practical benefits of this approach and itspotential to improve the accessibility of otherwise prohibitive tensoranalyses. On moderately large face image and fMRI neuroimaging datasets,empirical results show that substantial dimension reduction is possible withminimal increase in reconstruction error relative to traditional HOOI ($\leq$5%larger error, 50%-60% lower computation time for large models with 50%dimension reduction along each mode). Especially for large tensors, our methodoutperforms traditional higher-order singular value decomposition (HOSVD) andrecently proposed TensorSketch methods.</description><author>Matthew Pietrosanu, Bei Jiang, Linglong Kong</author><pubDate>Thu, 13 Jun 2024 18:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09387v1</guid></item><item><title>Towards Vision-Language Geo-Foundation Model: A Survey</title><link>http://arxiv.org/abs/2406.09385v1</link><description>Vision-Language Foundation Models (VLFMs) have made remarkable progress onvarious multimodal tasks, such as image captioning, image-text retrieval,visual question answering, and visual grounding. However, most methods rely ontraining with general image datasets, and the lack of geospatial data leads topoor performance on earth observation. Numerous geospatial image-text pairdatasets and VLFMs fine-tuned on them have been proposed recently. These newapproaches aim to leverage large-scale, multimodal geospatial data to buildversatile intelligent models with diverse geo-perceptive capabilities, which werefer to as Vision-Language Geo-Foundation Models (VLGFMs). This paperthoroughly reviews VLGFMs, summarizing and analyzing recent developments in thefield. In particular, we introduce the background and motivation behind therise of VLGFMs, highlighting their unique research significance. Then, wesystematically summarize the core technologies employed in VLGFMs, includingdata construction, model architectures, and applications of various multimodalgeospatial tasks. Finally, we conclude with insights, issues, and discussionsregarding future research directions. To the best of our knowledge, this is thefirst comprehensive literature review of VLGFMs. We keep tracing related worksat https://github.com/zytx121/Awesome-VLGFM.</description><author>Yue Zhou, Litong Feng, Yiping Ke, Xue Jiang, Junchi Yan, Xue Yang, Wayne Zhang</author><pubDate>Thu, 13 Jun 2024 18:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09385v1</guid></item><item><title>Reflecting on the State of Rehearsal-free Continual Learning with Pretrained Models</title><link>http://arxiv.org/abs/2406.09384v1</link><description>With the advent and recent ubiquity of foundation models, continual learning(CL) has recently shifted from continual training from scratch to the continualadaptation of pretrained models, seeing particular success on rehearsal-free CLbenchmarks (RFCL). To achieve this, most proposed methods adapt and restructureparameter-efficient finetuning techniques (PEFT) to suit the continual natureof the problem. Based most often on input-conditional query-mechanisms orregularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL(P-RFCL) approaches report peak performances; often convincingly outperformingexisting CL techniques. However, on the other end, critical studies haverecently highlighted competitive results by training on just the first task orvia simple non-parametric baselines. Consequently, questions arise about therelationship between methodological choices in P-RFCL and their reported highbenchmark scores. In this work, we tackle these questions to better understandthe true drivers behind strong P-RFCL performances, their placement w.r.t.recent first-task adaptation studies, and their relation to preceding CLstandards such as EWC or SI. In particular, we show: (1) P-RFCL techniquesrelying on input-conditional query mechanisms work not because, but ratherdespite them by collapsing towards standard PEFT shortcut solutions. (2)Indeed, we show how most often, P-RFCL techniques can be matched by a simpleand lightweight PEFT baseline. (3) Using this baseline, we identify theimplicit bound on tunable parameters when deriving RFCL approaches from PEFTmethods as a potential denominator behind P-RFCL efficacy. Finally, we (4)better disentangle continual versus first-task adaptation, and (5) motivatestandard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.</description><author>Lukas Thede, Karsten Roth, Olivier J. Hénaff, Matthias Bethge, Zeynep Akata</author><pubDate>Thu, 13 Jun 2024 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09384v1</guid></item><item><title>Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset</title><link>http://arxiv.org/abs/2406.09383v1</link><description>Large-scale datasets have fueled recent advancements in AI-based autonomousvehicle research. However, these datasets are usually collected from a singlevehicle's one-time pass of a certain location, lacking multiagent interactionsor repeated traversals of the same place. Such information could lead totransformative enhancements in autonomous vehicles' perception, prediction, andplanning capabilities. To bridge this gap, in collaboration with theself-driving company May Mobility, we present the MARS dataset which unifiesscenarios that enable MultiAgent, multitraveRSal, and multimodal autonomousvehicle research. More specifically, MARS is collected with a fleet ofautonomous vehicles driving within a certain geographical area. Each vehiclehas its own route and different vehicles may appear at nearby locations. Eachvehicle is equipped with a LiDAR and surround-view RGB cameras. We curate twosubsets in MARS: one facilitates collaborative driving with multiple vehiclessimultaneously present at the same location, and the other enables memoryretrospection through asynchronous traversals of the same location by multiplevehicles. We conduct experiments in place recognition and neuralreconstruction. More importantly, MARS introduces new research opportunitiesand challenges such as multitraversal 3D reconstruction, multiagent perception,and unsupervised object discovery. Our data and codes can be found athttps://ai4ce.github.io/MARS/.</description><author>Yiming Li, Zhiheng Li, Nuo Chen, Moonjun Gong, Zonglin Lyu, Zehong Wang, Peili Jiang, Chen Feng</author><pubDate>Thu, 13 Jun 2024 18:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09383v1</guid></item><item><title>Score Distillation via Reparametrized DDIM</title><link>http://arxiv.org/abs/2405.15891v2</link><description>While 2D diffusion models generate realistic, high-detail images, 3D shapegeneration methods like Score Distillation Sampling (SDS) built on these 2Ddiffusion models produce cartoon-like, over-smoothed shapes. To help explainthis discrepancy, we show that the image guidance used in Score Distillationcan be understood as the velocity field of a 2D denoising generative process,up to the choice of a noise term. In particular, after a change of variables,SDS resembles a high-variance version of Denoising Diffusion Implicit Models(DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d.randomly at each step, while DDIM infers it from the previous noisepredictions. This excessive variance can lead to over-smoothing and unrealisticoutputs. We show that a better noise approximation can be recovered byinverting DDIM in each SDS update step. This modification makes SDS'sgenerative process for 2D images almost identical to DDIM. In 3D, it removesover-smoothing, preserves higher-frequency detail, and brings the generationquality closer to that of 2D samplers. Experimentally, our method achievesbetter or similar 3D generation quality compared to other state-of-the-artScore Distillation methods, all without training additional neural networks ormulti-view supervision, and providing useful insights into relationship between2D and 3D asset generation with diffusion models.</description><author>Artem Lukoianov, Haitz Sáez de Ocáriz Borde, Kristjan Greenewald, Vitor Campagnolo Guizilini, Timur Bagautdinov, Vincent Sitzmann, Justin Solomon</author><pubDate>Thu, 13 Jun 2024 18:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15891v2</guid></item><item><title>GGHead: Fast and Generalizable 3D Gaussian Heads</title><link>http://arxiv.org/abs/2406.09377v1</link><description>Learning 3D head priors from large 2D image collections is an important steptowards high-quality 3D-aware human modeling. A core requirement is anefficient architecture that scales well to large-scale datasets and large imageresolutions. Unfortunately, existing 3D GANs struggle to scale to generatesamples at high resolutions due to their relatively slow train and renderspeeds, and typically have to rely on 2D superresolution networks at theexpense of global 3D consistency. To address these challenges, we proposeGenerative Gaussian Heads (GGHead), which adopts the recent 3D GaussianSplatting representation within a 3D GAN framework. To generate a 3Drepresentation, we employ a powerful 2D CNN generator to predict Gaussianattributes in the UV space of a template head mesh. This way, GGHead exploitsthe regularity of the template's UV layout, substantially facilitating thechallenging task of predicting an unstructured set of 3D Gaussians. We furtherimprove the geometric fidelity of the generated 3D representations with a noveltotal variation loss on rendered UV coordinates. Intuitively, thisregularization encourages that neighboring rendered pixels should stem fromneighboring Gaussians in the template's UV space. Taken together, our pipelinecan efficiently generate 3D heads trained only from single-view 2D imageobservations. Our proposed framework matches the quality of existing 3D headGANs on FFHQ while being both substantially faster and fully 3D consistent. Asa result, we demonstrate real-time generation and rendering of high-quality3D-consistent heads at $1024^2$ resolution for the first time.</description><author>Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner</author><pubDate>Thu, 13 Jun 2024 18:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09377v1</guid></item><item><title>Learning conditional distributions on continuous spaces</title><link>http://arxiv.org/abs/2406.09375v1</link><description>We investigate sample-based learning of conditional distributions onmulti-dimensional unit boxes, allowing for different dimensions of the featureand target spaces. Our approach involves clustering data near varying querypoints in the feature space to create empirical measures in the target space.We employ two distinct clustering schemes: one based on a fixed-radius ball andthe other on nearest neighbors. We establish upper bounds for the convergencerates of both methods and, from these bounds, deduce optimal configurations forthe radius and the number of neighbors. We propose to incorporate the nearestneighbors method into neural network training, as our empirical analysisindicates it has better performance in practice. For efficiency, our trainingprocess utilizes approximate nearest neighbors search with random binary spacepartitioning. Additionally, we employ the Sinkhorn algorithm and asparsity-enforced transport plan. Our empirical findings demonstrate that, witha suitably designed structure, the neural network has the ability to adapt to asuitable level of Lipschitz continuity locally. For reproducibility, our codeis available at \url{https://github.com/zcheng-a/LCD_kNN}.</description><author>Cyril Bénézet, Ziteng Cheng, Sebastian Jaimungal</author><pubDate>Thu, 13 Jun 2024 18:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09375v1</guid></item><item><title>Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios</title><link>http://arxiv.org/abs/2406.00343v2</link><description>The deployment of Large Language Models (LLMs) in real-world applicationspresents both opportunities and challenges, particularly in multilingual andcode-mixed communication settings. This research evaluates the performance ofseven leading LLMs in sentiment analysis on a dataset derived from multilingualand code-mixed WhatsApp chats, including Swahili, English and Sheng. Ourevaluation includes both quantitative analysis using metrics like F1 score andqualitative assessment of LLMs' explanations for their predictions. We findthat, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and otherLLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled withunderstanding linguistic and contextual nuances, as well as lack oftransparency in their decision-making process as observed from theirexplanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverselinguistic inputs and managing various contextual information, demonstratinghigh consistency with human alignment and transparency in their decision-makingprocess. The LLMs however, encountered difficulties in incorporating culturalnuance especially in non-English settings with GPT-4s doing so inconsistently.The findings emphasize the necessity of continuous improvement of LLMs toeffectively tackle the challenges of culturally nuanced, low-resourcereal-world settings and the need for developing evaluation benchmarks forcapturing these issues.</description><author>Millicent Ochieng, Varun Gumma, Sunayana Sitaram, Jindong Wang, Vishrav Chaudhary, Keshet Ronen, Kalika Bali, Jacki O'Neill</author><pubDate>Thu, 13 Jun 2024 18:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00343v2</guid></item><item><title>Improving the Fairness of Deep-Learning, Short-term Crime Prediction with Under-reporting-aware Models</title><link>http://arxiv.org/abs/2406.04382v2</link><description>Deep learning crime predictive tools use past crime data and additionalbehavioral datasets to forecast future crimes. Nevertheless, these tools havebeen shown to suffer from unfair predictions across minority racial and ethnicgroups. Current approaches to address this unfairness generally propose eitherpre-processing methods that mitigate the bias in the training datasets byapplying corrections to crime counts based on domain knowledge or in-processingmethods that are implemented as fairness regularizers to optimize for bothaccuracy and fairness. In this paper, we propose a novel deep learningarchitecture that combines the power of these two approaches to increaseprediction fairness. Our results show that the proposed model improves thefairness of crime predictions when compared to models with in-processingde-biasing approaches and with models without any type of bias correction,albeit at the cost of reducing accuracy.</description><author>Jiahui Wu, Vanessa Frias-Martinez</author><pubDate>Thu, 13 Jun 2024 18:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04382v2</guid></item><item><title>Scale-Invariant Monocular Depth Estimation via SSI Depth</title><link>http://arxiv.org/abs/2406.09374v1</link><description>Existing methods for scale-invariant monocular depth estimation (SI MDE)often struggle due to the complexity of the task, and limited and non-diversedatasets, hindering generalizability in real-world scenarios. This is whileshift-and-scale-invariant (SSI) depth estimation, simplifying the task andenabling training with abundant stereo datasets achieves high performance. Wepresent a novel approach that leverages SSI inputs to enhance SI depthestimation, streamlining the network's role and facilitating in-the-wildgeneralization for SI depth estimation while only using a synthetic dataset fortraining. Emphasizing the generation of high-resolution details, we introduce anovel sparse ordinal loss that substantially improves detail generation in SSIMDE, addressing critical limitations in existing approaches. Throughin-the-wild qualitative examples and zero-shot evaluation we substantiate thepractical utility of our approach in computational photography applications,showcasing its ability to generate highly detailed SI depth maps and achievegeneralization in diverse scenarios.</description><author>S. Mahdi H. Miangoleh, Mahesh Reddy, Yağız Aksoy</author><pubDate>Thu, 13 Jun 2024 18:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09374v1</guid></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>http://arxiv.org/abs/2406.09373v1</link><description>A fundamental notion of distance between train and test distributions fromthe field of domain adaptation is discrepancy distance. While in general hardto compute, here we provide the first set of provably efficient algorithms fortesting localized discrepancy distance, where discrepancy is computed withrespect to a fixed output classifier. These results imply a broad set of new,efficient learning algorithms in the recently introduced model of TestableLearning with Distribution Shift (TDS learning) due to Klivans et al. (2023). Our approach generalizes and improves all prior work on TDS learning: (1) weobtain universal learners that succeed simultaneously for large classes of testdistributions, (2) achieve near-optimal error rates, and (3) give exponentialimprovements for constant depth circuits. Our methods further extend tosemi-parametric settings and imply the first positive results forlow-dimensional convex sets. Additionally, we separate learning and testingphases and obtain algorithms that run in fully polynomial time at test time.</description><author>Gautam Chandrasekaran, Adam R. Klivans, Vasilis Kontonis, Konstantinos Stavropoulos, Arsen Vasilyan</author><pubDate>Thu, 13 Jun 2024 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09373v1</guid></item><item><title>LRM-Zero: Training Large Reconstruction Models with Synthesized Data</title><link>http://arxiv.org/abs/2406.09371v1</link><description>We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely onsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. Thecore of LRM-Zero is our procedural 3D dataset, Zeroverse, which isautomatically synthesized from simple primitive shapes with random texturingand augmentations (e.g., height fields, boolean differences, and wireframes).Unlike previous 3D datasets (e.g., Objaverse) which are often captured orcrafted by humans to approximate real 3D data, Zeroverse completely ignoresrealistic global semantics but is rich in complex geometric and texture detailsthat are locally similar to or even more intricate than real objects. Wedemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,can achieve high visual quality in the reconstruction of real-world objects,competitive with models trained on Objaverse. We also analyze several criticaldesign choices of Zeroverse that contribute to LRM-Zero's capability andtraining stability. Our work demonstrates that 3D reconstruction, one of thecore tasks in 3D vision, can potentially be addressed without the semantics ofreal-world objects. The Zeroverse's procedural synthesis code and interactivevisualization are available at: https://desaixie.github.io/lrm-zero/.</description><author>Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Sören Pirk, Arie Kaufman, Xin Sun, Hao Tan</author><pubDate>Thu, 13 Jun 2024 18:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09371v1</guid></item><item><title>Data-dependent and Oracle Bounds on Forgetting in Continual Learning</title><link>http://arxiv.org/abs/2406.09370v1</link><description>In continual learning, knowledge must be preserved and re-used between tasks,maintaining good transfer to future tasks and minimizing forgetting ofpreviously learned ones. While several practical algorithms have been devisedfor this setting, there have been few theoretical works aiming to quantify andbound the degree of Forgetting in general settings. We provide bothdata-dependent and oracle upper bounds that apply regardless of model andalgorithm choice, as well as bounds for Gibbs posteriors. We derive analgorithm inspired by our bounds and demonstrate empirically that our approachyields improved forward and backward transfer.</description><author>Lior Friedman, Ron Meir</author><pubDate>Thu, 13 Jun 2024 18:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09370v1</guid></item><item><title>CLIPAway: Harmonizing Focused Embeddings for Removing Objects via Diffusion Models</title><link>http://arxiv.org/abs/2406.09368v1</link><description>Advanced image editing techniques, particularly inpainting, are essential forseamlessly removing unwanted elements while preserving visual integrity.Traditional GAN-based methods have achieved notable success, but recentadvancements in diffusion models have produced superior results due to theirtraining on large-scale datasets, enabling the generation of remarkablyrealistic inpainted images. Despite their strengths, diffusion models oftenstruggle with object removal tasks without explicit guidance, leading tounintended hallucinations of the removed object. To address this issue, weintroduce CLIPAway, a novel approach leveraging CLIP embeddings to focus onbackground regions while excluding foreground elements. CLIPAway enhancesinpainting accuracy and quality by identifying embeddings that prioritize thebackground, thus achieving seamless object removal. Unlike other methods thatrely on specialized training datasets or costly manual annotations, CLIPAwayprovides a flexible, plug-and-play solution compatible with variousdiffusion-based inpainting techniques.</description><author>Yigit Ekin, Ahmet Burak Yildirim, Erdem Eren Caglar, Aykut Erdem, Erkut Erdem, Aysegul Dundar</author><pubDate>Thu, 13 Jun 2024 18:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09368v1</guid></item><item><title>Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs</title><link>http://arxiv.org/abs/2406.09367v1</link><description>Video understanding is a crucial next step for multimodal large languagemodels (MLLMs). To probe specific aspects of video understanding ability,existing video benchmarks typically require careful video selection based onthe target capability, along with laborious annotation of query-response pairsto match the specific video content. This process is both challenging andresource-intensive. In this paper, we propose VideoNIAH (Video Needle In AHaystack), a benchmark construction framework through synthetic videogeneration. VideoNIAH decouples test video content from their query-responsesby inserting unrelated image/text 'needles' into original videos. It generatesannotations solely from these needles, ensuring diversity in video sources anda variety of query-responses. Additionally, by inserting multiple needles,VideoNIAH rigorously evaluates the temporal understanding capabilities ofmodels. We utilized VideoNIAH to compile a video benchmark VNBench, includingtasks such as retrieval, ordering, and counting. VNBench can efficientlyevaluate the fine-grained understanding ability and spatio-temporal modelingability of a video model, while also supporting the long-context evaluation.Additionally, we evaluated recent video-centric multimodal large languagemodels (MLLMs), both open-source and proprietary, providing a comprehensiveanalysis. We found that although proprietary models have significant advantagesover open-source models, all existing video models still perform poorly onlong-distance dependency tasks. VideoNIAH is a simple yet highly scalablebenchmark construction framework, and we believe it will inspire future videobenchmark works. The code and data are available athttps://github.com/joez17/VideoNIAH.</description><author>Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, Jing Liu</author><pubDate>Thu, 13 Jun 2024 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09367v1</guid></item><item><title>Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations</title><link>http://arxiv.org/abs/2406.09366v1</link><description>Maximum Manifold Capacity Representations (MMCR) is a recent multi-viewself-supervised learning (MVSSL) method that matches or surpasses other leadingMVSSL methods. MMCR is intriguing because it does not fit neatly into any ofthe commonplace MVSSL lineages, instead originating from a statisticalmechanical perspective on the linear separability of data manifolds. In thispaper, we seek to improve our understanding and our utilization of MMCR. Tobetter understand MMCR, we leverage tools from high dimensional probability todemonstrate that MMCR incentivizes alignment and uniformity of learnedembeddings. We then leverage tools from information theory to show that suchembeddings maximize a well-known lower bound on mutual information betweenviews, thereby connecting the geometric perspective of MMCR to theinformation-theoretic perspective commonly discussed in MVSSL. To betterutilize MMCR, we mathematically predict and experimentally confirmnon-monotonic changes in the pretraining loss akin to double descent but withrespect to atypical hyperparameters. We also discover compute scaling laws thatenable predicting the pretraining loss as a function of gradients steps, batchsize, embedding dimension and number of views. We then show that MMCR,originally applied to image data, is performant on multimodal image-text data.By more deeply understanding the theoretical and empirical behavior of MMCR,our work reveals insights on improving MVSSL methods.</description><author>Rylan Schaeffer, Victor Lecomte, Dhruv Bhandarkar Pai, Andres Carranza, Berivan Isik, Alyssa Unell, Mikail Khona, Thomas Yerxa, Yann LeCun, SueYeon Chung, Andrey Gromov, Ravid Shwartz-Ziv, Sanmi Koyejo</author><pubDate>Thu, 13 Jun 2024 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09366v1</guid></item><item><title>ElicitationGPT: Text Elicitation Mechanisms via Language Models</title><link>http://arxiv.org/abs/2406.09363v1</link><description>Scoring rules evaluate probabilistic forecasts of an unknown state againstthe realized state and are a fundamental building block in the incentivizedelicitation of information and the training of machine learning models. Thispaper develops mechanisms for scoring elicited text against ground truth textusing domain-knowledge-free queries to a large language model (specificallyChatGPT) and empirically evaluates their alignment with human preferences. Theempirical evaluation is conducted on peer reviews from a peer-grading datasetand in comparison to manual instructor scores for the peer reviews.</description><author>Yifan Wu, Jason Hartline</author><pubDate>Thu, 13 Jun 2024 18:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09363v1</guid></item><item><title>Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits</title><link>http://arxiv.org/abs/2402.05689v2</link><description>We consider the infinite-horizon, average-reward restless bandit problem indiscrete time. We propose a new class of policies that are designed to drive aprogressively larger subset of arms toward the optimal distribution. We showthat our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimalitygap for an $N$-armed problem, provided that the single-armed MDP is unichainand aperiodic under the optimal single-armed policy. Our approach departs frommost existing work that focuses on index or priority policies, which rely onthe Uniform Global Attractor Property (UGAP) to guarantee convergence to theoptimum, or a recently developed simulation-based policy, which requires aSynchronization Assumption (SA).</description><author>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang</author><pubDate>Thu, 13 Jun 2024 18:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05689v2</guid></item><item><title>Understanding Hallucinations in Diffusion Models through Mode Interpolation</title><link>http://arxiv.org/abs/2406.09358v1</link><description>Colloquially speaking, image generation models based upon diffusion processesare frequently said to exhibit "hallucinations," samples that could never occurin the training data. But where do such hallucinations come from? In thispaper, we study a particular failure mode in diffusion models, which we termmode interpolation. Specifically, we find that diffusion models smoothly"interpolate" between nearby data modes in the training set, to generatesamples that are completely outside the support of the original trainingdistribution; this phenomenon leads diffusion models to generate artifacts thatnever existed in real data (i.e., hallucinations). We systematically study thereasons for, and the manifestation of this phenomenon. Through experiments on1D and 2D Gaussians, we show how a discontinuous loss landscape in thediffusion model's decoder leads to a region where any smooth approximation willcause such hallucinations. Through experiments on artificial datasets withvarious shapes, we show how hallucination leads to the generation ofcombinations of shapes that never existed. Finally, we show that diffusionmodels in fact know when they go out of support and hallucinate. This iscaptured by the high variance in the trajectory of the generated sample towardsthe final few backward sampling process. Using a simple metric to capture thisvariance, we can remove over 95% of hallucinations at generation time whileretaining 96% of in-support samples. We conclude our exploration by showing theimplications of such hallucination (and its removal) on the collapse (andstabilization) of recursive training on synthetic data with experiments onMNIST and 2D Gaussians dataset. We release our code athttps://github.com/locuslab/diffusion-model-hallucination.</description><author>Sumukh K Aithal, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter</author><pubDate>Thu, 13 Jun 2024 18:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09358v1</guid></item><item><title>Advancing Graph Generation through Beta Diffusion</title><link>http://arxiv.org/abs/2406.09357v1</link><description>Diffusion models have demonstrated effectiveness in generating natural imagesand have been extended to generate diverse data types, including graphs. Thisnew generation of diffusion-based graph generative models has demonstratedsignificant performance improvements over methods that rely on variationalautoencoders or generative adversarial networks. It's important to recognize,however, that most of these models employ Gaussian or categorical diffusionprocesses, which can struggle with sparse and long-tailed data distributions.In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-basedgenerative model particularly adept at capturing diverse graph structures. GBDutilizes a beta diffusion process, tailored for the sparse and range-boundedcharacteristics of graph adjacency matrices. Furthermore, we have developed amodulation technique that enhances the realism of the generated graphs bystabilizing the generation of critical graph structures, while preservingflexibility elsewhere. The outstanding performance of GBD across three generalgraph benchmarks and two biochemical graph benchmarks highlights its capabilityto effectively capture the complexities of real-world graph data. The code willbe made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion</description><author>Yilin He, Xinyang Liu, Bo Chen, Mingyuan Zhou</author><pubDate>Thu, 13 Jun 2024 18:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09357v1</guid></item><item><title>Strategic Facility Location with Clients that Minimize Total Waiting Time</title><link>http://arxiv.org/abs/2211.14016v3</link><description>We study a non-cooperative two-sided facility location game in whichfacilities and clients behave strategically. This is in contrast to many otherfacility location games in which clients simply visit their closest facility.Facility agents select a location on a graph to open a facility to attract asmuch purchasing power as possible, while client agents choose which facilitiesto patronize by strategically distributing their purchasing power in order tominimize their total waiting time. Here, the waiting time of a facility dependson its received total purchasing power. We show that our client stage is anatomic splittable congestion game, which implies existence, uniqueness andefficient computation of a client equilibrium. Therefore, facility agents canefficiently predict client behavior and make strategic decisions accordingly.Despite that, we prove that subgame perfect equilibria do not exist in allinstances of this game and that their existence is NP-hard to decide. On thepositive side, we provide a simple and efficient algorithm to compute3-approximate subgame perfect equilibria.</description><author>Simon Krogmann, Pascal Lenzner, Alexander Skopalik</author><pubDate>Thu, 13 Jun 2024 18:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14016v3</guid></item><item><title>CMC-Bench: Towards a New Paradigm of Visual Signal Compression</title><link>http://arxiv.org/abs/2406.09356v1</link><description>Ultra-low bitrate image compression is a challenging and demanding topic.With the development of Large Multimodal Models (LMMs), a Cross ModalityCompression (CMC) paradigm of Image-Text-Image has emerged. Compared withtraditional codecs, this semantic-level compression can reduce image data sizeto 0.1\% or even lower, which has strong potential applications. However, CMChas certain defects in consistency with the original image and perceptualquality. To address this problem, we introduce CMC-Bench, a benchmark of thecooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) modelsfor image compression. This benchmark covers 18,000 and 40,000 imagesrespectively to verify 6 mainstream I2T and 12 T2I models, including 160,000subjective preference scores annotated by human experts. At ultra-low bitrates,this paper proves that the combination of some I2T and T2I models has surpassedthe most advanced visual signal codecs; meanwhile, it highlights where LMMs canbe further optimized toward the compression task. We encourage LMM developersto participate in this test to promote the evolution of visual signal codecprotocols.</description><author>Chunyi Li, Xiele Wu, Haoning Wu, Donghui Feng, Zicheng Zhang, Guo Lu, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, Weisi Lin</author><pubDate>Thu, 13 Jun 2024 18:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09356v1</guid></item><item><title>Enhancing Domain Adaptation through Prompt Gradient Alignment</title><link>http://arxiv.org/abs/2406.09353v1</link><description>Prior Unsupervised Domain Adaptation (UDA) methods often aim to train adomain-invariant feature extractor, which may hinder the model from learningsufficiently discriminative features. To tackle this, a line of works based onprompt learning leverages the power of large-scale pre-trained vision-languagemodels to learn both domain-invariant and specific features through a set ofdomain-agnostic and domain-specific learnable prompts. Those studies typicallyenforce invariant constraints on representation, output, or prompt space tolearn such prompts. Differently, we cast UDA as a multiple-objectiveoptimization problem in which each objective is represented by a domain loss.Under this new framework, we propose aligning per-objective gradients to fosterconsensus between them. Additionally, to prevent potential overfitting whenfine-tuning this deep learning architecture, we penalize the norm of thesegradients. To achieve these goals, we devise a practical gradient updateprocedure that can work under both single-source and multi-source UDA.Empirically, our method consistently surpasses other prompt-based baselines bya large margin on different UDA benchmarks</description><author>Hoang Phan, Lam Tran, Quyen Tran, Trung Le</author><pubDate>Thu, 13 Jun 2024 18:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09353v1</guid></item><item><title>On the Expressibility of the Reconstructional Color Refinement</title><link>http://arxiv.org/abs/2406.09351v1</link><description>One of the most basic facts related to the famous Ulam reconstructionconjecture is that the connectedness of a graph can be determined by the deckof its vertex-deleted subgraphs, which are considered up to isomorphism. Westrengthen this result by proving that connectedness can still be determinedwhen the subgraphs in the deck are given up to equivalence under the colorrefinement isomorphism test. Consequently, this implies that connectedness isrecognizable by Reconstruction Graph Neural Networks, a recently introduced GNNarchitecture inspired by the reconstruction conjecture (Cotta, Morris, Ribeiro2021).</description><author>V. Arvind, Johannes Köbler, Oleg Verbitsky</author><pubDate>Thu, 13 Jun 2024 18:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09351v1</guid></item><item><title>Real2Code: Reconstruct Articulated Objects via Code Generation</title><link>http://arxiv.org/abs/2406.08474v2</link><description>We present Real2Code, a novel approach to reconstructing articulated objectsvia code generation. Given visual observations of an object, we firstreconstruct its part geometry using an image segmentation model and a shapecompletion model. We then represent the object parts with oriented boundingboxes, which are input to a fine-tuned large language model (LLM) to predictjoint articulation as code. By leveraging pre-trained vision and languagemodels, our approach scales elegantly with the number of articulated parts, andgeneralizes from synthetic training data to real world objects in unstructuredenvironments. Experimental results demonstrate that Real2Code significantlyoutperforms previous state-of-the-art in reconstruction accuracy, and is thefirst approach to extrapolate beyond objects' structural complexity in thetraining set, and reconstructs objects with up to 10 articulated parts. Whenincorporated with a stereo reconstruction model, Real2Code also generalizes toreal world objects from a handful of multi-view RGB images, without the needfor depth or camera information.</description><author>Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song</author><pubDate>Thu, 13 Jun 2024 18:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08474v2</guid></item><item><title>Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking</title><link>http://arxiv.org/abs/2405.12468v2</link><description>We demonstrate substantial performance gains in zero-shot dialogue statetracking (DST) by enhancing training data diversity through synthetic datageneration. Existing DST datasets are severely limited in the number ofapplication domains and slot types they cover due to the high costs of datacollection, restricting their adaptability to new domains. This work addressesthis challenge with a novel, fully automatic data generation approach thatcreates synthetic zero-shot DST datasets. Distinguished from previous methods,our approach can generate dialogues across a massive range of applicationdomains, complete with silver-standard dialogue state annotations and slotdescriptions. This technique is used to create the D0T dataset for trainingzero-shot DST models, encompassing an unprecedented 1,000+ domains. Experimentson the MultiWOZ benchmark show that training models on diverse synthetic dataimproves Joint Goal Accuracy by 6.7%, achieving results competitive with models13.5 times larger than ours.</description><author>James D. Finch, Jinho D. Choi</author><pubDate>Thu, 13 Jun 2024 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12468v2</guid></item><item><title>Separations in the Representational Capabilities of Transformers and Recurrent Architectures</title><link>http://arxiv.org/abs/2406.09347v1</link><description>Transformer architectures have been widely adopted in foundation models. Dueto their high inference costs, there is renewed interest in exploring thepotential of efficient recurrent architectures (RNNs). In this paper, weanalyze the differences in the representational capabilities of Transformersand RNNs across several tasks of practical relevance, including index lookup,nearest neighbor, recognizing bounded Dyck languages, and string equality. Forthe tasks considered, our results show separations based on the size of themodel required for different architectures. For example, we show that aone-layer Transformer of logarithmic width can perform index lookup, whereas anRNN requires a hidden state of linear size. Conversely, while constant-sizeRNNs can recognize bounded Dyck languages, we show that one-layer Transformersrequire a linear size for this task. Furthermore, we show that two-layerTransformers of logarithmic size can perform decision tasks such as stringequality or disjointness, whereas both one-layer Transformers and recurrentmodels require linear size for these tasks. We also show that a log-sizetwo-layer Transformer can implement the nearest neighbor algorithm in itsforward pass; on the other hand recurrent models require linear size. Ourconstructions are based on the existence of $N$ nearly orthogonal vectors in$O(\log N)$ dimensional space and our lower bounds are based on reductions fromcommunication complexity problems. We supplement our theoretical results withexperiments that highlight the differences in the performance of thesearchitectures on practical-size sequences.</description><author>Satwik Bhattamishra, Michael Hahn, Phil Blunsom, Varun Kanade</author><pubDate>Thu, 13 Jun 2024 18:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09347v1</guid></item><item><title>Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores</title><link>http://arxiv.org/abs/2406.09346v1</link><description>In this study, we present ScoreFormer, a novel graph transformer modeldesigned to accurately predict molecular docking scores, thereby optimizinghigh-throughput virtual screening (HTVS) in drug discovery. The architectureintegrates Principal Neighborhood Aggregation (PNA) and Learnable Random WalkPositional Encodings (LRWPE), enhancing the model's ability to understandcomplex molecular structures and their relationship with their respectivedocking scores. This approach significantly surpasses traditional HTVS methodsand recent Graph Neural Network (GNN) models in both recovery and efficiencydue to a wider coverage of the chemical space and enhanced performance. Ourresults demonstrate that ScoreFormer achieves competitive performance indocking score prediction and offers a substantial 1.65-fold reduction ininference time compared to existing models. We evaluated ScoreFormer acrossmultiple datasets under various conditions, confirming its robustness andreliability in identifying potential drug candidates rapidly.</description><author>Álvaro Ciudad, Adrián Morales-Pastor, Laura Malo, Isaac Filella-Mercè, Victor Guallar, Alexis Molina</author><pubDate>Thu, 13 Jun 2024 18:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09346v1</guid></item><item><title>DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding</title><link>http://arxiv.org/abs/2406.09345v1</link><description>The integration of pre-trained text-based large language models (LLM) withspeech input has enabled instruction-following capabilities for diverse speechtasks. This integration requires the use of a speech encoder, a speech adapter,and an LLM, trained on diverse tasks. We propose the use of discrete speechunits (DSU), rather than continuous-valued speech encoder outputs, that areconverted to the LLM token embedding space using the speech adapter. Wegenerate DSU using a self-supervised speech encoder followed by k-meansclustering. The proposed model shows robust performance on speech inputs fromseen/unseen domains and instruction-following capability in spoken questionanswering. We also explore various types of DSU extracted from different layersof the self-supervised speech encoder, as well as Mel frequency CepstralCoefficients (MFCC). Our findings suggest that the ASR task and datasets arenot crucial in instruction-tuning for spoken question answering tasks.</description><author>Suwon Shon, Kwangyoun Kim, Yi-Te Hsu, Prashant Sridhar, Shinji Watanabe, Karen Livescu</author><pubDate>Thu, 13 Jun 2024 18:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09345v1</guid></item><item><title>FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models</title><link>http://arxiv.org/abs/2402.10986v2</link><description>We introduce FinTral, a suite of state-of-the-art multimodal large languagemodels (LLMs) built upon the Mistral-7b model and tailored for financialanalysis. FinTral integrates textual, numerical, tabular, and image data. Weenhance FinTral with domain-specific pretraining, instruction fine-tuning, andRLAIF training by exploiting a large collection of textual and visual datasetswe curate for this work. We also introduce an extensive benchmark featuringnine tasks and 25 datasets for evaluation, including hallucinations in thefinancial domain. Our FinTral model trained with direct preference optimizationemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R,demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5in all tasks and surpasses GPT-4 in five out of nine tasks, marking asignificant advancement in AI-driven financial technology. We also demonstratethat FinTral has the potential to excel in real-time analysis anddecision-making in diverse financial contexts. The GitHub repository for\textit{FinTral} is available at \url{https://github.com/UBC-NLP/fintral}.</description><author>Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, Muhammad Abdul-Mageed</author><pubDate>Thu, 13 Jun 2024 18:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10986v2</guid></item><item><title>Personalized Product Assortment with Real-time 3D Perception and Bayesian Payoff Estimation</title><link>http://arxiv.org/abs/2406.07769v2</link><description>Product assortment selection is a critical challenge facing physicalretailers. Effectively aligning inventory with the preferences of shoppers canincrease sales and decrease out-of-stocks. However, in real-world settings theproblem is challenging due to the combinatorial explosion of product assortmentpossibilities. Consumer preferences are typically heterogeneous across spaceand time, making inventory-preference alignment challenging. Additionally,existing strategies rely on syndicated data, which tends to be aggregated, lowresolution, and suffer from high latency. To solve these challenges, weintroduce a real-time recommendation system, which we call EdgeRec3D. Oursystem utilizes recent advances in 3D computer vision for perception andautomatic, fine grained sales estimation. These perceptual components run onthe edge of the network and facilitate real-time reward signals. Additionally,we develop a Bayesian payoff model to account for noisy estimates from 3D LIDARdata. We rely on spatial clustering to allow the system to adapt toheterogeneous consumer preferences, and a graph-based candidate generationalgorithm to address the combinatorial search problem. We test our system inreal-world stores across two, 6-8 week A/B tests with beverage products anddemonstrate a 35% and 27% increase in sales respectively. Finally, we monitorthe deployed system for a period of 28 weeks with an observational study andshow a 9.4% increase in sales.</description><author>Porter Jenkins, Michael Selander, J. Stockton Jenkins, Andrew Merrill, Kyle Armstrong</author><pubDate>Thu, 13 Jun 2024 18:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07769v2</guid></item><item><title>Learning the Influence Graph of a High-Dimensional Markov Process with Memory</title><link>http://arxiv.org/abs/2406.09338v1</link><description>Motivated by multiple applications in social networks, nervous systems, andfinancial risk analysis, we consider the problem of learning the underlying(directed) influence graph or causal graph of a high-dimensional multivariatediscrete-time Markov process with memory. At any discrete time instant, eachobserved variable of the multivariate process is a binary string of randomlength, which is parameterized by an unobservable or hidden [0,1]-valuedscalar. The hidden scalars corresponding to the variables evolve according todiscrete-time linear stochastic dynamics dictated by the underlying influencegraph whose nodes are the variables. We extend an existing algorithm forlearning i.i.d. graphical models to this Markovian setting with memory andprove that it can learn the influence graph based on the binary observationsusing logarithmic (in number of variables or nodes) samples when the degree ofthe influence graph is bounded. The crucial analytical contribution of thiswork is the derivation of the sample complexity result by upper and lowerbounding the rate of convergence of the observed Markov process with memory toits stationary distribution in terms of the parameters of the influence graph.</description><author>Smita Bagewadi, Avhishek Chatterjee</author><pubDate>Thu, 13 Jun 2024 18:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09338v1</guid></item><item><title>Instance-level quantitative saliency in multiple sclerosis lesion segmentation</title><link>http://arxiv.org/abs/2406.09335v1</link><description>In recent years, explainable methods for artificial intelligence (XAI) havetried to reveal and describe models' decision mechanisms in the case ofclassification tasks. However, XAI for semantic segmentation and in particularfor single instances has been little studied to date. Understanding the processunderlying automatic segmentation of single instances is crucial to reveal whatinformation was used to detect and segment a given object of interest. In thisstudy, we proposed two instance-level explanation maps for semanticsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigatedtheir relevance for the detection and segmentation of white matter lesions(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scanswere collected at the University Hospital of Basel, Switzerland. Data wererandomly split into training, validation and test sets to train a 3D U-Net forMS lesion segmentation. We observed 3050 true positive (TP), 1818 falsepositive (FP), and 789 false negative (FN) cases. We generated instance-levelexplanation maps for semantic segmentation, by developing two XAI methods basedon SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradientsin saliency maps with respect to both input MRI sequences; 2) the model'sresponse in the case of synthetic lesions; 3) the amount of perilesional tissueneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) inFLAIR showed positive values inside a lesion and negative in its neighborhood.Peak values of saliency maps generated for these four groups of volumespresented distributions that differ significantly from one another, suggestinga quantitative nature of the proposed saliency. Contextual information of 7mmaround the lesion border was required for their segmentation.</description><author>Federico Spagnolo, Nataliia Molchanova, Roger Schaer, Meritxell Bach Cuadra, Mario Ocampo Pineda, Lester Melie-Garcia, Cristina Granziera, Vincent Andrearczyk, Adrien Depeursinge</author><pubDate>Thu, 13 Jun 2024 18:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09335v1</guid></item><item><title>ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models</title><link>http://arxiv.org/abs/2406.09334v1</link><description>Performance prediction is a method to estimate the performance ofmultilingual language models (LMs), mitigating computational costs associatedwith model capacity and data for fine-tuning. Our paper introduces ProxyLM, ascalable framework for predicting LM performance using proxy models inmultilingual tasks. These proxy models act as surrogates, approximating theperformance of fine-tuned LMs on specific downstream natural languageprocessing (NLP) tasks. By leveraging proxy models, ProxyLM significantlyreduces computational overhead on task evaluations, achieving up to a 37.08xspeedup compared to traditional methods, even with our smallest proxy models.Additionally, our methodology showcases adaptability to previously unseenlanguages in pre-trained LMs, outperforming the state-of-the-art performance by1.89x as measured by root-mean-square-error (RMSE). This framework streamlinesmodel selection, enabling efficient deployment and iterative LM enhancementswithout extensive computational resources.</description><author>David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, En-Shiun Annie Lee</author><pubDate>Thu, 13 Jun 2024 18:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09334v1</guid></item><item><title>Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image Analysis</title><link>http://arxiv.org/abs/2406.09333v1</link><description>Whole Slide Images (WSIs) are crucial for modern pathological diagnosis, yettheir gigapixel-scale resolutions and sparse informative regions posesignificant computational challenges. Traditional dense attention mechanisms,widely used in computer vision and natural language processing, are impracticalfor WSI analysis due to the substantial data scale and the redundant processingof uninformative areas. To address these challenges, we proposeMemory-Efficient Sparse Pyramid Attention Networks with Shifted Windows (SPAN),drawing inspiration from state-of-the-art sparse attention techniques in otherdomains. SPAN introduces a sparse pyramid attention architecture thathierarchically focuses on informative regions within the WSI, aiming to reducememory overhead while preserving critical features. Additionally, theincorporation of shifted windows enables the model to capture long-rangecontextual dependencies essential for accurate classification. We evaluatedSPAN on multiple public WSI datasets, observing its competitive performance.Unlike existing methods that often struggle to model spatial and contextualinformation due to memory constraints, our approach enables the accuratemodeling of these crucial features. Our study also highlights the importance ofkey design elements in attention mechanisms, such as the shifted-window schemeand the hierarchical structure, which contribute substantially to theeffectiveness of SPAN in WSI analysis. The potential of SPAN formemory-efficient and effective analysis of WSI data is thus demonstrated, andthe code will be made publicly available following the publication of thiswork.</description><author>Weiyi Wu, Chongyang Gao, Xinwen Xu, Siting Li, Jiang Gui</author><pubDate>Thu, 13 Jun 2024 18:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09333v1</guid></item><item><title>Learning from Natural Language Explanations for Generalizable Entity Matching</title><link>http://arxiv.org/abs/2406.09330v1</link><description>Entity matching is the task of linking records from different sources thatrefer to the same real-world entity. Past work has primarily treated entitylinking as a standard supervised learning problem. However, supervised entitymatching models often do not generalize well to new data, and collectingexhaustive labeled training data is often cost prohibitive. Further, recentefforts have adopted LLMs for this task in few/zero-shot settings, exploitingtheir general knowledge. But LLMs are prohibitively expensive for performinginference at scale for real-world entity matching tasks. As an efficient alternative, we re-cast entity matching as a conditionalgeneration task as opposed to binary classification. This enables us to"distill" LLM reasoning into smaller entity matching models via naturallanguage explanations. This approach achieves strong performance, especially onout-of-domain generalization tests (10.85% F-1) where standalone generativemethods struggle. We perform ablations that highlight the importance ofexplanations, both for performance and model robustness.</description><author>Somin Wadhwa, Adit Krishnan, Runhui Wang, Byron C. Wallace, Chris Kong</author><pubDate>Thu, 13 Jun 2024 18:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09330v1</guid></item><item><title>Is Value Learning Really the Main Bottleneck in Offline RL?</title><link>http://arxiv.org/abs/2406.09329v1</link><description>While imitation learning requires access to high-quality data, offlinereinforcement learning (RL) should, in principle, perform similarly or betterwith substantially lower data quality by using a value function. However,current results indicate that offline RL often performs worse than imitationlearning, and it is often unclear what holds back the performance of offlineRL. Motivated by this observation, we aim to understand the bottlenecks incurrent offline RL algorithms. While poor performance of offline RL istypically attributed to an imperfect value function, we ask: is the mainbottleneck of offline RL indeed in learning the value function, or somethingelse? To answer this question, we perform a systematic empirical study of (1)value learning, (2) policy extraction, and (3) policy generalization in offlineRL problems, analyzing how these components affect performance. We make twosurprising observations. First, we find that the choice of a policy extractionalgorithm significantly affects the performance and scalability of offline RL,often more so than the value learning objective. For instance, we show thatcommon value-weighted behavioral cloning objectives (e.g., AWR) do not fullyleverage the learned value function, and switching to behavior-constrainedpolicy gradient objectives (e.g., DDPG+BC) often leads to substantialimprovements in performance and scalability. Second, we find that a big barrierto improving offline RL performance is often imperfect policy generalization ontest-time states out of the support of the training data, rather than policylearning on in-distribution states. We then show that the use of suboptimal buthigh-coverage data or test-time policy training techniques can address thisgeneralization issue in practice. Specifically, we propose two simple test-timepolicy improvement methods and show that these methods lead to betterperformance.</description><author>Seohong Park, Kevin Frans, Sergey Levine, Aviral Kumar</author><pubDate>Thu, 13 Jun 2024 18:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09329v1</guid></item><item><title>Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN Pipeline applied on PSMA PET/CT Scans</title><link>http://arxiv.org/abs/2406.09327v1</link><description>Assessing tumor response to systemic therapies is one of the mainapplications of PET/CT. Routinely, only a small subset of index lesions out ofmultiple lesions is analyzed. However, this operator dependent selection maybias the results due to possible significant inter-metastatic heterogeneity ofresponse to therapy. Automated, AI based approaches for lesion tracking holdpromise in enabling the analysis of many more lesions and thus providing abetter assessment of tumor response. This work introduces a Siamese CNNapproach for lesion tracking between PET/CT scans. Our approach is applied onthe laborious task of tracking a high number of bone lesions in full-bodybaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cyclesof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancerpatients. Data preparation includes lesion segmentation and affineregistration. Our algorithm extracts suitable lesion patches and forwards theminto a Siamese CNN trained to classify the lesion patch pairs as correspondingor non-corresponding lesions. Experiments have been performed with differentinput patch types and a Siamese network in 2D and 3D. The CNN modelsuccessfully learned to classify lesion assignments, reaching a lesion trackingaccuracy of 83 % in its best configuration with an AUC = 0.91. For remaininglesions the pipeline accomplished a re-identification rate of 89 %. We provedthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CTscans. Future clinical studies are necessary if this improves the prediction ofthe outcome of therapies.</description><author>Stefan P. Hein, Manuel Schultheiss, Andrei Gafita, Raphael Zaum, Farid Yagubbayli, Isabel Rauscher, Matthias Eiber, Franz Pfeiffer, Wolfgang A. Weber</author><pubDate>Thu, 13 Jun 2024 18:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09327v1</guid></item><item><title>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</title><link>http://arxiv.org/abs/2306.04539v2</link><description>In many machine learning systems that jointly learn from multiple modalities,a core research question is to understand the nature of multimodalinteractions: how modalities combine to provide new task-relevant informationthat was not present in either alone. We study this challenge of interactionquantification in a semi-supervised setting with only labeled unimodal data andnaturally co-occurring multimodal data (e.g., unlabeled images and captions,video and corresponding audio) but when labeling them is time-consuming. Usinga precise information-theoretic definition of interactions, our keycontribution is the derivation of lower and upper bounds to quantify the amountof multimodal interactions in this semi-supervised setting. We propose twolower bounds: one based on the shared information between modalities and theother based on disagreement between separately trained unimodal classifiers,and derive an upper bound through connections to approximate algorithms formin-entropy couplings. We validate these estimated bounds and show how theyaccurately track true interactions. Finally, we show how these theoreticalresults can be used to estimate multimodal model performance, guide datacollection, and select appropriate multimodal models for various tasks.</description><author>Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Thu, 13 Jun 2024 18:05:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04539v2</guid></item><item><title>PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance</title><link>http://arxiv.org/abs/2406.09326v1</link><description>Recently, artificial intelligence techniques for education have been receivedincreasing attentions, while it still remains an open problem to design theeffective music instrument instructing systems. Although key presses can bedirectly derived from sheet music, the transitional movements among key pressesrequire more extensive guidance in piano performance. In this work, weconstruct a piano-hand motion generation benchmark to guide hand movements andfingerings for piano playing. To this end, we collect an annotated dataset,PianoMotion10M, consisting of 116 hours of piano playing videos from abird's-eye view with 10 million annotated hand poses. We also introduce apowerful baseline model that generates hand motions from piano audios through aposition predictor and a position-guided gesture generator. Furthermore, aseries of evaluation metrics are designed to assess the performance of thebaseline model, including motion similarity, smoothness, positional accuracy ofleft and right hands, and overall fidelity of movement distribution. Despitethat piano key presses with respect to music scores or audios are alreadyaccessible, PianoMotion10M aims to provide guidance on piano fingering forinstruction purposes. The dataset and source code can be accessed athttps://agnjason.github.io/PianoMotion-page.</description><author>Qijun Gan, Song Wang, Shengtao Wu, Jianke Zhu</author><pubDate>Thu, 13 Jun 2024 18:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09326v1</guid></item><item><title>REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space</title><link>http://arxiv.org/abs/2406.09325v1</link><description>Large language models (LLMs) risk inadvertently memorizing and divulgingsensitive or personally identifiable information (PII) seen in training data,causing privacy concerns. Current approaches to address this issue involvecostly dataset scrubbing, or model filtering through unlearning and modelediting, which can be bypassed through extraction attacks. We propose REVS, anovel model editing method for unlearning sensitive information from LLMs. REVSidentifies and modifies a small subset of neurons relevant for each piece ofsensitive information. By projecting these neurons to the vocabulary space(unembedding), we pinpoint the components driving its generation. We thencompute a model edit based on the pseudo-inverse of the unembedding matrix, andapply it to de-promote generation of the targeted sensitive data. To adequatelyevaluate our method on truly sensitive information, we curate two datasets: anemail dataset inherently memorized by GPT-J, and a synthetic social securitynumber dataset that we tune the model to memorize. Compared to otherstate-of-the-art model editing methods, REVS demonstrates superior performancein both eliminating sensitive information and robustness to extraction attacks,while retaining integrity of the underlying model. The code and a demo notebookare available at https://technion-cs-nlp.github.io/REVS.</description><author>Tomer Ashuach, Martin Tutek, Yonatan Belinkov</author><pubDate>Thu, 13 Jun 2024 18:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09325v1</guid></item><item><title>Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs</title><link>http://arxiv.org/abs/2406.09324v1</link><description>Although Large Language Models (LLMs) have demonstrated significantcapabilities in executing complex tasks in a zero-shot manner, they aresusceptible to jailbreak attacks and can be manipulated to produce harmfuloutputs. Recently, a growing body of research has categorized jailbreak attacksinto token-level and prompt-level attacks. However, previous work primarilyoverlooks the diverse key factors of jailbreak attacks, with most studiesconcentrating on LLM vulnerabilities and lacking exploration ofdefense-enhanced LLMs. To address these issues, we evaluate the impact ofvarious attack settings on LLM performance and provide a baseline benchmark forjailbreak attacks, encouraging the adoption of a standardized evaluationframework. Specifically, we evaluate the eight key factors of implementingjailbreak attacks on LLMs from both target-level and attack-level perspectives.We further conduct seven representative jailbreak attacks on six defensemethods across two widely used datasets, encompassing approximately 320experiments with about 50,000 GPU hours on A800-80G. Our experimental resultshighlight the need for standardized benchmarking to evaluate these attacks ondefense-enhanced LLMs. Our code is available athttps://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.</description><author>Zhao Xu, Fan Liu, Hao Liu</author><pubDate>Thu, 13 Jun 2024 18:01:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09324v1</guid></item><item><title>Effects of Multimodal Explanations for Autonomous Driving on Driving Performance, Cognitive Load, Expertise, Confidence, and Trust</title><link>http://arxiv.org/abs/2401.04206v4</link><description>Advances in autonomous driving provide an opportunity for AI-assisted drivinginstruction that directly addresses the critical need for human drivingimprovement. How should an AI instructor convey information to promotelearning? In a pre-post experiment (n = 41), we tested the impact of an AICoach's explanatory communications modeled after performance driving expertinstructions. Participants were divided into four (4) groups to assess two (2)dimensions of the AI coach's explanations: information type ('what' and'why'-type explanations) and presentation modality (auditory and visual). Wecompare how different explanatory techniques impact driving performance,cognitive load, confidence, expertise, and trust via observational learning.Through interview, we delineate participant learning processes. Results show AIcoaching can effectively teach performance driving skills to novices. We findthe type and modality of information influences performance outcomes.Differences in how successfully participants learned are attributed to howinformation directs attention, mitigates uncertainty, and influences overloadexperienced by participants. Results suggest efficient, modality-appropriateexplanations should be opted for when designing effective HMI communicationsthat can instruct without overwhelming. Further, results support the need toalign communications with human learning and cognitive processes. We provideeight design implications for future autonomous vehicle HMI and AI coachdesign.</description><author>Robert Kaufman, Jean Costa, Everlyne Kimani</author><pubDate>Thu, 13 Jun 2024 18:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04206v4</guid></item><item><title>Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret</title><link>http://arxiv.org/abs/2302.10796v2</link><description>While quantum reinforcement learning (RL) has attracted a surge of attentionrecently, its theoretical understanding is limited. In particular, it remainselusive how to design provably efficient quantum RL algorithms that can addressthe exploration-exploitation trade-off. To this end, we propose a novelUCRL-style algorithm that takes advantage of quantum computing for tabularMarkov decision processes (MDPs) with $S$ states, $A$ actions, and horizon $H$,and establish an $\mathcal{O}(\mathrm{poly}(S, A, H, \log T))$ worst-caseregret for it, where $T$ is the number of episodes. Furthermore, we extend ourresults to quantum RL with linear function approximation, which is capable ofhandling problems with large state spaces. Specifically, we develop a quantumalgorithm based on value target regression (VTR) for linear mixture MDPs with$d$-dimensional linear representation and prove that it enjoys$\mathcal{O}(\mathrm{poly}(d, H, \log T))$ regret. Our algorithms are variantsof UCRL/UCRL-VTR algorithms in classical RL, which also leverage a novelcombination of lazy updating mechanisms and quantum estimation subroutines.This is the key to breaking the $\Omega(\sqrt{T})$-regret barrier in classicalRL. To the best of our knowledge, this is the first work studying the onlineexploration in quantum RL with provable logarithmic worst-case regret.</description><author>Han Zhong, Jiachen Hu, Yecheng Xue, Tongyang Li, Liwei Wang</author><pubDate>Thu, 13 Jun 2024 18:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10796v2</guid></item><item><title>Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines</title><link>http://arxiv.org/abs/2406.09322v1</link><description>We investigate the application of active inference in developingenergy-efficient control agents for manufacturing systems. Active inference,rooted in neuroscience, provides a unified probabilistic framework integratingperception, learning, and action, with inherent uncertainty quantificationelements. Our study explores deep active inference, an emerging field thatcombines deep learning with the active inference decision-making framework.Leveraging a deep active inference agent, we focus on controlling parallel andidentical machine workstations to enhance energy efficiency. We addresschallenges posed by the problem's stochastic nature and delayed policy responseby introducing tailored enhancements to existing agent architectures.Specifically, we introduce multi-step transition and hybrid horizon methods tomitigate the need for complex planning. Our experimental results demonstratethe effectiveness of these enhancements and highlight the potential of theactive inference-based approach.</description><author>Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta</author><pubDate>Thu, 13 Jun 2024 18:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09322v1</guid></item><item><title>JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models</title><link>http://arxiv.org/abs/2406.09321v1</link><description>Jailbreak attacks aim to induce Large Language Models (LLMs) to generateharmful responses for forbidden instructions, presenting severe misuse threatsto LLMs. Up to now, research into jailbreak attacks and defenses is emerging,however, there is (surprisingly) no consensus on how to evaluate whether ajailbreak attempt is successful. In other words, the methods to assess theharmfulness of an LLM's response are varied, such as manual annotation orprompting GPT-4 in specific ways. Each approach has its own set of strengthsand weaknesses, impacting their alignment with human values, as well as thetime and financial cost. This diversity in evaluation presents challenges forresearchers in choosing suitable evaluation methods and conducting faircomparisons across different jailbreak attacks and defenses. In this paper, weconduct a comprehensive analysis of jailbreak evaluation methodologies, drawingfrom nearly ninety jailbreak research released between May 2023 and April 2024.Our study introduces a systematic taxonomy of jailbreak evaluators, offeringin-depth insights into their strengths and weaknesses, along with the currentstatus of their adaptation. Moreover, to facilitate subsequent research, wepropose JailbreakEval, a user-friendly toolkit focusing on the evaluation ofjailbreak attempts. It includes various well-known evaluators out-of-the-box,so that users can obtain evaluation results with only a single command.JailbreakEval also allows users to customize their own evaluation workflow in aunified framework with the ease of development and comparison. In summary, weregard JailbreakEval to be a catalyst that simplifies the evaluation process injailbreak research and fosters an inclusive standard for jailbreak evaluationwithin the community.</description><author>Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang</author><pubDate>Thu, 13 Jun 2024 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09321v1</guid></item><item><title>Khmer Semantic Search Engine: Digital Information Access and Document Retrieval</title><link>http://arxiv.org/abs/2406.09320v1</link><description>The search engine process is crucial for document content retrieval. ForKhmer documents, a tool is needed to extract essential keywords. Despite thedaily generation of significant Khmer content, Cambodians struggle to findnecessary documents due to the lack of an effective semantic searching tool.Even Google does not deliver high accuracy for Khmer content. Semantic searchengines improve search results by employing advanced algorithms to understandvarious content types. With the rise in Khmer digital content such as reports,articles, and social media feedback enhanced search capabilities are essential.This research proposes the first Khmer Semantic Search Engine (KSE), designedto improve traditional Khmer search methods. Utilizing semantic matchingtechniques and formally annotated semantic content, our tool extractsmeaningful keywords from user queries performs precise matching, and providesthe best matching offline documents and online URL documents. We propose twosemantic search frameworks based on keyword extraction and semantic searchmatching. Additionally, we developed tools for data preparation, includingdocument addition and manual keyword extraction. To evaluate performance, wecreated a ground truth dataset and discussed issues related to searching andsemantic search. Our findings show how understanding search term semantics canlead to more accurate results.</description><author>Nimol Thuon</author><pubDate>Thu, 13 Jun 2024 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09320v1</guid></item><item><title>Characterising Interventions in Causal Games</title><link>http://arxiv.org/abs/2406.09318v1</link><description>Causal games are probabilistic graphical models that enable causal queries tobe answered in multi-agent settings. They extend causal Bayesian networks byspecifying decision and utility variables to represent the agents' degrees offreedom and objectives. In multi-agent settings, whether each agent decides ontheir policy before or after knowing the causal intervention is important asthis affects whether they can respond to the intervention by adapting theirpolicy. Consequently, previous work in causal games imposed chronologicalconstraints on permissible interventions. We relax this by outlining a soundand complete set of primitive causal interventions so the effect of anyarbitrarily complex interventional query can be studied in multi-agentsettings. We also demonstrate applications to the design of safe AI systems byconsidering causal mechanism design and commitment.</description><author>Manuj Mishra, James Fox, Michael Wooldridge</author><pubDate>Thu, 13 Jun 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09318v1</guid></item><item><title>SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data</title><link>http://arxiv.org/abs/2405.09805v2</link><description>Traditional security mechanisms isolate resources from users who should notaccess them. We reflect the compositional nature of such security mechanismsback into the structure of LLMs to build a provably secure LLM; that we termSecureLLM. Other approaches to LLM safety attempt to protect against bad actorsor bad outcomes, but can only do so to an extent making them inappropriate forsensitive data. SecureLLM blends access security with fine-tuning methods. Eachdata silo has associated with it a separate fine-tuning and a user has accessonly to the collection of fine-tunings that they have permission for. The modelmust then perform on compositional tasks at the intersection of those datasilos with the combination of those individual fine-tunings. While applicableto any task like document QA or making API calls, in this work we concernourselves with models that learn the layouts of new SQL databases to providenatural-language-to-SQL translation capabilities. Existing fine-tuningcomposition methods fail in this challenging environment, as they are notwell-equipped for handling compositional tasks. Compositionality remains achallenge for LLMs. We contribute both a difficult new compositionalnatural-language-to-SQL translation task and a new perspective on LLM securitythat allows models to be deployed to secure environments today.</description><author>Abdulrahman Alabdulkareem, Christian M Arnold, Yerim Lee, Pieter M Feenstra, Boris Katz, Andrei Barbu</author><pubDate>Thu, 13 Jun 2024 17:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09805v2</guid></item><item><title>Common and Rare Fundus Diseases Identification Using Vision-Language Foundation Model with Knowledge of Over 400 Diseases</title><link>http://arxiv.org/abs/2406.09317v1</link><description>The current retinal artificial intelligence models were trained using datawith a limited category of diseases and limited knowledge. In this paper, wepresent a retinal vision-language foundation model (RetiZero) with knowledge ofover 400 fundus diseases. Specifically, we collected 341,896 fundus imagespaired with text descriptions from 29 publicly available datasets, 180ophthalmic books, and online resources, encompassing over 400 fundus diseasesacross multiple countries and ethnicities. RetiZero achieved outstandingperformance across various downstream tasks, including zero-shot retinaldisease recognition, image-to-image retrieval, internal domain and cross-domainretinal disease classification, and few-shot fine-tuning. Specially, in thezero-shot scenario, RetiZero achieved a Top5 score of 0.8430 and 0.7561 on 15and 52 fundus diseases respectively. In the image-retrieval task, RetiZeroachieved a Top5 score of 0.9500 and 0.8860 on 15 and 52 retinal diseasesrespectively. Furthermore, clinical evaluations by ophthalmology experts fromdifferent countries demonstrate that RetiZero can achieve performancecomparable to experienced ophthalmologists using zero-shot and image retrievalmethods without requiring model retraining. These capabilities of retinaldisease identification strengthen our RetiZero foundation model in clinicalimplementation.</description><author>Meng Wang, Tian Lin, Kai Yu, Aidi Lin, Yuanyuan Peng, Lianyu Wang, Cheng Chen, Ke Zou, Huiyu Liang, Man Chen, Xue Yao, Meiqin Zhang, Binwei Huang, Chaoxin Zheng, Wei Chen, Yilong Luo, Yifan Chen, Jingcheng Wang, Yih Chung Tham, Dianbo Liu, Wendy Wong, Sahil Thakur, Beau Fenner, Yanda Meng, Yukun Zhou, Zehua Jiang, Minghui Qiu, Changqing Zhang, Xinjian Chen, Sophia Y. Wang, Cecilia S. Lee, Lucia Sobrin, Pearse A. Keane, Ching-Yu Cheng, Haoyu Chen, Huazhu Fu</author><pubDate>Thu, 13 Jun 2024 17:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09317v1</guid></item><item><title>RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar</title><link>http://arxiv.org/abs/2405.14014v3</link><description>3D occupancy-based perception pipeline has significantly advanced autonomousdriving by capturing detailed scene descriptions and demonstrating stronggeneralizability across various object categories and shapes. Current methodspredominantly rely on LiDAR or camera inputs for 3D occupancy prediction. Thesemethods are susceptible to adverse weather conditions, limiting the all-weatherdeployment of self-driving cars. To improve perception robustness, we leveragethe recent advances in automotive radars and introduce a novel approach thatutilizes 4D imaging radar sensors for 3D occupancy prediction. Our method,RadarOcc, circumvents the limitations of sparse radar point clouds by directlyprocessing the 4D radar tensor, thus preserving essential scene details.RadarOcc innovatively addresses the challenges associated with the voluminousand noisy 4D radar data by employing Doppler bins descriptors, sidelobe-awarespatial sparsification, and range-wise self-attention mechanisms. To minimizethe interpolation errors associated with direct coordinate transformations, wealso devise a spherical-based feature encoding followed byspherical-to-Cartesian feature aggregation. We benchmark various baselinemethods based on distinct modalities on the public K-Radar dataset. The resultsdemonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancyprediction and promising results even when compared with LiDAR- or camera-basedmethods. Additionally, we present qualitative evidence of the superiorperformance of 4D radar in adverse weather conditions and explore the impact ofkey pipeline components through ablation studies.</description><author>Fangqiang Ding, Xiangyu Wen, Lawrence Zhu, Yiming Li, Chris Xiaoxuan Lu</author><pubDate>Thu, 13 Jun 2024 17:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14014v3</guid></item><item><title>Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers</title><link>http://arxiv.org/abs/2406.09315v1</link><description>In this paper, we show how Transformers can be interpreted as denseExpectation-Maximization algorithms performed on Bayesian Nets. Based on theabove interpretation, we propose a new model design paradigm, namely VerticalLoRA (VLoRA), which reduces the parameter count dramatically while preservingperformance. In VLoRA, a model consists of layers, each of which recursivelylearns an increment based on the previous layer. We then apply LoRAdecomposition to the increments. VLoRA works on the base model, which isorthogonal to LoRA, meaning they can be used together. We do experiments onvarious tasks and models. The results show that 1) with VLoRA, the Transformermodel parameter count can be reduced dramatically and 2) the performance of theoriginal model is preserved. The source code is available at\url{https://github.com/neverUseThisName/vlora}</description><author>Zhuolin Fu</author><pubDate>Thu, 13 Jun 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09315v1</guid></item><item><title>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images</title><link>http://arxiv.org/abs/2403.09871v3</link><description>In this work, we present ThermoHands, a new benchmark for thermal image-basedegocentric 3D hand pose estimation, aimed at overcoming challenges like varyinglighting conditions and obstructions (e.g., handwear). The benchmark includes amulti-view and multi-spectral dataset collected from 28 subjects performinghand-object and hand-virtual interactions under diverse scenarios, accuratelyannotated with 3D hand poses through an automated process. We introduce a newbaseline method, TherFormer, utilizing dual transformer modules for effectiveegocentric 3D hand pose estimation in thermal imagery. Our experimental resultshighlight TherFormer's leading performance and affirm thermal imaging'seffectiveness in enabling robust 3D hand pose estimation in adverse conditions.</description><author>Fangqiang Ding, Lawrence Zhu, Xiangyu Wen, Gaowen Liu, Chris Xiaoxuan Lu</author><pubDate>Thu, 13 Jun 2024 17:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09871v3</guid></item><item><title>Efficient Multimodal Learning from Data-centric Perspective</title><link>http://arxiv.org/abs/2402.11530v2</link><description>Multimodal Large Language Models (MLLMs) have demonstrated notablecapabilities in general visual understanding and reasoning tasks. However,their deployment is hindered by substantial computational costs in bothtraining and inference, limiting accessibility to the broader research and usercommunities. A straightforward solution is to leverage smaller pre-trainedvision and language models, which inevitably cause significant performancedrops. In this paper, we demonstrate the possibility of training a smaller butbetter MLLM with high-quality training data. Specifically, we introduce Bunny,a family of lightweight MLLMs with flexible vision and language backbones forefficient multimodal learning from selected training data. Experiments showthat our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiplebenchmarks. We expect that this work can provide the community with a clean andflexible open-source tool for further research and development. The code,models, and data can be found in https://github.com/BAAI-DCAI/Bunny.</description><author>Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao</author><pubDate>Thu, 13 Jun 2024 17:49:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11530v2</guid></item><item><title>DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks</title><link>http://arxiv.org/abs/2406.04470v2</link><description>This study assesses the ability of Large Vision-Language Models (LVLMs) todifferentiate between AI-generated and human-generated images. It introduces anew automated benchmark construction method for this evaluation. The experimentcompared common LVLMs with human participants using a mixed dataset of AI andhuman-created images. Results showed that LVLMs could distinguish between theimage types to some extent but exhibited a rightward bias, and performsignificantly worse compared to humans. To build on these findings, wedeveloped an automated benchmark construction process using AI. This processinvolved topic retrieval, narrative script generation, error embedding, andimage generation, creating a diverse set of text-image pairs with intentionalerrors. We validated our method through constructing two caparable benchmarks.This study highlights the strengths and weaknesses of LVLMs in real-worldunderstanding and advances benchmark construction techniques, providing ascalable and automatic approach for AI model evaluation.</description><author>Haokun Zhou, Yipeng Hong</author><pubDate>Thu, 13 Jun 2024 17:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04470v2</guid></item><item><title>Neural networks in non-metric spaces</title><link>http://arxiv.org/abs/2406.09310v1</link><description>Leveraging the infinite dimensional neural network architecture we proposedin arXiv:2109.13512v4 and which can process inputs from Fr\'echet spaces, andusing the universal approximation property shown therein, we now largely extendthe scope of this architecture by proving several universal approximationtheorems for a vast class of input and output spaces. More precisely, the inputspace $\mathfrak X$ is allowed to be a general topological space satisfyingonly a mild condition ("quasi-Polish"), and the output space can be eitheranother quasi-Polish space $\mathfrak Y$ or a topological vector space $E$.Similarly to arXiv:2109.13512v4, we show furthermore that our neural networkarchitectures can be projected down to "finite dimensional" subspaces with anydesirable accuracy, thus obtaining approximating networks that are easy toimplement and allow for fast computation and fitting. The resulting neuralnetwork architecture is therefore applicable for prediction tasks based onfunctional data. To the best of our knowledge, this is the first result whichdeals with such a wide class of input/output spaces and simultaneouslyguarantees the numerical feasibility of the ensuing architectures. Finally, weprove an obstruction result which indicates that the category of quasi-Polishspaces is in a certain sense the correct category to work with if one aims atconstructing approximating architectures on infinite-dimensional spaces$\mathfrak X$ which, at the same time, have sufficient expressive power toapproximate continuous functions on $\mathfrak X$, are specified by a finitenumber of parameters only and are "stable" with respect to these parameters.</description><author>Luca Galimberti</author><pubDate>Thu, 13 Jun 2024 17:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09310v1</guid></item><item><title>Transformers meet Neural Algorithmic Reasoners</title><link>http://arxiv.org/abs/2406.09308v1</link><description>Transformers have revolutionized machine learning with their simple yeteffective architecture. Pre-training Transformers on massive text datasets fromthe Internet has led to unmatched generalization for natural languageunderstanding (NLU) tasks. However, such language models remain fragile whentasked with algorithmic forms of reasoning, where computations must be preciseand robust. To address this limitation, we propose a novel approach thatcombines the Transformer's language understanding with the robustness of graphneural network (GNN)-based neural algorithmic reasoners (NARs). Such NARsproved effective as generic solvers for algorithmic tasks, when specified ingraph form. To make their embeddings accessible to a Transformer, we propose ahybrid architecture with a two-phase training procedure, allowing the tokens inthe language model to cross-attend to the node embeddings from the NAR. Weevaluate our resulting TransNAR model on CLRS-Text, the text-based version ofthe CLRS-30 benchmark, and demonstrate significant gains over Transformer-onlymodels for algorithmic reasoning, both in and out of distribution.</description><author>Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, Petar Veličković</author><pubDate>Thu, 13 Jun 2024 17:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09308v1</guid></item><item><title>A tutorial on fairness in machine learning in healthcare</title><link>http://arxiv.org/abs/2406.09307v1</link><description>OBJECTIVE: Ensuring that machine learning (ML) algorithms are safe andeffective within all patient groups, and do not disadvantage particularpatients, is essential to clinical decision making and preventing thereinforcement of existing healthcare inequities. The objective of this tutorialis to introduce the medical informatics community to the common notions offairness within ML, focusing on clinical applications and implementation inpractice. TARGET AUDIENCE: As gaps in fairness arise in a variety of healthcareapplications, this tutorial is designed to provide an understanding offairness, without assuming prior knowledge, to researchers and clinicians whomake use of modern clinical data. SCOPE: We describe the fundamental concepts and methods used to definefairness in ML, including an overview of why models in healthcare may beunfair, a summary and comparison of the metrics used to quantify fairness, anda discussion of some ongoing research. We illustrate some of the fairnessmethods introduced through a case study of mortality prediction in a publiclyavailable electronic health record dataset. Finally, we provide a user-friendlyR package for comprehensive group fairness evaluation, enabling researchers andclinicians to assess fairness in their own ML work.</description><author>Jianhui Gao, Benson Chou, Zachary R. McCaw, Hilary Thurston, Paul Varghese, Chuan Hong, Jessica Gronsbell</author><pubDate>Thu, 13 Jun 2024 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09307v1</guid></item><item><title>Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation</title><link>http://arxiv.org/abs/2406.09305v1</link><description>In subject-driven text-to-image generation, recent works have achievedsuperior performance by training the model on synthetic datasets containingnumerous image pairs. Trained on these datasets, generative models can producetext-aligned images for specific subject from arbitrary testing image in azero-shot manner. They even outperform methods which require additionalfine-tuning on testing images. However, the cost of creating such datasets isprohibitive for most researchers. To generate a single training pair, currentmethods fine-tune a pre-trained text-to-image model on the subject image tocapture fine-grained details, then use the fine-tuned model to create imagesfor the same subject based on creative text prompts. Consequently, constructinga large-scale dataset with millions of subjects can require hundreds ofthousands of GPU hours. To tackle this problem, we propose Toffee, an efficientmethod to construct datasets for subject-driven editing and generation.Specifically, our dataset construction does not need any subject-levelfine-tuning. After pre-training two generative models, we are able to generateinfinite number of high-quality samples. We construct the first large-scaledataset for subject-driven image editing and generation, which contains 5million image pairs, text prompts, and masks. Our dataset is 5 times the sizeof previous largest dataset, yet our cost is tens of thousands of GPU hourslower. To test the proposed dataset, we also propose a model which is capableof both subject-driven image editing and generation. By simply training themodel on our proposed dataset, it obtains competitive results, illustrating theeffectiveness of the proposed dataset construction framework.</description><author>Yufan Zhou, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, Tong Sun</author><pubDate>Thu, 13 Jun 2024 17:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09305v1</guid></item><item><title>Learning with little mixing</title><link>http://arxiv.org/abs/2206.08269v3</link><description>We study square loss in a realizable time-series framework with martingaledifference noise. Our main result is a fast rate excess risk bound which showsthat whenever a trajectory hypercontractivity condition holds, the risk of theleast-squares estimator on dependent data matches the iid rate order-wise aftera burn-in time. In comparison, many existing results in learning from dependentdata have rates where the effective sample size is deflated by a factor of themixing-time of the underlying process, even after the burn-in time.Furthermore, our results allow the covariate process to exhibit long rangecorrelations which are substantially weaker than geometric ergodicity. We callthis phenomenon learning with little mixing, and present several examples forwhen it occurs: bounded function classes for which the $L^2$ and$L^{2+\epsilon}$ norms are equivalent, ergodic finite state Markov chains,various parametric models, and a broad family of infinite dimensional$\ell^2(\mathbb{N})$ ellipsoids. By instantiating our main result to systemidentification of nonlinear dynamics with generalized linear model transitions,we obtain a nearly minimax optimal excess risk bound after only a polynomialburn-in time.</description><author>Ingvar Ziemann, Stephen Tu</author><pubDate>Thu, 13 Jun 2024 17:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08269v3</guid></item><item><title>MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</title><link>http://arxiv.org/abs/2406.09297v1</link><description>Auto-regressive inference of transformers benefit greatly from Key-Value (KV)caching, but can lead to major memory bottlenecks as model size, batch size,and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)sharing, a novel approach extending KV sharing across transformer layers toreduce memory usage beyond what was possible with Multi-Query Attention (MQA)and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks andinference metrics using uptrained Pythia-160M variants demonstrate that MLKVsignificantly reduces memory usage with minimal performance loss, reducing KVcache size down to a factor of 6x compared to MQA. These results highlightMLKV's potential for efficient deployment of transformer models at scale. Weprovide code at https://github.com/zaydzuhri/pythia-mlkv</description><author>Zayd Muhammad Kawakibi Zuhri, Muhammad Farid Adilazuarda, Ayu Purwarianti, Alham Fikri Aji</author><pubDate>Thu, 13 Jun 2024 17:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09297v1</guid></item><item><title>Parameter-Efficient Active Learning for Foundational models</title><link>http://arxiv.org/abs/2406.09296v1</link><description>Foundational vision transformer models have shown impressive few shotperformance on many vision tasks. This research presents a novel investigationinto the application of parameter efficient fine-tuning methods within anactive learning (AL) framework, to advance the sampling selection process inextremely budget constrained classification tasks. The focus on image datasets,known for their out-of-distribution characteristics, adds a layer of complexityand relevance to our study. Through a detailed evaluation, we illustrate theimproved AL performance on these challenging datasets, highlighting thestrategic advantage of merging parameter efficient fine tuning methods withfoundation models. This contributes to the broader discourse on optimizing ALstrategies, presenting a promising avenue for future exploration in leveragingfoundation models for efficient and effective data annotation in specializeddomains.</description><author>Athmanarayanan Lakshmi Narayanan, Ranganath Krishnan, Amrutha Machireddy, Mahesh Subedar</author><pubDate>Thu, 13 Jun 2024 17:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09296v1</guid></item><item><title>AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models</title><link>http://arxiv.org/abs/2406.09295v1</link><description>Evaluating the alignment capabilities of large Vision-Language Models (VLMs)is essential for determining their effectiveness as helpful assistants.However, existing benchmarks primarily focus on basic abilities using nonverbalmethods, such as yes-no and multiple-choice questions. In this paper, weaddress this gap by introducing AlignMMBench, a comprehensive alignmentbenchmark specifically designed for emerging Chinese VLMs. This benchmark ismeticulously curated from real-world scenarios and Chinese Internet sources,encompassing thirteen specific tasks across three categories, and includes bothsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewritestrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answerpairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, arule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, wereport the performance of representative VLMs on AlignMMBench, offeringinsights into the capabilities and limitations of different VLM architectures.All evaluation codes and data are available on https://alignmmbench.github.io.</description><author>Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong</author><pubDate>Thu, 13 Jun 2024 17:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09295v1</guid></item><item><title>You Don't Need Data-Augmentation in Self-Supervised Learning</title><link>http://arxiv.org/abs/2406.09294v1</link><description>Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) hasled to outstanding performances. All instantiations of this paradigm weretrained using strong and well-established hand-crafted data augmentations,leading to the general belief that they are required for the proper trainingand performance of such models. On the other hand, generativereconstruction-based models such as BEIT and MAE or Joint-Embedding PredictiveArchitectures such as I-JEPA have shown strong performance without using dataaugmentations except masking. In this work, we challenge the importance ofinvariance and data-augmentation in JEAs at scale. By running a case-study on arecent SSL foundation model - DINOv2 - we show that strong imagerepresentations can be obtained with JEAs and only cropping without resizingprovided the training data is large enough, reaching state-of-the-art resultsand using the least amount of augmentation in the literature. Through thisstudy, we also discuss the impact of compute constraints on the outcomes ofexperimental deep learning research, showing that they can lead to verydifferent conclusions.</description><author>Théo Moutakanni, Maxime Oquab, Marc Szafraniec, Maria Vakalopoulou, Piotr Bojanowski</author><pubDate>Thu, 13 Jun 2024 17:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09294v1</guid></item><item><title>StableMaterials: Enhancing Diversity in Material Generation via Semi-Supervised Learning</title><link>http://arxiv.org/abs/2406.09293v1</link><description>We introduce StableMaterials, a novel approach for generating photorealisticphysical-based rendering (PBR) materials that integrate semi-supervisedlearning with Latent Diffusion Models (LDMs). Our method employs adversarialtraining to distill knowledge from existing large-scale image generationmodels, minimizing the reliance on annotated data and enhancing the diversityin generation. This distillation approach aligns the distribution of thegenerated materials with that of image textures from an SDXL model, enablingthe generation of novel materials that are not present in the initial trainingdataset. Furthermore, we employ a diffusion-based refiner model to improve thevisual quality of the samples and achieve high-resolution generation. Finally,we distill a latent consistency model for fast generation in just four stepsand propose a new tileability technique that removes visual artifacts typicallyassociated with fewer diffusion steps. We detail the architecture and trainingprocess of StableMaterials, the integration of semi-supervised training withinexisting LDM frameworks and show the advantages of our approach. Comparativeevaluations with state-of-the-art methods show the effectiveness ofStableMaterials, highlighting its potential applications in computer graphicsand beyond. StableMaterials is publicly available athttps://gvecchio.com/stablematerials.</description><author>Giuseppe Vecchio</author><pubDate>Thu, 13 Jun 2024 17:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09293v1</guid></item><item><title>Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models</title><link>http://arxiv.org/abs/2406.09292v1</link><description>We address the problem of multi-object 3D pose control in image diffusionmodels. Instead of conditioning on a sequence of text tokens, we propose to usea set of per-object representations, Neural Assets, to control the 3D pose ofindividual objects in a scene. Neural Assets are obtained by pooling visualrepresentations of objects from a reference image, such as a frame in a video,and are trained to reconstruct the respective objects in a different image,e.g., a later frame in the video. Importantly, we encode object visuals fromthe reference image while conditioning on object poses from the target frame.This enables learning disentangled appearance and pose features. Combiningvisual and 3D pose representations in a sequence-of-tokens format allows us tokeep the text-to-image architecture of existing models, with Neural Assets inplace of text tokens. By fine-tuning a pre-trained text-to-image diffusionmodel with this information, our approach enables fine-grained 3D pose andplacement control of individual objects in a scene. We further demonstrate thatNeural Assets can be transferred and recomposed across different scenes. Ourmodel achieves state-of-the-art multi-object editing results on both synthetic3D scene datasets, as well as two real-world video datasets (Objectron, WaymoOpen).</description><author>Ziyi Wu, Yulia Rubanova, Rishabh Kabra, Drew A. Hudson, Igor Gilitschenski, Yusuf Aytar, Sjoerd van Steenkiste, Kelsey R. Allen, Thomas Kipf</author><pubDate>Thu, 13 Jun 2024 17:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09292v1</guid></item><item><title>A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening</title><link>http://arxiv.org/abs/2406.09291v1</link><description>Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity ofmessage-passing GNNs by representing graphs as sets of subgraphs. They haveshown impressive performance on several tasks, but their complexity limitsapplications to larger graphs. Previous approaches suggested processing onlysubsets of subgraphs, selected either randomly or via learnable sampling.However, they make suboptimal subgraph selections or can only cope with verysmall subset sizes, inevitably incurring performance degradation. This paperintroduces a new Subgraph GNNs framework to address these issues. We employ agraph coarsening function to cluster nodes into super-nodes with inducedconnectivity. The product between the coarsened and the original graph revealsan implicit structure whereby subgraphs are associated with specific sets ofnodes. By running generalized message-passing on such graph product, our methodeffectively implements an efficient, yet powerful Subgraph GNN. Controlling thecoarsening function enables meaningful selection of any number of subgraphswhile, contrary to previous methods, being fully compatible with standardtraining techniques. Notably, we discover that the resulting node featuretensor exhibits new, unexplored permutation symmetries. We leverage thisstructure, characterize the associated linear equivariant layers andincorporate them into the layers of our Subgraph GNN architecture. Extensiveexperiments on multiple graph learning benchmarks demonstrate that our methodis significantly more flexible than previous approaches, as it can seamlesslyhandle any number of subgraphs, while consistently outperforming baselineapproaches.</description><author>Guy Bar-Shalom, Yam Eitan, Fabrizio Frasca, Haggai Maron</author><pubDate>Thu, 13 Jun 2024 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09291v1</guid></item><item><title>Unlearning Traces the Influential Training Data of Language Models</title><link>http://arxiv.org/abs/2401.15241v2</link><description>Identifying the training datasets that influence a language model's outputsis essential for minimizing the generation of harmful content and enhancing itsperformance. Ideally, we can measure the influence of each dataset by removingit from training; however, it is prohibitively expensive to retrain a modelmultiple times. This paper presents UnTrac: unlearning traces the influence ofa training dataset on the model's performance. UnTrac is extremely simple; eachtraining dataset is unlearned by gradient ascent, and we evaluate how much themodel's predictions change after unlearning. Furthermore, we propose a morescalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates theunlearned model on training datasets. UnTrac-Inv resembles UnTrac, while beingefficient for massive training datasets. In the experiments, we examine if ourmethods can assess the influence of pretraining datasets on generating toxic,biased, and untruthful content. Our methods estimate their influence much moreaccurately than existing methods while requiring neither excessive memory spacenor multiple checkpoints.</description><author>Masaru Isonuma, Ivan Titov</author><pubDate>Thu, 13 Jun 2024 17:28:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15241v2</guid></item></channel></rss>