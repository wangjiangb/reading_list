<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 01 Oct 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Learning to Transform for Generalizable Instance-wise Invariance</title><link>http://arxiv.org/abs/2309.16672v1</link><description>Computer vision research has long aimed to build systems that are robust tospatial transformations found in natural data. Traditionally, this is doneusing data augmentation or hard-coding invariances into the architecture.However, too much or too little invariance can hurt, and the correct amount isunknown a priori and dependent on the instance. Ideally, the appropriateinvariance would be learned from data and inferred at test-time. We treat invariance as a prediction problem. Given any image, we use anormalizing flow to predict a distribution over transformations and average thepredictions over them. Since this distribution only depends on the instance, wecan align instances before classifying them and generalize invariance acrossclasses. The same distribution can also be used to adapt to out-of-distributionposes. This normalizing flow is trained end-to-end and can learn a much largerrange of transformations than Augerino and InstaAug. When used as dataaugmentation, our method shows accuracy and robustness gains on CIFAR 10,CIFAR10-LT, and TinyImageNet.</description><author>Utkarsh Singhal, Carlos Esteves, Ameesh Makadia, Stella X. Yu</author><pubDate>Thu, 28 Sep 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16672v1</guid></item><item><title>Demystifying CLIP Data</title><link>http://arxiv.org/abs/2309.16671v1</link><description>Contrastive Language-Image Pre-training (CLIP) is an approach that hasadvanced research and applications in computer vision, fueling modernrecognition systems and generative models. We believe that the main ingredientto the success of CLIP is its data and not the model architecture orpre-training objective. However, CLIP only provides very limited informationabout its data and how it has been collected, leading to works that aim toreproduce CLIP's data by filtering with its model parameters. In this work, weintend to reveal CLIP's data curation approach and in our pursuit of making itopen to the community introduce Metadata-Curated Language-Image Pre-training(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP'sconcepts) and yields a balanced subset over the metadata distribution. Ourexperimental study rigorously isolates the model and training settings,concentrating solely on data. MetaCLIP applied to CommonCrawl with 400Mimage-text data pairs outperforms CLIP's data on multiple standard benchmarks.In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintainingthe same training budget, attains 72.4%. Our observations hold across variousmodel sizes, exemplified by ViT-H achieving 80.5%, without anybells-and-whistles. Curation code and training data distribution on metadata ismade available at https://github.com/facebookresearch/MetaCLIP.</description><author>Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer</author><pubDate>Thu, 28 Sep 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16671v1</guid></item><item><title>Decaf: Monocular Deformation Capture for Face and Hand Interactions</title><link>http://arxiv.org/abs/2309.16670v1</link><description>Existing methods for 3D tracking from monocular RGB videos predominantlyconsider articulated and rigid objects. Modelling dense non-rigid objectdeformations in this setting remained largely unaddressed so far, although sucheffects can improve the realism of the downstream applications such as AR/VRand avatar communications. This is due to the severe ill-posedness of themonocular view setting and the associated challenges. While it is possible tonaively track multiple non-rigid objects independently using 3D templates orparametric 3D models, such an approach would suffer from multiple artefacts inthe resulting 3D estimates such as depth ambiguity, unnatural intra-objectcollisions and missing or implausible deformations. Hence, this paperintroduces the first method that addresses the fundamental challenges depictedabove and that allows tracking human hands interacting with human faces in 3Dfrom single monocular RGB videos. We model hands as articulated objectsinducing non-rigid face deformations during an active interaction. Our methodrelies on a new hand-face motion and interaction capture dataset with realisticface deformations acquired with a markerless multi-view camera system. As apivotal step in its creation, we process the reconstructed raw 3D shapes withposition-based dynamics and an approach for non-uniform stiffness estimation ofthe head tissues, which results in plausible annotations of the surfacedeformations, hand-face contact regions and head-hand positions. At the core ofour neural approach are a variational auto-encoder supplying the hand-facedepth prior and modules that guide the 3D tracking by estimating the contactsand the deformations. Our final 3D hand and face reconstructions are realisticand more plausible compared to several baselines applicable in our setting,both quantitatively and qualitatively.https://vcai.mpi-inf.mpg.de/projects/Decaf</description><author>Soshi Shimada, Vladislav Golyanik, Patrick Pérez, Christian Theobalt</author><pubDate>Thu, 28 Sep 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16670v1</guid></item><item><title>Training a Large Video Model on a Single Machine in a Day</title><link>http://arxiv.org/abs/2309.16669v1</link><description>Videos are big, complex to pre-process, and slow to train on.State-of-the-art large-scale video models are trained on clusters of 32 or moreGPUs for several days. As a consequence, academia largely ceded the training oflarge video models to industry. In this paper, we show how to still train astate-of-the-art video model on a single machine with eight consumer-grade GPUsin a day. We identify three bottlenecks, IO, CPU, and GPU computation, andoptimize each. The result is a highly efficient video training pipeline. Forcomparable architectures, our pipeline achieves higher accuracies with$\frac{1}{8}$ of the computation compared to prior work. Code is available athttps://github.com/zhaoyue-zephyrus/AVION.</description><author>Yue Zhao, Philipp Krähenbühl</author><pubDate>Thu, 28 Sep 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16669v1</guid></item><item><title>RealFill: Reference-Driven Generation for Authentic Image Completion</title><link>http://arxiv.org/abs/2309.16668v1</link><description>Recent advances in generative imagery have brought forth outpainting andinpainting models that can produce high-quality, plausible image content inunknown regions, but the content these models hallucinate is necessarilyinauthentic, since the models lack sufficient context about the true scene. Inthis work, we propose RealFill, a novel generative approach for imagecompletion that fills in missing regions of an image with the content thatshould have been there. RealFill is a generative inpainting model that ispersonalized using only a few reference images of a scene. These referenceimages do not have to be aligned with the target image, and can be taken withdrastically varying viewpoints, lighting conditions, camera apertures, or imagestyles. Once personalized, RealFill is able to complete a target image withvisually compelling contents that are faithful to the original scene. Weevaluate RealFill on a new image completion benchmark that covers a set ofdiverse and challenging scenarios, and find that it outperforms existingapproaches by a large margin. See more results on our project page:https://realfill.github.io</description><author>Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, Michael Rubinstein</author><pubDate>Thu, 28 Sep 2023 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16668v1</guid></item><item><title>HyperPPO: A scalable method for finding small policies for robotic control</title><link>http://arxiv.org/abs/2309.16663v1</link><description>Models with fewer parameters are necessary for the neural control ofmemory-limited, performant robots. Finding these smaller neural networkarchitectures can be time-consuming. We propose HyperPPO, an on-policyreinforcement learning algorithm that utilizes graph hypernetworks to estimatethe weights of multiple neural architectures simultaneously. Our methodestimates weights for networks that are much smaller than those in common-usenetworks yet encode highly performant policies. We obtain multiple trainedpolicies at the same time while maintaining sample efficiency and provide theuser the choice of picking a network architecture that satisfies theircomputational constraints. We show that our method scales well - more trainingresources produce faster convergence to higher-performing architectures. Wedemonstrate that the neural policies estimated by HyperPPO are capable ofdecentralized control of a Crazyflie2.1 quadrotor. Website:https://sites.google.com/usc.edu/hyperppo</description><author>Shashank Hegde, Zhehui Huang, Gaurav S. Sukhatme</author><pubDate>Thu, 28 Sep 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16663v1</guid></item><item><title>Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation</title><link>http://arxiv.org/abs/2309.16662v1</link><description>Women are at higher risk of Alzheimer's and other neurological diseases aftermenopause, and yet research connecting female brain health to sex hormonefluctuations is limited. We seek to investigate this connection by developingtools that quantify 3D shape changes that occur in the brain during sex hormonefluctuations. Geodesic regression on the space of 3D discrete surfaces offers aprincipled way to characterize the evolution of a brain's shape. However, inits current form, this approach is too computationally expensive for practicaluse. In this paper, we propose approximation schemes that accelerate geodesicregression on shape spaces of 3D discrete surfaces. We also provide rules ofthumb for when each approximation can be used. We test our approach onsynthetic data to quantify the speed-accuracy trade-off of these approximationsand show that practitioners can expect very significant speed-up while onlysacrificing little accuracy. Finally, we apply the method to real brain shapedata and produce the first characterization of how the female hippocampuschanges shape during the menstrual cycle as a function of progesterone: acharacterization made (practically) possible by our approximation schemes. Ourwork paves the way for comprehensive, practical shape analyses in the fields ofbio-medicine and computer vision. Our implementation is publicly available onGitHub: https://github.com/bioshape-lab/my28brains.</description><author>Adele Myers, Caitlin Taylor, Emily Jacobs, Nina Miolane</author><pubDate>Thu, 28 Sep 2023 18:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16662v1</guid></item><item><title>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation</title><link>http://arxiv.org/abs/2309.16661v1</link><description>Microscopic image segmentation is a challenging task, wherein the objectiveis to assign semantic labels to each pixel in a given microscopic image. Whileconvolutional neural networks (CNNs) form the foundation of many existingframeworks, they often struggle to explicitly capture long-range dependencies.Although transformers were initially devised to address this issue usingself-attention, it has been proven that both local and global features arecrucial for addressing diverse challenges in microscopic images, includingvariations in shape, size, appearance, and target region density. In thispaper, we introduce SA2-Net, an attention-guided method that leveragesmulti-scale feature learning to effectively handle diverse structures withinmicroscopic images. Specifically, we propose scale-aware attention (SA2) moduledesigned to capture inherent variations in scales and shapes of microscopicregions, such as cells, for accurate segmentation. This module incorporateslocal attention at each level of multi-stage features, as well as globalattention across multiple resolutions. Furthermore, we address the issue ofblurred region boundaries (e.g., cell boundaries) by introducing a novelupsampling strategy called the Adaptive Up-Attention (AuA) module. This moduleenhances the discriminative ability for improved localization of microscopicregions using an explicit attention mechanism. Extensive experiments on fivechallenging datasets demonstrate the benefits of our SA2-Net model. Our sourcecode is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.</description><author>Mustansar Fiaz, Moein Heidari, Rao Muhammad Anwer, Hisham Cholakkal</author><pubDate>Thu, 28 Sep 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16661v1</guid></item><item><title>Imbalanced Data Stream Classification using Dynamic Ensemble Selection</title><link>http://arxiv.org/abs/2309.09175v2</link><description>Modern streaming data categorization faces significant challenges fromconcept drift and class imbalanced data. This negatively impacts the output ofthe classifier, leading to improper classification. Furthermore, other factorssuch as the overlapping of multiple classes limit the extent of the correctnessof the output. This work proposes a novel framework for integrating datapre-processing and dynamic ensemble selection, by formulating theclassification framework for the nonstationary drifting imbalanced data stream,which employs the data pre-processing and dynamic ensemble selectiontechniques. The proposed framework was evaluated using six artificiallygenerated data streams with differing imbalance ratios in combination with twodifferent types of concept drifts. Each stream is composed of 200 chunks of 500objects described by eight features and contains five concept drifts. Sevenpre-processing techniques and two dynamic ensemble selection methods wereconsidered. According to experimental results, data pre-processing combinedwith Dynamic Ensemble Selection techniques significantly delivers more accuracywhen dealing with imbalanced data streams.</description><author>Priya. S, Haribharathi Sivakumar, Vijay Arvind. R</author><pubDate>Thu, 28 Sep 2023 18:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09175v2</guid></item><item><title>Visual In-Context Learning for Few-Shot Eczema Segmentation</title><link>http://arxiv.org/abs/2309.16656v1</link><description>Automated diagnosis of eczema from digital camera images is crucial fordeveloping applications that allow patients to self-monitor their recovery. Animportant component of this is the segmentation of eczema region from suchimages. Current methods for eczema segmentation rely on deep neural networkssuch as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. Whileeffective, these methods require high volume of annotated data, which can bedifficult to obtain. Here, we investigate the capabilities of visual in-contextlearning that can perform few-shot eczema segmentation with just a handful ofexamples and without any need for retraining models. Specifically, we propose astrategy for applying in-context learning for eczema segmentation with ageneralist vision model called SegGPT. When benchmarked on a dataset ofannotated eczema images, we show that SegGPT with just 2 representative exampleimages from the training dataset performs better (mIoU: 36.69) than a CNN U-Nettrained on 428 images (mIoU: 32.60). We also discover that using more number ofexamples for SegGPT may in fact be harmful to its performance. Our resulthighlights the importance of visual in-context learning in developing fasterand better solutions to skin imaging tasks. Our result also paves the way fordeveloping inclusive solutions that can cater to minorities in the demographicswho are typically heavily under-represented in the training data.</description><author>Neelesh Kumar, Oya Aran, Venugopal Vasudevan</author><pubDate>Thu, 28 Sep 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16656v1</guid></item><item><title>Novel Deep Learning Pipeline for Automatic Weapon Detection</title><link>http://arxiv.org/abs/2309.16654v1</link><description>Weapon and gun violence have recently become a pressing issue today. Thedegree of these crimes and activities has risen to the point of being termed asan epidemic. This prevalent misuse of weapons calls for an automatic systemthat detects weapons in real-time. Real-time surveillance video is captured andrecorded in almost all public forums and places. These videos contain abundantraw data which can be extracted and processed into meaningful information. Thispaper proposes a novel pipeline consisting of an ensemble of convolutionalneural networks with distinct architectures. Each neural network is trainedwith a unique mini-batch with little to no overlap in the training samples.This paper will present several promising results using multiple datasetsassociated with comparing the proposed architecture and state-of-the-art (SoA)models. The proposed pipeline produced an average increase of 5% in accuracy,specificity, and recall compared to the SoA systems.</description><author>Haribharathi Sivakumar, Vijay Arvind. R, Pawan Ragavendhar V, G. Balamurugan</author><pubDate>Thu, 28 Sep 2023 18:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16654v1</guid></item><item><title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title><link>http://arxiv.org/abs/2309.16653v1</link><description>Recent advances in 3D content creation mostly leverage optimization-based 3Dgeneration via score distillation sampling (SDS). Though promising results havebeen exhibited, these methods often suffer from slow per-sample optimization,limiting their practical usage. In this paper, we propose DreamGaussian, anovel 3D content generation framework that achieves both efficiency and qualitysimultaneously. Our key insight is to design a generative 3D Gaussian Splattingmodel with companioned mesh extraction and texture refinement in UV space. Incontrast to the occupancy pruning used in Neural Radiance Fields, wedemonstrate that the progressive densification of 3D Gaussians convergessignificantly faster for 3D generative tasks. To further enhance the texturequality and facilitate downstream applications, we introduce an efficientalgorithm to convert 3D Gaussians into textured meshes and apply a fine-tuningstage to refine the details. Extensive experiments demonstrate the superiorefficiency and competitive generation quality of our proposed approach.Notably, DreamGaussian produces high-quality textured meshes in just 2 minutesfrom a single-view image, achieving approximately 10 times accelerationcompared to existing methods.</description><author>Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng</author><pubDate>Thu, 28 Sep 2023 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16653v1</guid></item><item><title>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</title><link>http://arxiv.org/abs/2309.16650v1</link><description>For robots to perform a wide variety of tasks, they require a 3Drepresentation of the world that is semantically rich, yet compact andefficient for task-driven perception and planning. Recent approaches haveattempted to leverage features from large vision-language models to encodesemantics in 3D representations. However, these approaches tend to produce mapswith per-point feature vectors, which do not scale well in larger environments,nor do they contain semantic spatial relationships between entities in theenvironment, which are useful for downstream planning. In this work, we proposeConceptGraphs, an open-vocabulary graph-structured representation for 3Dscenes. ConceptGraphs is built by leveraging 2D foundation models and fusingtheir output to 3D by multi-view association. The resulting representationsgeneralize to novel semantic classes, without the need to collect large 3Ddatasets or finetune models. We demonstrate the utility of this representationthrough a number of downstream planning tasks that are specified throughabstract (language) prompts and require complex reasoning over spatial andsemantic concepts. (Project page: https://concept-graphs.github.io/ Explainervideo: https://youtu.be/mRhNkQwRYnc )</description><author>Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull</author><pubDate>Thu, 28 Sep 2023 18:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16650v1</guid></item><item><title>FLIP: Cross-domain Face Anti-spoofing with Language Guidance</title><link>http://arxiv.org/abs/2309.16649v1</link><description>Face anti-spoofing (FAS) or presentation attack detection is an essentialcomponent of face recognition systems deployed in security-criticalapplications. Existing FAS methods have poor generalizability to unseen spooftypes, camera sensors, and environmental conditions. Recently, visiontransformer (ViT) models have been shown to be effective for the FAS task dueto their ability to capture long-range dependencies among image patches.However, adaptive modules or auxiliary loss functions are often required toadapt pre-trained ViT weights learned on large-scale datasets such as ImageNet.In this work, we first show that initializing ViTs with multimodal (e.g., CLIP)pre-trained weights improves generalizability for the FAS task, which is inline with the zero-shot transfer capabilities of vision-language pre-trained(VLP) models. We then propose a novel approach for robust cross-domain FAS bygrounding visual representations with the help of natural language.Specifically, we show that aligning the image representation with an ensembleof class descriptions (based on natural language semantics) improves FASgeneralizability in low-data regimes. Finally, we propose a multimodalcontrastive learning strategy to boost feature generalization further andbridge the gap between source and target domains. Extensive experiments onthree standard protocols demonstrate that our method significantly outperformsthe state-of-the-art methods, achieving better zero-shot transfer performancethan five-shot transfer of adaptive ViTs. Code:https://github.com/koushiksrivats/FLIP</description><author>Koushik Srivatsan, Muzammal Naseer, Karthik Nandakumar</author><pubDate>Thu, 28 Sep 2023 18:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16649v1</guid></item><item><title>Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors</title><link>http://arxiv.org/abs/2309.16646v1</link><description>Dense depth and surface normal predictors should possess the equivariantproperty to cropping-and-resizing -- cropping the input image should result incropping the same output image. However, we find that state-of-the-art depthand normal predictors, despite having strong performances, surprisingly do notrespect equivariance. The problem exists even when crop-and-resize dataaugmentation is employed during training. To remedy this, we propose anequivariant regularization technique, consisting of an averaging procedure anda self-consistency loss, to explicitly promote cropping-and-resizingequivariance in depth and normal networks. Our approach can be applied to bothCNN and Transformer architectures, does not incur extra cost during testing,and notably improves the supervised and semi-supervised learning performance ofdense predictors on Taskonomy tasks. Finally, finetuning with our loss onunlabeled images improves not only equivariance but also accuracy ofstate-of-the-art depth and normal predictors when evaluated on NYU-v2. GitHublink: https://github.com/mikuhatsune/equivariance</description><author>Yuanyi Zhong, Anand Bhattad, Yu-Xiong Wang, David Forsyth</author><pubDate>Thu, 28 Sep 2023 18:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16646v1</guid></item><item><title>Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures</title><link>http://arxiv.org/abs/2309.16645v1</link><description>In, Elmarakeby et al., "Biologically informed deep neural network forprostate cancer discovery", a feedforward neural network with biologicallyinformed, sparse connections (P-NET) was presented to model the state ofprostate cancer. We verified the reproducibility of the study conducted byElmarakeby et al., using both their original codebase, and our ownre-implementation using more up-to-date libraries. We quantified thecontribution of network sparsification by Reactome biological pathways, andconfirmed its importance to P-NET's superior performance. Furthermore, weexplored alternative neural architectures and approaches to incorporatingbiological information into the networks. We experimented with three types ofgraph neural networks on the same training data, and investigated the clinicalprediction agreement between different models. Our analyses demonstrated thatdeep neural networks with distinct architectures make incorrect predictions forindividual patient that are persistent across different initializations of aspecific neural architecture. This suggests that different neural architecturesare sensitive to different aspects of the data, an important yet under-exploredchallenge for clinical prediction tasks.</description><author>Christian Pedersen, Tiberiu Tesileanu, Tinghui Wu, Siavash Golkar, Miles Cranmer, Zijun Zhang, Shirley Ho</author><pubDate>Thu, 28 Sep 2023 18:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16645v1</guid></item><item><title>Deep Geometrized Cartoon Line Inbetweening</title><link>http://arxiv.org/abs/2309.16643v1</link><description>We aim to address a significant but understudied problem in the animeindustry, namely the inbetweening of cartoon line drawings. Inbetweeninginvolves generating intermediate frames between two black-and-white linedrawings and is a time-consuming and expensive process that can benefit fromautomation. However, existing frame interpolation methods that rely on matchingand warping whole raster images are unsuitable for line inbetweening and oftenproduce blurring artifacts that damage the intricate line structures. Topreserve the precision and detail of the line drawings, we propose a newapproach, AnimeInbet, which geometrizes raster line drawings into graphs ofendpoints and reframes the inbetweening task as a graph fusion problem withvertex repositioning. Our method can effectively capture the sparsity andunique structure of line drawings while preserving the details duringinbetweening. This is made possible via our novel modules, i.e., vertexgeometric embedding, a vertex correspondence Transformer, an effectivemechanism for vertex repositioning and a visibility predictor. To train ourmethod, we introduce MixamoLine240, a new dataset of line drawings with groundtruth vectorization and matching labels. Our experiments demonstrate thatAnimeInbet synthesizes high-quality, clean, and complete intermediate linedrawings, outperforming existing methods quantitatively and qualitatively,especially in cases with large motions. Data and code are available athttps://github.com/lisiyao21/AnimeInbet.</description><author>Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, Chen Change Loy</author><pubDate>Thu, 28 Sep 2023 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16643v1</guid></item><item><title>MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention</title><link>http://arxiv.org/abs/2309.16639v1</link><description>Problematic smartphone use negatively affects physical and mental health.Despite the wide range of prior research, existing persuasive techniques arenot flexible enough to provide dynamic persuasion content based on users'physical contexts and mental states. We first conduct a Wizard-of-Oz study(N=12) and an interview study (N=10) to summarize the mental states behindproblematic smartphone use: boredom, stress, and inertia. This informs ourdesign of four persuasion strategies: understanding, comforting, evoking, andscaffolding habits. We leverage large language models (LLMs) to enable theautomatic and dynamic generation of effective persuasion content. We developMindShift, a novel LLM-powered problematic smartphone use interventiontechnique. MindShift takes users' in-the-moment physical contexts, mentalstates, app usage behaviors, users' goals &amp; habits as input, and generateshigh-quality and flexible persuasive content with appropriate persuasionstrategies. We conduct a 5-week field experiment (N=25) to compare MindShiftwith baseline techniques. The results show that MindShift significantlyimproves intervention acceptance rates by 17.8-22.5% and reduces smartphone usefrequency by 12.1-14.4%. Moreover, users have a significant drop in smartphoneaddiction scale scores and a rise in self-efficacy. Our study sheds light onthe potential of leveraging LLMs for context-aware persuasion in other behaviorchange domains.</description><author>Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu, Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi</author><pubDate>Thu, 28 Sep 2023 18:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16639v1</guid></item><item><title>Learning Interpretable Characteristic Kernels via Decision Forests</title><link>http://arxiv.org/abs/1812.00029v3</link><description>Decision forests are widely used for classification and regression tasks. Alesser known property of tree-based methods is that one can construct aproximity matrix from the tree(s), and these proximity matrices are inducedkernels. While there has been extensive research on the applications andproperties of kernels, there is relatively little research on kernels inducedby decision forests. We construct Kernel Mean Embedding Random Forests (KMERF),which induce kernels from random trees and/or forests using leaf-nodeproximity. We introduce the notion of an asymptotically characteristic kernel,and prove that KMERF kernels are asymptotically characteristic for bothdiscrete and continuous data. Because KMERF is data-adaptive, we suspected itwould outperform kernels selected a priori on finite sample data. We illustratethat KMERF nearly dominates current state-of-the-art kernel-based tests acrossa diverse range of high-dimensional two-sample and independence testingsettings. Furthermore, our forest-based approach is interpretable, and providesfeature importance metrics that readily distinguish important dimensions,unlike other high-dimensional non-parametric testing procedures. Hence, thiswork demonstrates the decision forest-based kernel can be more powerful andmore interpretable than existing methods, flying in the face of conventionalwisdom of the trade-off between the two.</description><author>Sambit Panda, Cencheng Shen, Joshua T. Vogelstein</author><pubDate>Thu, 28 Sep 2023 18:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1812.00029v3</guid></item><item><title>Data Augmentation in the Underparameterized and Overparameterized Regimes</title><link>http://arxiv.org/abs/2202.09134v3</link><description>We provide results that exactly quantify how data augmentation affects thevariance and limiting distribution of estimates, and analyze several specificmodels in detail. The results confirm some observations made in machinelearning practice, but also lead to unexpected findings: Data augmentation mayincrease rather than decrease the uncertainty of estimates, such as theempirical prediction risk. It can act as a regularizer, but fails to do so incertain high-dimensional problems, and it may shift the double-descent peak ofan empirical risk. Overall, the analysis shows that several properties dataaugmentation has been attributed with are not either true or false, but ratherdepend on a combination of factors -- notably the data distribution, theproperties of the estimator, and the interplay of sample size, number ofaugmentations, and dimension. Our main theoretical tool is a limit theorem forfunctions of randomly transformed, high-dimensional random vectors. The proofdraws on work in probability on noise stability of functions of many variables.</description><author>Kevin Han Huang, Peter Orbanz, Morgane Austern</author><pubDate>Thu, 28 Sep 2023 18:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.09134v3</guid></item><item><title>End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon</title><link>http://arxiv.org/abs/2309.16634v1</link><description>Most recent work in goal oriented visual navigation resorts to large-scalemachine learning in simulated environments. The main challenge lies in learningcompact representations generalizable to unseen environments and in learninghigh-capacity perception modules capable of reasoning on high-dimensionalinput. The latter is particularly difficult when the goal is not given as acategory ("ObjectNav") but as an exemplar image ("ImageNav"), as the perceptionmodule needs to learn a comparison strategy requiring to solve an underlyingvisual correspondence problem. This has been shown to be difficult from rewardalone or with standard auxiliary tasks. We address this problem through asequence of two pretext tasks, which serve as a prior for what we argue is oneof the main bottleneck in perception, extremely wide-baseline relative poseestimation and visibility prediction in complex scenes. The first pretext task,cross-view completion is a proxy for the underlying visual correspondenceproblem, while the second task addresses goal detection and finding directly.We propose a new dual encoder with a large-capacity binocular ViT model andshow that correspondence solutions naturally emerge from the training signals.Experiments show significant improvements and SOTA performance on the twobenchmarks, ImageNav and the Instance-ImageNav variant, where camera intrinsicsand height differ between observation and goal.</description><author>Guillaume Bono, Leonid Antsfeld, Boris Chidlovskii, Philippe Weinzaepfel, Christian Wolf</author><pubDate>Thu, 28 Sep 2023 18:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16634v1</guid></item><item><title>Mixup Your Own Pairs</title><link>http://arxiv.org/abs/2309.16633v1</link><description>In representation learning, regression has traditionally received lessattention than classification. Directly applying representation learningtechniques designed for classification to regression often results infragmented representations in the latent space, yielding sub-optimalperformance. In this paper, we argue that the potential of contrastive learningfor regression has been overshadowed due to the neglect of two crucial aspects:ordinality-awareness and hardness. To address these challenges, we advocate"mixup your own contrastive pairs for supervised contrastive regression",instead of relying solely on real/augmented samples. Specifically, we proposeSupervised Contrastive Learning for Regression with Mixup (SupReMix). It takesanchor-inclusive mixtures (mixup of the anchor and a distinct negative sample)as hard negative pairs and anchor-exclusive mixtures (mixup of two distinctnegative samples) as hard positive pairs at the embedding level. This strategyformulates harder contrastive pairs by integrating richer ordinal information.Through extensive experiments on six regression datasets including 2D images,volumetric images, text, tabular data, and time-series signals, coupled withtheoretical analysis, we demonstrate that SupReMix pre-training fosterscontinuous ordered representations of regression data, resulting in significantimprovement in regression performance. Furthermore, SupReMix is superior toother approaches in a range of regression challenges including transferlearning, imbalanced training data, and scenarios with fewer training samples.</description><author>Yilei Wu, Zijian Dong, Chongyao Chen, Wangchunshu Zhou, Juan Helen Zhou</author><pubDate>Thu, 28 Sep 2023 18:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16633v1</guid></item><item><title>Robust Offline Reinforcement Learning -- Certify the Confidence Interval</title><link>http://arxiv.org/abs/2309.16631v1</link><description>Currently, reinforcement learning (RL), especially deep RL, has received moreand more attention in the research area. However, the security of RL has beenan obvious problem due to the attack manners becoming mature. In order todefend against such adversarial attacks, several practical approaches aredeveloped, such as adversarial training, data filtering, etc. However, thesemethods are mostly based on empirical algorithms and experiments, withoutrigorous theoretical analysis of the robustness of the algorithms. In thispaper, we develop an algorithm to certify the robustness of a given policyoffline with random smoothing, which could be proven and conducted asefficiently as ones without random smoothing. Experiments on differentenvironments confirm the correctness of our algorithm.</description><author>Jiarui Yao, Simon Shaolei Du</author><pubDate>Thu, 28 Sep 2023 18:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16631v1</guid></item><item><title>On Learning with LAD</title><link>http://arxiv.org/abs/2309.16630v1</link><description>The logical analysis of data, LAD, is a technique that yields two-classclassifiers based on Boolean functions having disjunctive normal form (DNF)representation. Although LAD algorithms employ optimization techniques, theresulting binary classifiers or binary rules do not lead to overfitting. Wepropose a theoretical justification for the absence of overfitting byestimating the Vapnik-Chervonenkis dimension (VC dimension) for LAD modelswhere hypothesis sets consist of DNFs with a small number of cubic monomials.We illustrate and confirm our observations empirically.</description><author>C. A. Jothishwaran, Biplav Srivastava, Jitin Singla, Sugata Gangopadhyay</author><pubDate>Thu, 28 Sep 2023 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16630v1</guid></item><item><title>HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities</title><link>http://arxiv.org/abs/2211.10754v4</link><description>Event cameras detect changes in per-pixel intensity to generate asynchronous`event streams'. They offer great potential for accurate semantic map retrievalin real-time autonomous systems owing to their much higher temporal resolutionand high dynamic range (HDR) compared to conventional cameras. However,existing implementations for event-based segmentation suffer from sub-optimalperformance since these temporally dense events only measure the varyingcomponent of a visual signal, limiting their ability to encode dense spatialcontext compared to frames. To address this issue, we propose a hybridend-to-end learning framework HALSIE, utilizing three key concepts to reduceinference cost by up to $20\times$ versus prior art while retaining similarperformance: First, a simple and efficient cross-domain learning scheme toextract complementary spatio-temporal embeddings from both frames and events.Second, a specially designed dual-encoder scheme with Spiking Neural Network(SNN) and Artificial Neural Network (ANN) branches to minimize latency whileretaining cross-domain feature aggregation. Third, a multi-scale cue mixer tomodel rich representations of the fused embeddings. These qualities of HALSIEallow for a very lightweight architecture achieving state-of-the-artsegmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with upto $33\times$ higher parameter efficiency and favorable inference cost (17.9mJper cycle). Our ablation study also brings new insights into effective designchoices that can prove beneficial for research across other vision tasks.</description><author>Shristi Das Biswas, Adarsh Kosta, Chamika Liyanagedera, Marco Apolinario, Kaushik Roy</author><pubDate>Thu, 28 Sep 2023 18:35:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10754v4</guid></item><item><title>Alignment-free HDR Deghosting with Semantics Consistent Transformer</title><link>http://arxiv.org/abs/2305.18135v2</link><description>High dynamic range (HDR) imaging aims to retrieve information from multiplelow-dynamic range inputs to generate realistic output. The essence is toleverage the contextual information, including both dynamic and staticsemantics, for better image generation. Existing methods often focus on thespatial misalignment across input frames caused by the foreground and/or cameramotion. However, there is no research on jointly leveraging the dynamic andstatic context in a simultaneous manner. To delve into this problem, we proposea novel alignment-free network with a Semantics Consistent Transformer (SCTNet)with both spatial and channel attention modules in the network. The spatialattention aims to deal with the intra-image correlation to model the dynamicmotion, while the channel attention enables the inter-image intertwining toenhance the semantic consistency across frames. Aside from this, we introduce anovel realistic HDR dataset with more variations in foreground objects,environmental factors, and larger motions. Extensive comparisons on bothconventional datasets and ours validate the effectiveness of our method,achieving the best trade-off on the performance and the computational cost.</description><author>Steven Tel, Zongwei Wu, Yulun Zhang, Barthélémy Heyrman, Cédric Demonceaux, Radu Timofte, Dominique Ginhac</author><pubDate>Thu, 28 Sep 2023 18:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18135v2</guid></item><item><title>Class Activation Map-based Weakly supervised Hemorrhage Segmentation using Resnet-LSTM in Non-Contrast Computed Tomography images</title><link>http://arxiv.org/abs/2309.16627v1</link><description>In clinical settings, intracranial hemorrhages (ICH) are routinely diagnosedusing non-contrast CT (NCCT) for severity assessment. Accurate automatedsegmentation of ICH lesions is the initial and essential step, immensely usefulfor such assessment. However, compared to other structural imaging modalitiessuch as MRI, in NCCT images ICH appears with very low contrast and poor SNR.Over recent years, deep learning (DL)-based methods have shown great potential,however, training them requires a huge amount of manually annotatedlesion-level labels, with sufficient diversity to capture the characteristicsof ICH. In this work, we propose a novel weakly supervised DL method for ICHsegmentation on NCCT scans, using image-level binary classification labels,which are less time-consuming and labor-efficient when compared to the manuallabeling of individual ICH lesions. Our method initially determines theapproximate location of ICH using class activation maps from a classificationnetwork, which is trained to learn dependencies across contiguous slices. Wefurther refine the ICH segmentation using pseudo-ICH masks obtained in anunsupervised manner. The method is flexible and uses a computationally lightarchitecture during testing. On evaluating our method on the validation data ofthe MICCAI 2022 INSTANCE challenge, our method achieves a Dice value of 0.55,comparable with those of existing weakly supervised method (Dice value of0.47), despite training on a much smaller training data.</description><author>Shreyas H Ramananda, Vaanathi Sundaresan</author><pubDate>Thu, 28 Sep 2023 18:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16627v1</guid></item><item><title>Jointly Training Large Autoregressive Multimodal Models</title><link>http://arxiv.org/abs/2309.15564v2</link><description>In recent years, advances in the large-scale pretraining of language andtext-to-image models have revolutionized the field of machine learning. Yet,integrating these two modalities into a single, robust model capable ofgenerating seamless multimodal outputs remains a significant challenge. Toaddress this gap, we present the Joint Autoregressive Mixture (JAM) framework,a modular approach that systematically fuses existing text and image generationmodels. We also introduce a specialized, data-efficient instruction-tuningstrategy, tailored for mixed-modal generation tasks. Our final instruct-tunedmodel demonstrates unparalleled performance in generating high-qualitymultimodal outputs and represents the first model explicitly designed for thispurpose.</description><author>Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz</author><pubDate>Thu, 28 Sep 2023 18:23:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15564v2</guid></item><item><title>Stress Testing Chain-of-Thought Prompting for Large Language Models</title><link>http://arxiv.org/abs/2309.16621v1</link><description>This report examines the effectiveness of Chain-of-Thought (CoT) prompting inimproving the multi-step reasoning abilities of large language models (LLMs).Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze theimpact of three types of CoT prompt perturbations, namely CoT order, CoTvalues, and CoT operators on the performance of GPT-3 on various tasks. Ourfindings show that incorrect CoT prompting leads to poor performance onaccuracy metrics. Correct values in the CoT is crucial for predicting correctanswers. Moreover, incorrect demonstrations, where the CoT operators or the CoTorder are wrong, do not affect the performance as drastically when compared tothe value based perturbations. This research deepens our understanding of CoTprompting and opens some new questions regarding the capability of LLMs tolearn reasoning in context.</description><author>Aayush Mishra, Karan Thakkar</author><pubDate>Thu, 28 Sep 2023 18:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16621v1</guid></item><item><title>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit</title><link>http://arxiv.org/abs/2309.16620v1</link><description>The cost of hyperparameter tuning in deep learning has been rising with modelsizes, prompting practitioners to find new tuning methods using a proxy ofsmaller networks. One such proposal uses $\mu$P parameterized networks, wherethe optimal hyperparameters for small width networks transfer to networks witharbitrarily large width. However, in this scheme, hyperparameters do nottransfer across depths. As a remedy, we study residual networks with a residualbranch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$Pparameterization. We provide experiments demonstrating that residualarchitectures including convolutional ResNets and Vision Transformers trainedwith this parameterization exhibit transfer of optimal hyperparameters acrosswidth and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findingsare supported and motivated by theory. Using recent developments in thedynamical mean field theory (DMFT) description of neural network learningdynamics, we show that this parameterization of ResNets admits a well-definedfeature learning joint infinite-width and infinite-depth limit and showconvergence of finite-size network dynamics towards this limit.</description><author>Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, Cengiz Pehlevan</author><pubDate>Thu, 28 Sep 2023 18:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16620v1</guid></item><item><title>Adaptation of the super resolution SOTA for Art Restoration in camera capture images</title><link>http://arxiv.org/abs/2309.13655v3</link><description>Preserving cultural heritage is of paramount importance. In the domain of artrestoration, developing a computer vision model capable of effectivelyrestoring deteriorated images of art pieces was difficult, but now we have agood computer vision state-of-art. Traditional restoration methods are oftentime-consuming and require extensive expertise. The aim of this work is todesign an automated solution based on computer vision models that can enhanceand reconstruct degraded artworks, improving their visual quality whilepreserving their original characteristics and artifacts. The model shouldhandle a diverse range of deterioration types, including but not limited tonoise, blur, scratches, fading, and other common forms of degradation. We adaptthe current state-of-art for the image super-resolution based on the DiffusionModel (DM) and fine-tune it for Image art restoration. Our results show thatinstead of fine-tunning multiple different models for different kinds ofdegradation, fine-tuning one super-resolution. We train it on multiple datasetsto make it robust. code link: https://github.com/Naagar/art_restoration_DM</description><author>Sandeep Nagar, Abhinaba Bala, Sai Amrit Patnaik</author><pubDate>Thu, 28 Sep 2023 18:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13655v3</guid></item><item><title>Revisiting Neural Program Smoothing for Fuzzing</title><link>http://arxiv.org/abs/2309.16618v1</link><description>Testing with randomly generated inputs (fuzzing) has gained significanttraction due to its capacity to expose program vulnerabilities automatically.Fuzz testing campaigns generate large amounts of data, making them ideal forthe application of machine learning (ML). Neural program smoothing (NPS), aspecific family of ML-guided fuzzers, aims to use a neural network as a smoothapproximation of the program target for new test case generation. In this paper, we conduct the most extensive evaluation of NPS fuzzersagainst standard gray-box fuzzers (&gt;11 CPU years and &gt;5.5 GPU years), and makethe following contributions: (1) We find that the original performance claimsfor NPS fuzzers do not hold; a gap we relate to fundamental, implementation,and experimental limitations of prior works. (2) We contribute the firstin-depth analysis of the contribution of machine learning and gradient-basedmutations in NPS. (3) We implement Neuzz++, which shows that addressing thepractical limitations of NPS fuzzers improves performance, but that standardgray-box fuzzers almost always surpass NPS-based fuzzers. (4) As a consequence,we propose new guidelines targeted at benchmarking fuzzing based on machinelearning, and present MLFuzz, a platform with GPU access for easy andreproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our dataare public.</description><author>Maria-Irina Nicolae, Max Eisele, Andreas Zeller</author><pubDate>Thu, 28 Sep 2023 18:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16618v1</guid></item><item><title>Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space</title><link>http://arxiv.org/abs/2302.06807v3</link><description>Hyperbolic spaces have been quite popular in the recent past for representinghierarchically organized data. Further, several classification algorithms fordata in these spaces have been proposed in the literature. These algorithmsmainly use either hyperplanes or geodesics for decision boundaries in a largemargin classifiers setting leading to a non-convex optimization problem. Inthis paper, we propose a novel large margin classifier based on horosphericaldecision boundaries that leads to a geodesically convex optimization problemthat can be optimized using any Riemannian gradient descent techniqueguaranteeing a globally optimal solution. We present several experimentsdepicting the competitive performance of our classifier in comparison to SOTA.</description><author>Xiran Fan, Chun-Hao Yang, Baba C. Vemuri</author><pubDate>Thu, 28 Sep 2023 18:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06807v3</guid></item><item><title>Online Distribution Shift Detection via Recency Prediction</title><link>http://arxiv.org/abs/2211.09916v3</link><description>When deploying modern machine learning-enabled robotic systems in high-stakesapplications, detecting distribution shift is critical. However, most existingmethods for detecting distribution shift are not well-suited to roboticssettings, where data often arrives in a streaming fashion and may be veryhigh-dimensional. In this work, we present an online method for detectingdistribution shift with guarantees on the false positive rate - i.e., whenthere is no distribution shift, our system is very unlikely (with probability$&lt; \epsilon$) to falsely issue an alert; any alerts that are issued shouldtherefore be heeded. Our method is specifically designed for efficientdetection even with high dimensional data, and it empirically achieves up to11x faster detection on realistic robotics settings compared to prior workwhile maintaining a low false negative rate in practice (whenever there is adistribution shift in our experiments, our method indeed emits an alert). Wedemonstrate our approach in both simulation and hardware for a visual servoingtask, and show that our method indeed issues an alert before a failure occurs.</description><author>Rachel Luo, Rohan Sinha, Yixiao Sun, Ali Hindy, Shengjia Zhao, Silvio Savarese, Edward Schmerling, Marco Pavone</author><pubDate>Thu, 28 Sep 2023 18:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09916v3</guid></item><item><title>Qwen Technical Report</title><link>http://arxiv.org/abs/2309.16609v1</link><description>Large language models (LLMs) have revolutionized the field of artificialintelligence, enabling natural language processing tasks that were previouslythought to be exclusive to humans. In this work, we introduce Qwen, the firstinstallment of our large language model series. Qwen is a comprehensivelanguage model series that encompasses distinct models with varying parametercounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,the chat models finetuned with human alignment techniques. The base languagemodels consistently demonstrate superior performance across a multitude ofdownstream tasks, and the chat models, particularly those trained usingReinforcement Learning from Human Feedback (RLHF), are highly competitive. Thechat models possess advanced tool-use and planning capabilities for creatingagent applications, showcasing impressive performance even when compared tobigger models on complex tasks like utilizing a code interpreter. Furthermore,we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, aswell as mathematics-focused models, Math-Qwen-Chat, which are built upon baselanguage models. These models demonstrate significantly improved performance incomparison with open-source models, and slightly fall behind the proprietarymodels.</description><author>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu</author><pubDate>Thu, 28 Sep 2023 18:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16609v1</guid></item><item><title>KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing</title><link>http://arxiv.org/abs/2309.16608v1</link><description>Text-conditioned image editing is a recently emerged and highly practicaltask, and its potential is immeasurable. However, most of the concurrentmethods are unable to perform action editing, i.e. they can not produce resultsthat conform to the action semantics of the editing prompt and preserve thecontent of the original image. To solve the problem of action editing, wepropose KV Inversion, a method that can achieve satisfactory reconstructionperformance and action editing, which can solve two major problems: 1) theedited result can match the corresponding action, and 2) the edited object canretain the texture and identity of the original real image. In addition, ourmethod does not require training the Stable Diffusion model itself, nor does itrequire scanning a large-scale dataset to perform time-consuming training.</description><author>Jiancheng Huang, Yifan Liu, Jin Qin, Shifeng Chen</author><pubDate>Thu, 28 Sep 2023 18:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16608v1</guid></item><item><title>"AI enhances our performance, I have no doubt this one will do the same": The Placebo effect is robust to negative descriptions of AI</title><link>http://arxiv.org/abs/2309.16606v1</link><description>Heightened AI expectations facilitate performance in human-AI interactionsthrough placebo effects. While lowering expectations to control for placeboeffects is advisable, overly negative expectations could induce nocebo effects.In a letter discrimination task, we informed participants that an AI wouldeither increase or decrease their performance by adapting the interface, but inreality, no AI was present in any condition. A Bayesian analysis showed thatparticipants had high expectations and performed descriptively betterirrespective of the AI description when a sham-AI was present. Using cognitivemodeling, we could trace this advantage back to participants gathering moreinformation. A replication study verified that negative AI descriptions do notalter expectations, suggesting that performance expectations with AI are biasedand robust to negative verbal descriptions. We discuss the impact of userexpectations on AI interactions and evaluation and provide a behavioral placebomarker for human-AI interaction</description><author>Agnes M. Kloft, Robin Welsch, Thomas Kosch, Steeven Villa</author><pubDate>Thu, 28 Sep 2023 18:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16606v1</guid></item><item><title>Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance</title><link>http://arxiv.org/abs/2309.16604v1</link><description>Pairwise comparison of graphs is key to many applications in Machine learningranging from clustering, kernel-based classification/regression and morerecently supervised graph prediction. Distances between graphs usually rely oninformative representations of these structured objects such as bag ofsubstructures or other graph embeddings. A recently popular solution consistsin representing graphs as metric measure spaces, allowing to successfullyleverage Optimal Transport, which provides meaningful distances allowing tocompare them: the Gromov-Wasserstein distances. However, this family ofdistances overlooks edge attributes, which are essential for many structuredobjects. In this work, we introduce an extension of Gromov-Wasserstein distancefor comparing graphs whose both nodes and edges have features. We propose novelalgorithms for distance and barycenter computation. We empirically show theeffectiveness of the novel distance in learning tasks where graphs occur ineither input space or output space, such as classification and graphprediction.</description><author>Junjie Yang, Matthieu Labeau, Florence d'Alché-Buc</author><pubDate>Thu, 28 Sep 2023 18:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16604v1</guid></item><item><title>Deep Learning Based Uplink Multi-User SIMO Beamforming Design</title><link>http://arxiv.org/abs/2309.16603v1</link><description>The advancement of fifth generation (5G) wireless communication networks hascreated a greater demand for wireless resource management solutions that offerhigh data rates, extensive coverage, minimal latency and energy-efficientperformance. Nonetheless, traditional approaches have shortcomings when itcomes to computational complexity and their ability to adapt to dynamicconditions, creating a gap between theoretical analysis and the practicalexecution of algorithmic solutions for managing wireless resources. Deeplearning-based techniques offer promising solutions for bridging this gap withtheir substantial representation capabilities. We propose a novel unsuperviseddeep learning framework, which is called NNBF, for the design of uplink receivemulti-user single input multiple output (MU-SIMO) beamforming. The primaryobjective is to enhance the throughput by focusing on maximizing the sum-ratewhile also offering computationally efficient solution, in contrast toestablished conventional methods. We conduct experiments for several antennaconfigurations. Our experimental results demonstrate that NNBF exhibitssuperior performance compared to our baseline methods, namely, zero-forcingbeamforming (ZFBF) and minimum mean square error (MMSE) equalizer.Additionally, NNBF is scalable to the number of single-antenna user equipments(UEs) while baseline methods have significant computational burden due tomatrix pseudo-inverse operation.</description><author>Cemil Vahapoglu, Timothy J. O'Shea, Tamoghna Roy, Sennur Ulukus</author><pubDate>Thu, 28 Sep 2023 18:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16603v1</guid></item><item><title>S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees</title><link>http://arxiv.org/abs/2309.12041v2</link><description>Privacy-preserving learning of gradient boosting decision trees (GBDT) hasthe potential for strong utility-privacy tradeoffs for tabular data, such ascensus data or medical meta data: classical GBDT learners can extractnon-linear patterns from small sized datasets. The state-of-the-art notion forprovable privacy-properties is differential privacy, which requires that theimpact of single data points is limited and deniable. We introduce a noveldifferentially private GBDT learner and utilize four main techniques to improvethe utility-privacy tradeoff. (1) We use an improved noise scaling approachwith tighter accounting of privacy leakage of a decision tree leaf compared toprior work, resulting in noise that in expectation scales with $O(1/n)$, for$n$ data points. (2) We integrate individual R\'enyi filters to our method tolearn from data points that have been underutilized during an iterativetraining process, which -- potentially of independent interest -- results in anatural yet effective insight to learning streams of non-i.i.d. data. (3) Weincorporate the concept of random decision tree splits to concentrate privacybudget on learning leaves. (4) We deploy subsampling for privacy amplification.Our evaluation shows for the Abalone dataset ($&lt;4k$ training data points) a$R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work onlyachieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training datapoints) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which theclosest prior work only achieved for $\varepsilon=1.0$. For the Abalone datasetfor $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close tothe $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adultdataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is veryclose to the test error $13.7\,\%$ of the nonprivate version of GBDT.</description><author>Moritz Kirschte, Thorsten Peinemann, Joshua Stock, Carlos Cotrini, Esfandiar Mohammadi</author><pubDate>Thu, 28 Sep 2023 18:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12041v2</guid></item><item><title>Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation</title><link>http://arxiv.org/abs/2309.16599v1</link><description>Zero-shot translation (ZST), which is generally based on a multilingualneural machine translation model, aims to translate between unseen languagepairs in training data. The common practice to guide the zero-shot languagemapping during inference is to deliberately insert the source and targetlanguage IDs, e.g., &lt;EN&gt; for English and &lt;DE&gt; for German. Recent studies haveshown that language IDs sometimes fail to navigate the ZST task, making themsuffer from the off-target problem (non-target language words exist in thegenerated translation) and, therefore, difficult to apply the currentmultilingual translation model to a broad range of zero-shot languagescenarios. To understand when and why the navigation capabilities of languageIDs are weakened, we compare two extreme decoder input cases in the ZSTdirections: Off-Target (OFF) and On-Target (ON) cases. By contrastivelyvisualizing the contextual word representations (CWRs) of these cases withteacher forcing, we show that 1) the CWRs of different languages areeffectively distributed in separate regions when the sentence and ID arematched (ON setting), and 2) if the sentence and ID are unmatched (OFFsetting), the CWRs of different languages are chaotically distributed. Ouranalyses suggest that although they work well in ideal ON settings, languageIDs become fragile and lose their navigation ability when faced with off-targettokens, which commonly exist during inference but are rare in trainingscenarios. In response, we employ unlikelihood tuning on the negative (OFF)samples to minimize their probability such that the language IDs candiscriminate between the on- and off-target tokens during training. Experimentsspanning 40 ZST directions show that our method reduces the off-target ratio by-48.0% on average, leading to a +9.1 BLEU improvement with only an extra +0.3%tuning cost.</description><author>Changtong Zan, Liang Ding, Li Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, Dacheng Tao</author><pubDate>Thu, 28 Sep 2023 18:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16599v1</guid></item><item><title>Cross-Prediction-Powered Inference</title><link>http://arxiv.org/abs/2309.16598v1</link><description>While reliable data-driven decision-making hinges on high-quality labeleddata, the acquisition of quality labels often involves laborious humanannotations or slow and expensive scientific measurements. Machine learning isbecoming an appealing alternative as sophisticated predictive techniques arebeing used to quickly and cheaply produce large amounts of predicted labels;e.g., predicted protein structures are used to supplement experimentallyderived structures, predictions of socioeconomic indicators from satelliteimagery are used to supplement accurate survey data, and so on. Sincepredictions are imperfect and potentially biased, this practice brings intoquestion the validity of downstream inferences. We introduce cross-prediction:a method for valid inference powered by machine learning. With a small labeleddataset and a large unlabeled dataset, cross-prediction imputes the missinglabels via machine learning and applies a form of debiasing to remedy theprediction inaccuracies. The resulting inferences achieve the desired errorprobability and are more powerful than those that only leverage the labeleddata. Closely related is the recent proposal of prediction-powered inference,which assumes that a good pre-trained model is already available. We show thatcross-prediction is consistently more powerful than an adaptation ofprediction-powered inference in which a fraction of the labeled data is splitoff and used to train the model. Finally, we observe that cross-predictiongives more stable conclusions than its competitors; its confidence intervalstypically have significantly lower variability.</description><author>Tijana Zrnic, Emmanuel J. Candès</author><pubDate>Thu, 28 Sep 2023 18:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16598v1</guid></item><item><title>Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces</title><link>http://arxiv.org/abs/2309.16597v1</link><description>Bayesian optimization (BO) is a popular black-box function optimizationmethod, which makes sequential decisions based on a Bayesian model, typically aGaussian process (GP), of the function. To ensure the quality of the model,transfer learning approaches have been developed to automatically design GPpriors by learning from observations on "training" functions. These trainingfunctions are typically required to have the same domain as the "test" function(black-box function to be optimized). In this paper, we introduce MPHD, a modelpre-training method on heterogeneous domains, which uses a neural net mappingfrom domain-specific contexts to specifications of hierarchical GPs. MPHD canbe seamlessly integrated with BO to transfer knowledge across heterogeneoussearch spaces. Our theoretical and empirical results demonstrate the validityof MPHD and its superior performance on challenging black-box functionoptimization tasks.</description><author>Zhou Fan, Xinran Han, Zi Wang</author><pubDate>Thu, 28 Sep 2023 18:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16597v1</guid></item><item><title>Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why</title><link>http://arxiv.org/abs/2309.16595v1</link><description>This paper studies Large Language Models (LLMs) for structureddata--particularly graphs--a crucial data modality that remains underexploredin the LLM literature. We aim to understand when and why the incorporation ofstructural information inherent in graph data can improve the predictionperformance of LLMs on node classification tasks. To address the ``when''question, we examine a variety of prompting methods for encoding structuralinformation, in settings where textual node features are either rich or scarce.For the ``why'' questions, we probe into two potential contributing factors tothe LLM performance: data leakage and homophily. Our exploration of thesequestions reveals that (i) LLMs can benefit from structural information,especially when textual node features are scarce; (ii) there is no substantialevidence indicating that the performance of LLMs is significantly attributed todata leakage; and (iii) the performance of LLMs on a target node is stronglypositively related to the local homophily ratio of the node.</description><author>Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma</author><pubDate>Thu, 28 Sep 2023 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16595v1</guid></item><item><title>Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs</title><link>http://arxiv.org/abs/2309.16593v1</link><description>Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially indrug discovery and pharmaceutical research as they provide a structured way tointegrate diverse information sources, enhancing AI system interpretability.This interpretability is crucial in healthcare, where trust and transparencymatter, and eXplainable AI (XAI) supports decision making for healthcareprofessionals. This overview summarizes recent literature on the impact of KGsin healthcare and their role in developing explainable AI models. We cover KGworkflow, including construction, relationship extraction, reasoning, and theirapplications in areas like Drug-Drug Interactions (DDI), Drug TargetInteractions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), andbioinformatics. We emphasize the importance of making KGs more interpretablethrough knowledge-infused learning in healthcare. Finally, we highlightresearch challenges and provide insights for future directions.</description><author>Satvik Garg, Shivam Parikh, Somya Garg</author><pubDate>Thu, 28 Sep 2023 17:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16593v1</guid></item><item><title>Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection</title><link>http://arxiv.org/abs/2309.16592v1</link><description>The primary bottleneck towards obtaining good recognition performance in IRimages is the lack of sufficient labeled training data, owing to the cost ofacquiring such data. Realizing that object detection methods for the RGBmodality are quite robust (at least for some commonplace classes, like person,car, etc.), thanks to the giant training sets that exist, in this work we seekto leverage cues from the RGB modality to scale object detectors to the IRmodality, while preserving model performance in the RGB modality. At the coreof our method, is a novel tensor decomposition method called TensorFact whichsplits the convolution kernels of a layer of a Convolutional Neural Network(CNN) into low-rank factor matrices, with fewer parameters than the originalCNN. We first pretrain these factor matrices on the RGB modality, for whichplenty of training data are assumed to exist and then augment only a fewtrainable parameters for training on the IR modality to avoid over-fitting,while encouraging them to capture complementary cues from those trained only onthe RGB modality. We validate our approach empirically by first assessing howwell our TensorFact decomposed network performs at the task of detectingobjects in RGB images vis-a-vis the original network and then look at how wellit adapts to IR images of the FLIR ADAS v1 dataset. For the latter, we trainmodels under scenarios that pose challenges stemming from data paucity. Fromthe experiments, we observe that: (i) TensorFact shows performance gains on RGBimages; (ii) further, this pre-trained model, when fine-tuned, outperforms astandard state-of-the-art object detector on the FLIR ADAS v1 dataset by about4% in terms of mAP 50 score.</description><author>Manish Sharma, Moitreya Chatterjee, Kuan-Chuan Peng, Suhas Lohit, Michael Jones</author><pubDate>Thu, 28 Sep 2023 17:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16592v1</guid></item><item><title>Flexible and efficient spatial extremes emulation via variational autoencoders</title><link>http://arxiv.org/abs/2307.08079v2</link><description>Many real-world processes have complex tail dependence structures that cannotbe characterized using classical Gaussian processes. More flexible spatialextremes models exhibit appealing extremal dependence properties but are oftenexceedingly prohibitive to fit and simulate from in high dimensions. In thispaper, we develop a new spatial extremes model that has flexible andnon-stationary dependence properties, and we integrate it in theencoding-decoding structure of a variational autoencoder (XVAE), whoseparameters are estimated via variational Bayes combined with deep learning. TheXVAE can be used as a spatio-temporal emulator that characterizes thedistribution of potential mechanistic model output states and produces outputsthat have the same statistical properties as the inputs, especially in thetail. As an aside, our approach also provides a novel way of making fastinference with complex extreme-value processes. Through extensive simulationstudies, we show that our XVAE is substantially more time-efficient thantraditional Bayesian inference while also outperforming many spatial extremesmodels with a stationary dependence structure. To further demonstrate thecomputational power of the XVAE, we analyze a high-resolution satellite-deriveddataset of sea surface temperature in the Red Sea, which includes 30 years ofdaily measurements at 16703 grid cells. We find that the extremal dependencestrength is weaker in the interior of Red Sea and it has decreased slightlyover time.</description><author>Likun Zhang, Xiaoyu Ma, Christopher K. Wikle, Raphaël Huser</author><pubDate>Thu, 28 Sep 2023 17:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08079v2</guid></item><item><title>HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes</title><link>http://arxiv.org/abs/2212.10538v2</link><description>Bayesian optimization (BO), while proved highly effective for many black-boxfunction optimization tasks, requires practitioners to carefully select priorsthat well model their functions of interest. Rather than specifying by hand,researchers have investigated transfer learning based methods to automaticallylearn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO(Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, thoseprior learning methods typically assume that the input domains are the same forall tasks, weakening their ability to use observations on functions withdifferent domains or generalize the learned priors to BO on different searchspaces. In this work, we present HyperBO+: a pre-training approach forhierarchical Gaussian processes that enables the same prior to work universallyfor Bayesian optimization on functions with different domains. We propose atwo-step pre-training method and analyze its appealing asymptotic propertiesand benefits to BO both theoretically and empirically. On real-worldhyperparameter tuning tasks that involve multiple search spaces, we demonstratethat HyperBO+ is able to generalize to unseen search spaces and achieves lowerregrets than competitive baselines.</description><author>Zhou Fan, Xinran Han, Zi Wang</author><pubDate>Thu, 28 Sep 2023 17:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10538v2</guid></item><item><title>Vision Transformers Need Registers</title><link>http://arxiv.org/abs/2309.16588v1</link><description>Transformers have recently emerged as a powerful tool for learning visualrepresentations. In this paper, we identify and characterize artifacts infeature maps of both supervised and self-supervised ViT networks. The artifactscorrespond to high-norm tokens appearing during inference primarily inlow-informative background areas of images, that are repurposed for internalcomputations. We propose a simple yet effective solution based on providingadditional tokens to the input sequence of the Vision Transformer to fill thatrole. We show that this solution fixes that problem entirely for bothsupervised and self-supervised models, sets a new state of the art forself-supervised visual models on dense visual prediction tasks, enables objectdiscovery methods with larger models, and most importantly leads to smootherfeature maps and attention maps for downstream visual processing.</description><author>Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski</author><pubDate>Thu, 28 Sep 2023 17:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16588v1</guid></item><item><title>Text-to-3D using Gaussian Splatting</title><link>http://arxiv.org/abs/2309.16585v1</link><description>In this paper, we present Gaussian Splatting based text-to-3D generation(GSGEN), a novel approach for generating high-quality 3D objects. Previousmethods suffer from inaccurate geometry and limited fidelity due to the absenceof 3D prior and proper representation. We leverage 3D Gaussian Splatting, arecent state-of-the-art representation, to address existing shortcomings byexploiting the explicit nature that enables the incorporation of 3D prior.Specifically, our method adopts a progressive optimization strategy, whichincludes a geometry optimization stage and an appearance refinement stage. Ingeometry optimization, a coarse representation is established under a 3Dgeometry prior along with the ordinary 2D SDS loss, ensuring a sensible and3D-consistent rough shape. Subsequently, the obtained Gaussians undergo aniterative refinement to enrich details. In this stage, we increase the numberof Gaussians by compactness-based densification to enhance continuity andimprove fidelity. With these designs, our approach can generate 3D content withdelicate details and more accurate geometry. Extensive evaluations demonstratethe effectiveness of our method, especially for capturing high-frequencycomponents. Video results are provided at https://gsgen3d.github.io. Our codeis available at https://github.com/gsgen3d/gsgen</description><author>Zilong Chen, Feng Wang, Huaping Liu</author><pubDate>Thu, 28 Sep 2023 17:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16585v1</guid></item><item><title>A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems</title><link>http://arxiv.org/abs/2309.16584v1</link><description>To leverage training data for the sufficient training of ML models frommultiple parties in a confidentiality-preserving way, various collaborativedistributed machine learning (CDML) system designs have been developed, forexample, to perform assisted learning, federated learning, and split learning.CDML system designs show different traits, for example, high agent autonomy,machine learning (ML) model confidentiality, and fault tolerance. Facing a widevariety of CDML system designs with different traits, it is difficult fordevelopers to design CDML systems with traits that match use case requirementsin a targeted way. However, inappropriate CDML system designs may result inCDML systems failing their envisioned purposes. We developed a CDML designtoolbox that can guide the development of CDML systems. Based on the CDMLdesign toolbox, we present CDML system archetypes with distinct key traits thatcan support the design of CDML systems to meet use case requirements.</description><author>David Jin, Niclas Kannengießer, Sascha Rank, Ali Sunyaev</author><pubDate>Thu, 28 Sep 2023 17:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16584v1</guid></item><item><title>Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset</title><link>http://arxiv.org/abs/2305.10775v3</link><description>Accurate analysis of speech articulation is crucial for speech analysis.However, X-Y coordinates of articulators strongly depend on the anatomy of thespeakers and the variability of pellet placements, and existing methods formapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail tocapture the entire anatomy of the vocal tract. In this paper, we propose a newgeometric transformation that improves the accuracy of these measurements. Ourtransformation maps anatomical landmarks' X-Y coordinates along the midsagittalplane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), TongueBody Constriction Location (TTCL), Degree (TBCD), Tongue Tip ConstrictionLocation (TTCL) and Degree (TTCD). Our novel contribution is the extension ofthe palate trace towards the inferred anterior pharyngeal line, which improvesmeasurements of tongue body constriction.</description><author>Ahmed Adel Attia, Mark Tiede, Carol Y. Espy-Wilson</author><pubDate>Thu, 28 Sep 2023 17:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10775v3</guid></item><item><title>GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond</title><link>http://arxiv.org/abs/2309.16583v1</link><description>With the rapid advancement of large language models (LLMs), there is apressing need for a comprehensive evaluation suite to assess their capabilitiesand limitations. Existing LLM leaderboards often reference scores reported inother papers without consistent settings and prompts, which may inadvertentlyencourage cherry-picking favored settings and prompts for better results. Inthis work, we introduce GPT-Fathom, an open-source and reproducible LLMevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across7 capability categories, all under aligned settings. Our retrospective study onOpenAI's earlier models offers valuable insights into the evolutionary pathfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3progressively improves to GPT-4, including technical details like whetheradding code data improves LLM's reasoning capability, which aspects of LLMcapability can be improved by SFT and RLHF, how much is the alignment tax, etc.Our analysis sheds light on many of these questions, aiming to improve thetransparency of advanced LLMs.</description><author>Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang</author><pubDate>Thu, 28 Sep 2023 17:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16583v1</guid></item><item><title>Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach</title><link>http://arxiv.org/abs/2308.09267v3</link><description>Large Language Models (LLMs) have showcased impressive reasoningcapabilities, particularly when guided by specifically designed prompts incomplex reasoning tasks such as math word problems. These models typicallysolve tasks using a chain-of-thought approach, which not only bolsters theirreasoning abilities but also provides valuable insights into theirproblem-solving process. However, there is still significant room for enhancingthe reasoning abilities of LLMs. Some studies suggest that the integration ofan LLM output verifier can boost reasoning accuracy without necessitatingadditional model training. In this paper, we follow these studies and introducea novel graph-based method to further augment the reasoning capabilities ofLLMs. We posit that multiple solutions to a reasoning task, generated by anLLM, can be represented as a reasoning graph due to the logical connectionsbetween intermediate steps from different reasoning paths. Therefore, wepropose the Reasoning Graph Verifier (RGV) to analyze and verify the solutionsgenerated by LLMs. By evaluating these graphs, models can yield more accurateand reliable results.Our experimental results show that our graph-basedverification method not only significantly enhances the reasoning abilities ofLLMs but also outperforms existing verifier methods in terms of improving thesemodels' reasoning performance.</description><author>Lang Cao</author><pubDate>Thu, 28 Sep 2023 17:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09267v3</guid></item><item><title>M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning</title><link>http://arxiv.org/abs/2309.16578v1</link><description>Orbital-free density functional theory (OFDFT) is a quantum chemistryformulation that has a lower cost scaling than the prevailing Kohn-Sham DFT,which is increasingly desired for contemporary molecular research. However, itsaccuracy is limited by the kinetic energy density functional, which isnotoriously hard to approximate for non-periodic molecular systems. In thiswork, we propose M-OFDFT, an OFDFT approach capable of solving molecularsystems using a deep-learning functional model. We build the essentialnonlocality into the model, which is made affordable by the concise densityrepresentation as expansion coefficients under an atomic basis. With techniquesto address unconventional learning challenges therein, M-OFDFT achieves acomparable accuracy with Kohn-Sham DFT on a wide range of molecules untouchedby OFDFT before. More attractively, M-OFDFT extrapolates well to molecules muchlarger than those in training, which unleashes the appealing scaling forstudying large molecules including proteins, representing an advancement of theaccuracy-efficiency trade-off frontier in quantum chemistry.</description><author>He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng Lu, Tong Wang, Nanning Zheng, Bin Shao</author><pubDate>Thu, 28 Sep 2023 17:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16578v1</guid></item><item><title>A Benchmark for Learning to Translate a New Language from One Grammar Book</title><link>http://arxiv.org/abs/2309.16575v1</link><description>Large language models (LLMs) can perform impressive feats with in-contextlearning or lightweight finetuning. It is natural to wonder how well thesemodels adapt to genuinely new tasks, but how does one find tasks that areunseen in internet-scale training sets? We turn to a field that is explicitlymotivated and bottlenecked by a scarcity of web data: low-resource languages.In this paper, we introduce MTOB (Machine Translation from One Book), abenchmark for learning to translate between English and Kalamang -- a languagewith less than 200 speakers and therefore virtually no presence on the web --using several hundred pages of field linguistics reference materials. This taskframing is novel in that it asks a model to learn a language from a singlehuman-readable book of grammar explanations, rather than a large mined corpusof in-domain data, more akin to L2 learning than L1 acquisition. We demonstratethat baselines using current LLMs are promising but fall short of humanperformance, achieving 44.7 chrF on Kalamang to English translation and 45.8chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by ahuman who learned Kalamang from the same reference materials. We hope that MTOBwill help measure LLM capabilities along a new dimension, and that the methodsdeveloped to solve it could help expand access to language technology forunderserved communities by leveraging qualitatively different kinds of datathan traditional machine translation.</description><author>Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi</author><pubDate>Thu, 28 Sep 2023 17:32:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16575v1</guid></item><item><title>Efficient Adversarial Input Generation via Neural Net Patching</title><link>http://arxiv.org/abs/2211.16808v2</link><description>The generation of adversarial inputs has become a crucial issue inestablishing the robustness and trustworthiness of deep neural nets, especiallywhen they are used in safety-critical application domains such as autonomousvehicles and precision medicine. However, the problem poses multiple practicalchallenges, including scalability issues owing to large-sized networks, and thegeneration of adversarial inputs that lack important qualities such asnaturalness and output-impartiality. This problem shares its end goal with thetask of patching neural nets where small changes in some of the network'sweights need to be discovered so that upon applying these changes, the modifiednet produces the desirable output for a given set of inputs. We exploit thisconnection by proposing to obtain an adversarial input from a patch, with theunderlying observation that the effect of changing the weights can also bebrought about by changing the inputs instead. Thus, this paper presents a novelway to generate input perturbations that are adversarial for a given network byusing an efficient network patching technique. We note that the proposed methodis significantly more effective than the prior state-of-the-art techniques.</description><author>Tooba Khan, Kumar Madhukar, Subodh Vishnu Sharma</author><pubDate>Thu, 28 Sep 2023 17:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16808v2</guid></item><item><title>The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges</title><link>http://arxiv.org/abs/2309.16573v1</link><description>Some of the most powerful language models currently are proprietary systems,accessible only via (typically restrictive) web or software programminginterfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm.Contrasting with scenarios where full model access is available, as in the caseof open-source models, such closed-off language models create specificchallenges for evaluating, benchmarking, and testing them. This paper has twogoals: on the one hand, we delineate how the aforementioned challenges act asimpediments to the accessibility, replicability, reliability, andtrustworthiness (ARRT) of LMaaS. We systematically examine the issues thatarise from a lack of information about language models for each of these fouraspects. We shed light on current solutions, provide some recommendations, andhighlight the directions for future advancements. On the other hand, it servesas a one-stop-shop for the extant knowledge about current, major LMaaS,offering a synthesized overview of the licences and capabilities theirinterfaces offer.</description><author>Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Anthony G. Cohn, Nigel Shadbolt, Michael Wooldridge</author><pubDate>Thu, 28 Sep 2023 17:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16573v1</guid></item><item><title>Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials</title><link>http://arxiv.org/abs/2309.16571v1</link><description>Additive manufacturing has revolutionized the manufacturing of complex partsby enabling direct material joining and offers several advantages such ascost-effective manufacturing of complex parts, reducing manufacturing waste,and opening new possibilities for manufacturing automation. One group ofmaterials for which additive manufacturing holds great potential for enhancingcomponent performance and properties is Functionally Graded Materials (FGMs).FGMs are advanced composite materials that exhibit smoothly varying propertiesmaking them desirable for applications in aerospace, automobile, biomedical,and defense industries. Such composition differs from traditional compositematerials, since the location-dependent composition changes gradually in FGMs,leading to enhanced properties. Recently, machine learning techniques haveemerged as a promising means for fabrication of FGMs through optimizingprocessing parameters, improving product quality, and detecting manufacturingdefects. This paper first provides a brief literature review of works relatedto FGM fabrication, followed by reviewing works on employing machine learningin additive manufacturing, Afterward, we provide an overview of published worksin the literature related to the application of machine learning methods inDirected Energy Deposition and for fabrication of FGMs.</description><author>Mohammad Karimzadeh, Aleksandar Vakanski, Fei Xu, Xinchang Zhang</author><pubDate>Thu, 28 Sep 2023 17:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16571v1</guid></item><item><title>Audio-Visual Speaker Verification via Joint Cross-Attention</title><link>http://arxiv.org/abs/2309.16569v1</link><description>Speaker verification has been widely explored using speech signals, which hasshown significant improvement using deep models. Recently, there has been asurge in exploring faces and voices as they can offer more complementary andcomprehensive information than relying only on a single modality of speechsignals. Though current methods in the literature on the fusion of faces andvoices have shown improvement over that of individual face or voice modalities,the potential of audio-visual fusion is not fully explored for speakerverification. Most of the existing methods based on audio-visual fusion eitherrely on score-level fusion or simple feature concatenation. In this work, wehave explored cross-modal joint attention to fully leverage the inter-modalcomplementary information and the intra-modal information for speakerverification. Specifically, we estimate the cross-attention weights based onthe correlation between the joint feature presentation and that of theindividual feature representations in order to effectively capture bothintra-modal as well inter-modal relationships among the faces and voices. Wehave shown that efficiently leveraging the intra- and inter-modal relationshipssignificantly improves the performance of audio-visual fusion for speakerverification. The performance of the proposed approach has been evaluated onthe Voxceleb1 dataset. Results show that the proposed approach cansignificantly outperform the state-of-the-art methods of audio-visual fusionfor speaker verification.</description><author>R. Gnana Praveen, Jahangir Alam</author><pubDate>Thu, 28 Sep 2023 17:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16569v1</guid></item><item><title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</title><link>http://arxiv.org/abs/2309.14181v2</link><description>The rapid evolution of Multi-modality Large Language Models (MLLMs) hascatalyzed a shift in computer vision from specialized models to general-purposefoundation models. Nevertheless, there is still an inadequacy in assessing theabilities of MLLMs on low-level visual perception and understanding. To addressthis gap, we present Q-Bench, a holistic benchmark crafted to systematicallyevaluate potential abilities of MLLMs on three realms: low-level visualperception, low-level visual description, and overall visual qualityassessment. a) To evaluate the low-level perception ability, we construct theLLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equippedwith a human-asked question focusing on its low-level attributes. We thenmeasure the correctness of MLLMs on answering these questions. b) To examinethe description ability of MLLMs on low-level information, we propose theLLDescribe dataset consisting of long expert-labelled golden low-level textdescriptions on 499 images, and a GPT-involved comparison pipeline betweenoutputs of MLLMs and the golden descriptions. c) Besides these two tasks, wefurther measure their visual quality assessment ability to align with humanopinion scores. Specifically, we design a softmax-based strategy that enablesMLLMs to predict quantifiable quality scores, and evaluate them on variousexisting image quality assessment (IQA) datasets. Our evaluation across thethree abilities confirms that MLLMs possess preliminary low-level visualskills. However, these skills are still unstable and relatively imprecise,indicating the need for specific enhancements on MLLMs towards these abilities.We hope that our benchmark can encourage the research community to delve deeperto discover and enhance these untapped potentials of MLLMs. Project Page:https://vqassessment.github.io/Q-Bench.</description><author>Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin</author><pubDate>Thu, 28 Sep 2023 17:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14181v2</guid></item><item><title>Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings</title><link>http://arxiv.org/abs/2309.16564v1</link><description>Unsupervised learning allows us to leverage unlabelled data, which has becomeabundantly available, and to create embeddings that are usable on a variety ofdownstream tasks. However, the typical lack of interpretability of unsupervisedrepresentation learning has become a limiting factor with regard to recenttransparent-AI regulations. In this paper, we study graph representationlearning and we show that data augmentation that preserves semantics can belearned and used to produce interpretations. Our framework, which we namedINGENIOUS, creates inherently interpretable embeddings and eliminates the needfor costly additional post-hoc analysis. We also introduce additional metricsaddressing the lack of formalism and metrics in the understudied area ofunsupervised-representation learning interpretability. Our results aresupported by an experimental study applied to both graph-level and node-leveltasks and show that interpretable embeddings provide state-of-the-artperformance on subsequent downstream tasks.</description><author>Gregory Scafarto, Madalina Ciortan, Simon Tihon, Quentin Ferre</author><pubDate>Thu, 28 Sep 2023 17:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16564v1</guid></item><item><title>A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models</title><link>http://arxiv.org/abs/2309.10003v2</link><description>This work proposes to measure the scope of a patent claim as the reciprocalof the self-information contained in this claim. A probability of occurrence ofthe claim is obtained from a language model and this probability is used tocompute the self-information. Grounded in information theory, this approach isbased on the assumption that an unlikely concept is more informative than ausual concept, insofar as it is more surprising. In turn, the more surprisingthe information required to defined the claim, the narrower its scope. Fivelanguage models are considered, ranging from simplest models (each word orcharacter is assigned an identical probability) to intermediate models (usingaverage word or character frequencies), to a large language model (GPT2).Interestingly, the scope resulting from the simplest language models isproportional to the reciprocal of the number of words or characters involved inthe claim, a metric already used in previous works. Application is made tomultiple series of patent claims directed to distinct inventions, where eachseries consists of claims devised to have a gradually decreasing scope. Theperformance of the language models is assessed with respect to several ad hoctests. The more sophisticated the model, the better the results. I.e., the GPT2probability model outperforms models based on word and character frequencies,which themselves outdo the simplest models based on word or character counts.Still, the character count appears to be a more reliable indicator than theword count.</description><author>Sébastien Ragot</author><pubDate>Thu, 28 Sep 2023 17:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10003v2</guid></item><item><title>CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption</title><link>http://arxiv.org/abs/2309.16563v1</link><description>We investigate the regret-minimisation problem in a multi-armed banditsetting with arbitrary corruptions. Similar to the classical setup, the agentreceives rewards generated independently from the distribution of the armchosen at each time. However, these rewards are not directly observed. Instead,with a fixed $\varepsilon\in (0,\frac{1}{2})$, the agent observes a sample fromthe chosen arm's distribution with probability $1-\varepsilon$, or from anarbitrary corruption distribution with probability $\varepsilon$. Importantly,we impose no assumptions on these corruption distributions, which can beunbounded. In this setting, accommodating potentially unbounded corruptions, weestablish a problem-dependent lower bound on regret for a given family of armdistributions. We introduce CRIMED, an asymptotically-optimal algorithm thatachieves the exact lower bound on regret for bandits with Gaussiandistributions with known variance. Additionally, we provide a finite-sampleanalysis of CRIMED's regret performance. Notably, CRIMED can effectively handlecorruptions with $\varepsilon$ values as high as $\frac{1}{2}$. Furthermore, wedevelop a tight concentration result for medians in the presence of arbitrarycorruptions, even with $\varepsilon$ values up to $\frac{1}{2}$, which may beof independent interest. We also discuss an extension of the algorithm forhandling misspecification in Gaussian model.</description><author>Shubhada Agrawal, Timothée Mathieu, Debabrota Basu, Odalric-Ambrym Maillard</author><pubDate>Thu, 28 Sep 2023 17:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16563v1</guid></item><item><title>Causal Policy Gradient for Whole-Body Mobile Manipulation</title><link>http://arxiv.org/abs/2305.04866v4</link><description>Developing the next generation of household robot helpers requires combininglocomotion and interaction capabilities, which is generally referred to asmobile manipulation (MoMa). MoMa tasks are difficult due to the large actionspace of the robot and the common multi-objective nature of the task, e.g.,efficiently reaching a goal while avoiding obstacles. Current approaches oftensegregate tasks into navigation without manipulation and stationarymanipulation without locomotion by manually matching parts of the action spaceto MoMa sub-objectives (e.g. learning base actions for locomotion objectivesand learning arm actions for manipulation). This solution prevents simultaneouscombinations of locomotion and interaction degrees of freedom and requireshuman domain knowledge for both partitioning the action space and matching theaction parts to the sub-objectives. In this paper, we introduce Causal MoMa, anew reinforcement learning framework to train policies for typical MoMa tasksthat makes use of the most favorable subspace of the robot's action space toaddress each sub-objective. Causal MoMa automatically discovers the causaldependencies between actions and terms of the reward function and exploitsthese dependencies through causal policy gradient that reduces gradientvariance compared to previous state-of-the-art reinforcement learningalgorithms, improving convergence and results. We evaluate the performance ofCausal MoMa on three types of simulated robots across different MoMa tasks anddemonstrate success in transferring the policies trained in simulation directlyto a real robot, where our agent is able to follow moving goals and react todynamic obstacles while simultaneously and synergistically controlling thewhole-body: base, arm, and head. More information athttps://sites.google.com/view/causal-moma.</description><author>Jiaheng Hu, Peter Stone, Roberto Martín-Martín</author><pubDate>Thu, 28 Sep 2023 17:17:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04866v4</guid></item><item><title>Voting Network for Contour Levee Farmland Segmentation and Classification</title><link>http://arxiv.org/abs/2309.16561v1</link><description>High-resolution aerial imagery allows fine details in the segmentation offarmlands. However, small objects and features introduce distortions to thedelineation of object boundaries, and larger contextual views are needed tomitigate class confusion. In this work, we present an end-to-end trainablenetwork for segmenting farmlands with contour levees from high-resolutionaerial imagery. A fusion block is devised that includes multiple voting blocksto achieve image segmentation and classification. We integrate the fusion blockwith a backbone and produce both semantic predictions and segmentation slices.The segmentation slices are used to perform majority voting on the predictions.The network is trained to assign the most likely class label of a segment toits pixels, learning the concept of farmlands rather than analyzingconstitutive pixels separately. We evaluate our method using images from theNational Agriculture Imagery Program. Our method achieved an average accuracyof 94.34\%. Compared to the state-of-the-art methods, the proposed methodobtains an improvement of 6.96% and 2.63% in the F1 score on average.</description><author>Abolfazl Meyarian, Xiaohui Yuan</author><pubDate>Thu, 28 Sep 2023 17:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16561v1</guid></item><item><title>Generative Disco: Text-to-Video Generation for Music Visualization</title><link>http://arxiv.org/abs/2304.08551v2</link><description>Visuals can enhance our experience of music, owing to the way they canamplify the emotions and messages conveyed within it. However, creating musicvisualization is a complex, time-consuming, and resource-intensive process. Weintroduce Generative Disco, a generative AI system that helps generate musicvisualizations with large language models and text-to-video generation. Thesystem helps users visualize music in intervals by finding prompts to describethe images that intervals start and end on and interpolating between them tothe beat of the music. We introduce design patterns for improving thesegenerated videos: transitions, which express shifts in color, time, subject, orstyle, and holds, which help focus the video on subjects. A study withprofessionals showed that transitions and holds were a highly expressiveframework that enabled them to build coherent visual narratives. We conclude onthe generalizability of these patterns and the potential of generated video forcreative professionals.</description><author>Vivian Liu, Tao Long, Nathan Raw, Lydia Chilton</author><pubDate>Thu, 28 Sep 2023 17:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08551v2</guid></item><item><title>Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition</title><link>http://arxiv.org/abs/2309.13405v2</link><description>This paper studies the problem of learning the large-scale Gaussian graphicalmodels that are multivariate totally positive of order two ($\text{MTP}_2$). Byintroducing the concept of bridge, which commonly exists in large-scale sparsegraphs, we show that the entire problem can be equivalently optimized through(1) several smaller-scaled sub-problems induced by a \emph{bridge-blockdecomposition} on the thresholded sample covariance graph and (2) a set ofexplicit solutions on entries corresponding to \emph{bridges}. From practicalaspect, this simple and provable discipline can be applied to break down alarge problem into small tractable ones, leading to enormous reduction on thecomputational complexity and substantial improvements for all existingalgorithms. The synthetic and real-world experiments demonstrate that ourproposed method presents a significant speed-up compared to thestate-of-the-art benchmarks.</description><author>Xiwen Wang, Jiaxi Ying, Daniel P. Palomar</author><pubDate>Thu, 28 Sep 2023 17:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13405v2</guid></item><item><title>MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond</title><link>http://arxiv.org/abs/2309.16553v1</link><description>Neural radiance fields (NeRF) and its subsequent variants have led toremarkable progress in neural rendering. While most of recent neural renderingworks focus on objects and small-scale scenes, developing neural renderingmethods for city-scale scenes is of great potential in many real-worldapplications. However, this line of research is impeded by the absence of acomprehensive and high-quality dataset, yet collecting such a dataset over realcity-scale scenes is costly, sensitive, and technically difficult. To this end,we build a large-scale, comprehensive, and high-quality synthetic dataset forcity-scale neural rendering researches. Leveraging the Unreal Engine 5 CitySample project, we develop a pipeline to easily collect aerial and street cityviews, accompanied by ground-truth camera poses and a range of additional datamodalities. Flexible controls over environmental factors like light, weather,human and car crowd are also available in our pipeline, supporting the need ofvarious tasks covering city-scale neural rendering and beyond. The resultingpilot dataset, MatrixCity, contains 67k aerial images and 452k street imagesfrom two city maps of total size $28km^2$. On top of MatrixCity, a thoroughbenchmark is also conducted, which not only reveals unique challenges of thetask of city-scale neural rendering, but also highlights potential improvementsfor future works. The dataset and code will be publicly available at ourproject page: https://city-super.github.io/matrixcity/.</description><author>Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, Bo Dai</author><pubDate>Thu, 28 Sep 2023 17:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16553v1</guid></item><item><title>InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping</title><link>http://arxiv.org/abs/2308.08543v2</link><description>Vectorized high-definition (HD) maps contain detailed information aboutsurrounding road elements, which are crucial for various downstream tasks inmodern autonomous driving vehicles, such as vehicle planning and control.Recent works have attempted to directly detect the vectorized HD map as a pointset prediction task, resulting in significant improvements in detectionperformance. However, these approaches fail to analyze and exploit theinner-instance correlations between predicted points, impeding furtheradvancements. To address these challenges, we investigate the utilization ofinner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definitionmapping through $\textbf{T}$ransformers and introduce InsightMapper. This paperpresents three novel designs within InsightMapper that leverage inner-instanceinformation in distinct ways, including hybrid query generation, inner-instancequery fusion, and inner-instance feature aggregation. Comparative experimentsare conducted on the NuScenes dataset, showcasing the superiority of ourproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.Simultaneously, InsightMapper maintains high efficiency during both trainingand inference phases, resulting in remarkable comprehensive performance. Theproject page for this work is available athttps://tonyxuqaq.github.io/InsightMapper/ .</description><author>Zhenhua Xu, Kenneth K. Y. Wong, Hengshuang Zhao</author><pubDate>Thu, 28 Sep 2023 17:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08543v2</guid></item><item><title>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</title><link>http://arxiv.org/abs/2309.03847v2</link><description>We study the problem of estimating mixtures of Gaussians under the constraintof differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4\log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate amixture of $k$ Gaussians up to total variation distance $\alpha$ whilesatisfying $(\varepsilon, \delta)$-DP. This is the first finite samplecomplexity upper bound for the problem that does not make any structuralassumptions on the GMMs. To solve the problem, we devise a new framework which may be useful for othertasks. On a high level, we show that if a class of distributions (such asGaussians) is (1) list decodable and (2) admits a "locally small'' cover (Bunet al., 2021) with respect to total variation distance, then the class of itsmixtures is privately learnable. The proof circumvents a known barrierindicating that, unlike Gaussians, GMMs do not admit a locally small cover(Aden-Ali et al., 2021b).</description><author>Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</author><pubDate>Thu, 28 Sep 2023 16:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03847v2</guid></item><item><title>Correcting for heterogeneity in real-time epidemiological indicators</title><link>http://arxiv.org/abs/2309.16546v1</link><description>Auxiliary data sources have become increasingly important in epidemiologicalsurveillance, as they are often available at a finer spatial and temporalresolution, larger coverage, and lower latency than traditional surveillancesignals. We describe the problem of spatial and temporal heterogeneity in thesesignals derived from these data sources, where spatial and/or temporal biasesare present. We present a method to use a ``guiding'' signal to correct forthese biases and produce a more reliable signal that can be used for modelingand forecasting. The method assumes that the heterogeneity can be approximatedby a low-rank matrix and that the temporal heterogeneity is smooth over time.We also present a hyperparameter selection algorithm to choose the parametersrepresenting the matrix rank and degree of temporal smoothness of thecorrections. In the absence of ground truth, we use maps and plots to arguethat this method does indeed reduce heterogeneity. Reducing heterogeneity fromauxiliary data sources greatly increases their utility in modeling andforecasting epidemics.</description><author>Aaron Rumack, Roni Rosenfeld, F. William Townes</author><pubDate>Thu, 28 Sep 2023 16:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16546v1</guid></item><item><title>Unsupervised Fact Verification by Language Model Distillation</title><link>http://arxiv.org/abs/2309.16540v1</link><description>Unsupervised fact verification aims to verify a claim using evidence from atrustworthy knowledge base without any kind of data annotation. To address thischallenge, algorithms must produce features for every claim that are bothsemantically meaningful, and compact enough to find a semantic alignment withthe source information. In contrast to previous work, which tackled thealignment problem by learning over annotated corpora of claims and theircorresponding labels, we propose SFAVEL (Self-supervised Fact Verification viaLanguage Model Distillation), a novel unsupervised framework that leveragespre-trained language models to distil self-supervised features intohigh-quality claim-fact alignments without the need for annotations. This isenabled by a novel contrastive loss function that encourages features to attainhigh-quality claim and evidence alignments whilst preserving the semanticrelationships across the corpora. Notably, we present results that achieve anew state-of-the-art on the standard FEVER fact verification benchmark (+8%accuracy) with linear evaluation.</description><author>Adrián Bazaga, Pietro Liò, Gos Micklem</author><pubDate>Thu, 28 Sep 2023 16:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16540v1</guid></item><item><title>Uncertainty Quantification for Eosinophil Segmentation</title><link>http://arxiv.org/abs/2309.16536v1</link><description>Eosinophilic Esophagitis (EoE) is an allergic condition increasing inprevalence. To diagnose EoE, pathologists must find 15 or more eosinophilswithin a single high-power field (400X magnification). Determining whether ornot a patient has EoE can be an arduous process and any medical imagingapproaches used to assist diagnosis must consider both efficiency andprecision. We propose an improvement of Adorno et al's approach for quantifyingeosinphils using deep image segmentation. Our new approach leverages MonteCarlo Dropout, a common approach in deep learning to reduce overfitting, toprovide uncertainty quantification on current deep learning models. Theuncertainty can be visualized in an output image to evaluate model performance,provide insight to how deep learning algorithms function, and assistpathologists in identifying eosinophils.</description><author>Kevin Lin, Donald Brown, Sana Syed, Adam Greene</author><pubDate>Thu, 28 Sep 2023 16:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16536v1</guid></item><item><title>KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models</title><link>http://arxiv.org/abs/2309.16535v1</link><description>Recently, Locate-Then-Edit paradigm has emerged as one of the main approachesin changing factual knowledge stored in the Language models. However, there isa lack of research on whether present locating methods can pinpoint the exactparameters embedding the desired knowledge. Moreover, although many researchershave questioned the validity of locality hypothesis of factual knowledge, nomethod is provided to test the a hypothesis for more in-depth discussion andresearch. Therefore, we introduce KLoB, a benchmark examining three essentialproperties that a reliable knowledge locating method should satisfy. KLoB canserve as a benchmark for evaluating existing locating methods in languagemodels, and can contributes a method to reassessing the validity of localityhypothesis of factual knowledge. Our is publicly available at\url{https://github.com/juyiming/KLoB}.</description><author>Yiming Ju, Zheng Zhang</author><pubDate>Thu, 28 Sep 2023 16:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16535v1</guid></item><item><title>MotionLM: Multi-Agent Motion Forecasting as Language Modeling</title><link>http://arxiv.org/abs/2309.16534v1</link><description>Reliable forecasting of the future behavior of road agents is a criticalcomponent to safe planning in autonomous vehicles. Here, we representcontinuous trajectories as sequences of discrete motion tokens and castmulti-agent motion prediction as a language modeling task over this domain. Ourmodel, MotionLM, provides several advantages: First, it does not requireanchors or explicit latent variable optimization to learn multimodaldistributions. Instead, we leverage a single standard language modelingobjective, maximizing the average log probability over sequence tokens. Second,our approach bypasses post-hoc interaction heuristics where individual agenttrajectory generation is conducted prior to interactive scoring. Instead,MotionLM produces joint distributions over interactive agent futures in asingle autoregressive decoding process. In addition, the model's sequentialfactorization enables temporally causal conditional rollouts. The proposedapproach establishes new state-of-the-art performance for multi-agent motionprediction on the Waymo Open Motion Dataset, ranking 1st on the interactivechallenge leaderboard.</description><author>Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, Benjamin Sapp</author><pubDate>Thu, 28 Sep 2023 16:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16534v1</guid></item><item><title>Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices</title><link>http://arxiv.org/abs/2309.06612v2</link><description>The recent surge of interest surrounding Multimodal Neural Networks (MM-NN)is attributed to their ability to effectively process and integrate multiscaleinformation from diverse data sources. MM-NNs extract and fuse features frommultiple modalities using adequate unimodal backbones and specific fusionnetworks. Although this helps strengthen the multimodal informationrepresentation, designing such networks is labor-intensive. It requires tuningthe architectural parameters of the unimodal backbones, choosing the fusingpoint, and selecting the operations for fusion. Furthermore, multimodality AIis emerging as a cutting-edge option in Internet of Things (IoT) systems whereinference latency and energy consumption are critical metrics in addition toaccuracy. In this paper, we propose Harmonic-NAS, a framework for the jointoptimization of unimodal backbones and multimodal fusion networks with hardwareawareness on resource-constrained devices. Harmonic-NAS involves a two-tieroptimization approach for the unimodal backbone architectures and fusionstrategy and operators. By incorporating the hardware dimension into theoptimization, evaluation results on various devices and multimodal datasetshave demonstrated the superiority of Harmonic-NAS over state-of-the-artapproaches achieving up to 10.9% accuracy improvement, 1.91x latency reduction,and 2.14x energy efficiency gain.</description><author>Mohamed Imed Eddine Ghebriout, Halima Bouzidi, Smail Niar, Hamza Ouarnoughi</author><pubDate>Thu, 28 Sep 2023 16:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06612v2</guid></item><item><title>Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization</title><link>http://arxiv.org/abs/2304.12405v2</link><description>We present a method for synthesizing dynamic, reduced-order output-feedbackpolynomial control policies for control-affine nonlinear systems whichguarantees runtime stability to a goal state, when using visual observationsand a learned perception module in the feedback control loop. We leverageLyapunov analysis to formulate the problem of synthesizing such policies. Thisproblem is nonconvex in the policy parameters and the Lyapunov function that isused to prove the stability of the policy. To solve this problem approximately,we propose two approaches: the first solves a sequence of sum-of-squaresoptimization problems to iteratively improve a policy which is provably-stableby construction, while the second directly performs gradient-based optimizationon the parameters of the polynomial policy, and its closed-loop stability isverified a posteriori. We extend our approach to provide stability guaranteesin the presence of observation noise, which realistically arises due to errorsin the learned perception module. We evaluate our approach on severalunderactuated nonlinear systems, including pendula and quadrotors, showing thatour guarantees translate to empirical stability when controlling these systemsfrom images, while baseline approaches can fail to reliably stabilize thesystem.</description><author>Glen Chou, Russ Tedrake</author><pubDate>Thu, 28 Sep 2023 16:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12405v2</guid></item><item><title>HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs</title><link>http://arxiv.org/abs/2309.16524v1</link><description>Robots are becoming increasingly integrated into our lives, assisting us invarious tasks. To ensure effective collaboration between humans and robots, itis essential that they understand our intentions and anticipate our actions. Inthis paper, we propose a Human-Object Interaction (HOI) anticipation frameworkfor collaborative robots. We propose an efficient and robust transformer-basedmodel to detect and anticipate HOIs from videos. This enhanced anticipationempowers robots to proactively assist humans, resulting in more efficient andintuitive collaborations. Our model outperforms state-of-the-art results in HOIdetection and anticipation in VidHOI dataset with an increase of 1.76% and1.04% in mAP respectively while being 15.4 times faster. We showcase theeffectiveness of our approach through experimental results in a real robot,demonstrating that the robot's ability to anticipate HOIs is key for betterHuman-Robot Interaction. More information can be found on our project webpage:https://evm7.github.io/HOI4ABOT_page/</description><author>Esteve Valls Mascaro, Daniel Sliwowski, Dongheui Lee</author><pubDate>Thu, 28 Sep 2023 16:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16524v1</guid></item><item><title>Classical-to-quantum convolutional neural network transfer learning</title><link>http://arxiv.org/abs/2208.14708v2</link><description>Machine learning using quantum convolutional neural networks (QCNNs) hasdemonstrated success in both quantum and classical data classification. Inprevious studies, QCNNs attained a higher classification accuracy than theirclassical counterparts under the same training conditions in the few-parameterregime. However, the general performance of large-scale quantum models isdifficult to examine because of the limited size of quantum circuits, which canbe reliably implemented in the near future. We propose transfer learning as aneffective strategy for utilizing small QCNNs in the noisy intermediate-scalequantum era to the full extent. In the classical-to-quantum transfer learningframework, a QCNN can solve complex classification problems without requiring alarge-scale quantum circuit by utilizing a pre-trained classical convolutionalneural network (CNN). We perform numerical simulations of QCNN models withvarious sets of quantum convolution and pooling operations for MNIST dataclassification under transfer learning, in which a classical CNN is trainedwith Fashion-MNIST data. The results show that transfer learning from classicalto quantum CNN performs considerably better than purely classical transferlearning models under similar training conditions.</description><author>Juhyeon Kim, Joonsuk Huh, Daniel K. Park</author><pubDate>Thu, 28 Sep 2023 16:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.14708v2</guid></item><item><title>On the Role of Morphological Information for Contextual Lemmatization</title><link>http://arxiv.org/abs/2302.00407v2</link><description>Lemmatization is a natural language processing (NLP) task which consists ofproducing, from a given inflected word, its canonical form or lemma.Lemmatization is one of the basic tasks that facilitate downstream NLPapplications, and is of particular importance for high-inflected languages.Given that the process to obtain a lemma from an inflected word can beexplained by looking at its morphosyntactic category, including fine-grainedmorphosyntactic information to train contextual lemmatizers has become commonpractice, without considering whether that is the optimum in terms ofdownstream performance. In order to address this issue, in this paper weempirically investigate the role of morphological information to developcontextual lemmatizers in six languages within a varied spectrum ofmorphological complexity: Basque, Turkish, Russian, Czech, Spanish and English.Furthermore, and unlike the vast majority of previous work, we also evaluatelemmatizers in out-of-domain settings, which constitutes, after all, their mostcommon application use. The results of our study are rather surprising. Itturns out that providing lemmatizers with fine-grained morphological featuresduring training is not that beneficial, not even for agglutinative languages.In fact, modern contextual word representations seem to implicitly encodeenough morphological information to obtain competitive contextual lemmatizerswithout seeing any explicit morphological signal. Moreover, our experimentssuggest that the best lemmatizers out-of-domain are those using simple UPOStags or those trained without morphology and, finally, that current evaluationpractices for lemmatization are not adequate to clearly discriminate betweenmodels.</description><author>Olia Toporkov, Rodrigo Agerri</author><pubDate>Thu, 28 Sep 2023 16:29:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00407v2</guid></item><item><title>Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models</title><link>http://arxiv.org/abs/2309.16521v1</link><description>We propose a novel framework that combines deep generative time series modelswith decision theory for generating personalized treatment strategies. Itleverages historical patient trajectory data to jointly learn the generation ofrealistic personalized treatment and future outcome trajectories through deepgenerative time series models. In particular, our framework enables thegeneration of novel multivariate treatment strategies tailored to thepersonalized patient history and trained for optimal expected future outcomesbased on conditional expected utility maximization. We demonstrate ourframework by generating personalized insulin treatment strategies and bloodglucose predictions for hospitalized diabetes patients, showcasing thepotential of our approach for generating improved personalized treatmentstrategies. Keywords: deep generative model, probabilistic decision support,personalized treatment generation, insulin and blood glucose prediction</description><author>Manuel Schürch, Xiang Li, Ahmed Allam, Giulia Rathmes, Amina Mollaysa, Claudia Cavelti-Weder, Michael Krauthammer</author><pubDate>Thu, 28 Sep 2023 16:27:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16521v1</guid></item><item><title>AtomSurf : Surface Representation for Learning on Protein Structures</title><link>http://arxiv.org/abs/2309.16519v1</link><description>Recent advancements in Cryo-EM and protein structure prediction algorithmshave made large-scale protein structures accessible, paving the way for machinelearning-based functional annotations.The field of geometric deep learningfocuses on creating methods working on geometric data. An essential aspect oflearning from protein structures is representing these structures as ageometric object (be it a grid, graph, or surface) and applying a learningmethod tailored to this representation. The performance of a given approachwill then depend on both the representation and its corresponding learningmethod. In this paper, we investigate representing proteins as $\textit{3D meshsurfaces}$ and incorporate them into an established representation benchmark.Our first finding is that despite promising preliminary results, the surfacerepresentation alone does not seem competitive with 3D grids. Building on this,we introduce a synergistic approach, combining surface representations withgraph-based methods, resulting in a general framework that incorporates bothrepresentations in learning. We show that using this combination, we are ableto obtain state-of-the-art results across $\textit{all tested tasks}$. Our codeand data can be found online: https://github.com/Vincentx15/atom2D .</description><author>Vincent Mallet, Souhaib Attaiki, Maks Ovsjanikov</author><pubDate>Thu, 28 Sep 2023 16:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16519v1</guid></item><item><title>A Large-scale Dataset for Audio-Language Representation Learning</title><link>http://arxiv.org/abs/2309.11500v2</link><description>The AI community has made significant strides in developing powerfulfoundation models, driven by large-scale multimodal datasets. However, in theaudio representation learning community, the present audio-language datasetssuffer from limitations such as insufficient volume, simplistic content, andarduous collection procedures. To tackle these challenges, we present aninnovative and automatic audio caption generation pipeline based on a series ofpublic tools or APIs, and construct a large-scale, high-quality, audio-languagedataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. Todemonstrate the effectiveness of the proposed dataset, we train popular modelson our dataset and show performance improvement on various downstream tasks,namely, audio-language retrieval, audio captioning, environment classification.In addition, we establish a novel test set and provide a benchmark foraudio-text tasks. The proposed dataset will be released athttps://auto-acd.github.io/.</description><author>Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie</author><pubDate>Thu, 28 Sep 2023 16:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11500v2</guid></item><item><title>Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping</title><link>http://arxiv.org/abs/2309.16515v1</link><description>Deep Neural Networks (DNNs) that achieve human-level performance in generaltasks like object segmentation typically require supervised labels. Incontrast, humans are able to perform these tasks effortlessly withoutsupervision. To accomplish this, the human visual system makes use ofperceptual grouping. Understanding how perceptual grouping arises in anunsupervised manner is critical for improving both models of the visual system,and computer vision models. In this work, we propose a counterintuitiveapproach to unsupervised perceptual grouping and segmentation: that they arisebecause of neural noise, rather than in spite of it. We (1) mathematicallydemonstrate that under realistic assumptions, neural noise can be used toseparate objects from each other, and (2) show that adding noise in a DNNenables the network to segment images even though it was never trained on anysegmentation labels. Interestingly, we find that (3) segmenting objects usingnoise results in segmentation performance that aligns with the perceptualgrouping phenomena observed in humans. We introduce the Good Gestalt (GG)datasets -- six datasets designed to specifically test perceptual grouping, andshow that our DNN models reproduce many important phenomena in humanperception, such as illusory contours, closure, continuity, proximity, andocclusion. Finally, we (4) demonstrate the ecological plausibility of themethod by analyzing the sensitivity of the DNN to different magnitudes ofnoise. We find that some model variants consistently succeed with remarkablylow levels of neural noise ($\sigma&lt;0.001$), and surprisingly, that segmentingthis way requires as few as a handful of samples. Together, our results suggesta novel unsupervised segmentation method requiring few assumptions, a newexplanation for the formation of perceptual grouping, and a potential benefitof neural noise in the visual system.</description><author>Ben Lonnqvist, Zhengqing Wu, Michael H. Herzog</author><pubDate>Thu, 28 Sep 2023 16:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16515v1</guid></item><item><title>Gaining the Sparse Rewards by Exploring Binary Lottery Tickets in Spiking Neural Network</title><link>http://arxiv.org/abs/2309.13302v2</link><description>Spiking Neural Network (SNN) as a brain-inspired strategy receives lots ofattention because of the high-sparsity and low-power properties derived fromits inherent spiking information state. To further improve the efficiency ofSNN, some works declare that the Lottery Tickets (LTs) Hypothesis, whichindicates that the Artificial Neural Network (ANN) contains a subnetworkwithout sacrificing the performance of the original network, also exists inSNN. However, the spiking information handled by SNN has a natural similarityand affinity with binarization in sparsification. Therefore, to further exploreSNN efficiency, this paper focuses on (1) the presence or absence of LTs in thebinary SNN, and (2) whether the spiking mechanism is a superior strategy interms of handling binary information compared to simple model binarization. Tocertify these consumptions, a sparse training method is proposed to find BinaryWeights Spiking Lottery Tickets (BinW-SLT) under different network structures.Through comprehensive evaluations, we show that BinW-SLT could attain up to+5.86% and +3.17% improvement on CIFAR-10 and CIFAR-100 compared with binaryLTs, as well as achieve 1.86x and 8.92x energy saving compared withfull-precision SNN and ANN.</description><author>Hao Cheng, Jiahang Cao, Erjia Xiao, Pu Zhao, Mengshu Sun, Jiaxu Wang, Jize Zhang, Xue Lin, Bhavya Kailkhura, Kaidi Xu, Renjing Xu</author><pubDate>Thu, 28 Sep 2023 16:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13302v2</guid></item><item><title>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity</title><link>http://arxiv.org/abs/2309.16512v1</link><description>In this paper, we introduce a novel analysis of neural networks based ongeometric (Clifford) algebra and convex optimization. We show that optimalweights of deep ReLU neural networks are given by the wedge product of trainingsamples when trained with standard regularized loss. Furthermore, the trainingproblem reduces to convex optimization over wedge product features, whichencode the geometric structure of the training dataset. This structure is givenin terms of signed volumes of triangles and parallelotopes generated by datavectors. The convex problem finds a small subset of samples via $\ell_1$regularization to discover only relevant wedge product features. Our analysisprovides a novel perspective on the inner workings of deep neural networks andsheds light on the role of the hidden layers.</description><author>Mert Pilanci</author><pubDate>Thu, 28 Sep 2023 16:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16512v1</guid></item><item><title>Toloka Visual Question Answering Benchmark</title><link>http://arxiv.org/abs/2309.16511v1</link><description>In this paper, we present Toloka Visual Question Answering, a newcrowdsourced dataset allowing comparing performance of machine learning systemsagainst human level of expertise in the grounding visual question answeringtask. In this task, given an image and a textual question, one has to draw thebounding box around the object correctly responding to that question. Everyimage-question pair contains the response, with only one correct response perimage. Our dataset contains 45,199 pairs of images and questions in English,provided with ground truth bounding boxes, split into train and two testsubsets. Besides describing the dataset and releasing it under a CC BY license,we conducted a series of experiments on open source zero-shot baseline modelsand organized a multi-phase competition at WSDM Cup that attracted 48participants worldwide. However, by the time of paper submission, no machinelearning model outperformed the non-expert crowdsourcing baseline according tothe intersection over union evaluation score.</description><author>Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, Alisa Smirnova</author><pubDate>Thu, 28 Sep 2023 16:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16511v1</guid></item><item><title>Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis</title><link>http://arxiv.org/abs/2308.09830v3</link><description>This paper explores the integration of two AI subdisciplines employed in thedevelopment of artificial agents that exhibit intelligent behavior: LargeLanguage Models (LLMs) and Cognitive Architectures (CAs). We present threeintegration approaches, each grounded in theoretical models and supported bypreliminary empirical evidence. The modular approach, which introduces fourmodels with varying degrees of integration, makes use of chain-of-thoughtprompting, and draws inspiration from augmented LLMs, the Common Model ofCognition, and the simulation theory of cognition. The agency approach,motivated by the Society of Mind theory and the LIDA cognitive architecture,proposes the formation of agent collections that interact at micro and macrocognitive levels, driven by either LLMs or symbolic components. Theneuro-symbolic approach, which takes inspiration from the CLARION cognitivearchitecture, proposes a model where bottom-up learning extracts symbolicrepresentations from an LLM layer and top-down guidance utilizes symbolicrepresentations to direct prompt engineering in the LLM layer. These approachesaim to harness the strengths of both LLMs and CAs, while mitigating theirweaknesses, thereby advancing the development of more robust AI systems. Wediscuss the tradeoffs and challenges associated with each approach.</description><author>Oscar J. Romero, John Zimmerman, Aaron Steinfeld, Anthony Tomasic</author><pubDate>Thu, 28 Sep 2023 16:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09830v3</guid></item><item><title>CCEdit: Creative and Controllable Video Editing via Diffusion Models</title><link>http://arxiv.org/abs/2309.16496v1</link><description>In this work, we present CCEdit, a versatile framework designed to addressthe challenges of creative and controllable video editing. CCEdit accommodatesa wide spectrum of user editing requirements and enables enhanced creativecontrol through an innovative approach that decouples video structure andappearance. We leverage the foundational ControlNet architecture to preservestructural integrity, while seamlessly integrating adaptable temporal modulescompatible with state-of-the-art personalization techniques for text-to-imagegeneration, such as DreamBooth and LoRA.Furthermore, we introducereference-conditioned video editing, empowering users to exercise precisecreative control over video editing through the more manageable process ofediting key frames. Our extensive experimental evaluations confirm theexceptional functionality and editing capabilities of the proposed CCEditframework. Demo video is available athttps://www.youtube.com/watch?v=UQw4jq-igN4.</description><author>Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, Baining Guo</author><pubDate>Thu, 28 Sep 2023 16:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16496v1</guid></item><item><title>Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems</title><link>http://arxiv.org/abs/2309.16495v1</link><description>Searching for available parking spots in high-density urban centers is astressful task for drivers that can be mitigated by systems that know inadvance the nearest parking space available. To this end, image-based systems offer cost advantages over othersensor-based alternatives (e.g., ultrasonic sensors), requiring less physicalinfrastructure for installation and maintenance. Despite recent deep learning advances, deploying intelligent parkingmonitoring is still a challenge since most approaches involve collecting andlabeling large amounts of data, which is laborious and time-consuming. Ourstudy aims to uncover the challenges in creating a global framework, trainedusing publicly available labeled parking lot images, that performs accuratelyacross diverse scenarios, enabling the parking space monitoring as aready-to-use system to deploy in a new environment. Through exhaustiveexperiments involving different datasets and deep learning architectures,including fusion strategies and ensemble methods, we found that models trainedon diverse datasets can achieve 95\% accuracy without the burden of dataannotation and model training on the target parking lot</description><author>Andre Gustavo Hochuli, Jean Paul Barddal, Gillian Cezar Palhano, Leonardo Matheus Mendes, Paulo Ricardo Lisboa de Almeida</author><pubDate>Thu, 28 Sep 2023 15:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16495v1</guid></item><item><title>Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization</title><link>http://arxiv.org/abs/2309.16494v1</link><description>Recently, deep learning-based methods have dominated image dehazing domain.Although very competitive dehazing performance has been achieved withsophisticated models, effective solutions for extracting useful features arestill under-explored. In addition, non-local network, which has made abreakthrough in many vision tasks, has not been appropriately applied to imagedehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consistingof the multi-stream feature attention block (MSFAB) and cross non-local block(CNLB) is presented in this paper. We start with extracting richer features fordehazing. Specifically, we design a multi-stream feature extraction (MSFE)sub-block, which contains three parallel convolutions with different receptivefields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scalefeatures. Following MSFE, we employ an attention sub-block to make the modeladaptively focus on important channels/regions. The MSFE and attentionsub-blocks constitute our MSFAB. Then, we design a cross non-local block(CNLB), which can capture long-range dependencies beyond the query. Instead ofthe same input source of query branch, the key and value branches are enhancedby fusing more preceding features. CNLB is computation-friendly by leveraging aspatial pyramid down-sampling (SPDS) strategy to reduce the computation andmemory consumption without sacrificing the performance. Last but not least, anovel detail-focused contrastive regularization (DFCR) is presented byemphasizing the low-level details and ignoring the high-level semanticinformation in the representation space. Comprehensive experimental resultsdemonstrate that the proposed MRFNLN model outperforms recent state-of-the-artdehazing methods with less than 1.5 Million parameters.</description><author>Zewei He, Zixuan Chen, Ziqian Lu, Xuecheng Sun, Zhe-Ming Lu</author><pubDate>Thu, 28 Sep 2023 15:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16494v1</guid></item><item><title>Model Sparsity Can Simplify Machine Unlearning</title><link>http://arxiv.org/abs/2304.04934v8</link><description>In response to recent data regulation requirements, machine unlearning (MU)has emerged as a critical process to remove the influence of specific examplesfrom a given model. Although exact unlearning can be achieved through completemodel retraining using the remaining dataset, the associated computationalcosts have driven the development of efficient, approximate unlearningtechniques. Moving beyond data-centric MU approaches, our study introduces anovel model-based perspective: model sparsification via weight pruning, whichis capable of reducing the gap between exact unlearning and approximateunlearning. We show in both theory and practice that model sparsity can boostthe multi-criteria unlearning performance of an approximate unlearner, closingthe approximation gap, while continuing to be efficient. This leads to a new MUparadigm, termed prune first, then unlearn, which infuses a sparse model priorinto the unlearning process. Building on this insight, we also develop asparsity-aware unlearning method that utilizes sparsity regularization toenhance the training process of approximate unlearning. Extensive experimentsshow that our proposals consistently benefit MU in various unlearningscenarios. A notable highlight is the 77% unlearning efficacy gain offine-tuning (one of the simplest unlearning methods) when using sparsity-awareunlearning. Furthermore, we demonstrate the practical impact of our proposed MUmethods in addressing other machine learning challenges, such as defendingagainst backdoor attacks and enhancing transfer learning. Codes are availableat https://github.com/OPTML-Group/Unlearn-Sparse.</description><author>Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu</author><pubDate>Thu, 28 Sep 2023 15:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04934v8</guid></item><item><title>Asset Bundling for Wind Power Forecasting</title><link>http://arxiv.org/abs/2309.16492v1</link><description>The growing penetration of intermittent, renewable generation in US powergrids, especially wind and solar generation, results in increased operationaluncertainty. In that context, accurate forecasts are critical, especially forwind generation, which exhibits large variability and is historically harder topredict. To overcome this challenge, this work proposes a novelBundle-Predict-Reconcile (BPR) framework that integrates asset bundling,machine learning, and forecast reconciliation techniques. The BPR frameworkfirst learns an intermediate hierarchy level (the bundles), then predicts windpower at the asset, bundle, and fleet level, and finally reconciles allforecasts to ensure consistency. This approach effectively introduces anauxiliary learning task (predicting the bundle-level time series) to help themain learning tasks. The paper also introduces new asset-bundling criteria thatcapture the spatio-temporal dynamics of wind power time series. Extensivenumerical experiments are conducted on an industry-size dataset of 283 windfarms in the MISO footprint. The experiments consider short-term and day-aheadforecasts, and evaluates a large variety of forecasting models that includeweather predictions as covariates. The results demonstrate the benefits of BPR,which consistently and significantly improves forecast accuracy over baselines,especially at the fleet level.</description><author>Hanyu Zhang, Mathieu Tanneau, Chaofan Huang, V. Roshan Joseph, Shangkun Wang, Pascal Van Hentenryck</author><pubDate>Thu, 28 Sep 2023 15:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16492v1</guid></item><item><title>Towards Poisoning Fair Representations</title><link>http://arxiv.org/abs/2309.16487v1</link><description>Fair machine learning seeks to mitigate model prediction bias against certaindemographic subgroups such as elder and female. Recently, fair representationlearning (FRL) trained by deep neural networks has demonstrated superiorperformance, whereby representations containing no demographic information areinferred from the data and then used as the input to classification or otherdownstream tasks. Despite the development of FRL methods, their vulnerabilityunder data poisoning attack, a popular protocol to benchmark model robustnessunder adversarial scenarios, is under-explored. Data poisoning attacks havebeen developed for classical fair machine learning methods which incorporatefairness constraints into shallow-model classifiers. Nonetheless, these attacksfall short in FRL due to notably different fairness goals and modelarchitectures. This work proposes the first data poisoning framework attackingFRL. We induce the model to output unfair representations that contain as muchdemographic information as possible by injecting carefully crafted poisoningsamples into the training data. This attack entails a prohibitive bileveloptimization, wherefore an effective approximated solution is proposed. Atheoretical analysis on the needed number of poisoning samples is derived andsheds light on defending against the attack. Experiments on benchmark fairnessdatasets and state-of-the-art fair representation learning models demonstratethe superiority of our attack.</description><author>Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao</author><pubDate>Thu, 28 Sep 2023 15:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16487v1</guid></item><item><title>TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction</title><link>http://arxiv.org/abs/2303.04116v2</link><description>Data-driven simulation has become a favorable way to train and testautonomous driving algorithms. The idea of replacing the actual environmentwith a learned simulator has also been explored in model-based reinforcementlearning in the context of world models. In this work, we show data-driventraffic simulation can be formulated as a world model. We present TrafficBots,a multi-agent policy built upon motion prediction and end-to-end driving, andbased on TrafficBots we obtain a world model tailored for the planning moduleof autonomous vehicles. Existing data-driven traffic simulators are lackingconfigurability and scalability. To generate configurable behaviors, for eachagent we introduce a destination as navigational information, and atime-invariant latent personality that specifies the behavioral style. Toimprove the scalability, we present a new scheme of positional encoding forangles, allowing all agents to share the same vectorized context and the use ofan architecture based on dot-product attention. As a result, we can simulateall traffic participants seen in dense urban scenarios. Experiments on theWaymo open motion dataset show TrafficBots can simulate realistic multi-agentbehaviors and achieve good performance on the motion prediction task.</description><author>Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool</author><pubDate>Thu, 28 Sep 2023 15:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04116v2</guid></item><item><title>HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images</title><link>http://arxiv.org/abs/2309.16486v1</link><description>3D geo-information is of great significance for understanding the livingenvironment; however, 3D perception from remote sensing data, especially on alarge scale, is restricted. To tackle this problem, we propose a method formonocular height estimation from optical imagery, which is currently one of therichest sources of remote sensing data. As an ill-posed problem, monocularheight estimation requires well-designed networks for enhanced representationsto improve performance. Moreover, the distribution of height values islong-tailed with the low-height pixels, e.g., the background, as the head, andthus trained networks are usually biased and tend to underestimate buildingheights. To solve the problems, instead of formalizing the problem as aregression task, we propose HTC-DC Net following the classification-regressionparadigm, with the head-tail cut (HTC) and the distribution-based constraints(DCs) as the main contributions. HTC-DC Net is composed of the backbone networkas the feature extractor, the HTC-AdaBins module, and the hybrid regressionprocess. The HTC-AdaBins module serves as the classification phase to determinebins adaptive to each input image. It is equipped with a vision transformerencoder to incorporate local context with holistic information and involves anHTC to address the long-tailed problem in monocular height estimation forbalancing the performances of foreground and background pixels. The hybridregression process does the regression via the smoothing of bins from theclassification phase, which is trained via DCs. The proposed network is testedon three datasets of different resolutions, namely ISPRS Vaihingen (0.09 m),DFC19 (1.3 m) and GBH (3 m). Experimental results show the superiority of theproposed network over existing methods by large margins. Extensive ablationstudies demonstrate the effectiveness of each design component.</description><author>Sining Chen, Yilei Shi, Zhitong Xiong, Xiao Xiang Zhu</author><pubDate>Thu, 28 Sep 2023 15:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16486v1</guid></item><item><title>Rethinking Domain Generalization: Discriminability and Generalizability</title><link>http://arxiv.org/abs/2309.16483v1</link><description>Domain generalization (DG) endeavors to develop robust models that possessstrong generalizability while preserving excellent discriminability.Nonetheless, pivotal DG techniques tend to improve the feature generalizabilityby learning domain-invariant representations, inadvertently overlooking thefeature discriminability. On the one hand, the simultaneous attainment ofgeneralizability and discriminability of features presents a complex challenge,often entailing inherent contradictions. This challenge becomes particularlypronounced when domain-invariant features manifest reduced discriminabilityowing to the inclusion of unstable factors, \emph{i.e.,} spurious correlations.On the other hand, prevailing domain-invariant methods can be categorized ascategory-level alignment, susceptible to discarding indispensable featurespossessing substantial generalizability and narrowing intra-class variations.To surmount these obstacles, we rethink DG from a new perspective thatconcurrently imbues features with formidable discriminability and robustgeneralizability, and present a novel framework, namely, DiscriminativeMicroscopic Distribution Alignment (DMDA). DMDA incorporates two corecomponents: Selective Channel Pruning~(SCP) and Micro-level DistributionAlignment (MDA). Concretely, SCP attempts to curtail redundancy within neuralnetworks, prioritizing stable attributes conducive to accurate classification.This approach alleviates the adverse effect of spurious domain invariance andamplifies the feature discriminability. Besides, MDA accentuates micro-levelalignment within each class, going beyond mere category-level alignment. Thisstrategy accommodates sufficient generalizable features and facilitateswithin-class variations. Extensive experiments on four benchmark datasetscorroborate the efficacy of our method.</description><author>Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma, Yuan Luo</author><pubDate>Thu, 28 Sep 2023 15:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16483v1</guid></item><item><title>TinyMetaFed: Efficient Federated Meta-Learning for TinyML</title><link>http://arxiv.org/abs/2307.06822v3</link><description>The field of Tiny Machine Learning (TinyML) has made substantial advancementsin democratizing machine learning on low-footprint devices, such asmicrocontrollers. The prevalence of these miniature devices raises the questionof whether aggregating their knowledge can benefit TinyML applications.Federated meta-learning is a promising answer to this question, as it addressesthe scarcity of labeled data and heterogeneous data distribution across devicesin the real world. However, deploying TinyML hardware faces unique resourceconstraints, making existing methods impractical due to energy, privacy, andcommunication limitations. We introduce TinyMetaFed, a model-agnosticmeta-learning framework suitable for TinyML. TinyMetaFed facilitatescollaborative training of a neural network initialization that can be quicklyfine-tuned on new devices. It offers communication savings and privacyprotection through partial local reconstruction and Top-P% selectivecommunication, computational efficiency via online learning, and robustness toclient heterogeneity through few-shot learning. The evaluations on three TinyMLuse cases demonstrate that TinyMetaFed can significantly reduce energyconsumption and communication overhead, accelerate convergence, and stabilizethe training process.</description><author>Haoyu Ren, Xue Li, Darko Anicic, Thomas A. Runkler</author><pubDate>Thu, 28 Sep 2023 15:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06822v3</guid></item><item><title>Creating walls to avoid unwanted points in root finding and optimization</title><link>http://arxiv.org/abs/2309.11475v2</link><description>In root finding and optimization, there are many cases where there is aclosed set $A$ one likes that the sequence constructed by one's favouritemethod will not converge to A (here, we do not assume extra properties on $A$such as being convex or connected). For example, if one wants to find roots,and one chooses initial points in the basin of attraction for 1 root $x^*$ (afact which one may not know before hand), then one will always end up in thatroot. In this case, one would like to have a mechanism to avoid this point$z^*$ in the next runs of one's algorithm. In this paper, we propose two new methods aiming to achieve this. In thefirst method, we divide the cost function by an appropriate power of thedistance function to $A$. This idea is inspired by how one would try to findall roots of a function in 1 variable. In the second method, which is moresuitable for constrained optimization, we redefine the value of the function tobe a big constant on $A$. We also propose, based on this, an algorithm toescape the basin of attraction of a component of positive dimension to reachanother component. As an application, we prove a rigorous guarantee for findingroots of a meromorphic function of 1 complex variable in a given domain. Along the way, we compare with main existing relevant methods in the currentliterature. We provide several examples in various different settings toillustrate the usefulness of the new approach.</description><author>Tuyen Trung Truong</author><pubDate>Thu, 28 Sep 2023 15:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11475v2</guid></item></channel></rss>