<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 08 Nov 2024 13:00:44 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>http://arxiv.org/abs/2411.05007v1</link><description>Diffusion models have been proven highly effective at generating high-qualityimages. However, as these models grow larger, they require significantly morememory and suffer from higher latency, posing substantial challenges fordeployment. In this work, we aim to accelerate diffusion models by quantizingtheir weights and activations to 4 bits. At such an aggressive level, bothweights and activations are highly sensitive, where conventional post-trainingquantization methods for large language models like smoothing becomeinsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bitquantization paradigm. Different from smoothing which redistributes outliersbetween weights and activations, our approach absorbs these outliers using alow-rank branch. We first consolidate the outliers by shifting them fromactivations to weights, then employ a high-precision low-rank branch to take inthe weight outliers with Singular Value Decomposition (SVD). This process easesthe quantization on both sides. However, na\"{\i}vely running the low-rankbranch independently incurs significant overhead due to extra data movement ofactivations, negating the quantization speedup. To address this, we co-designan inference engine Nunchaku that fuses the kernels of the low-rank branch intothose of the low-bit branch to cut off redundant memory access. It can alsoseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need forre-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1validate the effectiveness of SVDQuant in preserving image quality. We reducethe memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GBlaptop 4090 GPU, paving the way for more interactive applications on PCs. Ourquantization library and inference engine are open-sourced.</description><author>Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han</author><pubDate>Thu, 07 Nov 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05007v1</guid></item><item><title>ProEdit: Simple Progression is All You Need for High-Quality 3D Scene Editing</title><link>http://arxiv.org/abs/2411.05006v1</link><description>This paper proposes ProEdit - a simple yet effective framework forhigh-quality 3D scene editing guided by diffusion distillation in a novelprogressive manner. Inspired by the crucial observation that multi-viewinconsistency in scene editing is rooted in the diffusion model's largefeasible output space (FOS), our framework controls the size of FOS and reducesinconsistency by decomposing the overall editing task into several subtasks,which are then executed progressively on the scene. Within this framework, wedesign a difficulty-aware subtask decomposition scheduler and an adaptive 3DGaussian splatting (3DGS) training strategy, ensuring high quality andefficiency in performing each subtask. Extensive evaluation shows that ourProEdit achieves state-of-the-art results in various scenes and challengingediting tasks, all through a simple framework without any expensive orsophisticated add-ons like distillation losses, components, or trainingprocedures. Notably, ProEdit also provides a new way to control, preview, andselect the "aggressivity" of editing operation during the editing process.</description><author>Jun-Kun Chen, Yu-Xiong Wang</author><pubDate>Thu, 07 Nov 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05006v1</guid></item><item><title>Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models</title><link>http://arxiv.org/abs/2411.05005v1</link><description>Beyond high-fidelity image synthesis, diffusion models have recentlyexhibited promising results in dense visual perception tasks. However, mostexisting work treats diffusion models as a standalone component for perceptiontasks, employing them either solely for off-the-shelf data augmentation or asmere feature extractors. In contrast to these isolated and thus sub-optimalefforts, we introduce a unified, versatile, diffusion-based framework,Diff-2-in-1, that can simultaneously handle both multi-modal data generationand dense visual perception, through a unique exploitation of thediffusion-denoising process. Within this framework, we further enhancediscriminative visual perception via multi-modal generation, by utilizing thedenoising network to create multi-modal data that mirror the distribution ofthe original training set. Importantly, Diff-2-in-1 optimizes the utilizationof the created diverse and faithful data by leveraging a novel self-improvinglearning mechanism. Comprehensive experimental evaluations validate theeffectiveness of our framework, showcasing consistent performance improvementsacross various discriminative backbones and high-quality multi-modal datageneration characterized by both realism and usefulness.</description><author>Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, Yu-Xiong Wang</author><pubDate>Thu, 07 Nov 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05005v1</guid></item><item><title>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</title><link>http://arxiv.org/abs/2411.05003v1</link><description>Recently, breakthroughs in video modeling have allowed for controllablecamera trajectories in generated videos. However, these methods cannot bedirectly applied to user-provided videos that are not generated by a videomodel. In this paper, we present ReCapture, a method for generating new videoswith novel camera trajectories from a single user-provided video. Our methodallows us to re-generate the reference video, with all its existing scenemotion, from vastly different angles and with cinematic camera motion. Notably,using our method we can also plausibly hallucinate parts of the scene that werenot observable in the reference video. Our method works by (1) generating anoisy anchor video with a new camera trajectory using multiview diffusionmodels or depth-based point cloud rendering and then (2) regenerating theanchor video into a clean and temporally consistent reangled video using ourproposed masked video fine-tuning technique.</description><author>David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, Nataniel Ruiz</author><pubDate>Thu, 07 Nov 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05003v1</guid></item><item><title>MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</title><link>http://arxiv.org/abs/2406.00922v3</link><description>Users typically engage with LLMs interactively, yet most existing benchmarksevaluate them in a static, single-turn format, posing reliability concerns ininteractive scenarios. We identify a key obstacle towards reliability: LLMs aretrained to answer any question, even with incomplete context or insufficientknowledge. In this paper, we propose to change the static paradigm to aninteractive one, develop systems that proactively ask questions to gather moreinformation and respond reliably, and introduce an benchmark - MediQ - toevaluate question-asking ability in LLMs. MediQ simulates clinical interactionsconsisting of a Patient System and an adaptive Expert System; with potentiallyincomplete initial information, the Expert refrains from making diagnosticdecisions when unconfident, and instead elicits missing details via follow-upquestions. We provide a pipeline to convert single-turn medical benchmarks intoan interactive format. Our results show that directly promptingstate-of-the-art LLMs to ask questions degrades performance, indicating thatadapting LLMs to proactive information-seeking settings is nontrivial. Weexperiment with abstention strategies to better estimate model confidence anddecide when to ask questions, improving diagnostic accuracy by 22.3%; however,performance still lags compared to an (unrealistic in practice) upper boundwith complete information upfront. Further analyses show improved interactiveperformance with filtering irrelevant contexts and reformatting conversations.Overall, we introduce a novel problem towards LLM reliability, an interactiveMediQ benchmark and a novel question-asking system, and highlight directions toextend LLMs' information-seeking abilities in critical domains.</description><author>Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov</author><pubDate>Thu, 07 Nov 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00922v3</guid></item><item><title>Analyzing The Language of Visual Tokens</title><link>http://arxiv.org/abs/2411.05001v1</link><description>With the introduction of transformer-based models for vision and languagetasks, such as LLaVA and Chameleon, there has been renewed interest in thediscrete tokenized representation of images. These models often treat imagepatches as discrete tokens, analogous to words in natural language, learningjoint alignments between visual and human languages. However, little is knownabout the statistical behavior of these visual languages - whether they followsimilar frequency distributions, grammatical structures, or topologies asnatural languages. In this paper, we take a natural-language-centric approachto analyzing discrete visual languages and uncover striking similarities andfundamental differences. We demonstrate that, although visual languages adhereto Zipfian distributions, higher token innovation drives greater entropy andlower compression, with tokens predominantly representing object parts,indicating intermediate granularity. We also show that visual languages lackcohesive grammatical structures, leading to higher perplexity and weakerhierarchical organization compared to natural languages. Finally, wedemonstrate that, while vision models align more closely with natural languagesthan other models, this alignment remains significantly weaker than thecohesion found within natural languages. Through these experiments, wedemonstrate how understanding the statistical properties of discrete visuallanguages can inform the design of more effective computer vision models.</description><author>David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell</author><pubDate>Thu, 07 Nov 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05001v1</guid></item><item><title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title><link>http://arxiv.org/abs/2411.05000v1</link><description>As the context limits of Large Language Models (LLMs) increase, the range ofpossible applications and downstream functions broadens. In many real-worldtasks, decisions depend on details scattered across collections of oftendisparate documents containing mostly irrelevant information. Long-context LLMsappear well-suited to this form of complex information retrieval and reasoning,which has traditionally proven costly and time-consuming. However, although thedevelopment of longer context models has seen rapid gains in recent years, ourunderstanding of how effectively LLMs use their context has not kept pace. Toaddress this, we conduct a set of retrieval experiments designed to evaluatethe capabilities of 17 leading LLMs, such as their ability to follow threads ofinformation through the context window. Strikingly, we find that many modelsare remarkably threadsafe: capable of simultaneously following multiple threadswithout significant loss in performance. Still, for many models, we find theeffective context limit is significantly shorter than the supported contextlength, with accuracy decreasing as the context window grows. Our study alsohighlights the important point that token counts from different tokenizersshould not be directly compared -- they often correspond to substantiallydifferent numbers of written characters. We release our code and long-contextexperimental data.</description><author>Jonathan Roberts, Kai Han, Samuel Albanie</author><pubDate>Thu, 07 Nov 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05000v1</guid></item><item><title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</title><link>http://arxiv.org/abs/2411.04999v1</link><description>Significant progress has been made in open-vocabulary mobile manipulation,where the goal is for a robot to perform tasks in any environment given anatural language description. However, most current systems assume a staticenvironment, which limits the system's applicability in real-world scenarioswhere environments frequently change due to human intervention or the robot'sown actions. In this work, we present DynaMem, a new approach to open-worldmobile manipulation that uses a dynamic spatio-semantic memory to represent arobot's environment. DynaMem constructs a 3D data structure to maintain adynamic memory of point clouds, and answers open-vocabulary object localizationqueries using multimodal LLMs or open-vocabulary features generated bystate-of-the-art vision-language models. Powered by DynaMem, our robots canexplore novel environments, search for objects not found in memory, andcontinuously update the memory as objects move, appear, or disappear in thescene. We run extensive experiments on the Stretch SE3 robots in three real andnine offline scenes, and achieve an average pick-and-drop success rate of 70%on non-stationary objects, which is more than a 2x improvement overstate-of-the-art static systems. Our code as well as our experiment anddeployment videos are open sourced and can be found on our project website:https://dynamem.github.io/</description><author>Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</author><pubDate>Thu, 07 Nov 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04999v1</guid></item><item><title>HourVideo: 1-Hour Video-Language Understanding</title><link>http://arxiv.org/abs/2411.04998v1</link><description>We present HourVideo, a benchmark dataset for hour-long video-languageunderstanding. Our dataset consists of a novel task suite comprisingsummarization, perception (recall, tracking), visual reasoning (spatial,temporal, predictive, causal, counterfactual), and navigation (room-to-room,object retrieval) tasks. HourVideo includes 500 manually curated egocentricvideos from the Ego4D dataset, spanning durations of 20 to 120 minutes, andfeatures 12,976 high-quality, five-way multiple-choice questions. Benchmarkingresults reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achievemarginal improvements over random chance. In stark contrast, human expertssignificantly outperform the state-of-the-art long-context multimodal model,Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodalcapabilities. Our benchmark, evaluation toolkit, prompts, and documentation areavailable at https://hourvideo.stanford.edu</description><author>Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei</author><pubDate>Thu, 07 Nov 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04998v1</guid></item><item><title>LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</title><link>http://arxiv.org/abs/2411.04997v1</link><description>CLIP is one of the most important multimodal foundational models today. Whatpowers CLIP's capabilities? The rich supervision signals provided by naturallanguage, the carrier of human knowledge, shape a powerful cross-modalrepresentation space. However, with the rapid advancements in large languagemodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension andgeneration are continually being pushed. This raises an intriguing question:can the capabilities of LLMs be harnessed to further improve multimodalrepresentation learning? The potential benefits of incorporating LLMs into CLIPare clear. LLMs' strong textual understanding can fundamentally improve CLIP'sability to handle image captions, drastically enhancing its ability to processlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMsare trained on a vast corpus of text, possessing open-world knowledge. Thisallows them to expand on caption information during training, increasing theefficiency of the learning process. In this paper, we propose LLM2CLIP, a novelapproach that embraces the power of LLMs to unlock CLIP's potential. Byfine-tuning the LLM in the caption space with contrastive learning, we extractits textual capabilities into the output embeddings, significantly improvingthe output layer's textual discriminability. We then design an efficienttraining process where the fine-tuned LLM acts as a powerful teacher for CLIP'svisual encoder. Thanks to the LLM's presence, we can now incorporate longer andmore complex captions without being restricted by vanilla CLIP's text encoder'scontext window and ability limitations. Our experiments demonstrate that thisapproach brings substantial improvements in cross-modal tasks.</description><author>Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu</author><pubDate>Thu, 07 Nov 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04997v1</guid></item><item><title>Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models</title><link>http://arxiv.org/abs/2411.04996v1</link><description>The development of large language models (LLMs) has expanded to multi-modalsystems capable of processing text, images, and speech within a unifiedframework. Training these models demands significantly larger datasets andcomputational resources compared to text-only LLMs. To address the scalingchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modaltransformer architecture that significantly reduces pretraining computationalcosts. MoT decouples non-embedding parameters of the model by modality --including feed-forward networks, attention matrices, and layer normalization --enabling modality-specific processing with global self-attention over the fullinput sequence. We evaluate MoT across multiple settings and model scales. Inthe Chameleon 7B setting (autoregressive text-and-image generation), MoTmatches the dense baseline's performance using only 55.8\% of the FLOPs. Whenextended to include speech, MoT reaches speech performance comparable to thedense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, wheretext and image are trained with different objectives, a 7B MoT model matchesthe image modality performance of the dense baseline with one third of theFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key imagegeneration metrics. System profiling further highlights MoT's practicalbenefits, achieving dense baseline image quality in 47.2\% of the wall-clocktime and text quality in 75.6\% of the wall-clock time (measured on AWSp4de.24xlarge instances with NVIDIA A100 GPUs).</description><author>Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin</author><pubDate>Thu, 07 Nov 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04996v1</guid></item><item><title>LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation</title><link>http://arxiv.org/abs/2411.04995v1</link><description>Neural fields or implicit neural representations (INRs) have attractedsignificant attention in machine learning and signal processing due to theirefficient continuous representation of images and 3D volumes. In this work, webuild on INRs and introduce a coordinate-based local processing framework forsolving imaging inverse problems, termed LoFi (Local Field). Unlikeconventional methods for image reconstruction, LoFi processes local informationat each coordinate \textit{separately} by multi-layer perceptrons (MLPs),recovering the object at that specific coordinate. Similar to INRs, LoFi canrecover images at any continuous coordinate, enabling image reconstruction atmultiple resolutions. With comparable or better performance than standard CNNsfor image reconstruction, LoFi achieves excellent generalization toout-of-distribution data and memory usage almost independent of imageresolution. Remarkably, training on $1024 \times 1024$ images requires just 3GBof memory -- over 20 times less than the memory typically needed by standardCNNs. Additionally, LoFi's local design allows it to train on extremely smalldatasets with less than 10 samples, without overfitting or the need forregularization or early stopping. Finally, we use LoFi as a denoising prior ina plug-and-play framework for solving general inverse problems to benefit fromits continuous image representation and strong generalization. Although trainedon low-resolution images, LoFi can be used as a low-dimensional prior to solveinverse problems at any resolution. We validate our framework across a varietyof imaging modalities, from low-dose computed tomography to radiointerferometric imaging.</description><author>AmirEhsan Khorashadizadeh, Tobías I. Liaudat, Tianlin Liu, Jason D. McEwen, Ivan Dokmanić</author><pubDate>Thu, 07 Nov 2024 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04995v1</guid></item><item><title>Public Procurement for Responsible AI? Understanding U.S. Cities' Practices, Challenges, and Needs</title><link>http://arxiv.org/abs/2411.04994v1</link><description>Most AI tools adopted by governments are not developed internally, butinstead are acquired from third-party vendors in a process called publicprocurement. While scholars and regulatory proposals have recently turnedtowards procurement as a site of intervention to encourage responsible AIgovernance practices, little is known about the practices and needs of cityemployees in charge of AI procurement. In this paper, we present findings fromsemi-structured interviews with 18 city employees across 7 US cities. We findthat AI acquired by cities often does not go through a conventional publicprocurement process, posing challenges to oversight and governance. We identifyfive key types of challenges to leveraging procurement for responsible AI thatcity employees face when interacting with colleagues, AI vendors, and membersof the public. We conclude by discussing recommendations and implications forgovernments, researchers, and policymakers.</description><author>Nari Johnson, Elise Silva, Harrison Leon, Motahhare Eslami, Beth Schwanke, Ravit Dotan, Hoda Heidari</author><pubDate>Thu, 07 Nov 2024 18:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04994v1</guid></item><item><title>Which bits went where? Past and future transfer entropy decomposition with the information bottleneck</title><link>http://arxiv.org/abs/2411.04992v1</link><description>Whether the system under study is a shoal of fish, a collection of neurons,or a set of interacting atmospheric and oceanic processes, transfer entropymeasures the flow of information between time series and can detect possiblecausal relationships. Much like mutual information, transfer entropy isgenerally reported as a single value summarizing an amount of shared variation,yet a more fine-grained accounting might illuminate much about the processesunder study. Here we propose to decompose transfer entropy and localize thebits of variation on both sides of information flow: that of the originatingprocess's past and that of the receiving process's future. We employ theinformation bottleneck (IB) to compress the time series and identify thetransferred entropy. We apply our method to decompose the transfer entropy inseveral synthetic recurrent processes and an experimental mouse dataset ofconcurrent behavioral and neural activity. Our approach highlights the nuanceddynamics within information flow, laying a foundation for future explorationsinto the intricate interplay of temporal processes in complex systems.</description><author>Kieran A. Murphy, Zhuowen Yin, Dani S. Bassett</author><pubDate>Thu, 07 Nov 2024 18:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04992v1</guid></item><item><title>Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives</title><link>http://arxiv.org/abs/2411.04991v1</link><description>The Bradley-Terry (BT) model is a common and successful practice in rewardmodeling for Large Language Model (LLM) alignment. However, it remains unclearwhy this model -- originally developed for multi-player stochastic gamematching -- can be adopted to convert pairwise response comparisons to rewardvalues and make predictions. Especially given the fact that only a limitednumber of prompt-response pairs are sparsely compared with others. In thispaper, we first revisit the foundations of using BT models in reward modeling,and establish the convergence rate of BT reward models based on deep neuralnetworks using embeddings, providing a theoretical foundation for their use.Despite theoretically sound, we argue that the BT model is not a necessarychoice from the perspective of downstream optimization. This is because areward model only needs to preserve the correct ranking predictions through amonotonic transformation of the true reward. We highlight the critical conceptof order consistency in reward modeling and demonstrate that the BT modelpossesses this property. Consequently, we propose a simple and straightforwardupper-bound algorithm, compatible with off-the-shelf binary classifiers, as analternative order-consistent reward modeling objective. To offer practicalinsights, we empirically evaluate the performance of these different rewardmodeling approaches across more than 12,000 experimental setups, using $6$ baseLLMs, $2$ datasets, and diverse annotation designs that vary in quantity,quality, and pairing choices in preference annotations.</description><author>Hao Sun, Yunyi Shen, Jean-Francois Ton</author><pubDate>Thu, 07 Nov 2024 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04991v1</guid></item><item><title>Clustering in Causal Attention Masking</title><link>http://arxiv.org/abs/2411.04990v1</link><description>This work presents a modification of the self-attention dynamics proposed byGeshkovski et al. (arXiv:2312.10794) to better reflect the practicallyrelevant, causally masked attention used in transformer architectures forgenerative AI. This modification translates into an interacting particle systemthat cannot be interpreted as a mean-field gradient flow. Despite this loss ofstructure, we significantly strengthen the results of Geshkovski et al.(arXiv:2312.10794) in this context: While previous rigorous results focused oncases where all three matrices (Key, Query, and Value) were scaled identities,we prove asymptotic convergence to a single cluster for arbitrary key-querymatrices and a value matrix equal to the identity. Additionally, we establish aconnection to the classical R\'enyi parking problem from combinatorial geometryto make initial theoretical steps towards demonstrating the existence ofmeta-stable states.</description><author>Nikita Karagodin, Yury Polyanskiy, Philippe Rigollet</author><pubDate>Thu, 07 Nov 2024 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04990v1</guid></item><item><title>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</title><link>http://arxiv.org/abs/2411.04989v1</link><description>Methods for image-to-video generation have achieved impressive,photo-realistic quality. However, adjusting specific elements in generatedvideos, such as object motion or camera movement, is often a tedious process oftrial and error, e.g., involving re-generating videos with different randomseeds. Recent techniques address this issue by fine-tuning a pre-trained modelto follow conditioning signals, such as bounding boxes or point trajectories.Yet, this fine-tuning procedure can be computationally expensive, and itrequires datasets with annotated object motion, which can be difficult toprocure. In this work, we introduce SG-I2V, a framework for controllableimage-to-video generation that is self-guided$\unicode{x2013}$offeringzero-shot control by relying solely on the knowledge present in a pre-trainedimage-to-video diffusion model without the need for fine-tuning or externalknowledge. Our zero-shot method outperforms unsupervised baselines while beingcompetitive with supervised models in terms of visual quality and motionfidelity.</description><author>Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell</author><pubDate>Thu, 07 Nov 2024 18:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04989v1</guid></item><item><title>Few-Shot Task Learning through Inverse Generative Modeling</title><link>http://arxiv.org/abs/2411.04987v1</link><description>Learning the intents of an agent, defined by its goals or motion style, isoften extremely challenging from just a few examples. We refer to this problemas task concept learning and present our approach, Few-Shot Task Learningthrough Inverse Generative Modeling (FTL-IGM), which learns new task conceptsby leveraging invertible neural generative models. The core idea is to pretraina generative model on a set of basic concepts and their demonstrations. Then,given a few demonstrations of a new concept (such as a new goal or a newaction), our method learns the underlying concepts through backpropagationwithout updating the model weights, thanks to the invertibility of thegenerative model. We evaluate our method in five domains -- objectrearrangement, goal-oriented navigation, motion caption of human actions,autonomous driving, and real-world table-top manipulation. Our experimentalresults demonstrate that via the pretrained generative model, we successfullylearn novel concepts and generate agent plans or motion corresponding to theseconcepts in (1) unseen environments and (2) in composition with trainingconcepts.</description><author>Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal</author><pubDate>Thu, 07 Nov 2024 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04987v1</guid></item><item><title>The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities</title><link>http://arxiv.org/abs/2411.04986v1</link><description>Modern language models can process inputs across diverse languages andmodalities. We hypothesize that models acquire this capability through learninga shared representation space across heterogeneous data types (e.g., differentlanguages and modalities), which places semantically similar inputs near oneanother, even if they are from different modalities/languages. We term this thesemantic hub hypothesis, following the hub-and-spoke model from neuroscience(Patterson et al., 2007) which posits that semantic knowledge in the humanbrain is organized through a transmodal semantic "hub" which integratesinformation from various modality-specific "spokes" regions. We first show thatmodel representations for semantically equivalent inputs in different languagesare similar in the intermediate layers, and that this space can be interpretedusing the model's dominant pretraining language via the logit lens. Thistendency extends to other data types, including arithmetic expressions, code,and visual/audio inputs. Interventions in the shared representation space inone data type also predictably affect model outputs in other data types,suggesting that this shared representations space is not simply a vestigialbyproduct of large-scale training on broad data, but something that is activelyutilized by the model during input processing.</description><author>Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim</author><pubDate>Thu, 07 Nov 2024 18:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04986v1</guid></item><item><title>Planar Reflection-Aware Neural Radiance Fields</title><link>http://arxiv.org/abs/2411.04984v1</link><description>Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities inreconstructing complex scenes with high fidelity. However, NeRF's viewdependency can only handle low-frequency reflections. It falls short whenhandling complex planar reflections, often interpreting them as erroneous scenegeometries and leading to duplicated and inaccurate scene representations. Toaddress this challenge, we introduce a reflection-aware NeRF that jointlymodels planar reflectors, such as windows, and explicitly casts reflected raysto capture the source of the high-frequency reflections. We query a singleradiance field to render the primary color and the source of the reflection. Wepropose a sparse edge regularization to help utilize the true sources ofreflections for rendering planar reflections rather than creating a duplicatealong the primary ray at the same depth. As a result, we obtain accurate scenegeometry. Rendering along the primary ray results in a clean, reflection-freeview, while explicitly rendering along the reflected ray allows us toreconstruct highly detailed reflections. Our extensive quantitative andqualitative evaluations of real-world datasets demonstrate our method'senhanced performance in accurately handling reflections.</description><author>Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf</author><pubDate>Thu, 07 Nov 2024 18:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04984v1</guid></item><item><title>DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning</title><link>http://arxiv.org/abs/2411.04983v1</link><description>The ability to predict future outcomes given control actions is fundamentalfor physical reasoning. However, such predictive models, often called worldmodels, have proven challenging to learn and are typically developed fortask-specific solutions with online policy learning. We argue that the truepotential of world models lies in their ability to reason and plan acrossdiverse problems using only passive data. Concretely, we require world modelsto have the following three properties: 1) be trainable on offline,pre-collected trajectories, 2) support test-time behavior optimization, and 3)facilitate task-agnostic reasoning. To realize this, we present DINO WorldModel (DINO-WM), a new method to model visual dynamics without reconstructingthe visual world. DINO-WM leverages spatial patch features pre-trained withDINOv2, enabling it to learn from offline behavioral trajectories by predictingfuture patch features. This design allows DINO-WM to achieve observationalgoals through action sequence optimization, facilitating task-agnostic behaviorplanning by treating desired goal patch features as prediction targets. Weevaluate DINO-WM across various domains, including maze navigation, tabletoppushing, and particle manipulation. Our experiments demonstrate that DINO-WMcan generate zero-shot behavioral solutions at test time without relying onexpert demonstrations, reward modeling, or pre-learned inverse models. Notably,DINO-WM exhibits strong generalization capabilities compared to priorstate-of-the-art work, adapting to diverse task families such as arbitrarilyconfigured mazes, push manipulation with varied object shapes, andmulti-particle scenarios.</description><author>Gaoyue Zhou, Hengkai Pan, Yann LeCun, Lerrel Pinto</author><pubDate>Thu, 07 Nov 2024 18:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04983v1</guid></item><item><title>Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries</title><link>http://arxiv.org/abs/2411.04981v1</link><description>Security experts reverse engineer (decompile) binary code to identifycritical security vulnerabilities. The limited access to source code in vitalsystems - such as firmware, drivers, and proprietary software used in CriticalInfrastructures (CI) - makes this analysis even more crucial on the binarylevel. Even with available source code, a semantic gap persists aftercompilation between the source and the binary code executed by the processor.This gap may hinder the detection of vulnerabilities in source code. That beingsaid, current research on Large Language Models (LLMs) overlooks thesignificance of decompiled binaries in this area by focusing solely on sourcecode. In this work, we are the first to empirically uncover the substantialsemantic limitations of state-of-the-art LLMs when it comes to analyzingvulnerabilities in decompiled binaries, largely due to the absence of relevantdatasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binarycode vulnerability dataset. Our dataset is multi-architecture andmulti-optimization, focusing on C/C++ due to their wide usage in CI andassociation with numerous vulnerabilities. Specifically, we curate 150,872samples of vulnerable and non-vulnerable decompiled binary code for the task of(i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv)recovering function names in the domain of decompiled binaries. Subsequently,we fine-tune state-of-the-art LLMs using DeBinVul and report on a performanceincrease of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, andCodeGen2 respectively, in detecting binary code vulnerabilities. Additionally,using DeBinVul, we report a high performance of 80-90% on the vulnerabilityclassification task. Furthermore, we report improved performance in functionname recovery and vulnerability description tasks.</description><author>Dylan Manuel, Nafis Tanveer Islam, Joseph Khoury, Ana Nunez, Elias Bou-Harb, Peyman Najafirad</author><pubDate>Thu, 07 Nov 2024 18:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04981v1</guid></item><item><title>Noisy Zero-Shot Coordination: Breaking The Common Knowledge Assumption In Zero-Shot Coordination Games</title><link>http://arxiv.org/abs/2411.04976v1</link><description>Zero-shot coordination (ZSC) is a popular setting for studying the ability ofreinforcement learning (RL) agents to coordinate with novel partners. Prior ZSCformulations assume the $\textit{problem setting}$ is common knowledge: eachagent knows the underlying Dec-POMDP, knows others have this knowledge, and soon ad infinitum. However, this assumption rarely holds in complex real-worldsettings, which are often difficult to fully and correctly specify. Hence, insettings where this common knowledge assumption is invalid, agents trainedusing ZSC methods may not be able to coordinate well. To address thislimitation, we formulate the $\textit{noisy zero-shot coordination}$ (NZSC)problem. In NZSC, agents observe different noisy versions of the ground truthDec-POMDP, which are assumed to be distributed according to a fixed noisemodel. Only the distribution of ground truth Dec-POMDPs and the noise model arecommon knowledge. We show that a NZSC problem can be reduced to a ZSC problemby designing a meta-Dec-POMDP with an augmented state space consisting of allthe ground-truth Dec-POMDPs. For solving NZSC problems, we propose a simple andflexible meta-learning method called NZSC training, in which the agents aretrained across a distribution of coordination problems - which they only get toobserve noisy versions of. We show that with NZSC training, RL agents can betrained to coordinate well with novel partners even when the (exact) problemsetting of the coordination is not common knowledge.</description><author>Usman Anwar, Ashish Pandian, Jia Wan, David Krueger, Jakob Foerster</author><pubDate>Thu, 07 Nov 2024 18:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04976v1</guid></item><item><title>SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference</title><link>http://arxiv.org/abs/2411.04975v1</link><description>We present SuffixDecoding, a novel model-free approach to accelerating largelanguage model (LLM) inference through speculative decoding. Unlike existingmethods that rely on draft models or specialized decoding heads, SuffixDecodingleverages suffix trees built from previously generated outputs to efficientlypredict candidate token sequences. Our approach enables flexibletree-structured speculation without the overhead of maintaining andorchestrating additional models. SuffixDecoding builds and dynamically updatessuffix trees to capture patterns in the generated text, using them to constructspeculation trees through a principled scoring mechanism based on empiricaltoken frequencies. SuffixDecoding requires only CPU memory which is plentifuland underutilized on typical LLM serving nodes. We demonstrate thatSuffixDecoding achieves competitive speedups compared to model-based approachesacross diverse workloads including open-domain chat, code generation, andtext-to-SQL tasks. For open-ended chat and code generation tasks,SuffixDecoding achieves up to $1.4\times$ higher output throughput thanSpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For aproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to$2.9\times$ higher output throughput and $3\times$ lower latency thanspeculative decoding. Our evaluation shows that SuffixDecoding maintains highacceptance rates even with small reference corpora of 256 examples, whilecontinuing to improve performance as more historical outputs are incorporated.</description><author>Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao</author><pubDate>Thu, 07 Nov 2024 18:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04975v1</guid></item><item><title>AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation</title><link>http://arxiv.org/abs/2411.04967v1</link><description>Neural network architecture design requires making many crucial decisions.The common desiderata is that similar decisions, with little modifications, canbe reused in a variety of tasks and applications. To satisfy that,architectures must provide promising latency and performance trade-offs,support a variety of tasks, scale efficiently with respect to the amounts ofdata and compute, leverage available data from other tasks, and efficientlysupport various hardware. To this end, we introduce AsCAN -- a hybridarchitecture, combining both convolutional and transformer blocks. We revisitthe key design principles of hybrid architectures and propose a simple andeffective \emph{asymmetric} architecture, where the distribution ofconvolutional and transformer blocks is \emph{asymmetric}, containing moreconvolutional blocks in the earlier stages, followed by more transformer blocksin later stages. AsCAN supports a variety of tasks: recognition, segmentation,class-conditional image generation, and features a superior trade-off betweenperformance and latency. We then scale the same architecture to solve alarge-scale text-to-image task and show state-of-the-art performance comparedto the most recent public and commercial models. Notably, even without anycomputation optimization for transformer blocks, our models still yield fasterinference speed than existing works featuring efficient attention mechanisms,highlighting the advantages and the value of our approach.</description><author>Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Jian Ren</author><pubDate>Thu, 07 Nov 2024 18:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04967v1</guid></item><item><title>A Comparative Analysis of U-Net-based models for Segmentation of Cardiac MRI</title><link>http://arxiv.org/abs/2401.09980v2</link><description>Medical imaging refers to the technologies and methods utilized to view thehuman body and its inside, in order to diagnose, monitor, or even treat medicaldisorders. This paper aims to explore the application of deep learningtechniques in the semantic segmentation of Cardiac short-axis MRI (MagneticResonance Imaging) images, aiming to enhance the diagnosis, monitoring, andtreatment of medical disorders related to the heart. The focus centers onimplementing various architectures that are derivatives of U-Net, toeffectively isolate specific parts of the heart for comprehensive anatomicaland functional analysis. Through a combination of images, graphs, andquantitative metrics, the efficacy of the models and their predictions areshowcased. Additionally, this paper addresses encountered challenges andoutline strategies for future improvements. This abstract provides a conciseoverview of the efforts in utilizing deep learning for cardiac imagesegmentation, emphasizing both the accomplishments and areas for furtherrefinement.</description><author>Ketan Suhaas Saichandran</author><pubDate>Thu, 07 Nov 2024 18:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09980v2</guid></item><item><title>BitNet a4.8: 4-bit Activations for 1-bit LLMs</title><link>http://arxiv.org/abs/2411.04965v1</link><description>Recent research on the 1-bit Large Language Models (LLMs), such as BitNetb1.58, presents a promising direction for reducing the inference cost of LLMswhile maintaining their performance. In this work, we introduce BitNet a4.8,enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybridquantization and sparsification strategy to mitigate the quantization errorsintroduced by the outlier channels. Specifically, we utilize 4-bit activationsfor inputs to the attention and feed-forward network layers, while sparsifyingintermediate states followed with 8-bit quantization. Extensive experimentsdemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58with equivalent training costs, while being faster in inference with enabling4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% ofparameters and supports 3-bit KV cache, further enhancing the efficiency oflarge-scale LLM deployment and inference.</description><author>Hongyu Wang, Shuming Ma, Furu Wei</author><pubDate>Thu, 07 Nov 2024 18:41:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04965v1</guid></item><item><title>VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal Transparent Surface Reconstruction in Indoor Scenes</title><link>http://arxiv.org/abs/2411.04963v1</link><description>Mobile robots operating indoors must be prepared to navigate challengingscenes that contain transparent surfaces. This paper proposes a novel methodfor the fusion of acoustic and visual sensing modalities through implicitneural representations to enable dense reconstruction of transparent surfacesin indoor scenes. We propose a novel model that leverages generative latentoptimization to learn an implicit representation of indoor scenes consisting oftransparent surfaces. We demonstrate that we can query the implicitrepresentation to enable volumetric rendering in image space or 3D geometryreconstruction (point clouds or mesh) with transparent surface prediction. Weevaluate our method's effectiveness qualitatively and quantitatively on a newdataset collected using a custom, low-cost sensing platform featuring RGB-Dcameras and ultrasonic sensors. Our method exhibits significant improvementover state-of-the-art for transparent surface reconstruction.</description><author>Advaith V. Sethuraman, Onur Bagoren, Harikrishnan Seetharaman, Dalton Richardson, Joseph Taylor, Katherine A. Skinner</author><pubDate>Thu, 07 Nov 2024 18:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04963v1</guid></item><item><title>Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability</title><link>http://arxiv.org/abs/2411.04962v1</link><description>Large language models (LLMs) are being explored for diagnostic decisionsupport, yet their ability to estimate pre-test probabilities, vital forclinical decision-making, remains limited. This study evaluates two LLMs,Mistral-7B and Llama3-70B, using structured electronic health record data onthree diagnosis tasks. We examined three current methods of extracting LLMprobability estimations and revealed their limitations. We aim to highlight theneed for improved techniques in LLM confidence estimation.</description><author>Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Guanhua Chen, Anoop Mayampurath, Matthew Churpek, Majid Afshar</author><pubDate>Thu, 07 Nov 2024 18:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04962v1</guid></item><item><title>Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification</title><link>http://arxiv.org/abs/2411.04956v1</link><description>Latent Video Diffusion Models can easily deceive casual observers and domainexperts alike thanks to the produced image quality and temporal consistency.Beyond entertainment, this creates opportunities around safe data sharing offully synthetic datasets, which are crucial in healthcare, as well as otherdomains relying on sensitive personal information. However, privacy concernswith this approach have not fully been addressed yet, and models trained onsynthetic data for specific downstream tasks still perform worse than thosetrained on real data. This discrepancy may be partly due to the sampling spacebeing a subspace of the training videos, effectively reducing the training datasize for downstream models. Additionally, the reduced temporal consistency whengenerating long videos could be a contributing factor. In this paper, we first show that training privacy-preserving models inlatent space is computationally more efficient and generalize better.Furthermore, to investigate downstream degradation factors, we propose to use are-identification model, previously employed as a privacy preservation filter.We demonstrate that it is sufficient to train this model on the latent space ofthe video generator. Subsequently, we use these models to evaluate the subspacecovered by synthetic video datasets and thus introduce a new way to measure thefaithfulness of generative machine learning models. We focus on a specificapplication in healthcare echocardiography to illustrate the effectiveness ofour novel methods. Our findings indicate that only up to 30.8% of the trainingvideos are learned in latent video diffusion models, which could explain thelack of performance when training downstream tasks on synthetic data.</description><author>Mischa Dombrowski, Hadrien Reynaud, Bernhard Kainz</author><pubDate>Thu, 07 Nov 2024 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04956v1</guid></item><item><title>CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM</title><link>http://arxiv.org/abs/2411.04954v1</link><description>This paper aims to design a unified Computer-Aided Design (CAD) generationsystem that can easily generate CAD models based on the user's inputs in theform of textual description, images, point clouds, or even a combination ofthem. Towards this goal, we introduce the CAD-MLLM, the first system capable ofgenerating parametric CAD models conditioned on the multimodal input.Specifically, within the CAD-MLLM framework, we leverage the command sequencesof CAD models and then employ advanced large language models (LLMs) to alignthe feature space across these diverse multi-modalities data and CAD models'vectorized representations. To facilitate the model training, we design acomprehensive data construction and annotation pipeline that equips each CADmodel with corresponding multimodal data. Our resulting dataset, namedOmni-CAD, is the first multimodal CAD dataset that contains textualdescription, multi-view images, points, and command sequence for each CADmodel. It contains approximately 450K instances and their CAD constructionsequences. To thoroughly evaluate the quality of our generated CAD models, wego beyond current evaluation metrics that focus on reconstruction quality byintroducing additional metrics that assess topology quality and surfaceenclosure extent. Extensive experimental results demonstrate that CAD-MLLMsignificantly outperforms existing conditional generative methods and remainshighly robust to noises and missing points. The project page and morevisualizations can be found at: https://cad-mllm.github.io/</description><author>Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, Shenghua Gao</author><pubDate>Thu, 07 Nov 2024 18:31:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04954v1</guid></item><item><title>On Softmax Direct Preference Optimization for Recommendation</title><link>http://arxiv.org/abs/2406.09215v3</link><description>Recommender systems aim to predict personalized rankings based on userpreference data. With the rise of Language Models (LMs), LM-based recommendershave been widely explored due to their extensive world knowledge and powerfulreasoning abilities. Most of the LM-based recommenders convert historicalinteractions into language prompts, pairing with a positive item as the targetresponse and fine-tuning LM with a language modeling loss. However, the currentobjective fails to fully leverage preference data and is not optimized forpersonalized ranking tasks, which hinders the performance of LM-basedrecommenders. Inspired by the current advancement of Direct PreferenceOptimization (DPO) in human preference alignment and the success of softmaxloss in recommendations, we propose Softmax-DPO (S-DPO) to instill rankinginformation into the LM to help LM-based recommenders distinguish preferreditems from negatives, rather than solely focusing on positives. Specifically,we incorporate multiple negatives in user preference data and devise analternative version of DPO loss tailored for LM-based recommenders, which isextended from the traditional full-ranking Plackett-Luce (PL) model to partialrankings and connected to softmax sampling strategies. Theoretically, we bridgeS-DPO with the softmax loss over negative sampling and find that it has aninherent benefit of mining hard negatives, which assures its exceptionalcapabilities in recommendation tasks. Empirically, extensive experimentsconducted on three real-world datasets demonstrate the superiority of S-DPO toeffectively model user preference and further boost recommendation performancewhile providing better rewards for preferred items. Our codes are available athttps://github.com/chenyuxin1999/S-DPO.</description><author>Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua</author><pubDate>Thu, 07 Nov 2024 18:30:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09215v3</guid></item><item><title>Meta-Models: An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language</title><link>http://arxiv.org/abs/2410.02472v3</link><description>As Large Language Models (LLMs) become increasingly integrated into our dailylives, the potential harms from deceptive behavior underlie the need forfaithfully interpreting their decision-making. While traditional probingmethods have shown some effectiveness, they remain best for narrowly scopedtasks while more comprehensive explanations are still necessary. To this end,we investigate meta-models-an architecture using a "meta-model" that takesactivations from an "input-model" and answers natural language questions aboutthe input-model's behaviors. We evaluate the meta-model's ability to generalizeby training them on selected task types and assessing their out-of-distributionperformance in deceptive scenarios. Our findings show that meta-modelsgeneralize well to out-of-distribution tasks and point towards opportunitiesfor future research in this area. Our code is available athttps://github.com/acostarelli/meta-models-public .</description><author>Anthony Costarelli, Mat Allen, Severin Field</author><pubDate>Thu, 07 Nov 2024 18:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02472v3</guid></item><item><title>M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding</title><link>http://arxiv.org/abs/2411.04952v1</link><description>Document visual question answering (DocVQA) pipelines that answer questionsfrom documents have broad applications. Existing methods focus on handlingsingle-page documents with multi-modal language models (MLMs), or rely ontext-based retrieval-augmented generation (RAG) that uses text extraction toolssuch as optical character recognition (OCR). However, there are difficulties inapplying these methods in real-world scenarios: (a) questions often requireinformation across different pages or documents, where MLMs cannot handle manylong documents; (b) documents often have important information in visualelements such as figures, but text extraction tools ignore them. We introduceM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates variousdocument contexts (closed-domain and open-domain), question hops (single-hopand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAGfinds relevant documents and answers questions using a multi-modal retrieverand an MLM, so that it can efficiently handle single or many documents whilepreserving visual information. Since previous DocVQA datasets ask questions inthe context of a specific document, we also present M3DocVQA, a new benchmarkfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical resultsshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performancethan many strong baselines, including state-of-the-art performance inMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, andretrieval models. Lastly, we qualitatively show that M3DocRAG can successfullyhandle various scenarios, such as when relevant information exists acrossmultiple pages and when answer evidence only exists in images.</description><author>Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal</author><pubDate>Thu, 07 Nov 2024 18:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04952v1</guid></item><item><title>Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach</title><link>http://arxiv.org/abs/2411.04950v1</link><description>Stylometry aims to distinguish authors by analyzing literary traits assumedto reflect semi-conscious choices distinct from elements like genre or theme.However, these components often overlap, complicating text classification basedsolely on feature distributions. While some literary properties, such asthematic content, are likely to manifest as correlations between adjacent textunits, others, like authorial style, may be independent thereof. We introduce ahypothesis-testing approach to evaluate the influence of sequentiallycorrelated literary properties on text classification, aiming to determine whenthese correlations drive classification. Using a multivariate binarydistribution, our method models sequential correlations between text units as astochastic process, assessing the likelihood of clustering across varyingadjacency scales. This enables us to examine whether classification isdominated by sequentially correlated properties or remains independent. Inexperiments on a diverse English prose corpus, our analysis integratestraditional and neural embeddings within supervised and unsupervisedframeworks. Results demonstrate that our approach effectively identifies whentextual classification is not primarily influenced by sequentially correlatedliterary properties, particularly in cases where texts differ in authorialstyle or genre rather than by a single author within a similar genre.</description><author>Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober</author><pubDate>Thu, 07 Nov 2024 18:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04950v1</guid></item><item><title>SPGD: Steepest Perturbed Gradient Descent Optimization</title><link>http://arxiv.org/abs/2411.04946v1</link><description>Optimization algorithms are pivotal in advancing various scientific andindustrial fields but often encounter obstacles such as trapping in localminima, saddle points, and plateaus (flat regions), which makes the convergenceto reasonable or near-optimal solutions particularly challenging. This paperpresents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm thatinnovatively combines the principles of the gradient descent method withperiodic uniform perturbation sampling to effectively circumvent theseimpediments and lead to better solutions whenever possible. SPGD isdistinctively designed to generate a set of candidate solutions and select theone exhibiting the steepest loss difference relative to the current solution.It enhances the traditional gradient descent approach by integrating astrategic exploration mechanism that significantly increases the likelihood ofescaping sub-optimal local minima and navigating complex optimizationlandscapes effectively. Our approach not only retains the directed efficiencyof gradient descent but also leverages the exploratory benefits of stochasticperturbations, thus enabling a more comprehensive search for global optimaacross diverse problem spaces. We demonstrate the efficacy of SPGD in solvingthe 3D component packing problem, an NP-hard challenge. Preliminary resultsshow a substantial improvement over four established methods, particularly onresponse surfaces with complex topographies and in multidimensional non-convexcontinuous optimization problems. Comparative analyses with established 2Dbenchmark functions highlight SPGD's superior performance, showcasing itsability to navigate complex optimization landscapes. These results emphasizeSPGD's potential as a versatile tool for a wide range of optimization problems.</description><author>Amir M. Vahedi, Horea T. Ilies</author><pubDate>Thu, 07 Nov 2024 18:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04946v1</guid></item><item><title>GD doesn't make the cut: Three ways that non-differentiability affects neural network training</title><link>http://arxiv.org/abs/2401.08426v5</link><description>This paper critically examines the fundamental distinctions between gradientmethods applied to non-differentiable functions (NGDMs) and classical gradientdescents (GDs) for differentiable functions, revealing significant gaps incurrent deep learning optimization theory. We demonstrate that NGDMs exhibitmarkedly different convergence properties compared to GDs, strongly challengingthe applicability of extensive neural network convergence literature based on$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxicalbehavior of NDGM solutions for $L_{1}$-regularized problems, where increasingregularization counterintuitively leads to larger $L_{1}$ norms of optimalsolutions. This finding calls into question widely adopted $L_{1}$ penalizationtechniques for network pruning. We further challenge the common assumption thatoptimization algorithms like RMSProp behave similarly in differentiable andnon-differentiable contexts. Expanding on the Edge of Stability phenomenon, wedemonstrate its occurrence in a broader class of functions, including Lipschitzcontinuous convex differentiable functions. This finding raises importantquestions about its relevance and interpretation in non-convex,non-differentiable neural networks, particularly those using ReLU activations.Our work identifies critical misunderstandings of NDGMs in influentialliterature, stemming from an overreliance on strong smoothness assumptions.These findings necessitate a reevaluation of optimization dynamics in deeplearning, emphasizing the crucial need for more nuanced theoretical foundationsin analyzing these complex systems.</description><author>Siddharth Krishna Kumar</author><pubDate>Thu, 07 Nov 2024 18:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08426v5</guid></item><item><title>A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model</title><link>http://arxiv.org/abs/2411.04942v1</link><description>In this era of videos, automatic video editing techniques attract more andmore attention from industry and academia since they can reduce workloads andlower the requirements for human editors. Existing automatic editing systemsare mainly scene- or event-specific, e.g., soccer game broadcasting, yet theautomatic systems for general editing, e.g., movie or vlog editing which coversvarious scenes and events, were rarely studied before, and converting theevent-driven editing method to a general scene is nontrivial. In this paper, wepropose a two-stage scheme for general editing. Firstly, unlike previous worksthat extract scene-specific features, we leverage the pre-trainedVision-Language Model (VLM) to extract the editing-relevant representations asediting context. Moreover, to close the gap between the professional-lookingvideos and the automatic productions generated with simple guidelines, wepropose a Reinforcement Learning (RL)-based editing framework to formulate theediting problem and train the virtual editor to make better sequential editingdecisions. Finally, we evaluate the proposed method on a more general editingtask with a real movie dataset. Experimental results demonstrate theeffectiveness and benefits of the proposed context representation and thelearning ability of our RL-based editing framework.</description><author>Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, Rui Huang</author><pubDate>Thu, 07 Nov 2024 18:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04942v1</guid></item><item><title>Pareto Set Identification With Posterior Sampling</title><link>http://arxiv.org/abs/2411.04939v1</link><description>The problem of identifying the best answer among a collection of items havingreal-valued distribution is well-understood. Despite its practical relevance for many applications, fewer works havestudied its extension when multiple and potentially conflicting metrics areavailable to assess an item's quality. Pareto set identification (PSI) aims to identify the set of answers whosemeans are not uniformly worse than another. This paper studies PSI in the transductive linear setting with potentiallycorrelated objectives. Building on posterior sampling in both the stopping and the sampling rules,we propose the PSIPS algorithm that deals simultaneously with structure andcorrelation without paying the computational cost of existing oracle-basedalgorithms. Both from a frequentist and Bayesian perspective, PSIPS is asymptoticallyoptimal. We demonstrate its good empirical performance in real-world and syntheticinstances.</description><author>Cyrille Kone, Marc Jourdan, Emilie Kaufmann</author><pubDate>Thu, 07 Nov 2024 18:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04939v1</guid></item><item><title>Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition</title><link>http://arxiv.org/abs/2406.14894v2</link><description>Verbs form the backbone of language, providing the structure and meaning tosentences. Yet, their intricate semantic nuances pose a longstanding challenge.Understanding verb relations through the concept of lexical entailment iscrucial for comprehending sentence meanings and grasping verb dynamics. Thiswork investigates the capabilities of eight Large Language Models inrecognizing lexical entailment relations among verbs through differentlydevised prompting strategies and zero-/few-shot settings over verb pairs fromtwo lexical databases, namely WordNet and HyperLex. Our findings unveil thatthe models can tackle the lexical entailment recognition task with moderatelygood performance, although at varying degree of effectiveness and underdifferent conditions. Also, utilizing few-shot prompting can enhance themodels' performance. However, perfectly solving the task arises as an unmetchallenge for all examined LLMs, which raises an emergence for further researchdevelopments on this topic.</description><author>Candida M. Greco, Lucio La Cava, Andrea Tagarelli</author><pubDate>Thu, 07 Nov 2024 18:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14894v2</guid></item><item><title>Fed-LDR: Federated Local Data-infused Graph Creation with Node-centric Model Refinement</title><link>http://arxiv.org/abs/2411.04936v1</link><description>The rapid acceleration of global urbanization has introduced novel challengesin enhancing urban infrastructure and services. Spatio-temporal data,integrating spatial and temporal dimensions, has emerged as a critical tool forunderstanding urban phenomena and promoting sustainability. In this context,Federated Learning (FL) has gained prominence as a distributed learningparadigm aligned with the privacy requirements of urban IoT environments.However, integrating traditional and deep learning models into the FL frameworkposes significant challenges, particularly in capturing complex spatio-temporaldependencies and adapting to diverse urban conditions. To address thesechallenges, we propose the Federated Local Data-Infused Graph Creation withNode-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR leverages FL andGraph Convolutional Networks (GCN) to enhance spatio-temporal data analysis inurban environments. The algorithm comprises two key modules: (1) the LocalData-Infused Graph Creation (LDIGC) module, which dynamically reconfiguresadjacency matrices to reflect evolving spatial relationships within urbanenvironments, and (2) the Node-centric Model Refinement (NoMoR) module, whichcustomizes model parameters for individual urban nodes to accommodateheterogeneity. Evaluations on the PeMSD4 and PeMSD8 datasets demonstrateFed-LDR's superior performance over six baseline methods. Fed-LDR achieved thelowest Mean Absolute Error (MAE) values of 20.15 and 17.30, and the lowest RootMean Square Error (RMSE) values of 32.30 and 27.15, respectively, whilemaintaining a high correlation coefficient of 0.96 across both datasets.Notably, on the PeMSD4 dataset, Fed-LDR reduced MAE and RMSE by up to 81\% and78\%, respectively, compared to the best-performing baseline FedMedian.</description><author>Jiechao Gao, Yuangang Li, Syeda Faiza Ahmed</author><pubDate>Thu, 07 Nov 2024 18:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04936v1</guid></item><item><title>SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering</title><link>http://arxiv.org/abs/2411.04933v1</link><description>Audio-Visual Question Answering (AVQA) is a challenging task that involvesanswering questions based on both auditory and visual information in videos. Asignificant challenge is interpreting complex multi-modal scenes, which includeboth visual objects and sound sources, and connecting them to the givenquestion. In this paper, we introduce the Source-aware Semantic RepresentationNetwork (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizessource-wise learnable tokens to efficiently capture and align audio-visualelements with the corresponding question. It streamlines the fusion of audioand visual information using spatial and temporal attention mechanisms toidentify answers in multi-modal scenes. Extensive experiments on the Music-AVQAand AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQAmethods.</description><author>ianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, Yapeng Tian, Xiangliang Zhang</author><pubDate>Thu, 07 Nov 2024 18:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04933v1</guid></item><item><title>DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</title><link>http://arxiv.org/abs/2411.04928v1</link><description>In this paper, we introduce \textbf{DimensionX}, a framework designed togenerate photorealistic 3D and 4D scenes from just a single image with videodiffusion. Our approach begins with the insight that both the spatial structureof a 3D scene and the temporal evolution of a 4D scene can be effectivelyrepresented through sequences of video frames. While recent video diffusionmodels have shown remarkable success in producing vivid visuals, they facelimitations in directly recovering 3D/4D scenes due to limited spatial andtemporal controllability during generation. To overcome this, we proposeST-Director, which decouples spatial and temporal factors in video diffusion bylearning dimension-aware LoRAs from dimension-variant data. This controllablevideo diffusion approach enables precise manipulation of spatial structure andtemporal dynamics, allowing us to reconstruct both 3D and 4D representationsfrom sequential frames with the combination of spatial and temporal dimensions.Additionally, to bridge the gap between generated videos and real-world scenes,we introduce a trajectory-aware mechanism for 3D generation and anidentity-preserving denoising strategy for 4D generation. Extensive experimentson various real-world and synthetic datasets demonstrate that DimensionXachieves superior results in controllable video generation, as well as in 3Dand 4D scene generation, compared with previous methods.</description><author>Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, Yikai Wang</author><pubDate>Thu, 07 Nov 2024 18:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04928v1</guid></item><item><title>StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2411.04925v1</link><description>The advent of AI-Generated Content (AIGC) has spurred research into automatedvideo generation to streamline conventional processes. However, automatingstorytelling video production, particularly for customized narratives, remainschallenging due to the complexity of maintaining subject consistency acrossshots. While existing approaches like Mora and AesopAgent integrate multipleagents for Story-to-Video (S2V) generation, they fall short in preservingprotagonist consistency and supporting Customized Storytelling Video Generation(CSVG). To address these limitations, we propose StoryAgent, a multi-agentframework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasksassigned to specialized agents, mirroring the professional production process.Notably, our framework includes agents for story design, storyboard generation,video creation, agent coordination, and result evaluation. Leveraging thestrengths of different models, StoryAgent enhances control over the generationprocess, significantly improving character consistency. Specifically, weintroduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhanceintra-shot temporal consistency, while a novel storyboard generation pipelineis proposed to maintain subject consistency across shots. Extensive experimentsdemonstrate the effectiveness of our approach in synthesizing highly consistentstorytelling videos, outperforming state-of-the-art methods. Our contributionsinclude the introduction of StoryAgent, a versatile framework for videogeneration tasks, and novel techniques for preserving protagonist consistency.</description><author>Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, Xiaodan Liang</author><pubDate>Thu, 07 Nov 2024 18:00:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04925v1</guid></item><item><title>MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views</title><link>http://arxiv.org/abs/2411.04924v1</link><description>We introduce MVSplat360, a feed-forward approach for 360{\deg} novel viewsynthesis (NVS) of diverse real-world scenes, using only sparse observations.This setting is inherently ill-posed due to minimal overlap among input viewsand insufficient visual information provided, making it challenging forconventional methods to achieve high-quality results. Our MVSplat360 addressesthis by effectively combining geometry-aware 3D reconstruction with temporallyconsistent video generation. Specifically, it refactors a feed-forward 3DGaussian Splatting (3DGS) model to render features directly into the latentspace of a pre-trained Stable Video Diffusion (SVD) model, where these featuresthen act as pose and visual cues to guide the denoising process and producephotorealistic 3D-consistent views. Our model is end-to-end trainable andsupports rendering arbitrary views with as few as 5 sparse input views. Toevaluate MVSplat360's performance, we introduce a new benchmark using thechallenging DL3DV-10K dataset, where MVSplat360 achieves superior visualquality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm theeffectiveness of our model. The video results are available on our projectpage: https://donydchen.github.io/mvsplat360.</description><author>Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai</author><pubDate>Thu, 07 Nov 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04924v1</guid></item><item><title>VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos</title><link>http://arxiv.org/abs/2411.04923v1</link><description>Fine-grained alignment between videos and text is challenging due to complexspatial and temporal dynamics in videos. Existing video-based Large MultimodalModels (LMMs) handle basic conversations but struggle with precise pixel-levelgrounding in videos. To address this, we introduce VideoGLaMM, a LMM designedfor fine-grained pixel-level grounding in videos based on user-provided textualinputs. Our design seamlessly connects three key components: a Large LanguageModel, a dual vision encoder that emphasizes both spatial and temporal details,and a spatio-temporal decoder for accurate mask generation. This connection isfacilitated via tunable V-L and L-V adapters that enable close Vision-Language(VL) alignment. The architecture is trained to synchronize both spatial andtemporal elements of video content with textual instructions. To enablefine-grained grounding, we curate a multimodal dataset featuring detailedvisually-grounded conversations using a semiautomatic annotation pipeline,resulting in a diverse set of 38k video-QA triplets along with 83k objects and671k masks. We evaluate VideoGLaMM on three challenging tasks: GroundedConversation Generation, Visual Grounding, and Referring Video Segmentation.Experimental results show that our model consistently outperforms existingapproaches across all three tasks.</description><author>Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Thu, 07 Nov 2024 17:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04923v1</guid></item><item><title>GPTKB: Building Very Large Knowledge Bases from Language Models</title><link>http://arxiv.org/abs/2411.04920v1</link><description>General-domain knowledge bases (KB), in particular the "big three" --Wikidata, Yago and DBpedia -- are the backbone of many intelligentapplications. While these three have seen steady development, comprehensive KBconstruction at large has seen few fresh attempts. In this work, we propose tobuild a large general-domain KB entirely from a large language model (LLM). Wedemonstrate the feasibility of large-scale KB construction from LLMs, whilehighlighting specific challenges arising around entity recognition, entity andproperty canonicalization, and taxonomy construction. As a prototype, we useGPT-4o-mini to construct GPTKB, which contains 105 million triples for morethan 2.9 million entities, at a cost 100x less than previous KBC projects. Ourwork is a landmark for two fields: For NLP, for the first time, it provides\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For theSemantic Web, it shows novel ways forward for the long-standing challenge ofgeneral-domain KB construction. GPTKB is accessible at https://gptkb.org.</description><author>Yujia Hu, Shrestha Ghosh, Tuan-Phong Nugyen, Simon Razniewski</author><pubDate>Thu, 07 Nov 2024 17:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04920v1</guid></item><item><title>TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings</title><link>http://arxiv.org/abs/2406.15586v2</link><description>The goal of text style transfer is to transform the style of texts whilepreserving their original meaning, often with only a few examples of the targetstyle. Existing style transfer methods generally rely on the few-shotcapabilities of large language models or on complex controllable textgeneration approaches that are inefficient and underperform on fluency metrics.We introduce TinyStyler, a lightweight but effective approach, which leveragesa small language model (800M params) and pre-trained authorship embeddings toperform efficient, few-shot text style transfer. We evaluate on the challengingtask of authorship style transfer and find TinyStyler outperforms strongapproaches such as GPT-4. We also evaluate TinyStyler's ability to perform textattribute style transfer (formal $\leftrightarrow$ informal) with automatic andhuman evaluations and find that the approach outperforms recent controllabletext generation methods. Our model has been made publicly available athttps://huggingface.co/tinystyler/tinystyler .</description><author>Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu</author><pubDate>Thu, 07 Nov 2024 17:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15586v2</guid></item><item><title>Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion</title><link>http://arxiv.org/abs/2411.04919v1</link><description>Visual imitation learning methods demonstrate strong performance, yet theylack generalization when faced with visual input perturbations, includingvariations in lighting and textures, impeding their real-world application. Wepropose Stem-OB that utilizes pretrained image diffusion models to suppresslow-level visual differences while maintaining high-level scene structures.This image inversion process is akin to transforming the observation into ashared representation, from which other observations stem, with extraneousdetails removed. Stem-OB contrasts with data-augmentation approaches as it isrobust to various unspecified appearance changes without the need foradditional training. Our method is a simple yet highly effective plug-and-playsolution. Empirical results confirm the effectiveness of our approach insimulated tasks and show an exceptionally significant improvement in real-worldapplications, with an average increase of 22.2% in success rates compared tothe best baseline. See https://hukz18.github.io/Stem-Ob/ for more info.</description><author>Kaizhe Hu, Zihang Rui, Yao He, Yuyao Liu, Pu Hua, Huazhe Xu</author><pubDate>Thu, 07 Nov 2024 17:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04919v1</guid></item><item><title>Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping</title><link>http://arxiv.org/abs/2411.04915v1</link><description>Recently, there has been growing interest in autonomous shipping due to itspotential to improve maritime efficiency and safety. The use of advancedtechnologies, such as artificial intelligence, can address the currentnavigational and operational challenges in autonomous shipping. In particular,inland waterway transport (IWT) presents a unique set of challenges, such ascrowded waterways and variable environmental conditions. In such dynamicsettings, the reliability and robustness of autonomous shipping solutions arecritical factors for ensuring safe operations. This paper examines therobustness of benchmark deep reinforcement learning (RL) algorithms,implemented for IWT within an autonomous shipping simulator, and their abilityto generate effective motion planning policies. We demonstrate that amodel-free approach can achieve an adequate policy in the simulator,successfully navigating port environments never encountered during training. Wefocus particularly on Soft-Actor Critic (SAC), which we show to be inherentlymore robust to environmental disturbances compared to MuZero, astate-of-the-art model-based RL algorithm. In this paper, we take a significantstep towards developing robust, applied RL frameworks that can be generalizedto various vessel types and navigate complex port- and inland environments andscenarios.</description><author>Bavo Lesy, Ali Anwar, Siegfried Mercelis</author><pubDate>Thu, 07 Nov 2024 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04915v1</guid></item><item><title>GASE: Generatively Augmented Sentence Encoding</title><link>http://arxiv.org/abs/2411.04914v1</link><description>We propose an approach to enhance sentence embeddings by applying generativetext models for data augmentation at inference time. Unlike conventional dataaugmentation that utilises synthetic training data, our approach does notrequire access to model parameters or the computational resources typicallyrequired for fine-tuning state-of-the-art models. Generatively AugmentedSentence Encoding uses diverse linguistic synthetic variants of input textsgenerated by paraphrasing, summarising, or extracting keywords, followed bypooling the original and synthetic embeddings. Experimental results on theMassive Text Embedding Benchmark for Semantic Textual Similarity (STS)demonstrate performance improvements across a range of embedding models usingdifferent generative models for augmentation. We find that generativeaugmentation leads to larger performance improvements for embedding models withlower baseline performance. These findings suggest that integrating generativeaugmentation at inference time adds semantic diversity and can enhance therobustness and generalizability of sentence embeddings for embedding models.Our results show that the degree to which generative augmentation can improveSTS performance depends not only on the embedding model but also on thedataset. From a broader perspective, the approach allows trading training forinference compute.</description><author>Manuel Frank, Haithem Afli</author><pubDate>Thu, 07 Nov 2024 17:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04914v1</guid></item><item><title>Structure Matters: Dynamic Policy Gradient</title><link>http://arxiv.org/abs/2411.04913v1</link><description>In this work, we study $\gamma$-discounted infinite-horizon tabular Markovdecision processes (MDPs) and introduce a framework called dynamic policygradient (DynPG). The framework directly integrates dynamic programming with(any) policy gradient method, explicitly leveraging the Markovian property ofthe environment. DynPG dynamically adjusts the problem horizon during training,decomposing the original infinite-horizon MDP into a sequence of contextualbandit problems. By iteratively solving these contextual bandits, DynPGconverges to the stationary optimal policy of the infinite-horizon MDP. Todemonstrate the power of DynPG, we establish its non-asymptotic globalconvergence rate under the tabular softmax parametrization, focusing on thedependencies on salient but essential parameters of the MDP. By combiningclassical arguments from dynamic programming with more recent convergencearguments of policy gradient schemes, we prove that softmax DynPG scalespolynomially in the effective horizon $(1-\gamma)^{-1}$. Our findings contrastrecent exponential lower bound examples for vanilla policy gradient.</description><author>Sara Klein, Xiangyuan Zhang, Tamer Başar, Simon Weissmann, Leif Döring</author><pubDate>Thu, 07 Nov 2024 17:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04913v1</guid></item><item><title>Robust Iris Centre Localisation for Assistive Eye-Gaze Tracking</title><link>http://arxiv.org/abs/2411.04912v1</link><description>In this research work, we address the problem of robust iris centrelocalisation in unconstrained conditions as a core component of our eye-gazetracking platform. We investigate the application of U-Net variants forsegmentation-based and regression-based approaches to improve our iris centrelocalisation, which was previously based on Bayes' classification. The achievedresults are comparable to or better than the state-of-the-art, offering adrastic improvement over those achieved by the Bayes' classifier, and withoutsacrificing the real-time performance of our eye-gaze tracking platform.</description><author>Nipun Sandamal Ranasekara Pathiranage, Stefania Cristina, Kenneth P. Camilleri</author><pubDate>Thu, 07 Nov 2024 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04912v1</guid></item><item><title>Enhancing Missing Data Imputation through Combined Bipartite Graph and Complete Directed Graph</title><link>http://arxiv.org/abs/2411.04907v1</link><description>In this paper, we aim to address a significant challenge in the field ofmissing data imputation: identifying and leveraging the interdependencies amongfeatures to enhance missing data imputation for tabular data. We introduce anovel framework named the Bipartite and Complete Directed Graph Neural Network(BCGNN). Within BCGNN, observations and features are differentiated as twodistinct node types, and the values of observed features are converted intoattributed edges linking them. The bipartite segment of our frameworkinductively learns embedding representations for nodes, efficiently utilizingthe comprehensive information encapsulated in the attributed edges. Inparallel, the complete directed graph segment adeptly outlines and communicatesthe complex interdependencies among features. When compared to contemporaryleading imputation methodologies, BCGNN consistently outperforms them,achieving a noteworthy average reduction of 15% in mean absolute error forfeature imputation tasks under different missing mechanisms. Our extensiveexperimental investigation confirms that an in-depth grasp of theinterdependence structure substantially enhances the model's feature embeddingability. We also highlight the model's superior performance in label predictiontasks involving missing data, and its formidable ability to generalize tounseen data points.</description><author>Zhaoyang Zhang, Hongtu Zhu, Ziqi Chen, Yingjie Zhang, Hai Shu</author><pubDate>Thu, 07 Nov 2024 17:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04907v1</guid></item><item><title>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</title><link>http://arxiv.org/abs/2411.04905v1</link><description>Large language models (LLMs) for code have become indispensable in variousdomains, including code generation, reasoning tasks and agent systems.Whileopen-access code LLMs are increasingly approaching the performance levels ofproprietary models, high-quality code LLMs suitable for rigorous scientificinvestigation, particularly those with reproducible data processing pipelinesand transparent training protocols, remain limited. The scarcity is due tovarious challenges, including resource constraints, ethical considerations, andthe competitive advantages of keeping models advanced. To address the gap, weintroduce OpenCoder, a top-tier code LLM that not only achieves performancecomparable to leading models but also serves as an ``open cookbook'' for theresearch community. Unlike most prior efforts, we release not only modelweights and inference code, but also the reproducible training data, completedata processing pipeline, rigorous experimental ablation results, and detailedtraining protocols for open scientific research. Through this comprehensiverelease, we identify the key ingredients for building a top-tier code LLM: (1)code optimized heuristic rules for data cleaning and methods for datadeduplication, (2) recall of text corpus related to code and (3) high-qualitysynthetic data in both annealing and supervised fine-tuning stages. By offeringthis level of openness, we aim to broaden access to all aspects of a top-tiercode LLM, with OpenCoder serving as both a powerful model and an openfoundation to accelerate research, and enable reproducible advancements in codeAI.</description><author>Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu</author><pubDate>Thu, 07 Nov 2024 17:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04905v1</guid></item><item><title>Scaling Law Hypothesis for Multimodal Model</title><link>http://arxiv.org/abs/2409.06754v3</link><description>We propose a scaling law hypothesis for multimodal models processing text,audio, images, and video within a shared token and embedding space. Ourframework predicts model performance based on modality-specific compression andtokenization efficiency, extending established scaling laws from text-baseddecoder models to mixed-modality systems. We explore whether leveraging moretraining data in multiple modalities can reduce the size of the multimodalmodel, enabling efficient deployment on resource-constrained devices.</description><author>Qingyun Sun, Zhen Guo</author><pubDate>Thu, 07 Nov 2024 17:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06754v3</guid></item><item><title>Sampling-guided Heterogeneous Graph Neural Network with Temporal Smoothing for Scalable Longitudinal Data Imputation</title><link>http://arxiv.org/abs/2411.04899v1</link><description>In this paper, we propose a novel framework, the Sampling-guidedHeterogeneous Graph Neural Network (SHT-GNN), to effectively tackle thechallenge of missing data imputation in longitudinal studies. Unliketraditional methods, which often require extensive preprocessing to handleirregular or inconsistent missing data, our approach accommodates arbitrarymissing data patterns while maintaining computational efficiency. SHT-GNNmodels both observations and covariates as distinct node types, connectingobservation nodes at successive time points through subject-specificlongitudinal subnetworks, while covariate-observation interactions arerepresented by attributed edges within bipartite graphs. By leveragingsubject-wise mini-batch sampling and a multi-layer temporal smoothingmechanism, SHT-GNN efficiently scales to large datasets, while effectivelylearning node representations and imputing missing data. Extensive experimentson both synthetic and real-world datasets, including the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) dataset, demonstrate that SHT-GNN significantlyoutperforms existing imputation methods, even with high missing data rates. Theempirical results highlight SHT-GNN's robust imputation capabilities andsuperior performance, particularly in the context of complex, large-scalelongitudinal data.</description><author>Zhaoyang Zhang, Ziqi Chen, Qiao Liu, Jinhan Xie, Hongtu Zhu</author><pubDate>Thu, 07 Nov 2024 17:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04899v1</guid></item><item><title>Perceptions of Linguistic Uncertainty by Language Models and Humans</title><link>http://arxiv.org/abs/2407.15814v2</link><description>_Uncertainty expressions_ such as "probably" or "highly unlikely" arepervasive in human language. While prior work has established that there ispopulation-level agreement in terms of how humans quantitatively interpretthese expressions, there has been little inquiry into the abilities of languagemodels in the same context. In this paper, we investigate how language modelsmap linguistic expressions of uncertainty to numerical responses. Our approachassesses whether language models can employ theory of mind in this setting:understanding the uncertainty of another agent about a particular statement,independently of the model's own certainty about that statement. We find that 7out of 10 models are able to map uncertainty expressions to probabilisticresponses in a human-like manner. However, we observe systematically differentbehavior depending on whether a statement is actually true or false. Thissensitivity indicates that language models are substantially more susceptibleto bias based on their prior knowledge (as compared to humans). These findingsraise important questions and have broad implications for human-AI and AI-AIcommunication.</description><author>Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth</author><pubDate>Thu, 07 Nov 2024 17:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15814v2</guid></item><item><title>In the Era of Prompt Learning with Vision-Language Models</title><link>http://arxiv.org/abs/2411.04892v1</link><description>Large-scale foundation models like CLIP have shown strong zero-shotgeneralization but struggle with domain shifts, limiting their adaptability. Inour work, we introduce \textsc{StyLIP}, a novel domain-agnostic prompt learningstrategy for Domain Generalization (DG). StyLIP disentangles visual style andcontent in CLIP`s vision encoder by using style projectors to learndomain-specific prompt tokens and combining them with content features. Trainedcontrastively, this approach enables seamless adaptation across domains,outperforming state-of-the-art methods on multiple DG benchmarks. Additionally,we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`sfrozen vision backbone to learn domain-invariant prompts through image styleand content features. By aligning domains in embedding space with entropyminimization, AD-CLIP effectively handles domain shifts, even when only targetdomain samples are available. Lastly, we outline future work on class discoveryusing prompt learning for semantic segmentation in remote sensing, focusing onidentifying novel or rare classes in unstructured environments. This paves theway for more adaptive and generalizable models in complex, real-worldscenarios.</description><author>Ankit Jha</author><pubDate>Thu, 07 Nov 2024 17:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04892v1</guid></item><item><title>GUI Agents with Foundation Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2411.04890v1</link><description>Recent advances in foundation models, particularly Large Language Models(LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligentagents being capable of performing complex tasks. By leveraging the ability of(M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agentscan autonomously execute user instructions by simulating human-likeinteractions such as clicking and typing. This survey consolidates recentresearch on (M)LLM-based GUI agents, highlighting key innovations in data,frameworks, and applications. We begin by discussing representative datasetsand benchmarks. Next, we summarize a unified framework that captures theessential components used in prior research, accompanied by a taxonomy.Additionally, we explore commercial applications of (M)LLM-based GUI agents.Drawing from existing work, we identify several key challenges and proposefuture research directions. We hope this paper will inspire furtherdevelopments in the field of (M)LLM-based GUI agents.</description><author>Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, Ruiming Tang</author><pubDate>Thu, 07 Nov 2024 17:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04890v1</guid></item><item><title>On the Rigour of Scientific Writing: Criteria, Analysis, and Insights</title><link>http://arxiv.org/abs/2410.04981v2</link><description>Rigour is crucial for scientific research as it ensures the reproducibilityand validity of results and findings. Despite its importance, little workexists on modelling rigour computationally, and there is a lack of analysis onwhether these criteria can effectively signal or measure the rigour ofscientific papers in practice. In this paper, we introduce a bottom-up,data-driven framework to automatically identify and define rigour criteria andassess their relevance in scientific writing. Our framework includes rigourkeyword extraction, detailed rigour definition generation, and salient criteriaidentification. Furthermore, our framework is domain-agnostic and can betailored to the evaluation of scientific rigour for different areas,accommodating the distinct salient criteria across fields. We conductedcomprehensive experiments based on datasets collected from two high impactvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate theeffectiveness of our framework in modelling rigour. In addition, we analyselinguistic patterns of rigour, revealing that framing certainty is crucial forenhancing the perception of scientific rigour, while suggestion certainty andprobability uncertainty diminish it.</description><author>Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin</author><pubDate>Thu, 07 Nov 2024 17:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.04981v2</guid></item><item><title>Exploring QUIC Dynamics: A Large-Scale Dataset for Encrypted Traffic Analysis</title><link>http://arxiv.org/abs/2410.03728v2</link><description>QUIC, a new and increasingly used transport protocol, addresses and resolvesthe limitations of TCP by offering improved security, performance, and featuressuch as stream multiplexing and connection migration. These features, however,also present challenges for network operators who need to monitor and analyzeweb traffic. In this paper, we introduce VisQUIC, a labeled dataset comprisingover 100,000 QUIC traces from more than 44,000 websites (URLs), collected overa four-month period. These traces provide the foundation for generating morethan seven million images, with configurable parameters of window length, pixelresolution, normalization, and labels. These images enable an observer lookingat the interactions between a client and a server to analyze and gain insightsabout QUIC encrypted connections. To illustrate the dataset's potential, weoffer a use-case example of an observer estimating the number of HTTP/3responses/requests pairs in a given QUIC, which can reveal server behavior,client--server interactions, and the load imposed by an observed connection. Weformulate the problem as a discrete regression problem, train a machinelearning (ML) model for it, and then evaluate it using the proposed dataset onan example use case.</description><author>Barak Gahtan, Robert J. Shahla, Alex M. Bronstein, Reuven Cohen</author><pubDate>Thu, 07 Nov 2024 17:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03728v2</guid></item><item><title>GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics</title><link>http://arxiv.org/abs/2310.09254v4</link><description>Single-cell genomics has significantly advanced our understanding of cellularbehavior, catalyzing innovations in treatments and precision medicine. However,single-cell sequencing technologies are inherently destructive and can onlymeasure a limited array of data modalities simultaneously. This limitationunderscores the need for new methods capable of realigning cells. Optimaltransport (OT) has emerged as a potent solution, but traditional discretesolvers are hampered by scalability, privacy, and out-of-sample estimationissues. These challenges have spurred the development of neural network-basedsolvers, known as neural OT solvers, that parameterize OT maps. Yet, thesemodels often lack the flexibility needed for broader life science applications.To address these deficiencies, our approach learns stochastic maps (i.e.transport plans), allows for any cost function, relaxes mass conservationconstraints and integrates quadratic solvers to tackle the complex challengesposed by the (Fused) Gromov-Wasserstein problem. Utilizing flow matching as abackbone, our method offers a flexible and effective framework. We demonstrateits versatility and robustness through applications in cell developmentstudies, cellular drug response modeling, and cross-modality cell translation,illustrating significant potential for enhancing therapeutic strategies.</description><author>Dominik Klein, Théo Uscidda, Fabian Theis, Marco Cuturi</author><pubDate>Thu, 07 Nov 2024 17:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09254v4</guid></item><item><title>Non-Euclidean Mixture Model for Social Network Embedding</title><link>http://arxiv.org/abs/2411.04876v1</link><description>It is largely agreed that social network links are formed due to eitherhomophily or social influence. Inspired by this, we aim at understanding thegeneration of links via providing a novel embedding-based graph formationmodel. Different from existing graph representation learning, where linkgeneration probabilities are defined as a simple function of the correspondingnode embeddings, we model the link generation as a mixture model of the twofactors. In addition, we model the homophily factor in spherical space and theinfluence factor in hyperbolic space to accommodate the fact that (1) homophilyresults in cycles and (2) influence results in hierarchies in networks. We alsodesign a special projection to align these two spaces. We call this modelNon-Euclidean Mixture Model, i.e., NMM. We further integrate NMM with ournon-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN. NMM-GNNlearns embeddings through a unified framework which uses non-Euclidean GNNencoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novelspace unification loss component to unify distinct non-Euclidean geometricspaces. Experiments on public datasets show NMM-GNN significantly outperformsstate-of-the-art baselines on social network generation and classificationtasks, demonstrating its ability to better explain how the social network isformed.</description><author>Roshni G. Iyer, Yewen Wang, Wei Wang, Yizhou Sun</author><pubDate>Thu, 07 Nov 2024 17:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04876v1</guid></item><item><title>C3T: Cross-modal Transfer Through Time for Human Action Recognition</title><link>http://arxiv.org/abs/2407.16803v2</link><description>In order to unlock the potential of diverse sensors, we investigate a methodto transfer knowledge between modalities using the structure of a unifiedmultimodal representation space for Human Action Recognition (HAR). Weformalize and explore an understudied cross-modal transfer setting we termUnsupervised Modality Adaptation (UMA), where the modality used in testing isnot used in supervised training, i.e. zero labeled instances of the testmodality are available during training. We develop three methods to performUMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal TransferThrough Time (C3T). Our extensive experiments on various camera+IMU datasetscompare these methods to each other in the UMA setting, and to their empiricalupper bound in the supervised setting. The results indicate C3T is the mostrobust and highest performing by at least a margin of 8%, and nears thesupervised setting performance even in the presence of temporal noise. Thismethod introduces a novel mechanism for aligning signals across time-varyinglatent vectors, extracted from the receptive field of temporal convolutions.Our findings suggest that C3T has significant potential for developinggeneralizable models for time-series sensor data, opening new avenues formulti-modal learning in various applications.</description><author>Abhi Kamboj, Anh Duy Nguyen, Minh Do</author><pubDate>Thu, 07 Nov 2024 17:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16803v2</guid></item><item><title>FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI</title><link>http://arxiv.org/abs/2411.04872v1</link><description>We introduce FrontierMath, a benchmark of hundreds of original, exceptionallychallenging mathematics problems crafted and vetted by expert mathematicians.The questions cover most major branches of modern mathematics -- fromcomputationally intensive problems in number theory and real analysis toabstract questions in algebraic geometry and category theory. Solving a typicalproblem requires multiple hours of effort from a researcher in the relevantbranch of mathematics, and for the upper end questions, multiple days.FrontierMath uses new, unpublished problems and automated verification toreliably evaluate models while minimizing risk of data contamination. Currentstate-of-the-art AI models solve under 2% of problems, revealing a vast gapbetween AI capabilities and the prowess of the mathematical community. As AIsystems advance toward expert-level mathematical abilities, FrontierMath offersa rigorous testbed that quantifies their progress.</description><author>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla</author><pubDate>Thu, 07 Nov 2024 17:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04872v1</guid></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>http://arxiv.org/abs/2411.02397v2</link><description>Generating temporally-consistent high-fidelity videos can be computationallyexpensive, especially over longer temporal spans. More-recent DiffusionTransformers (DiTs) -- despite making significant headway in this context --have only heightened such challenges as they rely on larger models and heavierattention mechanisms, resulting in slower inference speeds. In this paper, weintroduce a training-free method to accelerate video DiTs, termed AdaptiveCaching (AdaCache), which is motivated by the fact that "not all videos arecreated equal": meaning, some videos require fewer denoising steps to attain areasonable quality than others. Building on this, we not only cachecomputations through the diffusion process, but also devise a caching scheduletailored to each video generation, maximizing the quality-latency trade-off. Wefurther introduce a Motion Regularization (MoReg) scheme to utilize videoinformation within AdaCache, essentially controlling the compute allocationbased on motion content. Altogether, our plug-and-play contributions grantsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s videogeneration) without sacrificing the generation quality, across multiple videoDiT baselines.</description><author>Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, Tian Xie</author><pubDate>Thu, 07 Nov 2024 17:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02397v2</guid></item><item><title>Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2411.04867v1</link><description>An important challenge for enabling the deployment of reinforcement learning(RL) algorithms in the real world is safety. This has resulted in the recentresearch field of Safe RL, which aims to learn optimal policies that are safe.One successful approach in that direction is probabilistic logic shields (PLS),a model-based Safe RL technique that uses formal specifications based onprobabilistic logic programming, constraining an agent's policy to comply withthose specifications in a probabilistic sense. However, safety is inherently amulti-agent concept, since real-world environments often involve multipleagents interacting simultaneously, leading to a complex system which is hard tocontrol. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. Inorder to address this gap, in this paper we ($i$) introduce Shielded MARL(SMARL) by extending PLS to MARL -- in particular, we introduce ProbabilisticLogic Temporal Difference Learning (PLTD) to enable shielded independentQ-learning (SIQL), and introduce shielded independent PPO (SIPPO) usingprobabilistic logic policy gradients; ($ii$) show its positive effect and useas an equilibrium selection mechanism in various game-theoretic environmentsincluding two-player simultaneous games, extensive-form games, stochasticgames, and some grid-world extensions in terms of safety, cooperation, andalignment with normative behaviors; and ($iii$) look into the asymmetric casewhere only one agent is shielded, and show that the shielded agent has asignificant influence on the unshielded one, providing further evidence ofSMARL's ability to enhance safety and cooperation in diverse multi-agentenvironments.</description><author>Satchit Chatterji, Erman Acar</author><pubDate>Thu, 07 Nov 2024 16:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04867v1</guid></item><item><title>ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset</title><link>http://arxiv.org/abs/2411.04865v1</link><description>Facade semantic segmentation is a long-standing challenge in photogrammetryand computer vision. Although the last decades have witnessed the influx offacade segmentation methods, there is a lack of comprehensive facade classesand data covering the architectural variability. In ZAHA, we introduce Level ofFacade Generalization (LoFG), novel hierarchical facade classes designed basedon international urban modeling standards, ensuring compatibility withreal-world challenging classes and uniform methods' comparison. Realizing theLoFG, we present to date the largest semantic 3D facade segmentation dataset,providing 601 million annotated points at five and 15 classes of LoFG2 andLoFG3, respectively. Moreover, we analyze the performance of baseline semanticsegmentation methods on our introduced LoFG classes and data, complementing itwith a discussion on the unresolved challenges for facade segmentation. Wefirmly believe that ZAHA shall facilitate further development of 3D facadesemantic segmentation methods, enabling robust segmentation indispensable increating urban digital twins.</description><author>Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst</author><pubDate>Thu, 07 Nov 2024 16:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04865v1</guid></item><item><title>Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</title><link>http://arxiv.org/abs/2410.13835v2</link><description>Practitioners have consistently observed three puzzling phenomena intransformer-based large language models (LLMs): attention sinks, value-statedrains, and residual-state peaks, collectively referred to as extreme-tokenphenomena. These phenomena are characterized by certain so-called "sink tokens"receiving disproportionately high attention weights, exhibiting significantlysmaller value states, and having much larger residual-state norms than those ofother tokens. These extreme tokens give rise to various challenges in LLMinference, quantization, and interpretability. We elucidate the mechanisms behind extreme-token phenomena. First, we showthat these phenomena arise in very simple architectures -- transformers withone to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.In this setting, we identify an active-dormant mechanism, where attention headsbecome sinks for specific input domains while remaining non-sinks for others.Our theoretical analysis of the training dynamics reveals that these phenomenaare driven by a mutual reinforcement mechanism. Building on these insights, wepropose strategies to mitigate extreme-token phenomena during pretraining,including replacing softmax with ReLU and Adam with SGD. Next, we extend ouranalysis to pretrained LLMs, including Llama and OLMo, showing that manyattention heads exhibit a similar active-dormant mechanism as in the BB task,and that the mutual reinforcement mechanism also governs the emergence ofextreme-token phenomena during LLM pretraining. Our results reveal that many ofthe static and dynamic properties of extreme-token phenomena predicted by theBB task align with observations in pretrained LLMs.</description><author>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei</author><pubDate>Thu, 07 Nov 2024 16:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13835v2</guid></item><item><title>OneProt: Towards Multi-Modal Protein Foundation Models</title><link>http://arxiv.org/abs/2411.04863v1</link><description>Recent AI advances have enabled multi-modal systems to model and translatediverse information spaces. Extending beyond text and vision, we introduceOneProt, a multi-modal AI for proteins that integrates structural, sequence,alignment, and binding site data. Using the ImageBind framework, OneProt alignsthe latent spaces of modality encoders along protein sequences. It demonstratesstrong performance in retrieval tasks and surpasses state-of-the-art methods invarious downstream tasks, including metal ion binding classification,gene-ontology annotation, and enzyme function prediction. This work expandsmulti-modal capabilities in protein models, paving the way for applications indrug discovery, biocatalytic reaction planning, and protein engineering.</description><author>Klemens Flöge, Srisruthi Udayakumar, Johanna Sommer, Marie Piraud, Stefan Kesselheim, Vincent Fortuin, Stephan Günneman, Karel J van der Weg, Holger Gohlke, Alina Bazarova, Erinc Merdivan</author><pubDate>Thu, 07 Nov 2024 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04863v1</guid></item><item><title>Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained Language Models</title><link>http://arxiv.org/abs/2411.04862v1</link><description>Title: Sentiment Analysis of Spanish Political Party Communications onTwitter Using Pre-trained Language Models Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen Comments: 21 pages, 6 figures Abstract: This study investigates sentiment patterns within Spanish politicalparty communications on Twitter by leveraging BETO and RoBERTuito, twopre-trained language models optimized for Spanish text. Using a dataset oftweets from major Spanish political parties: PSOE, PP, Vox, Podemos, andCiudadanos, spanning 2019 to 2024, this research analyzes sentimentdistributions and explores the relationship between sentiment expression andparty ideology. The findings indicate that both models consistently identify apredominant Neutral sentiment across all parties, with significant variationsin Negative and Positive sentiments that align with ideological distinctions.Specifically, Vox exhibits higher levels of Negative sentiment, while PSOEdemonstrates relatively high Positive sentiment, supporting the hypothesis thatemotional appeals in political messaging reflect ideological stances. Thisstudy underscores the potential of pre-trained language models for non-Englishsentiment analysis on social media, providing insights into sentiment dynamicsthat shape public discourse within Spain's multi-party political system. Keywords: Spanish politics, sentiment analysis, pre-trained language models,Twitter, BETO, RoBERTuito, political ideology, multi-party system</description><author>Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen</author><pubDate>Thu, 07 Nov 2024 16:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04862v1</guid></item><item><title>An efficient likelihood-free Bayesian inference method based on sequential neural posterior estimation</title><link>http://arxiv.org/abs/2311.12530v3</link><description>Sequential neural posterior estimation (SNPE) techniques have been recentlyproposed for dealing with simulation-based models with intractable likelihoods.Unlike approximate Bayesian computation, SNPE techniques learn the posteriorfrom sequential simulation using neural network-based conditional densityestimators by minimizing a specific loss function. The SNPE method proposed byLueckmann et al. (2017) used a calibration kernel to boost the sample weightsaround the observed data, resulting in a concentrated loss function. However,the use of calibration kernels may increase the variances of both the empiricalloss and its gradient, making the training inefficient. To improve thestability of SNPE, this paper proposes to use an adaptive calibration kerneland several variance reduction techniques. The proposed method greatly speedsup the process of training and provides a better approximation of the posteriorthan the original SNPE method and some existing competitors as confirmed bynumerical experiments. We also manage to demonstrate the superiority of theproposed method for a high-dimensional model with real-world dataset.</description><author>Yifei Xiong, Xiliang Yang, Sanguo Zhang, Zhijian He</author><pubDate>Thu, 07 Nov 2024 16:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12530v3</guid></item><item><title>A multi-purpose automatic editing system based on lecture semantics for remote education</title><link>http://arxiv.org/abs/2411.04859v1</link><description>Remote teaching has become popular recently due to its convenience andsafety, especially under extreme circumstances like a pandemic. However, onlinestudents usually have a poor experience since the information acquired from theviews provided by the broadcast platforms is limited. One potential solution isto show more camera views simultaneously, but it is technically challenging anddistracting for the viewers. Therefore, an automatic multi-cameradirecting/editing system, which aims at selecting the most concerned view ateach time instance to guide the attention of online students, is in urgentdemand. However, existing systems mostly make simple assumptions and focus ontracking the position of the speaker instead of the real lecture semantics, andtherefore have limited capacities to deliver optimal information flow. To thisend, this paper proposes an automatic multi-purpose editing system based on thelecture semantics, which can both direct the multiple video streams forreal-time broadcasting and edit the optimal video offline for review purposes.Our system directs the views by semantically analyzing the class events whilefollowing the professional directing rules, mimicking a human director tocapture the regions of interest from the viewpoint of the onsite students. Weconduct both qualitative and quantitative analyses to verify the effectivenessof the proposed system and its components.</description><author>Panwen Hu, Rui Huang</author><pubDate>Thu, 07 Nov 2024 16:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04859v1</guid></item><item><title>Clinicians' Voice: Fundamental Considerations for XAI in Healthcare</title><link>http://arxiv.org/abs/2411.04855v1</link><description>Explainable AI (XAI) holds the promise of advancing the implementation andadoption of AI-based tools in practice, especially in high-stakes environmentslike healthcare. However, most of the current research is disconnected from itspractical applications and lacks input of end users. To address this, weconducted semi-structured interviews with clinicians to discuss their thoughts,hopes, and concerns. We find that clinicians generally think positively aboutdeveloping AI-based tools for clinical practice, but they have concerns abouthow these will fit into their workflow and how it will impact clinician-patientrelations. We further identify education of clinicians on AI as a crucialfactor for the success of AI in healthcare and highlight aspects clinicians arelooking for in (X)AI-based tools. In contrast to other studies, we take on aholistic and exploratory perspective to identify general requirements, which isnecessary before moving on to testing specific (X)AI products for healthcare.</description><author>T. E. Röber, R. Goedhart, S. İ. Birbil</author><pubDate>Thu, 07 Nov 2024 16:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04855v1</guid></item><item><title>Personalized Large Language Models</title><link>http://arxiv.org/abs/2402.09269v2</link><description>Large language models (LLMs) have significantly advanced Natural LanguageProcessing (NLP) tasks in recent years. However, their universal nature poseslimitations in scenarios requiring personalized responses, such asrecommendation systems and chatbots. This paper investigates methods topersonalize LLMs, comparing fine-tuning and zero-shot reasoning approaches onsubjective tasks. Results demonstrate that personalized fine-tuning improvesmodel reasoning compared to non-personalized models. Experiments on datasetsfor emotion recognition and hate speech detection show consistent performancegains with personalized methods across different LLM architectures. Thesefindings underscore the importance of personalization for enhancing LLMcapabilities in subjective text perception tasks.</description><author>Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń</author><pubDate>Thu, 07 Nov 2024 16:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09269v2</guid></item><item><title>Conformalized Credal Regions for Classification with Ambiguous Ground Truth</title><link>http://arxiv.org/abs/2411.04852v1</link><description>An open question in \emph{Imprecise Probabilistic Machine Learning} is how toempirically derive a credal region (i.e., a closed and convex family ofprobabilities on the output space) from the available data, without any priorknowledge or assumption. In classification problems, credal regions are a toolthat is able to provide provable guarantees under realistic assumptions bycharacterizing the uncertainty about the distribution of the labels. Buildingon previous work, we show that credal regions can be directly constructed usingconformal methods. This allows us to provide a novel extension of classicalconformal prediction to problems with ambiguous ground truth, that is, when theexact labels for given inputs are not exactly known. The resulting constructionenjoys desirable practical and theoretical properties: (i) conformal coverageguarantees, (ii) smaller prediction sets (compared to classical conformalprediction regions) and (iii) disentanglement of uncertainty sources(epistemic, aleatoric). We empirically verify our findings on both syntheticand real datasets.</description><author>Michele Caprio, David Stutz, Shuo Li, Arnaud Doucet</author><pubDate>Thu, 07 Nov 2024 16:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04852v1</guid></item><item><title>Prompt-Guided Internal States for Hallucination Detection of Large Language Models</title><link>http://arxiv.org/abs/2411.04847v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities acrossa variety of tasks in different domains. However, they sometimes generateresponses that are logically coherent but factually incorrect or misleading,which is known as LLM hallucinations. Data-driven supervised methods trainhallucination detectors by leveraging the internal states of LLMs, butdetectors trained on specific domains often struggle to generalize well toother domains. In this paper, we aim to enhance the cross-domain performance ofsupervised detectors with only in-domain data. We propose a novel framework,prompt-guided internal states for hallucination detection of LLMs, namelyPRISM. By utilizing appropriate prompts to guide changes in the structurerelated to text truthfulness within the LLM's internal states, we make thisstructure more salient and consistent across texts from different domains. Weintegrated our framework with existing hallucination detection methods andconducted experiments on datasets from different domains. The experimentalresults indicate that our framework significantly enhances the cross-domaingeneralization of existing hallucination detection methods.</description><author>Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu</author><pubDate>Thu, 07 Nov 2024 16:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04847v1</guid></item><item><title>Asymptotic regularity of a generalised stochastic Halpern scheme with applications</title><link>http://arxiv.org/abs/2411.04845v1</link><description>We provide abstract, general and highly uniform rates of asymptoticregularity for a generalized stochastic Halpern-style iteration, whichincorporates a second mapping in the style of a Krasnoselskii-Mann iteration.This iteration is general in two ways: First, it incorporates stochasticity ina completely abstract way rather than fixing a sampling method; secondly, itincludes as special cases stochastic versions of various schemes from theoptimization literature, including Halpern's iteration as well as aKrasnoselskii-Mann iteration with Tikhonov regularization terms in the sense ofBo\c{t}, Csetnek and Meier. For these particular cases, we in particular obtainlinear rates of asymptotic regularity, matching (or improving) the currentlybest known rates for these iterations in stochastic optimization, and quadraticrates of asymptotic regularity are obtained in the context of inner productspaces for the general iteration. We utilize these rates to give bounds on theoracle complexity of such iterations under suitable variance assumptions andbatching strategies, again presented in an abstract style. Finally, we sketchhow the schemes presented here can be instantiated in the context ofreinforcement learning to yield novel methods for Q-learning.</description><author>Nicholas Pischke, Thomas Powell</author><pubDate>Thu, 07 Nov 2024 16:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04845v1</guid></item><item><title>Differentiable Gaussian Representation for Incomplete CT Reconstruction</title><link>http://arxiv.org/abs/2411.04844v1</link><description>Incomplete Computed Tomography (CT) benefits patients by reducing radiationexposure. However, reconstructing high-fidelity images from limited views orangles remains challenging due to the ill-posed nature of the problem. DeepLearning Reconstruction (DLR) methods have shown promise in enhancing imagequality, but the paradox between training data diversity and highgeneralization ability remains unsolved. In this paper, we propose a novelGaussian Representation for Incomplete CT Reconstruction (GRCT) without theusage of any neural networks or full-dose CT data. Specifically, we model the3D volume as a set of learnable Gaussians, which are optimized directly fromthe incomplete sinogram. Our method can be applied to multiple views and angleswithout changing the architecture. Additionally, we propose a differentiableFast CT Reconstruction method for efficient clinical usage. Extensiveexperiments on multiple datasets and settings demonstrate significantimprovements in reconstruction quality metrics and high efficiency. We plan torelease our code as open-source.</description><author>Shaokai Wu, Yuxiang Lu, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu</author><pubDate>Thu, 07 Nov 2024 16:32:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04844v1</guid></item><item><title>Statistical optimal transport</title><link>http://arxiv.org/abs/2407.18163v2</link><description>We present an introduction to the field of statistical optimal transport,based on lectures given at \'Ecole d'\'Et\'e de Probabilit\'es de Saint-FlourXLIX.</description><author>Sinho Chewi, Jonathan Niles-Weed, Philippe Rigollet</author><pubDate>Thu, 07 Nov 2024 16:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18163v2</guid></item><item><title>Learning in Budgeted Auctions with Spacing Objectives</title><link>http://arxiv.org/abs/2411.04843v1</link><description>In many repeated auction settings, participants care not only about howfrequently they win but also how their winnings are distributed over time. Thisproblem arises in various practical domains where avoiding congested demand iscrucial, such as online retail sales and compute services, as well as inadvertising campaigns that require sustained visibility over time. We introducea simple model of this phenomenon, modeling it as a budgeted auction where thevalue of a win is a concave function of the time since the last win. Thisimplies that for a given number of wins, even spacing over time is optimal. Wealso extend our model and results to the case when not all wins result in"conversions" (realization of actual gains), and the probability of conversiondepends on a context. The goal is to maximize and evenly space conversionsrather than just wins. We study the optimal policies for this setting in second-price auctions andoffer learning algorithms for the bidders that achieve low regret against theoptimal bidding policy in a Bayesian online setting. Our main result is acomputationally efficient online learning algorithm that achieves $\tildeO(\sqrt T)$ regret. We achieve this by showing that an infinite-horizon Markovdecision process (MDP) with the budget constraint in expectation is essentiallyequivalent to our problem, even when limiting that MDP to a very small numberof states. The algorithm achieves low regret by learning a bidding policy thatchooses bids as a function of the context and the system's state, which will bethe time elapsed since the last win (or conversion). We show thatstate-independent strategies incur linear regret even without uncertainty ofconversions. We complement this by showing that there are state-independentstrategies that, while still having linear regret, achieve a $(1-\frac 1 e)$approximation to the optimal reward.</description><author>Giannis Fikioris, Robert Kleinberg, Yoav Kolumbus, Raunak Kumar, Yishay Mansour, Éva Tardos</author><pubDate>Thu, 07 Nov 2024 16:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04843v1</guid></item><item><title>Machine learning and optimization-based approaches to duality in statistical physics</title><link>http://arxiv.org/abs/2411.04838v1</link><description>The notion of duality -- that a given physical system can have two differentmathematical descriptions -- is a key idea in modern theoretical physics.Establishing a duality in lattice statistical mechanics models requires theconstruction of a dual Hamiltonian and a map from the original to the dualobservables. By using simple neural networks to parameterize these maps andintroducing a loss function that penalises the difference between correlationfunctions in original and dual models, we formulate the process of dualitydiscovery as an optimization problem. We numerically solve this problem andshow that our framework can rediscover the celebrated Kramers-Wannier dualityfor the 2d Ising model, reconstructing the known mapping of temperatures. Wealso discuss an alternative approach which uses known features of the mappingof topological lines to reduce the problem to optimizing the couplings in adual Hamiltonian, and explore next-to-nearest neighbour deformations of the 2dIsing duality. We discuss future directions and prospects for discovering newdualities within this framework.</description><author>Andrea E. V. Ferrari, Prateek Gupta, Nabil Iqbal</author><pubDate>Thu, 07 Nov 2024 16:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04838v1</guid></item><item><title>Plasticity Loss in Deep Reinforcement Learning: A Survey</title><link>http://arxiv.org/abs/2411.04832v1</link><description>Akin to neuroplasticity in human brains, the plasticity of deep neuralnetworks enables their quick adaption to new data. This makes plasticityparticularly crucial for deep Reinforcement Learning (RL) agents: Onceplasticity is lost, an agent's performance will inevitably plateau because itcannot improve its policy to account for changes in the data distribution,which are a necessary consequence of its learning process. Thus, developingwell-performing and sample-efficient agents hinges on their ability to remainplastic during training. Furthermore, the loss of plasticity can be connectedto many other issues plaguing deep RL, such as training instabilities, scalingfailures, overestimation bias, and insufficient exploration. With this survey,we aim to provide an overview of the emerging research on plasticity loss foracademics and practitioners of deep reinforcement learning. First, we propose aunified definition of plasticity loss based on recent works, relate it todefinitions from the literature, and discuss metrics for measuring plasticityloss. Then, we categorize and discuss numerous possible causes of plasticityloss before reviewing currently employed mitigation strategies. Our taxonomy isthe first systematic overview of the current state of the field. Lastly, wediscuss prevalent issues within the literature, such as a necessity for broaderevaluation, and provide recommendations for future research, like gaining abetter understanding of an agent's neural activity and behavior.</description><author>Timo Klein, Lukas Miklautz, Kevin Sidak, Claudia Plant, Sebastian Tschiatschek</author><pubDate>Thu, 07 Nov 2024 16:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04832v1</guid></item><item><title>Topological obstruction to the training of shallow ReLU neural networks</title><link>http://arxiv.org/abs/2410.14837v2</link><description>Studying the interplay between the geometry of the loss landscape and theoptimization trajectories of simple neural networks is a fundamental step forunderstanding their behavior in more complex settings. This paper reveals thepresence of topological obstruction in the loss landscape of shallow ReLUneural networks trained using gradient flow. We discuss how the homogeneousnature of the ReLU activation function constrains the training trajectories tolie on a product of quadric hypersurfaces whose shape depends on the particularinitialization of the network's parameters. When the neural network's output isa single scalar, we prove that these quadrics can have multiple connectedcomponents, limiting the set of reachable parameters during training. Weanalytically compute the number of these components and discuss the possibilityof mapping one to the other through neuron rescaling and permutation. In thissimple setting, we find that the non-connectedness results in a topologicalobstruction, which, depending on the initialization, can make the globaloptimum unreachable. We validate this result with numerical experiments.</description><author>Marco Nurisso, Pierrick Leroy, Francesco Vaccarino</author><pubDate>Thu, 07 Nov 2024 16:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14837v2</guid></item><item><title>Learning Latent Space Dynamics with Model-Form Uncertainties: A Stochastic Reduced-Order Modeling Approach</title><link>http://arxiv.org/abs/2409.00220v2</link><description>This paper presents a probabilistic approach to represent and quantifymodel-form uncertainties in the reduced-order modeling of complex systems usingoperator inference techniques. Such uncertainties can arise in the selection ofan appropriate state-space representation, in the projection step thatunderlies many reduced-order modeling methods, or as a byproduct ofconsiderations made during training, to name a few. Following previous works inthe literature, the proposed method captures these uncertainties by expandingthe approximation space through the randomization of the projection matrix.This is achieved by combining Riemannian projection and retraction operators -acting on a subset of the Stiefel manifold - with an information-theoreticformulation. The efficacy of the approach is assessed on canonical problems influid mechanics by identifying and quantifying the impact of model-formuncertainties on the inferred operators.</description><author>Jin Yi Yong, Rudy Geelen, Johann Guilleminot</author><pubDate>Thu, 07 Nov 2024 16:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00220v2</guid></item><item><title>D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes</title><link>http://arxiv.org/abs/2411.04826v1</link><description>Depth estimation is a crucial technology in robotics. Recently,self-supervised depth estimation methods have demonstrated great potential asthey can efficiently leverage large amounts of unlabelled real-world data.However, most existing methods are designed under the assumption of staticscenes, which hinders their adaptability in dynamic environments. To addressthis issue, we present D$^3$epth, a novel method for self-supervised depthestimation in dynamic scenes. It tackles the challenge of dynamic objects fromtwo key perspectives. First, within the self-supervised framework, we design areprojection constraint to identify regions likely to contain dynamic objects,allowing the construction of a dynamic mask that mitigates their impact at theloss level. Second, for multi-frame depth estimation, we introduce a costvolume auto-masking strategy that leverages adjacent frames to identify regionsassociated with dynamic objects and generate corresponding masks. This providesguidance for subsequent processes. Furthermore, we propose a spectral entropyuncertainty module that incorporates spectral entropy to guide uncertaintyestimation during depth fusion, effectively addressing issues arising from costvolume computation in dynamic environments. Extensive experiments on KITTI andCityscapes datasets demonstrate that the proposed method consistentlyoutperforms existing self-supervised monocular depth estimation baselines. Codeis available at \url{https://github.com/Csyunling/D3epth}.</description><author>Siyu Chen, Hong Liu, Wenhao Li, Ying Zhu, Guoquan Wang, Jianbing Wu</author><pubDate>Thu, 07 Nov 2024 16:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04826v1</guid></item><item><title>VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models</title><link>http://arxiv.org/abs/2411.04825v1</link><description>Existing text simplification or paraphrase datasets mainly focus onsentence-level text generation in a general domain. These datasets aretypically developed without using domain knowledge. In this paper, we release anovel dataset, VTechAGP, which is the first academic-to-general-audience textparaphrase dataset consisting of 4,938 document-level these and dissertationacademic and general-audience abstract pairs from 8 colleges authored over 25years. We also propose a novel dynamic soft prompt generative language model,DSPT5. For training, we leverage a contrastive-generative loss function tolearn the keyword vectors in the dynamic prompt. For inference, we adopt acrowd-sampling decoding strategy at both semantic and structural levels tofurther select the best output candidate. We evaluate DSPT5 and variousstate-of-the-art large language models (LLMs) from multiple perspectives.Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes,while the lightweight DSPT5 can achieve competitive results. To the best of ourknowledge, we are the first to build a benchmark dataset and solutions foracademic-to-general-audience text paraphrase dataset.</description><author>Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry</author><pubDate>Thu, 07 Nov 2024 16:06:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04825v1</guid></item><item><title>The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning</title><link>http://arxiv.org/abs/2110.14427v5</link><description>The paper concerns the $d$-dimensional stochastic approximation recursion, $$\theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ where $ \{\Phi_n \}$ is a stochastic process on a general state space, satisfying aconditional Markov property that allows for parameter-dependent noise. The mainresults are established under additional conditions on the mean flow and aversion of the Donsker-Varadhan Lyapunov drift condition known as (DV3): {(i)} An appropriate Lyapunov function is constructed that impliesconvergence of the estimates in $L_4$. {(ii)} A functional central limit theorem (CLT) is established, as well asthe usual one-dimensional CLT for the normalized error. Moment bounds combinedwith the CLT imply convergence of the normalized covariance $\textsf{E} [ z_nz_n^T ]$ to the asymptotic covariance in the CLT, where $z_n{=:}(\theta_n-\theta^*)/\sqrt{\alpha_n}$. {(iii)} The CLT holds for the normalized version $z^{\text{PR}}_n{=:}\sqrt{n} [\theta^{\text{PR}}_n -\theta^*]$, of the averaged parameters$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standardassumptions on the step-size. Moreover, the covariance in the CLT coincideswith the minimal covariance of Polyak and Ruppert. {(iv)} An example is given where $f$ and $\bar{f}$ are linear in $\theta$,and $\Phi$ is a geometrically ergodic Markov chain but does not satisfy (DV3).While the algorithm is convergent, the second moment of $\theta_n$ is unboundedand in fact diverges. {\bf This arXiv version 3 represents a major extension of the results inprior versions.} The main results now allow for parameter-dependent noise, asis often the case in applications to reinforcement learning.</description><author>Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis, Sean Meyn</author><pubDate>Thu, 07 Nov 2024 15:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.14427v5</guid></item><item><title>When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in Hanja and Kanbun</title><link>http://arxiv.org/abs/2411.04822v1</link><description>Historical and linguistic connections within the Sinosphere have ledresearchers to use Classical Chinese resources for cross-lingual transfer whenprocessing historical documents from Korea and Japan. In this paper, wequestion the assumption of cross-lingual transferability from Classical Chineseto Hanja and Kanbun, the ancient written languages of Korea and Japan,respectively. Our experiments across machine translation, named entityrecognition, and punctuation restoration tasks show minimal impact of ClassicalChinese datasets on language model performance for ancient Korean documentswritten in Hanja, with performance differences within $\pm{}0.0068$ F1-scorefor sequence labeling tasks and up to $+0.84$ BLEU score for translation. Theselimitations persist consistently across various model sizes, architectures, anddomain-specific datasets. Our analysis reveals that the benefits of ClassicalChinese resources diminish rapidly as local language data increases for Hanja,while showing substantial improvements only in extremely low-resource scenariosfor both Korean and Japanese historical documents. These mixed resultsemphasize the need for careful empirical validation rather than assumingbenefits from indiscriminate cross-lingual transfer.</description><author>Seyoung Song, Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh</author><pubDate>Thu, 07 Nov 2024 15:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04822v1</guid></item><item><title>End-to-end Inception-Unet based Generative Adversarial Networks for Snow and Rain Removals</title><link>http://arxiv.org/abs/2411.04821v1</link><description>The superior performance introduced by deep learning approaches in removingatmospheric particles such as snow and rain from a single image; favors theirusage over classical ones. However, deep learning-based approaches still sufferfrom challenges related to the particle appearance characteristics such assize, type, and transparency. Furthermore, due to the unique characteristics ofrain and snow particles, single network based deep learning approaches strugglein handling both degradation scenarios simultaneously. In this paper, a globalframework that consists of two Generative Adversarial Networks (GANs) isproposed where each handles the removal of each particle individually. Thearchitectures of both desnowing and deraining GANs introduce the integration ofa feature extraction phase with the classical U-net generator network which inturn enhances the removal performance in the presence of severe variations insize and appearance. Furthermore, a realistic dataset that contains pairs ofsnowy images next to their groundtruth images estimated using a low-rankapproximation approach; is presented. The experiments show that the proposeddesnowing and deraining approaches achieve significant improvements incomparison to the state-of-the-art approaches when tested on both synthetic andrealistic datasets.</description><author>Ibrahim Kajo, Mohamed Kas, Yassine Ruichek</author><pubDate>Thu, 07 Nov 2024 15:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04821v1</guid></item><item><title>CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep Learning and Geometric Insights</title><link>http://arxiv.org/abs/2407.03794v2</link><description>The ability to map left ventricle (LV) myocardial motion using computedtomography angiography (CTA) is essential to diagnosing cardiovascularconditions and guiding interventional procedures. Due to their inherentlocality, conventional neural networks typically have difficulty predictingsubtle tangential movements, which considerably lessens the level of precisionat which myocardium three-dimensional (3D) mapping can be performed. Using 3Doptical flow techniques and Functional Maps (FMs), we present a comprehensiveapproach to address this problem. FMs are known for their capacity to captureglobal geometric features, thus providing a fuller understanding of 3Dgeometry. As an alternative to traditional segmentation-based priors, we employsurface-based two-dimensional (2D) constraints derived from spectralcorrespondence methods. Our 3D deep learning architecture, based on the ARFlowmodel, is optimized to handle complex 3D motion analysis tasks. Byincorporating FMs, we can capture the subtle tangential movements of themyocardium surface precisely, hence significantly improving the accuracy of 3Dmapping of the myocardium. The experimental results confirm the effectivenessof this method in enhancing myocardium motion analysis. This approach cancontribute to improving cardiovascular diagnosis and treatment. Our code andadditional resources are available at:https://shaharzuler.github.io/CardioSpectrumPage</description><author>Shahar Zuler, Shai Tejman-Yarden, Dan Raviv</author><pubDate>Thu, 07 Nov 2024 15:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03794v2</guid></item><item><title>Harnessing the Power of Gradient-Based Simulations for Multi-Objective Optimization in Particle Accelerators</title><link>http://arxiv.org/abs/2411.04817v1</link><description>Particle accelerator operation requires simultaneous optimization of multipleobjectives. Multi-Objective Optimization (MOO) is particularly challenging dueto trade-offs between the objectives. Evolutionary algorithms, such as geneticalgorithm (GA), have been leveraged for many optimization problems, however,they do not apply to complex control problems by design. This paperdemonstrates the power of differentiability for solving MOO problems using aDeep Differentiable Reinforcement Learning (DDRL) algorithm in particleaccelerators. We compare DDRL algorithm with Model Free Reinforcement Learning(MFRL), GA and Bayesian Optimization (BO) for simultaneous optimization of heatload and trip rates in the Continuous Electron Beam Accelerator Facility(CEBAF). The underlying problem enforces strict constraints on both individualstates and actions as well as cumulative (global) constraint for energyrequirements of the beam. A physics-based surrogate model based on real data isdeveloped. This surrogate model is differentiable and allows back-propagationof gradients. The results are evaluated in the form of a Pareto-front for twoobjectives. We show that the DDRL outperforms MFRL, BO, and GA on highdimensional problems.</description><author>Kishansingh Rajput, Malachi Schram, Auralee Edelen, Jonathan Colen, Armen Kasparian, Ryan Roussel, Adam Carpenter, He Zhang, Jay Benesch</author><pubDate>Thu, 07 Nov 2024 15:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04817v1</guid></item><item><title>Latent Diffusion Model for Conditional Reservoir Facies Generation</title><link>http://arxiv.org/abs/2311.01968v2</link><description>Creating accurate and geologically realistic reservoir facies based onlimited measurements is crucial for field development and reservoir management,especially in the oil and gas sector. Traditional two-point geostatistics,while foundational, often struggle to capture complex geological patterns.Multi-point statistics offers more flexibility, but comes with its ownchallenges related to pattern configurations and storage limits. With the riseof Generative Adversarial Networks (GANs) and their success in various fields,there has been a shift towards using them for facies generation. However,recent advances in the computer vision domain have shown the superiority ofdiffusion models over GANs. Motivated by this, a novel Latent Diffusion Modelis proposed, which is specifically designed for conditional generation ofreservoir facies. The proposed model produces high-fidelity facies realizationsthat rigorously preserve conditioning data. It significantly outperforms aGAN-based alternative. Our implementation on GitHub:\url{https://github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation}.</description><author>Daesoo Lee, Oscar Ovanger, Jo Eidsvik, Erlend Aune, Jacob Skauvold, Ragnar Hauge</author><pubDate>Thu, 07 Nov 2024 15:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01968v2</guid></item><item><title>A Simple Packing Algorithm for Optimized Mapping of Artificial Neural Networks onto Non-Volatile Memory Cross-Bar Arrays</title><link>http://arxiv.org/abs/2411.04814v1</link><description>Neuromorphic computing with crossbar arrays has emerged as a promisingalternative to improve computing efficiency for machine learning. Previous workhas focused on implementing crossbar arrays to perform basic mathematicaloperations. However, in this paper, we explore the impact of mapping the layersof an artificial neural network onto physical cross-bar arrays arranged intiles across a chip. We have developed a simplified mapping algorithm todetermine the number of physical tiles, with fixed optimal array dimensions,and to estimate the minimum area occupied by these tiles for a given designobjective. This simplified algorithm is compared with conventional binarylinear optimization, which solves the equivalent bin-packing problem. We havefound that the optimum solution is not necessarily related to the minimumnumber of tiles; rather, it is shown to be an interaction between tile arraycapacity and the scaling properties of its peripheral circuits. Additionally,we have discovered that square arrays are not always the best choice foroptimal mapping, and that performance optimization comes at the cost of totaltile area</description><author>W. Haensch</author><pubDate>Thu, 07 Nov 2024 15:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04814v1</guid></item><item><title>LuxBank: The First Universal Dependency Treebank for Luxembourgish</title><link>http://arxiv.org/abs/2411.04813v1</link><description>The Universal Dependencies (UD) project has significantly expanded linguisticcoverage across 161 languages, yet Luxembourgish, a West Germanic languagespoken by approximately 400,000 people, has remained absent until now. In thispaper, we introduce LuxBank, the first UD Treebank for Luxembourgish,addressing the gap in syntactic annotation and analysis for this `low-research'language. We establish formal guidelines for Luxembourgish language annotation,providing the foundation for the first large-scale quantitative analysis of itssyntax. LuxBank serves not only as a resource for linguists and languagelearners but also as a tool for developing spell checkers and grammar checkers,organising existing text archives and even training large language models. Byincorporating Luxembourgish into the UD framework, we aim to enhance theunderstanding of syntactic variation within West Germanic languages and offer amodel for documenting smaller, semi-standardised languages. This work positionsLuxembourgish as a valuable resource in the broader linguistic and NLPcommunities, contributing to the study of languages with limited research andresources.</description><author>Alistair Plum, Caroline Döhmer, Emilia Milano, Anne-Marie Lutgen, Christoph Purschke</author><pubDate>Thu, 07 Nov 2024 15:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04813v1</guid></item><item><title>Soft Hoeffding Tree: A Transparent and Differentiable Model on Data Streams</title><link>http://arxiv.org/abs/2411.04812v1</link><description>We propose soft Hoeffding trees (SoHoT) as a new differentiable andtransparent model for possibly infinite and changing data streams. Streammining algorithms such as Hoeffding trees grow based on the incoming datastream, but they currently lack the adaptability of end-to-end deep learningsystems. End-to-end learning can be desirable if a feature representation islearned by a neural network and used in a tree, or if the outputs of trees arefurther processed in a deep learning model or workflow. Different fromHoeffding trees, soft trees can be integrated into such systems due to theirdifferentiability, but are neither transparent nor explainable. Our novel modelcombines the extensibility and transparency of Hoeffding trees with thedifferentiability of soft trees. We introduce a new gating function to regulatethe balance between univariate and multivariate splits in the tree. Experimentsare performed on 20 data streams, comparing SoHoT to standard Hoeffding trees,Hoeffding trees with limited complexity, and soft trees applying a sparseactivation function for sample routing. The results show that soft Hoeffdingtrees outperform Hoeffding trees in estimating class probabilities and, at thesame time, maintain transparency compared to soft trees, with relatively smalllosses in terms of AUROC and cross-entropy. We also demonstrate how to tradeoff transparency against performance using a hyperparameter, obtainingunivariate splits at one end of the spectrum and multivariate splits at theother.</description><author>Kirsten Köbschall, Lisa Hartung, Stefan Kramer</author><pubDate>Thu, 07 Nov 2024 15:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04812v1</guid></item><item><title>Defending Deep Regression Models against Backdoor Attacks</title><link>http://arxiv.org/abs/2411.04811v1</link><description>Deep regression models are used in a wide variety of safety-criticalapplications, but are vulnerable to backdoor attacks. Although many defenseshave been proposed for classification models, they are ineffective as they donot consider the uniqueness of regression models. First, the outputs ofregression models are continuous values instead of discretized labels. Thus,the potential infected target of a backdoored regression model has infinitepossibilities, which makes it impossible to be determined by existing defenses.Second, the backdoor behavior of backdoored deep regression models is triggeredby the activation values of all the neurons in the feature space, which makesit difficult to be detected and mitigated using existing defenses. To resolvethese problems, we propose DRMGuard, the first defense to identify if a deepregression model in the image domain is backdoored or not. DRMGuard formulatesthe optimization problem for reverse engineering based on the uniqueoutput-space and feature-space characteristics of backdoored deep regressionmodels. We conduct extensive evaluations on two regression tasks and fourdatasets. The results show that DRMGuard can consistently defend againstvarious backdoor attacks. We also generalize four state-of-the-art defensesdesigned for classifiers to regression models, and compare DRMGuard with them.The results show that DRMGuard significantly outperforms all those defenses.</description><author>Lingyu Du, Yupei Liu, Jinyuan Jia, Guohao Lan</author><pubDate>Thu, 07 Nov 2024 15:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04811v1</guid></item><item><title>FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)</title><link>http://arxiv.org/abs/2408.16163v2</link><description>This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating thesafety of Large Language Models (LLMs) against multi-turn conversationalattacks. Building upon the SORRY-Bench dataset, we propose a simple yeteffective method for generating adversarial prompts by breaking down harmfulqueries into seemingly innocuous sub-questions. Our approach achieves a maximumincrease of +46.22\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,GPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. Wedemonstrate that this technique poses a challenge to current LLM safetymeasures and highlights the need for more robust defenses against subtle,multi-turn attacks.</description><author>Aman Priyanshu, Supriti Vijay</author><pubDate>Thu, 07 Nov 2024 15:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16163v2</guid></item><item><title>GANESH: Generalizable NeRF for Lensless Imaging</title><link>http://arxiv.org/abs/2411.04810v1</link><description>Lensless imaging offers a significant opportunity to develop ultra-compactcameras by removing the conventional bulky lens system. However, without afocusing element, the sensor's output is no longer a direct image but a complexmultiplexed scene representation. Traditional methods have attempted to addressthis challenge by employing learnable inversions and refinement models, butthese methods are primarily designed for 2D reconstruction and do notgeneralize well to 3D reconstruction. We introduce GANESH, a novel frameworkdesigned to enable simultaneous refinement and novel view synthesis frommulti-view lensless images. Unlike existing methods that require scene-specifictraining, our approach supports on-the-fly inference without retraining on eachscene. Moreover, our framework allows us to tune our model to specific scenes,enhancing the rendering and refinement quality. To facilitate research in thisarea, we also present the first multi-view lensless dataset, LenslessScenes.Extensive experiments demonstrate that our method outperforms currentapproaches in reconstruction accuracy and refinement quality. Code and videoresults are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/</description><author>Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra</author><pubDate>Thu, 07 Nov 2024 15:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04810v1</guid></item></channel></rss>