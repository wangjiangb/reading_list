<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 07 Dec 2024 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail</title><link>http://arxiv.org/abs/2412.04472v1</link><description>We introduce Stereo Anywhere, a novel stereo-matching framework that combinesgeometric constraints with robust priors from monocular depth Vision FoundationModels (VFMs). By elegantly coupling these complementary worlds through adual-branch architecture, we seamlessly integrate stereo matching with learnedcontextual cues. Following this design, our framework introduces novel costvolume fusion mechanisms that effectively handle critical challenges such astextureless regions, occlusions, and non-Lambertian surfaces. Through our noveloptical illusion dataset, MonoTrap, and extensive evaluation across multiplebenchmarks, we demonstrate that our synthetic-only trained model achievesstate-of-the-art results in zero-shot generalization, significantlyoutperforming existing solutions while showing remarkable robustness tochallenging cases such as mirrors and transparencies.</description><author>Luca Bartolomei, Fabio Tosi, Matteo Poggi, Stefano Mattoccia</author><pubDate>Thu, 05 Dec 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04472v1</guid></item><item><title>PaintScene4D: Consistent 4D Scene Generation from Text Prompts</title><link>http://arxiv.org/abs/2412.04471v1</link><description>Recent advances in diffusion models have revolutionized 2D and 3D contentcreation, yet generating photorealistic dynamic 4D scenes remains a significantchallenge. Existing dynamic 4D generation methods typically rely on distillingknowledge from pre-trained 3D generative models, often fine-tuned on syntheticobject datasets. Consequently, the resulting scenes tend to be object-centricand lack photorealism. While text-to-video models can generate more realisticscenes with motion, they often struggle with spatial understanding and providelimited control over camera viewpoints during rendering. To address theselimitations, we present PaintScene4D, a novel text-to-4D scene generationframework that departs from conventional multi-view generative models in favorof a streamlined architecture that harnesses video generative models trained ondiverse real-world datasets. Our method first generates a reference video usinga video generation model, and then employs a strategic camera array selectionfor rendering. We apply a progressive warping and inpainting technique toensure both spatial and temporal consistency across multiple viewpoints.Finally, we optimize multi-view images using a dynamic renderer, enablingflexible camera control based on user preferences. Adopting a training-freearchitecture, our PaintScene4D efficiently produces realistic 4D scenes thatcan be viewed from arbitrary trajectories. The code will be made publiclyavailable. Our project page is at https://paintscene4d.github.io/</description><author>Vinayak Gupta, Yunze Man, Yu-Xiong Wang</author><pubDate>Thu, 05 Dec 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04471v1</guid></item><item><title>Turbo3D: Ultra-fast Text-to-3D Generation</title><link>http://arxiv.org/abs/2412.04470v1</link><description>We present Turbo3D, an ultra-fast text-to-3D system capable of generatinghigh-quality Gaussian splatting assets in under one second. Turbo3D employs arapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussianreconstructor, both operating in latent space. The 4-step, 4-view generator isa student model distilled through a novel Dual-Teacher approach, whichencourages the student to learn view consistency from a multi-view teacher andphoto-realism from a single-view teacher. By shifting the Gaussianreconstructor's inputs from pixel space to latent space, we eliminate the extraimage decoding time and halve the transformer sequence length for maximumefficiency. Our method demonstrates superior 3D generation results compared toprevious baselines, while operating in a fraction of their runtime.</description><author>Hanzhe Hu, Tianwei Yin, Fujun Luan, Yiwei Hu, Hao Tan, Zexiang Xu, Sai Bi, Shubham Tulsiani, Kai Zhang</author><pubDate>Thu, 05 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04470v1</guid></item><item><title>QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos</title><link>http://arxiv.org/abs/2412.04469v1</link><description>Online free-viewpoint video (FVV) streaming is a challenging problem, whichis relatively under-explored. It requires incremental on-the-fly updates to avolumetric representation, fast training and rendering to satisfy real-timeconstraints and a small memory footprint for efficient transmission. Ifachieved, it can enhance user experience by enabling novel applications, e.g.,3D video conferencing and live volumetric video broadcast, among others. Inthis work, we propose a novel framework for QUantized and Efficient ENcoding(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directlylearns Gaussian attribute residuals between consecutive frames at eachtime-step without imposing any structural constraints on them, allowing forhigh quality reconstruction and generalizability. To efficiently store theresiduals, we further propose a quantization-sparsity framework, which containsa learned latent-decoder for effectively quantizing attribute residuals otherthan Gaussian positions and a learned gating module to sparsify positionresiduals. We propose to use the Gaussian viewspace gradient difference vectoras a signal to separate the static and dynamic content of the scene. It acts asa guide for effective sparsity learning and speeds up training. On diverse FVVbenchmarks, QUEEN outperforms the state-of-the-art online FVV methods on allmetrics. Notably, for several highly dynamic scenes, it reduces the model sizeto just 0.7 MB per frame while training in under 5 sec and rendering at 350FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen</description><author>Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello</author><pubDate>Thu, 05 Dec 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04469v1</guid></item><item><title>NVILA: Efficient Frontier Visual Language Models</title><link>http://arxiv.org/abs/2412.04468v1</link><description>Visual language models (VLMs) have made significant advances in accuracy inrecent years. However, their efficiency has received much less attention. Thispaper introduces NVILA, a family of open VLMs designed to optimize bothefficiency and accuracy. Building on top of VILA, we improve its modelarchitecture by first scaling up the spatial and temporal resolutions, and thencompressing visual tokens. This "scale-then-compress" approach enables NVILA toefficiently process high-resolution images and long videos. We also conduct asystematic investigation to enhance the efficiency of NVILA throughout itsentire lifecycle, from training and fine-tuning to deployment. NVILA matches orsurpasses the accuracy of many leading open and proprietary VLMs across a widerange of image and video benchmarks. At the same time, it reduces trainingcosts by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code andmodels available to facilitate reproducibility.</description><author>Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu</author><pubDate>Thu, 05 Dec 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04468v1</guid></item><item><title>VisionZip: Longer is Better but Not Necessary in Vision Language Models</title><link>http://arxiv.org/abs/2412.04467v1</link><description>Recent advancements in vision-language models have enhanced performance byincreasing the length of visual tokens, making them much longer than texttokens and significantly raising computational costs. However, we observe thatthe visual tokens generated by popular vision encoders, such as CLIP andSigLIP, contain significant redundancy. To address this, we introduceVisionZip, a simple yet effective method that selects a set of informativetokens for input to the language model, reducing visual token redundancy andimproving efficiency while maintaining model performance. The proposedVisionZip can be widely applied to image and video understanding tasks and iswell-suited for multi-turn dialogues in real-world scenarios, where previousmethods tend to underperform. Experimental results show that VisionZipoutperforms the previous state-of-the-art method by at least 5% performancegains across nearly all settings. Moreover, our method significantly enhancesmodel inference speed, improving the prefilling time by 8x and enabling theLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model whileachieving better results. Furthermore, we analyze the causes of this redundancyand encourage the community to focus on extracting better visual featuresrather than merely increasing token length. Our code is available athttps://github.com/dvlab-research/VisionZip .</description><author>Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia</author><pubDate>Thu, 05 Dec 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04467v1</guid></item><item><title>UnZipLoRA: Separating Content and Style from a Single Image</title><link>http://arxiv.org/abs/2412.04465v1</link><description>This paper introduces UnZipLoRA, a method for decomposing an image into itsconstituent subject and style, represented as two distinct LoRAs (Low-RankAdaptations). Unlike existing personalization techniques that focus on eithersubject or style in isolation, or require separate training sets for each,UnZipLoRA disentangles these elements from a single image by training both theLoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs arecompatible, i.e., they can be seamlessly combined using direct addition.UnZipLoRA enables independent manipulation and recontextualization of subjectand style, including generating variations of each, applying the extractedstyle to new subjects, and recombining them to reconstruct the original imageor create novel variations. To address the challenge of subject and styleentanglement, UnZipLoRA employs a novel prompt separation technique, as well ascolumn and block separation strategies to accurately preserve thecharacteristics of subject and style, and ensure compatibility between thelearned LoRAs. Evaluation with human studies and quantitative metricsdemonstrates UnZipLoRA's effectiveness compared to other state-of-the-artmethods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA.</description><author>Chang Liu, Viraj Shah, Aiyu Cui, Svetlana Lazebnik</author><pubDate>Thu, 05 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04465v1</guid></item><item><title>DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction</title><link>http://arxiv.org/abs/2412.04464v1</link><description>The choice of data representation is a key factor in the success of deeplearning in geometric tasks. For instance, DUSt3R has recently introduced theconcept of viewpoint-invariant point maps, generalizing depth prediction, andshowing that one can reduce all the key problems in the 3D reconstruction ofstatic scenes to predicting such point maps. In this paper, we develop ananalogous concept for a very different problem, namely, the reconstruction ofthe 3D shape and pose of deformable objects. To this end, we introduce the DualPoint Maps (DualPM), where a pair of point maps is extracted from the {same}image, one associating pixels to their 3D locations on the object, and theother to a canonical version of the object at rest pose. We also extend pointmaps to amodal reconstruction, seeing through self-occlusions to obtain thecomplete shape of the object. We show that 3D reconstruction and 3D poseestimation reduce to the prediction of the DualPMs. We demonstrate empiricallythat this representation is a good target for a deep network to predict;specifically, we consider modeling horses, showing that DualPMs can be trainedpurely on 3D synthetic data, consisting of a single model of a horse, whilegeneralizing very well to real images. With this, we improve by a large marginprevious methods for the 3D analysis and reconstruction of this type ofobjects.</description><author>Ben Kaye, Tomas Jakab, Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Thu, 05 Dec 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04464v1</guid></item><item><title>MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos</title><link>http://arxiv.org/abs/2412.04463v1</link><description>We present a system that allows for accurate, fast, and robust estimation ofcamera parameters and depth maps from casual monocular videos of dynamicscenes. Most conventional structure from motion and monocular SLAM techniquesassume input videos that feature predominantly static scenes with large amountsof parallax. Such methods tend to produce erroneous estimates in the absence ofthese conditions. Recent neural network-based approaches attempt to overcomethese challenges; however, such methods are either computationally expensive orbrittle when run on dynamic videos with uncontrolled camera motion or unknownfield of view. We demonstrate the surprising effectiveness of a deep visualSLAM framework: with careful modifications to its training and inferenceschemes, this system can scale to real-world videos of complex dynamic sceneswith unconstrained camera paths, including videos with little camera parallax.Extensive experiments on both synthetic and real videos demonstrate that oursystem is significantly more accurate and robust at camera pose and depthestimation when compared with prior and concurrent work, with faster orcomparable running times. See interactive results on our project page:https://mega-sam.github.io/</description><author>Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, Noah Snavely</author><pubDate>Thu, 05 Dec 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04463v1</guid></item><item><title>4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion</title><link>http://arxiv.org/abs/2412.04462v1</link><description>We propose 4Real-Video, a novel framework for generating 4D videos, organizedas a grid of video frames with both time and viewpoint axes. In this grid, eachrow contains frames sharing the same timestep, while each column containsframes from the same viewpoint. We propose a novel two-stream architecture. Onestream performs viewpoint updates on columns, and the other stream performstemporal updates on rows. After each diffusion transformer layer, asynchronization layer exchanges information between the two token streams. Wepropose two implementations of the synchronization layer, using either hard orsoft synchronization. This feedforward architecture improves upon previous workin three ways: higher inference speed, enhanced visual quality (measured byFVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency(measured by VideoScore and Dust3R-Confidence).</description><author>Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee</author><pubDate>Thu, 05 Dec 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04462v1</guid></item><item><title>LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors</title><link>http://arxiv.org/abs/2412.04460v1</link><description>Large-scale diffusion models have achieved remarkable success in generatinghigh-quality images from textual descriptions, gaining popularity acrossvarious applications. However, the generation of layered content, such astransparent images with foreground and background layers, remains anunder-explored area. Layered content generation is crucial for creativeworkflows in fields like graphic design, animation, and digital art, wherelayer-based approaches are fundamental for flexible editing and composition. Inthis paper, we propose a novel image generation pipeline based on LatentDiffusion Models (LDMs) that generates images with two layers: a foregroundlayer (RGBA) with transparency information and a background layer (RGB). Unlikeexisting methods that generate these layers sequentially, our approachintroduces a harmonized generation mechanism that enables dynamic interactionsbetween the layers for more coherent outputs. We demonstrate the effectivenessof our method through extensive qualitative and quantitative experiments,showing significant improvements in visual coherence, image quality, and layerconsistency compared to baseline methods.</description><author>Yusuf Dalva, Yijun Li, Qing Liu, Nanxuan Zhao, Jianming Zhang, Zhe Lin, Pinar Yanardag</author><pubDate>Thu, 05 Dec 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04460v1</guid></item><item><title>Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering</title><link>http://arxiv.org/abs/2412.04459v1</link><description>We propose an efficient radiance field rendering algorithm that incorporatesa rasterization process on sparse voxels without neural networks or 3DGaussians. There are two key contributions coupled with the proposed system.The first is to render sparse voxels in the correct depth order along pixelrays by using dynamic Morton ordering. This avoids the well-known poppingartifact found in Gaussian splatting. Second, we adaptively fit sparse voxelsto different levels of detail within scenes, faithfully reproducing scenedetails while achieving high rendering frame rates. Our method improves theprevious neural-free voxel grid representation by over 4db PSNR and more than10x rendering FPS speedup, achieving state-of-the-art comparable novel-viewsynthesis results. Additionally, our neural-free sparse voxels are seamlesslycompatible with grid-based 3D processing algorithms. We achieve promising meshreconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into oursparse grid system.</description><author>Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang</author><pubDate>Thu, 05 Dec 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04459v1</guid></item><item><title>Cubify Anything: Scaling Indoor 3D Object Detection</title><link>http://arxiv.org/abs/2412.04458v1</link><description>We consider indoor 3D object detection with respect to a single RGB(-D) frameacquired from a commodity handheld device. We seek to significantly advance thestatus quo with respect to both data and modeling. First, we establish thatexisting datasets have significant limitations to scale, accuracy, anddiversity of objects. As a result, we introduce the Cubify-Anything 1M (CA-1M)dataset, which exhaustively labels over 400K 3D objects on over 1K highlyaccurate laser-scanned scenes with near-perfect registration to over 3.5Khandheld, egocentric captures. Next, we establish Cubify Transformer (CuTR), afully Transformer 3D object detection baseline which rather than operating in3D on point or voxel-based representations, predicts 3D boxes directly from 2Dfeatures derived from RGB(-D) inputs. While this approach lacks any 3Dinductive biases, we show that paired with CA-1M, CuTR outperforms point-basedmethods - accurately recalling over 62% of objects in 3D, and is significantlymore capable at handling noise and uncertainty present in commodityLiDAR-derived depth maps while also providing promising RGB only performancewithout architecture changes. Furthermore, by pre-training on CA-1M, CuTR canoutperform point-based methods on a more diverse variant of SUN RGB-D -supporting the notion that while inductive biases in 3D are useful at thesmaller sizes of existing datasets, they fail to scale to the data-rich regimeof CA-1M. Overall, this dataset and baseline model provide strong evidence thatwe are moving towards models which can effectively Cubify Anything.</description><author>Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, Afshin Dehghan</author><pubDate>Thu, 05 Dec 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04458v1</guid></item><item><title>Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps</title><link>http://arxiv.org/abs/2412.04457v1</link><description>Gaussian splatting methods are emerging as a popular approach for convertingmulti-view image data into scene representations that allow view synthesis. Inparticular, there is interest in enabling view synthesis for dynamic scenesusing only monocular input data -- an ill-posed and challenging problem. Thefast pace of work in this area has produced multiple simultaneous papers thatclaim to work best, which cannot all be true. In this work, we organize,benchmark, and analyze many Gaussian-splatting-based methods, providingapples-to-apples comparisons that prior works have lacked. We use multipleexisting datasets and a new instructive synthetic dataset designed to isolatefactors that affect reconstruction quality. We systematically categorizeGaussian splatting methods into specific motion representation types andquantify how their differences impact performance. Empirically, we find thattheir rank order is well-defined in synthetic data, but the complexity ofreal-world data currently overwhelms the differences. Furthermore, the fastrendering speed of all Gaussian-based methods comes at the cost of brittlenessin optimization. We summarize our experiments into a list of findings that canhelp to further progress in this lively problem setting. Project Webpage:https://lynl7130.github.io/MonoDyGauBench.github.io/</description><author>Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, Adam W. Harley</author><pubDate>Thu, 05 Dec 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04457v1</guid></item><item><title>HeatFormer: A Neural Optimizer for Multiview Human Mesh Recovery</title><link>http://arxiv.org/abs/2412.04456v1</link><description>We introduce a novel method for human shape and pose recovery that can fullyleverage multiple static views. We target fixed-multiview people monitoring,including elderly care and safety monitoring, in which calibrated cameras canbe installed at the corners of a room or an open space but whose configurationmay vary depending on the environment. Our key idea is to formulate it asneural optimization. We achieve this with HeatFormer, a neural optimizer thatiteratively refines the SMPL parameters given multiview images, which isfundamentally agonistic to the configuration of views. HeatFormer realizes thisSMPL parameter estimation as heat map generation and alignment with a noveltransformer encoder and decoder. We demonstrate the effectiveness of HeatFormerincluding its accuracy, robustness to occlusion, and generalizability throughan extensive set of experiments. We believe HeatFormer can serve a key role inpassive human behavior modeling.</description><author>Yuto Matsubara, Ko Nishino</author><pubDate>Thu, 05 Dec 2024 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04456v1</guid></item><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2412.01819v3</link><description>This work presents Switti, a scale-wise transformer for text-to-imagegeneration. Starting from existing next-scale prediction AR models, we firstexplore them for T2I generation and propose architectural modifications toimprove their convergence and overall performance. We then argue thatscale-wise transformers do not require causality and propose a non-causalcounterpart facilitating ~11% faster sampling and lower memory usage while alsoachieving slightly better generation quality. Furthermore, we reveal thatclassifier-free guidance at high-resolution scales is often unnecessary and caneven degrade performance. By disabling guidance at these scales, we achieve anadditional sampling acceleration of ~20% and improve the generation offine-grained details. Extensive human preference studies and automatedevaluations show that Switti outperforms existing T2I AR models and competeswith state-of-the-art T2I diffusion models while being up to 7 times faster.</description><author>Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, Dmitry Baranchuk</author><pubDate>Thu, 05 Dec 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01819v3</guid></item><item><title>Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</title><link>http://arxiv.org/abs/2412.04455v1</link><description>Automatic detection and prevention of open-set failures are crucial inclosed-loop robotic systems. Recent studies often struggle to simultaneouslyidentify unexpected failures reactively after they occur and preventforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), anovel paradigm leveraging the vision-language model (VLM) for both open-setreactive and proactive failure detection. The core of our method is toformulate both tasks as a unified set of spatio-temporal constraintsatisfaction problems and use VLM-generated code to evaluate them for real-timemonitoring. To enhance the accuracy and efficiency of monitoring, we furtherintroduce constraint elements that abstract constraint-related entities ortheir parts into compact geometric elements. This approach offers greatergenerality, simplifies tracking, and facilitates constraint-aware visualprogramming by leveraging these elements as visual prompts. Experiments showthat CaM achieves a 28.7% higher success rate and reduces execution time by31.8% under severe disturbances compared to baselines across three simulatorsand a real-world setting. Moreover, CaM can be integrated with open-loopcontrol policies to form closed-loop systems, enabling long-horizon tasks incluttered scenes with dynamic environments.</description><author>Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang</author><pubDate>Thu, 05 Dec 2024 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04455v1</guid></item><item><title>Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction</title><link>http://arxiv.org/abs/2412.04454v1</link><description>Graphical User Interfaces (GUIs) are critical to human-computer interaction,yet automating GUI tasks remains challenging due to the complexity andvariability of visual environments. Existing approaches often rely on textualrepresentations of GUIs, which introduce limitations in generalization,efficiency, and scalability. In this paper, we introduce Aguvis, a unified purevision-based framework for autonomous GUI agents that operates across variousplatforms. Our approach leverages image-based observations, and groundinginstructions in natural language to visual elements, and employs a consistentaction space to ensure cross-platform generalization. To address thelimitations of previous work, we integrate explicit planning and reasoningwithin the model, enhancing its ability to autonomously navigate and interactwith complex digital environments. We construct a large-scale dataset of GUIagent trajectories, incorporating multimodal reasoning and grounding, andemploy a two-stage training pipeline that first focuses on general GUIgrounding, followed by planning and reasoning. Through comprehensiveexperiments, we demonstrate that Aguvis surpasses previous state-of-the-artmethods in both offline and real-world online scenarios, achieving, to ourknowledge, the first fully autonomous pure vision GUI agent capable ofperforming tasks independently without collaboration with externalclosed-source models. We open-sourced all datasets, models, and trainingrecipes to facilitate future research at https://aguvis-project.github.io/.</description><author>Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong</author><pubDate>Thu, 05 Dec 2024 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04454v1</guid></item><item><title>Four-Plane Factorized Video Autoencoders</title><link>http://arxiv.org/abs/2412.04452v1</link><description>Latent variable generative models have emerged as powerful tools forgenerative tasks including image and video synthesis. These models are enabledby pretrained autoencoders that map high resolution data into a compressedlower dimensional latent space, where the generative models can subsequently bedeveloped while requiring fewer computational resources. Despite theireffectiveness, the direct application of latent variable models to higherdimensional domains such as videos continues to pose challenges for efficienttraining and inference. In this paper, we propose an autoencoder that projectsvolumetric data onto a four-plane factorized latent space that growssublinearly with the input size, making it ideal for higher dimensional datalike videos. The design of our factorized model supports straightforwardadoption in a number of conditional generation tasks with latent diffusionmodels (LDMs), such as class-conditional generation, frame prediction, andvideo interpolation. Our results show that the proposed four-plane latent spaceretains a rich representation needed for high-fidelity reconstructions despitethe heavy compression, while simultaneously enabling LDMs to operate withsignificant improvements in speed and memory.</description><author>Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia</author><pubDate>Thu, 05 Dec 2024 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04452v1</guid></item><item><title>NaVILA: Legged Robot Vision-Language-Action Model for Navigation</title><link>http://arxiv.org/abs/2412.04453v1</link><description>This paper proposes to solve the problem of Vision-and-Language Navigationwith legged robots, which not only provides a flexible way for humans tocommand but also allows the robot to navigate through more challenging andcluttered scenes. However, it is non-trivial to translate human languageinstructions all the way to low-level leg joint actions. We propose NaVILA, a2-level framework that unifies a Vision-Language-Action model (VLA) withlocomotion skills. Instead of directly predicting low-level actions from VLA,NaVILA first generates mid-level actions with spatial information in the formof language, (e.g., "moving forward 75cm"), which serves as an input for avisual locomotion RL policy for execution. NaVILA substantially improvesprevious approaches on existing benchmarks. The same advantages aredemonstrated in our newly developed benchmarks with IsaacLab, featuring morerealistic scenes, low-level controls, and real-world robot experiments. We showmore results at https://navila-bot.github.io/</description><author>An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang</author><pubDate>Thu, 05 Dec 2024 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04453v1</guid></item><item><title>p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay</title><link>http://arxiv.org/abs/2412.04449v1</link><description>Despite the remarkable performance of multimodal large language models(MLLMs) across diverse tasks, the substantial training and inference costsimpede their advancement. The majority of computation stems from theoverwhelming volume of vision tokens processed by the transformer decoder. Inthis paper, we propose to build efficient MLLMs by leveraging theMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selectsessential vision tokens to process while skipping redundant ones. However,integrating MoD into MLLMs is non-trivial. To address the challenges oftraining and inference stability as well as limited training data, we adapt theMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)and symmetric token reweighting (STRing). Moreover, we observe that visiontokens exhibit higher redundancy in deeper layer and thus design a progressiveratio decay (PRD) strategy, which gradually reduces the token retention ratiolayer by layer, employing a shifted cosine schedule. This crucial design fullyunleashes the potential of MoD, significantly boosting the efficiency andperformance of our models. To validate the effectiveness of our approach, weconduct extensive experiments with two baseline models across 14 benchmarks.Our model, p-MoD, matches or even surpasses the performance of the baselinemodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and77.7% GPU hours during training.</description><author>Jun Zhang, Desen Meng, Ji Qi, Zhenpeng Huang, Tao Wu, Limin Wang</author><pubDate>Thu, 05 Dec 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04449v1</guid></item><item><title>The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities</title><link>http://arxiv.org/abs/2411.04986v2</link><description>Modern language models can process inputs across diverse languages andmodalities. We hypothesize that models acquire this capability through learninga shared representation space across heterogeneous data types (e.g., differentlanguages and modalities), which places semantically similar inputs near oneanother, even if they are from different modalities/languages. We term this thesemantic hub hypothesis, following the hub-and-spoke model from neuroscience(Patterson et al., 2007) which posits that semantic knowledge in the humanbrain is organized through a transmodal semantic "hub" which integratesinformation from various modality-specific "spokes" regions. We first show thatmodel representations for semantically equivalent inputs in different languagesare similar in the intermediate layers, and that this space can be interpretedusing the model's dominant pretraining language via the logit lens. Thistendency extends to other data types, including arithmetic expressions, code,and visual/audio inputs. Interventions in the shared representation space inone data type also predictably affect model outputs in other data types,suggesting that this shared representations space is not simply a vestigialbyproduct of large-scale training on broad data, but something that is activelyutilized by the model during input processing.</description><author>Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim</author><pubDate>Thu, 05 Dec 2024 18:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04986v2</guid></item><item><title>MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation</title><link>http://arxiv.org/abs/2412.04448v1</link><description>Recent advances in video diffusion models have unlocked new potential forrealistic audio-driven talking video generation. However, achieving seamlessaudio-lip synchronization, maintaining long-term identity consistency, andproducing natural, audio-aligned expressions in generated talking videos remainsignificant challenges. To address these challenges, we propose Memory-guidedEMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animationapproach to generate identity-consistent and expressive talking videos. Ourapproach is built around two key modules: (1) a memory-guided temporal module,which enhances long-term identity consistency and motion smoothness bydeveloping memory states to store information from a longer past context toguide temporal modeling via linear attention; and (2) an emotion-aware audiomodule, which replaces traditional cross attention with multi-modal attentionto enhance audio-video interaction, while detecting emotions from audio torefine facial expressions via emotion adaptive layer norm. Extensivequantitative and qualitative results demonstrate that MEMO generates morerealistic talking videos across diverse image and audio types, outperformingstate-of-the-art methods in overall quality, audio-lip synchronization,identity consistency, and expression-emotion alignment.</description><author>Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan, Jiahao Lu, Chuanxin Tang, Bo An, Shuicheng Yan</author><pubDate>Thu, 05 Dec 2024 18:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04448v1</guid></item><item><title>EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios</title><link>http://arxiv.org/abs/2412.04447v1</link><description>The advent of Multimodal Large Language Models, leveraging the power of LargeLanguage Models, has recently demonstrated superior multimodal understandingand reasoning abilities, heralding a new era for artificial generalintelligence. However, achieving AGI necessitates more than just comprehensionand reasoning. A crucial capability required is effective planning in diversescenarios, which involves making reasonable decisions based on complexenvironments to solve real-world problems. Despite its importance, the planningabilities of current MLLMs in varied scenarios remain underexplored. In thispaper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmarkdesigned to assess the planning capabilities of MLLMs across a wide range ofreal-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4major domains and 24 detailed scenarios, closely aligned with human daily life.EgoPlan-Bench2 is constructed through a semi-automatic process utilizingegocentric videos, complemented by manual verification. Grounded in afirst-person perspective, it mirrors the way humans approach problem-solving ineveryday life. We evaluate 21 competitive MLLMs and provide an in-depthanalysis of their limitations, revealing that they face significant challengesin real-world planning. To further improve the planning proficiency of currentMLLMs, we propose a training-free approach using multimodal Chain-of-Thought(CoT) prompting through investigating the effectiveness of various multimodalprompts in complex planning. Our approach enhances the performance of GPT-4V by10.24 on EgoPlan-Bench2 without additional training. Our work not only shedslight on the current limitations of MLLMs in planning, but also providesinsights for future enhancements in this critical area. We have made data andcode available at https://qiulu66.github.io/egoplanbench2/.</description><author>Lu Qiu, Yuying Ge, Yi Chen, Yixiao Ge, Ying Shan, Xihui Liu</author><pubDate>Thu, 05 Dec 2024 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04447v1</guid></item><item><title>DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models</title><link>http://arxiv.org/abs/2412.04446v1</link><description>Videos are inherently temporal sequences by their very nature. In this work,we explore the potential of modeling videos in a chronological and scalablemanner with autoregressive (AR) language models, inspired by their success innatural language processing. We introduce DiCoDe, a novel approach thatleverages Diffusion-Compressed Deep Tokens to generate videos with a languagemodel in an autoregressive manner. Unlike existing methods that employlow-level representations with limited compression rates, DiCoDe utilizes deeptokens with a considerable compression rate (a 1000x reduction in token count).This significant compression is made possible by a tokenizer trained throughleveraging the prior knowledge of video diffusion models. Deep tokens enableDiCoDe to employ vanilla AR language models for video generation, akin totranslating one visual "language" into another. By treating videos as temporalsequences, DiCoDe fully harnesses the capabilities of language models forautoregressive generation. DiCoDe is scalable using readily available ARarchitectures, and is capable of generating videos ranging from a few secondsto one minute using only 4 A100 GPUs for training. We evaluate DiCoDe bothquantitatively and qualitatively, demonstrating that it performs comparably toexisting methods in terms of quality while ensuring efficient training. Toshowcase its scalability, we release a series of DiCoDe configurations withvarying parameter sizes and observe a consistent improvement in performance asthe model size increases from 100M to 3B. We believe that DiCoDe's explorationin academia represents a promising initial step toward scalable video modelingwith AR language models, paving the way for the development of larger and morepowerful video generation models.</description><author>Yizhuo Li, Yuying Ge, Yixiao Ge, Ping Luo, Ying Shan</author><pubDate>Thu, 05 Dec 2024 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04446v1</guid></item><item><title>Moto: Latent Motion Token as the Bridging Language for Robot Manipulation</title><link>http://arxiv.org/abs/2412.04445v1</link><description>Recent developments in Large Language Models pre-trained on extensive corporahave shown significant success in various natural language processing taskswith minimal fine-tuning. This success offers new promise for robotics, whichhas long been constrained by the high cost of action-labeled data. We ask:given the abundant video data containing interaction-related knowledgeavailable as a rich "corpus", can a similar generative pre-training approach beeffectively applied to enhance robot learning? The key challenge is to identifyan effective representation for autoregressive pre-training that benefits robotmanipulation tasks. Inspired by the way humans learn new skills throughobserving dynamic environments, we propose that effective robotic learningshould emphasize motion-related knowledge, which is closely tied to low-levelactions and is hardware-agnostic, facilitating the transfer of learned motionsto actual robot actions. To this end, we introduce Moto, which converts videocontent into latent Motion Token sequences by a Latent Motion Tokenizer,learning a bridging "language" of motion from videos in an unsupervised manner.We pre-train Moto-GPT through motion token autoregression, enabling it tocapture diverse visual motion knowledge. After pre-training, Moto-GPTdemonstrates the promising ability to produce semantically interpretable motiontokens, predict plausible motion trajectories, and assess trajectoryrationality through output likelihood. To transfer learned motion priors toreal robot actions, we implement a co-fine-tuning strategy that seamlesslybridges latent motion token prediction and real robot control. Extensiveexperiments show that the fine-tuned Moto-GPT exhibits superior robustness andefficiency on robot manipulation benchmarks, underscoring its effectiveness intransferring knowledge from video data to downstream visual manipulation tasks.</description><author>Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu</author><pubDate>Thu, 05 Dec 2024 18:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04445v1</guid></item><item><title>Learning Artistic Signatures: Symmetry Discovery and Style Transfer</title><link>http://arxiv.org/abs/2412.04441v1</link><description>Despite nearly a decade of literature on style transfer, there is noundisputed definition of artistic style. State-of-the-art models produceimpressive results but are difficult to interpret since, without a coherentdefinition of style, the problem of style transfer is inherently ill-posed.Early work framed style-transfer as an optimization problem but treated styleas a measure only of texture. This led to artifacts in the outputs of earlymodels where content features from the style image sometimes bled into theoutput image. Conversely, more recent work with diffusion models offerscompelling empirical results but provides little theoretical grounding. Toaddress these issues, we propose an alternative definition of artistic style.We suggest that style should be thought of as a set of global symmetries thatdictate the arrangement of local textures. We validate this perspectiveempirically by learning the symmetries of a large dataset of paintings andshowing that symmetries are predictive of the artistic movement to which eachpainting belongs. Finally, we show that by considering both local and globalfeatures, using both Lie generators and traditional measures of texture, we canquantitatively capture the stylistic similarity between artists better thanwith either set of features alone. This approach not only aligns well with arthistorians' consensus but also offers a robust framework for distinguishingnuanced stylistic differences, allowing for a more interpretable, theoreticallygrounded approach to style transfer.</description><author>Emma Finn, T. Anderson Keller, Emmanouil Theodosis, Demba E. Ba</author><pubDate>Thu, 05 Dec 2024 18:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04441v1</guid></item><item><title>GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2412.04440v1</link><description>Text-to-video generation models have shown significant progress in the recentyears. However, they still struggle with generating complex dynamic scenesbased on compositional text prompts, such as attribute binding for multipleobjects, temporal dynamics associated with different objects, and interactionsbetween objects. Our key motivation is that complex tasks can be decomposedinto simpler ones, each handled by a role-specialized MLLM agent. Multipleagents can collaborate together to achieve collective intelligence for complexgoals. We propose GenMAC, an iterative, multi-agent framework that enablescompositional text-to-video generation. The collaborative workflow includesthree stages: Design, Generation, and Redesign, with an iterative loop betweenthe Generation and Redesign stages to progressively verify and refine thegenerated videos. The Redesign stage is the most challenging stage that aims toverify the generated videos, suggest corrections, and redesign the textprompts, frame-wise layouts, and guidance scales for the next iteration ofgeneration. To avoid hallucination of a single MLLM agent, we decompose thisstage to four sequentially-executed MLLM-based agents: verification agent,suggestion agent, correction agent, and output structuring agent. Furthermore,to tackle diverse scenarios of compositional text-to-video generation, wedesign a self-routing mechanism to adaptively select the proper correctionagent from a collection of correction agents each specialized for one scenario.Extensive experiments demonstrate the effectiveness of GenMAC, achievingstate-of-the art performance in compositional text-to-video generation.</description><author>Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu Wang, Xihui Liu</author><pubDate>Thu, 05 Dec 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04440v1</guid></item><item><title>A method to benchmark high-dimensional process drift detection</title><link>http://arxiv.org/abs/2409.03669v2</link><description>Process curves are multivariate finite time series data coming frommanufacturing processes. This paper studies machine learning that detect driftsin process curve datasets. A theoretic framework to synthetically generateprocess curves in a controlled way is introduced in order to benchmark machinelearning algorithms for process drift detection. An evaluation score, calledthe temporal area under the curve, is introduced, which allows to quantify howwell machine learning models unveil curves belonging to drift segments.Finally, a benchmark study comparing popular machine learning approaches onsynthetic data generated with the introduced framework is presented that showsthat existing algorithms often struggle with datasets containing multiple driftsegments.</description><author>Edgar Wolf, Tobias Windisch</author><pubDate>Thu, 05 Dec 2024 18:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03669v2</guid></item><item><title>Masked Autoencoders are PDE Learners</title><link>http://arxiv.org/abs/2403.17728v3</link><description>Neural solvers for partial differential equations (PDEs) have great potentialto generate fast and accurate physics solutions, yet their practicality iscurrently limited by their generalizability. PDEs evolve over broad scales andexhibit diverse behaviors; predicting these phenomena will require learningrepresentations across a wide variety of inputs which may encompass differentcoefficients, boundary conditions, resolutions, or even equations. As a steptowards generalizable PDE modeling, we adapt masked pretraining for physicsproblems. Through self-supervised learning across PDEs, masked autoencoders canconsolidate heterogeneous physics to learn rich latent representations. We showthat learned representations can generalize to a limited set of unseenequations or parameters and are meaningful enough to regress PDE coefficientsor the classify PDE features. Furthermore, conditioning neural solvers onlearned latent representations can improve time-stepping and super-resolutionperformance across a variety of coefficients, discretizations, or boundaryconditions, as well as on certain unseen PDEs. We hope that masked pretrainingcan emerge as a unifying method across large, unlabeled, and heterogeneousdatasets to learn latent physics at scale.</description><author>Anthony Zhou, Amir Barati Farimani</author><pubDate>Thu, 05 Dec 2024 18:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17728v3</guid></item><item><title>Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data</title><link>http://arxiv.org/abs/2407.08726v2</link><description>Top-down Bird's Eye View (BEV) maps are a popular representation for groundrobot navigation due to their richness and flexibility for downstream tasks.While recent methods have shown promise for predicting BEV maps fromFirst-Person View (FPV) images, their generalizability is limited to smallregions captured by current autonomous vehicle-based datasets. In this context,we show that a more scalable approach towards generalizable map prediction canbe enabled by using two large-scale crowd-sourced mapping platforms, Mapillaryfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map ItAnywhere (MIA), a data engine that enables seamless curation and modeling oflabeled map prediction data from existing open-source map platforms. Using ourMIA data engine, we display the ease of automatically collecting a dataset of1.2 million pairs of FPV images &amp; BEV maps encompassing diverse geographies,landscapes, environmental factors, camera models &amp; capture scenarios. Wefurther train a simple camera model-agnostic model on this data for BEV mapprediction. Extensive evaluations using established benchmarks and our datasetshow that the data curated by MIA enables effective pretraining forgeneralizable BEV map prediction, with zero-shot performance far exceedingbaselines trained on existing datasets by 35%. Our analysis highlights thepromise of using large-scale public maps for developing &amp; testing generalizableBEV perception, paving the way for more robust autonomous navigation. Website:https://mapitanywhere.github.io/</description><author>Cherie Ho, Jiaye Zou, Omar Alama, Sai Mitheran Jagadesh Kumar, Benjamin Chiang, Taneesh Gupta, Chen Wang, Nikhil Keetha, Katia Sycara, Sebastian Scherer</author><pubDate>Thu, 05 Dec 2024 18:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08726v2</guid></item><item><title>Towards Real-Time Open-Vocabulary Video Instance Segmentation</title><link>http://arxiv.org/abs/2412.04434v1</link><description>In this paper, we address the challenge of performing open-vocabulary videoinstance segmentation (OV-VIS) in real-time. We analyze the computationalbottlenecks of state-of-the-art foundation models that performs OV-VIS, andpropose a new method, TROY-VIS, that significantly improves processing speedwhile maintaining high accuracy. We introduce three key techniques: (1)Decoupled Attention Feature Enhancer to speed up information interactionbetween different modalities and scales; (2) Flash Embedding Memory forobtaining fast text embeddings of object categories; and, (3) KernelInterpolation for exploiting the temporal continuity in videos. Our experimentsdemonstrate that TROY-VIS achieves the best trade-off between accuracy andspeed on two large-scale OV-VIS benchmarks, BURST and LV-VIS, running 20xfaster than GLEE-Lite (25 FPS v.s. 1.25 FPS) with comparable or even betteraccuracy. These results demonstrate TROY-VIS's potential for real-timeapplications in dynamic environments such as mobile robotics and augmentedreality. Code and model will be released athttps://github.com/google-research/troyvis.</description><author>Bin Yan, Martin Sundermeyer, David Joseph Tan, Huchuan Lu, Federico Tombari</author><pubDate>Thu, 05 Dec 2024 18:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04434v1</guid></item><item><title>PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human Avatars</title><link>http://arxiv.org/abs/2412.04433v1</link><description>This paper introduces a novel clothed human model that can be learned frommultiview RGB videos, with a particular emphasis on recovering physicallyaccurate body and cloth movements. Our method, Position Based Dynamic Gaussians(PBDyG), realizes ``movement-dependent'' cloth deformation via physicalsimulation, rather than merely relying on ``pose-dependent'' rigidtransformations. We model the clothed human holistically but with two distinctphysical entities in contact: clothing modeled as 3D Gaussians, which areattached to a skinned SMPL body that follows the movement of the person in theinput videos. The articulation of the SMPL body also drives physically-basedsimulation of the clothes' Gaussians to transform the avatar to novel poses. Inorder to run position based dynamics simulation, physical properties includingmass and material stiffness are estimated from the RGB videos through Dynamic3D Gaussian Splatting. Experiments demonstrate that our method not onlyaccurately reproduces appearance but also enables the reconstruction of avatarswearing highly deformable garments, such as skirts or coats, which have beenchallenging to reconstruct using existing methods.</description><author>Shota Sasaki, Jane Wu, Ko Nishino</author><pubDate>Thu, 05 Dec 2024 18:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04433v1</guid></item><item><title>Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation</title><link>http://arxiv.org/abs/2412.04432v1</link><description>In recent years, there has been a significant surge of interest in unifyingimage comprehension and generation within Large Language Models (LLMs). Thisgrowing interest has prompted us to explore extending this unification tovideos. The core challenge lies in developing a versatile video tokenizer thatcaptures both the spatial characteristics and temporal dynamics of videos toobtain representations for LLMs, and the representations can be further decodedinto realistic video clips to enable video generation. In this work, weintroduce Divot, a Diffusion-Powered Video Tokenizer, which leverages thediffusion process for self-supervised video representation learning. We positthat if a video diffusion model can effectively de-noise video clips by takingthe features of a video tokenizer as the condition, then the tokenizer hassuccessfully captured robust spatial and temporal information. Additionally,the video diffusion model inherently functions as a de-tokenizer, decodingvideos from their representations. Building upon the Divot tokenizer, wepresent Divot-Vicuna through video-to-text autoregression and text-to-videogeneration by modeling the distributions of continuous-valued Divot featureswith a Gaussian Mixture Model. Experimental results demonstrate that ourdiffusion-based video tokenizer, when integrated with a pre-trained LLM,achieves competitive performance across various video comprehension andgeneration benchmarks. The instruction tuned Divot-Vicuna also excels in videostorytelling, generating interleaved narratives and corresponding videos.</description><author>Yuying Ge, Yizhuo Li, Yixiao Ge, Ying Shan</author><pubDate>Thu, 05 Dec 2024 18:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04432v1</guid></item><item><title>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</title><link>http://arxiv.org/abs/2412.04431v1</link><description>We present Infinity, a Bitwise Visual AutoRegressive Modeling capable ofgenerating high-resolution, photorealistic images following languageinstruction. Infinity redefines visual autoregressive model under a bitwisetoken prediction framework with an infinite-vocabulary tokenizer &amp; classifierand bitwise self-correction mechanism, remarkably improving the generationcapacity and details. By theoretically scaling the tokenizer vocabulary size toinfinity and concurrently scaling the transformer size, our methodsignificantly unleashes powerful scaling capabilities compared to vanilla VAR.Infinity sets a new record for autoregressive text-to-image models,outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably,Infinity surpasses SD3-Medium by improving the GenEval benchmark score from0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving awin rate of 66%. Without extra optimization, Infinity generates a high-quality1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium andestablishing it as the fastest text-to-image model. Models and codes will bereleased to promote further exploration of Infinity for visual generation andunified tokenizer modeling.</description><author>Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</author><pubDate>Thu, 05 Dec 2024 18:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04431v1</guid></item><item><title>Grounding Descriptions in Images informs Zero-Shot Visual Recognition</title><link>http://arxiv.org/abs/2412.04429v1</link><description>Vision-language models (VLMs) like CLIP have been cherished for their abilityto perform zero-shot visual recognition on open-vocabulary concepts. This isachieved by selecting the object category whose textual representation bearsthe highest similarity with the query image. While successful in some domains,this method struggles with identifying fine-grained entities as well asgeneralizing to unseen concepts that are not captured by the trainingdistribution. Recent works attempt to mitigate these challenges by integratingcategory descriptions at test time, albeit yielding modest improvements. Weattribute these limited gains to a fundamental misalignment between image anddescription representations, which is rooted in the pretraining structure ofCLIP. In this paper, we propose GRAIN, a new pretraining strategy aimed ataligning representations at both fine and coarse levels simultaneously. Ourapproach learns to jointly ground textual descriptions in image regions alongwith aligning overarching captions with global image representations. To drivethis pre-training, we leverage frozen Multimodal Large Language Models (MLLMs)to derive large-scale synthetic annotations. We demonstrate the enhancedzero-shot performance of our model compared to current state-of-the art methodsacross 11 diverse image classification datasets. Additionally, we introduceProducts-2023, a newly curated, manually labeled dataset featuring novelconcepts, and showcase our model's ability to recognize these concepts bybenchmarking on it. Significant improvements achieved by our model on otherdownstream tasks like retrieval further highlight the superior quality ofrepresentations learned by our approach. Code available athttps://github.com/shaunak27/grain-clip .</description><author>Shaunak Halbe, Junjiao Tian, K J Joseph, James Seale Smith, Katherine Stevo, Vineeth N Balasubramanian, Zsolt Kira</author><pubDate>Thu, 05 Dec 2024 18:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04429v1</guid></item><item><title>Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy</title><link>http://arxiv.org/abs/2412.04426v1</link><description>The high costs and risks involved in extensive environment interactionshinder the practical application of current online safe reinforcement learning(RL) methods. While offline safe RL addresses this by learning policies fromstatic datasets, the performance therein is usually limited due to reliance ondata quality and challenges with out-of-distribution (OOD) actions. Inspired byrecent successes in offline-to-online (O2O) RL, it is crucial to explorewhether offline safe RL can be leveraged to facilitate faster and safer onlinepolicy learning, a direction that has yet to be fully investigated. To fillthis gap, we first demonstrate that naively applying existing O2O algorithmsfrom standard RL would not work well in the safe RL setting due to two uniquechallenges: \emph{erroneous Q-estimations}, resulted from offline-onlineobjective mismatch and offline cost sparsity, and \emph{Lagrangian mismatch},resulted from difficulties in aligning Lagrange multipliers between offline andonline policies. To address these challenges, we introduce \textbf{Marvel}, anovel framework for O2O safe RL, comprising two key components that work inconcert: \emph{Value Pre-Alignment} to align the Q-functions with theunderlying truth before online learning, and \emph{Adaptive PID Control} toeffectively adjust the Lagrange multipliers during online finetuning. Extensiveexperiments demonstrate that Marvel significantly outperforms existingbaselines in both reward maximization and safety constraint satisfaction. Byintroducing the first policy-finetuning based framework for O2O safe RL, whichis compatible with many offline and online safe RL methods, our work has thegreat potential to advance the field towards more efficient and practical safeRL solutions.</description><author>Keru Chen, Honghao Wei, Zhigang Deng, Sen Lin</author><pubDate>Thu, 05 Dec 2024 18:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04426v1</guid></item><item><title>CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing</title><link>http://arxiv.org/abs/2412.04425v1</link><description>We introduce Condition-Aware Self-Supervised Learning Representation(CA-SSLR), a generalist conditioning model broadly applicable to variousspeech-processing tasks. Compared to standard fine-tuning methods that optimizefor downstream models, CA-SSLR integrates language and speaker embeddings fromearlier layers, making the SSL model aware of the current language and speakercontext. This approach reduces the reliance on input audio features whilepreserving the integrity of the base SSLR. CA-SSLR improves the model'scapabilities and demonstrates its generality on unseen tasks with minimaltask-specific tuning. Our method employs linear modulation to dynamicallyadjust internal representations, enabling fine-grained adaptability withoutsignificantly altering the original model behavior. Experiments show thatCA-SSLR reduces the number of trainable parameters, mitigates overfitting, andexcels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a10% relative reduction in LID errors, a 37% improvement in ASR CER on theML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1, demonstratingits effectiveness.</description><author>Yen-Ju Lu, Jing Liu, Thomas Thebaud, Laureano Moro-Velazquez, Ariya Rastrow, Najim Dehak, Jesus Villalba</author><pubDate>Thu, 05 Dec 2024 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04425v1</guid></item><item><title>Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion</title><link>http://arxiv.org/abs/2412.04424v1</link><description>We present Florence-VL, a new family of multimodal large language models(MLLMs) with enriched visual representations produced by Florence-2, agenerative vision foundation model. Unlike the widely used CLIP-style visiontransformer trained by contrastive learning, Florence-2 can capture differentlevels and aspects of visual features, which are more versatile to be adaptedto diverse downstream tasks. We propose a novel feature-fusion architecture andan innovative training recipe that effectively integrates Florence-2's visualfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, wepropose "depth-breath fusion (DBFusion)" to fuse the visual features extractedfrom different depths and under multiple prompts. Our model training iscomposed of end-to-end pretraining of the whole model followed by finetuning ofthe projection layer and the LLM, on a carefully designed recipe of diverseopen-source datasets that include high-quality image captions andinstruction-tuning pairs. Our quantitative analysis and visualization ofFlorence-VL's visual features show its advantages over popular vision encoderson vision-language alignment, where the enriched depth and breath playimportant roles. Florence-VL achieves significant improvements over existingstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarkscovering general VQA, perception, hallucination, OCR, Chart,knowledge-intensive understanding, etc. To facilitate future research, ourmodels and the complete training recipe are open-sourced.https://github.com/JiuhaiChen/Florence-VL</description><author>Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, Bin Xiao</author><pubDate>Thu, 05 Dec 2024 18:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04424v1</guid></item><item><title>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models</title><link>http://arxiv.org/abs/2403.07384v2</link><description>Despite the effectiveness of data selection for large language models (LLMs)during pretraining and instruction fine-tuning phases, improving dataefficiency in supervised fine-tuning (SFT) for specialized domains posessignificant challenges due to the complexity of fine-tuning data. To bridgethis gap, we introduce an effective and scalable data selection method for SFT,SmallToLarge (S2L), which leverages training trajectories from small models toguide the data selection for larger models. We demonstrate through extensiveexperiments that S2L significantly improves data efficiency in SFT formathematical problem-solving, reducing the training data to just 11% of theoriginal MathInstruct dataset (Yue et al., 2023) to match full datasetperformance while outperforming state-of-the-art data selection algorithms byan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the mostchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li etal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset(Johnson et al., 2016), S2L again outperforms training on the full datasetusing only 50% of the data. Notably, S2L can perform data selection using areference model 40x smaller than the target model, proportionally reducing thecost of data selection.</description><author>Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman</author><pubDate>Thu, 05 Dec 2024 18:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07384v2</guid></item><item><title>Negative Token Merging: Image-based Adversarial Feature Guidance</title><link>http://arxiv.org/abs/2412.01339v2</link><description>Text-based adversarial guidance using a negative prompt has emerged as awidely adopted approach to steer diffusion models away from producing undesiredconcepts. While useful, performing adversarial guidance using text alone can beinsufficient to capture complex visual concepts or avoid specific visualelements like copyrighted characters. In this paper, for the first time weexplore an alternate modality in this direction by performing adversarialguidance directly using visual features from a reference image or other imagesin a batch. We introduce negative token merging (NegToMe), a simple buteffective training-free approach which performs adversarial guidance throughimages by selectively pushing apart matching visual features between referenceand generated images during the reverse diffusion process. By simply adjustingthe used reference, NegToMe enables a diverse range of applications. Notably,when using other images in same batch as reference, we find that NegToMesignificantly enhances output diversity (e.g., racial, gender, visual) byguiding features of each image away from others. Similarly, when used w.r.t.copyrighted reference images, NegToMe reduces visual similarity to copyrightedcontent by 34.57%. NegToMe is simple to implement using just few-lines of code,uses only marginally higher (&lt;4%) inference time and is compatible withdifferent diffusion architectures, including those like Flux, which don'tnatively support the use of a negative prompt. Code is available athttps://negtome.github.io</description><author>Jaskirat Singh, Lindsey Li, Weijia Shi, Ranjay Krishna, Yejin Choi, Pang Wei Koh, Michael F. Cohen, Stephen Gould, Liang Zheng, Luke Zettlemoyer</author><pubDate>Thu, 05 Dec 2024 18:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01339v2</guid></item><item><title>FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning</title><link>http://arxiv.org/abs/2412.04416v1</link><description>Federated Learning (FL) marks a transformative approach to distributed modeltraining by combining locally optimized models from various clients into aunified global model. While FL preserves data privacy by eliminatingcentralized storage, it encounters significant challenges such as performancedegradation, slower convergence, and reduced robustness of the global model dueto the heterogeneity in client data distributions. Among the various forms ofdata heterogeneity, label skew emerges as a particularly formidable andprevalent issue, especially in domains such as image classification. To addressthese challenges, we begin with comprehensive experiments to pinpoint theunderlying issues in the FL training process. Based on our findings, we thenintroduce an innovative dual-strategy approach designed to effectively resolvethese issues. First, we introduce an adaptive loss function for client-sidetraining, meticulously crafted to preserve previously acquired knowledge whilemaintaining an optimal equilibrium between local optimization and global modelcoherence. Secondly, we develop a dynamic aggregation strategy for aggregatingclient models at the server. This approach adapts to each client's uniquelearning patterns, effectively addressing the challenges of diverse data acrossthe network. Our comprehensive evaluation, conducted across three diversereal-world datasets, coupled with theoretical convergence guarantees,demonstrates the superior efficacy of our method compared to severalestablished state-of-the-art approaches.</description><author>Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal</author><pubDate>Thu, 05 Dec 2024 18:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04416v1</guid></item><item><title>Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation</title><link>http://arxiv.org/abs/2412.04415v1</link><description>AI agents, powered by large language models (LLMs), have transformedhuman-computer interactions by enabling seamless, natural, and context-awarecommunication. While these advancements offer immense utility, they alsoinherit and amplify inherent safety risks such as bias, fairness,hallucinations, privacy breaches, and a lack of transparency. This paperinvestigates a critical vulnerability: adversarial attacks targeting the LLMcore within AI agents. Specifically, we test the hypothesis that a deceptivelysimple adversarial prefix, such as \textit{Ignore the document}, can compelLLMs to produce dangerous or unintended outputs by bypassing their contextualsafeguards. Through experimentation, we demonstrate a high attack success rate(ASR), revealing the fragility of existing LLM defenses. These findingsemphasize the urgent need for robust, multi-layered security measures tailoredto mitigate vulnerabilities at the LLM level and within broader agent-basedarchitectures.</description><author>Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian</author><pubDate>Thu, 05 Dec 2024 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04415v1</guid></item><item><title>WaveletGPT: Wavelets Meet Large Language Models</title><link>http://arxiv.org/abs/2409.12924v3</link><description>Large Language Models (LLMs) have ushered in a new wave of artificialintelligence advancements impacting every scientific field and discipline. Theyare trained on a simple objective: to predict the next token given the previouscontext. We live in a world where most of the data around us, e.g., text,audio, and music, has a multi-scale structure associated with it. This paperinfuses LLMs with traditional signal processing ideas, namely wavelets, duringpre-training to take advantage of the structure. Without adding \textbf{anyextra parameters} to a GPT-style LLM architecture, we achieve the samepre-training performance almost twice as fast in text, raw audio, and symbolicmusic. This is achieved by imposing a structure on intermediate embeddings.When trained for the same number of training steps, we achieve significantgains in performance, which is comparable to pre-training a larger neuralarchitecture. Our architecture allows every next token prediction access tointermediate embeddings at different temporal resolutions in every Transformerdecoder block. This work will hopefully pave the way for incorporatingmulti-rate signal processing ideas into traditional LLM pre-training. Further,we showcase pushing model performance by improving internal structure insteadof just going after scale.</description><author>Prateek Verma</author><pubDate>Thu, 05 Dec 2024 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12924v3</guid></item><item><title>Efficient Task Grouping Through Samplewise Optimisation Landscape Analysis</title><link>http://arxiv.org/abs/2412.04413v1</link><description>Shared training approaches, such as multi-task learning (MTL) andgradient-based meta-learning, are widely used in various machine learningapplications, but they often suffer from negative transfer, leading toperformance degradation in specific tasks. While several optimisationtechniques have been developed to mitigate this issue for pre-selected taskcohorts, identifying optimal task combinations for joint learning - known astask grouping - remains underexplored and computationally challenging due tothe exponential growth in task combinations and the need for extensive trainingand evaluation cycles. This paper introduces an efficient task groupingframework designed to reduce these overwhelming computational demands of theexisting methods. The proposed framework infers pairwise task similaritiesthrough a sample-wise optimisation landscape analysis, eliminating the need forthe shared model training required to infer task similarities in existingmethods. With task similarities acquired, a graph-based clustering algorithm isemployed to pinpoint near-optimal task groups, providing an approximate yetefficient and effective solution to the originally NP-hard problem. Empiricalassessments conducted on 8 different datasets highlight the effectiveness ofthe proposed framework, revealing a five-fold speed enhancement compared toprevious state-of-the-art methods. Moreover, the framework consistentlydemonstrates comparable performance, confirming its remarkable efficiency andeffectiveness in task grouping.</description><author>Anshul Thakur, Yichen Huang, Soheila Molaei, Yujiang Wang, David A. Clifton</author><pubDate>Thu, 05 Dec 2024 18:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04413v1</guid></item><item><title>Stabilizing and Solving Inverse Problems using Data and Machine Learning</title><link>http://arxiv.org/abs/2412.04409v1</link><description>We consider an inverse problem involving the reconstruction of the solutionto a nonlinear partial differential equation (PDE) with unknown boundaryconditions. Instead of direct boundary data, we are provided with a largedataset of boundary observations for typical solutions (collective data) and abulk measurement of a specific realization. To leverage this collective data,we first compress the boundary data using proper orthogonal decomposition (POD)in a linear expansion. Next, we identify a possible nonlinear low-dimensionalstructure in the expansion coefficients using an auto-encoder, which provides aparametrization of the dataset in a lower-dimensional latent space. We thentrain a neural network to map the latent variables representing the boundarydata to the solution of the PDE. Finally, we solve the inverse problem byoptimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method in the linearsetting and establish optimal error estimates in the $H^1$ and $L^2$-norms. Thenonlinear problem is then studied numerically, demonstrating the effectivenessof our approach.</description><author>Erik Burman, Mats G. Larson, Karl Larsson, Carl Lundholm</author><pubDate>Thu, 05 Dec 2024 18:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04409v1</guid></item><item><title>Providing Differential Privacy for Federated Learning Over Wireless: A Cross-layer Framework</title><link>http://arxiv.org/abs/2412.04408v1</link><description>Federated Learning (FL) is a distributed machine learning framework thatinherently allows edge devices to maintain their local training data, thusproviding some level of privacy. However, FL's model updates still pose a riskof privacy leakage, which must be mitigated. Over-the-air FL (OTA-FL) is anadapted FL design for wireless edge networks that leverages the naturalsuperposition property of the wireless medium. We propose a wireless physicallayer (PHY) design for OTA-FL which improves differential privacy (DP) througha decentralized, dynamic power control that utilizes both inherent Gaussiannoise in the wireless channel and a cooperative jammer (CJ) for additionalartificial noise generation when higher privacy levels are required. Althoughprimarily implemented within the Upcycled-FL framework, where aresource-efficient method with first-order approximations is used at every eveniteration to decrease the required information from clients, our power controlstrategy is applicable to any FL framework, including FedAvg and FedProx asshown in the paper. This adaptation showcases the flexibility and effectivenessof our design across different learning algorithms while maintaining a strongemphasis on privacy. Our design removes the need for client-side artificialnoise injection for DP, utilizing a cooperative jammer to enhance privacywithout affecting transmission efficiency for higher privacy demands. Privacyanalysis is provided using the Moments Accountant method. We perform aconvergence analysis for non-convex objectives to tackle heterogeneous datadistributions, highlighting the inherent trade-offs between privacy andaccuracy. Numerical results show that our approach with various FL algorithmsoutperforms the state-of-the-art under the same DP conditions on the non-i.i.d.FEMNIST dataset, and highlight the cooperative jammer's effectiveness inensuring strict privacy.</description><author>Jiayu Mao, Tongxin Yin, Aylin Yener, Mingyan Liu</author><pubDate>Thu, 05 Dec 2024 18:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04408v1</guid></item><item><title>Federated Automated Feature Engineering</title><link>http://arxiv.org/abs/2412.04404v1</link><description>Automated feature engineering (AutoFE) is used to automatically create newfeatures from original features to improve predictive performance withoutneeding significant human intervention and expertise. Many algorithms exist forAutoFE, but very few approaches exist for the federated learning (FL) settingwhere data is gathered across many clients and is not shared between clients ora central server. We introduce AutoFE algorithms for the horizontal, vertical,and hybrid FL settings, which differ in how the data is gathered acrossclients. To the best of our knowledge, we are the first to develop AutoFEalgorithms for the horizontal and hybrid FL cases, and we show that thedownstream model performance of federated AutoFE is similar to the case wheredata is held centrally and AutoFE is performed centrally.</description><author>Tom Overman, Diego Klabjan</author><pubDate>Thu, 05 Dec 2024 18:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04404v1</guid></item><item><title>Establishing Task Scaling Laws via Compute-Efficient Model Ladders</title><link>http://arxiv.org/abs/2412.04403v1</link><description>We develop task scaling laws and model ladders to predict the individual taskperformance of pretrained language models (LMs) in the overtrained setting.Standard power laws for language modeling loss cannot accurately model taskperformance. Therefore, we leverage a two-step prediction approach: first usemodel and data size to predict a task-specific loss, and then use this taskloss to predict task performance. We train a set of small-scale "ladder"models, collect data points to fit the parameterized functions of the twoprediction steps, and make predictions for two target models: a 7B modeltrained to 4T tokens and a 13B model trained to 5T tokens. Training the laddermodels only costs 1% of the compute used for the target models. On fourmultiple-choice tasks written in ranked classification format, we can predictthe accuracy of both target models within 2 points of absolute error. We havehigher prediction error on four other tasks (average absolute error 6.9) andfind that these are often tasks with higher variance in task metrics. We alsofind that using less compute to train fewer ladder models tends to deterioratepredictions. Finally, we empirically show that our design choices and thetwo-step approach lead to superior performance in establishing scaling laws.</description><author>Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, Hannaneh Hajishirzi</author><pubDate>Thu, 05 Dec 2024 18:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04403v1</guid></item><item><title>Learning to Reconstruct Accelerated MRI Through K-space Cold Diffusion without Noise</title><link>http://arxiv.org/abs/2311.10162v3</link><description>Deep learning-based MRI reconstruction models have achieved superiorperformance these days. Most recently, diffusion models have shown remarkableperformance in image generation, in-painting, super-resolution, image editingand more. As a generalized diffusion model, cold diffusion further broadens thescope and considers models built around arbitrary image transformations such asblurring, down-sampling, etc. In this paper, we propose a k-space colddiffusion model that performs image degradation and restoration in k-spacewithout the need for Gaussian noise. We provide comparisons with multiple deeplearning-based MRI reconstruction models and perform tests on a well-knownlarge open-source MRI dataset. Our results show that this novel way ofperforming degradation can generate high-quality reconstruction images foraccelerated MRI.</description><author>Guoyao Shen, Mengyu Li, Chad W. Farris, Stephan Anderson, Xin Zhang</author><pubDate>Thu, 05 Dec 2024 18:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10162v3</guid></item><item><title>Regularization by Neural Style Transfer for MRI Field-Transfer Reconstruction with Limited Data</title><link>http://arxiv.org/abs/2308.10968v2</link><description>Recent advances in MRI reconstruction have achieved remarkable success withdeep learning-based models. However, most methods depend on large-scale,task-specific datasets, leaving reconstruction in data-limited settings as acritical but underexplored challenge. Regularization by denoising (RED) is ageneral pipeline that incorporates a denoiser as a prior for imagereconstruction, showing promising results in various image processing tasks,including denoising, deblurring, and super-resolution. In this work, we proposea regularization by neural style transfer (RNST) method to further leverage thepriors from the neural transfer and denoising engine. RNST effectivelyreconstructs high-quality images from noisy, low-quality inputs across varyingimage styles, even with limited data. We validate RNST on clinical MRI scans,demonstrating its ability to significantly improve image quality. Thesefindings underline the potential of RNST for MRI field-transfer reconstructionand its promise in addressing reconstruction tasks in data-constrainedscenarios.</description><author>Guoyao Shen, Yancheng Zhu, Mengyu Li, Ryan McNaughton, Hernan Jara, Sean B. Andersson, Chad W. Farris, Stephan Anderson, Xin Zhang</author><pubDate>Thu, 05 Dec 2024 18:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10968v2</guid></item><item><title>Asynchronous Batch Bayesian Optimization with Pipelining Evaluations for Experimental Resource$\unicode{x2013}$constrained Conditions</title><link>http://arxiv.org/abs/2412.04392v1</link><description>Bayesian optimization is efficient even with a small amount of data and isused in engineering and in science, including biology and chemistry. InBayesian optimization, a parameterized model with an uncertainty is fitted toexplain the experimental data, and then the model suggests parameters thatwould most likely improve the results. Batch Bayesian optimization reduces theprocessing time of optimization by parallelizing experiments. However, batchBayesian optimization cannot be applied if the number of parallelizedexperiments is limited by the cost or scarcity of equipment; in such cases,sequential methods require an unrealistic amount of time. In this study, wedeveloped pipelining Bayesian optimization (PipeBO) to reduce the processingtime of optimization even with a limited number of parallel experiments. PipeBOwas inspired by the pipelining of central processing unit architecture, whichdivides computational tasks into multiple processes. PipeBO was designed toachieve experiment parallelization by overlapping various processes of theexperiments. PipeBO uses the results of completed experiments to update theparameters of running parallelized experiments. Using the Black-BoxOptimization Benchmarking, which consists of 24 benchmark functions, wecompared PipeBO with the sequential Bayesian optimization methods. PipeBOreduced the average processing time of optimization to about 56% for theexperiments that consisted of two processes or even less for those with moreprocesses for 20 out of the 24 functions. Overall, PipeBO parallelizes Bayesianoptimization in the resource-constrained settings so that efficientoptimization can be achieved.</description><author>Yujin Taguchi, Yusuke Shibuya, Yusuke Hiki, Takashi Morikura, Takahiro G. Yamada, Akira Funahashi</author><pubDate>Thu, 05 Dec 2024 18:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04392v1</guid></item><item><title>Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2412.04384v1</link><description>3D semantic occupancy prediction is an important task for robustvision-centric autonomous driving, which predicts fine-grained geometry andsemantics of the surrounding scene. Most existing methods leverage densegrid-based scene representations, overlooking the spatial sparsity of thedriving scenes. Although 3D semantic Gaussian serves as an object-centricsparse alternative, most of the Gaussians still describe the empty region withlow efficiency. To address this, we propose a probabilistic Gaussiansuperposition model which interprets each Gaussian as a probabilitydistribution of its neighborhood being occupied and conforms to probabilisticmultiplication to derive the overall geometry. Furthermore, we adopt the exactGaussian mixture model for semantics calculation to avoid unnecessaryoverlapping of Gaussians. To effectively initialize Gaussians in non-emptyregion, we design a distribution-based initialization module which learns thepixel-aligned occupancy distribution instead of the depth of surfaces. Weconduct extensive experiments on nuScenes and KITTI-360 datasets and ourGaussianFormer-2 achieves state-of-the-art performance with high efficiency.Code: https://github.com/huang-yh/GaussianFormer.</description><author>Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jiwen Lu</author><pubDate>Thu, 05 Dec 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04384v1</guid></item><item><title>SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding</title><link>http://arxiv.org/abs/2412.04383v1</link><description>3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based ontextual descriptions, which is essential for applications like augmentedreality and robotics. Traditional 3DVG approaches rely on annotated 3D datasetsand predefined object categories, limiting scalability and adaptability. Toovercome these limitations, we introduce SeeGround, a zero-shot 3DVG frameworkleveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. Wepropose to represent 3D scenes as a hybrid of query-aligned rendered images andspatially enriched text descriptions, bridging the gap between 3D data and2D-VLMs input formats. We propose two modules: the Perspective AdaptationModule, which dynamically selects viewpoints for query-relevant imagerendering, and the Fusion Alignment Module, which integrates 2D images with 3Dspatial descriptions to enhance object localization. Extensive experiments onScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shotmethods by large margins. Notably, we exceed weakly supervised methods andrival some fully supervised ones, outperforming previous SOTA by 7.7% onScanRefer and 7.1% on Nr3D, showcasing its effectiveness.</description><author>Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang</author><pubDate>Thu, 05 Dec 2024 17:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04383v1</guid></item><item><title>EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding</title><link>http://arxiv.org/abs/2412.04380v1</link><description>3D occupancy prediction provides a comprehensive description of thesurrounding scenes and has become an essential task for 3D perception. Mostexisting methods focus on offline perception from one or a few views and cannotbe applied to embodied agents which demands to gradually perceive the scenethrough progressive embodied exploration. In this paper, we formulate anembodied 3D occupancy prediction task to target this practical scenario andpropose a Gaussian-based EmbodiedOcc framework to accomplish it. We initializethe global scene with uniform 3D semantic Gaussians and progressively updatelocal regions observed by the embodied agent. For each update, we extractsemantic and structural features from the observed image and efficientlyincorporate them via deformable cross-attention to refine the regionalGaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown(i.e., uniformly distributed) environment and maintains an explicit globalmemory of it with 3D Gaussians. It gradually gains knowledge through localrefinement of regional Gaussians, which is consistent with how humansunderstand new scenes through embodied exploration. We reorganize anEmbodiedOcc-ScanNet benchmark based on local annotations to facilitate theevaluation of the embodied 3D occupancy prediction task. Experimentsdemonstrate that our EmbodiedOcc outperforms existing local prediction methodsand accomplishes the embodied occupancy prediction with high accuracy andstrong expandability. Our code is available at:https://github.com/YkiWu/EmbodiedOcc.</description><author>Yuqi Wu, Wenzhao Zheng, Sicheng Zuo, Yuanhui Huang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 05 Dec 2024 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04380v1</guid></item><item><title>MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding</title><link>http://arxiv.org/abs/2411.17762v2</link><description>We introduce MUSE-VL, a Unified Vision-Language Model through Semanticdiscrete Encoding for multimodal understanding and generation. Recently, theresearch community has begun exploring unified models for visual generation andunderstanding. However, existing vision tokenizers (e.g., VQGAN) only considerlow-level information, which makes it difficult to align with texture semanticfeatures. This results in high training complexity and necessitates a largeamount of training data to achieve optimal performance. Additionally, theirperformance is still far from dedicated understanding models. This paperproposes Semantic Discrete Encoding (SDE), which effectively aligns theinformation of visual tokens and language tokens by adding semantic constraintsto the visual tokenizer. This greatly reduces training difficulty and improvesthe performance of the unified model. The proposed model significantlysurpasses the previous state-of-the-art in various vision-language benchmarksand achieves better performance than dedicated understanding models.</description><author>Rongchang Xie, Chen Du, Ping Song, Chang Liu</author><pubDate>Thu, 05 Dec 2024 17:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17762v2</guid></item><item><title>Discriminative Fine-tuning of LVLMs</title><link>http://arxiv.org/abs/2412.04378v1</link><description>Contrastively-trained Vision-Language Models (VLMs) like CLIP have become thede facto approach for discriminative vision-language representation learning.However, these models have limited language understanding, often exhibiting a"bag of words" behavior. At the same time, Large Vision-Language Models(LVLMs), which combine vision encoders with LLMs, have been shown capable ofdetailed vision-language reasoning, yet their autoregressive nature rendersthem less suitable for discriminative tasks. In this work, we propose to combine "the best of both worlds": a new trainingapproach for discriminative fine-tuning of LVLMs that results in strongdiscriminative and compositional capabilities. Essentially, our approachconverts a generative LVLM into a discriminative one, unlocking its capabilityfor powerful image-text discrimination combined with enhanced languageunderstanding. Our contributions include: (1) A carefully designed training/optimizationframework that utilizes image-text pairs of variable length and granularity fortraining the model with both contrastive and next-token prediction losses. Thisis accompanied by ablation studies that justify the necessity of ourframework's components. (2) A parameter-efficient adaptation method using acombination of soft prompting and LoRA adapters. (3) Significant improvementsover state-of-the-art CLIP-like models of similar size, including standardimage-text retrieval benchmarks and notable gains in compositionality.</description><author>Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Georgios Tzimiropoulos, Brais Martinez</author><pubDate>Thu, 05 Dec 2024 17:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04378v1</guid></item><item><title>SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation</title><link>http://arxiv.org/abs/2405.08807v2</link><description>Large multimodal models (LMMs) have proven flexible and generalisable acrossmany tasks and fields. Although they have strong potential to aid scientificresearch, their capabilities in this domain are not well characterised. A keyaspect of scientific research is the ability to understand and interpretfigures, which serve as a rich, compressed source of complex information. Inthis work, we present SciFIBench, a scientific figure interpretation benchmarkconsisting of 2000 questions split between two tasks across 8 categories. Thequestions are curated from arXiv paper figures and captions, using adversarialfiltering to find hard negatives and human verification for quality control. Weevaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark.Finally, we investigate the alignment and reasoning faithfulness of the LMMs onaugmented question sets from our benchmark. We release SciFIBench to encourageprogress in this domain.</description><author>Jonathan Roberts, Kai Han, Neil Houlsby, Samuel Albanie</author><pubDate>Thu, 05 Dec 2024 17:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08807v2</guid></item><item><title>A Hitchhiker's Guide to Understanding Performances of Two-Class Classifiers</title><link>http://arxiv.org/abs/2412.04377v1</link><description>Properly understanding the performances of classifiers is essential invarious scenarios. However, the literature often relies only on one or twostandard scores to compare classifiers, which fails to capture the nuances ofapplication-specific requirements, potentially leading to suboptimal classifierselection. Recently, a paper on the foundations of the theory ofperformance-based ranking introduced a tool, called the Tile, that organizes aninfinity of ranking scores into a 2D map. Thanks to the Tile, it is nowpossible to evaluate and compare classifiers efficiently, displaying allpossible application-specific preferences instead of having to rely on a pairof scores. In this paper, we provide a first hitchhiker's guide forunderstanding the performances of two-class classifiers by presenting fourscenarios, each showcasing a different user profile: a theoretical analyst, amethod designer, a benchmarker, and an application developer. Particularly, weshow that we can provide different interpretative flavors that are adapted tothe user's needs by mapping different values on the Tile. As an illustration,we leverage the newly introduced Tile tool and the different flavors to rankand analyze the performances of 74 state-of-the-art semantic segmentationmodels in two-class classification through the eyes of the four user profiles.Through these user profiles, we demonstrate that the Tile effectively capturesthe behavior of classifiers in a single visualization, while accommodating aninfinite number of ranking scores.</description><author>Anaïs Halin, Sébastien Piérard, Anthony Cioppa, Marc Van Droogenbroeck</author><pubDate>Thu, 05 Dec 2024 17:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04377v1</guid></item><item><title>CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels</title><link>http://arxiv.org/abs/2412.02819v2</link><description>Large Language Models (LLMs) have been well-researched in many long-contexttasks. However, due to high annotation costs, high-quality long-context summarydatasets for training or evaluation are scarce, limiting further research. Inthis work, we introduce CNNSum, a new multi-scale Chinese long-context novelsummarization benchmark, including four subsets, length covering16k\textasciitilde128k, 695 samples in total, the annotations are human-driven.We evaluate commercial and open-source models on CNNSum and conduct a detailedanalysis. Based on the observations, we further conduct fine-tuning explorationwith short-context summary data. In our study: (1) GPT-4o underperformed, dueto excessive subjective commentary. (2) Currently, long-context summarizationmainly relies on memory ability, small LLMs with stable longer context lengthsare the most cost-effective. Using long data concatenated from short-contextsummaries makes a significant improvement. (3) Prompt templates may cause alarge performance gap but can be mitigated through fine-tuning. (4) Fine-tunedChat or Instruction versions may harm the Base model and further fine-tuningcannot bridge performance gap. (5) while models with RoPE base scaling exhibitstrong extrapolation potential, their performance may vary significantly whencombined with other interpolation methods and need careful selection. (6)CNNSum provides more reliable and insightful evaluation results than otherbenchmarks. We release CNNSum to advance research in this field.</description><author>Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang</author><pubDate>Thu, 05 Dec 2024 17:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02819v2</guid></item><item><title>Adversarial Attacks on Large Language Models in Medicine</title><link>http://arxiv.org/abs/2406.12259v2</link><description>The integration of Large Language Models (LLMs) into healthcare applicationsoffers promising advancements in medical diagnostics, treatmentrecommendations, and patient care. However, the susceptibility of LLMs toadversarial attacks poses a significant threat, potentially leading to harmfuloutcomes in delicate medical contexts. This study investigates thevulnerability of LLMs to two types of adversarial attacks in three medicaltasks. Utilizing real-world patient data, we demonstrate that both open-sourceand proprietary LLMs are susceptible to manipulation across multiple tasks.This research further reveals that domain-specific tasks demand moreadversarial data in model fine-tuning than general domain tasks for effectiveattack execution, especially for more capable models. We discover that whileintegrating adversarial data does not markedly degrade overall modelperformance on medical benchmarks, it does lead to noticeable shifts infine-tuned model weights, suggesting a potential pathway for detecting andcountering model attacks. This research highlights the urgent need for robustsecurity measures and the development of defensive mechanisms to safeguard LLMsin medical applications, to ensure their safe and effective deployment inhealthcare settings.</description><author>Yifan Yang, Qiao Jin, Furong Huang, Zhiyong Lu</author><pubDate>Thu, 05 Dec 2024 17:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12259v2</guid></item><item><title>Don't Be So Positive: Negative Step Sizes in Second-Order Methods</title><link>http://arxiv.org/abs/2411.11224v2</link><description>The value of second-order methods lies in the use of curvature information.Yet, this information is costly to extract and once obtained, valuable negativecurvature information is often discarded so that the method is globallyconvergent. This limits the effectiveness of second-order methods in modernmachine learning. In this paper, we show that second-order andsecond-order-like methods are promising optimizers for neural networks providedthat we add one ingredient: negative step sizes. We show that under verygeneral conditions, methods that produce ascent directions are globallyconvergent when combined with a Wolfe line search that allows both positive andnegative step sizes. We experimentally demonstrate that using negative stepsizes is often more effective than common Hessian modification methods.</description><author>Betty Shea, Mark Schmidt</author><pubDate>Thu, 05 Dec 2024 17:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11224v2</guid></item><item><title>Context-Informed Machine Translation of Manga using Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.02589v2</link><description>Due to the significant time and effort required for handcraftingtranslations, most manga never leave the domestic Japanese market. Automaticmanga translation is a promising potential solution. However, it is a buddingand underdeveloped field and presents complexities even greater than thosefound in standard translation due to the need to effectively incorporate visualelements into the translation process to resolve ambiguities. In this work, weinvestigate to what extent multimodal large language models (LLMs) can provideeffective manga translation, thereby assisting manga authors and publishers inreaching wider audiences. Specifically, we propose a methodology that leveragesthe vision component of multimodal LLMs to improve translation quality andevaluate the impact of translation unit size, context length, and propose atoken efficient approach for manga translation. Moreover, we introduce a newevaluation dataset -- the first parallel Japanese-Polish manga translationdataset -- as part of a benchmark to be used in future research. Finally, wecontribute an open-source software suite, enabling others to benchmark LLMs formanga translation. Our findings demonstrate that our proposed methods achievestate-of-the-art results for Japanese-English translation and set a newstandard for Japanese-Polish.</description><author>Philip Lippmann, Konrad Skublicki, Joshua Tanner, Shonosuke Ishiwatari, Jie Yang</author><pubDate>Thu, 05 Dec 2024 17:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02589v2</guid></item><item><title>Finer Behavioral Foundation Models via Auto-Regressive Features and Advantage Weighting</title><link>http://arxiv.org/abs/2412.04368v1</link><description>The forward-backward representation (FB) is a recently proposed framework(Touati et al., 2023; Touati &amp; Ollivier, 2021) to train behavior foundationmodels (BFMs) that aim at providing zero-shot efficient policies for any newtask specified in a given reinforcement learning (RL) environment, withouttraining for each new task. Here we address two core limitations of FB modeltraining. First, FB, like all successor-feature-based methods, relies on alinear encoding of tasks: at test time, each new reward function is linearlyprojected onto a fixed set of pre-trained features. This limits expressivity aswell as precision of the task representation. We break the linearity limitationby introducing auto-regressive features for FB, which let finegrained taskfeatures depend on coarser-grained task information. This can representarbitrary nonlinear task encodings, thus significantly increasing expressivityof the FB framework. Second, it is well-known that training RL agents fromoffline datasets often requires specific techniques.We show that FB works welltogether with such offline RL techniques, by adapting techniques from (Nair etal.,2020b; Cetin et al., 2024) for FB. This is necessary to get non-flatliningperformance in some datasets, such as DMC Humanoid. As a result, we produceefficient FB BFMs for a number of new environments. Notably, in the D4RLlocomotion benchmark, the generic FB agent matches the performance of standardsingle-task offline agents (IQL, XQL). In many setups, the offline techniquesare needed to get any decent performance at all. The auto-regressive featureshave a positive but moderate impact, concentrated on tasks requiring spatialprecision and task generalization beyond the behaviors represented in thetrainset.</description><author>Edoardo Cetin, Ahmed Touati, Yann Ollivier</author><pubDate>Thu, 05 Dec 2024 17:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04368v1</guid></item><item><title>Machine Theory of Mind for Autonomous Cyber-Defence</title><link>http://arxiv.org/abs/2412.04367v1</link><description>Intelligent autonomous agents hold much potential for the domain ofcyber-security. However, due to many state-of-the-art approaches relying onuninterpretable black-box models, there is growing demand for methods thatoffer stakeholders clear and actionable insights into their latent beliefs andmotivations. To address this, we evaluate Theory of Mind (ToM) approaches forAutonomous Cyber Operations. Upon learning a robust prior, ToM models canpredict an agent's goals, behaviours, and contextual beliefs given only ahandful of past behaviour observations. In this paper, we introduce a novelGraph Neural Network (GNN)-based ToM architecture tailored for cyber-defence,Graph-In, Graph-Out (GIGO)-ToM, which can accurately predict both the targetsand attack trajectories of adversarial cyber agents over arbitrary computernetwork topologies. To evaluate the latter, we propose a novel extension of theWasserstein distance for measuring the similarity of graph-based probabilitydistributions. Whereas the standard Wasserstein distance lacks a fixedreference scale, we introduce a graph-theoretic normalization factor thatenables a standardized comparison between networks of different sizes. Wefurnish this metric, which we term the Network Transport Distance (NTD), with aweighting function that emphasizes predictions according to custom nodefeatures, allowing network operators to explore arbitrary strategicconsiderations. Benchmarked against a Graph-In, Dense-Out (GIDO)-ToMarchitecture in an abstract cyber-defence environment, our empiricalevaluations show that GIGO-ToM can accurately predict the goals and behavioursof various unseen cyber-attacking agents across a range of network topologies,as well as learn embeddings that can effectively characterize their policies.</description><author>Luke Swaby, Matthew Stewart, Daniel Harrold, Chris Willis, Gregory Palmer</author><pubDate>Thu, 05 Dec 2024 17:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04367v1</guid></item><item><title>Artificial intelligence and the internal processes of creativity</title><link>http://arxiv.org/abs/2412.04366v1</link><description>Artificial intelligence (AI) systems capable of generating creative outputsare reshaping our understanding of creativity. This shift presents anopportunity for creativity researchers to reevaluate the key components of thecreative process. In particular, the advanced capabilities of AI underscore theimportance of studying the internal processes of creativity. This paperexplores the neurobiological machinery that underlies these internal processesand describes the experiential component of creativity. It is concluded thatalthough the products of artificial and human creativity can be similar, theinternal processes are different. The paper also discusses how AI maynegatively affect the internal processes of human creativity, such as thedevelopment of skills, the integration of knowledge, and the diversity ofideas.</description><author>Jaan Aru</author><pubDate>Thu, 05 Dec 2024 17:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04366v1</guid></item><item><title>GeoPos: A Minimal Positional Encoding for Enhanced Fine-Grained Details in Image Synthesis Using Convolutional Neural Networks</title><link>http://arxiv.org/abs/2401.01951v2</link><description>The enduring inability of image generative models to recreate intricategeometric features, such as those present in human hands and fingers has beenan ongoing problem in image generation for nearly a decade. While strides havebeen made by increasing model sizes and diversifying training datasets, thisissue remains prevalent across all models, from denoising diffusion models toGenerative Adversarial Networks (GAN), pointing to a fundamental shortcoming inthe underlying architectures. In this paper, we demonstrate how this problemcan be mitigated by augmenting convolution layers geometric capabilitiesthrough providing them with a single input channel incorporating the relativen-dimensional Cartesian coordinate system. We show this drastically improvesquality of images generated by Diffusion Models, GANs, and VariationalAutoEncoders (VAE).</description><author>Mehran Hosseini, Peyman Hosseini</author><pubDate>Thu, 05 Dec 2024 17:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01951v2</guid></item><item><title>Is uniform expressivity too restrictive? Towards efficient expressivity of graph neural networks</title><link>http://arxiv.org/abs/2410.01910v2</link><description>Uniform expressivity guarantees that a Graph Neural Network (GNN) can expressa query without the parameters depending on the size of the input graphs. Thisproperty is desirable in applications in order to have number of trainableparameters that is independent of the size of the input graphs. Uniformexpressivity of the two variable guarded fragment (GC2) of first order logic isa well-celebrated result for Rectified Linear Unit (ReLU) GNNs [Barcelo &amp; al.,2020]. In this article, we prove that uniform expressivity of GC2 queries isnot possible for GNNs with a wide class of Pfaffian activation functions(including the sigmoid and tanh), answering a question formulated by [Grohe,2021]. We also show that despite these limitations, many of those GNNs canstill efficiently express GC2 queries in a way that the number of parametersremains logarithmic on the maximal degree of the input graphs. Furthermore, wedemonstrate that a log-log dependency on the degree is achievable for a certainchoice of activation function. This shows that uniform expressivity can besuccessfully relaxed by covering large graphs appearing in practicalapplications. Our experiments illustrates that our theoretical estimates holdin practice.</description><author>Sammy Khalife, Josué Tonelli-Cueto</author><pubDate>Thu, 05 Dec 2024 17:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01910v2</guid></item><item><title>Introducing the Large Medical Model: State of the art healthcare cost and risk prediction with transformers trained on patient event sequences</title><link>http://arxiv.org/abs/2409.13000v2</link><description>With U.S. healthcare spending approaching $5T (NHE Fact Sheet 2024), and 25%of it estimated to be wasteful (Waste in the US the health care system:estimated costs and potential for savings, n.d.), the need to better predictrisk and optimal patient care is evermore important. This paper introduces theLarge Medical Model (LMM), a generative pre-trained transformer (GPT) designedto guide and predict the broad facets of patient care and healthcareadministration. The model is trained on medical event sequences from over 140Mlongitudinal patient claims records with a specialized vocabulary built frommedical terminology systems and demonstrates a superior capability to forecasthealthcare costs and identify potential risk factors. Through experimentationand validation, we showcase the LMM's proficiency in not only in cost and riskpredictions, but also in discerning intricate patterns within complex medicalconditions and an ability to identify novel relationships in patient care. TheLMM is able to improve both cost prediction by 14.1% over the best commercialmodels and chronic conditions prediction by 1.9% over the best transformermodels in research predicting a broad set of conditions. The LMM is asubstantial advancement in healthcare analytics, offering the potential tosignificantly enhance risk assessment, cost management, and personalizedmedicine.</description><author>Ricky Sahu, Eric Marriott, Ethan Siegel, David Wagner, Flore Uzan, Troy Yang, Asim Javed</author><pubDate>Thu, 05 Dec 2024 17:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13000v2</guid></item><item><title>Approximate Top-$k$ for Increased Parallelism</title><link>http://arxiv.org/abs/2412.04358v1</link><description>We present an evaluation of bucketed approximate top-$k$ algorithms.Computing top-$k$ exactly suffers from limited parallelism, because the $k$largest values must be aggregated along the vector, thus is not well suited tocomputation on highly-parallel machine learning accelerators. By relaxing therequirement that the top-$k$ is exact, bucketed algorithms can dramaticallyincrease the parallelism available by independently computing many smallertop-$k$ operations. We explore the design choices of this class of algorithmsusing both theoretical analysis and empirical evaluation on downstream tasks.Our motivating examples are sparsity algorithms for language models, whichoften use top-$k$ to select the most important parameters or activations. Wealso release a fast bucketed top-$k$ implementation for PyTorch.</description><author>Oscar Key, Luka Ribar, Alberto Cattaneo, Luke Hudlass-Galley, Douglas Orr</author><pubDate>Thu, 05 Dec 2024 17:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04358v1</guid></item><item><title>Multi-Scale Node Embeddings for Graph Modeling and Generation</title><link>http://arxiv.org/abs/2412.04354v1</link><description>Lying at the interface between Network Science and Machine Learning, nodeembedding algorithms take a graph as input and encode its structure onto outputvectors that represent nodes in an abstract geometric space, enabling variousvector-based downstream tasks such as network modelling, data compression, linkprediction, and community detection. Two apparently unrelated limitationsaffect these algorithms. On one hand, it is not clear what the basic operationdefining vector spaces, i.e. the vector sum, corresponds to in terms of theoriginal nodes in the network. On the other hand, while the same input networkcan be represented at multiple levels of resolution by coarse-graining theconstituent nodes into arbitrary block-nodes, the relationship between nodeembeddings obtained at different hierarchical levels is not understood. Here,building on recent results in network renormalization theory, we address thesetwo limitations at once and define a multiscale node embedding method that,upon arbitrary coarse-grainings, ensures statistical consistency of theembedding vector of a block-node with the sum of the embedding vectors of itsconstituent nodes. We illustrate the power of this approach on two economicnetworks that can be naturally represented at multiple resolution levels:namely, the international trade between (sets of) countries and theinput-output flows among (sets of) industries in the Netherlands. We confirmthe statistical consistency between networks retrieved from coarse-grained nodevectors and networks retrieved from sums of fine-grained node vectors, a resultthat cannot be achieved by alternative methods. Several key network properties,including a large number of triangles, are successfully replicated already fromembeddings of very low dimensionality, allowing for the generation of faithfulreplicas of the original networks at arbitrary resolution levels.</description><author>Riccardo Milocco, Fabian Jansen, Diego Garlaschelli</author><pubDate>Thu, 05 Dec 2024 17:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04354v1</guid></item><item><title>ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation</title><link>http://arxiv.org/abs/2412.04353v1</link><description>Temporal action segmentation and long-term action anticipation are twopopular vision tasks for the temporal analysis of actions in videos. Despiteapparent relevance and potential complementarity, these two problems have beeninvestigated as separate and distinct tasks. In this work, we tackle these twoproblems, action segmentation and action anticipation, jointly using a unifieddiffusion model dubbed ActFusion. The key idea to unification is to train themodel to effectively handle both visible and invisible parts of the sequence inan integrated manner; the visible part is for temporal segmentation, and theinvisible part is for future anticipation. To this end, we introduce a newanticipative masking strategy during training in which a late part of the videoframes is masked as invisible, and learnable tokens replace these frames tolearn to predict the invisible future. Experimental results demonstrate thebi-directional benefits between action segmentation and anticipation. ActFusionachieves the state-of-the-art performance across the standard benchmarks of 50Salads, Breakfast, and GTEA, outperforming task-specific models in both of thetwo tasks with a single unified model through joint learning.</description><author>Dayoung Gong, Suha Kwak, Minsu Cho</author><pubDate>Thu, 05 Dec 2024 17:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04353v1</guid></item><item><title>BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages</title><link>http://arxiv.org/abs/2412.04351v1</link><description>This paper focuses on developing translation models and related applicationsfor 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj,Bodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada,Kangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili,Malayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi,Sanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu,Telugu, and Urdu. Achieving this requires parallel and other types of corporafor all 36 * 36 language pairs, addressing challenges like script variations,phonetic differences, and syntactic diversity. For instance, languages likeKashmiri and Sindhi, which use multiple scripts, demand script normalizationfor alignment, while low-resource languages such as Khasi and Santali requiresynthetic data augmentation to ensure sufficient coverage and quality. To address these challenges, this work proposes strategies for corpuscreation by leveraging existing resources, developing parallel datasets,generating domain-specific corpora, and utilizing synthetic data techniques.Additionally, it evaluates machine translation across various dimensions,including standard and discourse-level translation, domain-specifictranslation, reference-based and reference-free evaluation, error analysis, andautomatic post-editing. By integrating these elements, the study establishes acomprehensive framework to improve machine translation quality and enablebetter cross-lingual communication in India's linguistically diverse ecosystem.</description><author>Vandan Mujadia, Dipti Misra Sharma</author><pubDate>Thu, 05 Dec 2024 17:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04351v1</guid></item><item><title>DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.00112v2</link><description>Accurately and efficiently modeling dynamic scenes and motions is consideredso challenging a task due to temporal dynamics and motion complexity. Toaddress these challenges, we propose DynMF, a compact and efficientrepresentation that decomposes a dynamic scene into a few neural trajectories.We argue that the per-point motions of a dynamic scene can be decomposed into asmall set of explicit or learned trajectories. Our carefully designed neuralframework consisting of a tiny set of learned basis queried only in time allowsfor rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, whileat the same time, requiring only double the storage compared to static scenes.Our neural representation adequately constrains the inherently underconstrainedmotion field of a dynamic scene leading to effective and fast optimization.This is done by biding each point to motion coefficients that enforce theper-point sharing of basis trajectories. By carefully applying a sparsity lossto the motion coefficients, we are able to disentangle the motions thatcomprise the scene, independently control them, and generate novel motioncombinations that have never been seen before. We can reach state-of-the-artrender quality within just 5 minutes of training and in less than half an hour,we can synthesize novel views of dynamic scenes with superior photorealisticquality. Our representation is interpretable, efficient, and expressive enoughto offer real-time view synthesis of complex dynamic scene motions, inmonocular and multi-view scenarios.</description><author>Agelos Kratimenos, Jiahui Lei, Kostas Daniilidis</author><pubDate>Thu, 05 Dec 2024 17:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00112v2</guid></item><item><title>LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization</title><link>http://arxiv.org/abs/2404.01282v3</link><description>Temporal Action Localization (TAL) involves localizing and classifying actionsnippets in an untrimmed video. The emergence of large video foundation modelshas led RGB-only video backbones to outperform previous methods needing bothRGB and optical flow modalities. Leveraging these large models is often limitedto training only the TAL head due to the prohibitively large GPU memoryrequired to adapt the video backbone for TAL. To overcome this limitation, weintroduce LoSA, the first memory-and-parameter-efficient backbone adapterdesigned specifically for TAL to handle untrimmed videos. LoSA specializes forTAL by introducing Long-Short-range Adapters that adapt the intermediate layersof the video backbone over different temporal ranges. These adapters runparallel to the video backbone to significantly reduce memory footprint. LoSAalso includes Long-Short-range Gated Fusion that strategically combines theoutput of these adapters from the video backbone layers to enhance the videofeatures provided to the TAL head. Experiments show that LoSA significantlyoutperforms all existing methods on standard TAL benchmarks, THUMOS-14 andActivityNet-v1.3, by scaling end-to-end backbone adaptation tobillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging thembeyond head-only transfer learning.</description><author>Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen</author><pubDate>Thu, 05 Dec 2024 17:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01282v3</guid></item><item><title>Distributionally Robust Performative Prediction</title><link>http://arxiv.org/abs/2412.04346v1</link><description>Performative prediction aims to model scenarios where predictive outcomessubsequently influence the very systems they target. The pursuit of aperformative optimum (PO) -- minimizing performative risk -- is generallyreliant on modeling of the distribution map, which characterizes how a deployedML model alters the data distribution. Unfortunately, inevitablemisspecification of the distribution map can lead to a poor approximation ofthe true PO. To address this issue, we introduce a novel framework ofdistributionally robust performative prediction and study a new solutionconcept termed as distributionally robust performative optimum (DRPO). We showprovable guarantees for DRPO as a robust approximation to the true PO when thenominal distribution map is different from the actual one. Moreover,distributionally robust performative prediction can be reformulated as anaugmented performative prediction problem, enabling efficient optimization. Theexperimental results demonstrate that DRPO offers potential advantages overtraditional PO approach when the distribution map is misspecified at eithermicro- or macro-level.</description><author>Songkai Xue, Yuekai Sun</author><pubDate>Thu, 05 Dec 2024 17:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04346v1</guid></item><item><title>Unsupervised Denoising for Signal-Dependent and Row-Correlated Imaging Noise</title><link>http://arxiv.org/abs/2310.07887v3</link><description>Accurate analysis of microscopy images is hindered by the presence of noise.This noise is usually signal-dependent and often additionally correlated alongrows or columns of pixels. Current self- and unsupervised denoisers can addresssignal-dependent noise, but none can reliably remove noise that is also row- orcolumn-correlated. Here, we present the first fully unsupervised deeplearning-based denoiser capable of handling imaging noise that isrow-correlated as well as signal-dependent. Our approach uses a VariationalAutoencoder (VAE) with a specially designed autoregressive decoder. Thisdecoder is capable of modeling row-correlated and signal-dependent noise but isincapable of independently modeling underlying clean signal. The VAE thereforeproduces latent variables containing only clean signal information, and theseare mapped back into image space using a proposed second decoder network. Ourmethod does not require a pre-trained noise model and can be trained fromscratch using unpaired noisy data. We benchmark our approach on microscopydatatsets from a range of imaging modalities and sensor types, each with row-or column-correlated, signal-dependent noise, and show that it outperformsexisting self- and unsupervised denoisers.</description><author>Benjamin Salmon, Alexander Krull</author><pubDate>Thu, 05 Dec 2024 17:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07887v3</guid></item><item><title>Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery</title><link>http://arxiv.org/abs/2403.07369v2</link><description>In this paper, we study the problem of Generalized Category Discovery (GCD),which aims to cluster unlabeled data from both known and unknown categoriesusing the knowledge of labeled data from known categories. Current GCD methodsrely on only visual cues, which however neglect the multi-modality perceptivenature of human cognitive processes in discovering novel visual categories. Toaddress this, we propose a two-phase TextGCD framework to accomplishmulti-modality GCD by exploiting powerful Visual-Language Models. TextGCDmainly includes a retrieval-based text generation (RTG) phase and across-modality co-teaching (CCT) phase. First, RTG constructs a visual lexiconusing category tags from diverse datasets and attributes from Large LanguageModels, generating descriptive texts for images in a retrieval manner. Second,CCT leverages disparities between textual and visual modalities to fostermutual learning, thereby enhancing visual GCD. In addition, we design anadaptive class aligning strategy to ensure the alignment of categoryperceptions between modalities as well as a soft-voting mechanism to integratemulti-modality cues. Experiments on eight datasets show the large superiorityof our approach over state-of-the-art methods. Notably, our approachoutperforms the best competitor, by 7.7% and 10.8% in All accuracy onImageNet-1k and CUB, respectively.</description><author>Haiyang Zheng, Nan Pu, Wenjing Li, Nicu Sebe, Zhun Zhong</author><pubDate>Thu, 05 Dec 2024 17:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07369v2</guid></item><item><title>Limit Theorems for Stochastic Gradient Descent with Infinite Variance</title><link>http://arxiv.org/abs/2410.16340v3</link><description>Stochastic gradient descent is a classic algorithm that has gained greatpopularity especially in the last decades as the most common approach fortraining models in machine learning. While the algorithm has been well-studiedwhen stochastic gradients are assumed to have a finite variance, there issignificantly less research addressing its theoretical properties in the caseof infinite variance gradients. In this paper, we establish the asymptoticbehavior of stochastic gradient descent in the context of infinite variancestochastic gradients, assuming that the stochastic gradient is regular varyingwith index $\alpha\in(1,2)$. The closest result in this context was establishedin 1969 , in the one-dimensional case and assuming that stochastic gradientsbelong to a more restrictive class of distributions. We extend it to themultidimensional case, covering a broader class of infinite variancedistributions. As we show, the asymptotic distribution of the stochasticgradient descent algorithm can be characterized as the stationary distributionof a suitably defined Ornstein-Uhlenbeck process driven by an appropriatestable L\'evy process. Additionally, we explore the applications of theseresults in linear regression and logistic regression models.</description><author>Jose Blanchet, Aleksandar Mijatović, Wenhao Yang</author><pubDate>Thu, 05 Dec 2024 17:03:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16340v3</guid></item><item><title>RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse</title><link>http://arxiv.org/abs/2412.04343v1</link><description>While motion generation has made substantial progress, its practicalapplication remains constrained by dataset diversity and scale, limiting itsability to handle out-of-distribution scenarios. To address this, we propose asimple and effective baseline, RMD, which enhances the generalization of motiongeneration through retrieval-augmented techniques. Unlike previousretrieval-based methods, RMD requires no additional training and offers threekey advantages: (1) the external retrieval database can be flexibly replaced;(2) body parts from the motion database can be reused, with an LLM facilitatingsplitting and recombination; and (3) a pre-trained motion diffusion modelserves as a prior to improve the quality of motions obtained through retrievaland direct combination. Without any training, RMD achieves state-of-the-artperformance, with notable advantages on out-of-distribution data.</description><author>Zhouyingcheng Liao, Mingyuan Zhang, Wenjia Wang, Lei Yang, Taku Komura</author><pubDate>Thu, 05 Dec 2024 17:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04343v1</guid></item><item><title>Retrieval-Augmented Machine Translation with Unstructured Knowledge</title><link>http://arxiv.org/abs/2412.04342v1</link><description>Retrieval-augmented generation (RAG) introduces additional information toenhance large language models (LLMs). In machine translation (MT), previouswork typically retrieves in-context examples from paired MT corpora, ordomain-specific knowledge from knowledge graphs, to enhance models' MT ability.However, a large amount of world knowledge is organized in unstructureddocuments, and might not be fully paired across different languages. In thispaper, we study retrieval-augmented MT using unstructured documents.Specifically, we build RAGtrans, the first benchmark to train and evaluateLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samplescollected via GPT-4o and human translators. Besides, documents from differentlanguages are also provided to supply the knowledge to these samples. Based onRAGtrans, we further propose a multi-task training method to teach LLMs how touse information from multilingual documents during their translation. Themethod uses existing multilingual corpora to create auxiliary trainingobjectives without additional labeling requirements. Extensive experiments showthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.</description><author>Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou</author><pubDate>Thu, 05 Dec 2024 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04342v1</guid></item><item><title>Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET Image Reconstruction</title><link>http://arxiv.org/abs/2412.04339v1</link><description>Medical image reconstruction with pre-trained score-based generative models(SGMs) has advantages over other existing state-of-the-art deep-learnedreconstruction methods, including improved resilience to different scannersetups and advanced image distribution modeling. SGM-based reconstruction hasrecently been applied to simulated positron emission tomography (PET) datasets,showing improved contrast recovery for out-of-distribution lesions relative tothe state-of-the-art. However, existing methods for SGM-based reconstructionfrom PET data suffer from slow reconstruction, burdensome hyperparameter tuningand slice inconsistency effects (in 3D). In this work, we propose a practicalmethodology for fully 3D reconstruction that accelerates reconstruction andreduces the number of critical hyperparameters by matching the likelihood of anSGM's reverse diffusion process to a current iterate of the maximum-likelihoodexpectation maximization algorithm. Using the example of low-countreconstruction from simulated $[^{18}$F]DPA-714 datasets, we show ourmethodology can match or improve on the NRMSE and SSIM of existingstate-of-the-art SGM-based PET reconstruction while reducing reconstructiontime and the need for hyperparameter tuning. We evaluate our methodologyagainst state-of-the-art supervised and conventional reconstruction algorithms.Finally, we demonstrate a first-ever implementation of SGM-based reconstructionfor real 3D PET data, specifically $[^{18}$F]DPA-714 data, where we integrateperpendicular pre-trained SGMs to eliminate slice inconsistency issues.</description><author>George Webber, Yuya Mizuno, Oliver D. Howes, Alexander Hammers, Andrew P. King, Andrew J. Reader</author><pubDate>Thu, 05 Dec 2024 16:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04339v1</guid></item><item><title>Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in Bird's-Eye-View via Uncertainty Measure</title><link>http://arxiv.org/abs/2412.04337v1</link><description>Applying pseudo labeling techniques has been found to be advantageous insemi-supervised 3D object detection (SSOD) in Bird's-Eye-View (BEV) forautonomous driving, particularly where labeled data is limited. In theliterature, Exponential Moving Average (EMA) has been used for adjustments ofthe weights of teacher network by the student network. However, the sameinduces catastrophic forgetting in the teacher network. In this work, weaddress this issue by introducing a novel concept of Reflective Teacher wherethe student is trained by both labeled and pseudo labeled data while itsknowledge is progressively passed to the teacher through a regularizer toensure retention of previous knowledge. Additionally, we propose Geometry AwareBEV Fusion (GA-BEVFusion) for efficient alignment of multi-modal BEV features,thus reducing the disparity between the modalities - camera and LiDAR. Thishelps to map the precise geometric information embedded among LiDAR pointsreliably with the spatial priors for extraction of semantic information fromcamera images. Our experiments on the nuScenes and Waymo datasets demonstrate:1) improved performance over state-of-the-art methods in both fully supervisedand semi-supervised settings; 2) Reflective Teacher achieves equivalentperformance with only 25% and 22% of labeled data for nuScenes and Waymodatasets respectively, in contrast to other fully supervised methods thatutilize the full labeled dataset.</description><author>Saheli Hazra, Sudip Das, Rohit Choudhary, Arindam Das, Ganesh Sistu, Ciaran Eising, Ujjwal Bhattacharya</author><pubDate>Thu, 05 Dec 2024 16:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04337v1</guid></item><item><title>Liquid: Language Models are Scalable Multi-modal Generators</title><link>http://arxiv.org/abs/2412.04332v1</link><description>We present Liquid, an auto-regressive generation paradigm that seamlesslyintegrates visual comprehension and generation by tokenizing images intodiscrete codes and learning these code embeddings alongside text tokens withina shared feature space for both vision and language. Unlike previous multimodallarge language model (MLLM), Liquid achieves this integration using a singlelarge language model (LLM), eliminating the need for external pretrained visualembeddings such as CLIP. For the first time, Liquid uncovers a scaling law thatperformance drop unavoidably brought by the unified training of visual andlanguage tasks diminishes as the model size increases. Furthermore, the unifiedtoken space enables visual generation and comprehension tasks to mutuallyenhance each other, effectively removing the typical interference seen inearlier models. We show that existing LLMs can serve as strong foundations forLiquid, saving 100x in training costs while outperforming Chameleon inmultimodal capabilities and maintaining language performance comparable tomainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 andSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language andtext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2are powerful multimodal generators, offering a scalable solution for enhancingboth vision-language understanding and generation. The code and models will bereleased.</description><author>Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai</author><pubDate>Thu, 05 Dec 2024 16:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04332v1</guid></item><item><title>Action Mapping for Reinforcement Learning in Continuous Environments with Constraints</title><link>http://arxiv.org/abs/2412.04327v1</link><description>Deep reinforcement learning (DRL) has had success across various domains, butapplying it to environments with constraints remains challenging due to poorsample efficiency and slow convergence. Recent literature exploredincorporating model knowledge to mitigate these problems, particularly throughthe use of models that assess the feasibility of proposed actions. However,integrating feasibility models efficiently into DRL pipelines in environmentswith continuous action spaces is non-trivial. We propose a novel DRL trainingstrategy utilizing action mapping that leverages feasibility models tostreamline the learning process. By decoupling the learning of feasible actionsfrom policy optimization, action mapping allows DRL agents to focus onselecting the optimal action from a reduced feasible action set. We demonstratethrough experiments that action mapping significantly improves trainingperformance in constrained environments with continuous action spaces,especially with imperfect feasibility models.</description><author>Mirco Theile, Lukas Dirnberger, Raphael Trumpp, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</author><pubDate>Thu, 05 Dec 2024 16:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04327v1</guid></item><item><title>Multi-Subject Image Synthesis as a Generative Prior for Single-Subject PET Image Reconstruction</title><link>http://arxiv.org/abs/2412.04324v1</link><description>Large high-quality medical image datasets are difficult to acquire butnecessary for many deep learning applications. For positron emission tomography(PET), reconstructed image quality is limited by inherent Poisson noise. Wepropose a novel method for synthesising diverse and realistic pseudo-PET imageswith improved signal-to-noise ratio. We also show how our pseudo-PET images maybe exploited as a generative prior for single-subject PET image reconstruction.Firstly, we perform deep-learned deformable registration of multi-subjectmagnetic resonance (MR) images paired to multi-subject PET images. We then usethe anatomically-learned deformation fields to transform multiple PET images tothe same reference space, before averaging random subsets of the transformedmulti-subject data to form a large number of varying pseudo-PET images. Weobserve that using MR information for registration imbues the resultingpseudo-PET images with improved anatomical detail compared to the originals. Weconsider applications to PET image reconstruction, by generating pseudo-PETimages in the same space as the intended single-subject reconstruction andusing them as training data for a diffusion model-based reconstruction method.We show visual improvement and reduced background noise in our 2Dreconstructions as compared to OSEM, MAP-EM and an existing state-of-the-artdiffusion model-based approach. Our method shows the potential for utilisinghighly subject-specific prior information within a generative reconstructionframework. Future work may compare the benefits of our approach to explicitlyMR-guided reconstruction methodologies.</description><author>George Webber, Yuya Mizuno, Oliver D. Howes, Alexander Hammers, Andrew P. King, Andrew J. Reader</author><pubDate>Thu, 05 Dec 2024 16:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04324v1</guid></item><item><title>Learning in Wilson-Cowan model for metapopulation</title><link>http://arxiv.org/abs/2406.16453v2</link><description>The Wilson-Cowan model for metapopulation, a Neural Mass Network Model,treats different subcortical regions of the brain as connected nodes, withconnections representing various types of structural, functional, or effectiveneuronal connectivity between these regions. Each region comprises interactingpopulations of excitatory and inhibitory cells, consistent with the standardWilson-Cowan model. By incorporating stable attractors into such ametapopulation model's dynamics, we transform it into a learning algorithmcapable of achieving high image and text classification accuracy. We test it onMNIST and Fashion MNIST, in combination with convolutional neural networks, onCIFAR-10 and TF-FLOWERS, and, in combination with a transformer architecture(BERT), on IMDB, always showing high classification accuracy. These numericalevaluations illustrate that minimal modifications to the Wilson-Cowan model formetapopulation can reveal unique and previously unobserved dynamics.</description><author>Raffaele Marino, Lorenzo Buffoni, Lorenzo Chicchi, Francesca Di Patti, Diego Febbe, Lorenzo Giambagli, Duccio Fanelli</author><pubDate>Thu, 05 Dec 2024 16:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16453v2</guid></item><item><title>GRAM: Generalization in Deep RL with a Robust Adaptation Module</title><link>http://arxiv.org/abs/2412.04323v1</link><description>The reliable deployment of deep reinforcement learning in real-world settingsrequires the ability to generalize across a variety of conditions, includingboth in-distribution scenarios seen during training as well as novelout-of-distribution scenarios. In this work, we present a framework fordynamics generalization in deep reinforcement learning that unifies these twodistinct types of generalization within a single architecture. We introduce arobust adaptation module that provides a mechanism for identifying and reactingto both in-distribution and out-of-distribution environment dynamics, alongwith a joint training pipeline that combines the goals of in-distributionadaptation and out-of-distribution robustness. Our algorithm GRAM achievesstrong generalization performance across in-distribution andout-of-distribution scenarios upon deployment, which we demonstrate on avariety of realistic simulated locomotion tasks with a quadruped robot.</description><author>James Queeney, Xiaoyi Cai, Mouhacine Benosman, Jonathan P. How</author><pubDate>Thu, 05 Dec 2024 16:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04323v1</guid></item><item><title>A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces</title><link>http://arxiv.org/abs/2310.02951v2</link><description>We study the global convergence of a Fisher-Rao policy gradient flow forinfinite-horizon entropy-regularised Markov decision processes with Polishstate and action space. The flow is a continuous-time analogue of a policymirror descent method. We establish the global well-posedness of the gradientflow and demonstrate its exponential convergence to the optimal policy.Moreover, we prove the flow is stable with respect to gradient evaluation,offering insights into the performance of a natural policy gradient flow withlog-linear policy parameterisation. To overcome challenges stemming from thelack of the convexity of the objective function and the discontinuity arisingfrom the entropy regulariser, we leverage the performance difference lemma andthe duality relationship between the gradient and mirror descent flows. Ouranalysis provides a theoretical foundation for developing various discretepolicy gradient algorithms.</description><author>Bekzhan Kerimkulov, James-Michael Leahy, David Siska, Lukasz Szpruch, Yufei Zhang</author><pubDate>Thu, 05 Dec 2024 16:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02951v2</guid></item><item><title>Generative-Model-Based Fully 3D PET Image Reconstruction by Conditional Diffusion Sampling</title><link>http://arxiv.org/abs/2412.04319v1</link><description>Score-based generative models (SGMs) have recently shown promising resultsfor image reconstruction on simulated positron emission tomography (PET)datasets. In this work we have developed and implemented practical methodologyfor 3D image reconstruction with SGMs, and perform (to our knowledge) the firstSGM-based reconstruction of real fully 3D PET data. We train an SGM onfull-count reference brain images, and extend methodology to allow SGM-basedreconstructions at very low counts (1% of original, to simulate low-dose orshort-duration scanning). We then perform reconstructions for multipleindependent realisations of 1% count data, allowing us to analyse the bias andvariance characteristics of the method. We sample from the learned posteriordistribution of the generative algorithm to calculate uncertainty images forour reconstructions. We evaluate the method's performance on real full- andlow-count PET data and compare with conventional OSEM and MAP-EM baselines,showing that our SGM-based low-count reconstructions match full-dosereconstructions more closely and in a bias-variance trade-off comparison, ourSGM-reconstructed images have lower variance than existing baselines. Futurework will compare to supervised deep-learned methods, with other avenues forinvestigation including how data conditioning affects the SGM's posteriordistribution and the algorithm's performance with different tracers.</description><author>George Webber, Yuya Mizuno, Oliver D. Howes, Alexander Hammers, Andrew P. King, Andrew J. Reader</author><pubDate>Thu, 05 Dec 2024 16:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04319v1</guid></item><item><title>Enhancing Novel Object Detection via Cooperative Foundational Models</title><link>http://arxiv.org/abs/2311.12068v3</link><description>In this work, we address the challenging and emergent problem of novel objectdetection (NOD), focusing on the accurate detection of both known and novelobject categories during inference. Traditional object detection algorithms areinherently closed-set, limiting their capability to handle NOD. We present anovel approach to transform existing closed-set detectors into open-setdetectors. This transformation is achieved by leveraging the complementarystrengths of pre-trained foundational models, specifically CLIP and SAM,through our cooperative mechanism. Furthermore, by integrating this mechanismwith state-of-the-art open-set detectors such as GDINO, we establish newbenchmarks in object detection performance. Our method achieves 17.42 mAP innovel object detection and 42.08 mAP for known objects on the challenging LVISdataset. Adapting our approach to the COCO OVD split, we surpass the currentstate-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Ourcode is available at https://rohit901.github.io/coop-foundation-models/ .</description><author>Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Thu, 05 Dec 2024 16:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12068v3</guid></item><item><title>The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation</title><link>http://arxiv.org/abs/2412.04318v1</link><description>This paper introduces the counter-intuitive generalization results ofoverfitting pre-trained large language models (LLMs) on very small datasets. Inthe setting of open-ended text generation, it is well-documented that LLMs tendto generate repetitive and dull sequences, a phenomenon that is especiallyapparent when generating using greedy decoding. This issue persists even withstate-of-the-art LLMs containing billions of parameters, trained via next-tokenprediction on large datasets. We find that by further fine-tuning these modelsto achieve a near-zero training loss on a small set of samples -- a process werefer to as hyperfitting -- the long-sequence generative capabilities aregreatly enhanced. Greedy decoding with these Hyperfitted models even outperformTop-P sampling over long-sequences, both in terms of diversity and humanpreferences. This phenomenon extends to LLMs of various sizes, differentdomains, and even autoregressive image generation. We further find thisphenomena to be distinctly different from that of Grokking and double descent.Surprisingly, our experiments indicate that hyperfitted models rarely fall intorepeating sequences they were trained on, and even explicitly blocking thesesequences results in high-quality output. All hyperfitted models produceextremely low-entropy predictions, often allocating nearly all probability to asingle token.</description><author>Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre</author><pubDate>Thu, 05 Dec 2024 16:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04318v1</guid></item><item><title>FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression</title><link>http://arxiv.org/abs/2412.04317v1</link><description>Despite a big leap forward in capability, multimodal large language models(MLLMs) tend to behave like a sloth in practical use, i.e., slow response andlarge latency. Recent efforts are devoted to building tiny MLLMs for betterefficiency, but the plethora of visual tokens still used limit their actualspeedup. In this paper, we propose a powerful and fast tiny MLLM calledFlashSloth. Different from previous efforts, FlashSloth focuses on improvingthe descriptive power of visual tokens in the process of compressing theirredundant semantics. In particular, FlashSloth introduces embedded visualcompression designs to capture both visually salient and instruction-relatedimage information, so as to achieving superior multimodal performance withfewer visual tokens. Extensive experiments are conducted to validate theproposed FlashSloth, and a bunch of tiny but strong MLLMs are alsocomprehensively compared, e.g., InternVL2, MiniCPM-V2 and Qwen2-VL. Theexperimental results show that compared with these advanced tiny MLLMs, ourFlashSloth can greatly reduce the number of visual tokens, training memory andcomputation complexity while retaining high performance on various VL tasks.</description><author>Bo Tong, Bokai Lai, Yiyi Zhou, Gen Luo, Yunhang Shen, Ke Li, Xiaoshuai Sun, Rongrong Ji</author><pubDate>Thu, 05 Dec 2024 16:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04317v1</guid></item><item><title>Densing Law of LLMs</title><link>http://arxiv.org/abs/2412.04315v1</link><description>Large Language Models (LLMs) have emerged as a milestone in artificialintelligence, and their performance can improve as the model size increases.However, this scaling brings great challenges to training and inferenceefficiency, particularly for deploying LLMs in resource-constrainedenvironments, and the scaling trend is becoming increasingly unsustainable.This paper introduces the concept of ``\textit{capacity density}'' as a newmetric to evaluate the quality of the LLMs across different scales anddescribes the trend of LLMs in terms of both effectiveness and efficiency. Tocalculate the capacity density of a given target LLM, we first introduce a setof reference models and develop a scaling law to predict the downstreamperformance of these reference models based on their parameter sizes. We thendefine the \textit{effective parameter size} of the target LLM as the parametersize required by a reference model to achieve equivalent performance, andformalize the capacity density as the ratio of the effective parameter size tothe actual parameter size of the target LLM. Capacity density provides aunified framework for assessing both model effectiveness and efficiency. Ourfurther analysis of recent open-source base LLMs reveals an empirical law (thedensing law)that the capacity density of LLMs grows exponentially over time.More specifically, using some widely used benchmarks for evaluation, thecapacity density of LLMs doubles approximately every three months. The lawprovides new perspectives to guide future LLM development, emphasizing theimportance of improving capacity density to achieve optimal results withminimal computational overhead.</description><author>Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Xu Han, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 05 Dec 2024 16:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04315v1</guid></item><item><title>LocalSR: Image Super-Resolution in Local Region</title><link>http://arxiv.org/abs/2412.04314v1</link><description>Standard single-image super-resolution (SR) upsamples and restores entireimages. Yet several real-world applications require higher resolutions only inspecific regions, such as license plates or faces, making the super-resolutionof the entire image, along with the associated memory and computational cost,unnecessary. We propose a novel task, called LocalSR, to restore only localregions of the low-resolution image. For this problem setting, we propose acontext-based local super-resolution (CLSR) to super-resolve only specifiedregions of interest (ROI) while leveraging the entire image as context. Ourmethod uses three parallel processing modules: a base module forsuper-resolving the ROI, a global context module for gathering helpful featuresfrom across the image, and a proximity integration module for concentrating onareas surrounding the ROI, progressively propagating features from distantpixels to the target region. Experimental results indicate that our approach,with its reduced low complexity, outperforms variants that focus exclusively onthe ROI.</description><author>Bo Ji, Angela Yao</author><pubDate>Thu, 05 Dec 2024 16:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04314v1</guid></item><item><title>The Tile: A 2D Map of Ranking Scores for Two-Class Classification</title><link>http://arxiv.org/abs/2412.04309v1</link><description>In the computer vision and machine learning communities, as well as in manyother research domains, rigorous evaluation of any new method, includingclassifiers, is essential. One key component of the evaluation process is theability to compare and rank methods. However, ranking classifiers andaccurately comparing their performances, especially when takingapplication-specific preferences into account, remains challenging. Forinstance, commonly used evaluation tools like Receiver Operating Characteristic(ROC) and Precision/Recall (PR) spaces display performances based on twoscores. Hence, they are inherently limited in their ability to compareclassifiers across a broader range of scores and lack the capability toestablish a clear ranking among classifiers. In this paper, we present a novelversatile tool, named the Tile, that organizes an infinity of ranking scores ina single 2D map for two-class classifiers, including common evaluation scoressuch as the accuracy, the true positive rate, the positive predictive value,Jaccard's coefficient, and all F-beta scores. Furthermore, we study theproperties of the underlying ranking scores, such as the influence of thepriors or the correspondences with the ROC space, and depict how tocharacterize any other score by comparing them to the Tile. Overall, wedemonstrate that the Tile is a powerful tool that effectively captures all therankings in a single visualization and allows interpreting them.</description><author>Sébastien Piérard, Anaïs Halin, Anthony Cioppa, Adrien Deliège, Marc Van Droogenbroeck</author><pubDate>Thu, 05 Dec 2024 16:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04309v1</guid></item><item><title>ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics</title><link>http://arxiv.org/abs/2411.18825v2</link><description>Reinforcement learning (RL) has demonstrated compelling performance inrobotic tasks, but its success often hinges on the design of complex, ad hocreward functions. Researchers have explored how Large Language Models (LLMs)could enable non-expert users to specify reward functions more easily. However,LLMs struggle to balance the importance of different features, generalizepoorly to out-of-distribution robotic tasks, and cannot represent the problemproperly with only text-based descriptions. To address these challenges, wepropose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), anovel framework that combines natural language guidance with visual userdemonstrations to align robot behavior with user intentions better. Byincorporating visual inputs, ELEMENTAL overcomes the limitations of text-onlytask specifications, while leveraging inverse reinforcement learning (IRL) tobalance feature weights and match the demonstrated behaviors optimally.ELEMENTAL also introduces an iterative feedback-loop through self-reflection toimprove feature, reward, and policy learning. Our experiment resultsdemonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, andachieves 41.3% better generalization in out-of-distribution tasks, highlightingits robustness in LfD.</description><author>Letian Chen, Matthew Gombolay</author><pubDate>Thu, 05 Dec 2024 16:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18825v2</guid></item><item><title>ALMA: Alignment with Minimal Annotation</title><link>http://arxiv.org/abs/2412.04305v1</link><description>Recent approaches to large language model (LLM) alignment typically requiremillions of human annotations or rely on external aligned models for syntheticdata generation. This paper introduces ALMA: Alignment with Minimal Annotation,demonstrating that effective alignment can be achieved using only 9,000 labeledexamples -- less than 1% of conventional approaches. ALMA generates largeamounts of high-quality synthetic alignment data through new techniques:diverse prompt synthesis via few-shot learning, diverse response generationwith multiple model checkpoints, and judge (reward model) enhancement throughscore aggregation and self-distillation. Using only a pretrained Llama3 basemodel, 5,000 SFT examples, and 4,000 judge annotations, ALMA achievesperformance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,0.1% difference on AlpacaEval 2.0 score). These results are achieved with amulti-round, self-bootstrapped data synthesis and training recipe thatcontinues to improve for 10 rounds, surpassing the typical 3-round ceiling ofprevious methods. These results suggest that base models already possesssufficient knowledge for effective alignment, and that synthetic datageneration methods can expose it.</description><author>Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, Marjan Ghazvininejad</author><pubDate>Thu, 05 Dec 2024 16:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04305v1</guid></item><item><title>Towards Zero-shot 3D Anomaly Localization</title><link>http://arxiv.org/abs/2412.04304v1</link><description>3D anomaly detection and localization is of great significance for industrialinspection. Prior 3D anomaly detection and localization methods focus on thesetting that the testing data share the same category as the training datawhich is normal. However, in real-world applications, the normal training datafor the target 3D objects can be unavailable due to issues like data privacy orexport control regulation. To tackle these challenges, we identify a new task-- zero-shot 3D anomaly detection and localization, where the training andtesting classes do not overlap. To this end, we design 3DzAL, a novelpatch-level contrastive learning framework based on pseudo anomalies generatedusing the inductive bias from task-irrelevant 3D xyz data to learn morerepresentative feature representations. Furthermore, we train a normalcyclassifier network to classify the normal patches and pseudo anomalies andutilize the classification result jointly with feature distance to designanomaly scores. Instead of directly using the patch point clouds, we introduceadversarial perturbations to the input patch xyz data before feeding into the3D normalcy classifier for the classification-based anomaly score. We show that3DzAL outperforms the state-of-the-art anomaly detection and localizationperformance.</description><author>Yizhou Wang, Kuan-Chuan Peng, Yun Fu</author><pubDate>Thu, 05 Dec 2024 16:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04304v1</guid></item><item><title>HydraViT: Stacking Heads for a Scalable ViT</title><link>http://arxiv.org/abs/2409.17978v2</link><description>The architecture of Vision Transformers (ViTs), particularly the Multi-headAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTson devices with varying constraints, such as mobile phones, requires multiplemodels of different sizes. However, this approach has limitations, such astraining and storing each required model separately. This paper introducesHydraViT, a novel approach that addresses these limitations by stackingattention heads to achieve a scalable ViT. By repeatedly changing the size ofthe embedded dimensions throughout each layer and their corresponding number ofattention heads in MHA during training, HydraViT induces multiple subnetworks.Thereby, HydraViT achieves adaptability across a wide spectrum of hardwareenvironments while maintaining performance. Our experimental resultsdemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10subnetworks, covering a wide range of resource constraints. HydraViT achievesup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracywith the same throughput on ImageNet-1K compared to the baselines, making it aneffective solution for scenarios where hardware availability is diverse orvaries over time. Source code available at https://github.com/ds-kiel/HydraViT.</description><author>Janek Haberer, Ali Hojjat, Olaf Landsiedel</author><pubDate>Thu, 05 Dec 2024 16:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17978v2</guid></item></channel></rss>