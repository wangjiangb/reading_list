<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 18 Jul 2024 01:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LAB-Bench: Measuring Capabilities of Language Models for Biology Research</title><link>http://arxiv.org/abs/2407.10362v3</link><description>There is widespread optimism that frontier Large Language Models (LLMs) andLLM-augmented systems have the potential to rapidly accelerate scientificdiscovery across disciplines. Today, many benchmarks exist to measure LLMknowledge and reasoning on textbook-style science questions, but few if anybenchmarks are designed to evaluate language model performance on practicaltasks required for scientific research, such as literature search, protocolplanning, and data analysis. As a step toward building such benchmarks, weintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset ofover 2,400 multiple choice questions for evaluating AI systems on a range ofpractical biology research capabilities, including recall and reasoning overliterature, interpretation of figures, access and navigation of databases, andcomprehension and manipulation of DNA and protein sequences. Importantly, incontrast to previous scientific benchmarks, we expect that an AI system thatcan achieve consistently high scores on the more difficult LAB-Bench taskswould serve as a useful assistant for researchers in areas such as literaturesearch and molecular cloning. As an initial assessment of the emergentscientific task capabilities of frontier language models, we measureperformance of several against our benchmark and report results compared tohuman expert biology researchers. We will continue to update and expandLAB-Bench over time, and expect it to serve as a useful tool in the developmentof automated research systems going forward. A public subset of LAB-Bench isavailable for use at the following URL:https://huggingface.co/datasets/futurehouse/lab-bench</description><author>Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, Samuel G. Rodriques</author><pubDate>Wed, 17 Jul 2024 17:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10362v3</guid></item><item><title>Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding</title><link>http://arxiv.org/abs/2402.16844v3</link><description>Large language models (LLMs) have become ubiquitous in practice and arewidely used for generation tasks such as translation, summarization andinstruction following. However, their enormous size and reliance onautoregressive decoding increase deployment costs and complicate their use inlatency-critical applications. In this work, we propose a hybrid approach thatcombines language models of different sizes to increase the efficiency ofautoregressive decoding while maintaining high performance. Our method utilizesa pretrained frozen LLM that encodes all prompt tokens once in parallel, anduses the resulting representations to condition and guide a small languagemodel (SLM), which then generates the response more efficiently. We investigatethe combination of encoder-decoder LLMs with both encoder-decoder anddecoder-only SLMs from different model families and only require fine-tuning ofthe SLM. Experiments with various benchmarks show substantial speedups of up to$4\times$, with minor performance penalties of $1-2\%$ for translation andsummarization tasks compared to the LLM.</description><author>Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi</author><pubDate>Wed, 17 Jul 2024 13:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16844v3</guid></item><item><title>Neural Compression of Atmospheric States</title><link>http://arxiv.org/abs/2407.11666v2</link><description>Atmospheric states derived from reanalysis comprise a substantial portion ofweather and climate simulation outputs. Many stakeholders -- such asresearchers, policy makers, and insurers -- use this data to better understandthe earth system and guide policy decisions. Atmospheric states have alsoreceived increased interest as machine learning approaches to weatherprediction have shown promising results. A key issue for all audiences is thatdense time series of these high-dimensional states comprise an enormous amountof data, precluding all but the most well resourced groups from accessing andusing historical data and future projections. To address this problem, wepropose a method for compressing atmospheric states using methods from theneural network literature, adapting spherical data to processing byconventional neural architectures through the use of the area-preservingHEALPix projection. We investigate two model classes for building neuralcompressors: the hyperprior model from the neural image compression literatureand recent vector-quantised models. We show that both families of modelssatisfy the desiderata of small average error, a small number of high-errorreconstructed pixels, faithful reproduction of extreme events such ashurricanes and heatwaves, preservation of the spectral power distributionacross spatial scales. We demonstrate compression ratios in excess of 1000x,with compression and decompression at a rate of approximately one second perglobal atmospheric state.</description><author>Piotr Mirowski, David Warde-Farley, Mihaela Rosca, Matthew Koichi Grimes, Yana Hasson, Hyunjik Kim, MÃ©lanie Rey, Simon Osindero, Suman Ravuri, Shakir Mohamed</author><pubDate>Wed, 17 Jul 2024 13:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11666v2</guid></item><item><title>Statistics-aware Audio-visual Deepfake Detector</title><link>http://arxiv.org/abs/2407.11650v2</link><description>In this paper, we propose an enhanced audio-visual deep detection method.Recent methods in audio-visual deepfake detection mostly assess thesynchronization between audio and visual features. Although they have shownpromising results, they are based on the maximization/minimization of isolatedfeature distances without considering feature statistics. Moreover, they relyon cumbersome deep learning architectures and are heavily dependent onempirically fixed hyperparameters. Herein, to overcome these limitations, wepropose: (1) a statistical feature loss to enhance the discriminationcapability of the model, instead of relying solely on feature distances; (2)using the waveform for describing the audio as a replacement of frequency-basedrepresentations; (3) a post-processing normalization of the fakeness score; (4)the use of shallower network for reducing the computational complexity.Experiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance ofthe proposed method.</description><author>Marcella Astrid, Enjie Ghorbel, Djamila Aouada</author><pubDate>Wed, 17 Jul 2024 11:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11650v2</guid></item><item><title>The Oscars of AI Theater: A Survey on Role-Playing with Language Models</title><link>http://arxiv.org/abs/2407.11484v2</link><description>This survey explores the burgeoning field of role-playing with languagemodels, focusing on their development from early persona-based models toadvanced character-driven simulations facilitated by Large Language Models(LLMs). Initially confined to simple persona consistency due to limited modelcapabilities, role-playing tasks have now expanded to embrace complex characterportrayals involving character consistency, behavioral alignment, and overallattractiveness. We provide a comprehensive taxonomy of the critical componentsin designing these systems, including data, models and alignment, agentarchitecture and evaluation. This survey not only outlines the currentmethodologies and challenges, such as managing dynamic personal profiles andachieving high-level persona consistency but also suggests avenues for futureresearch in improving the depth and realism of role-playing applications. Thegoal is to guide future research by offering a structured overview of currentmethodologies and identifying potential areas for improvement. Relatedresources and papers are available athttps://github.com/nuochenpku/Awesome-Role-Play-Papers.</description><author>Nuo Chen, Yang Deng, Jia Li</author><pubDate>Wed, 17 Jul 2024 09:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11484v2</guid></item><item><title>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors</title><link>http://arxiv.org/abs/2407.11828v2</link><description>Vibravox is a dataset compliant with the General Data Protection Regulation(GDPR) containing audio recordings using five different body-conduction audiosensors : two in-ear microphones, two bone conduction vibration pickups and alaryngophone. The data set also includes audio data from an airborne microphoneused as a reference. The Vibravox corpus contains 38 hours of speech samplesand physiological sounds recorded by 188 participants under different acousticconditions imposed by an high order ambisonics 3D spatializer. Annotationsabout the recording conditions and linguistic transcriptions are also includedin the corpus. We conducted a series of experiments on various speech-relatedtasks, including speech recognition, speech enhancement and speakerverification. These experiments were carried out using state-of-the-art modelsto evaluate and compare their performances on signals captured by the differentaudio sensors offered by the Vibravox dataset, with the aim of gaining a bettergrasp of their individual characteristics.</description><author>Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah PoirÃ©e, VÃ©ronique Zimpfer, Ãric Bavu</author><pubDate>Wed, 17 Jul 2024 08:09:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11828v2</guid></item><item><title>Hierarchical Separable Video Transformer for Snapshot Compressive Imaging</title><link>http://arxiv.org/abs/2407.11946v2</link><description>Transformers have achieved the state-of-the-art performance on solving theinverse problem of Snapshot Compressive Imaging (SCI) for video, whoseill-posedness is rooted in the mixed degradation of spatial masking andtemporal aliasing. However, previous Transformers lack an insight into thedegradation and thus have limited performance and efficiency. In this work, wetailor an efficient reconstruction architecture without temporal aggregation inearly layers and Hierarchical Separable Video Transformer (HiSViT) as buildingblock. HiSViT is built by multiple groups of Cross-Scale Separable Multi-headSelf-Attention (CSS-MSA) and Gated Self-Modulated Feed-Forward Network(GSM-FFN) with dense connections, each of which is conducted within a separatechannel portions at a different scale, for multi-scale interactions andlong-range modeling. By separating spatial operations from temporal ones,CSS-MSA introduces an inductive bias of paying more attention within framesinstead of between frames while saving computational overheads. GSM-FFN furtherenhances the locality via gated mechanism and factorized spatial-temporalconvolutions. Extensive experiments demonstrate that our method outperformsprevious methods by $\!&gt;\!0.5$ dB with comparable or fewer parameters andcomplexity. The source codes and pretrained models are released athttps://github.com/pwangcs/HiSViT.</description><author>Ping Wang, Yulun Zhang, Lishun Wang, Xin Yuan</author><pubDate>Wed, 17 Jul 2024 08:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11946v2</guid></item><item><title>Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis</title><link>http://arxiv.org/abs/2406.10273v3</link><description>Context. Risk analysis assesses potential risks in specific scenarios. Riskanalysis principles are context-less; the same methodology can be applied to arisk connected to health and information technology security. Risk analysisrequires a vast knowledge of national and international regulations andstandards and is time and effort-intensive. A large language model can quicklysummarize information in less time than a human and can be fine-tuned tospecific tasks. Aim. Our empirical study aims to investigate the effectivenessof Retrieval-Augmented Generation and fine-tuned LLM in Risk analysis. To ourknowledge, no prior study has explored its capabilities in risk analysis.Method. We manually curated \totalscenarios unique scenarios leading to\totalsamples representative samples from over 50 mission-critical analysesarchived by the industrial context team in the last five years. We compared thebase GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation andfine-tuned counterparts. We employ two human experts as competitors of themodels and three other three human experts to review the models and the formerhuman expert's analysis. The reviewers analyzed 5,000 scenario analyses.Results and Conclusions. HEs demonstrated higher accuracy, but LLMs are quickerand more actionable. Moreover, our findings show that RAG-assisted LLMs havethe lowest hallucination rates, effectively uncovering hidden risks andcomplementing human expertise. Thus, the choice of model depends on specificneeds, with FTMs for accuracy, RAG for hidden risks discovery, and base modelsfor comprehensiveness and actionability. Therefore, experts can leverage LLMsfor an effective complementing companion in risk analysis within a condensedtimeframe. They can also save costs by averting unnecessary expenses associatedwith implementing unwarranted countermeasures.</description><author>Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi</author><pubDate>Wed, 17 Jul 2024 07:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10273v3</guid></item><item><title>Rate-Distortion-Cognition Controllable Versatile Neural Image Compression</title><link>http://arxiv.org/abs/2407.11700v2</link><description>Recently, the field of Image Coding for Machines (ICM) has garneredheightened interest and significant advances thanks to the rapid progress oflearning-based techniques for image compression and analysis. Previous studiesoften require training separate codecs to support various bitrate levels,machine tasks, and networks, thus lacking both flexibility and practicality. Toaddress these challenges, we propose a rate-distortion-cognition controllableversatile image compression, which method allows the users to adjust thebitrate (i.e., Rate), image reconstruction quality (i.e., Distortion), andmachine task accuracy (i.e., Cognition) with a single neural model, achievingultra-controllability. Specifically, we first introduce a cognition-orientedloss in the primary compression branch to train a codec for diverse machinetasks. This branch attains variable bitrate by regulating quantization degreethrough the latent code channels. To further enhance the quality of thereconstructed images, we employ an auxiliary branch to supplement residualinformation with a scalable bitstream. Ultimately, two branches use a `$\beta x+ (1 - \beta) y$' interpolation strategy to achieve a balancedcognition-distortion trade-off. Extensive experiments demonstrate that ourmethod yields satisfactory ICM performance and flexibleRate-Distortion-Cognition controlling.</description><author>Jinming Liu, Ruoyu Feng, Yunpeng Qi, Qiuyu Chen, Zhibo Chen, Wenjun Zeng, Xin Jin</author><pubDate>Wed, 17 Jul 2024 06:26:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11700v2</guid></item><item><title>QVD: Post-training Quantization for Video Diffusion Models</title><link>http://arxiv.org/abs/2407.11585v2</link><description>Recently, video diffusion models (VDMs) have garnered significant attentiondue to their notable advancements in generating coherent and realistic videocontent. However, processing multiple frame features concurrently, coupled withthe considerable model size, results in high latency and extensive memoryconsumption, hindering their broader application. Post-training quantization(PTQ) is an effective technique to reduce memory footprint and improvecomputational efficiency. Unlike image diffusion, we observe that the temporalfeatures, which are integrated into all frame features, exhibit pronouncedskewness. Furthermore, we investigate significant inter-channel disparities andasymmetries in the activation of video diffusion models, resulting in lowcoverage of quantization levels by individual channels and increasing thechallenge of quantization. To address these issues, we introduce the first PTQstrategy tailored for video diffusion models, dubbed QVD. Specifically, wepropose the High Temporal Discriminability Quantization (HTDQ) method, designedfor temporal features, which retains the high discriminability of quantizedfeatures, providing precise temporal guidance for all video frames. Inaddition, we present the Scattered Channel Range Integration (SCRI) methodwhich aims to improve the coverage of quantization levels across individualchannels. Experimental validations across various models, datasets, andbit-width settings demonstrate the effectiveness of our QVD in terms of diversemetrics. In particular, we achieve near-lossless performance degradation onW8A8, outperforming the current methods by 205.12 in FVD.</description><author>Shilong Tian, Hong Chen, Chengtao Lv, Yu Liu, Jinyang Guo, Xianglong Liu, Shengxi Li, Hao Yang, Tao Xie</author><pubDate>Wed, 17 Jul 2024 05:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11585v2</guid></item><item><title>Enhanced Safety in Autonomous Driving: Integrating Latent State Diffusion Model for End-to-End Navigation</title><link>http://arxiv.org/abs/2407.06317v4</link><description>With the advancement of autonomous driving, ensuring safety during motionplanning and navigation is becoming more and more important. However, mostend-to-end planning methods suffer from a lack of safety. This researchaddresses the safety issue in the control optimization problem of autonomousdriving, formulated as Constrained Markov Decision Processes (CMDPs). Wepropose a novel, model-based approach for policy optimization, utilizing aconditional Value-at-Risk based Soft Actor Critic to manage constraints incomplex, high-dimensional state spaces effectively. Our method introduces aworst-case actor to guide safe exploration, ensuring rigorous adherence tosafety requirements even in unpredictable scenarios. The policy optimizationemploys the Augmented Lagrangian method and leverages latent diffusion modelsto predict and simulate future trajectories. This dual approach not only aidsin navigating environments safely but also refines the policy's performance byintegrating distribution modeling to account for environmental uncertainties.Empirical evaluations conducted in both simulated and real environmentdemonstrate that our approach outperforms existing methods in terms of safety,efficiency, and decision-making capabilities.</description><author>Detian Chu, Linyuan Bai, Jianuo Huang, Zhenlong Fang, Peng Zhang, Wei Kang, Haifeng Lin</author><pubDate>Wed, 17 Jul 2024 04:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06317v4</guid></item><item><title>UP-Diff: Latent Diffusion Model for Remote Sensing Urban Prediction</title><link>http://arxiv.org/abs/2407.11578v2</link><description>This study introduces a novel Remote Sensing (RS) Urban Prediction (UP) taskfocused on future urban planning, which aims to forecast urban layouts byutilizing information from existing urban layouts and planned change maps. Toaddress the proposed RS UP task, we propose UP-Diff, which leverages a LatentDiffusion Model (LDM) to capture positionaware embeddings of pre-change urbanlayouts and planned change maps. In specific, the trainable cross-attentionlayers within UP-Diff's iterative diffusion modules enable the model todynamically highlight crucial regions for targeted modifications. By utilizingour UP-Diff, designers can effectively refine and adjust future urban cityplans by making modifications to the change maps in a dynamic and adaptivemanner. Compared with conventional RS Change Detection (CD) methods, theproposed UP-Diff for the RS UP task avoids the requirement of paired prechangeand post-change images, which enhances the practical usage in city development.Experimental results on LEVIRCD and SYSU-CD datasets show UP-Diff's ability toaccurately predict future urban layouts with high fidelity, demonstrating itspotential for urban planning. Code and model weights are available athttps://github.com/zeyuwang-zju/UP-Diff.</description><author>Zeyu Wang, Zecheng Hao, Jingyu Lin, Yuchao Feng, Yufei Guo</author><pubDate>Wed, 17 Jul 2024 03:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11578v2</guid></item><item><title>Monocular Occupancy Prediction for Scalable Indoor Scenes</title><link>http://arxiv.org/abs/2407.11730v2</link><description>Camera-based 3D occupancy prediction has recently garnered increasingattention in outdoor driving scenes. However, research in indoor scenes remainsrelatively unexplored. The core differences in indoor scenes lie in thecomplexity of scene scale and the variance in object size. In this paper, wepropose a novel method, named ISO, for predicting indoor scene occupancy usingmonocular images. ISO harnesses the advantages of a pretrained depth model toachieve accurate depth predictions. Furthermore, we introduce the Dual FeatureLine of Sight Projection (D-FLoSP) module within ISO, which enhances thelearning of 3D voxel features. To foster further research in this domain, weintroduce Occ-ScanNet, a large-scale occupancy benchmark for indoor scenes.With a dataset size 40 times larger than the NYUv2 dataset, it facilitatesfuture scalable research in indoor scene analysis. Experimental results on bothNYUv2 and Occ-ScanNet demonstrate that our method achieves state-of-the-artperformance. The dataset and code are made publicly athttps://github.com/hongxiaoy/ISO.git.</description><author>Hongxiao Yu, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang</author><pubDate>Wed, 17 Jul 2024 02:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11730v2</guid></item><item><title>CCoE: A Compact LLM with Collaboration of Experts</title><link>http://arxiv.org/abs/2407.11686v2</link><description>In the domain of Large Language Model (LLM), LLMs demonstrate significantcapabilities in natural language understanding and generation. With the growingneeds of applying LLMs on various domains, it is a research question that howto efficiently train and build a model that has expertise in different domainsbut with a low training cost. We propose CCoE architecture, a framework ofeasily coupling multiple strong domain experts together to fuse into a big LLM,provides a collective way of utilizing the different domain expert LLMs.Besides, training a large collaborative of multiple expert LLMs requires a highrequirements on training sources. CCoE bypasses this problem through isolatingother experts and train each expert separately. The design of CCoE assemblesmultiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoElayer could have one or more expert LLMs. Expert LLMs have different number oflayers and have been well-trained for different domain tasks. Each expert isfine-tuned to be able to achieve the comparable results with SOTA domain LLMs.We start from 5 experts in the domain of Code, Math, Law, text-to-SQL andMedical. The results indicate that our CCoE framework can easily andefficiently boost nearly 10%-20% performance on original base model indifferent domains but using less resources on training, as well as inference.</description><author>Shaomang Huang, Jianfeng Pan, Hanzhong Zheng</author><pubDate>Wed, 17 Jul 2024 02:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11686v2</guid></item><item><title>GV-Bench: Benchmarking Local Feature Matching for Geometric Verification of Long-term Loop Closure Detection</title><link>http://arxiv.org/abs/2407.11736v2</link><description>Visual loop closure detection is an important module in visual simultaneouslocalization and mapping (SLAM), which associates current camera observationwith previously visited places. Loop closures correct drifts in trajectoryestimation to build a globally consistent map. However, a false loop closurecan be fatal, so verification is required as an additional step to ensurerobustness by rejecting the false positive loops. Geometric verification hasbeen a well-acknowledged solution that leverages spatial clues provided bylocal feature matching to find true positives. Existing feature matchingmethods focus on homography and pose estimation in long-term visuallocalization, lacking references for geometric verification. To fill the gap,this paper proposes a unified benchmark targeting geometric verification ofloop closure detection under long-term conditional variations. Furthermore, weevaluate six representative local feature matching methods (handcrafted andlearning-based) under the benchmark, with in-depth analysis for limitations andfuture directions.</description><author>Jingwen Yu, Hanjing Ye, Jianhao Jiao, Ping Tan, Hong Zhang</author><pubDate>Wed, 17 Jul 2024 01:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11736v2</guid></item><item><title>Does Refusal Training in LLMs Generalize to the Past Tense?</title><link>http://arxiv.org/abs/2407.11969v1</link><description>Refusal training is widely used to prevent LLMs from generating harmful,undesirable, or illegal outputs. We reveal a curious generalization gap in thecurrent refusal training approaches: simply reformulating a harmful request inthe past tense (e.g., "How to make a Molotov cocktail?" to "How did people makea Molotov cocktail?") is often sufficient to jailbreak many state-of-the-artLLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as areformulation model. For example, the success rate of this simple attack onGPT-4o increases from 1% using direct requests to 88% using 20 past tensereformulation attempts on harmful requests from JailbreakBench with GPT-4 as ajailbreak judge. Interestingly, we also find that reformulations in the futuretense are less effective, suggesting that refusal guardrails tend to considerpast historical questions more benign than hypothetical future questions.Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defendingagainst past reformulations is feasible when past tense examples are explicitlyincluded in the fine-tuning data. Overall, our findings highlight that thewidely used alignment techniques -- such as SFT, RLHF, and adversarial training-- employed to align the studied models can be brittle and do not alwaysgeneralize as intended. We provide code and jailbreak artifacts athttps://github.com/tml-epfl/llm-past-tense.</description><author>Maksym Andriushchenko, Nicolas Flammarion</author><pubDate>Tue, 16 Jul 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11969v1</guid></item><item><title>Sparse Training for Federated Learning with Regularized Error Correction</title><link>http://arxiv.org/abs/2312.13795v2</link><description>Federated Learning (FL) has attracted much interest due to the significantadvantages it brings to training deep neural network (DNN) models. However,since communications and computation resources are limited, training DNN modelsin FL systems face challenges such as elevated computational and communicationcosts in complex tasks. Sparse training schemes gain increasing attention inorder to scale down the dimensionality of each client (i.e., node)transmission. Specifically, sparsification with error correction methods is apromising technique, where only important updates are sent to the parameterserver (PS) and the rest are accumulated locally. While error correctionmethods have shown to achieve a significant sparsification level of theclient-to-PS message without harming convergence, pushing sparsity furtherremains unresolved due to the staleness effect. In this paper, we propose anovel algorithm, dubbed Federated Learning with Accumulated RegularizedEmbeddings (FLARE), to overcome this challenge. FLARE presents a novel sparsetraining approach via accumulated pulling of the updated models withregularization on the embeddings in the FL process, providing a powerfulsolution to the staleness effect, and pushing sparsity to an exceptional level.The performance of FLARE is validated through extensive experiments on diverseand complex models, achieving a remarkable sparsity level (10 times and morebeyond the current state-of-the-art) along with significantly improvedaccuracy. Additionally, an open-source software package has been developed forthe benefit of researchers and developers in related fields.</description><author>Ran Greidi, Kobi Cohen</author><pubDate>Tue, 16 Jul 2024 17:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13795v2</guid></item><item><title>Efficient Training with Denoised Neural Weights</title><link>http://arxiv.org/abs/2407.11966v1</link><description>Good weight initialization serves as an effective measure to reduce thetraining cost of a deep neural network (DNN) model. The choice of how toinitialize parameters is challenging and may require manual tuning, which canbe time-consuming and prone to human error. To overcome such limitations, thiswork takes a novel step towards building a weight generator to synthesize theneural weights for initialization. We use the image-to-image translation taskwith generative adversarial networks (GANs) as an example due to the ease ofcollecting model weights spanning a wide range. Specifically, we first collecta dataset with various image editing concepts and their corresponding trainedweights, which are later used for the training of the weight generator. Toaddress the different characteristics among layers and the substantial numberof weights to be predicted, we divide the weights into equal-sized blocks andassign each block an index. Subsequently, a diffusion model is trained withsuch a dataset using both text conditions of the concept and the block indexes.By initializing the image translation model with the denoised weights predictedby our diffusion model, the training requires only 43.3 seconds. Compared totraining from scratch (i.e., Pix2pix), we achieve a 15x training timeacceleration for a new concept while obtaining even better image generationquality.</description><author>Yifan Gong, Zheng Zhan, Yanyu Li, Yerlan Idelbayev, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, Jian Ren</author><pubDate>Tue, 16 Jul 2024 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11966v1</guid></item><item><title>UrbanWorld: An Urban World Model for 3D City Generation</title><link>http://arxiv.org/abs/2407.11965v1</link><description>Cities, as the most fundamental environment of human life, encompass diversephysical elements such as buildings, roads and vegetation with complexinterconnection. Crafting realistic, interactive 3D urban environments plays acrucial role in constructing AI agents capable of perceiving, decision-making,and acting like humans in real-world environments. However, creatinghigh-fidelity 3D urban environments usually entails extensive manual labor fromdesigners, involving intricate detailing and accurate representation of complexurban features. Therefore, how to accomplish this in an automatical way remainsa longstanding challenge. Toward this problem, we propose UrbanWorld, the firstgenerative urban world model that can automatically create a customized,realistic and interactive 3D urban world with flexible control conditions.UrbanWorld incorporates four key stages in the automatical crafting pipeline:3D layout generation from openly accessible OSM data, urban scene planning anddesigning with a powerful urban multimodal large language model (Urban MLLM),controllable urban asset rendering with advanced 3D diffusion techniques, andfinally the MLLM-assisted scene refinement. The crafted high-fidelity 3D urbanenvironments enable realistic feedback and interactions for general AI andmachine perceptual systems in simulations. We are working on contributingUrbanWorld as an open-source and versatile platform for evaluating andimproving AI abilities in perception, decision-making, and interaction inrealistic urban environments.</description><author>Yu Shang, Jiansheng Chen, Hangyu Fan, Jingtao Ding, Jie Feng, Yong Li</author><pubDate>Tue, 16 Jul 2024 17:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11965v1</guid></item><item><title>NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?</title><link>http://arxiv.org/abs/2407.11963v1</link><description>In evaluating the long-context capabilities of large language models (LLMs),identifying content relevant to a user's query from original long documents isa crucial prerequisite for any LLM to answer questions based on long text. Wepresent NeedleBench, a framework consisting of a series of progressively morechallenging tasks for assessing bilingual long-context capabilities, spanningmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) anddifferent depth ranges, allowing the strategic insertion of critical datapoints in different text depth zones to rigorously test the retrieval andreasoning capabilities of models in diverse contexts. We use the NeedleBenchframework to assess how well the leading open-source models can identify keyinformation relevant to the question and apply that information to reasoning inbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge(ATC) to mimic the complexity of logical reasoning challenges that are likelyto be present in real-world long-context tasks, providing a simple method forevaluating LLMs in dealing with complex long-context situations. Our resultssuggest that current LLMs have significant room for improvement in practicallong-context applications, as they struggle with the complexity of logicalreasoning challenges that are likely to be present in real-world long-contexttasks. All codes and resources are available at OpenCompass:https://github.com/open-compass/opencompass.</description><author>Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen</author><pubDate>Tue, 16 Jul 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11963v1</guid></item><item><title>Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling</title><link>http://arxiv.org/abs/2407.11962v1</link><description>This paper introduces Motion-oriented Compositional Neural Radiance Fields(MoCo-NeRF), a framework designed to perform free-viewpoint rendering ofmonocular human videos via novel non-rigid motion modeling approach. In thecontext of dynamic clothed humans, complex cloth dynamics generate non-rigidmotions that are intrinsically distinct from skeletal articulations andcritically important for the rendering quality. The conventional approachmodels non-rigid motions as spatial (3D) deviations in addition to skeletaltransformations. However, it is either time-consuming or challenging to achieveoptimal quality due to its high learning complexity without a directsupervision. To target this problem, we propose a novel approach of modelingnon-rigid motions as radiance residual fields to benefit from more direct colorsupervision in the rendering and utilize the rigid radiance fields as a priorto reduce the complexity of the learning process. Our approach utilizes asingle multiresolution hash encoding (MHE) to concurrently learn the canonicalT-pose representation from rigid skeletal motions and the radiance residualfield for non-rigid motions. Additionally, to further improve both trainingefficiency and usability, we extend MoCo-NeRF to support simultaneous trainingof multiple subjects within a single framework, thanks to our effective designfor modeling non-rigid motions. This scalability is achieved through theintegration of a global MHE and learnable identity codes in addition tomultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,clearly demonstrating state-of-the-art performance in both single- andmulti-subject settings. The code and model will be made publicly available atthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.</description><author>Jaehyeok Kim, Dongyoon Wee, Dan Xu</author><pubDate>Tue, 16 Jul 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11962v1</guid></item><item><title>A Recipe for Unbounded Data Augmentation in Visual Reinforcement Learning</title><link>http://arxiv.org/abs/2405.17416v2</link><description>Q-learning algorithms are appealing for real-world applications due to theirdata-efficiency, but they are very prone to overfitting and traininginstabilities when trained from visual observations. Prior work, namely SVEA,finds that selective application of data augmentation can improve the visualgeneralization of RL agents without destabilizing training. We revisit itsrecipe for data augmentation, and find an assumption that limits itseffectiveness to augmentations of a photometric nature. Addressing theselimitations, we propose a generalized recipe, SADA, that works with widervarieties of augmentations. We benchmark its effectiveness on DMC-GB2 - ourproposed extension of the popular DMControl Generalization Benchmark - as wellas tasks from Meta-World and the Distracting Control Suite, and find that ourmethod, SADA, greatly improves training stability and generalization of RLagents across a diverse set of augmentations. For visualizations, code andbenchmark: see https://aalmuzairee.github.io/SADA/</description><author>Abdulaziz Almuzairee, Nicklas Hansen, Henrik I. Christensen</author><pubDate>Tue, 16 Jul 2024 17:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17416v2</guid></item><item><title>Unsupervised Concept Discovery Mitigates Spurious Correlations</title><link>http://arxiv.org/abs/2402.13368v2</link><description>Models prone to spurious correlations in training data often produce brittlepredictions and introduce unintended biases. Addressing this challengetypically involves methods relying on prior knowledge and group annotation toremove spurious correlations, which may not be readily available in manyapplications. In this paper, we establish a novel connection betweenunsupervised object-centric learning and mitigation of spurious correlations.Instead of directly inferring subgroups with varying correlations with labels,our approach focuses on discovering concepts: discrete ideas that are sharedacross input samples. Leveraging existing object-centric representationlearning, we introduce CoBalT: a concept balancing technique that effectivelymitigates spurious correlations without requiring human labeling of subgroups.Evaluation across the benchmark datasets for sub-population shifts demonstratesuperior or competitive performance compared state-of-the-art baselines,without the need for group annotation. Code is available athttps://github.com/rarefin/CoBalT.</description><author>Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, Kenji Kawaguchi</author><pubDate>Tue, 16 Jul 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13368v2</guid></item><item><title>LMExplainer: Grounding Knowledge and Explaining Language Models</title><link>http://arxiv.org/abs/2303.16537v3</link><description>Language models (LMs) like GPT-4 are important in AI applications, but theiropaque decision-making process reduces user trust, especially insafety-critical areas. We introduce LMExplainer, a novel knowledge-groundedexplainer that clarifies the reasoning process of LMs through intuitive,human-understandable explanations. By leveraging a graph attention network(GAT) with a large-scale knowledge graph (KG), LMExplainer not only preciselynarrows the reasoning space to focus on the most relevant knowledge but alsogrounds its reasoning in structured, verifiable knowledge to reducehallucinations and enhance interpretability. LMExplainer effectively generateshuman-understandable explanations to enhance transparency and streamline thedecision-making process. Additionally, by incorporating debugging into theexplanation, it offers expertise suggestions that improve LMs from adevelopmental perspective. Thus, LMExplainer stands as an enhancement in makingLMs more accessible and understandable to users. We evaluate LMExplainer onbenchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that itoutperforms most existing methods. By comparing the explanations generated byLMExplainer with those of other models, we show that our approach offers morecomprehensive and clearer explanations of the reasoning process. LMExplainerprovides a deeper understanding of the inner workings of LMs, advancing towardsmore reliable, transparent, and equitable AI.</description><author>Zichen Chen, Jianda Chen, Yuanyuan Chen, Han Yu, Ambuj K Singh, Misha Sra</author><pubDate>Tue, 16 Jul 2024 17:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16537v3</guid></item><item><title>Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation</title><link>http://arxiv.org/abs/2407.11954v1</link><description>Long-term action anticipation has become an important task for manyapplications such as autonomous driving and human-robot interaction. Unlikeshort-term anticipation, predicting more actions into the future imposes a realchallenge with the increasing uncertainty in longer horizons. While there hasbeen a significant progress in predicting more actions into the future, most ofthe proposed methods address the task in a deterministic setup and ignore theunderlying uncertainty. In this paper, we propose a novel Gated TemporalDiffusion (GTD) network that models the uncertainty of both the observation andthe future predictions. As generator, we introduce a Gated Anticipation Network(GTAN) to model both observed and unobserved frames of a video in a mutualrepresentation. On the one hand, using a mutual representation for past andfuture allows us to jointly model ambiguities in the observation and future,while on the other hand GTAN can by design treat the observed and unobservedparts differently and steer the information flow between them. Our modelachieves state-of-the-art results on the Breakfast, Assembly101 and 50Saladsdatasets in both stochastic and deterministic settings. Code:https://github.com/olga-zats/GTDA .</description><author>Olga Zatsarynna, Emad Bahrami, Yazan Abu Farha, Gianpiero Francesca, Juergen Gall</author><pubDate>Tue, 16 Jul 2024 17:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11954v1</guid></item><item><title>Temporally Consistent Stereo Matching</title><link>http://arxiv.org/abs/2407.11950v1</link><description>Stereo matching provides depth estimation from binocular images fordownstream applications. These applications mostly take video streams as inputand require temporally consistent depth maps. However, existing methods mainlyfocus on the estimation at the single-frame level. This commonly leads totemporally inconsistent results, especially in ill-posed regions. In thispaper, we aim to leverage temporal information to improve the temporalconsistency, accuracy, and efficiency of stereo matching. To achieve this, weformulate video stereo matching as a process of temporal disparity completionfollowed by continuous iterative refinements. Specifically, we first projectthe disparity of the previous timestamp to the current viewpoint, obtaining asemi-dense disparity map. Then, we complete this map through a disparitycompletion module to obtain a well-initialized disparity map. The statefeatures from the current completion module and from the past refinement arefused together, providing a temporally coherent state for subsequentrefinement. Based on this coherent state, we introduce a dual-space refinementmodule to iteratively refine the initialized result in both disparity anddisparity gradient spaces, improving estimations in ill-posed regions.Extensive experiments demonstrate that our method effectively alleviatestemporal inconsistency while enhancing both accuracy and efficiency.</description><author>Jiaxi Zeng, Chengtang Yao, Yuwei Wu, Yunde Jia</author><pubDate>Tue, 16 Jul 2024 17:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11950v1</guid></item><item><title>Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation</title><link>http://arxiv.org/abs/2407.11948v1</link><description>The utilization of Transformer-based models prospers the growth ofmulti-document summarization (MDS). Given the huge impact and widespreadadoption of Transformer-based models in various natural language processingtasks, investigating their performance and behaviors in the context of MDSbecomes crucial for advancing the field and enhancing the quality of summary.To thoroughly examine the behaviours of Transformer-based MDS models, thispaper presents five empirical studies on (1) measuring the impact of documentboundary separators quantitatively; (2) exploring the effectiveness ofdifferent mainstream Transformer structures; (3) examining the sensitivity ofthe encoder and decoder; (4) discussing different training strategies; and (5)discovering the repetition in a summary generation. The experimental results onprevalent MDS datasets and eleven evaluation metrics show the influence ofdocument boundary separators, the granularity of different level features anddifferent model training strategies. The results also reveal that the decoderexhibits greater sensitivity to noises compared to the encoder. Thisunderscores the important role played by the decoder, suggesting a potentialdirection for future research in MDS. Furthermore, the experimental resultsindicate that the repetition problem in the generated summaries hascorrelations with the high uncertainty scores.</description><author>Congbo Ma, Wei Emma Zhang, Dileepa Pitawela, Haojie Zhuang, Yanfeng Shu</author><pubDate>Tue, 16 Jul 2024 17:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11948v1</guid></item><item><title>Scaling Exponents Across Parameterizations and Optimizers</title><link>http://arxiv.org/abs/2407.05872v2</link><description>Robust and effective scaling of models from small to large width typicallyrequires the precise adjustment of many algorithmic and architectural details,such as parameterization and optimizer choices. In this work, we propose a newperspective on parameterization by investigating a key assumption in prior workabout the alignment between parameters and data and derive new theoreticalresults under weaker assumptions and a broader set of optimizers. Our extensiveempirical investigation includes tens of thousands of models trained with allcombinations of three optimizers, four parameterizations, several alignmentassumptions, more than a dozen learning rates, and fourteen model sizes up to26.8B parameters. We find that the best learning rate scaling prescriptionwould often have been excluded by the assumptions in prior work. Our resultsshow that all parameterizations, not just maximal update parameterization(muP), can achieve hyperparameter transfer; moreover, our novel per-layerlearning rate prescription for standard parameterization outperforms muP.Finally, we demonstrate that an overlooked aspect of parameterization, theepsilon parameter in Adam, must be scaled correctly to avoid gradient underflowand propose Adam-atan2, a new numerically stable, scale-invariant version ofAdam that eliminates the epsilon hyperparameter entirely.</description><author>Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, Jeffrey Pennington</author><pubDate>Tue, 16 Jul 2024 17:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05872v2</guid></item><item><title>NeSIG: A Neuro-Symbolic Method for Learning to Generate Planning Problems</title><link>http://arxiv.org/abs/2301.10280v2</link><description>In the field of Automated Planning there is often the need for a set ofplanning problems from a particular domain, e.g., to be used as training datafor Machine Learning or as benchmarks in planning competitions. In most cases,these problems are created either by hand or by a domain-specific generator,putting a burden on the human designers. In this paper we propose NeSIG, to thebest of our knowledge the first domain-independent method for automaticallygenerating planning problems that are valid, diverse and difficult to solve. Weformulate problem generation as a Markov Decision Process and train twogenerative policies with Deep Reinforcement Learning to generate problems withthe desired properties. We conduct experiments on three classical domains,comparing our approach against handcrafted, domain-specific instance generatorsand various ablations. Results show NeSIG is able to automatically generatevalid and diverse problems of much greater difficulty (15.5 times more ongeometric average) than domain-specific generators, while simultaneouslyreducing human effort when compared to them. Additionally, it can generalize tolarger problems than those seen during training.</description><author>Carlos NÃºÃ±ez-Molina, Pablo Mesejo, Juan FernÃ¡ndez-Olivares</author><pubDate>Tue, 16 Jul 2024 17:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10280v2</guid></item><item><title>Backpropagation through space, time, and the brain</title><link>http://arxiv.org/abs/2403.16933v2</link><description>How physical networks of neurons, bound by spatio-temporal localityconstraints, can perform efficient credit assignment, remains, to a largeextent, an open question. In machine learning, the answer is almost universallygiven by the error backpropagation algorithm, through both space and time.However, this algorithm is well-known to rely on biologically implausibleassumptions, in particular with respect to spatio-temporal (non-)locality.Alternative forward-propagation models such as real-time recurrent learningonly partially solve the locality problem, but only at the cost of scaling, dueto prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational frameworkfor fully local spatio-temporal credit assignment in physical, dynamicalnetworks of neurons. We start by defining an energy based on neuron-localmismatches, from which we derive both neuronal dynamics via stationarity andparameter dynamics via gradient descent. The resulting dynamics can beinterpreted as a real-time, biologically plausible approximation ofbackpropagation through space and time in deep cortical networks withcontinuous-time neuronal dynamics and continuously active, local synapticplasticity. In particular, GLE exploits the morphology of dendritic trees toenable more complex information storage and processing in single neurons, aswell as the ability of biological neurons to phase-shift their output rate withrespect to their membrane potential, which is essential in both directions ofinformation propagation. For the forward computation, it enables the mapping oftime-continuous inputs to neuronal space, effectively performing aspatio-temporal convolution. For the backward computation, it permits thetemporal inversion of feedback signals, which consequently approximate theadjoint variables necessary for useful parameter updates.</description><author>Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici</author><pubDate>Tue, 16 Jul 2024 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16933v2</guid></item><item><title>Hierarchical Separable Video Transformer for Snapshot Compressive Imaging</title><link>http://arxiv.org/abs/2407.11946v1</link><description>Transformers have achieved the state-of-the-art performance on solving theinverse problem of Snapshot Compressive Imaging (SCI) for video, whoseill-posedness is rooted in the mixed degradation of spatial masking andtemporal aliasing. However, previous Transformers lack an insight into thedegradation and thus have limited performance and efficiency. In this work, wetailor an efficient reconstruction architecture without temporal aggregation inearly layers and Hierarchical Separable Video Transformer (HiSViT) as buildingblock. HiSViT is built by multiple groups of Cross-Scale Separable Multi-headSelf-Attention (CSS-MSA) and Gated Self-Modulated Feed-Forward Network(GSM-FFN) with dense connections, each of which is conducted within a separatechannel portions at a different scale, for multi-scale interactions andlong-range modeling. By separating spatial operations from temporal ones,CSS-MSA introduces an inductive bias of paying more attention within framesinstead of between frames while saving computational overheads. GSM-FFN isdesign to enhance the locality via gated mechanism and factorizedspatial-temporal convolutions. Extensive experiments demonstrate that ourmethod outperforms previous methods by $&gt;\!0.5$ dB with comparable or fewercomplexity and parameters. The source codes and pretrained models are releasedat https://github.com/pwangcs/HiSViT.</description><author>Ping Wang, Yulun Zhang, Lishun Wang, Xin Yuan</author><pubDate>Tue, 16 Jul 2024 17:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11946v1</guid></item><item><title>Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design</title><link>http://arxiv.org/abs/2407.11942v1</link><description>Generative models have the potential to accelerate key steps in the discoveryof novel molecular therapeutics and materials. Diffusion models have recentlyemerged as a powerful approach, excelling at unconditional sample generationand, with data-driven guidance, conditional generation within their trainingdomain. Reliably sampling from high-value regions beyond the training data,however, remains an open challenge -- with current methods predominantlyfocusing on modifying the diffusion process itself. In this paper, we developcontext-guided diffusion (CGD), a simple plug-and-play method that leveragesunlabeled data and smoothness constraints to improve the out-of-distributiongeneralization of guided diffusion models. We demonstrate that this approachleads to substantial performance gains across various settings, includingcontinuous, discrete, and graph-structured diffusion processes withapplications across drug discovery, materials science, and protein design.</description><author>Leo Klarner, Tim G. J. Rudner, Garrett M. Morris, Charlotte M. Deane, Yee Whye Teh</author><pubDate>Tue, 16 Jul 2024 17:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11942v1</guid></item><item><title>Formal Verification of Unknown Dynamical Systems via Gaussian Process Regression</title><link>http://arxiv.org/abs/2201.00655v2</link><description>Leveraging autonomous systems in safety-critical scenarios requires verifyingtheir behaviors in the presence of uncertainties and black-box components thatinfluence the system dynamics. In this work, we develop a framework forverifying discrete-time dynamical systems with unmodelled dynamics and noisymeasurements against temporal logic specifications from an input-outputdataset. The verification framework employs Gaussian process (GP) regression tolearn the unknown dynamics from the dataset and abstracts the continuous-spacesystem as a finite-state, uncertain Markov decision process (MDP). Thisabstraction relies on space discretization and transition probability intervalsthat capture the uncertainty due to the error in GP regression by usingreproducible kernel Hilbert space analysis as well as the uncertainty inducedby discretization. The framework utilizes existing model checking tools forverification of the uncertain MDP abstraction against a given temporal logicspecification. We establish the correctness of extending the verificationresults on the abstraction created from noisy measurements to the underlyingsystem. We show that the computational complexity of the framework ispolynomial in the size of the dataset and discrete abstraction. The complexityanalysis illustrates a trade-off between the quality of the verificationresults and the computational burden to handle larger datasets and finerabstractions. Finally, we demonstrate the efficacy of our learning andverification framework on several case studies with linear, nonlinear, andswitched dynamical systems.</description><author>John Skovbekk, Luca Laurenti, Eric Frew, Morteza Lahijanian</author><pubDate>Tue, 16 Jul 2024 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.00655v2</guid></item><item><title>Beyond Spatial Explanations: Explainable Face Recognition in the Frequency Domain</title><link>http://arxiv.org/abs/2407.11941v1</link><description>The need for more transparent face recognition (FR), along with othervisual-based decision-making systems has recently attracted more attention inresearch, society, and industry. The reasons why two face images are matched ornot matched by a deep learning-based face recognition system are not obviousdue to the high number of parameters and the complexity of the models. However,it is important for users, operators, and developers to ensure trust andaccountability of the system and to analyze drawbacks such as biased behavior.While many previous works use spatial semantic maps to highlight the regionsthat have a significant influence on the decision of the face recognitionsystem, frequency components which are also considered by CNNs, are neglected.In this work, we take a step forward and investigate explainable facerecognition in the unexplored frequency domain. This makes this work the firstto propose explainability of verification-based decisions in the frequencydomain, thus explaining the relative influence of the frequency components ofeach input toward the obtained outcome. To achieve this, we manipulate faceimages in the spatial frequency domain and investigate the impact onverification outcomes. In extensive quantitative experiments, along withinvestigating two special scenarios cases, cross-resolution FR and morphingattacks (the latter in supplementary material), we observe the applicability ofour proposed frequency-based explanations.</description><author>Marco Huber, Naser Damer</author><pubDate>Tue, 16 Jul 2024 17:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11941v1</guid></item><item><title>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</title><link>http://arxiv.org/abs/2312.03641v2</link><description>Motions in a video primarily consist of camera motion, induced by cameramovement, and object motion, resulting from object movement. Accurate controlof both camera and object motion is essential for video generation. However,existing works either mainly focus on one type of motion or do not clearlydistinguish between the two, limiting their control capabilities and diversity.Therefore, this paper presents MotionCtrl, a unified and flexible motioncontroller for video generation designed to effectively and independentlycontrol camera and object motion. The architecture and training strategy ofMotionCtrl are carefully devised, taking into account the inherent propertiesof camera motion, object motion, and imperfect training data. Compared toprevious methods, MotionCtrl offers three main advantages: 1) It effectivelyand independently controls camera motion and object motion, enabling morefine-grained motion control and facilitating flexible and diverse combinationsof both types of motion. 2) Its motion conditions are determined by cameraposes and trajectories, which are appearance-free and minimally impact theappearance or shape of objects in generated videos. 3) It is a relativelygeneralizable model that can adapt to a wide array of camera poses andtrajectories once trained. Extensive qualitative and quantitative experimentshave been conducted to demonstrate the superiority of MotionCtrl over existingmethods. Project Page: https://wzhouxiff.github.io/projects/MotionCtrl/</description><author>Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan</author><pubDate>Tue, 16 Jul 2024 17:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03641v2</guid></item><item><title>Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and Apnea</title><link>http://arxiv.org/abs/2407.11936v1</link><description>Polysomnography (PSG), the current gold standard method for monitoring anddetecting sleep disorders, is cumbersome and costly. At-home testing solutions,known as home sleep apnea testing (HSAT), exist. However, they arecontact-based, a feature which limits the ability of some patient populationsto tolerate testing and discourages widespread deployment. Previous work onnon-contact sleep monitoring for sleep apnea detection either estimatesrespiratory effort using radar or nasal airflow using a thermal camera, but hasnot compared the two or used them together. We conducted a study on 10participants, ages 34 - 78, with suspected sleep disorders using a hardwaresetup with a synchronized radar and thermal camera. We show the firstcomparison of radar and thermal imaging for sleep monitoring, and find that ourthermal imaging method outperforms radar significantly. Our thermal imagingmethod detects apneas with an accuracy of 0.99, a precision of 0.68, a recallof 0.74, an F1 score of 0.71, and an intra-class correlation of 0.70; our radarmethod detects apneas with an accuracy of 0.83, a precision of 0.13, a recallof 0.86, an F1 score of 0.22, and an intra-class correlation of 0.13. We alsopresent a novel proposal for classifying obstructive and central sleep apnea byleveraging a multimodal setup. This method could be used accurately detect andclassify apneas during sleep with non-contact sensors, thereby improvingdiagnostic capacities in patient populations unable to tolerate currenttechnology.</description><author>Kai Del Regno, Alexander Vilesov, Adnan Armouti, Anirudh Bindiganavale Harish, Selim Emir Can, Ashley Kita, Achuta Kadambi</author><pubDate>Tue, 16 Jul 2024 17:26:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11936v1</guid></item><item><title>Learning Multi-view Anomaly Detection</title><link>http://arxiv.org/abs/2407.11935v1</link><description>This study explores the recently proposed challenging multi-view AnomalyDetection (AD) task. Single-view tasks would encounter blind spots from otherperspectives, resulting in inaccuracies in sample-level prediction. Therefore,we introduce the \textbf{M}ulti-\textbf{V}iew \textbf{A}nomaly\textbf{D}etection (\textbf{MVAD}) framework, which learns and integratesfeatures from multi-views. Specifically, we proposed a\textbf{M}ulti-\textbf{V}iew \textbf{A}daptive \textbf{S}election(\textbf{MVAS}) algorithm for feature learning and fusion across multipleviews. The feature maps are divided into neighbourhood attention windows tocalculate a semantic correlation matrix between single-view windows and allother views, which is a conducted attention mechanism for each single-viewwindow and the top-K most correlated multi-view windows. Adjusting the windowsizes and top-K can minimise the computational complexity to linear. Extensiveexperiments on the Real-IAD dataset for cross-setting (multi/single-class)validate the effectiveness of our approach, achieving state-of-the-artperformance among sample \textbf{4.1\%}$\uparrow$/ image\textbf{5.6\%}$\uparrow$/pixel \textbf{6.7\%}$\uparrow$ levels with a total often metrics with only \textbf{18M} parameters and fewer GPU memory and trainingtime.</description><author>Haoyang He, Jiangning Zhang, Guanzhong Tian, Chengjie Wang, Lei Xie</author><pubDate>Tue, 16 Jul 2024 17:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11935v1</guid></item><item><title>Fairly Accurate: Optimizing Accuracy Parity in Fair Target-Group Detection</title><link>http://arxiv.org/abs/2407.11933v1</link><description>In algorithmic toxicity detection pipelines, it is important to identifywhich demographic group(s) are the subject of a post, a task commonly known as\textit{target (group) detection}. While accurate detection is clearlyimportant, we further advocate a fairness objective: to provide equalprotection to all groups who may be targeted. To this end, we adopt\textit{Accuracy Parity} (AP) -- balanced detection accuracy across groups --as our fairness objective. However, in order to align model training with ourAP fairness objective, we require an equivalent loss function. Moreover, forgradient-based models such as neural networks, this loss function needs to bedifferentiable. Because no such loss function exists today for AP, we propose\emph{Group Accuracy Parity} (GAP): the first differentiable loss functionhaving a one-on-one mapping to AP. We empirically show that GAP addressesdisparate impact on groups for target detection. Furthermore, because a singlepost often targets multiple groups in practice, we also provide a mathematicalextension of GAP to larger multi-group settings, something typically requiringheuristics in prior work. Our findings show that by optimizing AP, GAP bettermitigates bias in comparison with other commonly employed loss functions.</description><author>Soumyajit Gupta, Venelin Kovatchev, Maria De-Arteaga, Matthew Lease</author><pubDate>Tue, 16 Jul 2024 17:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11933v1</guid></item><item><title>Impossibility of latent inner product recovery via rate distortion</title><link>http://arxiv.org/abs/2407.11932v1</link><description>In this largely expository note, we present an impossibility result for innerproduct recovery in a random geometric graph or latent space model using therate-distortion theory. More precisely, suppose that we observe a graph $A$ on$n$ vertices with average edge density $p$ generated from Gaussian or sphericallatent locations $z_1, \dots, z_n \in \mathbb{R}^d$ associated with the $n$vertices. It is of interest to estimate the inner products $\langle z_i, z_j\rangle$ which represent the geometry of the latent points. We prove that it isimpossible to recover the inner products if $d \gtrsim n h(p)$ where $h(p)$ isthe binary entropy function. This matches the condition required for positiveresults on inner product recovery in the literature. The proof follows thewell-established rate-distortion theory with the main technical ingredientbeing a lower bound on the rate-distortion function of the Wishart distributionwhich is interesting in its own right.</description><author>Cheng Mao, Shenduo Zhang</author><pubDate>Tue, 16 Jul 2024 17:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11932v1</guid></item><item><title>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title><link>http://arxiv.org/abs/2407.09879v2</link><description>Despite the remarkable success of LLMs in English, there is a significant gapin performance in non-English languages. In order to address this, we introducea novel recipe for creating a multilingual synthetic instruction tuningdataset, sPhinX, which is created by selectively translating instructionresponse pairs from English into 50 languages. We test the effectiveness ofsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small andMistral-7B and then evaluating them across a comprehensive suite ofmultilingual benchmarks that test reasoning, question answering, and readingcomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned withsPhinX perform better on an average by 4.2%pt and 5%pt respectively as comparedto the baselines. We also devise a strategy to incorporate N-shot examples ineach fine-tuning sample which further boosts the performance of these models by3%pt and 10%pt respectively. Additionally, sPhinX also outperforms othermultilingual instruction tuning datasets on the same benchmarks along withbeing sample efficient and diverse, thereby reducing dataset creation costs.Additionally, instruction tuning with sPhinX does not lead to regression onmost standard LLM benchmarks.</description><author>Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram</author><pubDate>Tue, 16 Jul 2024 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09879v2</guid></item><item><title>Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering</title><link>http://arxiv.org/abs/2407.11930v1</link><description>Long-form question answering (LFQA) aims to provide thorough and in-depthanswers to complex questions, enhancing comprehension. However, such detailedresponses are prone to hallucinations and factual inconsistencies, challengingtheir faithful evaluation. This work introduces HaluQuestQA, the firsthallucination dataset with localized error annotations for human-written andmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7kspan-level error annotations for five different error types by expertannotators, along with preference judgments. Using our collected data, wethoroughly analyze the shortcomings of long-form answers and find that theylack comprehensiveness and provide unhelpful references. We train an automaticfeedback model on this dataset that predicts error spans with incompleteinformation and provides associated explanations. Finally, we propose aprompt-based approach, Error-informed refinement, that uses signals from thelearned feedback model to refine generated answers, which we show reduceshallucination and improves answer quality. Furthermore, humans find answersgenerated by our approach comprehensive and highly prefer them (84%) over thebaseline answers.</description><author>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</author><pubDate>Tue, 16 Jul 2024 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11930v1</guid></item><item><title>Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach</title><link>http://arxiv.org/abs/2407.11928v1</link><description>Graph Neural Network (GNN) achieves great success for node-level andgraph-level tasks via encoding meaningful topological structures of networks invarious domains, ranging from social to biological networks. However, repeatedaggregation operations lead to excessive mixing of node representations,particularly in dense regions with multiple GNN layers, resulting in nearlyindistinguishable embeddings. This phenomenon leads to the oversmoothingproblem that hampers downstream graph analytics tasks. To overcome this issue,we propose a novel and flexible truss-based graph sparsification model thatprunes edges from dense regions of the graph. Pruning redundant edges in denseregions helps to prevent the aggregation of excessive neighborhood informationduring hierarchical message passing and pooling in GNN models. We then utilizeour sparsification model in the state-of-the-art baseline GNNs and poolingmodels, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, andAdamGNN. Extensive experiments on different real-world datasets show that ourmodel significantly improves the performance of the baseline GNN models in thegraph classification task.</description><author>Tanvir Hossain, Khaled Mohammed Saifuddin, Muhammad Ifte Khairul Islam, Farhan Tanvir, Esra Akbas</author><pubDate>Tue, 16 Jul 2024 17:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11928v1</guid></item><item><title>InterFusion: Text-Driven Generation of 3D Human-Object Interaction</title><link>http://arxiv.org/abs/2403.15612v2</link><description>In this study, we tackle the complex task of generating 3D human-objectinteractions (HOI) from textual descriptions in a zero-shot text-to-3D manner.We identify and address two key challenges: the unsatisfactory outcomes ofdirect text-to-3D methods in HOI, largely due to the lack of pairedtext-interaction data, and the inherent difficulties in simultaneouslygenerating multiple concepts with complex spatial relationships. To effectivelyaddress these issues, we present InterFusion, a two-stage frameworkspecifically designed for HOI generation. InterFusion involves human poseestimations derived from text as geometric priors, which simplifies thetext-to-3D conversion process and introduces additional constraints foraccurate object generation. At the first stage, InterFusion extracts 3D humanposes from a synthesized image dataset depicting a wide range of interactions,subsequently mapping these poses to interaction descriptions. The second stageof InterFusion capitalizes on the latest developments in text-to-3D generation,enabling the production of realistic and high-quality 3D HOI scenes. This isachieved through a local-global optimization process, where the generation ofhuman body and object is optimized separately, and jointly refined with aglobal optimization of the entire scene, ensuring a seamless and contextuallycoherent integration. Our experimental results affirm that InterFusionsignificantly outperforms existing state-of-the-art methods in 3D HOIgeneration.</description><author>Sisi Dai, Wenhao Li, Haowen Sun, Haibin Huang, Chongyang Ma, Hui Huang, Kai Xu, Ruizhen Hu</author><pubDate>Tue, 16 Jul 2024 17:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15612v2</guid></item><item><title>Bayesian Causal Forests for Longitudinal Data: Assessing the Impact of Part-Time Work on Growth in High School Mathematics Achievement</title><link>http://arxiv.org/abs/2407.11927v1</link><description>Modelling growth in student achievement is a significant challenge in thefield of education. Understanding how interventions or experiences such aspart-time work can influence this growth is also important. Traditional methodslike difference-in-differences are effective for estimating causal effects fromlongitudinal data. Meanwhile, Bayesian non-parametric methods have recentlybecome popular for estimating causal effects from single time pointobservational studies. However, there remains a scarcity of methods capable ofcombining the strengths of these two approaches to flexibly estimateheterogeneous causal effects from longitudinal data. Motivated by two waves ofdata from the High School Longitudinal Study, the NCES' most recentlongitudinal study which tracks a representative sample of over 20,000 studentsin the US, our study introduces a longitudinal extension of Bayesian CausalForests. This model allows for the flexible identification of both individualgrowth in mathematical ability and the effects of participation in part-timework. Simulation studies demonstrate the predictive performance and reliableuncertainty quantification of the proposed model. Results reveal the negativeimpact of part time work for most students, but hint at potential benefits forthose students with an initially low sense of school belonging. Clear signs ofa widening achievement gap between students with high and low academicachievement are also identified. Potential policy implications are discussed,along with promising areas for future research.</description><author>Nathan McJames, Ann O'Shea, Andrew Parnell</author><pubDate>Tue, 16 Jul 2024 17:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11927v1</guid></item><item><title>21cmEMU: an emulator of 21cmFAST summary observables</title><link>http://arxiv.org/abs/2309.05697v3</link><description>Recent years have witnessed rapid progress in observations of the Epoch ofReionization (EoR). These have enabled high-dimensional inference of galaxy andintergalactic medium (IGM) properties during the first billion years of ourUniverse. However, even using efficient, semi-numerical simulations,traditional inference approaches that compute 3D lightcones on-the-fly can take$10^5$ core hours. Here we present 21cmEMU: an emulator of several summaryobservables from the popular 21cmFAST simulation code. 21cmEMU takes as inputnine parameters characterizing EoR galaxies, and outputs the following summarystatistics: (i) the IGM mean neutral fraction; (ii) the 21-cm power spectrum;(iii) the mean 21-cm spin temperature; (iv) the sky-averaged (global) 21-cmsignal; (v) the ultraviolet (UV) luminosity functions (LFs); and (vi) theThomson scattering optical depth to the cosmic microwave background (CMB). Allobservables are predicted with sub-percent median accuracy, with a reduction ofthe computational cost by a factor of over 10$^4$. After validating inferenceresults, we showcase a few applications, including: (i) quantifying therelative constraining power of different observational datasets; (ii) seeinghow recent claims of a late EoR impact previous inferences; and (iii)forecasting upcoming constraints from the sixth observing season of theHydrogen Epoch of Reionization Array (HERA) telescope. 21cmEMU ispublicly-available, and is included as an alternative simulator in the public21CMMC sampler.</description><author>Daniela Breitman, Andrei Mesinger, Steven Murray, David Prelogovic, Yuxiang Qin, Roberto Trotta</author><pubDate>Tue, 16 Jul 2024 17:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05697v3</guid></item><item><title>Learning secondary tool affordances of human partners using iCub robot's egocentric data</title><link>http://arxiv.org/abs/2407.11922v1</link><description>Objects, in particular tools, provide several action possibilities to theagents that can act on them, which are generally associated with the term ofaffordances. A tool is typically designed for a specific purpose, such asdriving a nail in the case of a hammer, which we call as the primaryaffordance. A tool can also be used beyond its primary purpose, in which casewe can associate this auxiliary use with the term secondary affordance.Previous work on affordance perception and learning has been mostly focused onprimary affordances. Here, we address the less explored problem of learning thesecondary tool affordances of human partners. To do this, we use the iCub robotto observe human partners with three cameras while they perform actions ontwenty objects using four different tools. In our experiments, human partnersutilize tools to perform actions that do not correspond to their primaryaffordances. For example, the iCub robot observes a human partner using a rulerfor pushing, pulling, and moving objects instead of measuring their lengths. Inthis setting, we constructed a dataset by taking images of objects before andafter each action is executed. We then model learning secondary affordances bytraining three neural networks (ResNet-18, ResNet-50, and ResNet-101) each onthree tasks, using raw images showing the `initial' and `final' position ofobjects as input: (1) predicting the tool used to move an object, (2)predicting the tool used with an additional categorical input that encoded theaction performed, and (3) joint prediction of both tool used and actionperformed. Our results indicate that deep learning architectures enable theiCub robot to predict secondary tool affordances, thereby paving the road forhuman-robot collaborative object manipulation involving complex affordances.</description><author>Bosong Ding, Erhan Oztop, Giacomo Spigler, Murat Kirtay</author><pubDate>Tue, 16 Jul 2024 17:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11922v1</guid></item><item><title>IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields</title><link>http://arxiv.org/abs/2407.11921v1</link><description>Neural Radiance Field (NeRF) represents a significant advancement in computervision, offering implicit neural network-based scene representation and novelview synthesis capabilities. Its applications span diverse fields includingrobotics, urban mapping, autonomous navigation, virtual reality/augmentedreality, etc., some of which are considered high-risk AI applications. However,despite its widespread adoption, the robustness and security of NeRF remainlargely unexplored. In this study, we contribute to this area by introducingthe Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). Thisattack involves embedding a hidden backdoor view into NeRF, allowing it toproduce predetermined outputs, i.e. illusory, when presented with the specifiedbackdoor view while maintaining normal performance with standard inputs. Ourattack is specifically designed to deceive users or downstream models at aparticular position while ensuring that any abnormalities in NeRF remainundetectable from other viewpoints. Experimental results demonstrate theeffectiveness of our Illusory Poisoning Attack, successfully presenting thedesired illusory on the specified viewpoint without impacting other views.Notably, we achieve this attack by introducing small perturbations solely tothe training set. The code can be found athttps://github.com/jiang-wenxiang/IPA-NeRF.</description><author>Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang</author><pubDate>Tue, 16 Jul 2024 17:11:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11921v1</guid></item><item><title>What's Wrong? Refining Meeting Summaries with LLM Feedback</title><link>http://arxiv.org/abs/2407.11919v1</link><description>Meeting summarization has become a critical task since digital encountershave become a common practice. Large language models (LLMs) show greatpotential in summarization, offering enhanced coherence and contextunderstanding compared to traditional methods. However, they still struggle tomaintain relevance and avoid hallucination. We introduce a multi-LLM correctionapproach for meeting summarization using a two-phase process that mimics thehuman review process: mistake identification and summary refinement. We releaseQMSum Mistake, a dataset of 200 automatically generated meeting summariesannotated by humans on nine error types, including structural, omission, andirrelevance errors. Our experiments show that these errors can be identifiedwith high accuracy by an LLM. We transform identified mistakes into actionablefeedback to improve the quality of a given summary measured by relevance,informativeness, conciseness, and coherence. This post-hoc refinementeffectively improves summary quality by leveraging multiple LLMs to validateoutput quality. Our multi-LLM approach for meeting summarization showspotential for similar complex text generation tasks requiring robustness,action planning, and discussion towards a goal.</description><author>Frederic Kirstein, Terry Ruas, Bela Gipp</author><pubDate>Tue, 16 Jul 2024 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11919v1</guid></item><item><title>Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space</title><link>http://arxiv.org/abs/2407.11917v1</link><description>We propose a new uncertainty estimator for gradient-free optimisation ofblack-box simulators using deep generative surrogate models. Optimisation ofthese simulators is especially challenging for stochastic simulators and higherdimensions. To address these issues, we utilise a deep generative surrogateapproach to model the black box response for the entire parameter space. Wethen leverage this knowledge to estimate the proposed uncertainty based on theWasserstein distance - the Wasserstein uncertainty. This approach is employedin a posterior agnostic gradient-free optimisation algorithm that minimisesregret over the entire parameter space. A series of tests were conducted todemonstrate that our method is more robust to the shape of both the black boxfunction and the stochastic response of the black box than state-of-the-artmethods, such as efficient global optimisation with a deep Gaussian processsurrogate.</description><author>Tigran Ramazyan, Mikhail Hushchyn, Denis Derkach</author><pubDate>Tue, 16 Jul 2024 17:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11917v1</guid></item><item><title>Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task</title><link>http://arxiv.org/abs/2407.11915v1</link><description>Head movements are crucial for social human-human interaction. They cantransmit important cues (e.g., joint attention, speaker detection) that cannotbe achieved with verbal interaction alone. This advantage also holds forhuman-robot interaction. Even though modeling human motions through generativeAI models has become an active research area within robotics in recent years,the use of these methods for producing head movements in human-robotinteraction remains underexplored. In this work, we employed a generative AIpipeline to produce human-like head movements for a Nao humanoid robot. Inaddition, we tested the system on a real-time active-speaker tracking task in agroup conversation setting. Overall, the results show that the Nao robotsuccessfully imitates human head movements in a natural manner while activelytracking the speakers during the conversation. Code and data from this studyare available at https://github.com/dingdingding60/Humanoids2024HRI</description><author>Bosong Ding, Murat Kirtay, Giacomo Spigler</author><pubDate>Tue, 16 Jul 2024 17:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11915v1</guid></item><item><title>Enhanced Safety in Autonomous Driving: Integrating Latent State Diffusion Model for End-to-End Navigation</title><link>http://arxiv.org/abs/2407.06317v3</link><description>With the advancement of autonomous driving, ensuring safety during motionplanning and navigation is becoming more and more important. However, mostend-to-end planning methods suffer from a lack of safety. This researchaddresses the safety issue in the control optimization problem of autonomousdriving, formulated as Constrained Markov Decision Processes (CMDPs). Wepropose a novel, model-based approach for policy optimization, utilizing aconditional Value-at-Risk based Soft Actor Critic to manage constraints incomplex, high-dimensional state spaces effectively. Our method introduces aworst-case actor to guide safe exploration, ensuring rigorous adherence tosafety requirements even in unpredictable scenarios. The policy optimizationemploys the Augmented Lagrangian method and leverages latent diffusion modelsto predict and simulate future trajectories. This dual approach not only aidsin navigating environments safely but also refines the policy's performance byintegrating distribution modeling to account for environmental uncertainties.Empirical evaluations conducted in both simulated and real environmentdemonstrate that our approach outperforms existing methods in terms of safety,efficiency, and decision-making capabilities.</description><author>Detian Chu, Linyuan Bai, Jianuo Huang, Zhenlong Fang, Peng Zhang, Wei Kang</author><pubDate>Tue, 16 Jul 2024 17:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06317v3</guid></item><item><title>Quantised Global Autoencoder: A Holistic Approach to Representing Visual Data</title><link>http://arxiv.org/abs/2407.11913v1</link><description>In quantised autoencoders, images are usually split into local patches, eachencoded by one token. This representation is redundant in the sense that thesame number of tokens is spend per region, regardless of the visual informationcontent in that region. Adaptive discretisation schemes like quadtrees areapplied to allocate tokens for patches with varying sizes, but this just variesthe region of influence for a token which nevertheless remains a localdescriptor. Modern architectures add an attention mechanism to the autoencoderwhich infuses some degree of global information into the local tokens. Despitethe global context, tokens are still associated with a local image region. Incontrast, our method is inspired by spectral decompositions which transform aninput signal into a superposition of global frequencies. Taking the data-drivenperspective, we learn custom basis functions corresponding to the codebookentries in our VQ-VAE setup. Furthermore, a decoder combines these basisfunctions in a non-linear fashion, going beyond the simple linear superpositionof spectral decompositions. We can achieve this global description with anefficient transpose operation between features and channels and demonstrate ourperformance on compression.</description><author>Tim Elsner, Paula Usinger, Victor Czech, Gregor Kobsik, Yanjiang He, Isaak Lim, Leif Kobbelt</author><pubDate>Tue, 16 Jul 2024 17:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11913v1</guid></item><item><title>Benchmarking the Attribution Quality of Vision Models</title><link>http://arxiv.org/abs/2407.11910v1</link><description>Attribution maps are one of the most established tools to explain thefunctioning of computer vision models. They assign importance scores to inputfeatures, indicating how relevant each feature is for the prediction of a deepneural network. While much research has gone into proposing new attributionmethods, their proper evaluation remains a difficult challenge. In this work,we propose a novel evaluation protocol that overcomes two fundamentallimitations of the widely used incremental-deletion protocol, i.e., theout-of-domain issue and lacking inter-model comparisons. This allows us toevaluate 23 attribution methods and how eight different design choices ofpopular vision models affect their attribution quality. We find thatintrinsically explainable models outperform standard models and that rawattribution values exhibit a higher attribution quality than what is known fromprevious work. Further, we show consistent changes in the attribution qualitywhen varying the network design, indicating that some standard design choicespromote attribution quality.</description><author>Robin Hesse, Simone Schaub-Meyer, Stefan Roth</author><pubDate>Tue, 16 Jul 2024 17:02:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11910v1</guid></item><item><title>Learning local equivariant representations for quantum operators</title><link>http://arxiv.org/abs/2407.06053v3</link><description>Predicting quantum operator matrices such as Hamiltonian, overlap, anddensity matrices in the density functional theory (DFT) framework is crucialfor understanding material properties. Current methods often focus onindividual operators and struggle with efficiency and scalability for largesystems. Here we introduce a novel deep learning model, SLEM (strictlylocalized equivariant message-passing) for predicting multiple quantumoperators, that achieves state-of-the-art accuracy while dramatically improvingcomputational efficiency. SLEM's key innovation is its strict locality-baseddesign, constructing local, equivariant representations for quantum tensorswhile preserving physical symmetries. This enables complex many-body dependencewithout expanding the effective receptive field, leading to superior dataefficiency and transferability. Using an innovative SO(2) convolutiontechnique, SLEM reduces the computational complexity of high-order tensorproducts and is therefore capable of handling systems requiring the $f$ and $g$orbitals in their basis sets. We demonstrate SLEM's capabilities across diverse2D and 3D materials, achieving high accuracy even with limited training data.SLEM's design facilitates efficient parallelization, potentially extending DFTsimulations to systems with device-level sizes, opening new possibilities forlarge-scale quantum simulations and high-throughput materials discovery.</description><author>Zhanghao Zhouyin, Zixi Gan, Shishir Kumar Pandey, Linfeng Zhang, Qiangqiang Gu</author><pubDate>Tue, 16 Jul 2024 16:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06053v3</guid></item><item><title>Designing Decision Support Systems Using Counterfactual Prediction Sets</title><link>http://arxiv.org/abs/2306.03928v3</link><description>Decision support systems for classification tasks are predominantly designedto predict the value of the ground truth labels. However, since theirpredictions are not perfect, these systems also need to make human expertsunderstand when and how to use these predictions to update their ownpredictions. Unfortunately, this has been proven challenging. In this context,it has been recently argued that an alternative type of decision supportsystems may circumvent this challenge. Rather than providing a single labelprediction, these systems provide a set of label prediction values constructedusing a conformal predictor, namely a prediction set, and forcefully askexperts to predict a label value from the prediction set. However, the designand evaluation of these systems have so far relied on stylized expert models,questioning their promise. In this paper, we revisit the design of this type ofsystems from the perspective of online learning and develop a methodology thatdoes not require, nor assumes, an expert model. Our methodology leverages thenested structure of the prediction sets provided by any conformal predictor anda natural counterfactual monotonicity assumption to achieve an exponentialimprovement in regret in comparison to vanilla bandit algorithms. We conduct alarge-scale human subject study ($n = 2{,}751$) to compare our methodology toseveral competitive baselines. The results show that, for decision supportsystems based on prediction sets, limiting experts' level of agency leads togreater performance than allowing experts to always exercise their own agency.We have made available the data gathered in our human subject study as well asan open source implementation of our system athttps://github.com/Networks-Learning/counterfactual-prediction-sets.</description><author>Eleni Straitouri, Manuel Gomez Rodriguez</author><pubDate>Tue, 16 Jul 2024 16:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03928v3</guid></item><item><title>GraphFM: A Scalable Framework for Multi-Graph Pretraining</title><link>http://arxiv.org/abs/2407.11907v1</link><description>Graph neural networks are typically trained on individual datasets, oftenrequiring highly specialized models and extensive hyperparameter tuning. Thisdataset-specific approach arises because each graph dataset often has uniquenode features and diverse connectivity structures, making it difficult to builda generalist model. To address these challenges, we introduce a scalablemulti-graph multi-task pretraining approach specifically tailored for nodeclassification tasks across diverse graph datasets from different domains. Ourmethod, Graph Foundation Model (GraphFM), leverages a Perceiver-based encoderthat employs learned latent tokens to compress domain-specific features into acommon latent space. This approach enhances the model's ability to generalizeacross different graphs and allows for scaling across diverse data. Wedemonstrate the efficacy of our approach by training a model on 152 differentgraph datasets comprising over 7.4 million nodes and 189 million edges,establishing the first set of scaling laws for multi-graph pretraining ondatasets spanning many domains (e.g., molecules, citation and product graphs).Our results show that pretraining on a diverse array of real and syntheticgraphs improves the model's adaptability and stability, while performingcompetitively with state-of-the-art specialist models. This work illustratesthat multi-graph pretraining can significantly reduce the burden imposed by thecurrent graph training paradigm, unlocking new capabilities for the field ofgraph neural networks by creating a single generalist model that performscompetitively across a wide range of datasets and tasks.</description><author>Divyansha Lachi, Mehdi Azabou, Vinam Arora, Eva Dyer</author><pubDate>Tue, 16 Jul 2024 16:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11907v1</guid></item><item><title>The Importance of Online Data: Understanding Preference Fine-tuning via Coverage</title><link>http://arxiv.org/abs/2406.01462v2</link><description>Learning from human preference data has emerged as the dominant paradigm forfine-tuning large language models (LLMs). The two most common families oftechniques -- online reinforcement learning (RL) such as Proximal PolicyOptimization (PPO) and offline contrastive methods such as Direct PreferenceOptimization (DPO) -- were positioned as equivalent in prior work due to thefact that both have to start from the same offline preference dataset. Tofurther expand our theoretical understanding of the similarities anddifferences between online and offline techniques for preference fine-tuning,we conduct a rigorous analysis through the lens of dataset coverage, a conceptthat captures how the training data covers the test distribution and is widelyused in RL. We prove that a global coverage condition is both necessary andsufficient for offline contrastive methods to converge to the optimal policy,but a weaker partial coverage condition suffices for online RL methods. Thisseparation provides one explanation of why online RL methods can perform betterthan offline methods, especially when the offline preference data is notdiverse enough. Finally, motivated by our preceding theoretical observations,we derive a hybrid preference optimization (HyPO) algorithm that uses offlinedata for contrastive-based preference optimization and online data for KLregularization. Theoretically and empirically, we demonstrate that HyPO is moreperformant than its pure offline counterpart DPO, while still preserving itscomputation and memory efficiency.</description><author>Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun</author><pubDate>Tue, 16 Jul 2024 16:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01462v2</guid></item><item><title>SegSTRONG-C: Segmenting Surgical Tools Robustly On Non-adversarial Generated Corruptions -- An EndoVis'24 Challenge</title><link>http://arxiv.org/abs/2407.11906v1</link><description>Accurate segmentation of tools in robot-assisted surgery is critical formachine perception, as it facilitates numerous downstream tasks includingaugmented reality feedback. While current feed-forward neural network-basedmethods exhibit excellent segmentation performance under ideal conditions,these models have proven susceptible to even minor corruptions, significantlyimpairing the model's performance. This vulnerability is especially problematicin surgical settings where predictions might be used to inform high-stakesdecisions. To better understand model behavior under non-adversarialcorruptions, prior work has explored introducing artificial corruptions, likeGaussian noise or contrast perturbation to test set images, to assess modelrobustness. However, these corruptions are either not photo-realistic ormodel/task agnostic. Thus, these investigations provide limited insights intomodel deterioration under realistic surgical corruptions. To address thislimitation, we introduce the SegSTRONG-C challenge that aims to promote thedevelopment of algorithms robust to unforeseen but plausible image corruptionsof surgery, like smoke, bleeding, and low brightness. We collect and releasecorruption-free mock endoscopic video sequences for the challenge participantsto train their algorithms and benchmark them on video sequences withphoto-realistic non-adversarial corruptions for a binary robot toolsegmentation task. This new benchmark will allow us to carefully study neuralnetwork robustness to non-adversarial corruptions of surgery, thus constitutingan important first step towards more robust models for surgical computervision. In this paper, we describe the data collection and annotation protocol,baseline evaluations of established segmentation models, and dataaugmentation-based techniques to enhance model robustness.</description><author>Hao Ding, Tuxun Lu, Yuqian Zhang, Ruixing Liang, Hongchao Shu, Lalithkumar Seenivasan, Yonghao Long, Qi Dou, Cong Gao, Mathias Unberath</author><pubDate>Tue, 16 Jul 2024 16:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11906v1</guid></item><item><title>Encapsulating Knowledge in One Prompt</title><link>http://arxiv.org/abs/2407.11902v1</link><description>This paradigm encapsulates knowledge from various models into a solitaryprompt without altering the original models or requiring access to the trainingdata, which enables us to achieve efficient and convenient knowledge transferin more realistic scenarios. From a practicality standpoint, this paradigm notonly for the first time proves the effectiveness of Visual Prompt in datainaccessible contexts, but also solves the problems of low model reusabilityand high storage resource consumption faced by traditional Data-Free KnowledgeTransfer, which means that we can realize the parallel knowledge transfer ofmultiple models without modifying any source model. Extensive experimentsacross various datasets and models demonstrate the efficacy of the proposedKiOP knowledge transfer paradigm. Without access to real training data and withrigorous storage capacity constraints, it is also capable of yieldingconsiderable outcomes when dealing with cross-model backbone setups andhandling parallel knowledge transfer processing requests with multiple (morethan 2) models.</description><author>Qi Li, Runpeng Yu, Xinchao Wang</author><pubDate>Tue, 16 Jul 2024 16:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11902v1</guid></item><item><title>Combining Wasserstein-1 and Wasserstein-2 proximals: robust manifold learning via well-posed generative flows</title><link>http://arxiv.org/abs/2407.11901v1</link><description>We formulate well-posed continuous-time generative flows for learningdistributions that are supported on low-dimensional manifolds throughWasserstein proximal regularizations of $f$-divergences. Wasserstein-1 proximaloperators regularize $f$-divergences so that singular distributions can becompared. Meanwhile, Wasserstein-2 proximal operators regularize the paths ofthe generative flows by adding an optimal transport cost, i.e., a kineticenergy penalization. Via mean-field game theory, we show that the combinationof the two proximals is critical for formulating well-posed generative flows.Generative flows can be analyzed through optimality conditions of a mean-fieldgame (MFG), a system of a backward Hamilton-Jacobi (HJ) and a forwardcontinuity partial differential equations (PDEs) whose solution characterizesthe optimal generative flow. For learning distributions that are supported onlow-dimensional manifolds, the MFG theory shows that the Wasserstein-1proximal, which addresses the HJ terminal condition, and the Wasserstein-2proximal, which addresses the HJ dynamics, are both necessary for thecorresponding backward-forward PDE system to be well-defined and have a uniquesolution with provably linear flow trajectories. This implies that thecorresponding generative flow is also unique and can therefore be learned in arobust manner even for learning high-dimensional distributions supported onlow-dimensional manifolds. The generative flows are learned through adversarialtraining of continuous-time flows, which bypasses the need for reversesimulation. We demonstrate the efficacy of our approach for generatinghigh-dimensional images without the need to resort to autoencoders orspecialized architectures.</description><author>Hyemin Gu, Markos A. Katsoulakis, Luc Rey-Bellet, Benjamin J. Zhang</author><pubDate>Tue, 16 Jul 2024 16:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11901v1</guid></item><item><title>Qwen2 Technical Report</title><link>http://arxiv.org/abs/2407.10671v2</link><description>This report introduces the Qwen2 series, the latest addition to our largelanguage models and large multimodal models. We release a comprehensive suiteof foundational and instruction-tuned language models, encompassing a parameterrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Expertsmodel. Qwen2 surpasses most prior open-weight models, including its predecessorQwen1.5, and exhibits competitive performance relative to proprietary modelsacross diverse benchmarks on language understanding, generation, multilingualproficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 onMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a baselanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2demonstrates robust multilingual capabilities, proficient in approximately 30languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility andglobal reach. To foster community innovation and accessibility, we have made the Qwen2model weights openly available on Hugging Face and ModelScope, and thesupplementary materials including example code on GitHub. These platforms alsoinclude resources for quantization, fine-tuning, and deployment, facilitating awide range of applications and research endeavors.</description><author>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhihao Fan</author><pubDate>Tue, 16 Jul 2024 16:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10671v2</guid></item><item><title>OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces</title><link>http://arxiv.org/abs/2407.11895v1</link><description>Recently, human-computer interaction with various modalities has shownpromising applications, like GPT-4o and Gemini. Given the foundational role ofmultimodal joint representation in understanding and generation pipelines,high-quality omni joint representations would be a step toward co-processingmore diverse multimodal information. In this work, we present OmniBind,large-scale multimodal joint representation models ranging in scale from 7billion to 30 billion parameters, which support 3D, audio, image, and languageinputs. Due to the scarcity of data pairs across all modalities, instead oftraining large models from scratch, we propose remapping and binding the spacesof various pre-trained specialist models together. This approach enables"scaling up" by indirectly increasing the model parameters and the amount ofseen data. To effectively integrate various spaces, we dynamically assignweights to different spaces by learning routers with two objectives:cross-modal overall alignment and language representation decoupling. Notably,since binding and routing spaces both only require lightweight networks,OmniBind is extremely training-efficient. Learning the largest 30B modelrequires merely unpaired unimodal data and approximately 3 days on a single8-4090 node. Extensive experiments demonstrate the versatility and superiorityof OmniBind as an omni representation model, highlighting its great potentialfor diverse applications, such as any-query and composable multimodalunderstanding.</description><author>Zehan Wang, Ziang Zhang, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Hengshuang Zhao, Zhou Zhao</author><pubDate>Tue, 16 Jul 2024 16:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11895v1</guid></item><item><title>Deep Learning without Global Optimization by Random Fourier Neural Networks</title><link>http://arxiv.org/abs/2407.11894v1</link><description>We introduce a new training algorithm for variety of deep neural networksthat utilize random complex exponential activation functions. Our approachemploys a Markov Chain Monte Carlo sampling procedure to iteratively trainnetwork layers, avoiding global and gradient-based optimization whilemaintaining error control. It consistently attains the theoreticalapproximation rate for residual networks with complex exponential activationfunctions, determined by network complexity. Additionally, it enables efficientlearning of multiscale and high-frequency features, producing interpretableparameter distributions. Despite using sinusoidal basis functions, we do notobserve Gibbs phenomena in approximating discontinuous target functions.</description><author>Owen Davis, Gianluca Geraci, Mohammad Motamed</author><pubDate>Tue, 16 Jul 2024 16:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11894v1</guid></item><item><title>DepGAN: Leveraging Depth Maps for Handling Occlusions and Transparency in Image Composition</title><link>http://arxiv.org/abs/2407.11890v1</link><description>Image composition is a complex task which requires a lot of information aboutthe scene for an accurate and realistic composition, such as perspective,lighting, shadows, occlusions, and object interactions. Previous methods havepredominantly used 2D information for image composition, neglecting thepotentials of 3D spatial information. In this work, we propose DepGAN, aGenerative Adversarial Network that utilizes depth maps and alpha channels torectify inaccurate occlusions and enhance transparency effects in imagecomposition. Central to our network is a novel loss function called Depth AwareLoss which quantifies the pixel wise depth difference to accurately delineateocclusion boundaries while compositing objects at different depth levels.Furthermore, we enhance our network's learning process by utilizing opacitydata, enabling it to effectively manage compositions involving transparent andsemi-transparent objects. We tested our model against state-of-the-art imagecomposition GANs on benchmark (both real and synthetic) datasets. The resultsreveal that DepGAN significantly outperforms existing methods in terms ofaccuracy of object placement semantics, transparency and occlusion handling,both visually and quantitatively. Our code is available athttps://amrtsg.github.io/DepGAN/.</description><author>Amr Ghoneim, Jiju Poovvancheri, Yasushi Akiyama, Dong Chen</author><pubDate>Tue, 16 Jul 2024 16:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11890v1</guid></item><item><title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</title><link>http://arxiv.org/abs/2404.01318v4</link><description>Jailbreak attacks cause large language models (LLMs) to generate harmful,unethical, or otherwise objectionable content. Evaluating these attackspresents a number of challenges, which the current collection of benchmarks andevaluation techniques do not adequately address. First, there is no clearstandard of practice regarding jailbreaking evaluation. Second, existing workscompute costs and success rates in incomparable ways. And third, numerous worksare not reproducible, as they withhold adversarial prompts, involveclosed-source code, or rely on evolving proprietary APIs. To address thesechallenges, we introduce JailbreakBench, an open-sourced benchmark with thefollowing components: (1) an evolving repository of state-of-the-artadversarial prompts, which we refer to as jailbreak artifacts; (2) ajailbreaking dataset comprising 100 behaviors -- both original and sourced fromprior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align withOpenAI's usage policies; (3) a standardized evaluation framework athttps://github.com/JailbreakBench/jailbreakbench that includes a clearlydefined threat model, system prompts, chat templates, and scoring functions;and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks theperformance of attacks and defenses for various LLMs. We have carefullyconsidered the potential ethical implications of releasing this benchmark, andbelieve that it will be a net positive for the community.</description><author>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong</author><pubDate>Tue, 16 Jul 2024 16:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01318v4</guid></item><item><title>3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules</title><link>http://arxiv.org/abs/2404.05641v3</link><description>We introduce 3D-COCO, an extension of the original MS-COCO dataset providing3D models and 2D-3D alignment annotations. 3D-COCO was designed to achievecomputer vision tasks such as 3D reconstruction or image detection configurablewith textual, 2D image, and 3D CAD model queries. We complete the existingMS-COCO dataset with 28K 3D models collected on ShapeNet and Objaverse. Byusing an IoU-based method, we match each MS-COCO annotation with the best 3Dmodels to provide a 2D-3D alignment. The open-source nature of 3D-COCO is apremiere that should pave the way for new research on 3D-related topics. Thedataset and its source codes is available athttps://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/</description><author>Maxence Bideaux, Alice Phe, Mohamed Chaouch, Bertrand Luvison, Quoc-Cuong Pham</author><pubDate>Tue, 16 Jul 2024 16:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05641v3</guid></item><item><title>Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding</title><link>http://arxiv.org/abs/2402.16844v2</link><description>Large language models (LLMs) have become ubiquitous in practice and arewidely used for generation tasks such as translation, summarization andinstruction following. However, their enormous size and reliance onautoregressive decoding increase deployment costs and complicate their use inlatency-critical applications. In this work, we propose a hybrid approach thatcombines language models of different sizes to increase the efficiency ofautoregressive decoding while maintaining high performance. Our method utilizesa pretrained frozen LLM that encodes all prompt tokens once in parallel, anduses the resulting representations to condition and guide a small languagemodel (SLM), which then generates the response more efficiently. We investigatethe combination of encoder-decoder LLMs with both encoder-decoder anddecoder-only SLMs from different model families and only require fine-tuning ofthe SLM. Experiments with various benchmarks show substantial speedups of up to$4\times$, with minor performance penalties of $1-2\%$ for translation andsummarization tasks compared to the LLM.</description><author>Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi</author><pubDate>Tue, 16 Jul 2024 16:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16844v2</guid></item><item><title>Learning Network Representations with Disentangled Graph Auto-Encoder</title><link>http://arxiv.org/abs/2402.01143v2</link><description>The (variational) graph auto-encoder is widely used to learn representationsfor graph-structured data. However, the formation of real-world graphs is acomplicated and heterogeneous process influenced by latent factors. Existingencoders are fundamentally holistic, neglecting the entanglement of latentfactors. This reduces the effectiveness of graph analysis tasks, while alsomaking it more difficult to explain the learned representations. As a result,learning disentangled graph representations with the (variational) graphauto-encoder poses significant challenges and remains largely unexplored in thecurrent research. In this paper, we introduce the Disentangled GraphAuto-Encoder (DGA) and the Disentangled Variational Graph Auto-Encoder (DVGA)to learn disentangled representations. Specifically, we first design adisentangled graph convolutional network with multi-channel message-passinglayers to serve as the encoder. This allows each channel to aggregateinformation about each latent factor. The disentangled variational graphauto-encoder's expressive capability is then enhanced by applying acomponent-wise flow to each channel. In addition, we construct a factor-wisedecoder that takes into account the characteristics of disentangledrepresentations. We improve the independence of representations by imposingindependence constraints on the mapping channels for distinct latent factors.Empirical experiments on both synthetic and real-world datasets demonstrate thesuperiority of our proposed method compared to several state-of-the-artbaselines.</description><author>Di Fan, Chuanhou Gao</author><pubDate>Tue, 16 Jul 2024 16:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01143v2</guid></item><item><title>Learning Confidence Bounds for Classification with Imbalanced Data</title><link>http://arxiv.org/abs/2407.11878v1</link><description>Class imbalance poses a significant challenge in classification tasks, wheretraditional approaches often lead to biased models and unreliable predictions.Undersampling and oversampling techniques have been commonly employed toaddress this issue, yet they suffer from inherent limitations stemming fromtheir simplistic approach such as loss of information and additional biasesrespectively. In this paper, we propose a novel framework that leverageslearning theory and concentration inequalities to overcome the shortcomings oftraditional solutions. We focus on understanding the uncertainty in aclass-dependent manner, as captured by confidence bounds that we directly embedinto the learning process. By incorporating class-dependent estimates, ourmethod can effectively adapt to the varying degrees of imbalance acrossdifferent classes, resulting in more robust and reliable classificationoutcomes. We empirically show how our framework provides a promising directionfor handling imbalanced data in classification tasks, offering practitioners avaluable tool for building more accurate and trustworthy models.</description><author>Matt Clifford, Jonathan Erskine, Alexander Hepburn, RaÃºl Santos-RodrÃ­guez, Dario Garcia-Garcia</author><pubDate>Tue, 16 Jul 2024 16:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11878v1</guid></item><item><title>Bridging Weighted First Order Model Counting and Graph Polynomials</title><link>http://arxiv.org/abs/2407.11877v1</link><description>The Weighted First-Order Model Counting Problem (WFOMC) asks to compute theweighted sum of models of a given first-order logic sentence over a givendomain. It can be solved in time polynomial in the domain size for sentencesfrom the two-variable fragment with counting quantifiers, known as $C^2$. Thispolynomial-time complexity is also retained when extending $C^2$ by one of thefollowing axioms: linear order axiom, tree axiom, forest axiom, directedacyclic graph axiom or connectedness axiom. An interesting question remains asto which other axioms can be added to the first-order sentences in this way. Weprovide a new perspective on this problem by associating WFOMC with graphpolynomials. Using WFOMC, we define Weak Connectedness Polynomial and StrongConnectedness Polynomials for first-order logic sentences. It turns out thatthese polynomials have the following interesting properties. First, they can becomputed in polynomial time in the domain size for sentences from $C^2$.Second, we can use them to solve WFOMC with all of the existing axioms known tobe tractable as well as with new ones such as bipartiteness, strongconnectedness, being a spanning subgraph, having $k$ connected components, etc.Third, the well-known Tutte polynomial can be recovered as a special case ofthe Weak Connectedness Polynomial, and the Strict and Non-Strict DirectedChromatic Polynomials can be recovered from the Strong ConnectednessPolynomials, which allows us to show that these important graph polynomials canbe computed in time polynomial in the number of vertices for any graph that canbe encoded by a fixed $C^2$ sentence and a conjunction of an arbitrary numberof ground unary literals.</description><author>Qipeng Kuang, OndÅej KuÅ¾elka, Yuanhong Wang, Yuyi Wang</author><pubDate>Tue, 16 Jul 2024 16:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11877v1</guid></item><item><title>Simplifying the Theory on Over-Smoothing</title><link>http://arxiv.org/abs/2407.11876v1</link><description>Graph convolutions have gained popularity due to their ability to efficientlyoperate on data with an irregular geometric structure. However, graphconvolutions cause over-smoothing, which refers to representations becomingmore similar with increased depth. However, many different definitions andintuitions currently coexist, leading to research efforts focusing onincompatible directions. This paper attempts to align these directions byshowing that over-smoothing is merely a special case of power iteration. Thisgreatly simplifies the existing theory on over-smoothing, making it moreaccessible. Based on the theory, we provide a novel comprehensive definition ofrank collapse as a generalized form of over-smoothing and introduce therank-one distance as a corresponding metric. Our empirical evaluation of 14commonly used methods shows that more models than were previously known sufferfrom this issue.</description><author>Andreas Roth</author><pubDate>Tue, 16 Jul 2024 16:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11876v1</guid></item><item><title>Variance Norms for Kernelized Anomaly Detection</title><link>http://arxiv.org/abs/2407.11873v1</link><description>We present a unified theory for Mahalanobis-type anomaly detection on Banachspaces, using ideas from Cameron-Martin theory applied to non-Gaussianmeasures. This approach leads to a basis-free, data-driven notion of anomalydistance through the so-called variance norm of a probability measure, whichcan be consistently estimated using empirical measures. Our frameworkgeneralizes the classical $\mathbb{R}^d$, functional $(L^2[0,1])^d$, andkernelized settings, including the general case of non-injective covarianceoperator. We prove that the variance norm depends solely on the inner productin a given Hilbert space, and hence that the kernelized Mahalanobis distancecan naturally be recovered by working on reproducing kernel Hilbert spaces. Using the variance norm, we introduce the notion of a kernelizednearest-neighbour Mahalanobis distance for semi-supervised anomaly detection.In an empirical study on 12 real-world datasets, we demonstrate that thekernelized nearest-neighbour Mahalanobis distance outperforms the traditionalkernelized Mahalanobis distance for multivariate time series anomaly detection,using state-of-the-art time series kernels such as the signature, globalalignment, and Volterra reservoir kernels. Moreover, we provide an initialtheoretical justification of nearest-neighbour Mahalanobis distances bydeveloping concentration inequalities in the finite-dimensional Gaussian case.</description><author>Thomas Cass, Lukas Gonon, Nikita Zozoulenko</author><pubDate>Tue, 16 Jul 2024 15:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11873v1</guid></item><item><title>LAB-Bench: Measuring Capabilities of Language Models for Biology Research</title><link>http://arxiv.org/abs/2407.10362v2</link><description>There is widespread optimism that frontier Large Language Models (LLMs) andLLM-augmented systems have the potential to rapidly accelerate scientificdiscovery across disciplines. Today, many benchmarks exist to measure LLMknowledge and reasoning on textbook-style science questions, but few if anybenchmarks are designed to evaluate language model performance on practicaltasks required for scientific research, such as literature search, protocolplanning, and data analysis. As a step toward building such benchmarks, weintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset ofover 2,400 multiple choice questions for evaluating AI systems on a range ofpractical biology research capabilities, including recall and reasoning overliterature, interpretation of figures, access and navigation of databases, andcomprehension and manipulation of DNA and protein sequences. Importantly, incontrast to previous scientific benchmarks, we expect that an AI system thatcan achieve consistently high scores on the more difficult LAB-Bench taskswould serve as a useful assistant for researchers in areas such as literaturesearch and molecular cloning. As an initial assessment of the emergentscientific task capabilities of frontier language models, we measureperformance of several against our benchmark and report results compared tohuman expert biology researchers. We will continue to update and expandLAB-Bench over time, and expect it to serve as a useful tool in the developmentof automated research systems going forward. A public subset of LAB-Bench isavailable for use at the following URL:https://huggingface.co/datasets/futurehouse/lab-bench</description><author>Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, Samuel G. Rodriques</author><pubDate>Tue, 16 Jul 2024 15:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10362v2</guid></item><item><title>GVGEN: Text-to-3D Generation with Volumetric Representation</title><link>http://arxiv.org/abs/2403.12957v2</link><description>In recent years, 3D Gaussian splatting has emerged as a powerful techniquefor 3D reconstruction and generation, known for its fast and high-qualityrendering capabilities. To address these shortcomings, this paper introduces anovel diffusion-based framework, GVGEN, designed to efficiently generate 3DGaussian representations from text input. We propose two innovativetechniques:(1) Structured Volumetric Representation. We first arrangedisorganized 3D Gaussian points as a structured form GaussianVolume. Thistransformation allows the capture of intricate texture details within a volumecomposed of a fixed number of Gaussians. To better optimize the representationof these details, we propose a unique pruning and densifying method named theCandidate Pool Strategy, enhancing detail fidelity through selectiveoptimization. (2) Coarse-to-fine Generation Pipeline. To simplify thegeneration of GaussianVolume and empower the model to generate instances withdetailed 3D geometry, we propose a coarse-to-fine pipeline. It initiallyconstructs a basic geometric structure, followed by the prediction of completeGaussian attributes. Our framework, GVGEN, demonstrates superior performance inqualitative and quantitative assessments compared to existing 3D generationmethods. Simultaneously, it maintains a fast generation speed ($\sim$7seconds), effectively striking a balance between quality and efficiency. Ourproject page is: https://gvgen.github.io/</description><author>Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He</author><pubDate>Tue, 16 Jul 2024 15:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12957v2</guid></item><item><title>Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models</title><link>http://arxiv.org/abs/2306.12941v2</link><description>Adversarial robustness has been studied extensively in image classification,especially for the $\ell_\infty$-threat model, but significantly less so forrelated tasks such as object detection and semantic segmentation, where attacksturn out to be a much harder optimization problem than for imageclassification. We propose several problem-specific novel attacks minimizingdifferent metrics in accuracy and mIoU. The ensemble of our attacks, SEA, showsthat existing attacks severely overestimate the robustness of semanticsegmentation models. Surprisingly, existing attempts of adversarial trainingfor semantic segmentation models turn out to be weak or even completelynon-robust. We investigate why previous adaptations of adversarial training tosemantic segmentation failed and show how recently proposed robust ImageNetbackbones can be used to obtain adversarially robust semantic segmentationmodels with up to six times less training time for PASCAL-VOC and the morechallenging ADE20k. The associated code and robust models are available athttps://github.com/nmndeep/robust-segmentation</description><author>Francesco Croce, Naman D Singh, Matthias Hein</author><pubDate>Tue, 16 Jul 2024 15:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12941v2</guid></item><item><title>A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting</title><link>http://arxiv.org/abs/2401.10227v2</link><description>Panoptic and instance segmentation networks are often trained withspecialized object detection modules, complex loss functions, and ad-hocpost-processing steps to manage the permutation-invariance of the instancemasks. This work builds upon Stable Diffusion and proposes a latent diffusionapproach for panoptic segmentation, resulting in a simple architecture thatomits these complexities. Our training consists of two steps: (1) training ashallow autoencoder to project the segmentation masks to latent space; (2)training a diffusion model to allow image-conditioned sampling in latent space.This generative approach unlocks the exploration of mask completion orinpainting. The experimental validation on COCO and ADE20k yields strongsegmentation results. Finally, we demonstrate our model's adaptability tomulti-tasking by introducing learnable task embeddings.</description><author>Wouter Van Gansbeke, Bert De Brabandere</author><pubDate>Tue, 16 Jul 2024 15:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10227v2</guid></item><item><title>SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation</title><link>http://arxiv.org/abs/2404.05680v2</link><description>While recent advances in 3D-aware Generative Adversarial Networks (GANs) haveaided the development of near-frontal view human face synthesis, the challengeof comprehensively synthesizing a full 3D head viewable from all angles stillpersists. Although PanoHead proves the possibilities of using a large-scaledataset with images of both frontal and back views for full-head synthesis, itoften causes artifacts for back views. Based on our in-depth analysis, we foundthe reasons are mainly twofold. First, from network architecture perspective,we found each plane in the utilized tri-plane/tri-grid representation spacetends to confuse the features from both sides, causing "mirroring" artifacts(e.g., the glasses appear in the back). Second, from data supervision aspect,we found that existing discriminator training in 3D GANs mainly focuses on thequality of the rendered image itself, and does not care much about itsplausibility with the perspective from which it was rendered. This makes itpossible to generate "face" in non-frontal views, due to its easiness to foolthe discriminator. In response, we propose SphereHead, a novel tri-planerepresentation in the spherical coordinate system that fits the human head'sgeometric characteristics and efficiently mitigates many of the generatedartifacts. We further introduce a view-image consistency loss for thediscriminator to emphasize the correspondence of the camera parameters and theimages. The combination of these efforts results in visually superior outcomeswith significantly fewer artifacts. Our code and dataset are publicly availableat https://lhyfst.github.io/spherehead.</description><author>Heyuan Li, Ce Chen, Tianhao Shi, Yuda Qiu, Sizhe An, Guanying Chen, Xiaoguang Han</author><pubDate>Tue, 16 Jul 2024 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05680v2</guid></item><item><title>Single Layer Single Gradient Unlearning</title><link>http://arxiv.org/abs/2407.11867v1</link><description>Machine unlearning methods seek to revise pretrained models such that effectsof certain training samples can be removed. In addition to effective erasure,low computational cost and general utility retention are also highly desirable.Existing unlearning methods usually involve iterative updates over the modelparameters, which incurs a high computational cost. In this work, we propose anefficient method that only requires a one-time gradient computation, with whichwe modify only a single layer of model parameters. Specifically, we firstidentify a small number of model layers that lie on the Pareto front of highforget importance and low retain influence as critical layers. Then we searchfor a suitable step size and take a step along the gradient direction of asingle critical layer while keeping other layers frozen. This method is highlymodular and can be used to unlearn multiple concepts simultaneously in acontrollable manner. We demonstrate the effectiveness and efficiency of thismethod on various models including CLIP, stable diffusion, and VLMs, surpassingother state-of-the-art methods.</description><author>Zikui Cai, Yaoteng Tan, M. Salman Asif</author><pubDate>Tue, 16 Jul 2024 15:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11867v1</guid></item><item><title>Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for Binary Images Denoising</title><link>http://arxiv.org/abs/2407.11865v1</link><description>This paper introduces a novel approach to image denoising that leverages theadvantages of Generative Adversarial Networks (GANs). Specifically, we proposea model that combines elements of the Pix2Pix model and the Wasserstein GAN(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks tocapitalize on the denoising capabilities of conditional GANs, as demonstratedin the Pix2Pix model, while mitigating the need for an exhaustive search foroptimal hyperparameters that could potentially ruin the stability of thelearning process. In the proposed method, the GAN's generator is employed toproduce denoised images, harnessing the power of a conditional GAN for noisereduction. Simultaneously, the implementation of the Lipschitz continuityconstraint during updates, as featured in WGAN-GP, aids in reducingsusceptibility to mode collapse. This innovative design allows the proposedmodel to benefit from the strong points of both Pix2Pix and WGAN-GP, generatingsuperior denoising results while ensuring training stability. Drawing onprevious work on image-to-image translation and GAN stabilization techniques,the proposed research highlights the potential of GANs as a general-purposesolution for denoising. The paper details the development and testing of thismodel, showcasing its effectiveness through numerical experiments. The datasetwas created by adding synthetic noise to clean images. Numerical results basedon real-world dataset validation underscore the efficacy of this approach inimage-denoising tasks, exhibiting significant enhancements over traditionaltechniques. Notably, the proposed model demonstrates strong generalizationcapabilities, performing effectively even when trained with synthetic noise.</description><author>Luca Tirel, Ali Mohamed Ali, Hashim A. Hashim</author><pubDate>Tue, 16 Jul 2024 15:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11865v1</guid></item><item><title>A Novel Lexicon for the Moral Foundation of Liberty</title><link>http://arxiv.org/abs/2407.11862v1</link><description>The moral value of liberty is a central concept in our inference system whenit comes to taking a stance towards controversial social issues such as vaccinehesitancy, climate change, or the right to abortion. Here, we propose a novelLiberty lexicon evaluated on more than 3,000 manually annotated data both inin- and out-of-domain scenarios. As a result of this evaluation, we produce acombined lexicon that constitutes the main outcome of this work. This finallexicon incorporates information from an ensemble of lexicons that have beengenerated using word embedding similarity (WE) and compositional semantics(CS). Our key contributions include enriching the liberty annotations,developing a robust liberty lexicon for broader application, and revealing thecomplexity of expressions related to liberty across different platforms.Through the evaluation, we show that the difficulty of the task calls fordesigning approaches that combine knowledge, in an effort of improving therepresentations of learning systems.</description><author>Oscar Araque, Lorenzo Gatti, Sergio Consoli, Kyriaki Kalimeri</author><pubDate>Tue, 16 Jul 2024 15:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11862v1</guid></item><item><title>What Makes a Meme a Meme? Identifying Memes for Memetics-Aware Dataset Creation</title><link>http://arxiv.org/abs/2407.11861v1</link><description>Warning: This paper contains memes that may be offensive to some readers. Multimodal Internet Memes are now a ubiquitous fixture in online discourse.One strand of meme-based research is the classification of memes according tovarious affects, such as sentiment and hate, supported by manually compiledmeme datasets. Understanding the unique characteristics of memes is crucial formeme classification. Unlike other user-generated content, memes spread viamemetics, i.e. the process by which memes are imitated and transformed intosymbols used to create new memes. In effect, there exists an ever-evolving poolof visual and linguistic symbols that underpin meme culture and are crucial tointerpreting the meaning of individual memes. The current approach of trainingsupervised learning models on static datasets, without taking memetics intoaccount, limits the depth and accuracy of meme interpretation. We argue thatmeme datasets must contain genuine memes, as defined via memetics, so thateffective meme classifiers can be built. In this work, we develop a memeidentification protocol which distinguishes meme from non-memetic content byrecognising the memetics within it. We apply our protocol to random samplingsof the leading 7 meme classification datasets and observe that more than half(50. 4\%) of the evaluated samples were found to contain no signs of memetics.Our work also provides a meme typology grounded in memetics, providing thebasis for more effective approaches to the interpretation of memes and thecreation of meme datasets.</description><author>Muzhaffar Hazman, Susan McKeever, Josephine Griffith</author><pubDate>Tue, 16 Jul 2024 15:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11861v1</guid></item><item><title>Towards a Benchmark for Causal Business Process Reasoning with LLMs</title><link>http://arxiv.org/abs/2406.05506v2</link><description>Large Language Models (LLMs) are increasingly used for boostingorganizational efficiency and automating tasks. While not originally designedfor complex cognitive processes, recent efforts have further extended to employLLMs in activities such as reasoning, planning, and decision-making. Inbusiness processes, such abilities could be invaluable for leveraging on themassive corpora LLMs have been trained on for gaining deep understanding ofsuch processes. In this work, we plant the seeds for the development of abenchmark to assess the ability of LLMs to reason about causal and processperspectives of business operations. We refer to this view asCausally-augmented Business Processes (BP^C). The core of the benchmarkcomprises a set of BP^C related situations, a set of questions about thesesituations, and a set of deductive rules employed to systematically resolve theground truth answers to these questions. Also with the power of LLMs, the seedis then instantiated into a larger-scale set of domain-specific situations andquestions. Reasoning on BP^C is of crucial importance for process interventionsand process improvement. Our benchmark, accessible athttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possiblemodalities: testing the performance of any target LLM and training an LLM toadvance its capability to reason about BP^C.</description><author>Fabiana Fournier, Lior Limonad, Inna Skarbovsky</author><pubDate>Tue, 16 Jul 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05506v2</guid></item><item><title>Sociotechnical Implications of Generative Artificial Intelligence for Information Access</title><link>http://arxiv.org/abs/2405.11612v2</link><description>Robust access to trustworthy information is a critical need for society withimplications for knowledge production, public health education, and promotinginformed citizenry in democratic societies. Generative AI technologies mayenable new ways to access information and improve effectiveness of existinginformation retrieval systems but we are only starting to understand andgrapple with their long-term social implications. In this chapter, we presentan overview of some of the systemic consequences and risks of employinggenerative AI in the context of information access. We also providerecommendations for evaluation and mitigation, and discuss challenges forfuture research.</description><author>Bhaskar Mitra, Henriette Cramer, Olya Gurevich</author><pubDate>Tue, 16 Jul 2024 15:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11612v2</guid></item><item><title>Mitigating Background Shift in Class-Incremental Semantic Segmentation</title><link>http://arxiv.org/abs/2407.11859v1</link><description>Class-Incremental Semantic Segmentation(CISS) aims to learn new classeswithout forgetting the old ones, using only the labels of the new classes. Toachieve this, two popular strategies are employed: 1) pseudo-labeling andknowledge distillation to preserve prior knowledge; and 2) background weighttransfer, which leverages the broad coverage of background in learning newclasses by transferring background weight to the new class classifier. However,the first strategy heavily relies on the old model in detecting old classeswhile undetected pixels are regarded as the background, thereby leading to thebackground shift towards the old classes(i.e., misclassification of old classas background). Additionally, in the case of the second approach, initializingthe new class classifier with background knowledge triggers a similarbackground shift issue, but towards the new classes. To address these issues,we propose a background-class separation framework for CISS. To begin with,selective pseudo-labeling and adaptive feature distillation are to distill onlytrustworthy past knowledge. On the other hand, we encourage the separationbetween the background and new classes with a novel orthogonal objective alongwith label-guided output distillation. Our state-of-the-art results validatethe effectiveness of these proposed methods.</description><author>Gilhan Park, WonJun Moon, SuBeen Lee, Tae-Young Kim, Jae-Pil Heo</author><pubDate>Tue, 16 Jul 2024 15:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11859v1</guid></item><item><title>Characteristic Learning for Provable One Step Generation</title><link>http://arxiv.org/abs/2405.05512v4</link><description>We propose the characteristic generator, a novel one-step generative modelthat combines the efficiency of sampling in Generative Adversarial Networks(GANs) with the stable performance of flow-based models. Our model is driven bycharacteristics, along which the probability density transport can be describedby ordinary differential equations (ODEs). Specifically, We estimate thevelocity field through nonparametric regression and utilize Euler method tosolve the probability flow ODE, generating a series of discrete approximationsto the characteristics. We then use a deep neural network to fit thesecharacteristics, ensuring a one-step mapping that effectively pushes the priordistribution towards the target distribution. In the theoretical aspect, weanalyze the errors in velocity matching, Euler discretization, andcharacteristic fitting to establish a non-asymptotic convergence rate for thecharacteristic generator in 2-Wasserstein distance. To the best of ourknowledge, this is the first thorough analysis for simulation-free one stepgenerative models. Additionally, our analysis refines the error analysis offlow-based generative models in prior works. We apply our method on bothsynthetic and real datasets, and the results demonstrate that thecharacteristic generator achieves high generation quality with just a singleevaluation of neural network.</description><author>Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang</author><pubDate>Tue, 16 Jul 2024 15:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05512v4</guid></item><item><title>Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction</title><link>http://arxiv.org/abs/2407.11857v1</link><description>Task-oriented dialogues must maintain consistency both within the dialogueitself, ensuring logical coherence across turns, and with the conversationaldomain, accurately reflecting external knowledge. We propose to conceptualizedialogue consistency as a Constraint Satisfaction Problem (CSP), whereinvariables represent segments of the dialogue referencing the conversationaldomain, and constraints among variables reflect dialogue properties, includinglinguistic, conversational, and domain-based aspects. To demonstrate thefeasibility of the approach, we utilize a CSP solver to detect inconsistenciesin dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP iseffective to detect dialogue inconsistencies; and (ii) consistent dialoguere-lexicalization is challenging for state-of-the-art LLMs, achieving only a0.15 accuracy rate when compared to a CSP solver. Furthermore, through anablation study, we reveal that constraints derived from domain knowledge posethe greatest difficulty in being respected. We argue that CSP captures coreproperties of dialogue consistency that have been poorly considered byapproaches based on component pipelines.</description><author>Tiziano Labruna, Bernardo Magnini</author><pubDate>Tue, 16 Jul 2024 15:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11857v1</guid></item><item><title>Scaling Sign Language Translation</title><link>http://arxiv.org/abs/2407.11855v1</link><description>Sign language translation (SLT) addresses the problem of translatinginformation from a sign language in video to a spoken language in text.Existing studies, while showing progress, are often limited to narrow domainsand/or few sign languages and struggle with open-domain tasks. In this paper,we push forward the frontier of SLT by scaling pretraining data, model size,and number of translation directions. We perform large-scale SLT pretraining ondifferent data including 1) noisy multilingual YouTube SLT data, 2) paralleltext corpora, and 3) SLT data augmented by translating video captions to otherlanguages with off-the-shelf machine translation models. We unify differentpretraining tasks with task-specific prompts under the encoder-decoderarchitecture, and initialize the SLT model with pretrained (m/By)T5 modelsacross model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASLto 42 spoken languages) demonstrate the significance of data/model scaling andcross-lingual cross-modal transfer, as well as the feasibility of zero-shotSLT. We finetune the pretrained SLT models on 5 downstream open-domain SLTbenchmarks covering 5 sign languages. Experiments show substantial qualityimprovements over the vanilla baselines, surpassing the previousstate-of-the-art (SOTA) by wide margins.</description><author>Biao Zhang, Garrett Tanzer, Orhan Firat</author><pubDate>Tue, 16 Jul 2024 15:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11855v1</guid></item><item><title>Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection</title><link>http://arxiv.org/abs/2407.11854v1</link><description>Grammatical Error Detection (GED) methods rely heavily on human annotatederror corpora. However, these annotations are unavailable in many low-resourcelanguages. In this paper, we investigate GED in this context. Leveraging thezero-shot cross-lingual transfer capabilities of multilingual pre-trainedlanguage models, we train a model using data from a diverse set of languages togenerate synthetic errors in other languages. These synthetic error corpora arethen used to train a GED model. Specifically we propose a two-stage fine-tuningpipeline where the GED model is first fine-tuned on multilingual synthetic datafrom target languages followed by fine-tuning on human-annotated GED corporafrom source languages. This approach outperforms current state-of-the-artannotation-free GED methods. We also analyse the errors produced by our methodand other strong baselines, finding that our approach produces errors that aremore diverse and more similar to human errors.</description><author>Gaetan Lopez Latouche, Marc-AndrÃ© Carbonneau, Ben Swanson</author><pubDate>Tue, 16 Jul 2024 15:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11854v1</guid></item><item><title>Schema Matching with Large Language Models: an Experimental Study</title><link>http://arxiv.org/abs/2407.11852v1</link><description>Large Language Models (LLMs) have shown useful applications in a variety oftasks, including data wrangling. In this paper, we investigate the use of anoff-the-shelf LLM for schema matching. Our objective is to identify semanticcorrespondences between elements of two relational schemas using only names anddescriptions. Using a newly created benchmark from the health domain, wepropose different so-called task scopes. These are methods for prompting theLLM to do schema matching, which vary in the amount of context informationcontained in the prompt. Using these task scopes we compare LLM-based schemamatching against a string similarity baseline, investigating matching quality,verification effort, decisiveness, and complementarity of the approaches. Wefind that matching quality suffers from a lack of context information, but alsofrom providing too much context information. In general, using newer LLMversions increases decisiveness. We identify task scopes that have acceptableverification effort and succeed in identifying a significant number of truesemantic matches. Our study shows that LLMs have potential in bootstrapping theschema matching process and are able to assist data engineers in speeding upthis task solely based on schema element names and descriptions without theneed for data instances.</description><author>Marcel Parciak, Brecht Vandevoort, Frank Neven, Liesbet M. Peeters, Stijn Vansummeren</author><pubDate>Tue, 16 Jul 2024 15:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11852v1</guid></item><item><title>SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint Alignment of Images</title><link>http://arxiv.org/abs/2407.11850v1</link><description>The unsupervised task of Joint Alignment (JA) of images is beset bychallenges such as high complexity, geometric distortions, and convergence topoor local or even global optima. Although Vision Transformers (ViT) haverecently provided valuable features for JA, they fall short of fully addressingthese issues. Consequently, researchers frequently depend on expensive modelsand numerous regularization terms, resulting in long training times andchallenging hyperparameter tuning. We introduce the Spatial Joint AlignmentModel (SpaceJAM), a novel approach that addresses the JA task with efficiencyand simplicity. SpaceJAM leverages a compact architecture with only 16Ktrainable parameters and uniquely operates without the need for regularizationor atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstratethat SpaceJAM matches the alignment capabilities of existing methods whilesignificantly reducing computational demands and achieving at least a 10xspeedup. SpaceJAM sets a new standard for rapid and effective image alignment,making the process more accessible and efficient. Our code is available at:https://bgu-cs-vil.github.io/SpaceJAM/.</description><author>Nir Barel, Ron Shapira Weber, Nir Mualem, Shahaf E. Finder, Oren Freifeld</author><pubDate>Tue, 16 Jul 2024 15:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11850v1</guid></item><item><title>Variational Randomized Smoothing for Sample-Wise Adversarial Robustness</title><link>http://arxiv.org/abs/2407.11844v1</link><description>Randomized smoothing is a defensive technique to achieve enhanced robustnessagainst adversarial examples which are small input perturbations that degradethe performance of neural network models. Conventional randomized smoothingadds random noise with a fixed noise level for every input sample to smooth outadversarial perturbations. This paper proposes a new variational framework thatuses a per-sample noise level suitable for each input by introducing a noiselevel selector. Our experimental results demonstrate enhancement of empiricalrobustness against adversarial attacks. We also provide and analyze thecertified robustness for our sample-wise smoothing method.</description><author>Ryo Hase, Ye Wang, Toshiaki Koike-Akino, Jing Liu, Kieran Parsons</author><pubDate>Tue, 16 Jul 2024 15:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11844v1</guid></item><item><title>InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback</title><link>http://arxiv.org/abs/2407.11843v1</link><description>A crucial requirement for deploying LLM-based agents in real-lifeapplications is robustness against risky or irreversible mistakes. However,existing research lacks a focus on the preemptive evaluation of reasoningtrajectories performed by LLM agents, leading to a gap in ensuring safe andreliable operations. To explore better solutions, this paper introducesInferAct, a novel approach that leverages the Theory-of-Mind capability of LLMsto proactively detect potential errors before critical actions are executed(e.g., "buy-now" in automatic online trading or web shopping). InferAct is alsocapable of integrating human feedback to prevent irreversible risks and enhancethe actor agent's decision-making process. Experiments on three widely usedtasks demonstrate the effectiveness of InferAct. The proposed solution presentsa novel approach and concrete contributions toward developing LLM agents thatcan be safely deployed in different environments involving criticaldecision-making.</description><author>Haishuo Fang, Xiaodan Zhu, Iryna Gurevych</author><pubDate>Tue, 16 Jul 2024 15:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11843v1</guid></item><item><title>MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive Quantile-Based Geometric Consistency Densification</title><link>http://arxiv.org/abs/2407.11840v1</link><description>In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting(3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements.Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels toeffectively enhance mesh extraction quality, this compression can potentiallylead to a decrease in rendering quality. Additionally, unreliable densificationprocesses and the calculation of depth through the accumulation of opacity cancompromise the detail of mesh extraction. To address this issue, we introduceMVG-Splatting, a solution guided by Multi-View considerations. Specifically, weintegrate an optimized method for calculating normals, which, combined withimage gradients, helps rectify inconsistencies in the original depthcomputations. Additionally, utilizing projection strategies akin to those inMulti-View Stereo (MVS), we propose an adaptive quantile-based method thatdynamically determines the level of additional densification guided by depthmaps, from coarse to fine detail. Experimental evidence demonstrates that ourmethod not only resolves the issues of rendering quality degradation caused bydepth discrepancies but also facilitates direct mesh extraction from denseGaussian point clouds using the Marching Cubes algorithm. This approachsignificantly enhances the overall fidelity and accuracy of the 3Dreconstruction process, ensuring that both the geometric details and visualquality.</description><author>Zhuoxiao Li, Shanliang Yao, Yijie Chu, Angel F. Garcia-Fernandez, Yong Yue, Eng Gee Lim, Xiaohui Zhu</author><pubDate>Tue, 16 Jul 2024 15:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11840v1</guid></item><item><title>LoFTI: Localization and Factuality Transfer to Indian Locales</title><link>http://arxiv.org/abs/2407.11833v1</link><description>Large language models (LLMs) encode vast amounts of world knowledge acquiredvia training on large web-scale datasets crawled from the internet. However,these datasets typically exhibit a geographical bias towards English-speakingWestern countries. This results in LLMs producing biased or hallucinatedresponses to queries that require answers localized to other geographicalregions. In this work, we introduce a new benchmark named LoFTI (Localizationand Factuality Transfer to Indian Locales) that can be used to evaluate anLLM's localization and factual text transfer capabilities. LoFTI consists offactual statements about entities in source and target locations; the sourcelocations are spread across the globe and the target locations are all withinIndia with varying degrees of hyperlocality (country, states, cities). Theentities span a wide variety of categories. We use LoFTI to evaluate Mixtral,GPT-4 and two other Mixtral-based approaches well-suited to the task oflocalized factual transfer. We demonstrate that LoFTI is a high-qualityevaluation benchmark and all the models, including GPT-4, produce skewedresults across varying levels of hyperlocality.</description><author>Sona Elza Simon, Soumen Kumar Mondal, Abhishek Singhania, Sayambhu Sen, Preethi Jyothi</author><pubDate>Tue, 16 Jul 2024 15:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11833v1</guid></item><item><title>Approximating the Number of Relevant Variables in a Parity Implies Proper Learning</title><link>http://arxiv.org/abs/2407.11832v1</link><description>Consider the model where we can access a parity function through randomuniform labeled examples in the presence of random classification noise. Inthis paper, we show that approximating the number of relevant variables in theparity function is as hard as properly learning parities. More specifically, let $\gamma:{\mathbb R}^+\to {\mathbb R}^+$, where$\gamma(x) \ge x$, be any strictly increasing function. In our first result, weshow that from any polynomial-time algorithm that returns a$\gamma$-approximation, $D$ (i.e., $\gamma^{-1}(d(f)) \leq D \leq\gamma(d(f))$), of the number of relevant variables~$d(f)$ for any parity $f$,we can, in polynomial time, construct a solution to the long-standing openproblem of polynomial-time learning $k(n)$-sparse parities (parities with$k(n)\le n$ relevant variables), where $k(n) = \omega_n(1)$. In our second result, we show that from any $T(n)$-time algorithm that, forany parity $f$, returns a $\gamma$-approximation of the number of relevantvariables $d(f)$ of $f$, we can, in polynomial time, construct a$poly(\Gamma(n))T(\Gamma(n)^2)$-time algorithm that properly learns parities,where $\Gamma(x)=\gamma(\gamma(x))$. If $T(\Gamma(n)^2)=\exp({o(n/\log n)})$, this would resolve anotherlong-standing open problem of properly learning parities in the presence ofrandom classification noise in time $\exp({o(n/\log n)})$.</description><author>Nader H. Bshouty, George Haddad</author><pubDate>Tue, 16 Jul 2024 15:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11832v1</guid></item><item><title>UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential Recommendations</title><link>http://arxiv.org/abs/2406.18470v3</link><description>Representation learning in sequential recommendation is critical foraccurately modeling user interaction patterns and improving recommendationprecision. However, existing approaches predominantly emphasize item-to-itemtransitions, often neglecting the time intervals between interactions, whichare closely related to behavior pattern changes. Additionally, broaderinteraction attributes, such as item frequency, are frequently overlooked. Wefound that both sequences with more uniform time intervals and items withhigher frequency yield better prediction performance. Conversely, non-uniformsequences exacerbate user interest drift and less-frequent items are difficultto model due to sparse sampling, presenting unique challenges inadequatelyaddressed by current methods. In this paper, we propose UniRec, a novelbidirectional enhancement sequential recommendation method. UniRec leveragessequence uniformity and item frequency to enhance performance, particularlyimproving the representation of non-uniform sequences and less-frequent items.These two branches mutually reinforce each other, driving comprehensiveperformance optimization in complex sequential recommendation scenarios.Additionally, we present a multidimensional time module to further enhanceadaptability. To the best of our knowledge, UniRec is the first method toutilize the characteristics of uniformity and frequency for featureaugmentation. Comparing with eleven advanced models across four datasets, wedemonstrate that UniRec outperforms SOTA models significantly. The code isavailable at https://github.com/Linxi000/UniRec.</description><author>Yang Liu, Yitong Wang, Chenyue Feng</author><pubDate>Tue, 16 Jul 2024 15:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18470v3</guid></item><item><title>Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games</title><link>http://arxiv.org/abs/2407.04467v2</link><description>Large Language Models (LLMs) have been increasingly used in real-worldsettings, yet their strategic abilities remain largely unexplored. Game theoryprovides a good framework for assessing the decision-making abilities of LLMsin interactions with other agents. Although prior studies have shown that LLMscan solve these tasks with carefully curated prompts, they fail when theproblem setting or prompt changes. In this work we investigate LLMs' behaviourin strategic games, Stag Hunt and Prisoner Dilemma, analyzing performancevariations under different settings and prompts. Our results show that thetested state-of-the-art LLMs exhibit at least one of the following systematicbiases: (1) positional bias, (2) payoff bias, or (3) behavioural bias.Subsequently, we observed that the LLMs' performance drops when the gameconfiguration is misaligned with the affecting biases. Performance is assessedbased on the selection of the correct action, one which agrees with theprompted preferred behaviours of both players. Alignment refers to whether theLLM's bias aligns with the correct action. For example, GPT-4o's averageperformance drops by 34% when misaligned. Additionally, the current trend of"bigger and newer is better" does not hold for the above, where GPT-4o (thecurrent best-performing LLM) suffers the most substantial performance drop.Lastly, we note that while chain-of-thought prompting does reduce the effect ofthe biases on most models, it is far from solving the problem at thefundamental level.</description><author>Nathan Herr, Fernando Acero, Roberta Raileanu, MarÃ­a PÃ©rez-Ortiz, Zhibin Li</author><pubDate>Tue, 16 Jul 2024 15:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04467v2</guid></item><item><title>Personalized Conversational Travel Assistant powered by Generative AI</title><link>http://arxiv.org/abs/2407.11830v1</link><description>The Tourism and Destination Management Organization (DMO) industry is rapidlyevolving to adapt to new technologies and traveler expectations. GenerativeArtificial Intelligence (AI) offers an astonishing and innovative opportunityto enhance the tourism experience by providing personalized, interactive andengaging assistance. In this article, we propose a generative AI-based chatbotfor tourism assistance. The chatbot leverages AI ability to generate realisticand creative texts, adopting the friendly persona of the well-known Italianall-knowledgeable aunties, to provide tourists with personalized information,tailored and dynamic pre, during and post recommendations and trip plans andpersonalized itineraries, using both text and voice commands, and supportingdifferent languages to satisfy Italian and foreign tourists expectations. Thiswork is under development in the Molise CTE research project, funded by theItalian Minister of the Economic Growth (MIMIT), with the aim to leverage thebest emerging technologies available, such as Cloud and AI to produce state ofthe art solutions in the Smart City environment.</description><author>Alexio Cassani, Michele Ruberl, Antonio Salis, Giacomo Giannese, Gianluca Boanelli</author><pubDate>Tue, 16 Jul 2024 15:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11830v1</guid></item><item><title>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors</title><link>http://arxiv.org/abs/2407.11828v1</link><description>Vibravox is a dataset compliant with the General Data Protection Regulation(GDPR) containing audio recordings using five different body-conduction audiosensors : two in-ear microphones, two bone conduction vibration pickups and alaryngophone. The data set also includes audio data from an airborne microphoneused as a reference. The Vibravox corpus contains 38 hours of speech samplesand physiological sounds recorded by 188 participants under different acousticconditions imposed by an high order ambisonics 3D spatializer. Annotationsabout the recording conditions and linguistic transcriptions are also includedin the corpus. We conducted a series of experiments on various speech-relatedtasks, including speech recognition, speech enhancement and speakerverification. These experiments were carried out using state-of-the-art modelsto evaluate and compare their performances on signals captured by the differentaudio sensors offered by the Vibravox dataset, with the aim of gaining a bettergrasp of their individual characteristics.</description><author>Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah PoirÃ©e, VÃ©ronique Zimpfer, Ãric Bavu</author><pubDate>Tue, 16 Jul 2024 15:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11828v1</guid></item><item><title>GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text</title><link>http://arxiv.org/abs/2407.11827v1</link><description>While the use of machine learning for the detection of propaganda techniquesin text has garnered considerable attention, most approaches focus on"black-box" solutions with opaque inner workings. Interpretable approachesprovide a solution, however, they depend on careful feature engineering andcostly expert annotated data. Additionally, language features specific topropagandistic text are generally the focus of rhetoricians or linguists, andthere is no data set labeled with such features suitable for machine learning.This study codifies 22 rhetorical and linguistic features identified inliterature related to the language of persuasion for the purpose of annotatingan existing data set labeled with propaganda techniques. To help human expertsannotate natural language sentences with these features, RhetAnn, a webapplication, was specifically designed to minimize an otherwise considerablemental effort. Finally, a small set of annotated data was used to fine-tuneGPT-3.5, a generative large language model (LLM), to annotate the remainingdata while optimizing for financial cost and classification accuracy. Thisstudy demonstrates how combining a small number of human annotated exampleswith GPT can be an effective strategy for scaling the annotation process at afraction of the cost of traditional annotation relying solely on human experts.The results are on par with the best performing model at the time of writing,namely GPT-4, at 10x less the cost. Our contribution is a set of features,their properties, definitions, and examples in a machine-readable format, alongwith the code for RhetAnn and the GPT prompts and fine-tuning procedures foradvancing state-of-the-art interpretable propaganda technique detection.</description><author>Kyle Hamilton, Luca Longo, Bojan Bozic</author><pubDate>Tue, 16 Jul 2024 15:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11827v1</guid></item></channel></rss>