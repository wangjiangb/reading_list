<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 05 Dec 2024 01:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Motion Prompting: Controlling Video Generation with Motion Trajectories</title><link>http://arxiv.org/abs/2412.02700v1</link><description>Motion control is crucial for generating expressive and compelling videocontent; however, most existing video generation models rely mainly on textprompts for control, which struggle to capture the nuances of dynamic actionsand temporal compositions. To this end, we train a video generation modelconditioned on spatio-temporally sparse or dense motion trajectories. Incontrast to prior motion conditioning work, this flexible representation canencode any number of trajectories, object-specific or global scene motion, andtemporally sparse motion; due to its flexibility we refer to this conditioningas motion prompts. While users may directly specify sparse trajectories, wealso show how to translate high-level user requests into detailed, semi-densemotion prompts, a process we term motion prompt expansion. We demonstrate theversatility of our approach through various applications, including camera andobject motion control, "interacting" with an image, motion transfer, and imageediting. Our results showcase emergent behaviors, such as realistic physics,suggesting the potential of motion prompts for probing video models andinteracting with future generative world models. Finally, we evaluatequantitatively, conduct a human study, and demonstrate strong performance.Video results are available on our webpage: https://motion-prompting.github.io/</description><author>Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun</author><pubDate>Tue, 03 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02700v1</guid></item><item><title>Scaling BERT Models for Turkish Automatic Punctuation and Capitalization Correction</title><link>http://arxiv.org/abs/2412.02698v1</link><description>This paper investigates the effectiveness of BERT based models for automatedpunctuation and capitalization corrections in Turkish texts across fivedistinct model sizes. The models are designated as Tiny, Mini, Small, Medium,and Base. The design and capabilities of each model are tailored to address thespecific challenges of the Turkish language, with a focus on optimizingperformance while minimizing computational overhead. The study presents asystematic comparison of the performance metrics precision, recall, and F1score of each model, offering insights into their applicability in diverseoperational contexts. The results demonstrate a significant improvement in textreadability and accuracy as model size increases, with the Base model achievingthe highest correction precision. This research provides a comprehensive guidefor selecting the appropriate model size based on specific user needs andcomputational resources, establishing a framework for deploying these models inreal-world applications to enhance the quality of written Turkish.</description><author>Abdulkader Saoud, Mahmut Alomeyr, Himmet Toprak Kesgin, Mehmet Fatih Amasyali</author><pubDate>Tue, 03 Dec 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02698v1</guid></item><item><title>An ADHD Diagnostic Interface Based on EEG Spectrograms and Deep Learning Techniques</title><link>http://arxiv.org/abs/2412.02695v1</link><description>This paper introduces an innovative approach toAttention-deficit/hyperactivity disorder (ADHD) diagnosis by employing deeplearning (DL) techniques on electroencephalography (EEG) signals. This methodaddresses the limitations of current behavior-based diagnostic methods, whichoften lead to misdiagnosis and gender bias. By utilizing a publicly availableEEG dataset and converting the signals into spectrograms, a Resnet-18convolutional neural network (CNN) architecture was used to extract featuresfor ADHD classification. The model achieved a high precision, recall, and anoverall F1 score of 0.9. Feature extraction highlighted significant brainregions (frontopolar, parietal, and occipital lobes) associated with ADHD.These insights guided the creation of a three-part digital diagnostic system,facilitating cost-effective and accessible ADHD screening, especially in schoolenvironments. This system enables earlier and more accurate identification ofstudents at risk for ADHD, providing timely support to enhance theirdevelopmental outcomes. This study showcases the potential of integrating EEGanalysis with DL to enhance ADHD diagnostics, presenting a viable alternativeto traditional methods.</description><author>Medha Pappula, Syed Muhammad Anwar</author><pubDate>Tue, 03 Dec 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02695v1</guid></item><item><title>Diffusion-based Visual Anagram as Multi-task Learning</title><link>http://arxiv.org/abs/2412.02693v1</link><description>Visual anagrams are images that change appearance upon transformation, likeflipping or rotation. With the advent of diffusion models, generating suchoptical illusions can be achieved by averaging noise across multiple viewsduring the reverse denoising process. However, we observe two critical failuremodes in this approach: (i) concept segregation, where concepts in differentviews are independently generated, which can not be considered a true anagram,and (ii) concept domination, where certain concepts overpower others. In thiswork, we cast the visual anagram generation problem in a multi-task learningsetting, where different viewpoint prompts are analogous to different tasks,andderive denoising trajectories that align well across tasks simultaneously. Atthe core of our designed framework are two newly introduced techniques, where(i) an anti-segregation optimization strategy that promotes overlap incross-attention maps between different concepts, and (ii) a noise vectorbalancing method that adaptively adjusts the influence of different tasks.Additionally, we observe that directly averaging noise predictions yieldssuboptimal performance because statistical properties may not be preserved,prompting us to derive a noise variance rectification method. Extensivequalitative and quantitative experiments demonstrate our method's superiorability to generate visual anagrams spanning diverse concepts.</description><author>Zhiyuan Xu, Yinhe Chen, Huan-ang Gao, Weiyan Zhao, Guiyu Zhang, Hao Zhao</author><pubDate>Tue, 03 Dec 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02693v1</guid></item><item><title>Taming Scalable Visual Tokenizer for Autoregressive Image Generation</title><link>http://arxiv.org/abs/2412.02692v1</link><description>Existing vector quantization (VQ) methods struggle with scalability, largelyattributed to the instability of the codebook that undergoes partial updatesduring training. The codebook is prone to collapse as utilization decreases,due to the progressively widening distribution gap between non-activated codesand visual features. To solve the problem, we propose Index BackpropagationQuantization (IBQ), a new VQ method for the joint optimization of all codebookembeddings and the visual encoder. Applying a straight-through estimator on theone-hot categorical distribution between the encoded feature and codebook, allcodes are differentiable and maintain a consistent latent space with the visualencoder. IBQ enables scalable training of visual tokenizers and, for the firsttime, achieves a large-scale codebook ($2^{18}$) with high dimension ($256$)and high utilization. Experiments on the standard ImageNet benchmarkdemonstrate the scalability and superiority of IBQ, achieving competitiveresults on both reconstruction ($1.00$ rFID) and autoregressive visualgeneration ($2.05$ gFID). The code and models are available athttps://github.com/TencentARC/SEED-Voken.</description><author>Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, Limin Wang</author><pubDate>Tue, 03 Dec 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02692v1</guid></item><item><title>Towards Neuro-Symbolic Video Understanding</title><link>http://arxiv.org/abs/2403.11021v3</link><description>The unprecedented surge in video data production in recent years necessitatesefficient tools to extract meaningful frames from videos for downstream tasks.Long-term temporal reasoning is a key desideratum for frame retrieval systems.While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, areproficient in short-term semantic understanding, they surprisingly fail atlong-term reasoning across frames. A key reason for this failure is that theyintertwine per-frame perception and temporal reasoning into a single deepnetwork. Hence, decoupling but co-designing semantic understanding and temporalreasoning is essential for efficient scene identification. We propose a systemthat leverages vision-language models for semantic understanding of individualframes but effectively reasons about the long-term evolution of events usingstate machines and temporal logic (TL) formulae that inherently capture memory.Our TL-based reasoning improves the F1 score of complex event identification by9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-artself-driving datasets such as Waymo and NuScenes.</description><author>Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah, Sandeep Chinchali</author><pubDate>Tue, 03 Dec 2024 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11021v3</guid></item><item><title>FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation</title><link>http://arxiv.org/abs/2412.02690v1</link><description>Despite remarkable progress in image generation models, generating realistichands remains a persistent challenge due to their complex articulation, varyingviewpoints, and frequent occlusions. We present FoundHand, a large-scaledomain-specific diffusion model for synthesizing single and dual hand images.To train our model, we introduce FoundHand-10M, a large-scale hand dataset with2D keypoints and segmentation mask annotations. Our insight is to use 2D handkeypoints as a universal representation that encodes both hand articulation andcamera viewpoint. FoundHand learns from image pairs to capture physicallyplausible hand articulations, natively enables precise control through 2Dkeypoints, and supports appearance control. Our model exhibits corecapabilities that include the ability to repose hands, transfer handappearance, and even synthesize novel views. This leads to zero-shotcapabilities for fixing malformed hands in previously generated images, orsynthesizing hand video sequences. We present extensive experiments andevaluations that demonstrate state-of-the-art performance of our method.</description><author>Kefan Chen, Chaerin Min, Linguang Zhang, Shreyas Hampali, Cem Keskin, Srinath Sridhar</author><pubDate>Tue, 03 Dec 2024 18:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02690v1</guid></item><item><title>Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification</title><link>http://arxiv.org/abs/2411.16718v3</link><description>Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,and CogVideoX are pushing the boundaries of synthetic video generation, withadoption seen in fields like robotics, autonomous driving, and entertainment.As these models become prevalent, various metrics and benchmarks have emergedto evaluate the quality of the generated videos. However, these metricsemphasize visual quality and smoothness, neglecting temporal fidelity andtext-to-video alignment, which are crucial for safety-critical applications. Toaddress this gap, we introduce NeuS-V, a novel synthetic video evaluationmetric that rigorously assesses text-to-video alignment using neuro-symbolicformal verification techniques. Our approach first converts the prompt into aformally defined Temporal Logic (TL) specification and translates the generatedvideo into an automaton representation. Then, it evaluates the text-to-videoalignment by formally checking the video automaton against the TLspecification. Furthermore, we present a dataset of temporally extended promptsto evaluate state-of-the-art video generation models against our benchmark. Wefind that NeuS-V demonstrates a higher correlation by over 5x with humanevaluations when compared to existing metrics. Our evaluation further revealsthat current video generation models perform poorly on these temporally complexprompts, highlighting the need for future work in improving text-to-videogeneration capabilities.</description><author>S. P. Sharan, Minkyu Choi, Sahil Shah, Harsh Goel, Mohammad Omama, Sandeep Chinchali</author><pubDate>Tue, 03 Dec 2024 18:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16718v3</guid></item><item><title>SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance</title><link>http://arxiv.org/abs/2412.02687v1</link><description>Recent approaches have yielded promising results in distilling multi-steptext-to-image diffusion models into one-step ones. The state-of-the-artefficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses theteacher model's performance with limited resources. However, our study revealsits instability when handling different diffusion model backbones due to usinga fixed guidance scale within the Variational Score Distillation (VSD) loss.Another weakness of the existing one-step diffusion models is the missingsupport for negative prompt guidance, which is crucial in practical imagegeneration. This paper presents SNOOPI, a novel framework designed to addressthese limitations by enhancing the guidance in one-step diffusion models duringboth training and inference. First, we effectively enhance training stabilitythrough Proper Guidance-SwiftBrush (PG-SB), which employs a random-scaleclassifier-free guidance approach. By varying the guidance scale of bothteacher models, we broaden their output distributions, resulting in a morerobust VSD loss that enables SB to perform effectively across diverse backboneswhile maintaining competitive performance. Second, we propose a training-freemethod called Negative-Away Steer Attention (NASA), which integrates negativeprompts into one-step diffusion models via cross-attention to suppressundesired elements in generated images. Our experimental results show that ourproposed methods significantly improve baseline models across various metrics.Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-artbenchmark for one-step diffusion models.</description><author>Viet Nguyen, Anh Aengus Nguyen, Trung Dao, Khoi Nguyen, Cuong Pham, Toan Tran, Anh Tran</author><pubDate>Tue, 03 Dec 2024 18:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02687v1</guid></item><item><title>T-REG: Preference Optimization with Token-Level Reward Regularization</title><link>http://arxiv.org/abs/2412.02685v1</link><description>Reinforcement learning from human feedback (RLHF) has been crucial inaligning large language models (LLMs) with human values. Traditionally, RLHFinvolves generating responses to a query and using a reward model to assign areward to the entire response. However, this approach faces challenges due toits reliance on a single, sparse reward, which makes it challenging for themodel to identify which parts of the sequence contribute most significantly tothe final reward. Recent methods have attempted to address this limitation byintroducing token-level rewards. However, these methods often rely on either atrained credit assignment model or AI annotators, raising concerns about thequality and reliability of the rewards. In this paper, we propose token-levelreward regularization (T-REG), a novel approach that leverages bothsequence-level and token-level rewards for preference optimization. Harnessingthe self-refinement capabilities of LLMs, our method uses contrastive promptingto enable LLMs to self-generate token-level rewards. These self-generatedrewards then act as reward regularization, guiding the model to moreeffectively distribute sequence-level rewards across tokens. This facilitatesbetter token-level credit assignment and enhances alignment performance.Experiments on the instruction following benchmarks, including Alpaca Eval 2and Arena-Hard, show that our method consistently outperforms baseline methodsby up to 3.8% and 4.4%, respectively. We will release the code and models athttps://github.com/wzhouad/T-REG.</description><author>Wenxuan Zhou, Shujian Zhang, Lingxiao Zhao, Tao Meng</author><pubDate>Tue, 03 Dec 2024 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02685v1</guid></item><item><title>AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction</title><link>http://arxiv.org/abs/2412.02684v1</link><description>Generating animatable human avatars from a single image is essential forvarious digital human modeling applications. Existing 3D reconstruction methodsoften struggle to capture fine details in animatable models, while generativeapproaches for controllable animation, though avoiding explicit 3D modeling,suffer from viewpoint inconsistencies in extreme poses and computationalinefficiencies. In this paper, we address these challenges by leveraging thepower of generative models to produce detailed multi-view canonical poseimages, which help resolve ambiguities in animatable human reconstruction. Wethen propose a robust method for 3D reconstruction of inconsistent images,enabling real-time rendering during inference. Specifically, we adapt atransformer-based video generation model to generate multi-view canonical poseimages and normal maps, pretraining on a large-scale video dataset to improvegeneralization. To handle view inconsistencies, we recast the reconstructionproblem as a 4D task and introduce an efficient 3D modeling approach using 4DGaussian Splatting. Experiments demonstrate that our method achievesphotorealistic, real-time animation of 3D human avatars from in-the-wildimages, showcasing its effectiveness and generalization capability.</description><author>Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong</author><pubDate>Tue, 03 Dec 2024 18:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02684v1</guid></item><item><title>The Asymptotic Behavior of Attention in Transformers</title><link>http://arxiv.org/abs/2412.02682v1</link><description>A key component of transformers is the attention mechanism orchestrating howeach token influences the propagation of every other token through atransformer. In this paper we provide a rigorous, mathematical analysis of theasymptotic properties of attention in transformers. Although we present severalresults based on different assumptions, all of them point to the sameconclusion, all tokens asymptotically converge to each other, a phenomenon thathas been empirically reported in the literature. Our findings are carefullycompared with existing theoretical results and illustrated by simulations andexperimental studies using the GPT-2 model.</description><author>Álvaro Rodríguez Abella, João Pedro Silvestre, Paulo Tabuada</author><pubDate>Tue, 03 Dec 2024 18:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02682v1</guid></item><item><title>Planning-Guided Diffusion Policy Learning for Generalizable Contact-Rich Bimanual Manipulation</title><link>http://arxiv.org/abs/2412.02676v1</link><description>Contact-rich bimanual manipulation involves precise coordination of two armsto change object states through strategically selected contacts and motions.Due to the inherent complexity of these tasks, acquiring sufficientdemonstration data and training policies that generalize to unseen scenariosremain a largely unresolved challenge. Building on recent advances in planningthrough contacts, we introduce Generalizable Planning-Guided Diffusion PolicyLearning (GLIDE), an approach that effectively learns to solve contact-richbimanual manipulation tasks by leveraging model-based motion planners togenerate demonstration data in high-fidelity physics simulation. Throughefficient planning in randomized environments, our approach generateslarge-scale and high-quality synthetic motion trajectories for tasks involvingdiverse objects and transformations. We then train a task-conditioned diffusionpolicy via behavior cloning using these demonstrations. To tackle thesim-to-real gap, we propose a set of essential design options in featureextraction, task representation, action prediction, and data augmentation thatenable learning robust prediction of smooth action sequences and generalizationto unseen scenarios. Through experiments in both simulation and the real world,we demonstrate that our approach can enable a bimanual robotic system toeffectively manipulate objects of diverse geometries, dimensions, and physicalproperties. Website: https://glide-manip.github.io/</description><author>Xuanlin Li, Tong Zhao, Xinghao Zhu, Jiuguang Wang, Tao Pang, Kuan Fang</author><pubDate>Tue, 03 Dec 2024 18:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02676v1</guid></item><item><title>From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs</title><link>http://arxiv.org/abs/2410.14052v2</link><description>Recent advancements in large language models have significantly improvedtheir context windows, yet challenges in effective long-term memory managementremain. We introduce MemTree, an algorithm that leverages a dynamic,tree-structured memory representation to optimize the organization, retrieval,and integration of information, akin to human cognitive schemas. MemTreeorganizes memory hierarchically, with each node encapsulating aggregatedtextual content, corresponding semantic embeddings, and varying abstractionlevels across the tree's depths. Our algorithm dynamically adapts this memorystructure by computing and comparing semantic embeddings of new and existinginformation to enrich the model's context-awareness. This approach allowsMemTree to handle complex reasoning and extended interactions more effectivelythan traditional memory augmentation methods, which often rely on flat lookuptables. Evaluations on benchmarks for multi-turn dialogue understanding anddocument question answering show that MemTree significantly enhancesperformance in scenarios that demand structured memory management.</description><author>Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao</author><pubDate>Tue, 03 Dec 2024 18:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14052v2</guid></item><item><title>Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2412.02674v1</link><description>Self-improvement is a mechanism in Large Language Model (LLM) pre-training,post-training and test-time inference. We explore a framework where the modelverifies its own outputs, filters or reweights data based on this verification,and distills the filtered data. Despite several empirical successes, afundamental understanding is still lacking. In this work, we initiate acomprehensive, modular and controlled study on LLM self-improvement. We providea mathematical formulation for self-improvement, which is largely governed by aquantity which we formalize as the generation-verification gap. Throughexperiments with various model families and tasks, we discover a scalingphenomenon of self-improvement -- a variant of the generation-verification gapscales monotonically with the model pre-training flops. We also examine whenself-improvement is possible, an iterative self-improvement procedure, and waysto improve its performance. Our findings not only advance understanding of LLMself-improvement with practical implications, but also open numerous avenuesfor future research into its capabilities and boundaries.</description><author>Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai</author><pubDate>Tue, 03 Dec 2024 18:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02674v1</guid></item><item><title>Diffusion Models with Anisotropic Gaussian Splatting for Image Inpainting</title><link>http://arxiv.org/abs/2412.01682v2</link><description>Image inpainting is a fundamental task in computer vision, aiming to restoremissing or corrupted regions in images realistically. While recent deeplearning approaches have significantly advanced the state-of-the-art,challenges remain in maintaining structural continuity and generating coherenttextures, particularly in large missing areas. Diffusion models have shownpromise in generating high-fidelity images but often lack the structuralguidance necessary for realistic inpainting. We propose a novel inpaintingmethod that combines diffusion models with anisotropic Gaussian splatting tocapture both local structures and global context effectively. By modelingmissing regions using anisotropic Gaussian functions that adapt to local imagegradients, our approach provides structural guidance to the diffusion-basedinpainting network. The Gaussian splat maps are integrated into the diffusionprocess, enhancing the model's ability to generate high-fidelity andstructurally coherent inpainting results. Extensive experiments demonstratethat our method outperforms state-of-the-art techniques, producing visuallyplausible results with enhanced structural integrity and texture realism.</description><author>Jacob Fein-Ashley, Benjamin Fein-Ashley</author><pubDate>Tue, 03 Dec 2024 18:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01682v2</guid></item><item><title>The Broader Landscape of Robustness in Algorithmic Statistics</title><link>http://arxiv.org/abs/2412.02670v1</link><description>The last decade has seen a number of advances in computationally efficientalgorithms for statistical methods subject to robustness constraints. Anestimator may be robust in a number of different ways: to contamination of thedataset, to heavy-tailed data, or in the sense that it preserves privacy of thedataset. We survey recent results in these areas with a focus on the problem ofmean estimation, drawing technical and conceptual connections between thevarious forms of robustness, showing that the same underlying algorithmic ideaslead to computationally efficient estimators in all these settings.</description><author>Gautam Kamath</author><pubDate>Tue, 03 Dec 2024 18:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02670v1</guid></item><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2412.01819v2</link><description>This work presents Switti, a scale-wise transformer for text-to-imagegeneration. Starting from existing next-scale prediction AR models, we firstexplore them for T2I generation and propose architectural modifications toimprove their convergence and overall performance. We then observe thatself-attention maps of our pretrained scale-wise AR model exhibit weakdependence on preceding scales. Based on this insight, we propose a non-ARcounterpart facilitating ~11% faster sampling and lower memory usage while alsoachieving slightly better generation quality. Furthermore, we reveal thatclassifier-free guidance at high-resolution scales is often unnecessary and caneven degrade performance. By disabling guidance at these scales, we achieve anadditional sampling acceleration of ~20% and improve the generation offine-grained details. Extensive human preference studies and automatedevaluations show that Switti outperforms existing T2I AR models and competeswith state-of-the-art T2I diffusion models while being up to 7 times faster.</description><author>Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, Dmitry Baranchuk</author><pubDate>Tue, 03 Dec 2024 18:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01819v2</guid></item><item><title>D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup</title><link>http://arxiv.org/abs/2405.14276v3</link><description>Over the past years, we have observed an abundance of approaches for modelingdynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS torepresent the scene's structure and the neural network to model dynamics. Suchapproaches allow fast rendering and extracting each element of such a dynamicscene. However, modifying such objects over time is challenging. SC-GS (SparseControlled Gaussian Splatting) enhanced with Deformed Control Points partiallysolves this issue. However, this approach necessitates selecting elements thatneed to be kept fixed, as well as centroids that should be adjusted throughoutediting. Moreover, this task poses additional difficulties regarding there-productivity of such editing. To address this, we propose DynamicMulti-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspiredrepresentation of dynamic GS. Additionally, we propose a strategy of linkingparameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.Consequently, we can separately construct new trajectories for the 3D objectscomposing the scene. Thus, we can make the scene's dynamic editable over timeor while maintaining partial dynamics.</description><author>Joanna Waczyńska, Piotr Borycki, Joanna Kaleta, Sławomir Tadeja, Przemysław Spurek</author><pubDate>Tue, 03 Dec 2024 18:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14276v3</guid></item><item><title>STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.16221v3</link><description>The capability to accurately estimate 3D human poses is crucial for diversefields such as action recognition, gait recognition, and virtual/augmentedreality. However, a persistent and significant challenge within this field isthe accurate prediction of human poses under conditions of severe occlusion.Traditional image-based estimators struggle with heavy occlusions due to a lackof temporal context, resulting in inconsistent predictions. While video-basedmodels benefit from processing temporal data, they encounter limitations whenfaced with prolonged occlusions that extend over multiple frames. Thischallenge arises because these models struggle to generalize beyond theirtraining datasets, and the variety of occlusions is hard to capture in thetraining data. Addressing these challenges, we propose STRIDE (Single-videobased TempoRally contInuous occlusion Robust 3D Pose Estimation), a novelTest-Time Training (TTT) approach to fit a human motion prior for each video.This approach specifically handles occlusions that were not encountered duringthe model's training. By employing STRIDE, we can refine a sequence of noisyinitial pose estimates into accurate, temporally coherent poses during testtime, effectively overcoming the limitations of prior methods. Our frameworkdemonstrates flexibility by being model-agnostic, allowing us to use anyoff-the-shelf 3D pose estimation method for improving robustness and temporalconsistency. We validate STRIDE's efficacy through comprehensive experiments onchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where itnot only outperforms existing single-image and video-based pose estimationmodels but also showcases superior handling of substantial occlusions,achieving fast, robust, accurate, and temporally consistent 3D pose estimates.Code is made publicly available at https://github.com/take2rohit/stride</description><author>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury</author><pubDate>Tue, 03 Dec 2024 18:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16221v3</guid></item><item><title>Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Environments with Delayed Rewards</title><link>http://arxiv.org/abs/2411.17861v2</link><description>In this paper, we tackle the challenging problem of delayed rewards inreinforcement learning (RL). While Proximal Policy Optimization (PPO) hasemerged as a leading Policy Gradient method, its performance can degrade underdelayed rewards. We introduce two key enhancements to PPO: a hybrid policyarchitecture that combines an offline policy (trained on expert demonstrations)with an online PPO policy, and a reward shaping mechanism using Time WindowTemporal Logic (TWTL). The hybrid architecture leverages offline datathroughout training while maintaining PPO's theoretical guarantees. Building onthe monotonic improvement framework of Trust Region Policy Optimization (TRPO),we prove that our approach ensures improvement over both the offline policy andprevious iterations, with a bounded performance gap of$(2\varsigma\gamma\alpha^2)/(1-\gamma)^2$, where $\alpha$ is the mixingparameter, $\gamma$ is the discount factor, and $\varsigma$ bounds the expectedadvantage. Additionally, we prove that our TWTL-based reward shaping preservesthe optimal policy of the original problem. TWTL enables formal translation oftemporal objectives into immediate feedback signals that guide learning. Wedemonstrate the effectiveness of our approach through extensive experiments onan inverted pendulum and a lunar lander environments, showing improvements inboth learning speed and final performance compared to standard PPO andoffline-only approaches.</description><author>Ahmad Ahmad, Mehdi Kermanshah, Kevin Leahy, Zachary Serlin, Ho Chit Siu, Makai Mann, Cristian-Ioan Vasile, Roberto Tron, Calin Belta</author><pubDate>Tue, 03 Dec 2024 18:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17861v2</guid></item><item><title>Probing the statistical properties of enriched co-occurrence networks</title><link>http://arxiv.org/abs/2412.02664v1</link><description>Recent studies have explored the addition of virtual edges to wordco-occurrence networks using word embeddings to enhance graph representations,particularly for short texts. While these enriched networks have demonstratedsome success, the impact of incorporating semantic edges into traditionalco-occurrence networks remains uncertain. This study investigates two keystatistical properties of text-based network models. First, we assess whethernetwork metrics can effectively distinguish between meaningless and meaningfultexts. Second, we analyze whether these metrics are more sensitive to syntacticor semantic aspects of the text. Our results show that incorporating virtualedges can have positive and negative effects, depending on the specific networkmetric. For instance, the informativeness of the average shortest path andcloseness centrality improves in short texts, while the clusteringcoefficient's informativeness decreases as more virtual edges are added.Additionally, we found that including stopwords affects the statisticalproperties of enriched networks. Our results can serve as a guideline fordetermining which network metrics are most appropriate for specificapplications, depending on the typical text size and the nature of the problem.</description><author>Diego R. Amancio, Jeaneth Machicao, Laura V. C. Quispe</author><pubDate>Tue, 03 Dec 2024 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02664v1</guid></item><item><title>Efficient Graph Matching for Correlated Stochastic Block Models</title><link>http://arxiv.org/abs/2412.02661v1</link><description>We study learning problems on correlated stochastic block models with twobalanced communities. Our main result gives the first efficient algorithm forgraph matching in this setting. In the most interesting regime where theaverage degree is logarithmic in the number of vertices, this algorithmcorrectly matches all but a vanishing fraction of vertices with highprobability, whenever the edge correlation parameter $s$ satisfies $s^2 &gt;\alpha \approx 0.338$, where $\alpha$ is Otter's tree-counting constant.Moreover, we extend this to an efficient algorithm for exact graph matchingwhenever this is information-theoretically possible, positively resolving anopen problem of R\'acz and Sridhar (NeurIPS 2021). Our algorithm generalizesthe recent breakthrough work of Mao, Wu, Xu, and Yu (STOC 2023), which is basedon centered subgraph counts of a large family of trees termed chandeliers. Amajor technical challenge that we overcome is dealing with the additionalestimation errors that are necessarily present due to the fact that, inrelevant parameter regimes, the latent community partition cannot be exactlyrecovered from a single graph. As an application of our results, we give anefficient algorithm for exact community recovery using multiple correlatedgraphs in parameter regimes where it is information-theoretically impossible todo so using just a single graph.</description><author>Shuwen Chai, Miklós Z. Rácz</author><pubDate>Tue, 03 Dec 2024 18:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02661v1</guid></item><item><title>Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply</title><link>http://arxiv.org/abs/2312.07636v2</link><description>Traditional end-to-end (E2E) training of deep networks necessitates storingintermediate activations for back-propagation, resulting in a large memoryfootprint on GPUs and restricted model parallelization. As an alternative,greedy local learning partitions the network into gradient-isolated modules andtrains supervisely based on local preliminary losses, thereby providingasynchronous and parallel training methods that substantially reduce memorycost. However, empirical experiments reveal that as the number of segmentationsof the gradient-isolated module increases, the performance of the locallearning scheme degrades substantially, severely limiting its expansibility. Toavoid this issue, we theoretically analyze the greedy local learning from thestandpoint of information theory and propose a ContSup scheme, whichincorporates context supply between isolated modules to compensate forinformation loss. Experiments on benchmark datasets (i.e. CIFAR, SVHN, STL-10)achieve SOTA results and indicate that our proposed method can significantlyimprove the performance of greedy local learning with minimal memory andcomputational overhead, allowing for the boost of the number of isolatedmodules. Our codes are available at https://github.com/Tab-ct/ContSup.</description><author>Chengting Yu, Fengzhao Zhang, Hanzhi Ma, Aili Wang, Erping Li</author><pubDate>Tue, 03 Dec 2024 18:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07636v2</guid></item><item><title>Adaptive Informed Deep Neural Networks for Power Flow Analysis</title><link>http://arxiv.org/abs/2412.02659v1</link><description>This study introduces PINN4PF, an end-to-end deep learning architecture forpower flow (PF) analysis that effectively captures the nonlinear dynamics oflarge-scale modern power systems. The proposed neural network (NN) architectureconsists of two important advancements in the training pipeline: (A) adouble-head feed-forward NN that aligns with PF analysis, including anactivation function that adjusts to active and reactive power consumptionpatterns, and (B) a physics-based loss function that partially incorporatespower system topology information. The effectiveness of the proposedarchitecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus testsystems and is evaluated against two baselines: a linear regression model (LR)and a black-box NN (MLP). The comparison is based on (i) generalizationability, (ii) robustness, (iii) impact of training dataset size ongeneralization ability, (iv) accuracy in approximating derived PF quantities(specifically line current, line active power, and line reactive power), and(v) scalability. Results demonstrate that PINN4PF outperforms both baselinesacross all test systems by up to two orders of magnitude not only in terms ofdirect criteria, e.g., generalization ability but also in terms ofapproximating derived physical quantities.</description><author>Zeynab Kaseb, Stavros Orfanoudakis, Pedro P. Vergara, Peter Palensky</author><pubDate>Tue, 03 Dec 2024 18:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02659v1</guid></item><item><title>A Fast Convergence Theory for Offline Decision Making</title><link>http://arxiv.org/abs/2406.01378v2</link><description>This paper proposes the first generic fast convergence result in generalfunction approximation for offline decision making problems, which includeoffline reinforcement learning (RL) and off-policy evaluation (OPE) as specialcases. To unify different settings, we introduce a framework called DecisionMaking with Offline Feedback (DMOF), which captures a wide range of offlinedecision making problems. Within this framework, we propose a simple yetpowerful algorithm called Empirical Decision with Divergence (EDD), whose upperbound can be termed as a coefficient named Empirical Offline EstimationCoefficient (EOEC). We show that EOEC is instance-dependent and actuallymeasures the correlation of the problem. When assuming partial coverage in thedataset, EOEC will reduce in a rate of $1/N$ where $N$ is the size of thedataset, endowing EDD with a fast convergence guarantee. Finally, we complementthe above results with a lower bound in the DMOF framework, which furtherdemonstrates the soundness of our theory.</description><author>Chenjie Mao, Qiaosheng Zhang</author><pubDate>Tue, 03 Dec 2024 18:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01378v2</guid></item><item><title>Decoupling Dark Knowledge via Block-wise Logit Distillation for Feature-level Alignment</title><link>http://arxiv.org/abs/2411.01547v2</link><description>Knowledge Distillation (KD), a learning manner with a larger teacher networkguiding a smaller student network, transfers dark knowledge from the teacher tothe student via logits or intermediate features, with the aim of producing awell-performed lightweight model. Notably, many subsequent feature-based KDmethods outperformed the earliest logit-based KD method and iterativelygenerated numerous state-of-the-art distillation methods. Nevertheless, recentwork has uncovered the potential of the logit-based method, bringing the simpleKD form based on logits back into the limelight. Features or logits? Theypartially implement the KD with entirely distinct perspectives; therefore,choosing between logits and features is not straightforward. This paperprovides a unified perspective of feature alignment in order to obtain a bettercomprehension of their fundamental distinction. Inheriting the designphilosophy and insights of feature-based and logit-based methods, we introducea block-wise logit distillation framework to apply implicit logit-based featurealignment by gradually replacing teacher's blocks as intermediatestepping-stone models to bridge the gap between the student and the teacher.Our method obtains comparable or superior results to state-of-the-artdistillation methods. This paper demonstrates the great potential of combininglogit and features, and we hope it will inspire future research to revisit KDfrom a higher vantage point.</description><author>Chengting Yu, Fengzhao Zhang, Ruizhe Chen, Aili Wang, Zuozhu Liu, Shurun Tan, Er-Ping Li</author><pubDate>Tue, 03 Dec 2024 18:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01547v2</guid></item><item><title>Scaffold or Crutch? Examining College Students' Use and Views of Generative AI Tools for STEM Education</title><link>http://arxiv.org/abs/2412.02653v1</link><description>Developing problem-solving competency is central to Science, Technology,Engineering, and Mathematics (STEM) education, yet translating this priorityinto effective approaches to problem-solving instruction and assessment remaina significant challenge. The recent proliferation of generative artificialintelligence (genAI) tools like ChatGPT in higher education introduces newconsiderations about how these tools can help or hinder students' developmentof STEM problem-solving competency. Our research examines these considerationsby studying how and why college students use genAI tools in their STEMcoursework, focusing on their problem-solving support. We surveyed 40 STEMcollege students from diverse U.S. institutions and 28 STEM faculty tounderstand instructor perspectives on effective genAI tool use and guidance inSTEM courses. Our findings reveal high adoption rates and diverse applicationsof genAI tools among STEM students. The most common use cases include findingexplanations, exploring related topics, summarizing readings, and helping withproblem-set questions. The primary motivation for using genAI tools was to savetime. Moreover, over half of student participants reported simply inputtingproblems for AI to generate solutions, potentially bypassing their ownproblem-solving processes. These findings indicate that despite high adoptionrates, students' current approaches to utilizing genAI tools often fall shortin enhancing their own STEM problem-solving competencies. The study alsoexplored students' and STEM instructors' perceptions of the benefits and risksassociated with using genAI tools in STEM education. Our findings provideinsights into how to guide students on appropriate genAI use in STEM coursesand how to design genAI-based tools to foster students' problem-solvingcompetency.</description><author>Karen D. Wang, Zhangyang Wu, L'Nard Tufts II, Carl Wieman, Shima Salehi, Nick Haber</author><pubDate>Tue, 03 Dec 2024 18:27:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02653v1</guid></item><item><title>Interpretable Generalized Additive Models for Datasets with Missing Values</title><link>http://arxiv.org/abs/2412.02646v1</link><description>Many important datasets contain samples that are missing one or more featurevalues. Maintaining the interpretability of machine learning models in thepresence of such missing data is challenging. Singly or multiply imputingmissing values complicates the model's mapping from features to labels. On theother hand, reasoning on indicator variables that represent missingnessintroduces a potentially large number of additional terms, sacrificingsparsity. We solve these problems with M-GAM, a sparse, generalized, additivemodeling approach that incorporates missingness indicators and theirinteraction terms while maintaining sparsity through l0 regularization. We showthat M-GAM provides similar or superior accuracy to prior methods whilesignificantly improving sparsity relative to either imputation or naiveinclusion of indicator variables.</description><author>Hayden McTavish, Jon Donnelly, Margo Seltzer, Cynthia Rudin</author><pubDate>Tue, 03 Dec 2024 18:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02646v1</guid></item><item><title>A Bidirectional Long Short Term Memory Approach for Infrastructure Health Monitoring Using On-board Vibration Response</title><link>http://arxiv.org/abs/2412.02643v1</link><description>The growing volume of available infrastructural monitoring data enables thedevelopment of powerful datadriven approaches to estimate infrastructure healthconditions using direct measurements. This paper proposes a deep learningmethodology to estimate infrastructure physical parameters, such as railwaytrack stiffness, using drive-by vibration response signals. The proposed methodemploys a Long Short-term Memory (LSTM) feature extractor accounting fortemporal dependencies in the feature extraction phase, and a bidirectional LongShort-term Memory (BiLSTM) networks to leverage bidirectional temporaldependencies in both the forward and backward paths of the drive-by vibrationresponse in condition estimation phase. Additionally, a framing approach isemployed to enhance the resolution of the monitoring task to the beam level bysegmenting the vibration signal into frames equal to the distance betweenindividual beams, centering the frames over the beam nodes. The proposedLSTM-BiLSTM model offers a versatile tool for various bridge and railwayinfrastructure conditions monitoring using direct drive-by vibration responsemeasurements. The results demonstrate the potential of incorporating temporalanalysis in the feature extraction phase and emphasize the pivotal role ofbidirectional temporal information in infrastructure health conditionestimation. The proposed methodology can accurately and automatically estimaterailway track stiffness and identify local stiffness reductions in the presenceof noise using drive-by measurements. An illustrative case study ofvehicle-track interaction simulation is used to demonstrate the performance ofthe proposed model, achieving a maximum mean absolute percentage error of 1.7%and 0.7% in estimating railpad and ballast stiffness, respectively.</description><author>R. R. Samani, A. Nunez, B. De Schutter</author><pubDate>Tue, 03 Dec 2024 18:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02643v1</guid></item><item><title>Robust soybean seed yield estimation using high-throughput ground robot videos</title><link>http://arxiv.org/abs/2412.02642v1</link><description>We present a novel method for soybean (Glycine max (L.) Merr.) yieldestimation leveraging high throughput seed counting via computer vision anddeep learning techniques. Traditional methods for collecting yield data arelabor-intensive, costly, prone to equipment failures at critical datacollection times, and require transportation of equipment across field sites.Computer vision, the field of teaching computers to interpret visual data,allows us to extract detailed yield information directly from images. Bytreating it as a computer vision task, we report a more efficient alternative,employing a ground robot equipped with fisheye cameras to capture comprehensivevideos of soybean plots from which images are extracted in a variety ofdevelopment programs. These images are processed through the P2PNet-Yieldmodel, a deep learning framework where we combined a Feature Extraction Module(the backbone of the P2PNet-Soy) and a Yield Regression Module to estimate seedyields of soybean plots. Our results are built on three years of yield testingplot data - 8500 in 2021, 2275 in 2022, and 650 in 2023. With these datasets,our approach incorporates several innovations to further improve the accuracyand generalizability of the seed counting and yield estimation architecture,such as the fisheye image correction and data augmentation with random sensoreffects. The P2PNet-Yield model achieved a genotype ranking accuracy score ofup to 83%. It demonstrates up to a 32% reduction in time to collect yield dataas well as costs associated with traditional yield estimation, offering ascalable solution for breeding programs and agricultural productivityenhancement.</description><author>Jiale Feng, Samuel W. Blair, Timilehin Ayanlade, Aditya Balu, Baskar Ganapathysubramanian, Arti Singh, Soumik Sarkar, Asheesh K Singh</author><pubDate>Tue, 03 Dec 2024 18:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02642v1</guid></item><item><title>The Space Complexity of Approximating Logistic Loss</title><link>http://arxiv.org/abs/2412.02639v1</link><description>We provide space complexity lower bounds for data structures that approximatelogistic loss up to $\epsilon$-relative error on a logistic regression problemwith data $\mathbf{X} \in \mathbb{R}^{n \times d}$ and labels $\mathbf{y} \in\{-1,1\}^d$. The space complexity of existing coreset constructions depend on anatural complexity measure $\mu_\mathbf{y}(\mathbf{X})$, first defined in(Munteanu, 2018). We give an $\tilde{\Omega}(\frac{d}{\epsilon^2})$ spacecomplexity lower bound in the regime $\mu_\mathbf{y}(\mathbf{X}) = O(1)$ thatshows existing coresets are optimal in this regime up to lower order factors.We also prove a general $\tilde{\Omega}(d\cdot \mu_\mathbf{y}(\mathbf{X}))$space lower bound when $\epsilon$ is constant, showing that the dependency on$\mu_\mathbf{y}(\mathbf{X})$ is not an artifact of mergeable coresets. Finally,we refute a prior conjecture that $\mu_\mathbf{y}(\mathbf{X})$ is hard tocompute by providing an efficient linear programming formulation, and weempirically compare our algorithm to prior approximate methods.</description><author>Gregory Dexter, Petros Drineas, Rajiv Khanna</author><pubDate>Tue, 03 Dec 2024 18:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02639v1</guid></item><item><title>QA-TOOLBOX: Conversational Question-Answering for process task guidance in manufacturing</title><link>http://arxiv.org/abs/2412.02638v1</link><description>In this work we explore utilizing LLMs for data augmentation formanufacturing task guidance system. The dataset consists of representativesamples of interactions with technicians working in an advanced manufacturingsetting. The purpose of this work to explore the task, data augmentation forthe supported tasks and evaluating the performance of the existing LLMs. Weobserve that that task is complex requiring understanding from procedurespecification documents, actions and objects sequenced temporally. The datasetconsists of 200,000+ question/answer pairs that refer to the spec document andare grounded in narrations and/or video demonstrations. We compared theperformance of several popular open-sourced LLMs by developing a baseline usingeach LLM and then compared the responses in a reference-free setting usingLLM-as-a-judge and compared the ratings with crowd-workers whilst validatingthe ratings with experts.</description><author>Ramesh Manuvinakurike, Elizabeth Watkins, Celal Savur, Anthony Rhodes, Sovan Biswas, Gesem Gudino Mejia, Richard Beckwith, Saurav Sahay, Giuseppe Raffa, Lama Nachman</author><pubDate>Tue, 03 Dec 2024 18:10:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02638v1</guid></item><item><title>Words and Action: Modeling Linguistic Leadership in #BlackLivesMatter Communities</title><link>http://arxiv.org/abs/2412.02637v1</link><description>In this project, we describe a method of modeling semantic leadership acrossa set of communities associated with the #BlackLivesMatter movement, which hasbeen informed by qualitative research on the structure of social media andBlack Twitter in particular. We describe our bespoke approaches totime-binning, community clustering, and connecting communities over time, aswell as our adaptation of state-of-the-art approaches to semantic changedetection and semantic leadership induction. We find substantial evidence ofthe leadership role of BLM activists and progressives, as well as Blackcelebrities. We also find evidence of the sustained engagement of theconservative community with this discourse, suggesting an alternativeexplanation for how we arrived at the present moment, in which "anti-woke" and"anti-CRT" bills are being enacted nationwide.</description><author>Dani Roytburg, Deborah Olorunisola, Sandeep Soni, Lauren Klein</author><pubDate>Tue, 03 Dec 2024 18:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02637v1</guid></item><item><title>MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis</title><link>http://arxiv.org/abs/2412.02635v1</link><description>Shadows are often under-considered or even ignored in image editingapplications, limiting the realism of the edited results. In this paper, weintroduce MetaShadow, a three-in-one versatile framework that enablesdetection, removal, and controllable synthesis of shadows in natural images inan object-centered fashion. MetaShadow combines the strengths of twocooperative components: Shadow Analyzer, for object-centered shadow detectionand removal, and Shadow Synthesizer, for reference-based controllable shadowsynthesis. Notably, we optimize the learning of the intermediate features fromShadow Analyzer to guide Shadow Synthesizer to generate more realistic shadowsthat blend seamlessly with the scene. Extensive evaluations on multiple shadowbenchmark datasets show significant improvements of MetaShadow over theexisting state-of-the-art methods on object-centered shadow detection, removal,and synthesis. MetaShadow excels in image-editing tasks such as object removal,relocation, and insertion, pushing the boundaries of object-centered imageediting.</description><author>Tianyu Wang, Jianming Zhang, Haitian Zheng, Zhihong Ding, Scott Cohen, Zhe Lin, Wei Xiong, Chi-Wing Fu, Luis Figueroa, Soo Ye Kim</author><pubDate>Tue, 03 Dec 2024 18:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02635v1</guid></item><item><title>Scaling Image Tokenizers with Grouped Spherical Quantization</title><link>http://arxiv.org/abs/2412.02632v1</link><description>Vision tokenizers have gained a lot of attraction due to their scalabilityand compactness; previous works depend on old-school GAN-based hyperparameters,biased comparisons, and a lack of comprehensive analysis of the scalingbehaviours. To tackle those issues, we introduce Grouped Spherical Quantization(GSQ), featuring spherical codebook initialization and lookup regularization toconstrain codebook latent to a spherical surface. Our empirical analysis ofimage tokenizer training strategies demonstrates that GSQ-GAN achieves superiorreconstruction quality over state-of-the-art methods with fewer trainingiterations, providing a solid foundation for scaling studies. Building on this,we systematically examine the scaling behaviours of GSQ, specifically in latentdimensionality, codebook size, and compression ratios, and their impact onmodel performance. Our findings reveal distinct behaviours at high and lowspatial compression levels, underscoring challenges in representinghigh-dimensional latent spaces. We show that GSQ can restructurehigh-dimensional latent into compact, low-dimensional spaces, thus enablingefficient scaling with improved quality. As a result, GSQ-GAN achieves a 16xdown-sampling with a reconstruction FID (rFID) of 0.50.</description><author>Jiangtao Wang, Zhen Qin, Yifan Zhang, Vincent Tao Hu, Björn Ommer, Rania Briq, Stefan Kesselheim</author><pubDate>Tue, 03 Dec 2024 18:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02632v1</guid></item><item><title>Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation</title><link>http://arxiv.org/abs/2412.02631v1</link><description>Advancements in text-to-image diffusion models have led to significantprogress in fast 3D content creation. One common approach is to generate a setof multi-view images of an object, and then reconstruct it into a 3D model.However, this approach bypasses the use of a native 3D representation of theobject and is hence prone to geometric artifacts and limited in controllabilityand manipulation capabilities. An alternative approach involves native 3Dgenerative models that directly produce 3D representations. These models,however, are typically limited in their resolution, resulting in lower quality3D objects. In this work, we bridge the quality gap between methods thatdirectly generate 3D representations and ones that reconstruct 3D objects frommulti-view images. We introduce a multi-view to multi-view diffusion modelcalled Sharp-It, which takes a 3D consistent set of multi-view images renderedfrom a low-quality object and enriches its geometric details and texture. Thediffusion model operates on the multi-view set in parallel, in the sense thatit shares features across the generated views. A high-quality 3D model can thenbe reconstructed from the enriched multi-view set. By leveraging the advantagesof both 2D and 3D approaches, our method offers an efficient and controllablemethod for high-quality 3D content creation. We demonstrate that Sharp-Itenables various 3D applications, such as fast synthesis, editing, andcontrolled generation, while attaining high-quality assets.</description><author>Yiftach Edelstein, Or Patashnik, Dana Cohen-Bar, Lihi Zelnik-Manor</author><pubDate>Tue, 03 Dec 2024 17:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02631v1</guid></item><item><title>Continual Learning of Personalized Generative Face Models with Experience Replay</title><link>http://arxiv.org/abs/2412.02627v1</link><description>We introduce a novel continual learning problem: how to sequentially updatethe weights of a personalized 2D and 3D generative face model as new batches ofphotos in different appearances, styles, poses, and lighting are capturedregularly. We observe that naive sequential fine-tuning of the model leads tocatastrophic forgetting of past representations of the individual's face. Wethen demonstrate that a simple random sampling-based experience replay methodis effective at mitigating catastrophic forgetting when a relatively largenumber of images can be stored and replayed. However, for long-term deploymentof these models with relatively smaller storage, this simple randomsampling-based replay technique also forgets past representations. Thus, weintroduce a novel experience replay algorithm that combines random samplingwith StyleGAN's latent space to represent the buffer as an optimal convex hull.We observe that our proposed convex hull-based experience replay is moreeffective in preventing forgetting than a random sampling baseline and thelower bound.</description><author>Annie N. Wang, Luchao Qi, Roni Sengupta</author><pubDate>Tue, 03 Dec 2024 17:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02627v1</guid></item><item><title>Time-Reversal Provides Unsupervised Feedback to LLMs</title><link>http://arxiv.org/abs/2412.02626v1</link><description>Large Language Models (LLMs) are typically trained to predict in the forwarddirection of time. However, recent works have shown that prompting these modelsto look back and critique their own generations can produce useful feedback.Motivated by this, we explore the question of whether LLMs can be empowered tothink (predict and score) backwards to provide unsupervised feedback thatcomplements forward LLMs. Towards this, we introduce Time Reversed LanguageModels (TRLMs), which can score and generate queries when conditioned onresponses, effectively functioning in the reverse direction of time. Further,to effectively infer in the response to query direction, we pre-train andfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.We show empirically (and theoretically in a stylized setting) thattime-reversed models can indeed complement forward model predictions when usedto score the query given response for re-ranking multiple forward generations.We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard overthe competent baseline of best-of-N re-ranking using self log-perplexityscores. We further show that TRLM scoring outperforms conventional forwardscoring of response given query, resulting in significant gains in applicationssuch as citation generation and passage retrieval. We next leverage thegenerative ability of TRLM to augment or provide unsupervised feedback to inputsafety filters of LLMs, demonstrating a drastic reduction in false negativerate with negligible impact on false positive rates against several attackspublished on the popular JailbreakBench leaderboard.</description><author>Yerram Varun, Rahul Madhavan, Sravanti Addepalli, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain</author><pubDate>Tue, 03 Dec 2024 17:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02626v1</guid></item><item><title>The effect of priors on Learning with Restricted Boltzmann Machines</title><link>http://arxiv.org/abs/2412.02623v1</link><description>Restricted Boltzmann Machines (RBMs) are generative models designed to learnfrom data with a rich underlying structure. In this work, we explore ateacher-student setting where a student RBM learns from examples generated by ateacher RBM, with a focus on the effect of the unit priors on learningefficiency. We consider a parametric class of priors that interpolate betweencontinuous (Gaussian) and binary variables. This approach models variouspossible choices of visible units, hidden units, and weights for both theteacher and student RBMs. By analyzing the phase diagram of the posterior distribution in both theBayes optimal and mismatched regimes, we demonstrate the existence of a triplepoint that defines the critical dataset size necessary for learning throughgeneralization. The critical size is strongly influenced by the properties ofthe teacher, and thus the data, but is unaffected by the properties of thestudent RBM. Nevertheless, a prudent choice of student priors can facilitatetraining by expanding the so-called signal retrieval region, where the machinegeneralizes effectively.</description><author>Gianluca Manzan, Daniele Tantari</author><pubDate>Tue, 03 Dec 2024 17:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02623v1</guid></item><item><title>Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions</title><link>http://arxiv.org/abs/2412.02621v1</link><description>Recent advancements in deep learning have significantly revolutionized thefield of clinical diagnosis and treatment, offering novel approaches to improvediagnostic precision and treatment efficacy across diverse clinical domains,thus driving the pursuit of precision medicine. The growing availability ofmulti-organ and multimodal datasets has accelerated the development oflarge-scale Medical Multimodal Foundation Models (MMFMs). These models, knownfor their strong generalization capabilities and rich representational power,are increasingly being adapted to address a wide range of clinical tasks, fromearly diagnosis to personalized treatment strategies. This review offers acomprehensive analysis of recent developments in MMFMs, focusing on three keyaspects: datasets, model architectures, and clinical applications. We alsoexplore the challenges and opportunities in optimizing multimodalrepresentations and discuss how these advancements are shaping the future ofhealthcare by enabling improved patient outcomes and more efficient clinicalworkflows.</description><author>Kai Sun, Siyan Xue, Fuchun Sun, Haoran Sun, Yu Luo, Ling Wang, Siyuan Wang, Na Guo, Lei Liu, Tian Zhao, Xinzhou Wang, Lei Yang, Shuo Jin, Jun Yan, Jiahong Dong</author><pubDate>Tue, 03 Dec 2024 17:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02621v1</guid></item><item><title>Neural Thermodynamic Integration: Free Energies from Energy-based Diffusion Models</title><link>http://arxiv.org/abs/2406.02313v4</link><description>Thermodynamic integration (TI) offers a rigorous method for estimatingfree-energy differences by integrating over a sequence of interpolatingconformational ensembles. However, TI calculations are computationallyexpensive and typically limited to coupling a small number of degrees offreedom due to the need to sample numerous intermediate ensembles withsufficient conformational-space overlap. In this work, we propose to perform TIalong an alchemical pathway represented by a trainable neural network, which weterm Neural TI. Critically, we parametrize a time-dependent Hamiltonianinterpolating between the interacting and non-interacting systems, and optimizeits gradient using a score matching objective. The ability of the resultingenergy-based diffusion model to sample all intermediate ensembles allows us toperform TI from a single reference calculation. We apply our method toLennard-Jones fluids, where we report accurate calculations of the excesschemical potential, demonstrating that Neural TI reproduces the underlyingchanges in free energy without the need for simulations at interpolatingHamiltonians.</description><author>Bálint Máté, François Fleuret, Tristan Bereau</author><pubDate>Tue, 03 Dec 2024 17:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02313v4</guid></item><item><title>Leveraging LLM for Automated Ontology Extraction and Knowledge Graph Generation</title><link>http://arxiv.org/abs/2412.00608v2</link><description>Extracting relevant and structured knowledge from large, complex technicaldocuments within the Reliability and Maintainability (RAM) domain islabor-intensive and prone to errors. Our work addresses this challenge bypresenting OntoKGen, a genuine pipeline for ontology extraction and KnowledgeGraph (KG) generation. OntoKGen leverages Large Language Models (LLMs) throughan interactive user interface guided by our adaptive iterative Chain of Thought(CoT) algorithm to ensure that the ontology extraction process and, thus, KGgeneration align with user-specific requirements. Although KG generationfollows a clear, structured path based on the confirmed ontology, there is nouniversally correct ontology as it is inherently based on the user'spreferences. OntoKGen recommends an ontology grounded in best practices,minimizing user effort and providing valuable insights that may have beenoverlooked, all while giving the user complete control over the final ontology.Having generated the KG based on the confirmed ontology, OntoKGen enablesseamless integration into schemeless, non-relational databases like Neo4j. Thisintegration allows for flexible storage and retrieval of knowledge fromdiverse, unstructured sources, facilitating advanced querying, analysis, anddecision-making. Moreover, the generated KG serves as a robust foundation forfuture integration into Retrieval Augmented Generation (RAG) systems, offeringenhanced capabilities for developing domain-specific intelligent applications.</description><author>Mohammad Sadeq Abolhasani, Rong Pan</author><pubDate>Tue, 03 Dec 2024 17:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00608v2</guid></item><item><title>Demonstrating the Advantages of Analog Wafer-Scale Neuromorphic Hardware</title><link>http://arxiv.org/abs/2412.02619v1</link><description>As numerical simulations grow in size and complexity, they becomeincreasingly resource-intensive in terms of time and energy. While specializedhardware accelerators often provide order-of-magnitude gains and are state ofthe art in other scientific fields, their availability and applicability incomputational neuroscience is still limited. In this field, neuromorphicaccelerators, particularly mixed-signal architectures like the BrainScaleSsystems, offer the most significant performance benefits. These systemsmaintain a constant, accelerated emulation speed independent of network modeland size. This is especially beneficial when traditional simulators reach theirlimits, such as when modeling complex neuron dynamics, incorporating plasticitymechanisms, or running long or repetitive experiments. However, the analognature of these systems introduces new challenges. In this paper we demonstratethe capabilities and advantages of the BrainScaleS-1 system and how it can beused in combination with conventional software simulations. We report theemulation time and energy consumption for two biologically inspired networksadapted to the neuromorphic hardware substrate: a balanced random network basedon Brunel and the cortical microcircuit from Potjans and Diesmann.</description><author>Hartmut Schmidt, Andreas Grübl, José Montes, Eric Müller, Sebastian Schmitt, Johannes Schemmel</author><pubDate>Tue, 03 Dec 2024 17:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02619v1</guid></item><item><title>Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback</title><link>http://arxiv.org/abs/2412.02617v1</link><description>Large text-to-video models hold immense potential for a wide range ofdownstream applications. However, these models struggle to accurately depictdynamic object interactions, often resulting in unrealistic movements andfrequent violations of real-world physics. One solution inspired by largelanguage models is to align generated outputs with desired outcomes usingexternal feedback. This enables the model to refine its responses autonomously,eliminating extensive manual data collection. In this work, we investigate theuse of feedback to enhance the object dynamics in text-to-video models. We aimto answer a critical question: what types of feedback, paired with whichspecific self-improvement algorithms, can most effectively improve text-videoalignment and realistic object interactions? We begin by deriving a unifiedprobabilistic objective for offline RL finetuning of text-to-video models. Thisperspective highlights how design elements in existing algorithms like KLregularization and policy projection emerge as specific choices within aunified framework. We then use derived methods to optimize a set of text-videoalignment metrics (e.g., CLIP scores, optical flow), but notice that they oftenfail to align with human perceptions of generation quality. To address thislimitation, we propose leveraging vision-language models to provide morenuanced feedback specifically tailored to object dynamics in videos. Ourexperiments demonstrate that our method can effectively optimize a wide varietyof rewards, with binary AI feedback driving the most significant improvementsin video quality for dynamic interactions, as confirmed by both AI and humanevaluations. Notably, we observe substantial gains when using reward signalsderived from AI feedback, particularly in scenarios involving complexinteractions between multiple objects and realistic depictions of objectsfalling.</description><author>Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, Sherry Yang</author><pubDate>Tue, 03 Dec 2024 17:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02617v1</guid></item><item><title>Projection Abstractions in Planning Under the Lenses of Abstractions for MDPs</title><link>http://arxiv.org/abs/2412.02615v1</link><description>The concept of abstraction has been independently developed both in thecontext of AI Planning and discounted Markov Decision Processes (MDPs).However, the way abstractions are built and used in the context of Planning andMDPs is different even though lots of commonalities can be highlighted. To thisday there is no work trying to relate and unify the two fields on the matter ofabstractions unraveling all the different assumptions and their effect on theway they can be used. Therefore, in this paper we aim to do so by looking atprojection abstractions in Planning through the lenses of discounted MDPs.Starting from a projection abstraction built according to Classical orProbabilistic Planning techniques, we will show how the same abstraction can beobtained under the abstraction frameworks available for discounted MDPs. Alongthe way, we will focus on computational as well as representational advantagesand disadvantages of both worlds pointing out new research directions that areof interest for both fields.</description><author>Giuseppe Canonaco, Alberto Pozanco, Daniel Borrajo</author><pubDate>Tue, 03 Dec 2024 17:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02615v1</guid></item><item><title>GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot</title><link>http://arxiv.org/abs/2412.02612v1</link><description>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spokenchatbot. It supports both Chinese and English, engages in real-time voiceconversations, and varies vocal nuances such as emotion, intonation, speechrate, and dialect according to user instructions. GLM-4-Voice uses an ultra-lowbitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame ratederived from an automatic speech recognition (ASR) model by incorporating avector-quantized bottleneck into the encoder. To efficiently transfer knowledgefrom text to speech modalities, we synthesize speech-text interleaved data fromexisting text pre-training corpora using a text-to-token model. We continuepre-training from the pre-trained text language model GLM-4-9B with acombination of unsupervised speech data, interleaved speech-text data, andsupervised speech-text data, scaling up to 1 trillion tokens, achievingstate-of-the-art performance in both speech language modeling and spokenquestion answering. We then fine-tune the pre-trained model with high-qualityconversational speech data, achieving superior performance compared to existingbaselines in both conversational ability and speech quality. The open modelscan be accessed through https://github.com/THUDM/GLM-4-Voice andhttps://huggingface.co/THUDM/glm-4-voice-9b.</description><author>Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang</author><pubDate>Tue, 03 Dec 2024 17:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02612v1</guid></item><item><title>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</title><link>http://arxiv.org/abs/2412.02611v1</link><description>Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini1.5 Pro, and Reka Core, have expanded their capabilities to include vision andaudio modalities. While these models demonstrate impressive performance acrossa wide range of audio-visual applications, our proposed DeafTest reveals thatMLLMs often struggle with simple tasks humans find trivial: 1) determiningwhich of two sounds is louder, and 2) determining which of two sounds has ahigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, acomprehensive audio-visual benchmark designed to assess whether those MLLMs cantruly understand the audio-visual information. This benchmark encompasses 4,555carefully crafted problems, each incorporating text, visual, and audiocomponents. To successfully infer answers, models must effectively leverageclues from both visual and audio inputs. To ensure precise and objectiveevaluation of MLLM responses, we have structured the questions asmultiple-choice, eliminating the need for human evaluation or LLM-assistedassessment. We benchmark a series of closed-source and open-source models andsummarize the observations. By revealing the limitations of current models, weaim to provide useful insight for future dataset collection and modeldevelopment.</description><author>Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, Xiangyu Yue</author><pubDate>Tue, 03 Dec 2024 17:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02611v1</guid></item><item><title>AI-Driven Resource Allocation Framework for Microservices in Hybrid Cloud Platforms</title><link>http://arxiv.org/abs/2412.02610v1</link><description>The increasing demand for scalable, efficient resource management in hybridcloud environments has led to the exploration of AI-driven approaches fordynamic resource allocation. This paper presents an AI-driven framework forresource allocation among microservices in hybrid cloud platforms. Theframework employs reinforcement learning (RL)-based resource utilizationoptimization to reduce costs and improve performance. The framework integratesAI models with cloud management tools to respond to challenges of dynamicscaling and cost-efficient low-latency service delivery. The reinforcementlearning model continuously adjusts provisioned resources as required by themicroservices and predicts the future consumption trends to minimize bothunder- and over-provisioning of resources. Preliminary simulation resultsindicate that using AI in the provision of resources related to costs canreduce expenditure by up to 30-40% compared to manual provisioning andthreshold-based auto-scaling approaches. It is also estimated that theefficiency in resource utilization is expected to improve by 20%-30% with acorresponding latency cut of 15%-20% during the peak demand periods. This studycompares the AI-driven approach with existing static and rule-based resourceallocation methods, demonstrating the capability of this new model tooutperform them in terms of flexibility and real-time interests. The resultsindicate that reinforcement learning can make optimization of hybrid cloudplatforms even better, offering a 25-35% improvement in cost efficiency and thepower of scaling for microservice-based applications. The proposed framework isa strong and scalable solution to managing cloud resources in dynamic andperformance-critical environments.</description><author>Biman Barua, M. Shamim Kaiser</author><pubDate>Tue, 03 Dec 2024 17:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02610v1</guid></item><item><title>Wasserstein Markets for Differentially-Private Data</title><link>http://arxiv.org/abs/2412.02609v1</link><description>Data is an increasingly vital component of decision making processes acrossindustries. However, data access raises privacy concerns motivating the needfor privacy-preserving techniques such as differential privacy. Data marketsprovide a means to enable wider access as well as determine the appropriateprivacy-utility trade-off. Existing data market frameworks either require atrusted third party to perform computationally expensive valuations or areunable to capture the combinatorial nature of data value and do notendogenously model the effect of differential privacy. This paper addressesthese shortcomings by proposing a valuation mechanism based on the Wassersteindistance for differentially-private data, and corresponding procurementmechanisms by leveraging incentive mechanism design theory, for task-agnosticdata procurement, and task-specific procurement co-optimisation. The mechanismsare reformulated into tractable mixed-integer second-order cone programs, whichare validated with numerical studies.</description><author>Saurab Chhachhi, Fei Teng</author><pubDate>Tue, 03 Dec 2024 17:40:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02609v1</guid></item><item><title>BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving</title><link>http://arxiv.org/abs/2411.17404v2</link><description>LLMs exhibit advanced reasoning capabilities, offering the potential totransform natural language questions into mathematical models. However,existing open-source datasets in operations research domain lack detailedannotations of the modeling process, such as variable definitions, focusingsolely on objective values, which hinders reinforcement learning applications.To address this, we release the StructuredOR dataset, annotated withcomprehensive labels that capture the complete mathematical modeling process.We further propose BPP-Search, a algorithm that integrates reinforcementlearning into a tree-of-thought structure using Beam search, a Process rewardmodel, and a pairwise Preference algorithm. This approach enables efficientexploration of tree structures, avoiding exhaustive search while improvingaccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLPdatasets show that BPP-Search significantly outperforms state-of-the-artmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,enabling faster retrieval of correct solutions.</description><author>Teng Wang, Wing-Yin Yu, Zhenqi He, Zehua Liu, Xiongwei Han, Hailei Gong, Han Wu, Wei Shi, Ruifeng She, Fangzhou Zhu, Tao Zhong</author><pubDate>Tue, 03 Dec 2024 17:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17404v2</guid></item><item><title>Interpretable Company Similarity with Sparse Autoencoders</title><link>http://arxiv.org/abs/2412.02605v1</link><description>Determining company similarity is a vital task in finance, underpinninghedging, risk management, portfolio diversification, and more. Practitionersoften rely on sector and industry classifications to gauge similarity, such asSIC-codes and GICS-codes, the former being used by the U.S. Securities andExchange Commission (SEC), and the latter widely used by the investmentcommunity. Clustering embeddings of company descriptions has been proposed as apotential technique for determining company similarity, but the lack ofinterpretability in token embeddings poses a significant barrier to adoption inhigh-stakes contexts. Sparse Autoencoders have shown promise in enhancing theinterpretability of Large Language Models by decomposing LLM activations intointerpretable features. In this paper, we explore the use of SAE features inmeasuring company similarity and benchmark them against (1) SIC codes and (2)Major Group codes. We conclude that SAE features can reproduce and even surpasssector classifications in quantifying fundamental characteristics of companies,evaluated by the correlation of monthly returns, a proxy for similarity, andPnL from cointegration.</description><author>Marco Molinari, Vladimir Tregubiak, Victor Shao, Abhimanyu Pandey, Mateusz Mikolajczak, Sebastião Kuznetsov Ryder Torres Pereira</author><pubDate>Tue, 03 Dec 2024 17:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02605v1</guid></item><item><title>CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs</title><link>http://arxiv.org/abs/2412.02602v1</link><description>This paper analyzes the performance of Small Language Models (SLMs) andVision Language Models (VLMs) and evaluates the trade-off between modelperformance and carbon emissions across 4 essential tasks: Image Captioning,Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQLconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecturefamily are chosen and variants based on model size in terms of the number ofparameters, quantization level and fine-tuning parameters are evaluated. Themodel variant's performance and carbon emissions are calculated. To quantifythe trade-off between model performance and carbon emissions, we introduce anovel metric called CEGI (Carbon Efficient Gain Index). This metric representsthe carbon emission per unit percentage gain per million trainable parameters .This metric provides a normalized measure to compare model's efficiency interms of performance improvement relative to their environmental cost. Theexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieveperformance levels comparable to Large Language Models (LLMs) while producingsignificantly less carbon emissions. Our findings suggest that the marginalgains in accuracy from larger models do not justify the substantial increase incarbon emissions. Leveraging lower-bit quantization levels, the proposed metricfurther enhances energy efficiency without compromising performance. This studyhighlights balancing high performance and environmental sustainability. Itoffers a valuable metric for selecting models suitable forenvironmentally-friendly AI development.</description><author>Abhas Kumar, Kapil Pathak, Rajesh Kavuru, Prabhakar Srinivasan</author><pubDate>Tue, 03 Dec 2024 17:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02602v1</guid></item><item><title>MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images</title><link>http://arxiv.org/abs/2412.02601v1</link><description>Recent advances in Spatial Transcriptomics (ST) pair histology images withspatially resolved gene expression profiles, enabling predictions of geneexpression across different tissue locations based on image patches. This opensup new possibilities for enhancing whole slide image (WSI) prediction taskswith localized gene expression. However, existing methods fail to fullyleverage the interactions between different tissue locations, which are crucialfor accurate joint prediction. To address this, we introduce MERGE(Multi-faceted hiErarchical gRaph for Gene Expressions), which combines amulti-faceted hierarchical graph construction strategy with graph neuralnetworks (GNN) to improve gene expression predictions from WSIs. By clusteringtissue image patches based on both spatial and morphological features, andincorporating intra- and inter-cluster edges, our approach fosters interactionsbetween distant tissue locations during GNN learning. As an additionalcontribution, we evaluate different data smoothing techniques that arenecessary to mitigate artifacts in ST data, often caused by technicalimperfections. We advocate for adopting gene-aware smoothing methods that aremore biologically justified. Experimental results on gene expression predictionshow that our GNN method outperforms state-of-the-art techniques acrossmultiple metrics.</description><author>Aniruddha Ganguly, Debolina Chatterjee, Wentao Huang, Jie Zhang, Alisa Yurovsky, Travis Steele Johnson, Chao Chen</author><pubDate>Tue, 03 Dec 2024 17:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02601v1</guid></item><item><title>Class-wise Autoencoders Measure Classification Difficulty And Detect Label Mistakes</title><link>http://arxiv.org/abs/2412.02596v1</link><description>We introduce a new framework for analyzing classification datasets based onthe ratios of reconstruction errors between autoencoders trained on individualclasses. This analysis framework enables efficient characterization of datasetson the sample, class, and entire dataset levels. We define reconstruction errorratios (RERs) that probe classification difficulty and allow its decompositioninto (1) finite sample size and (2) Bayes error and decision-boundarycomplexity. Through systematic study across 19 popular visual datasets, we findthat our RER-based dataset difficulty probe strongly correlates with error ratefor state-of-the-art (SOTA) classification models. By interpreting sample-levelclassification difficulty as a label mistakenness score, we further find thatRERs achieve SOTA performance on mislabel detection tasks on hard datasetsunder symmetric and asymmetric label noise. Our code is publicly available athttps://github.com/voxel51/reconstruction-error-ratios.</description><author>Jacob Marks, Brent A. Griffin, Jason J. Corso</author><pubDate>Tue, 03 Dec 2024 17:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02596v1</guid></item><item><title>Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset</title><link>http://arxiv.org/abs/2412.02595v1</link><description>Recent English Common Crawl datasets like FineWeb-Edu and DCLM achievedsignificant benchmark gains via aggressive model-based filtering, but at thecost of removing 90% of data. This limits their suitability for long tokenhorizon training, such as 15T tokens for Llama 3.1. In this paper, we show howto achieve better trade-offs between accuracy and data quantity by acombination of classifier ensembling, synthetic data rephrasing, and reducedreliance on heuristic filters. When training 8B parameter models for 1T tokens,using a high-quality subset of our data improves MMLU by 5.6 over DCLM,demonstrating the efficacy of our methods for boosting accuracies over arelatively short token horizon. Furthermore, our full 6.3T token datasetmatches DCLM on MMLU, but contains four times more unique real tokens thanDCLM. This unlocks state-of-the-art training over a long token horizon: an 8Bparameter model trained for 15T tokens, of which 7.2T came from our dataset, isbetter than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5on average across ten diverse tasks. The dataset is available athttps://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html</description><author>Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Tue, 03 Dec 2024 17:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02595v1</guid></item><item><title>PrefixLLM: LLM-aided Prefix Circuit Design</title><link>http://arxiv.org/abs/2412.02594v1</link><description>Prefix circuits are fundamental components in digital adders, widely used indigital systems due to their efficiency in calculating carry signals.Synthesizing prefix circuits with minimized area and delay is crucial forenhancing the performance of modern computing systems. Recently, large languagemodels (LLMs) have demonstrated a surprising ability to perform text generationtasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.PrefixLLM transforms the prefix circuit synthesis task into a structured textgeneration problem, termed the Structured Prefix Circuit Representation (SPCR),and introduces an iterative framework to automatically and accurately generatevalid SPCRs. We further present a design space exploration (DSE) framework thatuses LLMs to iteratively search for area and delay optimized prefix circuits.Compared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under thesame delay constraint. This work highlights the use of LLMs in the synthesis ofarithmetic circuits, which can be transformed into the structured textgeneration.</description><author>Weihua Xiao, Venkata Sai Charan Putrevu, Raghu Vamshi Hemadri, Siddharth Garg, Ramesh Karri</author><pubDate>Tue, 03 Dec 2024 17:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02594v1</guid></item><item><title>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2412.02592v1</link><description>Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) byintegrating external knowledge to reduce hallucinations and incorporateup-to-date information without retraining. As an essential part of RAG,external knowledge bases are commonly built by extracting structured data fromunstructured PDF documents using Optical Character Recognition (OCR). However,given the imperfect prediction of OCR and the inherent non-uniformrepresentation of structured data, knowledge bases inevitably contain variousOCR noises. In this paper, we introduce OHRBench, the first benchmark forunderstanding the cascading impact of OCR on RAG systems. OHRBench includes 350carefully selected unstructured PDF documents from six real-world RAGapplication domains, along with Q&amp;As derived from multimodal elements indocuments, challenging existing OCR solutions used for RAG To better understandOCR's impact on RAG systems, we identify two primary types of OCR noise:Semantic Noise and Formatting Noise and apply perturbation to generate a set ofstructured data with varying degrees of each OCR noise. Using OHRBench, wefirst conduct a comprehensive evaluation of current OCR solutions and revealthat none is competent for constructing high-quality knowledge bases for RAGsystems. We then systematically evaluate the impact of these two noise typesand demonstrate the vulnerability of RAG systems. Furthermore, we discuss thepotential of employing Vision-Language Models (VLMs) without OCR in RAGsystems. Code: https://github.com/opendatalab/OHR-Bench</description><author>Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, Wentao Zhang</author><pubDate>Tue, 03 Dec 2024 17:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02592v1</guid></item><item><title>Denoising: A Powerful Building-Block for Imaging, Inverse Problems, and Machine Learning</title><link>http://arxiv.org/abs/2409.06219v4</link><description>Denoising, the process of reducing random fluctuations in a signal toemphasize essential patterns, has been a fundamental problem of interest sincethe dawn of modern scientific inquiry. Recent denoising techniques,particularly in imaging, have achieved remarkable success, nearing theoreticallimits by some measures. Yet, despite tens of thousands of research papers, thewide-ranging applications of denoising beyond noise removal have not been fullyrecognized. This is partly due to the vast and diverse literature, making aclear overview challenging. This paper aims to address this gap. We present a clarifying perspective ondenoisers, their structure, and desired properties. We emphasize the increasingimportance of denoising and showcase its evolution into an essential buildingblock for complex tasks in imaging, inverse problems, and machine learning.Despite its long history, the community continues to uncover unexpected andgroundbreaking uses for denoising, further solidifying its place as acornerstone of scientific and engineering practice.</description><author>Peyman Milanfar, Mauricio Delbracio</author><pubDate>Tue, 03 Dec 2024 17:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06219v4</guid></item><item><title>Filtered Direct Preference Optimization</title><link>http://arxiv.org/abs/2404.13846v4</link><description>Reinforcement learning from human feedback (RLHF) plays a crucial role inaligning language models with human preferences. While the significance ofdataset quality is generally recognized, explicit investigations into itsimpact within the RLHF framework, to our knowledge, have been limited. Thispaper addresses the issue of text quality within the preference dataset byfocusing on direct preference optimization (DPO), an increasingly adoptedreward-model-free RLHF method. We confirm that text quality significantlyinfluences the performance of models optimized with DPO more than thoseoptimized with reward-model-based RLHF. Building on this new insight, wepropose an extension of DPO, termed filtered direct preference optimization(fDPO). fDPO uses a trained reward model to monitor the quality of texts withinthe preference dataset during DPO training. Samples of lower quality arediscarded based on comparisons with texts generated by the model beingoptimized, resulting in a more accurate dataset. Experimental resultsdemonstrate that fDPO enhances the final model performance. Our code isavailable at https://github.com/CyberAgentAILab/filtered-dpo.</description><author>Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, Kaito Ariu</author><pubDate>Tue, 03 Dec 2024 17:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13846v4</guid></item><item><title>LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting</title><link>http://arxiv.org/abs/2412.00177v2</link><description>We introduce LumiNet, a novel architecture that leverages generative modelsand latent intrinsic representations for effective lighting transfer. Given asource image and a target lighting image, LumiNet synthesizes a relit versionof the source scene that captures the target's lighting. Our approach makes twokey contributions: a data curation strategy from the StyleGAN-based relightingmodel for our training, and a modified diffusion-based ControlNet thatprocesses both latent intrinsic properties from the source image and latentextrinsic properties from the target image. We further improve lightingtransfer through a learned adaptor (MLP) that injects the target's latentextrinsic properties via cross-attention and fine-tuning. Unlike traditional ControlNet, which generates images with conditional mapsfrom a single scene, LumiNet processes latent representations from twodifferent images - preserving geometry and albedo from the source whiletransferring lighting characteristics from the target. Experiments demonstratethat our method successfully transfers complex lighting phenomena includingspecular highlights and indirect illumination across scenes with varyingspatial layouts and materials, outperforming existing approaches on challengingindoor scenes using only images as input.</description><author>Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad</author><pubDate>Tue, 03 Dec 2024 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00177v2</guid></item><item><title>MedTet: An Online Motion Model for 4D Heart Reconstruction</title><link>http://arxiv.org/abs/2412.02589v1</link><description>We present a novel approach to reconstruction of 3D cardiac motion fromsparse intraoperative data. While existing methods can accurately reconstruct3D organ geometries from full 3D volumetric imaging, they cannot be used duringsurgical interventions where usually limited observed data, such as a few 2Dframes or 1D signals, is available in real-time. We propose a versatileframework for reconstructing 3D motion from such partial data. It discretizesthe 3D space into a deformable tetrahedral grid with signed distance values,providing implicit unlimited resolution while maintaining explicit control overmotion dynamics. Given an initial 3D model reconstructed from pre-operativefull volumetric data, our system, equipped with an universal observationencoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few2D MRI slices or even 1D signals. Extensive experiments on cardiac interventionscenarios demonstrate our ability to generate plausible and anatomicallyconsistent 3D motion reconstructions from various sparse real-timeobservations, highlighting its potential for multimodal cardiac imaging. Ourcode and model will be made available at https://github.com/Scalsol/MedTet.</description><author>Yihong Chen, Jiancheng Yang, Deniz Sayin Mercadier, Hieu Le, Pascal Fua</author><pubDate>Tue, 03 Dec 2024 17:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02589v1</guid></item><item><title>Explainable CTR Prediction via LLM Reasoning</title><link>http://arxiv.org/abs/2412.02588v1</link><description>Recommendation Systems have become integral to modern user experiences, butlack transparency in their decision-making processes. Existing explainablerecommendation methods are hindered by reliance on a post-hoc paradigm, whereinexplanation generators are trained independently of the underlying recommendermodels. This paradigm necessitates substantial human effort in dataconstruction and raises concerns about explanation reliability. In this paper,we present ExpCTR, a novel framework that integrates large language model basedexplanation generation directly into the CTR prediction process. Inspired byrecent advances in reinforcement learning, we employ two carefully designedreward mechanisms, LC alignment, which ensures explanations reflect userintentions, and IC alignment, which maintains consistency with traditionalID-based CTR models. Our approach incorporates an efficient training paradigmwith LoRA and a three-stage iterative process. ExpCTR circumvents the need forextensive explanation datasets while fostering synergy between CTR predictionand explanation generation. Experimental results demonstrate that ExpCTRsignificantly enhances both recommendation accuracy and interpretability acrossthree real-world datasets.</description><author>Xiaohan Yu, Li Zhang, Chong Chen</author><pubDate>Tue, 03 Dec 2024 17:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02588v1</guid></item><item><title>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</title><link>http://arxiv.org/abs/2411.14494v2</link><description>A facial morph is an image created by combining two face images pertaining totwo distinct identities. Face demorphing inverts the process and tries torecover the original images constituting a facial morph. While morph attackdetection (MAD) techniques can be used to flag morph images, they do notdivulge any visual information about the faces used to create them. Demorphinghelps address this problem. Existing demorphing techniques are either veryrestrictive (assume identities during testing) or produce feeble outputs (bothoutputs look very similar). In this paper, we overcome these issues byproposing dc-GAN, a novel GAN-based demorphing method conditioned on the morphimages. Our method overcomes morph-replication and produces high qualityreconstructions of the bonafide images used to create the morphs. Moreover, ourmethod is highly generalizable across demorphing paradigms(differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs andMorDiff datasets to showcase the efficacy of our method.</description><author>Nitish Shukla, Arun Ross</author><pubDate>Tue, 03 Dec 2024 17:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14494v2</guid></item><item><title>Factored space models: Towards causality between levels of abstraction</title><link>http://arxiv.org/abs/2412.02579v1</link><description>Causality plays an important role in understanding intelligent behavior, andthere is a wealth of literature on mathematical models for causality, most ofwhich is focused on causal graphs. Causal graphs are a powerful tool for a widerange of applications, in particular when the relevant variables are known andat the same level of abstraction. However, the given variables can also beunstructured data, like pixels of an image. Meanwhile, the causal variables,such as the positions of objects in the image, can be arbitrary deterministicfunctions of the given variables. Moreover, the causal variables may form ahierarchy of abstractions, in which the macro-level variables are deterministicfunctions of the micro-level variables. Causal graphs are limited when it comesto modeling this kind of situation. In the presence of deterministicrelationships there is generally no causal graph that satisfies both the Markovcondition and the faithfulness condition. We introduce factored space models asan alternative to causal graphs which naturally represent both probabilisticand deterministic relationships at all levels of abstraction. Moreover, weintroduce structural independence and establish that it is equivalent tostatistical independence in every distribution that factorizes over thefactored space. This theorem generalizes the classical soundness andcompleteness theorem for d-separation.</description><author>Scott Garrabrant, Matthias Georg Mayer, Magdalena Wache, Leon Lang, Sam Eisenstat, Holger Dell</author><pubDate>Tue, 03 Dec 2024 17:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02579v1</guid></item><item><title>Private Linear Regression with Differential Privacy and PAC Privacy</title><link>http://arxiv.org/abs/2412.02578v1</link><description>Linear regression is a fundamental tool for statistical analysis, which hasmotivated the development of linear regression methods that satisfy provableprivacy guarantees so that the learned model reveals little about any one datapoint used to construct it. Most existing privacy-preserving linear regressionmethods rely on the well-established framework of differential privacy, whilethe newly proposed PAC Privacy has not yet been explored in this context. Inthis paper, we systematically compare linear regression models trained withdifferential privacy and PAC privacy across three real-world datasets,observing several key findings that impact the performance ofprivacy-preserving linear regression.</description><author>Hillary Yang</author><pubDate>Tue, 03 Dec 2024 17:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02578v1</guid></item><item><title>Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients</title><link>http://arxiv.org/abs/2409.05305v2</link><description>It has been demonstrated in many scientific fields that artificial neuralnetworks like autoencoders or Siamese networks encode meaningful concepts intheir latent spaces. However, there does not exist a comprehensive frameworkfor retrieving this information in a human-readable form without priorknowledge. In order to extract these concepts, we introduce a framework forfinding closed-form interpretations of neurons in latent spaces of artificialneural networks. The interpretation framework is based on embedding trainedneural networks into an equivalence class of functions that encode the sameconcept. We interpret these neural networks by finding an intersection betweenthe equivalence class and human-readable equations defined by a symbolic searchspace. The approach is demonstrated by retrieving invariants of matrices andconserved quantities of dynamical systems from latent spaces of Siamese neuralnetworks.</description><author>Zakaria Patel, Sebastian J. Wetzel</author><pubDate>Tue, 03 Dec 2024 17:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05305v2</guid></item><item><title>Copy-Move Forgery Detection and Question Answering for Remote Sensing Image</title><link>http://arxiv.org/abs/2412.02575v1</link><description>This paper introduces the task of Remote Sensing Copy-Move Question Answering(RSCMQA). Unlike traditional Remote Sensing Visual Question Answering (RSVQA),RSCMQA focuses on interpreting complex tampering scenarios and inferringrelationships between objects. Based on the practical needs of national defensesecurity and land resource monitoring, we have developed an accurate andcomprehensive global dataset for remote sensing image copy-move questionanswering, named RS-CMQA-2.1M. These images were collected from 29 differentregions across 14 countries. Additionally, we have refined a balanced dataset,RS-CMQA-B, to address the long-standing issue of long-tail data in the remotesensing field. Furthermore, we propose a region-discriminative guidedmultimodal CMQA model, which enhances the accuracy of answering questions abouttampered images by leveraging prompt about the differences and connectionsbetween the source and tampered domains. Extensive experiments demonstrate thatour method provides a stronger benchmark for RS-CMQA compared to general VQAand RSVQA models. Our dataset and code are available athttps://github.com/shenyedepisa/RSCMQA.</description><author>Ze Zhang, Enyuan Zhao, Ziyi Wan, Jie Nie, Xinyue Liang, Lei Huang</author><pubDate>Tue, 03 Dec 2024 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02575v1</guid></item><item><title>Generating Critical Scenarios for Testing Automated Driving Systems</title><link>http://arxiv.org/abs/2412.02574v1</link><description>Autonomous vehicles (AVs) have demonstrated significant potential inrevolutionizing transportation, yet ensuring their safety and reliabilityremains a critical challenge, especially when exposed to dynamic andunpredictable environments. Real-world testing of an Autonomous Driving System(ADS) is both expensive and risky, making simulation-based testing a preferredapproach. In this paper, we propose AVASTRA, a Reinforcement Learning(RL)-based approach to generate realistic critical scenarios for testing ADSsin simulation environments. To capture the complexity of driving scenarios,AVASTRA comprehensively represents the environment by both the internal statesof an ADS under-test (e.g., the status of the ADS's core components, speed, oracceleration) and the external states of the surrounding factors in thesimulation environment (e.g., weather, traffic flow, or road condition).AVASTRA trains the RL agent to effectively configure the simulation environmentthat places the AV in dangerous situations and potentially leads it tocollisions. We introduce a diverse set of actions that allows the RL agent tosystematically configure both environmental conditions and trafficparticipants. Additionally, based on established safety requirements, weenforce heuristic constraints to ensure the realism and relevance of thegenerated test scenarios. AVASTRA is evaluated on two popular simulation mapswith four different road configurations. Our results show AVASTRA's ability tooutperform the state-of-the-art approach by generating 30% to 115% morecollision scenarios. Compared to the baseline based on Random Search, AVASTRAachieves up to 275% better performance. These results highlight theeffectiveness of AVASTRA in enhancing the safety testing of AVs throughrealistic comprehensive critical scenario generation.</description><author>Trung-Hieu Nguyen, Truong-Giang Vuong, Hong-Nam Duong, Son Nguyen, Hieu Dinh Vo, Toshiaki Aoki, Thu-Trang Nguyen</author><pubDate>Tue, 03 Dec 2024 16:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02574v1</guid></item><item><title>Remote Sensing Temporal Vision-Language Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2412.02573v1</link><description>Temporal image analysis in remote sensing has traditionally centered onchange detection, which identifies regions of change between images captured atdifferent times. However, change detection remains limited by its focus onvisual-level interpretation, often lacking contextual or descriptiveinformation. The rise of Vision-Language Models (VLMs) has introduced a newdimension to remote sensing temporal image analysis by integrating visualinformation with natural language, creating an avenue for advancedinterpretation of temporal image changes. Remote Sensing Temporal VLMs(RSTVLMs) allow for dynamic interactions, generating descriptive captions,answering questions, and providing a richer semantic understanding of temporalimages. This temporal vision-language capability is particularly valuable forcomplex remote sensing applications, where higher-level insights are crucial.This paper comprehensively reviews the progress of RSTVLM research, with afocus on the latest VLM applications for temporal image analysis. We categorizeand discuss core methodologies, datasets, and metrics, highlight recentadvances in temporal vision-language tasks, and outline key challenges andfuture directions for research in this emerging field. This survey fills acritical gap in the literature by providing an integrated overview of RSTVLM,offering a foundation for further advancements in remote sensing temporal imageunderstanding. We will keep tracing related works at\url{https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM}</description><author>Chenyang Liu, Jiafan Zhang, Keyan Chen, Man Wang, Zhengxia Zou, Zhenwei Shi</author><pubDate>Tue, 03 Dec 2024 16:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02573v1</guid></item><item><title>TAB-Fields: A Maximum Entropy Framework for Mission-Aware Adversarial Planning</title><link>http://arxiv.org/abs/2412.02570v1</link><description>Autonomous agents operating in adversarial scenarios face a fundamentalchallenge: while they may know their adversaries' high-level objectives, suchas reaching specific destinations within time constraints, the exact policiesthese adversaries will employ remain unknown. Traditional approaches addressthis challenge by treating the adversary's state as a partially observableelement, leading to a formulation as a Partially Observable Markov DecisionProcess (POMDP). However, the induced belief-space dynamics in a POMDP requireknowledge of the system's transition dynamics, which, in this case, depend onthe adversary's unknown policy. Our key observation is that while anadversary's exact policy is unknown, their behavior is necessarily constrainedby their mission objectives and the physical environment, allowing us tocharacterize the space of possible behaviors without assuming specificpolicies. In this paper, we develop Task-Aware Behavior Fields (TAB-Fields), arepresentation that captures adversary state distributions over time bycomputing the most unbiased probability distribution consistent with knownconstraints. We construct TAB-Fields by solving a constrained optimizationproblem that minimizes additional assumptions about adversary behavior beyondmission and environmental requirements. We integrate TAB-Fields with standardplanning algorithms by introducing TAB-conditioned POMCP, an adaptation ofPartially Observable Monte Carlo Planning. Through experiments in simulationwith underwater robots and hardware implementations with ground robots, wedemonstrate that our approach achieves superior performance compared tobaselines that either assume specific adversary policies or neglect missionconstraints altogether. Evaluation videos and code are available athttps://tab-fields.github.io.</description><author>Gokul Puthumanaillam, Jae Hyuk Song, Nurzhan Yesmagambet, Shinkyu Park, Melkior Ornik</author><pubDate>Tue, 03 Dec 2024 16:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02570v1</guid></item><item><title>FullStack Bench: Evaluating LLMs as Full Stack Coders</title><link>http://arxiv.org/abs/2412.00535v2</link><description>As the capabilities of code large language models (LLMs) continue to expand,their applications across diverse code intelligence domains are rapidlyincreasing. However, most existing datasets only evaluate limited applicationdomains. To address this gap, we have developed a comprehensive code evaluationdataset FullStack Bench focusing on full-stack programming, which encompasses awide range of application domains (e.g., basic programming, data analysis,software engineering, mathematics, and machine learning). Besides, to assessmultilingual programming capabilities, in FullStack Bench, we design real-worldinstructions and corresponding unit test cases from 16 widely-used programminglanguages to reflect real-world usage scenarios rather than simpletranslations. Moreover, we also release an effective code sandbox executiontool (i.e., SandboxFusion) supporting various programming languages andpackages to evaluate the performance of our FullStack Bench efficiently.Comprehensive experimental results on our FullStack Bench demonstrate thenecessity and effectiveness of our FullStack Bench and SandboxFusion.</description><author>Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Jing Mai, Ge Zhang, Wenhao Huang, Kai Shen, Liang Xiang</author><pubDate>Tue, 03 Dec 2024 16:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00535v2</guid></item><item><title>Segmentation of Coronary Artery Stenosis in X-ray Angiography using Mamba Models</title><link>http://arxiv.org/abs/2412.02568v1</link><description>Coronary artery disease stands as one of the primary contributors to globalmortality rates. The automated identification of coronary artery stenosis fromX-ray images plays a critical role in the diagnostic process for coronary heartdisease. This task is challenging due to the complex structure of coronaryarteries, intrinsic noise in X-ray images, and the fact that stenotic coronaryarteries appear narrow and blurred in X-ray angiographies. This study employsfive different variants of the Mamba-based model and one variant of the SwinTransformer-based model, primarily based on the U-Net architecture, for thelocalization of stenosis in Coronary artery disease. Our best results showed anF1 score of 68.79% for the U-Mamba BOT model, representing an 11.8% improvementover the semi-supervised approach.</description><author>Ali Rostami, Fatemeh Fouladi, Hedieh Sajedi</author><pubDate>Tue, 03 Dec 2024 16:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02568v1</guid></item><item><title>SJTU:Spatial judgments in multimodal models towards unified segmentation through coordinate detection</title><link>http://arxiv.org/abs/2412.02565v1</link><description>Despite advances in vision-language understanding, implementing imagesegmentation within multimodal architectures remains a fundamental challenge inmodern artificial intelligence systems. Existing vision-language models, whichprimarily rely on backbone architectures or CLIP-based embedding learning,demonstrate inherent limitations in fine-grained spatial localization andoperational capabilities. This paper introduces SJTU: Spatial Judgments inmultimodal models - Towards Unified segmentation through coordinate detection,a novel framework that leverages spatial coordinate understanding to bridgevision-language interaction and precise segmentation, enabling accurate targetidentification through natural language instructions. The framework proposes anovel approach for integrating segmentation techniques with vision-languagemodels based on multimodal spatial inference. By leveraging normalizedcoordinate detection for bounding boxes and translating it into actionablesegmentation outputs, we explore the possibility of integrating multimodalspatial and language representations. Based on the proposed technical approach,the framework demonstrates superior performance on various benchmark datasetsas well as accurate object segmentation. Results on the COCO 2017 dataset forgeneral object detection and Pascal VOC datasets for semantic segmentationdemonstrate the generalization capabilities of the framework.</description><author>Joongwon Chae, Zhenyu Wang, Peiwu Qin</author><pubDate>Tue, 03 Dec 2024 16:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02565v1</guid></item><item><title>Semantic Tokens in Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2412.02563v1</link><description>Retrieval-Augmented Generation (RAG) architectures have recently garneredsignificant attention for their ability to improve truth grounding andcoherence in natural language processing tasks. However, the reliability of RAGsystems in producing accurate answers diminishes as the volume of data theyaccess increases. Even with smaller datasets, these systems occasionally failto address simple queries. This issue arises from their dependence onstate-of-the-art large language models (LLMs), which can introduce uncertaintyinto the system's outputs. In this work, I propose a novel Comparative RAGsystem that introduces an evaluator module to bridge the gap betweenprobabilistic RAG systems and deterministically verifiable responses. Theevaluator compares external recommendations with the retrieved document chunks,adding a decision-making layer that enhances the system's reliability. Thisapproach ensures that the chunks retrieved are both semantically relevant andlogically consistent with deterministic insights, thereby improving theaccuracy and overall efficiency of RAG systems. This framework paves the wayfor more reliable and scalable question-answering applications in domainsrequiring high precision and verifiability.</description><author>Joel Suro</author><pubDate>Tue, 03 Dec 2024 16:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02563v1</guid></item><item><title>Patent-CR: A Dataset for Patent Claim Revision</title><link>http://arxiv.org/abs/2412.02549v1</link><description>This paper presents Patent-CR, the first dataset created for the patent claimrevision task in English. It includes both initial patent applications rejectedby patent examiners and the final granted versions. Unlike normal text revisiontasks that predominantly focus on enhancing sentence quality, such as grammarcorrection and coherence improvement, patent claim revision aims at ensuringthe claims meet stringent legal criteria. These criteria are beyond novelty andinventiveness, including clarity of scope, technical accuracy, languageprecision, and legal robustness. We assess various large language models (LLMs)through professional human evaluation, including general LLMs with differentsizes and architectures, text revision models, and domain-specific models. Ourresults indicate that LLMs often bring ineffective edits that deviate from thetarget revisions. In addition, domain-specific models and the method offine-tuning show promising results. Notably, GPT-4 outperforms other testedLLMs, but further revisions are still necessary to reach the examinationstandard. Furthermore, we demonstrate the inconsistency between automated andhuman evaluation results, suggesting that GPT-4-based automated evaluation hasthe highest correlation with human judgment. This dataset, along with ourpreliminary empirical research, offers invaluable insights for furtherexploration in patent claim revision.</description><author>Lekang Jiang, Pascal A Scherz, Stephan Goetz</author><pubDate>Tue, 03 Dec 2024 16:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02549v1</guid></item><item><title>Plug-and-Play Half-Quadratic Splitting for Ptychography</title><link>http://arxiv.org/abs/2412.02548v1</link><description>Ptychography is a coherent diffraction imaging method that uses phaseretrieval techniques to reconstruct complex-valued images. It achieves this bysequentially illuminating overlapping regions of a sample with a coherent beamand recording the diffraction pattern. Although this addresses traditionalimaging system challenges, it is computationally intensive and highly sensitiveto noise, especially with reduced illumination overlap. Data-drivenregularisation techniques have been applied in phase retrieval to improvereconstruction quality. In particular, plug-and-play (PnP) offers flexibilityby integrating data-driven denoisers as implicit priors. In this work, wepropose a half-quadratic splitting framework for using PnP and otherdata-driven priors for ptychography. We evaluate our method both on naturalimages and real test objects to validate its effectiveness for ptychographicimage reconstruction.</description><author>Alexander Denker, Johannes Hertrich, Zeljko Kereta, Silvia Cipiccia, Ecem Erin, Simon Arridge</author><pubDate>Tue, 03 Dec 2024 16:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02548v1</guid></item><item><title>Fractional Order Distributed Optimization</title><link>http://arxiv.org/abs/2412.02546v1</link><description>Distributed optimization is fundamental to modern machine learningapplications like federated learning, but existing methods often struggle withill-conditioned problems and face stability-versus-speed tradeoffs. Weintroduce fractional order distributed optimization (FrODO); atheoretically-grounded framework that incorporates fractional-order memoryterms to enhance convergence properties in challenging optimization landscapes.Our approach achieves provable linear convergence for any strongly connectednetwork. Through empirical validation, our results suggest that FrODO achievesup to 4 times faster convergence versus baselines on ill-conditioned problemsand 2-3 times speedup in federated neural network training, while maintainingstability and theoretical guarantees.</description><author>Andrei Lixandru, Marcel van Gerven, Sergio Pequito</author><pubDate>Tue, 03 Dec 2024 16:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02546v1</guid></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>http://arxiv.org/abs/2411.02337v2</link><description>Large language models (LLMs) have shown remarkable potential as autonomousagents, particularly in web-based tasks. However, existing LLM web agentsheavily rely on expensive proprietary LLM APIs, while open LLMs lack thenecessary decision-making capabilities. This paper introduces WebRL, aself-evolving online curriculum reinforcement learning framework designed totrain high-performance web agents using open LLMs. WebRL addresses three keychallenges in building LLM web agents, including the scarcity of trainingtasks, sparse feedback signals, and policy distribution drift in onlinelearning. Specifically, WebRL incorporates 1) a self-evolving curriculum thatgenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervisedreward model (ORM), and 3) adaptive reinforcement learning strategies to ensureconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4models into proficient web agents. On WebArena-Lite, WebRL improves the successrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.These open models significantly surpass the performance of GPT-4-Turbo (17.6%)and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trainedon open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL'seffectiveness in bridging the gap between open and proprietary LLM-based webagents, paving the way for more accessible and powerful autonomous webinteraction systems.</description><author>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong</author><pubDate>Tue, 03 Dec 2024 16:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02337v2</guid></item><item><title>ShadowHack: Hacking Shadows via Luminance-Color Divide and Conquer</title><link>http://arxiv.org/abs/2412.02545v1</link><description>Shadows introduce challenges such as reduced brightness, texturedeterioration, and color distortion in images, complicating a holisticsolution. This study presents \textbf{ShadowHack}, a divide-and-conquerstrategy that tackles these complexities by decomposing the original task intoluminance recovery and color remedy. To brighten shadow regions and repair thecorrupted textures in the luminance space, we customize LRNet, a U-shapednetwork with a rectified outreach attention module, to enhance informationinteraction and recalibrate contaminated attention maps. With luminancerecovered, CRNet then leverages cross-attention mechanisms to revive vibrantcolors, producing visually compelling results. Extensive experiments onmultiple datasets are conducted to demonstrate the superiority of ShadowHackover existing state-of-the-art solutions both quantitatively and qualitatively,highlighting the effectiveness of our design. Our code will be made publiclyavailable at https://github.com/lime-j/ShadowHack</description><author>Jin Hu, Mingjia Li, Xiaojie Guo</author><pubDate>Tue, 03 Dec 2024 16:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02545v1</guid></item><item><title>Unveiling Concept Attribution in Diffusion Models</title><link>http://arxiv.org/abs/2412.02542v1</link><description>Diffusion models have shown remarkable abilities in generating realistic andhigh-quality images from text prompts. However, a trained model remainsblack-box; little do we know about the role of its components in exhibiting aconcept such as objects or styles. Recent works employ causal tracing tolocalize layers storing knowledge in generative models without showing howthose layers contribute to the target concept. In this work, we approach themodel interpretability problem from a more general perspective and pose aquestion: \textit{``How do model components work jointly to demonstrateknowledge?''}. We adapt component attribution to decompose diffusion models,unveiling how a component contributes to a concept. Our framework allowseffective model editing, in particular, we can erase a concept from diffusionmodels by removing positive components while remaining knowledge of otherconcepts. Surprisingly, we also show there exist components that contributenegatively to a concept, which has not been discovered in the knowledgelocalization approach. Experimental results confirm the role of positive andnegative components pinpointed by our framework, depicting a complete view ofinterpreting generative models. Our code is available at\url{https://github.com/mail-research/CAD-attribution4diffusion}</description><author>Quang H. Nguyen, Hoang Phan, Khoa D. Doan</author><pubDate>Tue, 03 Dec 2024 16:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02542v1</guid></item><item><title>Graph-Powered Defense: Controller Area Network Intrusion Detection for Unmanned Aerial Vehicles</title><link>http://arxiv.org/abs/2412.02539v1</link><description>The network of services, including delivery, farming, and environmentalmonitoring, has experienced exponential expansion in the past decade withUnmanned Aerial Vehicles (UAVs). Yet, UAVs are not robust enough againstcyberattacks, especially on the Controller Area Network (CAN) bus. The CAN busis a general-purpose vehicle-bus standard to enable microcontrollers andin-vehicle computers to interact, primarily connecting different ElectronicControl Units (ECUs). In this study, we focus on solving some of the mostcritical security weaknesses in UAVs by developing a novel graph-basedintrusion detection system (IDS) leveraging the Uncomplicated Application-levelVehicular Communication and Networking (UAVCAN) protocol. First, we decode CANmessages based on UAVCAN protocol specification; second, we present acomprehensive method of transforming tabular UAVCAN messages into graphstructures. Lastly, we apply various graph-based machine learning models fordetecting cyber-attacks on the CAN bus, including graph convolutional neuralnetworks (GCNNs), graph attention networks (GATs), Graph Sample and AggregateNetworks (GraphSAGE), and graph structure-based transformers. Our findings showthat inductive models such as GATs, GraphSAGE, and graph-based transformers canachieve competitive and even better accuracy than transductive models likeGCNNs in detecting various types of intrusions, with minimum information onprotocol specification, thus providing a generic robust solution for CAN bussecurity for the UAVs. We also compared our results with baseline single-layerLong Short-Term Memory (LSTM) and found that all our graph-based models performbetter without using any decoded features based on the UAVCAN protocol,highlighting higher detection performance with protocol-independent capability.</description><author>Reek Majumder, Gurcan Comert, David Werth, Adrian Gale, Mashrur Chowdhury, M Sabbir Salek</author><pubDate>Tue, 03 Dec 2024 16:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02539v1</guid></item><item><title>Tomographic SAR Reconstruction for Forest Height Estimation</title><link>http://arxiv.org/abs/2412.00903v2</link><description>Tree height estimation serves as an important proxy for biomass estimation inecological and forestry applications. While traditional methods such asphotogrammetry and Light Detection and Ranging (LiDAR) offer accurate heightmeasurements, their application on a global scale is often cost-prohibitive andlogistically challenging. In contrast, remote sensing techniques, particularly3D tomographic reconstruction from Synthetic Aperture Radar (SAR) imagery,provide a scalable solution for global height estimation. SAR images have beenused in earth observation contexts due to their ability to work in allweathers, unobscured by clouds. In this study, we use deep learning to estimateforest canopy height directly from 2D Single Look Complex (SLC) images, aderivative of SAR. Our method attempts to bypass traditional tomographic signalprocessing, potentially reducing latency from SAR capture to end product. Wealso quantify the impact of varying numbers of SLC images on height estimationaccuracy, aiming to inform future satellite operations and optimize datacollection strategies. Compared to full tomographic processing combined withdeep learning, our minimal method (partial processing + deep learning) fallsshort, with an error 16-21\% higher, highlighting the continuing relevance ofgeometric signal processing.</description><author>Grace Colverd, Jumpei Takami, Laura Schade, Karol Bot, Joseph A. Gallego-Mejia</author><pubDate>Tue, 03 Dec 2024 16:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00903v2</guid></item><item><title>On the Privacy, Security, and Trustworthy for Distributed Wireless Large AI Model (WLAM)</title><link>http://arxiv.org/abs/2412.02538v1</link><description>Combining wireless communication with large artificial intelligence (AI)models can open up a myriad of novel application scenarios. In sixth generation(6G) networks, ubiquitous communication and computing resources allow large AImodels to serve democratic large AI models-related services to enable real-timeapplications like autonomous vehicles, smart cities, and Internet of Things(IoT) ecosystems. However, the security considerations and sustainablecommunication resources limit the deployment of large AI models overdistributed wireless networks. This paper provides a comprehensive overview ofprivacy, security, and trustworthy for distributed wireless large AI model(WLAM). In particular, the detailed privacy and security are analysis fordistributed WLAM is fist revealed. The classifications and theoretical findingsabout privacy and security in distributed WLAM are discussed. Then thetrustworthy and ethics for implementing distributed WLAM are described.Finally, the comprehensive applications of distributed WLAM is provided in theaspect of electromagnetic signal processing.</description><author>Zhaohui Yang, Wei Xu, Le Liang, Yuanhao Cui, Zhijin Qin, Merouane Debbah</author><pubDate>Tue, 03 Dec 2024 16:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02538v1</guid></item><item><title>Comparative Analysis of Resource-Efficient CNN Architectures for Brain Tumor Classification</title><link>http://arxiv.org/abs/2411.15596v2</link><description>Accurate brain tumor classification in MRI images is critical for timelydiagnosis and treatment planning. While deep learning models like ResNet-18,VGG-16 have shown high accuracy, they often come with increased complexity andcomputational demands. This study presents a comparative analysis of effectiveyet simple Convolutional Neural Network (CNN) architecture and pre-trainedResNet18, and VGG16 model for brain tumor classification using two publiclyavailable datasets: Br35H:: Brain Tumor Detection 2020 and Brain Tumor MRIDataset. The custom CNN architecture, despite its lower complexity,demonstrates competitive performance with the pre-trained ResNet18 and VGG16models. In binary classification tasks, the custom CNN achieved an accuracy of98.67% on the Br35H dataset and 99.62% on the Brain Tumor MRI Dataset. Formulti-class classification, the custom CNN, with a slight architecturalmodification, achieved an accuracy of 98.09%, on the Brain Tumor MRI Dataset.Comparatively, ResNet18 and VGG16 maintained high performance levels, but thecustom CNNs provided a more computationally efficient alternative.Additionally,the custom CNNs were evaluated using few-shot learning (0, 5, 10,15, 20, 40, and 80 shots) to assess their robustness, achieving notableaccuracy improvements with increased shots. This study highlights the potentialof well-designed, less complex CNN architectures as effective andcomputationally efficient alternatives to deeper, pre-trained models formedical imaging tasks, including brain tumor classification. This studyunderscores the potential of custom CNNs in medical imaging tasks andencourages further exploration in this direction.</description><author>Md Ashik Khan, Rafath Bin Zafar Auvee</author><pubDate>Tue, 03 Dec 2024 16:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15596v2</guid></item><item><title>Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization</title><link>http://arxiv.org/abs/2412.02535v1</link><description>Adversarial attacks pose significant challenges in many machine learningapplications, particularly in the setting of distributed training and federatedlearning, where malicious agents seek to corrupt the training process with thegoal of jeopardizing and compromising the performance and reliability of thefinal models. In this paper, we address the problem of robust federatedlearning in the presence of such attacks by formulating the training task as abi-level optimization problem. We conduct a theoretical analysis of theresilience of consensus-based bi-level optimization (CB$^2$O), an interactingmulti-particle metaheuristic optimization method, in adversarial settings.Specifically, we provide a global convergence analysis of CB$^2$O in mean-fieldlaw in the presence of malicious agents, demonstrating the robustness ofCB$^2$O against a diverse range of attacks. Thereby, we offer insights into howspecific hyperparameter choices enable to mitigate adversarial effects. On thepractical side, we extend CB$^2$O to the clustered federated learning settingby proposing FedCB$^2$O, a novel interacting multi-particle system, and designa practical algorithm that addresses the demands of real-world applications.Extensive experiments demonstrate the robustness of the FedCB$^2$O algorithmagainst label-flipping attacks in decentralized clustered federated learningscenarios, showcasing its effectiveness in practical contexts.</description><author>Nicolás García Trillos, Aditya Kumar Akash, Sixu Li, Konstantin Riedl, Yuhua Zhu</author><pubDate>Tue, 03 Dec 2024 16:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02535v1</guid></item><item><title>Grid-augmented vision: A simple yet effective approach for enhanced spatial understanding in multi-modal agents</title><link>http://arxiv.org/abs/2411.18270v2</link><description>Recent advances in multimodal models have demonstrated impressivecapabilities in object recognition and scene understanding. However, thesemodels often struggle with precise spatial localization - a critical capabilityfor real-world applications. Inspired by how humans use grid-based referenceslike chess boards and maps, we propose introducing explicit visual positionencoding through a simple grid overlay approach. By adding a 9x9 black gridpattern onto input images, our method provides visual spatial guidanceanalogous to how positional encoding works in transformers, but in an explicit,visual form. Experiments on the COCO 2017 dataset demonstrate that our grid-based approachachieves significant improvements in localization accuracy, with a 107.4%increase in IoU (from 0.27 to 0.56) and a 194.4% improvement in GIoU (from 0.18to 0.53) compared to baseline performance. Through attention visualizationanalysis, we show how this visual position encoding helps models better groundspatial relationships. Our method's simplicity and effectiveness make itparticularly valuable for applications requiring accurate spatial reasoning,such as robotic manipulation, medical imaging, and autonomous navigation.</description><author>Joongwon Chae, Zhenyu Wang, Lian Zhang, Dongmei Yu, Peiwu Qin</author><pubDate>Tue, 03 Dec 2024 16:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18270v2</guid></item><item><title>Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Markov Decision Processes</title><link>http://arxiv.org/abs/2410.10578v5</link><description>Average-reward Markov decision processes (MDPs) provide a foundationalframework for sequential decision-making under uncertainty. However,average-reward MDPs have remained largely unexplored in reinforcement learning(RL) settings, with the majority of RL-based efforts having been allocated toepisodic and discounted MDPs. In this work, we study a unique structuralproperty of average-reward MDPs and utilize it to introduce Reward-ExtendedDifferential (or RED) reinforcement learning: a novel RL framework that can beused to effectively and efficiently solve various subtasks simultaneously inthe average-reward setting. We introduce a family of RED learning algorithmsfor prediction and control, including proven-convergent algorithms for thetabular case. We then showcase the power of these algorithms by demonstratinghow they can be used to learn a policy that optimizes, for the first time, thewell-known conditional value-at-risk (CVaR) risk measure in a fully-onlinemanner, without the use of an explicit bi-level optimization scheme or anaugmented state-space.</description><author>Juan Sebastian Rojas, Chi-Guhn Lee</author><pubDate>Tue, 03 Dec 2024 16:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10578v5</guid></item><item><title>LiDAR-based Registration against Georeferenced Models for Globally Consistent Allocentric Maps</title><link>http://arxiv.org/abs/2412.02533v1</link><description>Modern unmanned aerial vehicles (UAVs) are irreplaceable in search and rescue(SAR) missions to obtain a situational overview or provide closeups withoutendangering personnel. However, UAVs heavily rely on global navigationsatellite system (GNSS) for localization which works well in open spaces, butthe precision drastically degrades in the vicinity of buildings. Theseinaccuracies hinder aggregation of diverse data from multiple sources in aunified georeferenced frame for SAR operators. In contrast, CityGML modelsprovide approximate building shapes with accurate georeferenced poses. Besides,LiDAR works best in the vicinity of 3D structures. Hence, we refine coarse GNSSmeasurements by registering LiDAR maps against CityGML and digital elevationmap (DEM) models as a prior for allocentric mapping. An intuitive plausibilityscore selects the best hypothesis based on occupancy using a 2D height map.Afterwards, we integrate the registration results in a continuous-timespline-based pose graph optimizer with LiDAR odometry and further sensingmodalities to obtain globally consistent, georeferenced trajectories and maps.We evaluate the viability of our approach on multiple flights captured at twodistinct testing sites. Our method successfully reduced GNSS offset errors fromup-to 16 m to below 0.5 m on multiple flights. Furthermore, we obtain globallyconsistent maps w.r.t. prior 3D geospatial models.</description><author>Jan Quenzel, Linus T. Mallwitz, Benedikt T. Arnold, Sven Behnke</author><pubDate>Tue, 03 Dec 2024 16:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02533v1</guid></item><item><title>Multimodal Remote Sensing Scene Classification Using VLMs and Dual-Cross Attention Networks</title><link>http://arxiv.org/abs/2412.02531v1</link><description>Remote sensing scene classification (RSSC) is a critical task with diverseapplications in land use and resource management. While unimodal image-basedapproaches show promise, they often struggle with limitations such as highintra-class variance and inter-class similarity. Incorporating textualinformation can enhance classification by providing additional context andsemantic understanding, but manual text annotation is labor-intensive andcostly. In this work, we propose a novel RSSC framework that integrates textdescriptions generated by large vision-language models (VLMs) as an auxiliarymodality without incurring expensive manual annotation costs. To fully leveragethe latent complementarities between visual and textual data, we propose a dualcross-attention-based network to fuse these modalities into a unifiedrepresentation. Extensive experiments with both quantitative and qualitativeevaluation across five RSSC datasets demonstrate that our frameworkconsistently outperforms baseline models. We also verify the effectiveness ofVLM-generated text descriptions compared to human-annotated descriptions.Additionally, we design a zero-shot classification scenario to show that thelearned multimodal representation can be effectively utilized for unseen classclassification. This research opens new opportunities for leveraging textualinformation in RSSC tasks and provides a promising multimodal fusion structure,offering insights and inspiration for future studies. Code is available at:https://github.com/CJR7/MultiAtt-RSSC</description><author>Jinjin Cai, Kexin Meng, Baijian Yang, Gang Shao</author><pubDate>Tue, 03 Dec 2024 16:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02531v1</guid></item><item><title>WEM-GAN: Wavelet transform based facial expression manipulation</title><link>http://arxiv.org/abs/2412.02530v1</link><description>Facial expression manipulation aims to change human facial expressionswithout affecting face recognition. In order to transform the facialexpressions to target expressions, previous methods relied on expression labelsto guide the manipulation process. However, these methods failed to preservethe details of facial features, which causes the weakening or the loss ofidentity information in the output image. In our work, we propose WEM-GAN, inshort for wavelet-based expression manipulation GAN, which puts more efforts onpreserving the details of the original image in the editing process. Firstly,we take advantage of the wavelet transform technique and combine it with ourgenerator with a U-net autoencoder backbone, in order to improve thegenerator's ability to preserve more details of facial features. Secondly, wealso implement the high-frequency component discriminator, and usehigh-frequency domain adversarial loss to further constrain the optimization ofour model, providing the generated face image with more abundant details.Additionally, in order to narrow the gap between generated facial expressionsand target expressions, we use residual connections between encoder anddecoder, while also using relative action units (AUs) several times. Extensivequalitative and quantitative experiments have demonstrated that our modelperforms better in preserving identity features, editing capability, and imagegeneration quality on the AffectNet dataset. It also shows superior performancein metrics such as Average Content Distance (ACD) and Expression Distance (ED).</description><author>Dongya Sun, Yunfei Hu, Xianzhe Zhang, Yingsong Hu</author><pubDate>Tue, 03 Dec 2024 16:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02530v1</guid></item><item><title>Active learning of neural population dynamics using two-photon holographic optogenetics</title><link>http://arxiv.org/abs/2412.02529v1</link><description>Recent advances in techniques for monitoring and perturbing neuralpopulations have greatly enhanced our ability to study circuits in the brain.In particular, two-photon holographic optogenetics now enables precisephotostimulation of experimenter-specified groups of individual neurons, whilesimultaneous two-photon calcium imaging enables the measurement of ongoing andinduced activity across the neural population. Despite the enormous space ofpotential photostimulation patterns and the time-consuming nature ofphotostimulation experiments, very little algorithmic work has been done todetermine the most effective photostimulation patterns for identifying theneural population dynamics. Here, we develop methods to efficiently selectwhich neurons to stimulate such that the resulting neural responses will bestinform a dynamical model of the neural population activity. Using neuralpopulation responses to photostimulation in mouse motor cortex, we demonstratethe efficacy of a low-rank linear dynamical systems model, and develop anactive learning procedure which takes advantage of low-rank structure todetermine informative photostimulation patterns. We demonstrate our approach onboth real and synthetic data, obtaining in some cases as much as a two-foldreduction in the amount of data required to reach a given predictive power. Ouractive stimulation design method is based on a novel active learning procedurefor low-rank regression, which may be of independent interest.</description><author>Andrew Wagenmaker, Lu Mi, Marton Rozsa, Matthew S. Bull, Karel Svoboda, Kayvon Daie, Matthew D. Golub, Kevin Jamieson</author><pubDate>Tue, 03 Dec 2024 16:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02529v1</guid></item><item><title>Bias Analysis of AI Models for Undergraduate Student Admissions</title><link>http://arxiv.org/abs/2412.02528v1</link><description>Bias detection and mitigation is an active area of research in machinelearning. This work extends previous research done by the authors to provide arigorous and more complete analysis of the bias found in AI predictive models.Admissions data spanning six years was used to create an AI model to determinewhether a given student would be directly admitted into the School of Scienceunder various scenarios at a large urban research university. During this time,submission of standardized test scores as part of an application becameoptional which led to interesting questions about the impact of standardizedtest scores on admission decisions. We developed and analyzed AI models tounderstand which variables are important in admissions decisions, and how thedecision to exclude test scores affects the demographics of the students whoare admitted. We then evaluated the predictive models to detect and analyzebiases these models may carry with respect to three variables chosen torepresent sensitive populations: gender, race, and whether a student was thefirst in his or her family to attend college. We also extended our analysis toshow that the biases detected were persistent. Finally, we included severalfairness metrics in our analysis and discussed the uses and limitations ofthese metrics.</description><author>Kelly Van Busum, Shiaofen Fang</author><pubDate>Tue, 03 Dec 2024 16:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02528v1</guid></item><item><title>LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data</title><link>http://arxiv.org/abs/2412.02525v1</link><description>Modern time-series forecasting models often fail to make full use of richunstructured information about the time series themselves. This lack of properconditioning can lead to obvious model failures; for example, models may beunaware of the details of a particular product, and hence fail to anticipateseasonal surges in customer demand in the lead up to major exogenous eventslike holidays for clearly relevant products. To address this shortcoming, thispaper introduces a novel forecast post-processor -- which we call LLMForecaster-- that fine-tunes large language models (LLMs) to incorporate unstructuredsemantic and contextual information and historical data to improve theforecasts from an existing demand forecasting pipeline. In an industry-scaleretail application, we demonstrate that our technique yields statisticallysignificantly forecast improvements across several sets of products subject toholiday-driven demand surges.</description><author>Hanyu Zhang, Chuck Arvin, Dmitry Efimov, Michael W. Mahoney, Dominique Perrault-Joncas, Shankar Ramasubramanian, Andrew Gordon Wilson, Malcolm Wolff</author><pubDate>Tue, 03 Dec 2024 16:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02525v1</guid></item><item><title>The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</title><link>http://arxiv.org/abs/2404.16019v2</link><description>Human feedback is central to the alignment of Large Language Models (LLMs).However, open questions remain about methods (how), domains (where), people(who) and objectives (to what end) of feedback processes. To navigate thesequestions, we introduce PRISM, a dataset that maps the sociodemographics andstated preferences of 1,500 diverse participants from 75 countries, to theircontextual preferences and fine-grained feedback in 8,011 live conversationswith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographicparticipation in feedback; (ii) census-representative samples for two countries(UK, US); and (iii) individualised ratings that link to detailed participantprofiles, permitting personalisation and attribution of sample artefacts. Wetarget subjective and multicultural perspectives on value-laden andcontroversial issues, where we expect interpersonal and cross-culturaldisagreement. We use PRISM in three case studies to demonstrate the need forcareful consideration of which humans provide what alignment data.</description><author>Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, Scott A. Hale</author><pubDate>Tue, 03 Dec 2024 16:18:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16019v2</guid></item><item><title>Introduction to Reinforcement Learning</title><link>http://arxiv.org/abs/2408.07712v3</link><description>Reinforcement Learning (RL), a subfield of Artificial Intelligence (AI),focuses on training agents to make decisions by interacting with theirenvironment to maximize cumulative rewards. This paper provides an overview ofRL, covering its core concepts, methodologies, and resources for furtherlearning. It offers a thorough explanation of fundamental components such asstates, actions, policies, and reward signals, ensuring readers develop a solidfoundational understanding. Additionally, the paper presents a variety of RLalgorithms, categorized based on the key factors such as model-free,model-based, value-based, policy-based, and other key factors. Resources forlearning and implementing RL, such as books, courses, and online communitiesare also provided. By offering a clear, structured introduction, this paperaims to simplify the complexities of RL for beginners, providing astraightforward pathway to understanding.</description><author>Majid Ghasemi, Dariush Ebrahimi</author><pubDate>Tue, 03 Dec 2024 16:17:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07712v3</guid></item><item><title>Cooperative Cruising: Reinforcement Learning based Time-Headway Control for Increased Traffic Efficiency</title><link>http://arxiv.org/abs/2412.02520v1</link><description>The proliferation of Connected Automated Vehicles represents an unprecedentedopportunity for improving driving efficiency and alleviating trafficcongestion. However, existing research fails to address realistic multi-lanehighway scenarios without assuming connectivity, perception, and controlcapabilities that are typically unavailable in current vehicles. This paperproposes a novel AI system that is the first to improve highway trafficefficiency compared with human-like traffic in realistic, simulated multi-lanescenarios, while relying on existing connectivity, perception, and controlcapabilities. At the core of our approach is a reinforcement learning basedcontroller that dynamically communicates time-headways to automated vehiclesnear bottlenecks based on real-time traffic conditions. These desiredtime-headways are then used by Adaptive Cruise Control (ACC) systems to adjusttheir following distance. By (i) integrating existing traffic estimationtechnology and low-bandwidth vehicle-to-infrastructure connectivity, (ii)leveraging safety-certified ACC systems, and (iii) targeting localizedbottleneck challenges that can be addressed independently in differentlocations, we propose a practical, safe, and scalable system that canpositively impact numerous road users.</description><author>Yaron Veksler, Sharon Hornstein, Han Wang, Maria Laura Delle Monache, Daniel Urieli</author><pubDate>Tue, 03 Dec 2024 16:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02520v1</guid></item><item><title>Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification</title><link>http://arxiv.org/abs/2412.00876v2</link><description>Multimodal Large Language Models (MLLMs) have achieved remarkable success invision understanding, reasoning, and interaction. However, the inferencecomputation and memory increase progressively with the generation of outputtokens during decoding, directly affecting the efficacy of MLLMs. Existingmethods attempt to reduce the vision context redundancy to achieve efficientMLLMs. Unfortunately, the efficiency benefits of the vision context reductionin the prefill stage gradually diminish during the decoding stage. To addressthis problem, we proposed a dynamic vision-language context sparsificationframework Dynamic-LLaVA, which dynamically reduces the redundancy of visioncontext in the prefill stage and decreases the memory and computation overheadof the generated language context during decoding. Dynamic-LLaVA designs atailored sparsification inference scheme for different inference modes, i.e.,prefill, decoding with and without KV cache, to achieve efficient inference ofMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by$\sim$75\% in the prefill stage. Meanwhile, throughout the entire generationprocess of MLLMs, Dynamic-LLaVA reduces the $\sim$50\% computation consumptionunder decoding without KV cache, while saving $\sim$50\% GPU memory overheadwhen decoding with KV cache, due to the vision-language context sparsification.Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficientinference for MLLMs with negligible understanding and generation abilitydegradation or even performance gains compared to the full-context inferencebaselines. Code is available at https://github.com/Osilly/dynamic_llava .</description><author>Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin</author><pubDate>Tue, 03 Dec 2024 16:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00876v2</guid></item><item><title>Understanding complex crowd dynamics with generative neural simulators</title><link>http://arxiv.org/abs/2412.01491v2</link><description>Understanding the dynamics of pedestrian crowds is an outstanding challengecrucial for designing efficient urban infrastructure and ensuring safe crowdmanagement. To this end, both small-scale laboratory and large-scale real-worldmeasurements have been used. However, these approaches respectively lackstatistical resolution and parametric controllability, both essential todiscovering physical relationships underlying the complex stochastic dynamicsof crowds. Here, we establish an investigation paradigm that offerslaboratory-like controllability, while ensuring the statistical resolution oflarge-scale real-world datasets. Using our data-driven Neural Crowd Simulator(NeCS), which we train on large-scale data and validate against key statisticalfeatures of crowd dynamics, we show that we can perform effective surrogatecrowd dynamics experiments without training on specific scenarios. We not onlyreproduce known experimental results on pairwise avoidance, but also uncoverthe vision-guided and topological nature of N-body interactions. These findingsshow how virtual experiments based on neural simulation enable data-drivenscientific discovery.</description><author>Koen Minartz, Fleur Hendriks, Simon Martinus Koop, Alessandro Corbetta, Vlado Menkovski</author><pubDate>Tue, 03 Dec 2024 16:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01491v2</guid></item><item><title>Paired Autoencoders for Likelihood-free Estimation in Inverse Problems</title><link>http://arxiv.org/abs/2405.13220v2</link><description>We consider the solution of nonlinear inverse problems where the forwardproblem is a discretization of a partial differential equation. Such problemsare notoriously difficult to solve in practice and require minimizing acombination of a data-fit term and a regularization term. The maincomputational bottleneck of typical algorithms is the direct estimation of thedata misfit. Therefore, likelihood-free approaches have become appealingalternatives. Nonetheless, difficulties in generalization and limitations inaccuracy have hindered their broader utility and applicability. In this work,we use a paired autoencoder framework as a likelihood-free estimator forinverse problems. We show that the use of such an architecture allows us toconstruct a solution efficiently and to overcome some known open problems whenusing likelihood-free estimators. In particular, our framework can assess thequality of the solution and improve on it if needed. We demonstrate theviability of our approach using examples from full waveform inversion andinverse electromagnetic imaging.</description><author>Matthias Chung, Emma Hart, Julianne Chung, Bas Peters, Eldad Haber</author><pubDate>Tue, 03 Dec 2024 16:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13220v2</guid></item></channel></rss>