<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 15 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</title><link>http://arxiv.org/abs/2404.08640v1</link><description>Monocular egocentric 3D human motion capture is a challenging and activelyresearched problem. Existing methods use synchronously operating visual sensors(e.g. RGB cameras) and often fail under low lighting and fast motions, whichcan be restricting in many applications involving head-mounted devices. Inresponse to the existing limitations, this paper 1) introduces a new problem,i.e., 3D human motion capture from an egocentric monocular event camera with afisheye lens, and 2) proposes the first approach to it called EventEgo3D(EE3D). Event streams have high temporal resolution and provide reliable cuesfor 3D human motion capture under high-speed human motions and rapidly changingillumination. The proposed EE3D framework is specifically tailored for learningwith event streams in the LNES representation, enabling high 3D reconstructionaccuracy. We also design a prototype of a mobile head-mounted device with anevent camera and record a real dataset with event observations and theground-truth 3D human poses (in addition to the synthetic dataset). Our EE3Ddemonstrates robustness and superior 3D accuracy compared to existing solutionsacross various challenging experiments while supporting real-time 3D poseupdate rates of 140Hz.</description><author>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik</author><pubDate>Fri, 12 Apr 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08640v1</guid></item><item><title>COCONut: Modernizing COCO Segmentation</title><link>http://arxiv.org/abs/2404.08639v1</link><description>In recent decades, the vision community has witnessed remarkable progress invisual recognition, partially owing to advancements in dataset benchmarks.Notably, the established COCO benchmark has propelled the development of moderndetection and segmentation systems. However, the COCO segmentation benchmarkhas seen comparatively slow improvement over the last decade. Originallyequipped with coarse polygon annotations for thing instances, it graduallyincorporated coarse superpixel annotations for stuff regions, which weresubsequently heuristically amalgamated to yield panoptic segmentationannotations. These annotations, executed by different groups of raters, haveresulted not only in coarse segmentation masks but also in inconsistenciesbetween segmentation types. In this study, we undertake a comprehensivereevaluation of the COCO segmentation annotations. By enhancing the annotationquality and expanding the dataset to encompass 383K images with more than 5.18Mpanoptic masks, we introduce COCONut, the COCO Next Universal segmenTationdataset. COCONut harmonizes segmentation annotations across semantic, instance,and panoptic segmentation with meticulously crafted high-quality masks, andestablishes a robust benchmark for all segmentation tasks. To our knowledge,COCONut stands as the inaugural large-scale universal segmentation dataset,verified by human raters. We anticipate that the release of COCONut willsignificantly contribute to the community's ability to assess the progress ofnovel neural networks.</description><author>Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen</author><pubDate>Fri, 12 Apr 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08639v1</guid></item><item><title>What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?</title><link>http://arxiv.org/abs/2212.02705v5</link><description>Various methods for Multi-Agent Reinforcement Learning (MARL) have beendeveloped with the assumption that agents' policies are based on accurate stateinformation. However, policies learned through Deep Reinforcement Learning(DRL) are susceptible to adversarial state perturbation attacks. In this work,we propose a State-Adversarial Markov Game (SAMG) and make the first attempt toinvestigate different solution concepts of MARL under state uncertainties. Ouranalysis shows that the commonly used solution concepts of optimal agent policyand robust Nash equilibrium do not always exist in SAMGs. To circumvent thisdifficulty, we consider a new solution concept called robust agent policy,where agents aim to maximize the worst-case expected state value. We prove theexistence of robust agent policy for finite state and finite action SAMGs.Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C)algorithm to learn robust policies for MARL agents under state uncertainties.Our experiments demonstrate that our algorithm outperforms existing methodswhen faced with state perturbations and greatly improves the robustness of MARLpolicies. Our code is public onhttps://songyanghan.github.io/what_is_solution/.</description><author>Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, Shaofeng Zou, Fei Miao</author><pubDate>Fri, 12 Apr 2024 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02705v5</guid></item><item><title>Probing the 3D Awareness of Visual Foundation Models</title><link>http://arxiv.org/abs/2404.08636v1</link><description>Recent advances in large-scale pretraining have yielded visual foundationmodels with strong capabilities. Not only can recent models generalize toarbitrary images for their training task, their intermediate representationsare useful for other visual tasks such as detection and segmentation. Giventhat such models can classify, delineate, and localize objects in 2D, we askwhether they also represent their 3D structure? In this work, we analyze the 3Dawareness of visual foundation models. We posit that 3D awareness implies thatrepresentations (1) encode the 3D structure of the scene and (2) consistentlyrepresent the surface across views. We conduct a series of experiments usingtask-specific probes and zero-shot inference procedures on frozen features. Ourexperiments reveal several limitations of the current models. Our code andanalysis can be found at https://github.com/mbanani/probe3d.</description><author>Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, Varun Jampani</author><pubDate>Fri, 12 Apr 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08636v1</guid></item><item><title>Pre-training Small Base LMs with Fewer Tokens</title><link>http://arxiv.org/abs/2404.08634v1</link><description>We study the effectiveness of a simple approach to develop a small baselanguage model (LM) starting from an existing large base LM: first inherit afew transformer blocks from the larger LM, and then train this smaller model ona very small subset (0.1\%) of the raw pretraining data of the larger model. Wecall our simple recipe Inheritune and first demonstrate it for building a smallbase LM with 1.5B parameters using 1B tokens (and a starting few layers oflarger LM of 3B parameters); we do this using a single A6000 GPU for less thanhalf a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark,the resulting model compares favorably to publicly available base models of1B-2B size, some of which have been trained using 50-1000 times more tokens. We investigate Inheritune in a slightly different setting where we trainsmall LMs utilizing larger LMs and their full pre-training dataset. Here weshow that smaller LMs trained utilizing some of the layers of GPT2-medium(355M) and GPT-2-large (770M) can effectively match the val loss of theirbigger counterparts when trained from scratch for the same number of trainingsteps on OpenWebText dataset with 9B tokens. We analyze our recipe withextensive experiments and demonstrate it efficacy on diverse settings. Our codeis available at https://github.com/sanyalsunny111/LLM-Inheritune.</description><author>Sunny Sanyal, Sujay Sanghavi, Alexandros G. Dimakis</author><pubDate>Fri, 12 Apr 2024 18:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08634v1</guid></item><item><title>A State-Space Perspective on Modelling and Inference for Online Skill Rating</title><link>http://arxiv.org/abs/2308.02414v3</link><description>We summarise popular methods used for skill rating in competitive sports,along with their inferential paradigms and introduce new approaches based onsequential Monte Carlo and discrete hidden Markov models. We advocate for astate-space model perspective, wherein players' skills are represented astime-varying, and match results serve as observed quantities. We explore thesteps to construct the model and the three stages of inference: filtering,smoothing and parameter estimation. We examine the challenges of scaling up tonumerous players and matches, highlighting the main approximations andreductions which facilitate statistical and computational efficiency. Weadditionally compare approaches in a realistic experimental pipeline that canbe easily reproduced and extended with our open-source Python package,https://github.com/SamDuffield/abile.</description><author>Samuel Duffield, Samuel Power, Lorenzo Rimella</author><pubDate>Fri, 12 Apr 2024 18:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02414v3</guid></item><item><title>A Conceptual Framework for Conversational Search and Recommendation: Conceptualizing Agent-Human Interactions During the Conversational Search Process</title><link>http://arxiv.org/abs/2404.08630v1</link><description>The conversational search task aims to enable a user to resolve informationneeds via natural language dialogue with an agent. In this paper, we aim todevelop a conceptual framework of the actions and intents of users and agentsexplaining how these actions enable the user to explore the search space andresolve their information need. We outline the different actions and intents,before discussing key decision points in the conversation where the agent needsto decide how to steer the conversational search process to a successful and/orsatisfactory conclusion. Essentially, this paper provides a conceptualizationof the conversational search process between an agent and user, which providesa framework and a starting point for research, development and evaluation ofconversational search agents.</description><author>Leif Azzopardi, Mateusz Dubiel, Martin Halvey, Jeffery Dalton</author><pubDate>Fri, 12 Apr 2024 18:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08630v1</guid></item><item><title>Is ChatGPT Transforming Academics' Writing Style?</title><link>http://arxiv.org/abs/2404.08627v1</link><description>Based on one million arXiv papers submitted from May 2018 to January 2024, weassess the textual density of ChatGPT's writing style in their abstracts bymeans of a statistical analysis of word frequency changes. Our model iscalibrated and validated on a mixture of real abstracts and ChatGPT-modifiedabstracts (simulated data) after a careful noise analysis. We find that ChatGPTis having an increasing impact on arXiv abstracts, especially in the field ofcomputer science, where the fraction of ChatGPT-revised abstracts is estimatedto be approximately 35%, if we take the output of one of the simplest prompts,"revise the following sentences", as a baseline. We conclude with an analysisof both positive and negative aspects of the penetration of ChatGPT intoacademics' writing style.</description><author>Mingmeng Geng, Roberto Trotta</author><pubDate>Fri, 12 Apr 2024 18:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08627v1</guid></item><item><title>Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks</title><link>http://arxiv.org/abs/2404.08624v1</link><description>In this work, we instantiate a regularized form of the gradient clippingalgorithm and prove that it can converge to the global minima of deep neuralnetwork loss functions provided that the net is of sufficient width. We presentempirical evidence that our theoretically founded regularized gradient clippingalgorithm is also competitive with the state-of-the-art deep-learningheuristics. Hence the algorithm presented here constitutes a new approach torigorous deep learning. The modification we do to standard gradient clipping is designed to leveragethe PL* condition, a variant of the Polyak-Lojasiewicz inequality which wasrecently proven to be true for various neural networks for any depth within aneighborhood of the initialisation.</description><author>Matteo Tucat, Anirbit Mukherjee</author><pubDate>Fri, 12 Apr 2024 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08624v1</guid></item><item><title>LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models</title><link>http://arxiv.org/abs/2403.15388v4</link><description>Large Multimodal Models (LMMs) have shown significant reasoning capabilitiesby connecting a visual encoder and a large language model. LMMs typically use afixed amount of visual tokens, such as the penultimate layer features in theCLIP visual encoder, as the prefix content. Recent LMMs incorporate morecomplex visual inputs, such as high-resolution images and videos, whichincrease the number of visual tokens significantly. However, due to the designof the Transformer architecture, computational costs associated with thesemodels tend to increase quadratically with the number of input tokens. Totackle this problem, we explore a token reduction mechanism and find, similarto prior work, that many visual tokens are spatially redundant. Based on this,we propose PruMerge, a novel adaptive visual token reduction approach, whichlargely reduces the number of visual tokens while maintaining comparable modelperformance. We first select the unpruned visual tokens based on theirsimilarity to class tokens and spatial tokens. We then cluster the prunedtokens based on key similarity and merge the clustered tokens with the unprunedtokens to supplement their information. Empirically, when applied to LLaVA-1.5,our approach can compress the visual tokens by 18 times on average, and achievecomparable performance across diverse visual question-answering and reasoningtasks. Code and checkpoints are at https://llava-prumerge.github.io/.</description><author>Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan</author><pubDate>Fri, 12 Apr 2024 18:34:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15388v4</guid></item><item><title>Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian</title><link>http://arxiv.org/abs/2404.08617v1</link><description>In this paper, we focus on generating a synthetic question answering (QA)dataset using an adapted Translate-Align-Retrieve method. Using this method, wecreated the largest Serbian QA dataset of more than 87K samples, which we nameSQuAD-sr. To acknowledge the script duality in Serbian, we generated bothCyrillic and Latin versions of the dataset. We investigate the dataset qualityand use it to fine-tune several pre-trained QA models. Best results wereobtained by fine-tuning the BERTi\'c model on our Latin SQuAD-sr dataset,achieving 73.91% Exact Match and 82.97% F1 score on the benchmark XQuADdataset, which we translated into Serbian for the purpose of evaluation. Theresults show that our model exceeds zero-shot baselines, but fails to go beyondhuman performance. We note the advantage of using a monolingual pre-trainedmodel over multilingual, as well as the performance increase gained by usingLatin over Cyrillic. By performing additional analysis, we show that questionsabout numeric values or dates are more likely to be answered correctly thanother types of questions. Finally, we conclude that SQuAD-sr is of sufficientquality for fine-tuning a Serbian QA model, in the absence of a manuallycrafted and annotated dataset.</description><author>Aleksa Cvetanović, Predrag Tadić</author><pubDate>Fri, 12 Apr 2024 18:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08617v1</guid></item><item><title>Using Explainable AI and Transfer Learning to understand and predict the maintenance of Atlantic blocking with limited observational data</title><link>http://arxiv.org/abs/2404.08613v1</link><description>Blocking events are an important cause of extreme weather, especiallylong-lasting blocking events that trap weather systems in place. The durationof blocking events is, however, underestimated in climate models. ExplainableArtificial Intelligence are a class of data analysis methods that can helpidentify physical causes of prolonged blocking events and diagnose modeldeficiencies. We demonstrate this approach on an idealized quasigeostrophicmodel developed by Marshall and Molteni (1993). We train a convolutional neuralnetwork (CNN), and subsequently, build a sparse predictive model for thepersistence of Atlantic blocking, conditioned on an initial high-pressureanomaly. Shapley Additive ExPlanation (SHAP) analysis reveals thathigh-pressure anomalies in the American Southeast and North Atlantic, separatedby a trough over Atlantic Canada, contribute significantly to prediction ofsustained blocking events in the Atlantic region. This agrees with previouswork that identified precursors in the same regions via wave train analysis.When we apply the same CNN to blockings in the ERA5 atmospheric reanalysis,there is insufficient data to accurately predict persistent blocks. Wepartially overcome this limitation by pre-training the CNN on the plentifuldata of the Marshall-Molteni model, and then using Transfer Learning to achievebetter predictions than direct training. SHAP analysis before and aftertransfer learning allows a comparison between the predictive features in thereanalysis and the quasigeostrophic model, quantifying dynamical biases in theidealized model. This work demonstrates the potential for machine learningmethods to extract meaningful precursors of extreme weather events and achievebetter prediction using limited observational data.</description><author>Huan Zhang, Justin Finkel, Dorian S. Abbot, Edwin P. Gerber, Jonathan Weare</author><pubDate>Fri, 12 Apr 2024 18:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08613v1</guid></item><item><title>Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin Lymphoma Patients Using a Longitudinally-Aware Segmentation Network</title><link>http://arxiv.org/abs/2404.08611v1</link><description>$\textbf{Purpose}$: Automatic quantification of longitudinal changes in PETscans for lymphoma patients has proven challenging, as residual disease ininterim-therapy scans is often subtle and difficult to detect. Our goal was todevelop a longitudinally-aware segmentation network (LAS-Net) that can quantifyserial PET/CT images for pediatric Hodgkin lymphoma patients.$\textbf{Materials and Methods}$: This retrospective study included baseline(PET1) and interim (PET2) PET/CT images from 297 patients enrolled in twoChildren's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Netincorporates longitudinal cross-attention, allowing relevant features from PET1to inform the analysis of PET2. Model performance was evaluated using Dicecoefficients for PET1 and detection F1 scores for PET2. Additionally, weextracted and compared quantitative PET metrics, including metabolic tumorvolume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and$\Delta$SUVmax in PET2, against physician measurements. We quantified theiragreement using Spearman's $\rho$ correlations and employed bootstrapresampling for statistical analysis. $\textbf{Results}$: LAS-Net detectedresidual lymphoma in PET2 with an F1 score of 0.606 (precision/recall:0.615/0.600), outperforming all comparator methods (P&lt;0.01). For baselinesegmentation, LAS-Net achieved a mean Dice score of 0.772. In PETquantification, LAS-Net's measurements of qPET, $\Delta$SUVmax, MTV and TLGwere strongly correlated with physician measurements, with Spearman's $\rho$ of0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with aslight decrease, in an external testing cohort. $\textbf{Conclusion}$: LAS-Netachieved high performance in quantifying PET metrics across serial scans,highlighting the value of longitudinal awareness in evaluating multi-time-pointimaging datasets.</description><author>Xin Tie, Muheon Shin, Changhee Lee, Scott B. Perlman, Zachary Huemann, Amy J. Weisman, Sharon M. Castellino, Kara M. Kelly, Kathleen M. McCarten, Adina L. Alazraki, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw</author><pubDate>Fri, 12 Apr 2024 18:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08611v1</guid></item><item><title>A Dynamical Model of Neural Scaling Laws</title><link>http://arxiv.org/abs/2402.01092v2</link><description>On a variety of tasks, the performance of neural networks predictablyimproves with training time, dataset size and model size across many orders ofmagnitude. This phenomenon is known as a neural scaling law. Of fundamentalimportance is the compute-optimal scaling law, which reports the performance asa function of units of compute when choosing model sizes optimally. We analyzea random feature model trained with gradient descent as a solvable model ofnetwork training and generalization. This reproduces many observations aboutneural scaling laws. First, our model makes a prediction about why the scalingof performance with training time and with model size have different power lawexponents. Consequently, the theory predicts an asymmetric compute-optimalscaling rule where the number of training steps are increased faster than modelparameters, consistent with recent empirical observations. Second, it has beenobserved that early in training, networks converge to their infinite-widthdynamics at a rate $1/\textit{width}$ but at late time exhibit a rate$\textit{width}^{-c}$, where $c$ depends on the structure of the architectureand task. We show that our model exhibits this behavior. Lastly, our theoryshows how the gap between training and test loss can gradually build up overtime due to repeated reuse of data.</description><author>Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan</author><pubDate>Fri, 12 Apr 2024 18:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01092v2</guid></item><item><title>Hyperbolic Delaunay Geometric Alignment</title><link>http://arxiv.org/abs/2404.08608v1</link><description>Hyperbolic machine learning is an emerging field aimed at representing datawith a hierarchical structure. However, there is a lack of tools for evaluationand analysis of the resulting hyperbolic data representations. To this end, wepropose Hyperbolic Delaunay Geometric Alignment (HyperDGA) -- a similarityscore for comparing datasets in a hyperbolic space. The core idea is countingthe edges of the hyperbolic Delaunay graph connecting datapoints across thegiven sets. We provide an empirical investigation on synthetic and real-lifebiological data and demonstrate that HyperDGA outperforms the hyperbolicversion of classical distances between sets. Furthermore, we showcase thepotential of HyperDGA for evaluating latent representations inferred by aHyperbolic Variational Auto-Encoder.</description><author>Aniss Aiman Medbouhi, Giovanni Luca Marchetti, Vladislav Polianskii, Alexander Kravberg, Petra Poklukar, Anastasia Varava, Danica Kragic</author><pubDate>Fri, 12 Apr 2024 18:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08608v1</guid></item><item><title>FloCoDe: Unbiased Dynamic Scene Graph Generation with Temporal Consistency and Correlation Debiasing</title><link>http://arxiv.org/abs/2310.16073v3</link><description>Dynamic scene graph generation (SGG) from videos requires not only acomprehensive understanding of objects across scenes but also a method tocapture the temporal motions and interactions with different objects. Moreover,the long-tailed distribution of visual relationships is a crucial bottleneckfor most dynamic SGG methods. This is because many of them focus on capturingspatio-temporal context using complex architectures, leading to the generationof biased scene graphs. To address these challenges, we propose FloCoDe:Flow-aware Temporal Consistency and Correlation Debiasing with uncertaintyattenuation for unbiased dynamic scene graphs. FloCoDe employs feature warpingusing flow to detect temporally consistent objects across frames. To addressthe long-tail issue of visual relationships, we propose correlation debiasingand a label correlation-based loss to learn unbiased relation representationsfor long-tailed classes. Specifically, we propose to incorporate labelcorrelations using contrastive loss to capture commonly co-occurring relations,which aids in learning robust representations for long-tailed classes. Further,we adopt the uncertainty attenuation-based classifier framework to handle noisyannotations in the SGG data. Extensive experimental evaluation shows aperformance gain as high as 4.1%, demonstrating the superiority of generatingmore unbiased scene graphs.</description><author>Anant Khandelwal</author><pubDate>Fri, 12 Apr 2024 18:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16073v3</guid></item><item><title>Training-free Boost for Open-Vocabulary Object Detection with Confidence Aggregation</title><link>http://arxiv.org/abs/2404.08603v1</link><description>Open-vocabulary object detection (OVOD) aims at localizing and recognizingvisual objects from novel classes unseen at the training time. Whereas,empirical studies reveal that advanced detectors generally assign lower scoresto those novel instances, which are inadvertently suppressed during inferenceby commonly adopted greedy strategies like Non-Maximum Suppression (NMS),leading to sub-optimal detection performance for novel classes. This papersystematically investigates this problem with the commonly-adopted two-stageOVOD paradigm. Specifically, in the region-proposal stage, proposals thatcontain novel instances showcase lower objectness scores, since they aretreated as background proposals during the training phase. Meanwhile, in theobject-classification stage, novel objects share lower region-text similarities(i.e., classification scores) due to the biased visual-language alignment byseen training samples. To alleviate this problem, this paper introduces twoadvanced measures to adjust confidence scores and conserve erroneouslydismissed objects: (1) a class-agnostic localization quality estimate viaoverlap degree of region/object proposals, and (2) a text-guided visualsimilarity estimate with proxy prototypes for novel classes. Integrated withadjusting techniques specifically designed for the region-proposal andobject-classification stages, this paper derives the aggregated confidenceestimate for the open-vocabulary object detection paradigm (AggDet). Our AggDetis a generic and training-free post-processing scheme, which consistentlybolsters open-vocabulary detectors across model scales and architecturedesigns. For instance, AggDet receives 3.3% and 1.5% gains on OV-COCO andOV-LVIS benchmarks respectively, without any training cost.</description><author>Yanhao Zheng, Kai Liu</author><pubDate>Fri, 12 Apr 2024 18:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08603v1</guid></item><item><title>Sliding down the stairs: how correlated latent variables accelerate learning with neural networks</title><link>http://arxiv.org/abs/2404.08602v1</link><description>Neural networks extract features from data using stochastic gradient descent(SGD). In particular, higher-order input cumulants (HOCs) are crucial for theirperformance. However, extracting information from the $p$th cumulant of$d$-dimensional inputs is computationally hard: the number of samples requiredto recover a single direction from an order-$p$ tensor (tensor PCA) usingonline SGD grows as $d^{p-1}$, which is prohibitive for high-dimensionalinputs. This result raises the question of how neural networks extract relevantdirections from the HOCs of their inputs efficiently. Here, we show thatcorrelations between latent variables along the directions encoded in differentinput cumulants speed up learning from higher-order correlations. We show thiseffect analytically by deriving nearly sharp thresholds for the number ofsamples required by a single neuron to weakly-recover these directions usingonline SGD from a random start in high dimensions. Our analytical results areconfirmed in simulations of two-layer neural networks and unveil a newmechanism for hierarchical learning in neural networks.</description><author>Lorenzo Bardone, Sebastian Goldt</author><pubDate>Fri, 12 Apr 2024 18:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08602v1</guid></item><item><title>PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination</title><link>http://arxiv.org/abs/2404.07520v2</link><description>The potential for zero-shot generalization in vision-language (V-L) modelssuch as CLIP has spurred their widespread adoption in addressing numerousdownstream tasks. Previous methods have employed test-time prompt tuning toadapt the model to unseen domains, but they overlooked the issue of imbalancedclass distributions. In this study, we explicitly address this problem byemploying class-aware prototype alignment weighted by mean class probabilitiesobtained for the test sample and filtered augmented views. Additionally, weensure that the class probabilities are as accurate as possible by performingprototype discrimination using contrastive learning. The combination ofalignment and discriminative loss serves as a geometric regularizer, preventingthe prompt representation from collapsing onto a single class and effectivelybridging the distribution gap between the source and test domains. Our method,named PromptSync, synchronizes the prompts for each test sample on both thetext and vision branches of the V-L model. In empirical evaluations on thedomain generalization benchmark, our method outperforms previous best methodsby 2.33% in overall performance, by 1% in base-to-novel generalization, and by2.84% in cross-dataset transfer tasks.</description><author>Anant Khandelwal</author><pubDate>Fri, 12 Apr 2024 18:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07520v2</guid></item><item><title>Generating Synthetic Time Series Data for Cyber-Physical Systems</title><link>http://arxiv.org/abs/2404.08601v1</link><description>Data augmentation is an important facilitator of deep learning applicationsin the time series domain. A gap is identified in the literature, demonstratingsparse exploration of the transformer, the dominant sequence model, for dataaugmentation in time series. A architecture hybridizing several successfulpriors is put forth and tested using a powerful time domain similarity metric.Results suggest the challenge of this domain, and several valuable directionsfor future work.</description><author>Alexander Sommers, Somayeh Bakhtiari Ramezani, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure</author><pubDate>Fri, 12 Apr 2024 17:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08601v1</guid></item><item><title>WonderJourney: Going from Anywhere to Everywhere</title><link>http://arxiv.org/abs/2312.03884v2</link><description>We introduce WonderJourney, a modularized framework for perpetual 3D scenegeneration. Unlike prior work on view generation that focuses on a single typeof scenes, we start at any user-provided location (by a text description or animage) and generate a journey through a long sequence of diverse yet coherentlyconnected 3D scenes. We leverage an LLM to generate textual descriptions of thescenes in this journey, a text-driven point cloud generation pipeline to make acompelling and coherent sequence of 3D scenes, and a large VLM to verify thegenerated scenes. We show compelling, diverse visual results across variousscene types and styles, forming imaginary "wonderjourneys". Project website:https://kovenyu.com/WonderJourney/</description><author>Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T. Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, Charles Herrmann</author><pubDate>Fri, 12 Apr 2024 17:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03884v2</guid></item><item><title>Improving Referring Image Segmentation using Vision-Aware Text Features</title><link>http://arxiv.org/abs/2404.08590v1</link><description>Referring image segmentation is a challenging task that involves generatingpixel-wise segmentation masks based on natural language descriptions. Existingmethods have relied mostly on visual features to generate the segmentationmasks while treating text features as supporting components. This over-relianceon visual features can lead to suboptimal results, especially in complexscenarios where text prompts are ambiguous or context-dependent. To overcomethese challenges, we present a novel framework VATEX to improve referring imagesegmentation by enhancing object and context understanding with Vision-AwareText Feature. Our method involves using CLIP to derive a CLIP Prior thatintegrates an object-centric visual heatmap with text description, which can beused as the initial query in DETR-based architecture for the segmentation task.Furthermore, by observing that there are multiple ways to describe an instancein an image, we enforce feature similarity between text variations referring tothe same visual input by two components: a novel Contextual Multimodal Decoderthat turns text embeddings into vision-aware text features, and a MeaningConsistency Constraint to ensure further the coherent and consistentinterpretation of language expressions with the context understanding obtainedfrom the image. Our method achieves a significant performance improvement onthree benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at:https://nero1342.github.io/VATEX\_RIS.</description><author>Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung</author><pubDate>Fri, 12 Apr 2024 17:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08590v1</guid></item><item><title>ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification</title><link>http://arxiv.org/abs/2401.01448v2</link><description>Multi-label image classification presents a challenging task in many domains,including computer vision and medical imaging. Recent advancements haveintroduced graph-based and transformer-based methods to improve performance andcapture label dependencies. However, these methods often include complexmodules that entail heavy computation and lack interpretability. In this paper,we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novelframework to address these challenges in multi-label image classificationtasks. Our simple yet effective approach employs supervised contrastivelearning, in which samples that share enough labels with an anchor image basedon a decision threshold are introduced as a positive set. This structurecaptures label dependencies by pulling positive pair embeddings together andpushing away negative samples that fall below the threshold. We enhancerepresentation learning by incorporating a mixture density network intocontrastive learning and generating Gaussian mixture distributions to explorethe epistemic uncertainty of the feature encoder. We validate the effectivenessof our framework through experimentation with datasets from the computer visionand medical imaging domains. Our method outperforms the existingstate-of-the-art methods while achieving a low computational footprint on bothdatasets. Visualization analyses also demonstrate that ProbMCL-learnedclassifiers maintain a meaningful semantic topology.</description><author>Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis</author><pubDate>Fri, 12 Apr 2024 17:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01448v2</guid></item><item><title>Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts</title><link>http://arxiv.org/abs/2404.08589v1</link><description>Visual question answering (VQA) is known as an AI-complete task as itrequires understanding, reasoning, and inferring about the vision and thelanguage content. Over the past few years, numerous neural architectures havebeen suggested for the VQA problem. However, achieving success in zero-shot VQAremains a challenge due to its requirement for advanced generalization andreasoning skills. This study explores the impact of incorporating imagecaptioning as an intermediary process within the VQA pipeline. Specifically, weexplore the efficacy of utilizing image captions instead of images andleveraging large language models (LLMs) to establish a zero-shot setting. Sinceimage captioning is the most crucial step in this process, we compare theimpact of state-of-the-art image captioning models on VQA performance acrossvarious question types in terms of structure and semantics. We propose astraightforward and efficient question-driven image captioning approach withinthis pipeline to transfer contextual information into the question-answering(QA) model. This method involves extracting keywords from the question,generating a caption for each image-question pair using the keywords, andincorporating the question-driven caption into the LLM prompt. We evaluate theefficacy of using general-purpose and question-driven image captions in the VQApipeline. Our study highlights the potential of employing image captions andharnessing the capabilities of LLMs to achieve competitive performance on GQAunder the zero-shot setting. Our code is available at\url{https://github.com/ovguyo/captions-in-VQA}.</description><author>Övgü Özdemir, Erdem Akagündüz</author><pubDate>Fri, 12 Apr 2024 17:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08589v1</guid></item><item><title>Advanced wood species identification based on multiple anatomical sections and using deep feature transfer and fusion</title><link>http://arxiv.org/abs/2404.08585v1</link><description>In recent years, we have seen many advancements in wood speciesidentification. Methods like DNA analysis, Near Infrared (NIR) spectroscopy,and Direct Analysis in Real Time (DART) mass spectrometry complement thelong-established wood anatomical assessment of cell and tissue morphology.However, most of these methods have some limitations such as high costs, theneed for skilled experts for data interpretation, and the lack of good datasetsfor professional reference. Therefore, most of these methods, and certainly thewood anatomical assessment, may benefit from tools based on ArtificialIntelligence. In this paper, we apply two transfer learning techniques withConvolutional Neural Networks (CNNs) to a multi-view Congolese wood speciesdataset including sections from different orientations and viewed at differentmicroscopic magnifications. We explore two feature extraction methods indetail, namely Global Average Pooling (GAP) and Random Encoding of AggregatedDeep Activation Maps (RADAM), for efficient and accurate wood speciesidentification. Our results indicate superior accuracy on diverse datasets andanatomical sections, surpassing the results of other methods. Our proposalrepresents a significant advancement in wood species identification, offering arobust tool to support the conservation of forest ecosystems and promotesustainable forestry practices.</description><author>Kallil M. Zielinski, Leonardo Scabini, Lucas C. Ribas, Núbia R. da Silva, Hans Beeckman, Jan Verwaeren, Odemir M. Bruno, Bernard De Baets</author><pubDate>Fri, 12 Apr 2024 17:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08585v1</guid></item><item><title>Pathological Primitive Segmentation Based on Visual Foundation Model with Zero-Shot Mask Generation</title><link>http://arxiv.org/abs/2404.08584v1</link><description>Medical image processing usually requires a model trained with carefullycrafted datasets due to unique image characteristics and domain-specificchallenges, especially in pathology. Primitive detection and segmentation indigitized tissue samples are essential for objective and automated diagnosisand prognosis of cancer. SAM (Segment Anything Model) has recently beendeveloped to segment general objects from natural images with high accuracy,but it requires human prompts to generate masks. In this work, we present anovel approach that adapts pre-trained natural image encoders of SAM fordetection-based region proposals. Regions proposed by a pre-trained encoder aresent to cascaded feature propagation layers for projection. Then, localsemantic and global context is aggregated from multi-scale for bounding boxlocalization and classification. Finally, the SAM decoder uses the identifiedbounding boxes as essential prompts to generate a comprehensive primitivesegmentation map. The entire base framework, SAM, requires no additionaltraining or fine-tuning but could produce an end-to-end result for twofundamental segmentation tasks in pathology. Our method compares withstate-of-the-art models in F1 score for nuclei detection and binary/multiclasspanoptic(bPQ/mPQ) and mask quality(dice) for segmentation quality on thePanNuke dataset while offering end-to-end efficiency. Our model also achievesremarkable Average Precision (+4.5%) on the secondary dataset (HuBMAP Kidney)compared to Faster RCNN. The code is publicly available athttps://github.com/learner-codec/autoprom_sam.</description><author>Abu Bakor Hayat Arnob, Xiangxue Wang, Yiping Jiao, Xiao Gan, Wenlong Ming, Jun Xu</author><pubDate>Fri, 12 Apr 2024 17:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08584v1</guid></item><item><title>FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation</title><link>http://arxiv.org/abs/2404.08582v1</link><description>In the realm of fashion object detection and segmentation for online shoppingimages, existing state-of-the-art fashion parsing models encounter limitations,particularly when exposed to non-model-worn apparel and close-up shots. Toaddress these failures, we introduce FashionFail; a new fashion dataset withe-commerce images for object detection and segmentation. The dataset isefficiently curated using our novel annotation tool that leverages recentfoundation models. The primary objective of FashionFail is to serve as a testbed for evaluating the robustness of models. Our analysis reveals theshortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer.Additionally, we propose a baseline approach using naive data augmentation tomitigate common failure cases and improve model robustness. Through this work,we aim to inspire and support further research in fashion item detection andsegmentation for industrial applications. The dataset, annotation tool, code,and models are available at \url{https://rizavelioglu.github.io/fashionfail/}.</description><author>Riza Velioglu, Robin Chan, Barbara Hammer</author><pubDate>Fri, 12 Apr 2024 17:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08582v1</guid></item><item><title>Leap: molecular synthesisability scoring with intermediates</title><link>http://arxiv.org/abs/2403.13005v2</link><description>Assessing whether a molecule can be synthesised is a primary task in drugdiscovery. It enables computational chemists to filter for viable compounds orbias molecular generative models. The notion of synthesisability is dynamic asit evolves depending on the availability of key compounds. A common approach indrug discovery involves exploring the chemical space surroundingsynthetically-accessible intermediates. This strategy improves thesynthesisability of the derived molecules due to the availability of keyintermediates. Existing synthesisability scoring methods such as SAScore,SCScore and RAScore, cannot condition on intermediates dynamically. Ourapproach, Leap, is a GPT-2 model trained on the depth, or longest linear path,of predicted synthesis routes that allows information on the availability ofkey intermediates to be included at inference time. We show that Leap surpassesall other scoring methods by at least 5% on AUC score when identifyingsynthesisable molecules, and can successfully adapt predicted scores whenpresented with a relevant intermediate compound.</description><author>Antonia Calvi, Théophile Gaudin, Dominik Miketa, Dominique Sydow, Liam Wilbraham</author><pubDate>Fri, 12 Apr 2024 17:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13005v2</guid></item><item><title>Lossy Image Compression with Foundation Diffusion Models</title><link>http://arxiv.org/abs/2404.08580v1</link><description>Incorporating diffusion models in the image compression domain has thepotential to produce realistic and detailed reconstructions, especially atextremely low bitrates. Previous methods focus on using diffusion models asexpressive decoders robust to quantization errors in the conditioning signals,yet achieving competitive results in this manner requires costly training ofthe diffusion model and long inference times due to the iterative generativeprocess. In this work we formulate the removal of quantization error as adenoising task, using diffusion to recover lost information in the transmittedimage latent. Our approach allows us to perform less than 10\% of the fulldiffusion generative process and requires no architectural changes to thediffusion model, enabling the use of foundation models as a strong priorwithout additional fine tuning of the backbone. Our proposed codec outperformsprevious methods in quantitative realism metrics, and we verify that ourreconstructions are qualitatively preferred by end users, even when othermethods use twice the bitrate.</description><author>Lucas Relic, Roberto Azevedo, Markus Gross, Christopher Schroers</author><pubDate>Fri, 12 Apr 2024 17:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08580v1</guid></item><item><title>Small Models Are (Still) Effective Cross-Domain Argument Extractors</title><link>http://arxiv.org/abs/2404.08579v1</link><description>Effective ontology transfer has been a major goal of recent work on eventargument extraction (EAE). Two methods in particular -- question answering (QA)and template infilling (TI) -- have emerged as promising approaches to thisproblem. However, detailed explorations of these techniques' ability toactually enable this transfer are lacking. In this work, we provide such astudy, exploring zero-shot transfer using both techniques on six major EAEdatasets at both the sentence and document levels. Further, we challenge thegrowing reliance on LLMs for zero-shot extraction, showing that vastly smallermodels trained on an appropriate source ontology can yield zero-shotperformance superior to that of GPT-3.5 or GPT-4.</description><author>William Gantt, Aaron Steven White</author><pubDate>Fri, 12 Apr 2024 17:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08579v1</guid></item><item><title>Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation</title><link>http://arxiv.org/abs/2404.08570v1</link><description>This paper introduces CRITICAL, a novel closed-loop framework for autonomousvehicle (AV) training and testing. CRITICAL stands out for its ability togenerate diverse scenarios, focusing on critical driving situations that targetspecific learning and performance gaps identified in the Reinforcement Learning(RL) agent. The framework achieves this by integrating real-world trafficdynamics, driving behavior analysis, surrogate safety measures, and an optionalLarge Language Model (LLM) component. It is proven that the establishment of aclosed feedback loop between the data generation pipeline and the trainingprocess can enhance the learning rate during training, elevate overall systemperformance, and augment safety resilience. Our evaluations, conducted usingthe Proximal Policy Optimization (PPO) and the HighwayEnv simulationenvironment, demonstrate noticeable performance improvements with theintegration of critical case generation and LLM analysis, indicating CRITICAL'spotential to improve the robustness of AV systems and streamline the generationof critical scenarios. This ultimately serves to hasten the development of AVagents, expand the general scope of RL training, and ameliorate validationefforts for AV safety.</description><author>Hanlin Tian, Kethan Reddy, Yuxiang Feng, Mohammed Quddus, Yiannis Demiris, Panagiotis Angeloudis</author><pubDate>Fri, 12 Apr 2024 17:13:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08570v1</guid></item><item><title>Incremental Extractive Opinion Summarization Using Cover Trees</title><link>http://arxiv.org/abs/2401.08047v2</link><description>Extractive opinion summarization involves automatically producing a summaryof text about an entity (e.g., a product's reviews) by extractingrepresentative sentences that capture prevalent opinions in the review set.Typically, in online marketplaces user reviews accumulate over time, andopinion summaries need to be updated periodically to provide customers withup-to-date information. In this work, we study the task of extractive opinionsummarization in an incremental setting, where the underlying review setevolves over time. Many of the state-of-the-art extractive opinionsummarization approaches are centrality-based, such as CentroidRank (Radev etal., 2004; Chowdhury et al., 2022). CentroidRank performs extractivesummarization by selecting a subset of review sentences closest to the centroidin the representation space as the summary. However, these methods are notcapable of operating efficiently in an incremental setting, where reviewsarrive one at a time. In this paper, we present an efficient algorithm foraccurately computing the CentroidRank summaries in an incremental setting. Ourapproach, CoverSumm, relies on indexing review representations in a cover treeand maintaining a reservoir of candidate summary review sentences. CoverSumm'sefficacy is supported by a theoretical and empirical analysis of running time.Empirically, on a diverse collection of data (both real and syntheticallycreated to illustrate scaling considerations), we demonstrate that CoverSumm isup to 36x faster than baseline methods, and capable of adapting to nuancedchanges in data distribution. We also conduct human evaluations of thegenerated summaries and find that CoverSumm is capable of producing informativesummaries consistent with the underlying review set.</description><author>Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Manzil Zaheer, Andrew McCallum, Amr Ahmed, Snigdha Chaturvedi</author><pubDate>Fri, 12 Apr 2024 17:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08047v2</guid></item><item><title>Towards Measuring and Modeling "Culture" in LLMs: A Survey</title><link>http://arxiv.org/abs/2403.15412v2</link><description>We present a survey of 39 recent papers that aim to study culturalrepresentation and inclusion in large language models. We observe that none ofthe studies define "culture," which is a complex, multifaceted concept;instead, they probe the models on some specially designed datasets whichrepresent certain aspects of "culture." We call these aspects the proxies ofcultures, and organize them across three dimensions of demographic, semanticand linguistic-cultural interaction proxies. We also categorize the probingmethods employed. Our analysis indicates that only certain aspects of"culture," such as values and objectives, have been studied, leaving severalother interesting and important facets, especially the multitude of semanticdomains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022),unexplored. Two other crucial gaps are the lack of robustness and situatednessof the current methods. Based on these observations, we provide severalrecommendations for a holistic and practically useful research agenda forfurthering cultural inclusion in LLMs and LLM-based applications.</description><author>Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Ashutosh Dwivedi, Alham Fikri Aji, Jacki O'Neill, Ashutosh Modi, Monojit Choudhury</author><pubDate>Fri, 12 Apr 2024 17:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15412v2</guid></item><item><title>Mitigating Receiver Impact on Radio Frequency Fingerprint Identification via Domain Adaptation</title><link>http://arxiv.org/abs/2404.08566v1</link><description>Radio Frequency Fingerprint Identification (RFFI), which exploits non-idealhardware-induced unique distortion resident in the transmit signals to identifyan emitter, is emerging as a means to enhance the security of communicationsystems. Recently, machine learning has achieved great success in developingstate-of-the-art RFFI models. However, few works consider cross-receiver RFFIproblems, where the RFFI model is trained and deployed on different receivers.Due to altered receiver characteristics, direct deployment of RFFI model on anew receiver leads to significant performance degradation. To address thisissue, we formulate the cross-receiver RFFI as a model adaptation problem,which adapts the trained model to unlabeled signals from a new receiver. Wefirst develop a theoretical generalization error bound for the adaptationmodel. Motivated by the bound, we propose a novel method to solve thecross-receiver RFFI problem, which includes domain alignment and adaptivepseudo-labeling. The former aims at finding a feature space where both domainsexhibit similar distributions, effectively reducing the domain discrepancy.Meanwhile, the latter employs a dynamic pseudo-labeling scheme to implicitlytransfer the label information from the labeled receiver to the new receiver.Experimental results indicate that the proposed method can effectively mitigatethe receiver impact and improve the cross-receiver RFFI performance.</description><author>Liu Yang, Qiang Li, Xiaoyang Ren, Yi Fang, Shafei Wang</author><pubDate>Fri, 12 Apr 2024 17:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08566v1</guid></item><item><title>A Change Detection Reality Check</title><link>http://arxiv.org/abs/2402.06994v2</link><description>In recent years, there has been an explosion of proposed change detectiondeep learning architectures in the remote sensing literature. These approachesclaim to offer state-of-the-art performance on different standard benchmarkdatasets. However, has the field truly made significant progress? In this paperwe perform experiments which conclude a simple U-Net segmentation baselinewithout training tricks or complicated architectural changes is still a topperformer for the task of change detection.</description><author>Isaac Corley, Caleb Robinson, Anthony Ortiz</author><pubDate>Fri, 12 Apr 2024 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06994v2</guid></item><item><title>The Impact of Variable Ordering on Bayesian Network Structure Learning</title><link>http://arxiv.org/abs/2206.08952v2</link><description>Causal Bayesian Networks provide an important tool for reasoning underuncertainty with potential application to many complex causal systems.Structure learning algorithms that can tell us something about the causalstructure of these systems are becoming increasingly important. In theliterature, the validity of these algorithms is often tested for sensitivityover varying sample sizes, hyper-parameters, and occasionally objectivefunctions. In this paper, we show that the order in which the variables areread from data can have much greater impact on the accuracy of the algorithmthan these factors. Because the variable ordering is arbitrary, any significanteffect it has on learnt graph accuracy is concerning, and this raises questionsabout the validity of the results produced by algorithms that are sensitive to,but have not been assessed against, different variable orderings.</description><author>Neville K Kitson, Anthony C Constantinou</author><pubDate>Fri, 12 Apr 2024 17:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08952v2</guid></item><item><title>IDD-X: A Multi-View Dataset for Ego-relative Important Object Localization and Explanation in Dense and Unstructured Traffic</title><link>http://arxiv.org/abs/2404.08561v1</link><description>Intelligent vehicle systems require a deep understanding of the interplaybetween road conditions, surrounding entities, and the ego vehicle's drivingbehavior for safe and efficient navigation. This is particularly critical indeveloping countries where traffic situations are often dense and unstructuredwith heterogeneous road occupants. Existing datasets, predominantly gearedtowards structured and sparse traffic scenarios, fall short of capturing thecomplexity of driving in such environments. To fill this gap, we present IDD-X,a large-scale dual-view driving video dataset. With 697K bounding boxes, 9Kimportant object tracks, and 1-12 objects per video, IDD-X offers comprehensiveego-relative annotations for multiple important road objects covering 10categories and 19 explanation label categories. The dataset also incorporatesrearview information to provide a more complete representation of the drivingenvironment. We also introduce custom-designed deep networks aimed at multipleimportant object localization and per-object explanation prediction. Overall,our dataset and introduced prediction models form the foundation for studyinghow road conditions and surrounding entities affect driving behavior in complextraffic situations.</description><author>Chirag Parikh, Rohit Saluja, C. V. Jawahar, Ravi Kiran Sarvadevabhatla</author><pubDate>Fri, 12 Apr 2024 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08561v1</guid></item><item><title>MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking</title><link>http://arxiv.org/abs/2404.08559v1</link><description>Zero-shot dialogue state tracking (DST) transfers knowledge to unseendomains, reducing the cost of annotating new datasets. Previous zero-shot DSTmodels mainly suffer from domain transferring and partial prediction problems.To address these challenges, we propose Mixture of Prefix Experts (MoPE) toestablish connections between similar slots in different domains, whichstrengthens the model transfer performance in unseen domains. Empirical resultsdemonstrate that MoPE-DST achieves the joint goal accuracy of 57.13% onMultiWOZ2.1 and 55.40% on SGD.</description><author>Tianwen Tang, Tong Zhu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen</author><pubDate>Fri, 12 Apr 2024 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08559v1</guid></item><item><title>Scalability in Building Component Data Annotation: Enhancing Facade Material Classification with Synthetic Data</title><link>http://arxiv.org/abs/2404.08557v1</link><description>Computer vision models trained on Google Street View images can creatematerial cadastres. However, current approaches need manually annotateddatasets that are difficult to obtain and often have class imbalance. Toaddress these challenges, this paper fine-tuned a Swin Transformer model on asynthetic dataset generated with DALL-E and compared the performance to asimilar manually annotated dataset. Although manual annotation remains the goldstandard, the synthetic dataset performance demonstrates a reasonablealternative. The findings will ease annotation needed to develop materialcadastres, offering architects insights into opportunities for material reuse,thus contributing to the reduction of demolition waste.</description><author>Josie Harrison, Alexander Hollberg, Yinan Yu</author><pubDate>Fri, 12 Apr 2024 16:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08557v1</guid></item><item><title>RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs</title><link>http://arxiv.org/abs/2404.08555v1</link><description>State-of-the-art large language models (LLMs) have become indispensable toolsfor various tasks. However, training LLMs to serve as effective assistants forhumans requires careful consideration. A promising approach is reinforcementlearning from human feedback (RLHF), which leverages human feedback to updatethe model in accordance with human preferences and mitigate issues liketoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largelyentangled with initial design choices that popularized the method and currentresearch focuses on augmenting those choices rather than fundamentallyimproving the framework. In this paper, we analyze RLHF through the lens ofreinforcement learning principles to develop an understanding of itsfundamentals, dedicating substantial focus to the core component of RLHF -- thereward model. Our study investigates modeling choices, caveats of functionapproximation, and their implications on RLHF training algorithms, highlightingthe underlying assumptions made about the expressivity of reward. Our analysisimproves the understanding of the role of reward models and methods for theirtraining, concurrently revealing limitations of the current methodology. Wecharacterize these limitations, including incorrect generalization, modelmisspecification, and the sparsity of feedback, along with their impact on theperformance of a language model. The discussion and analysis are substantiatedby a categorical review of current literature, serving as a reference forresearchers and practitioners to understand the challenges of RLHF and buildupon existing efforts.</description><author>Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva</author><pubDate>Fri, 12 Apr 2024 16:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08555v1</guid></item><item><title>Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations</title><link>http://arxiv.org/abs/2211.03226v3</link><description>The difficult problem of relating the static structure of glassy liquids andtheir dynamics is a good target for Machine Learning, an approach which excelsat finding complex patterns hidden in data. Indeed, this approach is currentlya hot topic in the glassy liquids community, where the state of the artconsists in Graph Neural Networks (GNNs), which have great expressive power butare heavy models and lack interpretability. Inspired by recent advances in thefield of Machine Learning group-equivariant representations, we build a GNNthat learns a robust representation of the glass' static structure byconstraining it to preserve the roto-translation (SE(3)) equivariance. We showthat this constraint significantly improves the predictive power at comparableor reduced number of parameters but most importantly, improves the ability togeneralize to unseen temperatures. While remaining a Deep network, our modelhas improved interpretability compared to other GNNs, as the action of ourbasic convolution layer relates directly to well-known rotation-invariantexpert features. Through transfer-learning experiments displaying unprecedentedperformance, we demonstrate that our network learns a robust representation,which allows us to push forward the idea of a learned structural orderparameter for glasses.</description><author>Francesco Saverio Pezzicoli, Guillaume Charpiat, François P. Landes</author><pubDate>Fri, 12 Apr 2024 16:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03226v3</guid></item><item><title>TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability</title><link>http://arxiv.org/abs/2312.06499v3</link><description>The fairness of Natural Language Processing (NLP) models has emerged as acrucial concern. Information theory indicates that to achieve fairness, a modelshould not be able to predict sensitive variables, such as gender, ethnicity,and age. However, information related to these variables often appearsimplicitly in language, posing a challenge in identifying and mitigating biaseseffectively. To tackle this issue, we present a novel approach that operates atthe embedding level of an NLP model, independent of the specific architecture.Our method leverages insights from recent advances in XAI techniques andemploys an embedding transformation to eliminate implicit information from aselected variable. By directly manipulating the embeddings in the final layer,our approach enables a seamless integration into existing models withoutrequiring significant modifications or retraining. In evaluation, we show thatthe proposed post-hoc approach significantly reduces gender-relatedassociations in NLP models while preserving the overall performance andfunctionality of the models. An implementation of our method is available:https://github.com/fanny-jourdan/TaCo</description><author>Fanny Jourdan, Louis Béthune, Agustin Picard, Laurent Risser, Nicholas Asher</author><pubDate>Fri, 12 Apr 2024 16:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06499v3</guid></item><item><title>Generalization in diffusion models arises from geometry-adaptive harmonic representations</title><link>http://arxiv.org/abs/2310.02557v3</link><description>Deep neural networks (DNNs) trained for image denoising are able to generatehigh-quality samples with score-based reverse diffusion algorithms. Theseimpressive capabilities seem to imply an escape from the curse ofdimensionality, but recent reports of memorization of the training set raisethe question of whether these networks are learning the "true" continuousdensity of the data. Here, we show that two DNNs trained on non-overlappingsubsets of a dataset learn nearly the same score function, and thus the samedensity, when the number of training images is large enough. In this regime ofstrong generalization, diffusion-generated images are distinct from thetraining set, and are of high visual quality, suggesting that the inductivebiases of the DNNs are well-aligned with the data density. We analyze thelearned denoising functions and show that the inductive biases give rise to ashrinkage operation in a basis adapted to the underlying image. Examination ofthese bases reveals oscillating harmonic structures along contours and inhomogeneous regions. We demonstrate that trained denoisers are inductivelybiased towards these geometry-adaptive harmonic bases since they arise not onlywhen the network is trained on photographic images, but also when it is trainedon image classes supported on low-dimensional manifolds for which the harmonicbasis is suboptimal. Finally, we show that when trained on regular imageclasses for which the optimal basis is known to be geometry-adaptive andharmonic, the denoising performance of the networks is near-optimal.</description><author>Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, Stéphane Mallat</author><pubDate>Fri, 12 Apr 2024 16:48:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02557v3</guid></item><item><title>Benchmarking the Cell Image Segmentation Models Robustness under the Microscope Optical Aberrations</title><link>http://arxiv.org/abs/2404.08549v1</link><description>Cell segmentation is essential in biomedical research for analyzing cellularmorphology and behavior. Deep learning methods, particularly convolutionalneural networks (CNNs), have revolutionized cell segmentation by extractingintricate features from images. However, the robustness of these methods undermicroscope optical aberrations remains a critical challenge. This studycomprehensively evaluates the performance of cell instance segmentation modelsunder simulated aberration conditions using the DynamicNuclearNet (DNN) andLIVECell datasets. Aberrations, including Astigmatism, Coma, Spherical, andTrefoil, were simulated using Zernike polynomial equations. Varioussegmentation models, such as Mask R-CNN with different network heads (FPN, C3)and backbones (ResNet, VGG19, SwinS), were trained and tested under aberratedconditions. Results indicate that FPN combined with SwinS demonstrates superiorrobustness in handling simple cell images affected by minor aberrations.Conversely, Cellpose2.0 proves effective for complex cell images under similarconditions. Our findings provide insights into selecting appropriatesegmentation models based on cell morphology and aberration severity, enhancingthe reliability of cell segmentation in biomedical applications. Furtherresearch is warranted to validate these methods with diverse aberration typesand emerging segmentation models. Overall, this research aims to guideresearchers in effectively utilizing cell segmentation models in the presenceof minor optical aberrations.</description><author>Boyuan Peng, Jiaju Chen, Qihui Ye, Minjiang Chen, Peiwu Qin, Chenggang Yan, Dongmei Yu, Zhenglin Chen</author><pubDate>Fri, 12 Apr 2024 16:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08549v1</guid></item><item><title>Analyzing Decades-Long Environmental Changes in Namibia Using Archival Aerial Photography and Deep Learning</title><link>http://arxiv.org/abs/2404.08544v1</link><description>This study explores object detection in historical aerial photographs ofNamibia to identify long-term environmental changes. Specifically, we aim toidentify key objects -- \textit{Waterholes}, \textit{Omuti homesteads}, and\textit{Big trees} -- around Oshikango in Namibia using sub-meter gray-scaleaerial imagery from 1943 and 1972. In this work, we propose a workflow foranalyzing historical aerial imagery using a deep semantic segmentation model onsparse hand-labels. To this end, we employ a number of strategies includingclass-weighting, pseudo-labeling and empirical p-value-based filtering tobalance skewed and sparse representations of objects in the ground truth data.Results demonstrate the benefits of these different training strategiesresulting in an average $F_1=0.661$ and $F_1=0.755$ over the three objects ofinterest for the 1943 and 1972 imagery, respectively. We also identified thatthe average size of Waterhole and Big trees increased while the average size ofOmutis decreased between 1943 and 1972 reflecting some of the local effects ofthe massive post-Second World War economic, agricultural, demographic, andenvironmental changes. This work also highlights the untapped potential ofhistorical aerial photographs in understanding long-term environmental changesbeyond Namibia (and Africa). With the lack of adequate satellite technology inthe past, archival aerial photography offers a great alternative to uncoverdecades-long environmental changes.</description><author>Girmaw Abebe Tadesse, Caleb Robinson, Gilles Quentin Hacheme, Akram Zaytar, Rahul Dodhia, Tsering Wangyal Shawa, Juan M. Lavista Ferres, Emmanuel H. Kreike</author><pubDate>Fri, 12 Apr 2024 16:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08544v1</guid></item><item><title>Memory Traces: Are Transformers Tulving Machines?</title><link>http://arxiv.org/abs/2404.08543v1</link><description>Memory traces--changes in the memory system that result from the perceptionand encoding of an event--were measured in pioneering studies by Endel Tulvingand Michael J. Watkins in 1975. These and further experiments informed thematuration of Tulving's memory model, from the GAPS (General AbstractProcessing System} to the SPI (Serial-Parallel Independent) model. Havingcurrent top of the line LLMs revisit the original Tulving-Watkins tests mayhelp in assessing whether foundation models completely instantiate or not thisclass of psychological models.</description><author>Jean-Marie Chauvet</author><pubDate>Fri, 12 Apr 2024 16:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08543v1</guid></item><item><title>On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation</title><link>http://arxiv.org/abs/2404.08540v1</link><description>Recent advances in monocular depth estimation have been made by incorporatingnatural language as additional guidance. Although yielding impressive results,the impact of the language prior, particularly in terms of generalization androbustness, remains unexplored. In this paper, we address this gap byquantifying the impact of this prior and introduce methods to benchmark itseffectiveness across various settings. We generate "low-level" sentences thatconvey object-centric, three-dimensional spatial relationships, incorporatethem as additional language priors and evaluate their downstream impact ondepth estimation. Our key finding is that current language-guided depthestimators perform optimally only with scene-level descriptions andcounter-intuitively fare worse with low level descriptions. Despite leveragingadditional data, these methods are not robust to directed adversarial attacksand decline in performance with an increase in distribution shift. Finally, toprovide a foundation for future research, we identify points of failures andoffer insights to better understand these shortcomings. With an increasingnumber of methods using language for depth estimation, our findings highlightthe opportunities and pitfalls that require careful consideration for effectivedeployment in real-world settings</description><author>Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang</author><pubDate>Fri, 12 Apr 2024 16:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08540v1</guid></item><item><title>Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization</title><link>http://arxiv.org/abs/2402.16891v2</link><description>Vehicle routing problems (VRPs), which can be found in numerous real-worldapplications, have been an important research topic for several decades.Recently, the neural combinatorial optimization (NCO) approach that leverages alearning-based model to solve VRPs without manual algorithm design has gainedsubstantial attention. However, current NCO methods typically require buildingone model for each routing problem, which significantly hinders their practicalapplication for real-world industry problems with diverse attributes. In thiswork, we make the first attempt to tackle the crucial challenge ofcross-problem generalization. In particular, we formulate VRPs as differentcombinations of a set of shared underlying attributes and solve themsimultaneously via a single model through attribute composition. In this way,our proposed model can successfully solve VRPs with unseen attributecombinations in a zero-shot generalization manner. Extensive experiments areconducted on eleven VRP variants, benchmark datasets, and industry logisticscenarios. The results show that the unified model demonstrates superiorperformance in the eleven VRPs, reducing the average gap to around 5% from over20% in the existing approach and achieving a significant performance boost onbenchmark datasets as well as a real-world logistics application. The sourcecode is included in https://github.com/FeiLiu36/MTNCO.</description><author>Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan</author><pubDate>Fri, 12 Apr 2024 16:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16891v2</guid></item><item><title>VertAttack: Taking advantage of Text Classifiers' horizontal vision</title><link>http://arxiv.org/abs/2404.08538v1</link><description>Text classification systems have continuously improved in performance overthe years. However, nearly all current SOTA classifiers have a similarshortcoming, they process text in a horizontal manner. Vertically written wordswill not be recognized by a classifier. In contrast, humans are easily able torecognize and read words written both horizontally and vertically. Hence, ahuman adversary could write problematic words vertically and the meaning wouldstill be preserved to other humans. We simulate such an attack, VertAttack.VertAttack identifies which words a classifier is reliant on and then rewritesthose words vertically. We find that VertAttack is able to greatly drop theaccuracy of 4 different transformer models on 5 datasets. For example, on theSST2 dataset, VertAttack is able to drop RoBERTa's accuracy from 94 to 13%.Furthermore, since VertAttack does not replace the word, meaning is easilypreserved. We verify this via a human study and find that crowdworkers are ableto correctly label 77% perturbed texts perturbed, compared to 81% of theoriginal texts. We believe VertAttack offers a look into how humans mightcircumvent classifiers in the future and thus inspire a look into more robustalgorithms.</description><author>Jonathan Rusert</author><pubDate>Fri, 12 Apr 2024 16:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08538v1</guid></item><item><title>Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking</title><link>http://arxiv.org/abs/2404.08535v1</link><description>Contrastive learning has gained widespread adoption for retrieval tasks dueto its minimal requirement for manual annotations. However, popular contrastiveframeworks typically learn from binary relevance, making them ineffective atincorporating direct fine-grained rankings. In this paper, we curate alarge-scale dataset featuring detailed relevance scores for each query-documentpair to facilitate future research and evaluation. Subsequently, we proposeGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),which is designed to learn from fine-grained rankings beyond binary relevancescores. Our results show that GCL achieves a 94.5% increase in NDCG@10 forin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relativeto the CLIP baseline and involving ground truth rankings.</description><author>Tianyu Zhu, Myong Chol Jung, Jesse Clark</author><pubDate>Fri, 12 Apr 2024 16:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08535v1</guid></item><item><title>Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection</title><link>http://arxiv.org/abs/2404.08531v1</link><description>Weakly supervised video anomaly detection (WSVAD) is a challenging task.Generating fine-grained pseudo-labels based on weak-label and thenself-training a classifier is currently a promising solution. However, sincethe existing methods use only RGB visual modality and the utilization ofcategory text information is neglected, thus limiting the generation of moreaccurate pseudo-labels and affecting the performance of self-training. Inspiredby the manual labeling process based on the event description, in this paper,we propose a novel pseudo-label generation and self-training framework based onText Prompt with Normality Guidance (TPWNG) for WSVAD. Our idea is to transferthe rich language-visual knowledge of the contrastive language-imagepre-training (CLIP) model for aligning the video event description text andcorresponding video frames to generate pseudo-labels. Specifically, We firstfine-tune the CLIP for domain adaptation by designing two ranking losses and adistributional inconsistency loss. Further, we propose a learnable text promptmechanism with the assist of a normality visual prompt to further improve thematching accuracy of video event description text and video frames. Then, wedesign a pseudo-label generation module based on the normality guidance toinfer reliable frame-level pseudo-labels. Finally, we introduce a temporalcontext self-adaptive learning module to learn the temporal dependencies ofdifferent video events more flexibly and accurately. Extensive experiments showthat our method achieves state-of-the-art performance on two benchmarkdatasets, UCF-Crime and XD-Viole</description><author>Zhiwei Yang, Jing Liu, Peng Wu</author><pubDate>Fri, 12 Apr 2024 16:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08531v1</guid></item><item><title>A novel Fourier neural operator framework for classification of multi-sized images: Application to three dimensional digital porous media</title><link>http://arxiv.org/abs/2402.11568v2</link><description>Fourier neural operators (FNOs) are invariant with respect to the size ofinput images, and thus images with any size can be fed into FNO-basedframeworks without any modification of network architectures, in contrast totraditional convolutional neural networks (CNNs). Leveraging the advantage ofFNOs, we propose a novel deep-learning framework for classifying images withvarying sizes. Particularly, we simultaneously train the proposed network onmulti-sized images. As a practical application, we consider the problem ofpredicting the label (e.g., permeability) of three-dimensional digital porousmedia. To construct the framework, an intuitive approach is to connect FNOlayers to a classifier using adaptive max pooling. First, we show that thisapproach is only effective for porous media with fixed sizes, whereas it failsfor porous media of varying sizes. To overcome this limitation, we introduceour approach: instead of using adaptive max pooling, we use static max poolingwith the size of channel width of FNO layers. Since the channel width of theFNO layers is independent of input image size, the introduced framework canhandle multi-sized images during training. We show the effectiveness of theintroduced framework and compare its performance with the intuitive approachthrough the example of the classification of three-dimensional digital porousmedia of varying sizes.</description><author>Ali Kashefi, Tapan Mukerji</author><pubDate>Fri, 12 Apr 2024 16:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11568v2</guid></item><item><title>Masked Image Modeling as a Framework for Self-Supervised Learning across Eye Movements</title><link>http://arxiv.org/abs/2404.08526v1</link><description>To make sense of their surroundings, intelligent systems must transformcomplex sensory inputs to structured codes that are reduced to task-relevantinformation such as object category. Biological agents achieve this in alargely autonomous manner, presumably via self-\allowbreak super-\allowbreakvised learning. Whereas previous attempts to model the underlying mechanismswere largely discriminative in nature, there is ample evidence that the brainemploys a generative model of the world. Here, we propose that eye movements,in combination with the focused nature of primate vision, constitute agenerative, self-supervised task of predicting and revealing visualinformation. We construct a proof-of-principle model starting from theframework of masked image modeling (MIM), a common approach in deeprepresentation learning. To do so, we analyze how core components of MIM suchas masking technique and data augmentation influence the formation ofcategory-specific representations. This allows us not only to better understandthe principles behind MIM, but to then reassemble a MIM more in line with thefocused nature of biological perception. From a theoretical angle, we find thatMIM disentangles neurons in latent space, a property that has been suggested tostructure visual representations in primates, without explicit regulation.Together with previous findings of invariance learning, this highlights aninteresting connection of MIM to latent regularization approaches forself-supervised learning. The source code is available underhttps://github.com/RobinWeiler/FocusMIM</description><author>Robin Weiler, Matthias Brucklacher, Cyriel M. A. Pennartz, Sander M. Bohté</author><pubDate>Fri, 12 Apr 2024 16:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08526v1</guid></item><item><title>Advancing Forest Fire Prevention: Deep Reinforcement Learning for Effective Firebreak Placement</title><link>http://arxiv.org/abs/2404.08523v1</link><description>Over the past decades, the increase in both frequency and intensity oflarge-scale wildfires due to climate change has emerged as a significantnatural threat. The pressing need to design resilient landscapes capable ofwithstanding such disasters has become paramount, requiring the development ofadvanced decision-support tools. Existing methodologies, including MixedInteger Programming, Stochastic Optimization, and Network Theory, have proveneffective but are hindered by computational demands, limiting theirapplicability. In response to this challenge, we propose using artificial intelligencetechniques, specifically Deep Reinforcement Learning, to address the complexproblem of firebreak placement in the landscape. We employ value-function basedapproaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling DoubleDeep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined withConvolutional Neural Networks, we have successfully implemented a computationalagent capable of learning firebreak locations within a forest environment,achieving good results. Furthermore, we incorporate a pre-training loop, initially teaching our agentto mimic a heuristic-based algorithm and observe that it consistently exceedsthe performance of these solutions. Our findings underscore the immensepotential of Deep Reinforcement Learning for operational research challenges,especially in fire prevention. Our approach demonstrates convergence withhighly favorable results in problem instances as large as 40 x 40 cells,marking a significant milestone in applying Reinforcement Learning to thiscritical issue. To the best of our knowledge, this study represents a pioneering effort inusing Reinforcement Learning to address the aforementioned problem, offeringpromising perspectives in fire prevention and landscape management</description><author>Lucas Murray, Tatiana Castillo, Jaime Carrasco, Andrés Weintraub, Richard Weber, Isaac Martín de Diego, José Ramón González, Jordi García-Gonzalo</author><pubDate>Fri, 12 Apr 2024 16:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08523v1</guid></item><item><title>Rethinking How to Evaluate Language Model Jailbreak</title><link>http://arxiv.org/abs/2404.06407v2</link><description>Large language models (LLMs) have become increasingly integrated with variousapplications. To ensure that LLMs do not generate unsafe responses, they arealigned with safeguards that specify what content is restricted. However, suchalignment can be bypassed to produce prohibited content using a techniquecommonly referred to as jailbreak. Different systems have been proposed toperform the jailbreak automatically. These systems rely on evaluation methodsto determine whether a jailbreak attempt is successful. However, our analysisreveals that current jailbreak evaluation methods have two limitations. (1)Their objectives lack clarity and do not align with the goal of identifyingunsafe responses. (2) They oversimplify the jailbreak result as a binaryoutcome, successful or not. In this paper, we propose three metrics, safeguardviolation, informativeness, and relative truthfulness, to evaluate languagemodel jailbreak. Additionally, we demonstrate how these metrics correlate withthe goal of different malicious actors. To compute these metrics, we introducea multifaceted approach that extends the natural language generation evaluationmethod after preprocessing the response. We evaluate our metrics on a benchmarkdataset produced from three malicious intent datasets and three jailbreaksystems. The benchmark dataset is labeled by three annotators. We compare ourmultifaceted approach with three existing jailbreak evaluation methods.Experiments demonstrate that our multifaceted evaluation outperforms existingmethods, with F1 scores improving on average by 17% compared to existingbaselines. Our findings motivate the need to move away from the binary view ofthe jailbreak problem and incorporate a more comprehensive evaluation to ensurethe safety of the language model.</description><author>Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik</author><pubDate>Fri, 12 Apr 2024 16:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06407v2</guid></item><item><title>Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for Assimilating Satellite Observations</title><link>http://arxiv.org/abs/2404.08522v1</link><description>Data assimilation (DA), as an indispensable component within contemporaryNumerical Weather Prediction (NWP) systems, plays a crucial role in generatingthe analysis that significantly impacts forecast performance. Nevertheless, thedevelopment of an efficient DA system poses significant challenges,particularly in establishing intricate relationships between the backgrounddata and the vast amount of multi-source observation data within limited timewindows in operational settings. To address these challenges, researchersdesign complex pre-processing methods for each observation type, leveragingapproximate modeling and the power of super-computing clusters to expeditesolutions. The emergence of deep learning (DL) models has been a game-changer,offering unified multi-modal modeling, enhanced nonlinear representationcapabilities, and superior parallelization. These advantages have spurredefforts to integrate DL models into various domains of weather modeling.Remarkably, DL models have shown promise in matching, even surpassing, theforecast accuracy of leading operational NWP models worldwide. This successmotivates the exploration of DL-based DA frameworks tailored for weatherforecasting models. In this study, we introduces FuxiDA, a generalized DL-basedDA framework for assimilating satellite observations. By assimilating data fromAdvanced Geosynchronous Radiation Imager (AGRI) aboard Fengyun-4B, FuXi-DAconsistently mitigates analysis errors and significantly improves forecastperformance. Furthermore, through a series of single-observation experiments,Fuxi-DA has been validated against established atmospheric physics,demonstrating its consistency and reliability.</description><author>Xiaoze Xu, Xiuyu Sun, Wei Han, Xiaohui Zhong, Lei Chen, Hao Li</author><pubDate>Fri, 12 Apr 2024 16:02:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08522v1</guid></item><item><title>View-Consistent 3D Editing with Gaussian Splatting</title><link>http://arxiv.org/abs/2403.11868v3</link><description>The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,offering efficient, high-fidelity rendering and enabling precise localmanipulations. Currently, diffusion-based 2D editing models are harnessed tomodify multi-view rendered images, which then guide the editing of 3DGS models.However, this approach faces a critical issue of multi-view inconsistency,where the guidance images exhibit significant discrepancies across views,leading to mode collapse and visual artifacts of 3DGS. To this end, weintroduce View-consistent Editing (VcEdit), a novel framework that seamlesslyincorporates 3DGS into image editing processes, ensuring multi-view consistencyin edited guidance images and effectively mitigating mode collapse issues.VcEdit employs two innovative consistency modules: the Cross-attentionConsistency Module and the Editing Consistency Module, both designed to reduceinconsistencies in edited images. By incorporating these consistency modulesinto an iterative pattern, VcEdit proficiently resolves the issue of multi-viewinconsistency, facilitating high-quality 3DGS editing across a diverse range ofscenes.</description><author>Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang</author><pubDate>Fri, 12 Apr 2024 16:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11868v3</guid></item><item><title>SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera</title><link>http://arxiv.org/abs/2404.06710v3</link><description>One of the most critical factors in achieving sharp Novel View Synthesis(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3DGaussian Splatting (3DGS) is the quality of the training images. However,Conventional RGB cameras are susceptible to motion blur. In contrast,neuromorphic cameras like event and spike cameras inherently capture morecomprehensive temporal information, which can provide a sharp representation ofthe scene as additional training data. Recent methods have explored theintegration of event cameras to improve the quality of NVS. The event-RGBapproaches have some limitations, such as high training costs and the inabilityto work effectively in the background. Instead, our study introduces a newmethod that uses the spike camera to overcome these limitations. By consideringtexture reconstruction from spike streams as ground truth, we design theTexture from Spike (TfS) loss. Since the spike camera relies on temporalintegration instead of temporal differentiation used by event cameras, ourproposed TfS loss maintains manageable training costs. It handles foregroundobjects with backgrounds simultaneously. We also provide a real-world datasetcaptured with our spike-RGB camera system to facilitate future researchendeavors. We conduct extensive experiments using synthetic and real-worlddatasets to demonstrate that our design can enhance novel view synthesis acrossNeRF and 3DGS. The code and dataset will be made available for public access.</description><author>Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang</author><pubDate>Fri, 12 Apr 2024 15:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06710v3</guid></item><item><title>Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward</title><link>http://arxiv.org/abs/2404.08517v1</link><description>While Large Language Models (LLMs) have seen widespread applications acrossnumerous fields, their limited interpretability poses concerns regarding theirsafe operations from multiple aspects, e.g., truthfulness, robustness, andfairness. Recent research has started developing quality assurance methods forLLMs, introducing techniques such as offline detector-based or uncertaintyestimation methods. However, these approaches predominantly concentrate onpost-generation analysis, leaving the online safety analysis for LLMs duringthe generation phase an unexplored area. To bridge this gap, we conduct in thiswork a comprehensive evaluation of the effectiveness of existing online safetyanalysis methods on LLMs. We begin with a pilot study that validates thefeasibility of detecting unsafe outputs in the early generation process.Following this, we establish the first publicly available benchmark of onlinesafety analysis for LLMs, including a broad spectrum of methods, models, tasks,datasets, and evaluation metrics. Utilizing this benchmark, we extensivelyanalyze the performance of state-of-the-art online safety analysis methods onboth open-source and closed-source LLMs. This analysis reveals the strengthsand weaknesses of individual methods and offers valuable insights intoselecting the most appropriate method based on specific application scenariosand task requirements. Furthermore, we also explore the potential of usinghybridization methods, i.e., combining multiple methods to derive a collectivesafety conclusion, to enhance the efficacy of online safety analysis for LLMs.Our findings indicate a promising direction for the development of innovativeand trustworthy quality assurance methodologies for LLMs, facilitating theirreliable deployments across diverse domains.</description><author>Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, Lei Ma</author><pubDate>Fri, 12 Apr 2024 15:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08517v1</guid></item><item><title>ChatGPT and general-purpose AI count fruits in pictures surprisingly well</title><link>http://arxiv.org/abs/2404.08515v1</link><description>Object counting is a popular task in deep learning applications in variousdomains, including agriculture. A conventional deep learning approach requiresa large amount of training data, often a logistic problem in a real-worldapplication. To address this issue, we examined how well ChatGPT (GPT4V) and ageneral-purpose AI (foundation model for object counting, T-Rex) can count thenumber of fruit bodies (coffee cherries) in 100 images. The foundation modelwith few-shot learning outperformed the trained YOLOv8 model (R2 = 0.923 and0.900, respectively). ChatGPT also showed some interesting potential,especially when few-shot learning with human feedback was applied (R2 = 0.360and 0.460, respectively). Moreover, we examined the time required forimplementation as a practical question. Obtaining the results with thefoundation model and ChatGPT were much shorter than the YOLOv8 model (0.83 hrs,1.75 hrs, and 161 hrs). We interpret these results as two surprises for deeplearning users in applied domains: a foundation model with few-shotdomain-specific learning can drastically save time and effort compared to theconventional approach, and ChatGPT can reveal a relatively good performance.Both approaches do not need coding skills, which can foster AI education anddissemination.</description><author>Konlavach Mengsuwan, Juan Camilo Rivera Palacio, Masahiro Ryo</author><pubDate>Fri, 12 Apr 2024 15:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08515v1</guid></item><item><title>NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Datase</title><link>http://arxiv.org/abs/2404.08514v1</link><description>Despite the significant progress in image denoising, it is still challengingto restore fine-scale details while removing noise, especially in extremelylow-light environments. Leveraging near-infrared (NIR) images to assist visibleRGB image denoising shows the potential to address this issue, becoming apromising technology. Nonetheless, existing works still struggle with takingadvantage of NIR information effectively for real-world image denoising, due tothe content inconsistency between NIR-RGB images and the scarcity of real-worldpaired datasets. To alleviate the problem, we propose an efficient SelectiveFusion Module (SFM), which can be plug-and-played into the advanced denoisingnetworks to merge the deep NIR-RGB features. Specifically, we sequentiallyperform the global and local modulation for NIR and RGB features, and thenintegrate the two modulated features. Furthermore, we present a Real-worldNIR-Assisted Image Denoising (Real-NAID) dataset, which covers diversescenarios as well as various noise levels. Extensive experiments on bothsynthetic and our real-world datasets demonstrate that the proposed methodachieves better results than state-of-the-art ones. The dataset, codes, andpre-trained models will be publicly available athttps://github.com/ronjonxu/NAID.</description><author>Rongjian Xu, Zhilu Zhang, Renlong Wu, Wangmeng Zuo</author><pubDate>Fri, 12 Apr 2024 15:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08514v1</guid></item><item><title>Adversarial Imitation Learning via Boosting</title><link>http://arxiv.org/abs/2404.08513v1</link><description>Adversarial imitation learning (AIL) has stood out as a dominant frameworkacross various imitation learning (IL) applications, with Discriminator ActorCritic (DAC) (Kostrikov et al.,, 2019) demonstrating the effectiveness ofoff-policy learning algorithms in improving sample efficiency and scalabilityto higher-dimensional observations. Despite DAC's empirical success, theoriginal AIL objective is on-policy and DAC's ad-hoc application of off-policytraining does not guarantee successful imitation (Kostrikov et al., 2019;2020). Follow-up work such as ValueDICE (Kostrikov et al., 2020) tackles thisissue by deriving a fully off-policy AIL objective. Instead in this work, wedevelop a novel and principled AIL algorithm via the framework of boosting.Like boosting, our new algorithm, AILBoost, maintains an ensemble of properlyweighted weak learners (i.e., policies) and trains a discriminator thatwitnesses the maximum discrepancy between the distributions of the ensemble andthe expert policy. We maintain a weighted replay buffer to represent thestate-action distribution induced by the ensemble, allowing us to traindiscriminators using the entire data collected so far. In the weighted replaybuffer, the contribution of the data from older policies are properlydiscounted with the weight computed based on the boosting framework.Empirically, we evaluate our algorithm on both controller state-based andpixel-based environments from the DeepMind Control Suite. AILBoost outperformsDAC on both types of environments, demonstrating the benefit of properlyweighting replay buffer data for off-policy training. On state-basedenvironments, DAC outperforms ValueDICE and IQ-Learn (Gary et al., 2021),achieving competitive performance with as little as one expert trajectory.</description><author>Jonathan D. Chang, Dhruv Sreenivas, Yingbing Huang, Kianté Brantley, Wen Sun</author><pubDate>Fri, 12 Apr 2024 15:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08513v1</guid></item><item><title>Re-evaluating the Need for Multimodal Signals in Unsupervised Grammar Induction</title><link>http://arxiv.org/abs/2212.10564v3</link><description>Are multimodal inputs necessary for grammar induction? Recent work has shownthat multimodal training inputs can improve grammar induction. However, theseimprovements are based on comparisons to weak text-only baselines that weretrained on relatively little textual data. To determine whether multimodalinputs are needed in regimes with large amounts of textual training data, wedesign a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG isa C-PFCG that incorporates em-beddings from text-only large language models(LLMs). We use a fixed grammar family to directly compare LC-PCFG to variousmulti-modal grammar induction methods. We compare performance on four benchmarkdatasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1compared to state-of-the-art multimodal grammar induction methods. LC-PCFG isalso more computationally efficient, providing an up to 85% reduction inparameter count and 8.8x reduction in training time compared to multimodalapproaches. These results suggest that multimodal inputs may not be necessaryfor grammar induction, and emphasize the importance of strong vision-freebaselines for evaluating the benefit of multimodal approaches.</description><author>Boyi Li, Rodolfo Corona, Karttikeya Mangalam, Catherine Chen, Daniel Flaherty, Serge Belongie, Kilian Q. Weinberger, Jitendra Malik, Trevor Darrell, Dan Klein</author><pubDate>Fri, 12 Apr 2024 15:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10564v3</guid></item><item><title>RFFNet: Large-Scale Interpretable Kernel Methods via Random Fourier Features</title><link>http://arxiv.org/abs/2211.06410v2</link><description>Kernel methods provide a flexible and theoretically grounded approach tononlinear and nonparametric learning. While memory and run-time requirementshinder their applicability to large datasets, many low-rank kernelapproximations, such as random Fourier features, were recently developed toscale up such kernel methods. However, these scalable approaches are based onapproximations of isotropic kernels, which cannot remove the influence ofirrelevant features. In this work, we design random Fourier features for afamily of automatic relevance determination (ARD) kernels, and introduceRFFNet, a new large-scale kernel method that learns the kernel relevances' onthe fly via first-order stochastic optimization. We present an effectiveinitialization scheme for the method's non-convex objective function, evaluateif hard-thresholding RFFNet's learned relevances yield a sensible rule forvariable selection, and perform an extensive ablation study of RFFNet'scomponents. Numerical validation on simulated and real-world data shows thatour approach has a small memory footprint and run-time, achieves low predictionerror, and effectively identifies relevant features, thus leading to moreinterpretable solutions. We supply users with an efficient, PyTorch-basedlibrary, that adheres to the scikit-learn standard API and code for fullyreproducing our results.</description><author>Mateus P. Otto, Rafael Izbicki</author><pubDate>Fri, 12 Apr 2024 15:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06410v2</guid></item><item><title>Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery</title><link>http://arxiv.org/abs/2404.08511v1</link><description>In the rapidly evolving field of artificial intelligence, the ability toharness and integrate knowledge across various domains stands as a paramountchallenge and opportunity. This study introduces a novel approach tocross-domain knowledge discovery through the deployment of multi-AI agents,each specialized in distinct knowledge domains. These AI agents, designed tofunction as domain-specific experts, collaborate in a unified framework tosynthesize and provide comprehensive insights that transcend the limitations ofsingle-domain expertise. By facilitating seamless interaction among theseagents, our platform aims to leverage the unique strengths and perspectives ofeach, thereby enhancing the process of knowledge discovery and decision-making.We present a comparative analysis of the different multi-agent workflowscenarios evaluating their performance in terms of efficiency, accuracy, andthe breadth of knowledge integration. Through a series of experiments involvingcomplex, interdisciplinary queries, our findings demonstrate the superiorcapability of domain specific multi-AI agent system in identifying and bridgingknowledge gaps. This research not only underscores the significance ofcollaborative AI in driving innovation but also sets the stage for futureadvancements in AI-driven, cross-disciplinary research and application. Ourmethods were evaluated on a small pilot data and it showed a trend we expected,if we increase the amount of data we custom train the agents, the trend isexpected to be more smooth.</description><author>Shiva Aryal, Tuyen Do, Bisesh Heyojoo, Sandeep Chataut, Bichar Dip Shrestha Gurung, Venkataramana Gadhamshetty, Etienne Gnimpieba</author><pubDate>Fri, 12 Apr 2024 15:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08511v1</guid></item><item><title>Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction</title><link>http://arxiv.org/abs/2404.08509v1</link><description>Large language models (LLMs) have been driving a new wave of interactive AIapplications across numerous domains. However, efficiently serving LLMinference requests is challenging due to their unpredictable execution timesoriginating from the autoregressive nature of generative models. Existing LLMserving systems exploit first-come-first-serve (FCFS) scheduling, sufferingfrom head-of-line blocking issues. To address the non-deterministic nature ofLLMs and enable efficient interactive LLM serving, we present a speculativeshortest-job-first (SSJF) scheduler that uses a light proxy model to predictLLM output sequence lengths. Our open-source SSJF implementation does notrequire changes to memory management or batching strategies. Evaluations onreal-world datasets and production workload traces show that SSJF reducesaverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6xcompared to FCFS schedulers, across no batching, dynamic batching, andcontinuous batching settings.</description><author>Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Başar, Ravishankar K. Iyer</author><pubDate>Fri, 12 Apr 2024 15:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08509v1</guid></item><item><title>Approximate Stein Classes for Truncated Density Estimation</title><link>http://arxiv.org/abs/2306.00602v2</link><description>Estimating truncated density models is difficult, as these models haveintractable normalising constants and hard to satisfy boundary conditions.Score matching can be adapted to solve the truncated density estimationproblem, but requires a continuous weighting function which takes zero at theboundary and is positive elsewhere. Evaluation of such a weighting function(and its gradient) often requires a closed-form expression of the truncationboundary and finding a solution to a complicated optimisation problem. In thispaper, we propose approximate Stein classes, which in turn leads to a relaxedStein identity for truncated density estimation. We develop a novel discrepancymeasure, truncated kernelised Stein discrepancy (TKSD), which does not requirefixing a weighting function in advance, and can be evaluated using only sampleson the boundary. We estimate a truncated density model by minimising theLagrangian dual of TKSD. Finally, experiments show the accuracy of our methodto be an improvement over previous works even without the explicit functionalform of the boundary.</description><author>Daniel J. Williams, Song Liu</author><pubDate>Fri, 12 Apr 2024 15:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00602v2</guid></item><item><title>Identifying Important Group of Pixels using Interactions</title><link>http://arxiv.org/abs/2401.03785v2</link><description>To better understand the behavior of image classifiers, it is useful tovisualize the contribution of individual pixels to the model prediction. Inthis study, we propose a method, MoXI ($\textbf{Mo}$del e$\textbf{X}$planationby $\textbf{I}$nteractions), that efficiently and accurately identifies a groupof pixels with high prediction confidence. The proposed method employsgame-theoretic concepts, Shapley values and interactions, taking into accountthe effects of individual pixels and the cooperative influence of pixels onmodel confidence. Theoretical analysis and experiments demonstrate that ourmethod better identifies the pixels that are highly contributing to the modeloutputs than widely-used visualization by Grad-CAM, Attention rollout, andShapley value. While prior studies have suffered from the exponentialcomputational cost in the computation of Shapley value and interactions, weshow that this can be reduced to quadratic cost for our task. The code isavailable at https://github.com/KosukeSumiyasu/MoXI.</description><author>Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera</author><pubDate>Fri, 12 Apr 2024 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03785v2</guid></item><item><title>Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis</title><link>http://arxiv.org/abs/2310.19806v3</link><description>With the increase in industrial applications using Answer Set Programming,the need for formal verification tools, particularly for critical applications,has also increased. During the program optimisation process, it would bedesirable to have a tool which can automatically verify whether an optimisedsubprogram can replace the original subprogram. Formally this corresponds tothe problem of verifying the strong equivalence of two programs. In order to doso, the translation tool anthem was developed. It can be used in conjunctionwith an automated theorem prover for classical logic to verify that twoprograms are strongly equivalent. With the current version of anthem, only thestrong equivalence of positive programs with a restricted input language can beverified. This is a result of the translation $\tau^*$ implemented in anthemthat produces formulas in the logic of here-and-there, which coincides withclassical logic only for positive programs. This thesis extends anthem in orderto overcome these limitations. First, the transformation $\sigma^*$ ispresented, which transforms formulas from the logic of here-and-there toclassical logic. A theorem formalises how $\sigma^*$ can be used to expressequivalence in the logic of here-and-there in classical logic. Second, thetranslation $\tau^*$ is extended to programs containing pools. Another theoremshows how $\sigma^*$ can be combined with $\tau^*$ to express the strongequivalence of two programs in classical logic. With $\sigma^*$ and theextended $\tau^*$, it is possible to express the strong equivalence of logicprograms containing negation, simple choices, and pools. Both the extended$\tau^*$ and $\sigma^*$ are implemented in a new version of anthem. Severalexamples of logic programs containing pools, negation, and simple choice rules,which the new version of anthem can translate to classical logic, arepresented. Some a...</description><author>Jan Heuer</author><pubDate>Fri, 12 Apr 2024 15:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19806v3</guid></item><item><title>LaSagnA: Language-based Segmentation Assistant for Complex Queries</title><link>http://arxiv.org/abs/2404.08506v1</link><description>Recent advancements have empowered Large Language Models for Vision (vLLMs)to generate detailed perceptual outcomes, including bounding boxes and masks.Nonetheless, there are two constraints that restrict the further application ofthese vLLMs: the incapability of handling multiple targets per query and thefailure to identify the absence of query objects in the image. In this study,we acknowledge that the main cause of these problems is the insufficientcomplexity of training queries. Consequently, we define the general sequenceformat for complex queries. Then we incorporate a semantic segmentation task inthe current pipeline to fulfill the requirements of training data. Furthermore,we present three novel strategies to effectively handle the challenges arisingfrom the direct integration of the proposed format. The effectiveness of ourmodel in processing complex queries is validated by the comparable results withconventional methods on both close-set and open-set semantic segmentationdatasets. Additionally, we outperform a series of vLLMs in reasoning andreferring segmentation, showcasing our model's remarkable capabilities. Werelease the code at https://github.com/congvvc/LaSagnA.</description><author>Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, Lin Ma</author><pubDate>Fri, 12 Apr 2024 15:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08506v1</guid></item><item><title>Beyond Bayesian Model Averaging over Paths in Probabilistic Programs with Stochastic Support</title><link>http://arxiv.org/abs/2310.14888v2</link><description>The posterior in probabilistic programs with stochastic support decomposes asa weighted sum of the local posterior distributions associated with eachpossible program path. We show that making predictions with this full posteriorimplicitly performs a Bayesian model averaging (BMA) over paths. This ispotentially problematic, as BMA weights can be unstable due to modelmisspecification or inference approximations, leading to sub-optimalpredictions in turn. To remedy this issue, we propose alternative mechanismsfor path weighting: one based on stacking and one based on ideas fromPAC-Bayes. We show how both can be implemented as a cheap post-processing stepon top of existing inference engines. In our experiments, we find them to bemore robust and lead to better predictions compared to the default BMA weights.</description><author>Tim Reichelt, Luke Ong, Tom Rainforth</author><pubDate>Fri, 12 Apr 2024 15:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14888v2</guid></item><item><title>3D Human Scan With A Moving Event Camera</title><link>http://arxiv.org/abs/2404.08504v1</link><description>Capturing the 3D human body is one of the important tasks in computer visionwith a wide range of applications such as virtual reality and sports analysis.However, conventional frame cameras are limited by their temporal resolutionand dynamic range, which imposes constraints in real-world application setups.Event cameras have the advantages of high temporal resolution and high dynamicrange (HDR), but the development of event-based methods is necessary to handledata with different characteristics. This paper proposes a novel event-basedmethod for 3D pose estimation and human mesh recovery. Prior work onevent-based human mesh recovery require frames (images) as well as event data.The proposed method solely relies on events; it carves 3D voxels by moving theevent camera around a stationary body, reconstructs the human pose and mesh byattenuated rays, and fit statistical body models, preserving high-frequencydetails. The experimental results show that the proposed method outperformsconventional frame-based methods in the estimation accuracy of both pose andbody mesh. We also demonstrate results in challenging situations where aconventional camera has motion blur. This is the first to demonstrateevent-only human mesh recovery, and we hope that it is the first step towardachieving robust and accurate 3D human body scanning from vision sensors.</description><author>Kai Kohyama, Shintaro Shiba, Yoshimitsu Aoki</author><pubDate>Fri, 12 Apr 2024 15:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08504v1</guid></item><item><title>Score Matching for Truncated Density Estimation on a Manifold</title><link>http://arxiv.org/abs/2206.14668v2</link><description>When observations are truncated, we are limited to an incomplete picture ofour dataset. Recent methods propose to use score matching for truncated densityestimation, where the access to the intractable normalising constant is notrequired. We present a novel extension of truncated score matching to aRiemannian manifold with boundary. Applications are presented for the vonMises-Fisher and Kent distributions on a two dimensional sphere in$\mathbb{R}^3$, as well as a real-world application of extreme stormobservations in the USA. In simulated data experiments, our score matchingestimator is able to approximate the true parameter values with a lowestimation error and shows improvements over a naive maximum likelihoodestimator.</description><author>Daniel J. Williams, Song Liu</author><pubDate>Fri, 12 Apr 2024 15:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14668v2</guid></item><item><title>Integrated Variational Fourier Features for Fast Spatial Modelling with Gaussian Processes</title><link>http://arxiv.org/abs/2308.14142v2</link><description>Sparse variational approximations are popular methods for scaling upinference and learning in Gaussian processes to larger datasets. For $N$training points, exact inference has $O(N^3)$ cost; with $M \ll N$ features,state of the art sparse variational methods have $O(NM^2)$ cost. Recently,methods have been proposed using more sophisticated features; these promise$O(M^3)$ cost, with good performance in low dimensional tasks such as spatialmodelling, but they only work with a very limited class of kernels, excludingsome of the most commonly used. In this work, we propose integrated Fourierfeatures, which extends these performance benefits to a very broad class ofstationary covariance functions. We motivate the method and choice ofparameters from a convergence analysis and empirical exploration, and showpractical speedup in synthetic and real world spatial regression tasks.</description><author>Talay M Cheema, Carl Edward Rasmussen</author><pubDate>Fri, 12 Apr 2024 15:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14142v2</guid></item><item><title>Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing</title><link>http://arxiv.org/abs/2404.00589v2</link><description>Handling graph data is one of the most difficult tasks. Traditionaltechniques, such as those based on geometry and matrix factorization, rely onassumptions about the data relations that become inadequate when handling largeand complex graph data. On the other hand, deep learning approaches demonstratepromising results in handling large graph data, but they often fall short ofproviding interpretable explanations. To equip the graph processing with bothhigh accuracy and explainability, we introduce a novel approach that harnessesthe power of a large language model (LLM), enhanced by an uncertainty-awaremodule to provide a confidence score on the generated answer. We experimentwith our approach on two graph processing tasks: few-shot knowledge graphcompletion and graph classification. Our results demonstrate that throughparameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithmsby a substantial margin across ten diverse benchmark datasets. Moreover, toaddress the challenge of explainability, we propose an uncertainty estimationbased on perturbation, along with a calibration scheme to quantify theconfidence scores of the generated answers. Our confidence measure achieves anAUC of 0.8 or higher on seven out of the ten datasets in predicting thecorrectness of the answer generated by LLM.</description><author>Zhenyu Qian, Yiming Qian, Yuting Song, Fei Gao, Hai Jin, Chen Yu, Xia Xie</author><pubDate>Fri, 12 Apr 2024 15:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00589v2</guid></item><item><title>Analyzing and Overcoming Local Optima in Complex Multi-Objective Optimization by Decomposition-Based Evolutionary Algorithms</title><link>http://arxiv.org/abs/2404.08501v1</link><description>When addressing the challenge of complex multi-objective optimizationproblems, particularly those with non-convex and non-uniform Pareto fronts,Decomposition-based Multi-Objective Evolutionary Algorithms (MOEADs) oftenconverge to local optima, thereby limiting solution diversity. Despite itssignificance, this issue has received limited theoretical exploration. Througha comprehensive geometric analysis, we identify that the traditional method ofReference Point (RP) selection fundamentally contributes to this challenge. Inresponse, we introduce an innovative RP selection strategy, the WeightVector-Guided and Gaussian-Hybrid method, designed to overcome the local optimaissue. This approach employs a novel RP type that aligns with weight vectordirections and integrates a Gaussian distribution to combine three distinct RPcategories. Our research comprises two main experimental components: anablation study involving 14 algorithms within the MOEADs framework, spanningfrom 2014 to 2022, to validate our theoretical framework, and a series ofempirical tests to evaluate the effectiveness of our proposed method againstboth traditional and cutting-edge alternatives. Results demonstrate that ourmethod achieves remarkable improvements in both population diversity andconvergence.</description><author>Ting Dong, Haoxin Wang, Hengxi Zhang, Wenbo Ding</author><pubDate>Fri, 12 Apr 2024 15:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08501v1</guid></item><item><title>On the Minimax Regret in Online Ranking with Top-k Feedback</title><link>http://arxiv.org/abs/2309.02425v2</link><description>In online ranking, a learning algorithm sequentially ranks a set of items andreceives feedback on its ranking in the form of relevance scores. Sinceobtaining relevance scores typically involves human annotation, it is of greatinterest to consider a partial feedback setting where feedback is restricted tothe top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed aframework to analyze online ranking algorithms with top $k$ feedback. A keyelement in their work was the use of techniques from partial monitoring. Inthis paper, we further investigate online ranking with top $k$ feedback andsolve some open problems posed by Chaudhuri and Tewari [2017]. We provide afull characterization of minimax regret rates with the top $k$ feedback modelfor all $k$ and for the following ranking performance measures: Pairwise Loss,Discounted Cumulative Gain, and Precision@n. In addition, we give an efficientalgorithm that achieves the minimax regret rate for Precision@n.</description><author>Mingyuan Zhang, Ambuj Tewari</author><pubDate>Fri, 12 Apr 2024 15:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02425v2</guid></item><item><title>Dataset Reset Policy Optimization for RLHF</title><link>http://arxiv.org/abs/2404.08495v1</link><description>Reinforcement Learning (RL) from Human Preference-based feedback is a popularparadigm for fine-tuning generative models, which has produced impressivemodels such as GPT-4 and Claude3 Opus. This framework often consists of twosteps: learning a reward model from an offline preference dataset followed byrunning online RL to optimize the learned reward model. In this work,leveraging the idea of reset, we propose a new RLHF algorithm with provableguarantees. Motivated by the fact that offline preference dataset providesinformative states (i.e., data that is preferred by the labelers), our newalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existingoffline preference dataset into the online policy training procedure viadataset reset: it directly resets the policy optimizer to the states in theoffline dataset, instead of always starting from the initial statedistribution. In theory, we show that DR-PO learns to perform at least as goodas any policy that is covered by the offline dataset under general functionapproximation with finite sample complexity. In experiments, we demonstratethat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)dataset, the generation from DR-PO is better than that from Proximal PolicyOptimization (PPO) and Direction Preference Optimization (DPO), under themetric of GPT4 win-rate. Code for this work can be found athttps://github.com/Cornell-RL/drpo.</description><author>Jonathan D. Chang, Wenhao Shan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D. Lee, Wen Sun</author><pubDate>Fri, 12 Apr 2024 15:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08495v1</guid></item><item><title>FoodLMM: A Versatile Food Assistant using Large Multi-modal Model</title><link>http://arxiv.org/abs/2312.14991v2</link><description>Large Multi-modal Models (LMMs) have made impressive progress in manyvision-language tasks. Nevertheless, the performance of general LMMs inspecific domains is still far from satisfactory. This paper proposes FoodLMM, aversatile food assistant based on LMMs with various capabilities, includingfood recognition, ingredient recognition, recipe generation, nutritionestimation, food segmentation and multi-round conversation. To facilitateFoodLMM to deal with tasks beyond pure text output, we introduce a series ofnovel task-specific tokens and heads, enabling the model to predict foodnutritional values and multiple segmentation masks. We adopt a two-stagetraining strategy. In the first stage, we utilize multiple public foodbenchmarks for multi-task learning by leveraging the instruct-followingparadigm. In the second stage, we construct a multi-round conversation datasetand a reasoning segmentation dataset to fine-tune the model, enabling it toconduct professional dialogues and generate segmentation masks based on complexreasoning in the food domain. Our fine-tuned FoodLMM achieves state-of-the-artresults across several food benchmarks. We will make our code, models anddatasets publicly available.</description><author>Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, Yu-Gang Jiang, Chong-Wah Ngo</author><pubDate>Fri, 12 Apr 2024 15:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14991v2</guid></item><item><title>Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation</title><link>http://arxiv.org/abs/2404.08491v1</link><description>Large-scale multilingual Pretrained Language Models (mPLMs) yield impressiveperformance on cross-language tasks, yet significant performance disparitiesexist across different languages within the same mPLM. Previous studiesendeavored to narrow these disparities by supervise fine-tuning the mPLMs withmultilingual data. However, obtaining labeled multilingual data istime-consuming, and fine-tuning mPLM with limited labeled multilingual datamerely encapsulates the knowledge specific to the labeled data. Therefore, weintroduce ALSACE to leverage the learned knowledge from the well-performinglanguages to guide under-performing ones within the same mPLM, eliminating theneed for additional labeled multilingual data. Experiments show that ALSACEeffectively mitigates language-level performance disparity across various mPLMswhile showing the competitive performance on different multilingual NLU tasks,ranging from full resource to limited resource settings. The code for ourapproach is available at https://github.com/pkunlp-icler/ALSACE.</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Yufeng He, Kaikai An, Baobao Chang</author><pubDate>Fri, 12 Apr 2024 15:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08491v1</guid></item><item><title>Multimodal Learning for Materials</title><link>http://arxiv.org/abs/2312.00111v3</link><description>Artificial intelligence is transforming computational materials science,improving the prediction of material properties, and accelerating the discoveryof novel materials. Recently, publicly available material data repositorieshave grown rapidly. This growth encompasses not only more materials, but also agreater variety and quantity of their associated properties. Existing machinelearning efforts in materials science focus primarily on single-modality tasks,i.e., relationships between materials and a single physical property, thus nottaking advantage of the rich and multimodal set of material properties. Here,we introduce Multimodal Learning for Materials (MultiMat), which enablesself-supervised multi-modality training of foundation models for materials. Wedemonstrate our framework's potential using data from the Materials Projectdatabase on multiple axes: (i) MultiMat achieves state-of-the-art performancefor challenging material property prediction tasks; (ii) MultiMat enables noveland accurate material discovery via latent space similarity, enabling screeningfor stable materials with desired properties; and (iii) MultiMat encodesinterpretable emergent features that may provide novel scientific insights.</description><author>Viggo Moro, Charlotte Loh, Rumen Dangovski, Ali Ghorashi, Andrew Ma, Zhuo Chen, Samuel Kim, Peter Y. Lu, Thomas Christensen, Marin Soljačić</author><pubDate>Fri, 12 Apr 2024 15:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00111v3</guid></item><item><title>SpectralMamba: Efficient Mamba for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2404.08489v1</link><description>Recurrent neural networks and Transformers have recently dominated mostapplications in hyperspectral (HS) imaging, owing to their capability tocapture long-range dependencies from spectrum sequences. However, despite thesuccess of these sequential architectures, the non-ignorable inefficiencycaused by either difficulty in parallelization or computationally prohibitiveattention still hinders their practicality, especially for large-scaleobservation in remote sensing scenarios. To address this issue, we hereinpropose SpectralMamba -- a novel state space model incorporated efficient deeplearning framework for HS image classification. SpectralMamba features thesimplified but adequate modeling of HS data dynamics at two levels. First, inspatial-spectral space, a dynamical mask is learned by efficient convolutionsto simultaneously encode spatial regularity and spectral peculiarity, thusattenuating the spectral variability and confusion in discriminativerepresentation learning. Second, the merged spectrum can then be efficientlyoperated in the hidden state space with all parameters learned input-dependent,yielding selectively focused responses without reliance on redundant attentionor imparallelizable recurrence. To explore the room for further computationaldownsizing, a piece-wise scanning mechanism is employed in-between,transferring approximately continuous spectrum into sequences with squeezedlength while maintaining short- and long-term contextual profiles amonghundreds of bands. Through extensive experiments on four benchmark HS datasetsacquired by satellite-, aircraft-, and UAV-borne imagers, SpectralMambasurprisingly creates promising win-wins from both performance and efficiencyperspectives.</description><author>Jing Yao, Danfeng Hong, Chenyu Li, Jocelyn Chanussot</author><pubDate>Fri, 12 Apr 2024 15:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08489v1</guid></item><item><title>Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian</title><link>http://arxiv.org/abs/2404.08488v1</link><description>This paper proposes a test to perform Thematic Analysis (TA) with LargeLanguage Model (LLM) on data which is in a different language than English.While there has been initial promising work on using pre-trained LLMs for TA ondata in English, we lack any tests on whether these models can reasonablyperform the same analysis with good quality in other language. In this paper atest will be proposed using an open access dataset of semi-structuredinterviews in Italian. The test shows that a pre-trained model can perform sucha TA on the data, also using prompts in Italian. A comparative test shows themodel capacity to produce themes which have a good resemblance with thoseproduced independently by human researchers. The main implication of this studyis that pre-trained LLMs may thus be suitable to support analysis inmultilingual situations, so long as the language is supported by the modelused.</description><author>Stefano De Paoli</author><pubDate>Fri, 12 Apr 2024 15:10:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08488v1</guid></item><item><title>Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding</title><link>http://arxiv.org/abs/2311.08380v2</link><description>Minimum Bayes Risk (MBR) decoding can significantly improve translationperformance of Multilingual Large Language Models (MLLMs). However, MBRdecoding is computationally expensive. We show how the recently developedReinforcement Learning technique, Direct Preference Optimization (DPO), canfine-tune MLLMs to get the gains of MBR without any additional computation ininference. Our method uses only a small monolingual fine-tuning set and yieldssignificantly improved performance on multiple NMT test sets compared to MLLMswithout DPO.</description><author>Guangyu Yang, Jinghong Chen, Weizhe Lin, Bill Byrne</author><pubDate>Fri, 12 Apr 2024 15:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08380v2</guid></item><item><title>Semantic Communication for Cooperative Multi-Task Processing over Wireless Networks</title><link>http://arxiv.org/abs/2404.08483v1</link><description>In this paper, we have expanded the current status of semantic communicationlimited to processing one task to a more general system that can handlemultiple tasks concurrently. In pursuit of this, we first introduced ourdefinition of the "semantic source", enabling the interpretation of multiplesemantics based on a single observation. A semantic encoder design is thenintroduced, featuring the division of the encoder into a common unit andmultiple specific units enabling cooperative multi-task processing. Simulationresults demonstrate the effectiveness of the proposed semantic source and thesystem design. Our approach employs information maximization (infomax) andend-to-end design principles.</description><author>Ahmad Halimi Razlighi, Carsten Bockelmann, Armin Dekorsy</author><pubDate>Fri, 12 Apr 2024 15:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08483v1</guid></item><item><title>A Quadratic Synchronization Rule for Distributed Deep Learning</title><link>http://arxiv.org/abs/2310.14423v2</link><description>In distributed deep learning with data parallelism, synchronizing gradientsat each training step can cause a huge communication overhead, especially whenmany nodes work together to train large models. Local gradient methods, such asLocal SGD, address this issue by allowing workers to compute locally for $H$steps without synchronizing with others, hence reducing communicationfrequency. While $H$ has been viewed as a hyperparameter to trade optimizationefficiency for communication cost, recent research indicates that setting aproper $H$ value can lead to generalization improvement. Yet, selecting aproper $H$ is elusive. This work proposes a theory-grounded method fordetermining $H$, named the Quadratic Synchronization Rule (QSR), whichrecommends dynamically setting $H$ in proportion to $\frac{1}{\eta^2}$ as thelearning rate $\eta$ decays over time. Extensive ImageNet experiments on ResNetand ViT show that local gradient methods with QSR consistently improve the testaccuracy over other synchronization strategies. Compared with the standard dataparallel training, QSR enables Local AdamW on ViT-B to cut the training time on16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at thesame time, achieves $1.16\%$ or $0.84\%$ higher top-1 validation accuracy.</description><author>Xinran Gu, Kaifeng Lyu, Sanjeev Arora, Jingzhao Zhang, Longbo Huang</author><pubDate>Fri, 12 Apr 2024 14:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14423v2</guid></item><item><title>Transformer based Pluralistic Image Completion with Reduced Information Loss</title><link>http://arxiv.org/abs/2404.00513v2</link><description>Transformer based methods have achieved great success in image inpaintingrecently. However, we find that these solutions regard each pixel as a token,thus suffering from an information loss issue from two aspects: 1) Theydownsample the input image into much lower resolutions for efficiencyconsideration. 2) They quantize $256^3$ RGB values to a small number (such as512) of quantized color values. The indices of quantized pixels are used astokens for the inputs and prediction targets of the transformer. To mitigatethese issues, we propose a new transformer based framework called "PUT".Specifically, to avoid input downsampling while maintaining computationefficiency, we design a patch-based auto-encoder P-VQVAE. The encoder convertsthe masked image into non-overlapped patch tokens and the decoder recovers themasked regions from the inpainted tokens while keeping the unmasked regionsunchanged. To eliminate the information loss caused by input quantization, anUn-quantized Transformer is applied. It directly takes features from theP-VQVAE encoder as input without any quantization and only regards thequantized tokens as prediction targets. Furthermore, to make the inpaintingprocess more controllable, we introduce semantic and structural conditions asextra guidance. Extensive experiments show that our method greatly outperformsexisting transformer based methods on image fidelity and achieves much higherdiversity and better fidelity than state-of-the-art pluralistic inpaintingmethods on complex large-scale datasets (e.g., ImageNet). Codes are availableat https://github.com/liuqk3/PUT.</description><author>Qiankun Liu, Yuqi Jiang, Zhentao Tan, Dongdong Chen, Ying Fu, Qi Chu, Gang Hua, Nenghai Yu</author><pubDate>Fri, 12 Apr 2024 14:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00513v2</guid></item><item><title>Decoding AI: The inside story of data analysis in ChatGPT</title><link>http://arxiv.org/abs/2404.08480v1</link><description>As a result of recent advancements in generative AI, the field of DataScience is prone to various changes. This review critically examines the DataAnalysis (DA) capabilities of ChatGPT assessing its performance across a widerange of tasks. While DA provides researchers and practitioners withunprecedented analytical capabilities, it is far from being perfect, and it isimportant to recognize and address its limitations.</description><author>Ozan Evkaya, Miguel de Carvalho</author><pubDate>Fri, 12 Apr 2024 14:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08480v1</guid></item><item><title>New Efficient Visual OILU Markers</title><link>http://arxiv.org/abs/2404.08477v1</link><description>Basic patterns are the source of a wide range of more or less complexgeometric structures. We will exploit such patterns to develop new efficientvisual markers. Besides being projective invariants, the proposed markers allowproducing rich panel of unique identifiers, highly required forresource-intensive navigation and augmented reality applications. The spiraltopology of our markers permits the validation of an accurate identificationscheme, which is based on level set methods. The robustness of the markersagainst acquisition and geometric distortions is validated by extensiveexperimental tests.</description><author>Youssef Chahir, Messaoud Mostefai, Hamza Saida</author><pubDate>Fri, 12 Apr 2024 14:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08477v1</guid></item><item><title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title><link>http://arxiv.org/abs/2404.08476v1</link><description>We measure the Out-of-domain uncertainty in the prediction of Neural Networksusing a statistical notion called ``Lens Depth'' (LD) combined with FermatDistance, which is able to capture precisely the ``depth'' of a point withrespect to a distribution in feature space, without any assumption about theform of distribution. Our method has no trainable parameter. The method isapplicable to any classification model as it is applied directly in featurespace at test time and does not intervene in training process. As such, it doesnot impact the performance of the original model. The proposed method givesexcellent qualitative result on toy datasets and can give competitive or betteruncertainty estimation on standard deep learning datasets compared to strongbaseline methods.</description><author>Hai-Vy Nguyen, Fabrice Gamboa, Reda Chhaibi, Sixin Zhang, Serge Gratton, Thierry Giaccone</author><pubDate>Fri, 12 Apr 2024 14:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08476v1</guid></item><item><title>Solving Parametric PDEs with Radial Basis Functions and Deep Neural Networks</title><link>http://arxiv.org/abs/2404.06834v2</link><description>We propose the POD-DNN, a novel algorithm leveraging deep neural networks(DNNs) along with radial basis functions (RBFs) in the context of the properorthogonal decomposition (POD) reduced basis method (RBM), aimed atapproximating the parametric mapping of parametric partial differentialequations on irregular domains. The POD-DNN algorithm capitalizes on thelow-dimensional characteristics of the solution manifold for parametricequations, alongside the inherent offline-online computational strategy of RBMand DNNs. In numerical experiments, POD-DNN demonstrates significantlyaccelerated computation speeds during the online phase. Compared to otheralgorithms that utilize RBF without integrating DNNs, POD-DNN substantiallyimproves the computational speed in the online inference process. Furthermore,under reasonable assumptions, we have rigorously derived upper bounds on thecomplexity of approximating parametric mappings with POD-DNN, thereby providinga theoretical analysis of the algorithm's empirical performance.</description><author>Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng</author><pubDate>Fri, 12 Apr 2024 14:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06834v2</guid></item><item><title>WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space</title><link>http://arxiv.org/abs/2311.13570v2</link><description>Modern learning-based approaches to 3D-aware image synthesis achieve highphotorealism and 3D-consistent viewpoint changes for the generated images.Existing approaches represent instances in a shared canonical space. However,for in-the-wild datasets a shared canonical system can be difficult to defineor might not even exist. In this work, we instead model instances in viewspace, alleviating the need for posed images and learned camera distributions.We find that in this setting, existing GAN-based methods are prone togenerating flat geometry and struggle with distribution coverage. We hencepropose WildFusion, a new approach to 3D-aware image synthesis based on latentdiffusion models (LDMs). We first train an autoencoder that infers a compressedlatent representation, which additionally captures the images' underlying 3Dstructure and enables not only reconstruction but also novel view synthesis. Tolearn a faithful 3D representation, we leverage cues from monocular depthprediction. Then, we train a diffusion model in the 3D-aware latent space,thereby enabling synthesis of high-quality 3D-consistent image samples,outperforming recent state-of-the-art GAN-based methods. Importantly, our3D-aware LDM is trained without any direct supervision from multiview images or3D geometry and does not require posed images or learned pose or cameradistributions. It directly learns a 3D representation without relying oncanonical camera coordinates. This opens up promising research avenues forscalable 3D-aware image synthesis and 3D content creation from in-the-wildimage data. See https://katjaschwarz.github.io/wildfusion for videos of our 3Dresults.</description><author>Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler, Andreas Geiger, Karsten Kreis</author><pubDate>Fri, 12 Apr 2024 14:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13570v2</guid></item><item><title>TSLANet: Rethinking Transformers for Time Series Representation Learning</title><link>http://arxiv.org/abs/2404.08472v1</link><description>Time series data, characterized by its intrinsic long and short-rangedependencies, poses a unique challenge across analytical applications. WhileTransformer-based models excel at capturing long-range dependencies, they facelimitations in noise sensitivity, computational efficiency, and overfittingwith smaller datasets. In response, we introduce a novel Time SeriesLightweight Adaptive Network (TSLANet), as a universal convolutional model fordiverse time series tasks. Specifically, we propose an Adaptive Spectral Block,harnessing Fourier analysis to enhance feature representation and to captureboth long-term and short-term interactions while mitigating noise via adaptivethresholding. Additionally, we introduce an Interactive Convolution Block andleverage self-supervised learning to refine the capacity of TSLANet fordecoding complex temporal patterns and improve its robustness on differentdatasets. Our comprehensive experiments demonstrate that TSLANet outperformsstate-of-the-art models in various tasks spanning classification, forecasting,and anomaly detection, showcasing its resilience and adaptability across aspectrum of noise levels and data sizes. The code is available at\url{https://github.com/emadeldeen24/TSLANet}</description><author>Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li</author><pubDate>Fri, 12 Apr 2024 14:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08472v1</guid></item><item><title>OTTER: Improving Zero-Shot Classification via Optimal Transport</title><link>http://arxiv.org/abs/2404.08461v1</link><description>Popular zero-shot models suffer due to artifacts inherited from pretraining.A particularly detrimental artifact, caused by unbalanced web-scale pretrainingdata, is mismatched label distribution. Existing approaches that seek to repairthe label distribution are not suitable in zero-shot settings, as they haveincompatible requirements such as access to labeled downstream task data orknowledge of the true label balance in the pretraining distribution. Wesidestep these challenges and introduce a simple and lightweight approach toadjust pretrained model predictions via optimal transport. Our techniquerequires only an estimate of the label distribution of a downstream task.Theoretically, we characterize the improvement produced by our procedure undercertain mild conditions and provide bounds on the error caused bymisspecification. Empirically, we validate our method in a wide array ofzero-shot image and text classification tasks, improving accuracy by 4.8% and15.9% on average, and beating baselines like Prior Matching -- often bysignificant margins -- in 17 out of 21 datasets.</description><author>Changho Shin, Jitian Zhao, Sonia Cromp, Harit Vishwakarma, Frederic Sala</author><pubDate>Fri, 12 Apr 2024 14:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08461v1</guid></item><item><title>Unsupervised Learning of Group Invariant and Equivariant Representations</title><link>http://arxiv.org/abs/2202.07559v3</link><description>Equivariant neural networks, whose hidden features transform according torepresentations of a group G acting on the data, exhibit training efficiencyand an improved generalisation performance. In this work, we extend groupinvariant and equivariant representation learning to the field of unsuperviseddeep learning. We propose a general learning strategy based on anencoder-decoder framework in which the latent representation is separated in aninvariant term and an equivariant group action component. The key idea is thatthe network learns to encode and decode data to and from a group-invariantrepresentation by additionally learning to predict the appropriate group actionto align input and output pose to solve the reconstruction task. We derive thenecessary conditions on the equivariant encoder, and we present a constructionvalid for any G, both discrete and continuous. We describe explicitly ourconstruction for rotations, translations and permutations. We test the validityand the robustness of our approach in a variety of experiments with diversedata types employing different network architectures.</description><author>Robin Winter, Marco Bertolini, Tuan Le, Frank Noé, Djork-Arné Clevert</author><pubDate>Fri, 12 Apr 2024 14:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.07559v3</guid></item><item><title>On the Independence Assumption in Neurosymbolic Learning</title><link>http://arxiv.org/abs/2404.08458v1</link><description>State-of-the-art neurosymbolic learning systems use probabilistic reasoningto guide neural networks towards predictions that conform to logicalconstraints over symbols. Many such systems assume that the probabilities ofthe considered symbols are conditionally independent given the input tosimplify learning and reasoning. We study and criticise this assumption,highlighting how it can hinder optimisation and prevent uncertaintyquantification. We prove that loss functions bias conditionally independentneural networks to become overconfident in their predictions. As a result, theyare unable to represent uncertainty over multiple valid options. Furthermore,we prove that these loss functions are difficult to optimise: they arenon-convex, and their minima are usually highly disconnected. Our theoreticalanalysis gives the foundation for replacing the conditional independenceassumption and designing more expressive neurosymbolic probabilistic models.</description><author>Emile van Krieken, Pasquale Minervini, Edoardo M. Ponti, Antonio Vergari</author><pubDate>Fri, 12 Apr 2024 14:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08458v1</guid></item><item><title>A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations</title><link>http://arxiv.org/abs/2404.08456v1</link><description>In this work, we propose a novel backward differential deep learning-basedalgorithm for solving high-dimensional nonlinear backward stochasticdifferential equations (BSDEs), where the deep neural network (DNN) models aretrained not only on the inputs and labels but also the differentials of thecorresponding labels. This is motivated by the fact that differential deeplearning can provide an efficient approximation of the labels and theirderivatives with respect to inputs. The BSDEs are reformulated as differentialdeep learning problems by using Malliavin calculus. The Malliavin derivativesof solution to a BSDE satisfy themselves another BSDE, resulting thus in asystem of BSDEs. Such formulation requires the estimation of the solution, itsgradient, and the Hessian matrix, represented by the triple of processes$\left(Y, Z, \Gamma\right).$ All the integrals within this system arediscretized by using the Euler-Maruyama method. Subsequently, DNNs are employedto approximate the triple of these unknown processes. The DNN parameters arebackwardly optimized at each time step by minimizing a differential learningtype loss function, which is defined as a weighted sum of the dynamics of thediscretized BSDE system, with the first term providing the dynamics of theprocess $Y$ and the other the process $Z$. An error analysis is carried out toshow the convergence of the proposed algorithm. Various numerical experimentsup to $50$ dimensions are provided to demonstrate the high efficiency. Boththeoretically and numerically, it is demonstrated that our proposed scheme ismore efficient compared to other contemporary deep learning-basedmethodologies, especially in the computation of the process $\Gamma$.</description><author>Lorenc Kapllani, Long Teng</author><pubDate>Fri, 12 Apr 2024 14:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08456v1</guid></item><item><title>Lightweight Multi-System Multivariate Interconnection and Divergence Discovery</title><link>http://arxiv.org/abs/2404.08453v1</link><description>Identifying outlier behavior among sensors and subsystems is essential fordiscovering faults and facilitating diagnostics in large systems. At the sametime, exploring large systems with numerous multivariate data sets ischallenging. This study presents a lightweight interconnection and divergencediscovery mechanism (LIDD) to identify abnormal behavior in multi-systemenvironments. The approach employs a multivariate analysis technique that firstestimates the similarity heatmaps among the sensors for each system and thenapplies information retrieval algorithms to provide relevant multi-levelinterconnection and discrepancy details. Our experiment on the readout systemsof the Hadron Calorimeter of the Compact Muon Solenoid (CMS) experiment at CERNdemonstrates the effectiveness of the proposed method. Our approach clustersreadout systems and their sensors consistent with the expected calorimeterinterconnection configurations, while capturing unusual behavior in divergentclusters and estimating their root causes.</description><author>Mulugeta Weldezgina Asres, Christian Walter Omlin, Jay Dittmann, Pavel Parygin, Joshua Hiltbrand, Seth I. Cooper, Grace Cummings, David Yu</author><pubDate>Fri, 12 Apr 2024 14:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08453v1</guid></item><item><title>MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face Forgery Detection</title><link>http://arxiv.org/abs/2404.08452v1</link><description>Deepfakes have recently raised significant trust issues and security concernsamong the public. Compared to CNN face forgery detectors, ViT-based methodstake advantage of the expressivity of transformers, achieving superiordetection performance. However, these approaches still exhibit the followinglimitations: (1). Fully fine-tuning ViT-based models from ImageNet weightsdemands substantial computational and storage resources; (2). ViT-based methodsstruggle to capture local forgery clues, leading to model bias and limitedgeneralizability. To tackle these challenges, this work introducesMixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalizedyet parameter-efficient ViT-based approach. MoE-FFD only updates lightweightLow-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbonefrozen, thereby achieving parameter-efficient training. Moreover, MoE-FFDleverages the expressivity of transformers and local priors of CNNs tosimultaneously extract global and local forgery clues. Additionally, novel MoEmodules are designed to scale the model's capacity and select optimal forgeryexperts, further enhancing forgery detection performance. The proposed MoElearning scheme can be seamlessly adapted to various transformer backbones in aplug-and-play manner. Extensive experimental results demonstrate that theproposed method achieves state-of-the-art face forgery detection performancewith reduced parameter overhead. The code will be released upon acceptance.</description><author>Chenqi Kong, Anwei Luo, Song Xia, Yi Yu, Haoliang Li, Alex C. Kot</author><pubDate>Fri, 12 Apr 2024 14:02:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08452v1</guid></item><item><title>Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing Clues</title><link>http://arxiv.org/abs/2404.08450v1</link><description>Face recognition systems are frequently subjected to a variety of physicaland digital attacks of different types. Previous methods have achievedsatisfactory performance in scenarios that address physical attacks and digitalattacks, respectively. However, few methods are considered to integrate a modelthat simultaneously addresses both physical and digital attacks, implying thenecessity to develop and maintain multiple models. To jointly detect physicaland digital attacks within a single model, we propose an innovative approachthat can adapt to any network architecture. Our approach mainly contains twotypes of data augmentation, which we call Simulated Physical Spoofing Cluesaugmentation (SPSC) and Simulated Digital Spoofing Clues augmentation (SDSC).SPSC and SDSC augment live samples into simulated attack samples by simulatingspoofing clues of physical and digital attacks, respectively, whichsignificantly improve the capability of the model to detect "unseen" attacktypes. Extensive experiments show that SPSC and SDSC can achievestate-of-the-art generalization in Protocols 2.1 and 2.2 of the UniAttackDatadataset, respectively. Our method won first place in "Unified Physical-DigitalFace Attack Detection" of the 5th Face Anti-spoofing Challenge@CVPR2024. Ourfinal submission obtains 3.75% APCER, 0.93% BPCER, and 2.34% ACER,respectively. Our code is available athttps://github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.</description><author>Xianhua He, Dashuang Liang, Song Yang, Zhanlong Hao, Hui Ma, Binjie Mao, Xi Li, Yao Wang, Pengfei Yan, Ajian Liu</author><pubDate>Fri, 12 Apr 2024 14:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08450v1</guid></item></channel></rss>