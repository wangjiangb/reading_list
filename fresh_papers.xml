<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 11 May 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</title><link>http://arxiv.org/abs/2305.06356v1</link><description>Representing human performance at high-fidelity is an essential buildingblock in diverse applications, such as film production, computer games orvideoconferencing. To close the gap to production-level quality, we introduceHumanRF, a 4D dynamic neural scene representation that captures full-bodyappearance in motion from multi-view video input, and enables playback fromnovel, unseen viewpoints. Our novel representation acts as a dynamic videoencoding that captures fine details at high compression rates by factorizingspace-time into a temporal matrix-vector decomposition. This allows us toobtain temporally coherent reconstructions of human actors for long sequences,while representing high-resolution details even in the context of challengingmotion. While most research focuses on synthesizing at resolutions of 4MP orlower, we address the challenge of operating at 12MP. To this end, we introduceActorsHQ, a novel multi-view dataset that provides 12MP footage from 160cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. Wedemonstrate challenges that emerge from using such high-resolution data andshow that our newly introduced HumanRF effectively leverages this data, makinga significant step towards production-level quality novel view synthesis.</description><author>Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner</author><pubDate>Wed, 10 May 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06356v1</guid></item><item><title>VideoChat: Chat-Centric Video Understanding</title><link>http://arxiv.org/abs/2305.06355v1</link><description>In this study, we initiate an exploration into video understanding byintroducing VideoChat, an end-to-end chat-centric video understanding system.It integrates video foundation models and large language models via a learnableneural interface, excelling in spatiotemporal reasoning, event localization,and causal relationship inference. To instructively tune this system, wepropose a video-centric instruction dataset, composed of thousands of videosmatched with detailed descriptions and conversations. This dataset emphasizesspatiotemporal reasoning and causal relationships, providing a valuable assetfor training chat-centric video understanding systems. Preliminary qualitativeexperiments reveal our system's potential across a broad spectrum of videoapplications and set the standard for future research. Access our code and dataat https://github.com/OpenGVLab/Ask-Anything</description><author>KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao</author><pubDate>Wed, 10 May 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06355v1</guid></item><item><title>Reconstructing Animatable Categories from Videos</title><link>http://arxiv.org/abs/2305.06351v1</link><description>Building animatable 3D models is challenging due to the need for 3D scans,laborious registration, and manual rigging, which are difficult to scale toarbitrary categories. Recently, differentiable rendering provides a pathway toobtain high-quality 3D models from monocular videos, but these are limited torigid categories or single instances. We present RAC that builds category 3Dmodels from monocular videos while disentangling variations over instances andmotion over time. Three key ideas are introduced to solve this problem: (1)specializing a skeleton to instances via optimization, (2) a method for latentspace regularization that encourages shared structure across a category whilemaintaining instance details, and (3) using 3D background models to disentangleobjects from the background. We show that 3D models of humans, cats, and dogscan be learned from 50-100 internet videos.</description><author>Gengshan Yang, Chaoyang Wang, N Dinesh Reddy, Deva Ramanan</author><pubDate>Wed, 10 May 2023 18:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06351v1</guid></item><item><title>RECKONING: Reasoning through Dynamic Knowledge Encoding</title><link>http://arxiv.org/abs/2305.06349v1</link><description>Recent studies on transformer-based language models show that they can answerquestions by reasoning over knowledge provided as part of the context (i.e.,in-context reasoning). However, since the available knowledge is often notfiltered for a particular question, in-context reasoning can be sensitive todistractor facts, additional content that is irrelevant to a question but thatmay be relevant for a different question (i.e., not necessarily random noise).In these situations, the model fails to distinguish the knowledge that isnecessary to answer the question, leading to spurious reasoning and degradedperformance. This reasoning failure contrasts with the model's apparent abilityto distinguish its contextual knowledge from all the knowledge it has memorizedduring pre-training. Following this observation, we propose teaching the modelto reason more robustly by folding the provided contextual knowledge into themodel's parameters before presenting it with a question. Our method, RECKONING,is a bi-level learning algorithm that teaches language models to reason byupdating their parametric knowledge through back-propagation, allowing them tothen answer questions using the updated parameters. During training, the innerloop rapidly adapts a copy of the model weights to encode contextual knowledgeinto its parameters. In the outer loop, the model learns to uses the updatedweights to reproduce and answer reasoning questions about the memorizedknowledge. Our experiments on two multi-hop reasoning datasets show thatRECKONING's performance improves over the in-context reasoning baseline (by upto 4.5%). We also find that compared to in-context reasoning, RECKONINGgeneralizes better to longer reasoning chains unseen during training, is morerobust to distractors in the context, and is more computationally efficientwhen multiple questions are asked about the same knowledge.</description><author>Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut</author><pubDate>Wed, 10 May 2023 18:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06349v1</guid></item><item><title>Supervised learning with probabilistic morphisms and kernel mean embeddings</title><link>http://arxiv.org/abs/2305.06348v1</link><description>In this paper I propose a concept of a correct loss function in a generativemodel of supervised learning for an input space $\mathcal{X}$ and a label space$\mathcal{Y}$, which are measurable spaces. A correct loss function in agenerative model of supervised learning must correctly measure the discrepancybetween elements of a hypothesis space $\mathcal{H}$ of possible predictors andthe supervisor operator, which may not belong to $\mathcal{H}$. To definecorrect loss functions, I propose a characterization of a regular conditionalprobability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure$\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection$\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solutionof a linear operator equation. If $\mathcal{Y}$ is a separable metrizabletopological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$,I propose another characterization of a regular conditional probability measure$\mu_{\mathcal{Y}|\mathcal{X}}$ as a minimizer of a mean square error on thespace of Markov kernels, called probabilistic morphisms, from $\mathcal{X}$ to$\mathcal{Y}$, using kernel mean embedding. Using these results and using innermeasure to quantify generalizability of a learning algorithm, I give ageneralization of a result due to Cucker-Smale, which concerns the learnabilityof a regression model, to a setting of a conditional probability estimationproblem. I also give a variant of Vapnik's method of solving stochasticill-posed problem, using inner measure and discuss its applications.</description><author>Hông Vân Lê</author><pubDate>Wed, 10 May 2023 18:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06348v1</guid></item><item><title>CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators</title><link>http://arxiv.org/abs/2305.06347v1</link><description>We present CosmoPower-JAX, a JAX-based implementation of the CosmoPowerframework, which accelerates cosmological inference by building neuralemulators of cosmological power spectra. We show how, using the automaticdifferentiation, batch evaluation and just-in-time compilation features of JAX,and running the inference pipeline on graphics processing units (GPUs),parameter estimation can be accelerated by orders of magnitude with advancedgradient-based sampling techniques. These can be used to efficiently explorehigh-dimensional parameter spaces, such as those needed for the analysis ofnext-generation cosmological surveys. We showcase the accuracy andcomputational efficiency of CosmoPower-JAX on two simulated Stage IVconfigurations. We first consider a single survey performing a cosmic shearanalysis totalling 37 model parameters. We validate the contours derived withCosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived witha nested sampler and without emulators, obtaining a speed-up factor of$\mathcal{O}(10^3)$. We then consider a combination of three Stage IV surveys,each performing a joint cosmic shear and galaxy clustering (3x2pt) analysis,for a total of 157 model parameters. Even with such a high-dimensionalparameter space, CosmoPower-JAX provides converged posterior contours in 3days, as opposed to the estimated 6 years required by standard methods.CosmoPower-JAX is fully written in Python, and we make it publicly available tohelp the cosmological community meet the accuracy requirements set bynext-generation surveys.</description><author>D. Piras, A. Spurio Mancini</author><pubDate>Wed, 10 May 2023 18:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06347v1</guid></item><item><title>Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification</title><link>http://arxiv.org/abs/2305.06344v1</link><description>Neural networks are a very general type of model capable of learning variousrelationships between multiple variables. One example of such relationships,particularly interesting in practice, is the input-output relation of nonlinearsystems, which has a multitude of applications. Studying models capable ofestimating such relation is a broad discipline with numerous theoretical andpractical results. Neural networks are very general, but multiple special casesexist, including convolutional neural networks and recurrent neural networks,which are adjusted for specific applications, which are image and sequenceprocessing respectively. We formulate a hypothesis that adjusting generalnetwork structure by incorporating frequency information into it should resultin a network specifically well suited to nonlinear system identification.Moreover, we show that it is possible to add this frequency information withoutthe loss of generality from a theoretical perspective. We call this newstructure Frequency-Supported Neural Network (FSNN) and empirically investigateits properties.</description><author>Krzysztof Zając, Paweł Wachel</author><pubDate>Wed, 10 May 2023 18:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06344v1</guid></item><item><title>Incorporating Structured Representations into Pretrained Vision &amp; Language Models Using Scene Graphs</title><link>http://arxiv.org/abs/2305.06343v1</link><description>Vision and Language (VL) models have demonstrated remarkable zero-shotperformance in a variety of tasks. However, recent studies have shown that eventhe best VL models struggle to capture aspects of scene understanding, such asobject attributes, relationships, and action states. In contrast, obtainingstructured annotations, e.g., scene graphs (SGs) that could improve thesemodels is time-consuming, costly, and tedious, and thus cannot be used on alarge scale. Here we ask, can small datasets containing SG annotations providesufficient information for enhancing structured understanding of VL models? Weshow that it is indeed possible to improve VL models using such data byutilizing a specialized model architecture and a new training paradigm. Ourapproach captures structure-related information for both the visual and textualencoders by directly supervising both components when learning from SG labels.We use scene graph supervision to generate fine-grained captions based onvarious graph augmentations highlighting different compositional aspects of thescene, and to predict SG information using an open vocabulary approach byadding special ``Adaptive SG tokens'' to the visual encoder. Moreover, wedesign a new adaptation technique tailored specifically to the SG tokens thatallows better learning of the graph prediction task while still maintainingzero-shot capabilities. Our model shows strong performance improvements on theWinoground and VL-checklist datasets with only a mild degradation in zero-shotperformance.</description><author>Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, Amir Globerson</author><pubDate>Wed, 10 May 2023 18:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06343v1</guid></item><item><title>InternChat: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language</title><link>http://arxiv.org/abs/2305.05662v2</link><description>We present an interactive visual framework named InternChat, or iChat forshort. The framework integrates chatbots that have planning and reasoningcapabilities, such as ChatGPT, with non-verbal instructions like pointingmovements that enable users to directly manipulate images or videos on thescreen. Pointing (including gestures, cursors, etc.) movements can provide moreflexibility and precision in performing vision-centric tasks that requirefine-grained control, editing, and generation of visual content. The nameInternChat stands for interaction, nonverbal, and chatbots. Different fromexisting interactive systems that rely on pure language, by incorporatingpointing instructions, the proposed iChat significantly improves the efficiencyof communication between users and chatbots, as well as the accuracy ofchatbots in vision-centric tasks, especially in complicated visual scenarioswhere the number of objects is greater than 2. Additionally, in iChat, anauxiliary control mechanism is used to improve the control capability of LLM,and a large vision-language model termed Husky is fine-tuned for high-qualitymulti-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89% GPT-4 Quality).We hope this work can spark new ideas and directions for future interactivevisual systems. Welcome to watch the code athttps://github.com/OpenGVLab/InternChat.</description><author>Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao</author><pubDate>Wed, 10 May 2023 18:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05662v2</guid></item><item><title>K-UniMorph: Korean Universal Morphology and its Feature Schema</title><link>http://arxiv.org/abs/2305.06335v1</link><description>We present in this work a new Universal Morphology dataset for Korean.Previously, the Korean language has been underrepresented in the field ofmorphological paradigms amongst hundreds of diverse world languages. Hence, wepropose this Universal Morphological paradigms for the Korean language thatpreserve its distinct characteristics. For our K-UniMorph dataset, we outlineeach grammatical criterion in detail for the verbal endings, clarify how toextract inflected forms, and demonstrate how we generate the morphologicalschemata. This dataset adopts morphological feature schema from Sylak-Glassmanet al. (2015) and Sylak-Glassman (2016) for the Korean language as we extractinflected verb forms from the Sejong morphologically analyzed corpus that isone of the largest annotated corpora for Korean. During the data creation, ourmethodology also includes investigating the correctness of the conversion fromthe Sejong corpus. Furthermore, we carry out the inflection task using threedifferent Korean word forms: letters, syllables and morphemes. Finally, wediscuss and describe future perspectives on Korean morphological paradigms andthe dataset.</description><author>Eunkyul Leah Jo, Kyuwon Kim, Xihan Wu, KyungTae Lim, Jungyeul Park, Chulwoo Park</author><pubDate>Wed, 10 May 2023 18:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06335v1</guid></item><item><title>Korean Named Entity Recognition Based on Language-Specific Features</title><link>http://arxiv.org/abs/2305.06330v1</link><description>In the paper, we propose a novel way of improving named entity recognition inthe Korean language using its language-specific features. While the field ofnamed entity recognition has been studied extensively in recent years, themechanism of efficiently recognizing named entities in Korean has hardly beenexplored. This is because the Korean language has distinct linguisticproperties that prevent models from achieving their best performances.Therefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-Uformat, which decomposes Korean words into morphemes and reduces the ambiguityof named entities in the original segmentation that may contain functionalmorphemes such as postpositions and particles, is proposed herein. Weinvestigate how the named entity tags are best represented in thismorpheme-based scheme and implement an algorithm to convert word-based {andsyllable-based Korean corpora} with named entities into the proposedmorpheme-based format. Analyses of the results of {statistical and neural}models reveal that the proposed morpheme-based format is feasible, and the{varied} performances of the models under the influence of various additionallanguage-specific features are demonstrated. Extrinsic conditions were alsoconsidered to observe the variance of the performances of the proposed models,given different types of data, including the original segmentation anddifferent types of tagging formats.</description><author>Yige Chen, KyungTae Lim, Jungyeul Park</author><pubDate>Wed, 10 May 2023 18:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06330v1</guid></item><item><title>Similarity of Neural Network Models: A Survey of Functional and Representational Measures</title><link>http://arxiv.org/abs/2305.06329v1</link><description>Measuring similarity of neural networks has become an issue of greatimportance and research interest to understand and utilize differences ofneural networks. While there are several perspectives on how neural networkscan be similar, we specifically focus on two complementing perspectives, i.e.,(i) representational similarity, which considers how activations ofintermediate neural layers differ, and (ii) functional similarity, whichconsiders how models differ in their outputs. In this survey, we provide acomprehensive overview of these two families of similarity measures for neuralnetwork models. In addition to providing detailed descriptions of existingmeasures, we summarize and discuss results on the properties and relationshipsof these measures, and point to open research problems. Further, we providepractical recommendations that can guide researchers as well as practitionersin applying the measures. We hope our work lays a foundation for our communityto engage in more systematic research on the properties, nature andapplicability of similarity measures for neural network models.</description><author>Max Klabunde, Tobias Schumacher, Markus Strohmaier, Florian Lemmerich</author><pubDate>Wed, 10 May 2023 18:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06329v1</guid></item><item><title>Learning to Personalize Recommendation based on Customers' Shopping Intents</title><link>http://arxiv.org/abs/2305.05279v2</link><description>Understanding the customers' high level shopping intent, such as their desireto go camping or hold a birthday party, is critically important for anE-commerce platform; it can help boost the quality of shopping experience byenabling provision of more relevant, explainable, and diversifiedrecommendations. However, such high level shopping intent has been overlookedin the industry due to practical challenges. In this work, we introduceAmazon's new system that explicitly identifies and utilizes each customer'shigh level shopping intents for personalizing recommendations. We develop anovel technique that automatically identifies various high level goals beingpursued by the Amazon customers, such as "go camping", and "preparing for abeach party". Our solution is in a scalable fashion (in 14 languages across 21countries). Then a deep learning model maps each customer's online behavior,e.g. product search and individual item engagements, into a subset of highlevel shopping intents. Finally, a realtime ranker considers both theidentified intents as well as the granular engagements to present personalizedintent-aware recommendations. Extensive offline analysis ensures accuracy andrelevance of the new recommendations and we further observe an 10% improvementin the business metrics. This system is currently serving online traffic atamazon.com, powering several production features, driving significant businessimpacts</description><author>Xin Shen, Jiaying Shi, Sungro Yoon, Jon Katzur, Hanbo Wang, Jim Chan, Jin Li</author><pubDate>Wed, 10 May 2023 18:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05279v2</guid></item><item><title>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</title><link>http://arxiv.org/abs/2305.06324v1</link><description>We present Integrated Multimodal Perception (IMP), a simple and scalablemultimodal multi-task training and modeling approach. IMP integrates multimodalinputs including image, video, text, and audio into a single Transformerencoder with minimal modality-specific components. IMP makes use of a noveldesign that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts(MoE) for efficient model \&amp; task scaling. We conduct extensive empiricalstudies about IMP and reveal the following key insights: 1) performing gradientdescent updates by alternating on diverse heterogeneous modalities, lossfunctions, and tasks, while also varying input resolutions, efficientlyimproves multimodal understanding. 2) model sparsification with MoE on a singlemodality-agnostic encoder substantially improves the performance, outperformingdense models that use modality-specific encoders or additional fusion layersand greatly mitigating the conflicts between modalities. IMP achievescompetitive performance on a wide range of downstream tasks including imageclassification, video classification, image-text, and video-text retrieval.Most notably, we train a sparse IMP-MoE-L focusing on video tasks that achievesnew state-of-the-art in zero-shot video classification. Our model achieves77.0% on Kinetics-400, 76.8% on Kinetics-600, and 76.8% on Kinetics-700zero-shot classification accuracy, improving the previous state-of-the-art by+5%, +6.7%, and +5.8%, respectively, while using only 15% of their totaltraining computational cost.</description><author>Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, Hartwig Adam</author><pubDate>Wed, 10 May 2023 18:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06324v1</guid></item><item><title>SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection</title><link>http://arxiv.org/abs/2305.06321v1</link><description>Malicious Deepfakes have led to a sharp conflict over distinguishing betweengenuine and forged faces. Although many countermeasures have been developed todetect Deepfakes ex-post, undoubtedly, passive forensics has not considered anypreventive measures for the pristine face before foreseeable manipulations. Tocomplete this forensics ecosystem, we thus put forward the proactive solutiondubbed SepMark, which provides a unified framework for source tracing andDeepfake detection. SepMark originates from encoder-decoder-based deepwatermarking but with two separable decoders. For the first time the deepseparable watermarking, SepMark brings a new paradigm to the established studyof deep watermarking, where a single encoder embeds one watermark elegantly,while two decoders can extract the watermark separately at different levels ofrobustness. The robust decoder termed Tracer that resists various distortionsmay have an overly high level of robustness, allowing the watermark to surviveboth before and after Deepfake. The semi-robust one termed Detector isselectively sensitive to malicious distortions, making the watermark disappearafter Deepfake. Only SepMark comprising of Tracer and Detector can reliablytrace the trusted source of the marked face and detect whether it has beenaltered since being marked; neither of the two alone can achieve this.Extensive experiments demonstrate the effectiveness of the proposed SepMark ontypical Deepfakes, including face swapping, expression reenactment, andattribute editing.</description><author>Xiaoshuai Wu, Xin Liao, Bo Ou</author><pubDate>Wed, 10 May 2023 18:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06321v1</guid></item><item><title>NervePool: A Simplicial Pooling Layer</title><link>http://arxiv.org/abs/2305.06315v1</link><description>For deep learning problems on graph-structured data, pooling layers areimportant for down sampling, reducing computational cost, and to minimizeoverfitting. We define a pooling layer, NervePool, for data structured assimplicial complexes, which are generalizations of graphs that includehigher-dimensional simplices beyond vertices and edges; this structure allowsfor greater flexibility in modeling higher-order relationships. The proposedsimplicial coarsening scheme is built upon partitions of vertices, which allowus to generate hierarchical representations of simplicial complexes, collapsinginformation in a learned fashion. NervePool builds on the learned vertexcluster assignments and extends to coarsening of higher dimensional simplicesin a deterministic fashion. While in practice, the pooling operations arecomputed via a series of matrix operations, the topological motivation is aset-theoretic construction based on unions of stars of simplices and the nervecomplex</description><author>Sarah McGuire, Elizabeth Munch, Matthew Hirn</author><pubDate>Wed, 10 May 2023 18:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06315v1</guid></item><item><title>Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks</title><link>http://arxiv.org/abs/2305.06314v1</link><description>Reconstructing semantic 3D building models at the level of detail (LoD) 3 isa long-standing challenge. Unlike mesh-based models, they require watertightgeometry and object-wise semantics at the fa\c{c}ade level. The principalchallenge of such demanding semantic 3D reconstruction is reliablefa\c{c}ade-level semantic segmentation of 3D input data. We present a novelmethod, called Scan2LoD3, that accurately reconstructs semantic LoD3 buildingmodels by improving fa\c{c}ade-level semantic 3D segmentation. To this end, weleverage laser physics and 3D building model priors to probabilisticallyidentify model conflicts. These probabilistic physical conflicts proposelocations of model openings: Their final semantics and shapes are inferred in aBayesian network fusing multimodal probabilistic maps of conflicts, 3D pointclouds, and 2D images. To fulfill demanding LoD3 requirements, we use theestimated shapes to cut openings in 3D building priors and fit semantic 3Dobjects from a library of fa\c{c}ade objects. Extensive experiments on the TUMcity campus datasets demonstrate the superior performance of the proposedScan2LoD3 over the state-of-the-art methods in fa\c{c}ade-level detection,semantic segmentation, and LoD3 building model reconstruction. We believe ourmethod can foster the development of probability-driven semantic 3Dreconstruction at LoD3 since not only the high-definition reconstruction butalso reconstruction confidence becomes pivotal for various applications such asautonomous driving and urban simulations.</description><author>Olaf Wysocki, Yan Xia, Magdalena Wysocki, Eleonora Grilli, Ludwig Hoegner, Daniel Cremers, Uwe Stilla</author><pubDate>Wed, 10 May 2023 18:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06314v1</guid></item><item><title>Automatic Evaluation of Attribution by Large Language Models</title><link>http://arxiv.org/abs/2305.06311v1</link><description>A recent focus of large language model (LLM) development, as exemplified bygenerative search engines, is to incorporate external references to generateand support their claims. However, evaluating the attribution, i.e., verifyingwhether the generated statement is indeed fully supported by the citedreference, remains an open problem. Although human evaluation is commonpractice, it is costly and time-consuming. In this paper, we investigate theautomatic evaluation of attribution by LLMs. We begin by providing a definitionof attribution and then explore two approaches for automatic evaluation:prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposedfrom related tasks, such as question answering, fact-checking, natural languageinference, and summarization. To facilitate the evaluation, we manually curatea set of test examples covering 12 domains from a generative search engine, NewBing. Our results on the curated test set and simulated test examples fromexisting benchmark questions highlight both promising signals as well asremaining challenges for the automatic evaluation of attribution. We hope ourtestbed, modeling methodology, and insights will help lay the foundation forfuture studies on this important problem.</description><author>Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun</author><pubDate>Wed, 10 May 2023 17:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06311v1</guid></item><item><title>Analysis of Adversarial Image Manipulations</title><link>http://arxiv.org/abs/2305.06307v1</link><description>As virtual and physical identity grow increasingly intertwined, theimportance of privacy and security in the online sphere becomes paramount. Inrecent years, multiple news stories have emerged of private companies scrapingweb content and doing research with or selling the data. Images uploaded onlinecan be scraped without users' consent or knowledge. Users of social mediaplatforms whose images are scraped may be at risk of being identified in otheruploaded images or in real-world identification situations. This paperinvestigates how simple, accessible image manipulation techniques affect theaccuracy of facial recognition software in identifying an individual's variousface images based on one unique image.</description><author>Ahsi Lo, Gabriella Pangelinan, Michael C. King</author><pubDate>Wed, 10 May 2023 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06307v1</guid></item><item><title>An Empirical Study on How the Developers Discussed about Pandas Topics</title><link>http://arxiv.org/abs/2210.03519v2</link><description>Pandas is defined as a software library which is used for data analysis inPython programming language. As pandas is a fast, easy and open source dataanalysis tool, it is rapidly used in different software engineering projectslike software development, machine learning, computer vision, natural languageprocessing, robotics, and others. So a huge interests are shown in softwaredevelopers regarding pandas and a huge number of discussions are now becomingdominant in online developer forums, like Stack Overflow (SO). Such discussionscan help to understand the popularity of pandas library and also can help tounderstand the importance, prevalence, difficulties of pandas topics. The mainaim of this research paper is to find the popularity and difficulty of pandastopics. For this regard, SO posts are collected which are related to pandastopic discussions. Topic modeling are done on the textual contents of theposts. We found 26 topics which we further categorized into 5 board categories.We observed that developers discuss variety of pandas topics in SO related toerror and excepting handling, visualization, External support, dataframe, andoptimization. In addition, a trend chart is generated according to thediscussion of topics in a predefined time series. The finding of this paper canprovide a path to help the developers, educators and learners. For example,beginner developers can learn most important topics in pandas which areessential for develop any model. Educators can understand the topics which seemhard to learners and can build different tutorials which can make that pandastopic understandable. From this empirical study it is possible to understandthe preferences of developers in pandas topic by processing their SO posts</description><author>Sajib Kumar Saha Joy, Farzad Ahmed, Al Hasib Mahamud, Nibir Chandra Mandal</author><pubDate>Wed, 10 May 2023 17:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03519v2</guid></item><item><title>Self-Supervised Instance Segmentation by Grasping</title><link>http://arxiv.org/abs/2305.06305v1</link><description>Instance segmentation is a fundamental skill for many robotic applications.We propose a self-supervised method that uses grasp interactions to collectsegmentation supervision for an instance segmentation model. When a robotgrasps an item, the mask of that grasped item can be inferred from the imagesof the scene before and after the grasp. Leveraging this insight, we learn agrasp segmentation model to segment the grasped object from before and aftergrasp images. Such a model can segment grasped objects from thousands of graspinteractions without costly human annotation. Using the segmented graspedobjects, we can "cut" objects from their original scenes and "paste" them intonew scenes to generate instance supervision. We show that our graspsegmentation model provides a 5x error reduction when segmenting graspedobjects compared with traditional image subtraction approaches. Combined withour "cut-and-paste" generation method, instance segmentation models trainedwith our method achieve better performance than a model trained with 10x theamount of labeled data. On a real robotic grasping system, our instancesegmentation model reduces the rate of grasp errors by over 3x compared to animage subtraction baseline.</description><author>YuXuan Liu, Xi Chen, Pieter Abbeel</author><pubDate>Wed, 10 May 2023 17:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06305v1</guid></item><item><title>Evaluating Embedding APIs for Information Retrieval</title><link>http://arxiv.org/abs/2305.06300v1</link><description>The ever-increasing size of language models curtails their widespread accessto the community, thereby galvanizing many companies and startups into offeringaccess to large language models through APIs. One particular API, suitable fordense retrieval, is the semantic embedding API that builds vectorrepresentations of a given text. With a growing number of APIs at our disposal,in this paper, our goal is to analyze semantic embedding APIs in realisticretrieval scenarios in order to assist practitioners and researchers in findingsuitable services according to their needs. Specifically, we wish toinvestigate the capabilities of existing APIs on domain generalization andmultilingual retrieval. For this purpose, we evaluate the embedding APIs on twostandard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 resultsusing the APIs is a budget-friendly approach and is most effective on English,in contrast to the standard practice, i.e., employing them as first-stageretrievers. For non-English retrieval, re-ranking still improves the results,but a hybrid model with BM25 works best albeit at a higher cost. We hope ourwork lays the groundwork for thoroughly evaluating APIs that are critical insearch and more broadly, in information retrieval.</description><author>Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, Jimmy Lin</author><pubDate>Wed, 10 May 2023 17:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06300v1</guid></item><item><title>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)</title><link>http://arxiv.org/abs/2305.06299v1</link><description>Large language models, particularly GPT-3, are able to produce high qualitysummaries of general domain news articles in few- and zero-shot settings.However, it is unclear if such models are similarly capable in morespecialized, high-stakes domains such as biomedicine. In this paper, we enlistdomain experts (individuals with medical training) to evaluate summaries ofbiomedical articles generated by GPT-3, given zero supervision. We considerboth single- and multi-document settings. In the former, GPT-3 is tasked withgenerating regular and plain-language summaries of articles describingrandomized controlled trials; in the latter, we assess the degree to whichGPT-3 is able to \emph{synthesize} evidence reported across a collection ofarticles. We design an annotation scheme for evaluating model outputs, with anemphasis on assessing the factual accuracy of generated summaries. We find thatwhile GPT-3 is able to summarize and simplify single biomedical articlesfaithfully, it struggles to provide accurate aggregations of findings overmultiple documents. We release all data and annotations used in this work.</description><author>Chantal Shaib, Millicent L. Li, Sebastian Joseph, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace</author><pubDate>Wed, 10 May 2023 17:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06299v1</guid></item><item><title>TarViS: A Unified Approach for Target-based Video Segmentation</title><link>http://arxiv.org/abs/2301.02657v2</link><description>The general domain of video segmentation is currently fragmented intodifferent tasks spanning multiple benchmarks. Despite rapid progress in thestate-of-the-art, current methods are overwhelmingly task-specific and cannotconceptually generalize to other tasks. Inspired by recent approaches withmulti-task capability, we propose TarViS: a novel, unified network architecturethat can be applied to any task that requires segmenting a set of arbitrarilydefined 'targets' in video. Our approach is flexible with respect to how tasksdefine these targets, since it models the latter as abstract 'queries' whichare then used to predict pixel-precise target masks. A single TarViS model canbe trained jointly on a collection of datasets spanning different tasks, andcan hot-swap between tasks during inference without any task-specificretraining. To demonstrate its effectiveness, we apply TarViS to four differenttasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation(VPS), Video Object Segmentation (VOS) and Point Exemplar-guided Tracking(PET). Our unified, jointly trained model achieves state-of-the-art performanceon 5/7 benchmarks spanning these four tasks, and competitive performance on theremaining two. Code and model weights are available at:https://github.com/Ali2500/TarViS</description><author>Ali Athar, Alexander Hermans, Jonathon Luiten, Deva Ramanan, Bastian Leibe</author><pubDate>Wed, 10 May 2023 17:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02657v2</guid></item><item><title>Why Don't You Do Something About It? Outlining Connections between AI Explanations and User Actions</title><link>http://arxiv.org/abs/2305.06297v1</link><description>A core assumption of explainable AI systems is that explanations change whatusers know, thereby enabling them to act within their complex socio-technicalenvironments. Despite the centrality of action, explanations are oftenorganized and evaluated based on technical aspects. Prior work varies widely inthe connections it traces between information provided in explanations andresulting user actions. An important first step in centering action inevaluations is understanding what the XAI community collectively recognizes asthe range of information that explanations can present and what actions areassociated with them. In this paper, we present our framework, which maps priorwork on information presented in explanations and user action, and we discussthe gaps we uncovered about the information presented to users.</description><author>Gennie Mansi, Mark Riedl</author><pubDate>Wed, 10 May 2023 17:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06297v1</guid></item><item><title>Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2305.06295v1</link><description>Clinical diagnosis guidelines aim at specifying the steps that may lead to adiagnosis. Guidelines enable rationalizing and normalizing clinical decisionsbut suffer drawbacks as they are built to cover the majority of the populationand may fail in guiding to the right diagnosis for patients with uncommonconditions or multiple pathologies. Moreover, their updates are long andexpensive, making them unsuitable to emerging practices. Inspired byguidelines, we formulate the task of diagnosis as a sequential decision-makingproblem and study the use of Deep Reinforcement Learning (DRL) algorithmstrained on Electronic Health Records (EHRs) to learn the optimal sequence ofobservations to perform in order to obtain a correct diagnosis. Because of thevariety of DRL algorithms and of their sensitivity to the context, weconsidered several approaches and settings that we compared to each other, andto classical classifiers. We experimented on a synthetic but realistic datasetto differentially diagnose anemia and its subtypes and particularly evaluatedthe robustness of various approaches to noise and missing data as those arefrequent in EHRs. Within the DRL algorithms, Dueling DQN with PrioritizedExperience Replay, and Dueling Double DQN with Prioritized Experience Replayshow the best and most stable performances. In the presence of imperfect data,the DRL algorithms show competitive, but less stable performances when comparedto the classifiers (Random Forest and XGBoost); although they enable theprogressive generation of a pathway to the suggested diagnosis, which can bothguide or explain the decision process.</description><author>Lillian Muyama, Antoine Neuraz, Adrien Coulet</author><pubDate>Wed, 10 May 2023 17:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06295v1</guid></item><item><title>CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation</title><link>http://arxiv.org/abs/2305.06294v1</link><description>Commonsense knowledge is crucial to many natural language processing tasks.Existing works usually incorporate graph knowledge with conventional graphneural networks (GNNs), leading to the text and graph knowledge encodingprocesses being separated in a serial pipeline. We argue that these separaterepresentation learning stages may be suboptimal for neural networks to learnthe overall context contained in both types of input knowledge. In this paper,we propose a novel context-aware graph-attention model (Context-aware GAT),which can effectively incorporate global features of relevant knowledge graphsbased on a context-enhanced knowledge aggregation process. Specifically, ourframework leverages a novel representation learning approach to processheterogeneous features - combining flattened graph knowledge with text. To thebest of our knowledge, this is the first attempt at hierarchically applyinggraph knowledge aggregation on a connected subgraph in addition to contextualinformation to support commonsense dialogue generation. This framework showssuperior performance compared to conventional GNN-based language frameworks.Both automatic and human evaluation demonstrates that our proposed model hassignificant performance uplifts over state-of-the-art baselines.</description><author>Hongbo Zhanga, Chen Tang, Tyler Loakmana, Chenghua Lina, Stefan Goetze</author><pubDate>Wed, 10 May 2023 17:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06294v1</guid></item><item><title>Joint Metrics Matter: A Better Standard for Trajectory Forecasting</title><link>http://arxiv.org/abs/2305.06292v1</link><description>Multi-modal trajectory forecasting methods commonly evaluate usingsingle-agent metrics (marginal metrics), such as minimum Average DisplacementError (ADE) and Final Displacement Error (FDE), which fail to capture jointperformance of multiple interacting agents. Only focusing on marginal metricscan lead to unnatural predictions, such as colliding trajectories or divergingtrajectories for people who are clearly walking together as a group.Consequently, methods optimized for marginal metrics lead to overly-optimisticestimations of performance, which is detrimental to progress in trajectoryforecasting research. In response to the limitations of marginal metrics, wepresent the first comprehensive evaluation of state-of-the-art (SOTA)trajectory forecasting methods with respect to multi-agent metrics (jointmetrics): JADE, JFDE, and collision rate. We demonstrate the importance ofjoint metrics as opposed to marginal metrics with quantitative evidence andqualitative examples drawn from the ETH / UCY and Stanford Drone datasets. Weintroduce a new loss function incorporating joint metrics that, when applied toa SOTA trajectory forecasting method, achieves a 7% improvement in JADE / JFDEon the ETH / UCY datasets with respect to the previous SOTA. Our results alsoindicate that optimizing for joint metrics naturally leads to an improvement ininteraction modeling, as evidenced by a 16% decrease in mean collision rate onthe ETH / UCY datasets with respect to the previous SOTA.</description><author>Erica Weng, Hana Hoshino, Deva Ramanan, Kris Kitani</author><pubDate>Wed, 10 May 2023 17:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06292v1</guid></item><item><title>Learning Video-Conditioned Policies for Unseen Manipulation Tasks</title><link>http://arxiv.org/abs/2305.06289v1</link><description>The ability to specify robot commands by a non-expert user is critical forbuilding generalist agents capable of solving a large variety of tasks. Oneconvenient way to specify the intended robot goal is by a video of a persondemonstrating the target task. While prior work typically aims to imitate humandemonstrations performed in robot environments, here we focus on a morerealistic and challenging setup with demonstrations recorded in natural anddiverse human environments. We propose Video-conditioned Policy learning (ViP),a data-driven approach that maps human demonstrations of previously unseentasks to robot manipulation skills. To this end, we learn our policy togenerate appropriate actions given current scene observations and a video ofthe target task. To encourage generalization to new tasks, we avoid particulartasks during training and learn our policy from unlabelled robot trajectoriesand corresponding robot videos. Both robot and human videos in our frameworkare represented by video embeddings pre-trained for human action recognition.At test time we first translate human videos to robot videos in the commonvideo embedding space, and then use resulting embeddings to condition ourpolicies. Notably, our approach enables robot control by human demonstrationsin a zero-shot manner, i.e., without using robot trajectories paired with humaninstructions during training. We validate our approach on a set of challengingmulti-task robot manipulation environments and outperform state of the art. Ourmethod also demonstrates excellent performance in a new challenging zero-shotsetup where no paired data is used during training.</description><author>Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev</author><pubDate>Wed, 10 May 2023 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06289v1</guid></item><item><title>Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems</title><link>http://arxiv.org/abs/2212.07313v2</link><description>We consider the sequential decision-making problem of making proactiverequest assignment and rejection decisions for a profit-maximizing operator ofan autonomous mobility on demand system. We formalize this problem as a Markovdecision process and propose a novel combination of multi-agent SoftActor-Critic and weighted bipartite matching to obtain an anticipative controlpolicy. Thereby, we factorize the operator's otherwise intractable actionspace, but still obtain a globally coordinated decision. Experiments based onreal-world taxi data show that our method outperforms state of the artbenchmarks with respect to performance, stability, and computationaltractability.</description><author>Tobias Enders, James Harrison, Marco Pavone, Maximilian Schiffer</author><pubDate>Wed, 10 May 2023 17:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07313v2</guid></item><item><title>Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search</title><link>http://arxiv.org/abs/2302.11223v2</link><description>Symbolic regression (SR) is the problem of learning a symbolic expressionfrom numerical data. Recently, deep neural models trained onprocedurally-generated synthetic datasets showed competitive performancecompared to more classical Genetic Programming (GP) algorithms. Unlike their GPcounterparts, these neural approaches are trained to generate expressions fromdatasets given as context. This allows them to produce accurate expressions ina single forward pass at test time. However, they usually do not benefit fromsearch abilities, which result in low performance compared to GP onout-of-distribution datasets. In this paper, we propose a novel method whichprovides the best of both worlds, based on a Monte-Carlo Tree Search procedureusing a context-aware neural mutation model, which is initially pre-trained tolearn promising mutations, and further refined from successful experiences inan online fashion. The approach demonstrates state-of-the-art performance onthe well-known \texttt{SRBench} benchmark.</description><author>Pierre-Alexandre Kamienny, Guillaume Lample, Sylvain Lamprier, Marco Virgolin</author><pubDate>Wed, 10 May 2023 17:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11223v2</guid></item><item><title>A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot</title><link>http://arxiv.org/abs/2305.06278v1</link><description>Recovering an outdoor environment's surface mesh is vital for an agriculturalrobot during task planning and remote visualization. Our proposed solution isbased on a newly-designed panoramic stereo camera along with a hybrid novelsoftware framework that consists of three fusion modules. The panoramic stereocamera with a pentagon shape consists of 5 stereo vision camera pairs to streamsynchronized panoramic stereo images for the following three fusion modules. Inthe disparity fusion module, rectified stereo images produce the initialdisparity maps using multiple stereo vision algorithms. Then, these initialdisparity maps, along with the intensity images, are input into a disparityfusion network to produce refined disparity maps. Next, the refined disparitymaps are converted into full-view point clouds or single-view point clouds forthe pose fusion module. The pose fusion module adopts a two-stageglobal-coarse-to-local-fine strategy. In the first stage, each pair offull-view point clouds is registered by a global point cloud matching algorithmto estimate the transformation for a global pose graph's edge, whicheffectively implements loop closure. In the second stage, a local point cloudmatching algorithm is used to match single-view point clouds in differentnodes. Next, we locally refine the poses of all corresponding edges in theglobal pose graph using three proposed rules, thus constructing a refined posegraph. The refined pose graph is optimized to produce a global pose trajectoryfor volumetric fusion. In the volumetric fusion module, the global poses of allthe nodes are used to integrate the single-view point clouds into the volume toproduce the mesh of the whole garden. The proposed framework and its threefusion modules are tested on a real outdoor garden dataset to show thesuperiority of the performance.</description><author>Can Pu, Chuanyu Yang, Jinnian Pu, Radim Tylecek, Robert B. Fisher</author><pubDate>Wed, 10 May 2023 17:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06278v1</guid></item><item><title>Context-Aware Document Simplification</title><link>http://arxiv.org/abs/2305.06274v1</link><description>To date, most work on text simplification has focused on sentence-levelinputs. Early attempts at document simplification merely applied theseapproaches iteratively over the sentences of a document. However, this fails tocoherently preserve the discourse structure, leading to suboptimal outputquality. Recently, strategies from controllable simplification have beenleveraged to achieve state-of-the-art results on document simplification byfirst generating a document-level plan (a sequence of sentence-levelsimplification operations) and using this plan to guide sentence-levelsimplification downstream. However, this is still limited in that thesimplification model has no direct access to the local inter-sentence documentcontext, likely having a negative impact on surface realisation. We explorevarious systems that use document context within the simplification processitself, either by iterating over larger text units or by extending the systemarchitecture to attend over a high-level representation of document context. Indoing so, we achieve state-of-the-art performance on the documentsimplification task, even when not relying on plan-guidance. Further, weinvestigate the performance and efficiency tradeoffs of system variants andmake suggestions of when each should be preferred.</description><author>Liam Cripwell, Joël Legrand, Claire Gardent</author><pubDate>Wed, 10 May 2023 17:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06274v1</guid></item><item><title>Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification</title><link>http://arxiv.org/abs/2305.04228v2</link><description>Code classification is a difficult issue in program understanding andautomatic coding. Due to the elusive syntax and complicated semantics inprograms, most existing studies use techniques based on abstract syntax tree(AST) and graph neural network (GNN) to create code representations for codeclassification. These techniques utilize the structure and semantic informationof the code, but they only take into account pairwise associations and neglectthe high-order correlations that already exist between nodes in the AST, whichmay result in the loss of code structural information. On the other hand, whilea general hypergraph can encode high-order data correlations, it is homogeneousand undirected which will result in a lack of semantic and structuralinformation such as node types, edge types, and directions between child nodesand parent nodes when modeling AST. In this study, we propose to represent ASTas a heterogeneous directed hypergraph (HDHG) and process the graph byheterogeneous directed hypergraph neural network (HDHGN) for codeclassification. Our method improves code understanding and can representhigh-order data correlations beyond paired interactions. We assessheterogeneous directed hypergraph neural network (HDHGN) on public datasets ofPython and Java programs. Our method outperforms previous AST-based andGNN-based methods, which demonstrates the capability of our model.</description><author>Guang Yang, Tiancheng Jin, Liang Dou</author><pubDate>Wed, 10 May 2023 16:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04228v2</guid></item><item><title>Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization</title><link>http://arxiv.org/abs/2208.11303v3</link><description>Most current multi-modal summarization methods follow a cascaded manner,where an off-the-shelf object detector is first used to extract visualfeatures, then these features are fused with language representations togenerate the summary with an encoder-decoder model. The cascaded way cannotcapture the semantic alignments between images and paragraphs, which arecrucial to a precise summary. In this paper, we propose ViL-Sum to jointlymodel paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment andMulti-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modalencoder with two well-designed tasks, image reordering and image selection. Thejoint multi-modal encoder captures the interactions between modalities, wherethe reordering task guides the model to learn paragraph-level semanticalignment and the selection task guides the model to selected summary-relatedimages in the final summary. Experimental results show that our proposedViL-Sum significantly outperforms current state-of-the-art methods. In furtheranalysis, we find that two well-designed tasks and joint multi-modal encodercan effectively guide the model to learn reasonable paragraphs-images andsummary-images relations.</description><author>Chenhao Cui, Xinnian Liang, Shuangzhi Wu, Zhoujun Li</author><pubDate>Wed, 10 May 2023 16:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11303v3</guid></item><item><title>Generalised Scale-Space Properties for Probabilistic Diffusion Models</title><link>http://arxiv.org/abs/2303.07900v3</link><description>Probabilistic diffusion models enjoy increasing popularity in the deeplearning community. They generate convincing samples from a learneddistribution of input images with a wide field of practical applications.Originally, these approaches were motivated from drift-diffusion processes, butthese origins find less attention in recent, practice-oriented publications. Weinvestigate probabilistic diffusion models from the viewpoint of scale-spaceresearch and show that they fulfil generalised scale-space properties onevolving probability distributions. Moreover, we discuss similarities anddifferences between interpretations of the physical core concept ofdrift-diffusion in the deep learning and model-based world. To this end, weexamine relations of probabilistic diffusion to osmosis filters.</description><author>Pascal Peter</author><pubDate>Wed, 10 May 2023 16:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07900v3</guid></item><item><title>Embedded Feature Correlation Optimization with Specific Parameter Initialization for 2D/3D Registration</title><link>http://arxiv.org/abs/2305.06252v1</link><description>We present a novel deep learning-based framework: Embedded FeatureCorrelation Optimization with Specific Parameter Initialization (COSPI) for2D/3D registration which is a most challenging problem due to the difficultysuch as dimensional mismatch, heavy computation load and lack of goldenevaluating standard. The framework we designed includes a parameterspecification module to efficiently choose initialization pose parameter and afine-registration network to align images. The proposed framework takesextracting multi-scale features into consideration using a novel compositeconnection encoder with special training techniques. The method is comparedwith both learning-based methods and optimization-based methods to furtherevaluate the performance. Our experiments demonstrate that the method in thispaper has improved the registration performance, and thereby outperforms theexisting methods in terms of accuracy and running time. We also show thepotential of the proposed method as an initial pose estimator.</description><author>Minheng Chen, Zhirun Zhang, Shuheng Gu, Youyong Kong</author><pubDate>Wed, 10 May 2023 16:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06252v1</guid></item><item><title>Deep Reinforcement Learning Based Resource Allocation for Cloud Native Wireless Network</title><link>http://arxiv.org/abs/2305.06249v1</link><description>Cloud native technology has revolutionized 5G beyond and 6G communicationnetworks, offering unprecedented levels of operational automation, flexibility,and adaptability. However, the vast array of cloud native services andapplications presents a new challenge in resource allocation for dynamic cloudcomputing environments. To tackle this challenge, we investigate a cloud nativewireless architecture that employs container-based virtualization to enableflexible service deployment. We then study two representative use cases:network slicing and Multi-Access Edge Computing. To optimize resourceallocation in these scenarios, we leverage deep reinforcement learningtechniques and introduce two model-free algorithms capable of monitoring thenetwork state and dynamically training allocation policies. We validate theeffectiveness of our algorithms in a testbed developed using Free5gc. Ourfindings demonstrate significant improvements in network efficiency,underscoring the potential of our proposed techniques in unlocking the fullpotential of cloud native wireless networks.</description><author>Lin Wang, Jiasheng Wu, Yue Gao, Jingjing Zhang</author><pubDate>Wed, 10 May 2023 16:32:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06249v1</guid></item><item><title>Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis</title><link>http://arxiv.org/abs/2303.08134v2</link><description>We present a Non-parametric Network for 3D point cloud analysis, Point-NN,which consists of purely non-learnable components: farthest point sampling(FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometricfunctions. Surprisingly, it performs well on various 3D tasks, requiring noparameters or training, and even surpasses existing fully trained models.Starting from this basic non-parametric model, we propose two extensions.First, Point-NN can serve as a base architectural framework to constructParametric Networks by simply inserting linear layers on top. Given thesuperior non-parametric foundation, the derived Point-PN exhibits a highperformance-efficiency trade-off with only a few learnable parameters. Second,Point-NN can be regarded as a plug-and-play module for the already trained 3Dmodels during inference. Point-NN captures the complementary geometricknowledge and enhances existing methods for different 3D benchmarks withoutre-training. We hope our work may cast a light on the community forunderstanding 3D point clouds with non-parametric methods. Code is available athttps://github.com/ZrrSkywalker/Point-NN.</description><author>Renrui Zhang, Liuhui Wang, Ziyu Guo, Yali Wang, Peng Gao, Hongsheng Li, Jianbo Shi</author><pubDate>Wed, 10 May 2023 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08134v2</guid></item><item><title>Rethinking the Value of Labels for Instance-Dependent Label Noise Learning</title><link>http://arxiv.org/abs/2305.06247v1</link><description>Label noise widely exists in large-scale datasets and significantlydegenerates the performances of deep learning algorithms. Due to thenon-identifiability of the instance-dependent noise transition matrix, mostexisting algorithms address the problem by assuming the noisy label generationprocess to be independent of the instance features. Unfortunately, noisy labelsin real-world applications often depend on both the true label and thefeatures. In this work, we tackle instance-dependent label noise with a noveldeep generative model that avoids explicitly modeling the noise transitionmatrix. Our algorithm leverages casual representation learning andsimultaneously identifies the high-level content and style latent factors fromthe data. By exploiting the supervision information of noisy labels withstructural causal models, our empirical evaluations on a wide range ofsynthetic and real-world instance-dependent label noise datasets demonstratethat the proposed algorithm significantly outperforms the state-of-the-artcounterparts.</description><author>Hanwen Deng, Weijia Zhang, Min-Ling Zhang</author><pubDate>Wed, 10 May 2023 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06247v1</guid></item><item><title>A Joint Python/C++ Library for Efficient yet Accessible Black-Box and Gray-Box Optimization with GOMEA</title><link>http://arxiv.org/abs/2305.06246v1</link><description>Exploiting knowledge about the structure of a problem can greatly benefit theefficiency and scalability of an Evolutionary Algorithm (EA). Model-Based EAs(MBEAs) are capable of doing this by explicitly modeling the problem structure.The Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) is among thestate-of-the-art of MBEAs due to its use of a linkage model and the optimalmixing variation operator. Especially in a Gray-Box Optimization (GBO) settingthat allows for partial evaluations, i.e., the relatively efficient evaluationof a partial modification of a solution, GOMEA is known to excel. Such GBOsettings are known to exist in various real-world applications to which GOMEAhas successfully been applied. In this work, we introduce the GOMEA library,making existing GOMEA code in C++ accessible through Python, which serves as acentralized way of maintaining and distributing code of GOMEA for variousoptimization domains. Moreover, it allows for the straightforward definition ofBBO as well as GBO fitness functions within Python, which are called from theC++ optimization code for each required (partial) evaluation. We describe thestructure of the GOMEA library and how it can be used, and we show itsperformance in both GBO and Black-Box Optimization (BBO).</description><author>Anton Bouter, Peter A. N. Bosman</author><pubDate>Wed, 10 May 2023 16:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06246v1</guid></item><item><title>Explainable Knowledge Distillation for On-device Chest X-Ray Classification</title><link>http://arxiv.org/abs/2305.06244v1</link><description>Automated multi-label chest X-rays (CXR) image classification has achievedsubstantial progress in clinical diagnosis via utilizing sophisticated deeplearning approaches. However, most deep models have high computational demands,which makes them less feasible for compact devices with low computationalrequirements. To overcome this problem, we propose a knowledge distillation(KD) strategy to create the compact deep learning model for the real-timemulti-label CXR image classification. We study different alternatives of CNNsand Transforms as the teacher to distill the knowledge to a smaller student.Then, we employed explainable artificial intelligence (XAI) to provide thevisual explanation for the model decision improved by the KD. Our results onthree benchmark CXR datasets show that our KD strategy provides the improvedperformance on the compact student model, thus being the feasible choice formany limited hardware platforms. For instance, when using DenseNet161 as theteacher network, EEEA-Net-C2 achieved an AUC of 83.7%, 87.1%, and 88.7% on theChestX-ray14, CheXpert, and PadChest datasets, respectively, with fewerparameters of 4.7 million and computational cost of 0.3 billion FLOPS.</description><author>Chakkrit Termritthikun, Ayaz Umer, Suwichaya Suwanwimolkul, Feng Xia, Ivan Lee</author><pubDate>Wed, 10 May 2023 16:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06244v1</guid></item><item><title>Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2305.06242v1</link><description>End-to-end autonomous driving has made impressive progress in recent years.Existing methods usually adopt the decoupled encoder-decoder paradigm, wherethe encoder extracts hidden features from raw sensor data, and the decoderoutputs the ego-vehicle's future trajectories or actions. Under such aparadigm, the encoder does not have access to the intended behavior of the egoagent, leaving the burden of finding out safety-critical regions from themassive receptive field and inferring about future situations to the decoder.Even worse, the decoder is usually composed of several simple multi-layerperceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., acombination of heavy ResNets or Transformer). Such an imbalanced resource-taskdivision hampers the learning process. In this work, we aim to alleviate the aforementioned problem by twoprinciples: (1) fully utilizing the capacity of the encoder; (2) increasing thecapacity of the decoder. Concretely, we first predict a coarse-grained futureposition and action based on the encoder features. Then, conditioned on theposition and action, the future scene is imagined to check the ramification ifwe drive accordingly. We also retrieve the encoder features around thepredicted coordinate to obtain fine-grained information about thesafety-critical region. Finally, based on the predicted future and theretrieved salient feature, we refine the coarse-grained position and action bypredicting its offset from ground-truth. The above refinement module could bestacked in a cascaded fashion, which extends the capacity of the decoder withspatial-temporal prior knowledge about the conditioned future. We conductexperiments on the CARLA simulator and achieve state-of-the-art performance inclosed-loop benchmarks. Extensive ablation studies demonstrate theeffectiveness of each proposed module.</description><author>Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, Hongyang Li</author><pubDate>Wed, 10 May 2023 16:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06242v1</guid></item><item><title>Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization</title><link>http://arxiv.org/abs/2303.10758v2</link><description>This work studies the generalization error of gradient methods. Morespecifically, we focus on how training steps $T$ and step-size $\eta$ mightaffect generalization in smooth stochastic convex optimization (SCO) problems.We first provide tight excess risk lower bounds for Gradient Descent (GD) andStochastic Gradient Descent (SGD) under the general non-realizable smooth SCOsetting, suggesting that existing stability analyses are tight in step-size anditeration dependence, and that overfitting provably happens. Next, we study thecase when the loss is realizable, i.e. an optimal solution minimizes all thedata points. Recent works show better rates can be attained but the improvementis reduced when training time is long. Our paper examines this observation byproviding excess risk lower bounds for GD and SGD in two realizable settings:1) $\eta T = \bigO{n}$, and (2) $\eta T = \bigOmega{n}$, where $n$ is the sizeof dataset. In the first case $\eta T = \bigOmega{n}$, our lower bounds tightlymatch and certify the respective upper bounds. However, for the case $\eta T =\bigOmega{n}$, our analysis indicates a gap between the lower and upper bounds.A conjecture is proposed that the gap can be closed by improving upper bounds,supported by analyses in two special scenarios.</description><author>Peiyuan Zhang, Jiaye Teng, Jingzhao Zhang</author><pubDate>Wed, 10 May 2023 16:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10758v2</guid></item><item><title>Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation</title><link>http://arxiv.org/abs/2305.06236v1</link><description>X-ray images are the first steps for diagnosing and further treating dentalproblems. So, early diagnosis prevents the development and increase of oral anddental diseases. In this paper, we developed a semantic segmentation algorithmbased on BEIT adaptor and Mask2Former to detect and identify teeth, roots, andmultiple dental diseases and abnormalities such as pulp chamber, restoration,endodontics, crown, decay, pin, composite, bridge, pulpitis, orthodontics,radicular cyst, periapical cyst, cyst, implant, and bone graft material inpanoramic, periapical, and bitewing X-ray images. We compared the result of ouralgorithm to two state-of-the-art algorithms in image segmentation named:Deeplabv3 and Segformer on our own data set. We discovered that Radiousoutperformed those algorithms by increasing the mIoU scores by 9% and 33% inDeeplabv3+ and Segformer, respectively.</description><author>Mohammad Mashayekhi, Sara Ahmadi Majd, Arian Amiramjadi, Babak Mashayekhi</author><pubDate>Wed, 10 May 2023 16:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06236v1</guid></item><item><title>Penalized deep neural networks estimator with general loss functions under weak dependence</title><link>http://arxiv.org/abs/2305.06230v1</link><description>This paper carries out sparse-penalized deep neural networks predictors forlearning weakly dependent processes, with a broad class of loss functions. Wedeal with a general framework that includes, regression estimation,classification, times series prediction, $\cdots$ The $\psi$-weak dependencestructure is considered, and for the specific case of bounded observations,$\theta_\infty$-coefficients are also used. In this case of$\theta_\infty$-weakly dependent, a non asymptotic generalization bound withinthe class of deep neural networks predictors is provided. For learning both$\psi$ and $\theta_\infty$-weakly dependent processes, oracle inequalities forthe excess risk of the sparse-penalized deep neural networks estimators areestablished. When the target function is sufficiently smooth, the convergencerate of these excess risk is close to $\mathcal{O}(n^{-1/3})$. Some simulationresults are provided, and application to the forecast of the particulate matterin the Vit\'{o}ria metropolitan area is also considered.</description><author>William Kengne, Modou Wade</author><pubDate>Wed, 10 May 2023 16:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06230v1</guid></item><item><title>DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation</title><link>http://arxiv.org/abs/2305.06225v1</link><description>Predominant techniques on talking head generation largely depend on 2Dinformation, including facial appearances and motions from input face images.Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays acritical role in constructing accurate 3D facial structures and suppressingcomplex background noises for generation. However, dense 3D annotations forfacial videos is prohibitively costly to obtain. In this work, firstly, wepresent a novel self-supervised method for learning dense 3D facial geometry(ie, depth) from face videos, without requiring camera parameters and 3Dgeometry annotations in training. We further propose a strategy to learnpixel-level uncertainties to perceive more reliable rigid-motion pixels forgeometry learning. Secondly, we design an effective geometry-guided facialkeypoint estimation module, providing accurate keypoints for generating motionfields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth)attention mechanism, which can be applied to each generation layer, to capturefacial geometries in a coarse-to-fine manner. Extensive experiments areconducted on three challenging benchmarks (ie, VoxCeleb1, VoxCeleb2, and HDTF).The results demonstrate that our proposed framework can generate highlyrealistic-looking reenacted talking videos, with new state-of-the-artperformances established on these benchmarks. The codes and trained models arepublicly available on the GitHub project page athttps://github.com/harlanhong/CVPR2022-DaGAN</description><author>Fa-Ting Hong, Li Shen, Dan Xu</author><pubDate>Wed, 10 May 2023 15:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06225v1</guid></item><item><title>Learnware: Small Models Do Big</title><link>http://arxiv.org/abs/2210.03647v2</link><description>There are complaints about current machine learning techniques such as therequirement of a huge amount of training data and proficient training skills,the difficulty of continual learning, the risk of catastrophic forgetting, theleaking of data privacy/proprietary, etc. Most research efforts have beenfocusing on one of those concerned issues separately, paying less attention tothe fact that most issues are entangled in practice. The prevailing big modelparadigm, which has achieved impressive results in natural language processingand computer vision applications, has not yet addressed those issues, whereasbecoming a serious source of carbon emissions. This article offers an overviewof the learnware paradigm, which attempts to enable users not need to buildmachine learning models from scratch, with the hope of reusing small models todo things even beyond their original purposes, where the key ingredient is thespecification which enables a trained model to be adequately identified toreuse according to the requirement of future users who know nothing about themodel in advance.</description><author>Zhi-Hua Zhou, Zhi-Hao Tan</author><pubDate>Wed, 10 May 2023 15:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03647v2</guid></item><item><title>Multi-Prompt with Depth Partitioned Cross-Modal Learning</title><link>http://arxiv.org/abs/2305.06221v1</link><description>In recent years, soft prompt learning methods have been proposed to fine-tunelarge-scale vision-language pre-trained models for various downstream tasks.These methods typically combine learnable textual tokens with class tokens asinput for models with frozen parameters. However, they often employ a singleprompt to describe class contexts, failing to capture categories' diverseattributes adequately. This study introduces the Partitioned Multi-modal Prompt(PMPO), a multi-modal prompting technique that extends the soft prompt from asingle learnable prompt to multiple prompts. Our method divides the visualencoder depths and connects learnable prompts to the separated visual depths,enabling different prompts to capture the hierarchical contextual depths ofvisual representations. Furthermore, to maximize the advantages of multi-promptlearning, we incorporate prior information from manually designed templates andlearnable multi-prompts, thus improving the generalization capabilities of ourapproach. We evaluate the effectiveness of our approach on three challengingtasks: new class generalization, cross-dataset evaluation, and domaingeneralization. For instance, our method achieves a $79.28$ harmonic mean,averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),demonstrating significant competitiveness compared to state-of-the-artprompting methods.</description><author>Yiqi Wang, Xianda Guo, Zheng Zhu, Yingjie Tian</author><pubDate>Wed, 10 May 2023 15:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06221v1</guid></item><item><title>Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources</title><link>http://arxiv.org/abs/2305.06217v1</link><description>Machine learning (ML) in healthcare presents numerous opportunities forenhancing patient care, population health, and healthcare providers' workflows.However, the real-world clinical and cost benefits remain limited due tochallenges in data privacy, heterogeneous data sources, and the inability tofully leverage multiple data modalities. In this perspective paper, weintroduce "patchwork learning" (PL), a novel paradigm that addresses theselimitations by integrating information from disparate datasets composed ofdifferent data modalities (e.g., clinical free-text, medical images, omics) anddistributed across separate and secure sites. PL allows the simultaneousutilization of complementary data sources while preserving data privacy,enabling the development of more holistic and generalizable ML models. Wepresent the concept of patchwork learning and its current implementations inhealthcare, exploring the potential opportunities and applicable data sourcesfor addressing various healthcare challenges. PL leverages bridging modalitiesor overlapping feature spaces across sites to facilitate information sharingand impute missing data, thereby addressing related prediction tasks. Wediscuss the challenges associated with PL, many of which are shared byfederated and multimodal learning, and provide recommendations for futureresearch in this field. By offering a more comprehensive approach to healthcaredata integration, patchwork learning has the potential to revolutionize theclinical applicability of ML models. This paradigm promises to strike a balancebetween personalization and generalizability, ultimately enhancing patientexperiences, improving population health, and optimizing healthcare providers'workflows.</description><author>Suraj Rajendran, Weishen Pan, Mert R. Sabuncu, Jiayu Zhou, Fei Wang</author><pubDate>Wed, 10 May 2023 15:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06217v1</guid></item><item><title>Privacy-Preserving Prompt Tuning for Large Language Model Services</title><link>http://arxiv.org/abs/2305.06212v1</link><description>Prompt tuning provides an efficient way for users to customize Large LanguageModels (LLMs) with their private data in the emerging LLM service scenario.However, the sensitive nature of private data brings the need for privacypreservation in LLM service customization. Based on prompt tuning, we proposePrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacyguarantees for LLM services. \textsc{rapt} adopts a local privacy setting,allowing users to privatize their data locally with local differential privacy.As prompt tuning performs poorly when directly trained on privatized data, weintroduce a novel privatized token reconstruction task that is trained jointlywith the downstream task, allowing LLMs to learn better task-dependentrepresentations. Despite the simplicity of our framework, experiments show thatRAPT achieves competitive performance across tasks while providing privacyguarantees against adversaries.</description><author>Yansong Li, Zhixing Tan, Yang Liu</author><pubDate>Wed, 10 May 2023 15:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06212v1</guid></item><item><title>Multiclass MRI Brain Tumor Segmentation using 3D Attention-based U-Net</title><link>http://arxiv.org/abs/2305.06203v1</link><description>This paper proposes a 3D attention-based U-Net architecture for multi-regionsegmentation of brain tumors using a single stacked multi-modal volume createdby combining three non-native MRI volumes. The attention mechanism added to thedecoder side of the U-Net helps to improve segmentation accuracy byde-emphasizing healthy tissues and accentuating malignant tissues, resulting inbetter generalization power and reduced computational resources. The method istrained and evaluated on the BraTS 2021 Task 1 dataset, and demonstratesimprovement of accuracy over other approaches. My findings suggest that theproposed approach has potential to enhance brain tumor segmentation usingmulti-modal MRI data, contributing to better understanding and diagnosis ofbrain diseases. This work highlights the importance of combining multipleimaging modalities and incorporating attention mechanisms for improved accuracyin brain tumor segmentation.</description><author>Maryann M. Gitonga</author><pubDate>Wed, 10 May 2023 15:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06203v1</guid></item><item><title>Learning in a Single Domain for Non-Stationary Multi-Texture Synthesis</title><link>http://arxiv.org/abs/2305.06200v1</link><description>This paper aims for a new generation task: non-stationary multi-texturesynthesis, which unifies synthesizing multiple non-stationary textures in asingle model. Most non-stationary textures have large scale variance and canhardly be synthesized through one model. To combat this, we propose amulti-scale generator to capture structural patterns of various scales andeffectively synthesize textures with a minor cost. However, it is still hard tohandle textures of different categories with different texture patterns.Therefore, we present a category-specific training strategy to focus onlearning texture pattern of a specific domain. Interestingly, once trained, ourmodel is able to produce multi-pattern generations with dynamic variationswithout the need to finetune the model for different styles. Moreover, anobjective evaluation metric is designed for evaluating the quality of textureexpansion and global structure consistency. To our knowledge, ours is the firstscheme for this challenging task, including model, training, and evaluation.Experimental results demonstrate the proposed method achieves superiorperformance and time efficiency. The code will be available after thepublication.</description><author>Xudong Xie, Zijie Wu, Zhiliang Xu, Zhen Zhu</author><pubDate>Wed, 10 May 2023 15:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06200v1</guid></item><item><title>Computationally Efficient and Statistically Optimal Robust High-Dimensional Linear Regression</title><link>http://arxiv.org/abs/2305.06199v1</link><description>High-dimensional linear regression under heavy-tailed noise or outliercorruption is challenging, both computationally and statistically. Convexapproaches have been proven statistically optimal but suffer from highcomputational costs, especially since the robust loss functions are usuallynon-smooth. More recently, computationally fast non-convex approaches viasub-gradient descent are proposed, which, unfortunately, fail to deliver astatistically consistent estimator even under sub-Gaussian noise. In thispaper, we introduce a projected sub-gradient descent algorithm for both thesparse linear regression and low-rank linear regression problems. The algorithmis not only computationally efficient with linear convergence but alsostatistically optimal, be the noise Gaussian or heavy-tailed with a finite 1 +epsilon moment. The convergence theory is established for a general frameworkand its specific applications to absolute loss, Huber loss and quantile lossare investigated. Compared with existing non-convex methods, ours reveals asurprising phenomenon of two-phase convergence. In phase one, the algorithmbehaves as in typical non-smooth optimization that requires gradually decayingstepsizes. However, phase one only delivers a statistically sub-optimalestimator, which is already observed in the existing literature. Interestingly,during phase two, the algorithm converges linearly as if minimizing a smoothand strongly convex objective function, and thus a constant stepsize suffices.Underlying the phase-two convergence is the smoothing effect of random noise tothe non-smooth robust losses in an area close but not too close to the truth.Numerical simulations confirm our theoretical discovery and showcase thesuperiority of our algorithm over prior methods.</description><author>Yinan Shen, Jingyang Li, Jian-Feng Cai, Dong Xia</author><pubDate>Wed, 10 May 2023 15:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06199v1</guid></item><item><title>Survey of Federated Learning Models for Spatial-Temporal Mobility Applications</title><link>http://arxiv.org/abs/2305.05257v2</link><description>Federated learning involves training statistical models over edge devicessuch as mobile phones such that the training data is kept local. FederatedLearning (FL) can serve as an ideal candidate for training spatial temporalmodels that rely on heterogeneous and potentially massive numbers ofparticipants while preserving the privacy of highly sensitive location data.However, there are unique challenges involved with transitioning existingspatial temporal models to decentralized learning. In this survey paper, wereview the existing literature that has proposed FL-based models for predictinghuman mobility, traffic prediction, community detection, location-basedrecommendation systems, and other spatial-temporal tasks. We describe themetrics and datasets these works have been using and create a baseline of theseapproaches in comparison to the centralized settings. Finally, we discuss thechallenges of applying spatial-temporal models in a decentralized setting andby highlighting the gaps in the literature we provide a road map andopportunities for the research community.</description><author>Yacine Belal, Sonia Ben Mokhtar, Hamed Haddadi, Jaron Wang, Afra Mashhadi</author><pubDate>Wed, 10 May 2023 15:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05257v2</guid></item><item><title>Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications</title><link>http://arxiv.org/abs/2304.12330v2</link><description>The coupling of deep reinforcement learning to numerical flow controlproblems has recently received a considerable attention, leading togroundbreaking results and opening new perspectives for the domain. Due to theusually high computational cost of fluid dynamics solvers, the use of parallelenvironments during the learning process represents an essential ingredient toattain efficient control in a reasonable time. Yet, most of the deepreinforcement learning literature for flow control relies on on-policyalgorithms, for which the massively parallel transition collection may breaktheoretical assumptions and lead to suboptimal control models. To overcome thisissue, we propose a parallelism pattern relying on partial-trajectory buffersterminated by a return bootstrapping step, allowing a flexible use of parallelenvironments while preserving the on-policiness of the updates. This approachis illustrated on a CPU-intensive continuous flow control problem from theliterature.</description><author>J. Viquerat, E. Hachem</author><pubDate>Wed, 10 May 2023 15:23:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12330v2</guid></item><item><title>Optimally-Weighted Estimators of the Maximum Mean Discrepancy for Likelihood-Free Inference</title><link>http://arxiv.org/abs/2301.11674v4</link><description>Likelihood-free inference methods typically make use of a distance betweensimulated and real data. A common example is the maximum mean discrepancy(MMD), which has previously been used for approximate Bayesian computation,minimum distance estimation, generalised Bayesian inference, and within thenonparametric learning framework. The MMD is commonly estimated at a root-$m$rate, where $m$ is the number of simulated samples. This can lead tosignificant computational challenges since a large $m$ is required to obtain anaccurate estimate, which is crucial for parameter estimation. In this paper, wepropose a novel estimator for the MMD with significantly improved samplecomplexity. The estimator is particularly well suited for computationallyexpensive smooth simulators with low- to mid-dimensional inputs. This claim issupported through both theoretical results and an extensive simulation study onbenchmark simulators.</description><author>Ayush Bharti, Masha Naslidnyk, Oscar Key, Samuel Kaski, François-Xavier Briol</author><pubDate>Wed, 10 May 2023 15:19:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11674v4</guid></item><item><title>Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification</title><link>http://arxiv.org/abs/2211.10933v2</link><description>In recent years, person Re-identification (ReID) has rapidly progressed withwide real-world applications, but also poses significant risks of adversarialattacks. In this paper, we focus on the backdoor attack on deep ReID models.Existing backdoor attack methods follow an all-to-one or all-to-all attackscenario, where all the target classes in the test set have already been seenin the training set. However, ReID is a much more complex fine-grained open-setrecognition problem, where the identities in the test set are not contained inthe training set. Thus, previous backdoor attack methods for classification arenot applicable for ReID. To ameliorate this issue, we propose a novel backdoorattack on deep ReID under a new all-to-unknown scenario, called DynamicTriggers Invisible Backdoor Attack (DT-IBA). Instead of learning fixed triggersfor the target classes from the training set, DT-IBA can dynamically generatenew triggers for any unknown identities. Specifically, an identity hashingnetwork is proposed to first extract target identity information from areference image, which is then injected into the benign images by imagesteganography. We extensively validate the effectiveness and stealthiness ofthe proposed attack on benchmark datasets, and evaluate the effectiveness ofseveral defense methods against our attack.</description><author>Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao</author><pubDate>Wed, 10 May 2023 15:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10933v2</guid></item><item><title>Wooden Sleeper Decayed Detection for Rural Railway Prognostics Using Unsupervised Deeper FCDDs</title><link>http://arxiv.org/abs/2305.05103v2</link><description>It is critical for railway managers to maintain a high standard to ensureuser safety during daily operations. Top-view or side-view cameras and GPSpositioning system have enabled progress toward automating the periodicinspection of defective features and assessing the deteriorated status of therailway components. Frequently, collecting deteriorated status data constraintstime consuming and repeated data acquisition, because the temporal occurrenceis extremely imbalanced. Supervised learning approach requires thousands ofpaired dataset of defective raw images and annotated labels. However, one-classclassification approach has a merit that fewer images enables us to optimizethe parameters for training normal and anomalous feature. Simultaneously, thevisual heat map explanation enables us to discriminate the localized damagefeature. In this paper, we propose a prognostic discriminator pipeline toautomate one-class damage classification towards defective railway components.We also sensitivity analyze toward the backbone and the receptive field basedon convolutional neural networks (CNNs) using pretrained networks: baselineCNN27, VGG16, ResNet101, and Inception Networks. We also visualize theexplanation of the defective railway feature using a transposed Gaussianupsampling. We demonstrate our application for railway inspection in anopen-accessed dataset of defective railway components, and wooden sleeperdeterioration in rural railway. The heatmap is so important that thehazard-marks could cause an operational delay, an urgent inspection, andunexpected accident to passenger impact in railway inspection. Furthermore, wemention its usability for prognostic monitoring and future works for railwaycomponents inspection in the predictive maintenance of railway systems.</description><author>Takato Yasuno, Masahiro Okano, Junichiro Fujii</author><pubDate>Wed, 10 May 2023 15:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05103v2</guid></item><item><title>Differentiable Rendering with Reparameterized Volume Sampling</title><link>http://arxiv.org/abs/2302.10970v2</link><description>In view synthesis, a neural radiance field approximates underlying densityand radiance fields based on a sparse set of scene pictures. To generate apixel of a novel view, it marches a ray through the pixel and computes aweighted sum of radiance emitted from a dense set of ray points. This renderingalgorithm is fully differentiable and facilitates gradient-based optimizationof the fields. However, in practice, only a tiny opaque portion of the raycontributes most of the radiance to the sum. We propose a simple end-to-enddifferentiable sampling algorithm based on inverse transform sampling. Itgenerates samples according to the probability distribution induced by thedensity field and picks non-transparent points on the ray. We utilize thealgorithm in two ways. First, we propose a novel rendering approach based onMonte Carlo estimates. This approach allows for evaluating and optimizing aneural radiance field with just a few radiance field calls per ray. Second, weuse the sampling algorithm to modify the hierarchical scheme proposed in theoriginal NeRF work. We show that our modification improves reconstructionquality of hierarchical models, at the same time simplifying the trainingprocedure by removing the need for auxiliary proposal network losses.</description><author>Nikita Morozov, Denis Rakitin, Oleg Desheulin, Dmitry Vetrov, Kirill Struminsky</author><pubDate>Wed, 10 May 2023 15:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10970v2</guid></item><item><title>A Multi-modal Approach to Single-modal Visual Place Classification</title><link>http://arxiv.org/abs/2305.06179v1</link><description>Visual place classification from a first-person-view monocular RGB image is afundamental problem in long-term robot navigation. A difficulty arises from thefact that RGB image classifiers are often vulnerable to spatial and appearancechanges and degrade due to domain shifts, such as seasonal, weather, andlighting differences. To address this issue, multi-sensor fusion approachescombining RGB and depth (D) (e.g., LIDAR, radar, stereo) have gained popularityin recent years. Inspired by these efforts in multimodal RGB-D fusion, weexplore the use of pseudo-depth measurements from recently-developed techniquesof ``domain invariant" monocular depth estimation as an additional pseudo depthmodality, by reformulating the single-modal RGB image classification task as apseudo multi-modal RGB-D classification problem. Specifically, a practical,fully self-supervised framework for training, appropriately processing, fusing,and classifying these two modalities, RGB and pseudo-D, is described.Experiments on challenging cross-domain scenarios using public NCLT datasetsvalidate effectiveness of the proposed framework.</description><author>Tomoya Iwasaki, Kanji Tanaka, Kenta Tsukahara</author><pubDate>Wed, 10 May 2023 15:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06179v1</guid></item><item><title>Sequence-Agnostic Multi-Object Navigation</title><link>http://arxiv.org/abs/2305.06178v1</link><description>The Multi-Object Navigation (MultiON) task requires a robot to localize aninstance (each) of multiple object classes. It is a fundamental task for anassistive robot in a home or a factory. Existing methods for MultiON haveviewed this as a direct extension of Object Navigation (ON), the task oflocalising an instance of one object class, and are pre-sequenced, i.e., thesequence in which the object classes are to be explored is provided in advance.This is a strong limitation in practical applications characterized by dynamicchanges. This paper describes a deep reinforcement learning framework forsequence-agnostic MultiON based on an actor-critic architecture and a suitablereward specification. Our framework leverages past experiences and seeks toreward progress toward individual as well as multiple target object classes. Weuse photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat3D simulation environment to experimentally show that our method performsbetter than a pre-sequenced approach and a state of the art ON method extendedto MultiON.</description><author>Nandiraju Gireesh, Ayush Agrawal, Ahana Datta, Snehasis Banerjee, Mohan Sridharan, Brojeshwar Bhowmick, Madhava Krishna</author><pubDate>Wed, 10 May 2023 15:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06178v1</guid></item><item><title>A FPGA-based architecture for real-time cluster finding in the LHCb silicon pixel detector</title><link>http://arxiv.org/abs/2302.03972v2</link><description>This article describes a custom VHDL firmware implementation of atwo-dimensional cluster-finder architecture for reconstructing hit positions inthe new vertex pixel detector (VELO) that is part of the LHCb Upgrade. Thisfirmware has been deployed to the existing FPGA cards that perform the readoutof the VELO, as a further enhancement of the DAQ system, and will run in realtime during physics data taking, reconstructing VELO hits coordinateson-the-fly at the LHC collision rate. This pre-processing allows the firstlevel of the software trigger to accept a 11% higher rate of events, as theready-made hits coordinates accelerate the track reconstruction and consumessignificantly less electrical power. It additionally allows the raw pixel datato be dropped at the readout level, thus saving approximately 14% of the DAQbandwidth. Detailed simulation studies have shown that the use of thisreal-time cluster finding does not introduce any appreciable degradation in thetracking performance in comparison to a full-fledged software implementation.This work is part of a wider effort aimed at boosting the real-time processingcapability of HEP experiments by delegating intensive tasks to dedicatedcomputing accelerators deployed at the earliest stages of the data acquisitionchain.</description><author>G. Bassi, L. Giambastiani, K. Hennessy, F. Lazzari, M. J. Morello, T. Pajero, A. Fernandez Prieto, G. Punzi</author><pubDate>Wed, 10 May 2023 15:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03972v2</guid></item><item><title>Speech Modeling with a Hierarchical Transformer Dynamical VAE</title><link>http://arxiv.org/abs/2303.09404v2</link><description>The dynamical variational autoencoders (DVAEs) are a family oflatent-variable deep generative models that extends the VAE to model a sequenceof observed data and a corresponding sequence of latent vectors. In almost allthe DVAEs of the literature, the temporal dependencies within each sequence andacross the two sequences are modeled with recurrent neural networks. In thispaper, we propose to model speech signals with the Hierarchical TransformerDVAE (HiT-DVAE), which is a DVAE with two levels of latent variable(sequence-wise and frame-wise) and in which the temporal dependencies areimplemented with the Transformer architecture. We show that HiT-DVAEoutperforms several other DVAEs for speech spectrogram modeling, while enablinga simpler training procedure, revealing its high potential for downstreamlow-level speech processing tasks such as speech enhancement.</description><author>Xiaoyu Lin, Xiaoyu Bie, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda</author><pubDate>Wed, 10 May 2023 14:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09404v2</guid></item><item><title>Medical Image Deidentification, Cleaning and Compression Using Pylogik</title><link>http://arxiv.org/abs/2304.12322v5</link><description>Leveraging medical record information in the era of big data and machinelearning comes with the caveat that data must be cleaned and de-identified.Facilitating data sharing and harmonization for multi-center collaborations areparticularly difficult when protected health information (PHI) is contained orembedded in image meta-data. We propose a novel library in the Pythonframework, called PyLogik, to help alleviate this issue for ultrasound images,which are particularly challenging because of the frequent inclusion of PHIdirectly on the images. PyLogik processes the image volumes through a series oftext detection/extraction, filtering, thresholding, morphological and contourcomparisons. This methodology de-identifies the images, reduces file sizes, andprepares image volumes for applications in deep learning and data sharing. Toevaluate its effectiveness in processing ultrasound data, a random sample of 50cardiac ultrasounds (echocardiograms) were processed through PyLogik, and theoutputs were compared with the manual segmentations by an expert user. The Dicecoefficient of the two approaches achieved an average value of 0.976. Next, aninvestigation was conducted to ascertain the degree of information compressionachieved using the algorithm. Resultant data was found to be on average ~72%smaller after processing by PyLogik. Our results suggest that PyLogik is aviable methodology for data cleaning and de-identification, determining ROI,and file compression which will facilitate efficient storage, use, anddissemination of ultrasound data. Variants of the pipeline have also beencreated for use with other medical imaging data types.</description><author>Adrienne Kline, Vinesh Appadurai, Yuan Luo, Sanjiv Shah</author><pubDate>Wed, 10 May 2023 14:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12322v5</guid></item><item><title>Approximation of nearly-periodic symplectic maps via structure-preserving neural networks</title><link>http://arxiv.org/abs/2210.05087v2</link><description>A continuous-time dynamical system with parameter $\varepsilon$ isnearly-periodic if all its trajectories are periodic with nowhere-vanishingangular frequency as $\varepsilon$ approaches 0. Nearly-periodic maps arediscrete-time analogues of nearly-periodic systems, defined asparameter-dependent diffeomorphisms that limit to rotations along a circleaction, and they admit formal $U(1)$ symmetries to all orders when the limitingrotation is non-resonant. For Hamiltonian nearly-periodic maps on exactpresymplectic manifolds, the formal $U(1)$ symmetry gives rise to adiscrete-time adiabatic invariant. In this paper, we construct a novelstructure-preserving neural network to approximate nearly-periodic symplecticmaps. This neural network architecture, which we call symplectic gyroceptron,ensures that the resulting surrogate map is nearly-periodic and symplectic, andthat it gives rise to a discrete-time adiabatic invariant and a long-timestability. This new structure-preserving neural network provides a promisingarchitecture for surrogate modeling of non-dissipative dynamical systems thatautomatically steps over short timescales without introducing spuriousinstabilities.</description><author>Valentin Duruisseaux, Joshua W. Burby, Qi Tang</author><pubDate>Wed, 10 May 2023 14:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05087v2</guid></item><item><title>A Double Machine Learning Trend Model for Citizen Science Data</title><link>http://arxiv.org/abs/2210.15524v2</link><description>1. Citizen and community-science (CS) datasets have great potential forestimating interannual patterns of population change given the large volumes ofdata collected globally every year. Yet, the flexible protocols that enablemany CS projects to collect large volumes of data typically lack the structurenecessary to keep consistent sampling across years. This leads to interannualconfounding, as changes to the observation process over time are confoundedwith changes in species population sizes. 2. Here we describe a novel modeling approach designed to estimate speciespopulation trends while controlling for the interannual confounding common incitizen science data. The approach is based on Double Machine Learning, astatistical framework that uses machine learning methods to estimate populationchange and the propensity scores used to adjust for confounding discovered inthe data. Additionally, we develop a simulation method to identify and adjustfor residual confounding missed by the propensity scores. Using this newmethod, we can produce spatially detailed trend estimates from citizen sciencedata. 3. To illustrate the approach, we estimated species trends using data fromthe CS project eBird. We used a simulation study to assess the ability of themethod to estimate spatially varying trends in the face of real-worldconfounding. Results showed that the trend estimates distinguished betweenspatially constant and spatially varying trends at a 27km resolution. Therewere low error rates on the estimated direction of population change(increasing/decreasing) and high correlations on the estimated magnitude. 4. The ability to estimate spatially explicit trends while accounting forconfounding in citizen science data has the potential to fill importantinformation gaps, helping to estimate population trends for species, regions,or seasons without rigorous monitoring data.</description><author>Daniel Fink, Alison Johnston, Matt Strimas-Mackey, Tom Auer, Wesley M. Hochachka, Shawn Ligocki, Lauren Oldham Jaromczyk, Orin Robinson, Chris Wood, Steve Kelling, Amanda D. Rodewald</author><pubDate>Wed, 10 May 2023 14:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.15524v2</guid></item><item><title>Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification</title><link>http://arxiv.org/abs/2305.06145v1</link><description>Clothes-invariant feature extraction is critical to the clothes-changingperson re-identification (CC-ReID). It can provide discriminative identityfeatures and eliminate the negative effects caused by the confounder--clothingchanges. But we argue that there exists a strong spurious correlation betweenclothes and human identity, that restricts the common likelihood-based ReIDmethod P(Y|X) to extract clothes-irrelevant features. In this paper, we proposea new Causal Clothes-Invariant Learning (CCIL) method to achieveclothes-invariant feature learning by modeling causal intervention P(Y|do(X)).This new causality-based model is inherently invariant to the confounder in thecausal view, which can achieve the clothes-invariant features and avoid thebarrier faced by the likelihood-based methods. Extensive experiments on threeCC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate theeffectiveness of our approach, which achieves a new state of the art.</description><author>Xulin Li, Yan Lu, Bin Liu, Yuenan Hou, Yating Liu, Qi Chu, Wanli Ouyang, Nenghai Yu</author><pubDate>Wed, 10 May 2023 14:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06145v1</guid></item><item><title>Double Robust Bayesian Inference on Average Treatment Effects</title><link>http://arxiv.org/abs/2211.16298v3</link><description>We study a double robust Bayesian inference procedure on the averagetreatment effect (ATE) under unconfoundedness. Our robust Bayesian approachinvolves two adjustment steps: first, we make a correction for priordistributions of the conditional mean function; second, we introduce arecentering term on the posterior distribution of the resulting ATE. We proveasymptotic equivalence of our Bayesian estimator and double robust frequentistestimators by establishing a new semiparametric Bernstein-von Mises theoremunder double robustness; i.e., the lack of smoothness of conditional meanfunctions can be compensated by high regularity of the propensity score andvice versa. Consequently, the resulting Bayesian point estimator internalizesthe bias correction as the frequentist-type doubly robust estimator, and theBayesian credible sets form confidence intervals with asymptotically exactcoverage probability. In simulations, we find that this robust Bayesianprocedure leads to significant bias reduction of point estimation and accuratecoverage of confidence intervals, especially when the dimensionality ofcovariates is large relative to the sample size and the underlying functionsbecome complex. We illustrate our method in an application to the NationalSupported Work Demonstration.</description><author>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</author><pubDate>Wed, 10 May 2023 14:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16298v3</guid></item><item><title>Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery</title><link>http://arxiv.org/abs/2305.06144v1</link><description>In this paper, we address the problem of generalized category discovery(GCD), \ie, given a set of images where part of them are labelled and the restare not, the task is to automatically cluster the images in the unlabelleddata, leveraging the information from the labelled data, while the unlabelleddata contain images from the labelled classes and also new ones. GCD is similarto semi-supervised learning (SSL) but is more realistic and challenging, as SSLassumes all the unlabelled images are from the same classes as the labelledones. We also do not assume the class number in the unlabelled data is knowna-priori, making the GCD problem even harder. To tackle the problem of GCDwithout knowing the class number, we propose an EM-like framework thatalternates between representation learning and class number estimation. Wepropose a semi-supervised variant of the Gaussian Mixture Model (GMM) with astochastic splitting and merging mechanism to dynamically determine theprototypes by examining the cluster compactness and separability. With theseprototypes, we leverage prototypical contrastive learning for representationlearning on the partially labelled data subject to the constraints imposed bythe labelled data. Our framework alternates between these two steps untilconvergence. The cluster assignment for an unlabelled instance can then beretrieved by identifying its nearest prototype. We comprehensively evaluate ourframework on both generic image classification datasets and challengingfine-grained object recognition datasets, achieving state-of-the-artperformance.</description><author>Bingchen Zhao, Xin Wen, Kai Han</author><pubDate>Wed, 10 May 2023 14:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06144v1</guid></item><item><title>Feature Expansion for Graph Neural Networks</title><link>http://arxiv.org/abs/2305.06142v1</link><description>Graph neural networks aim to learn representations for graph-structured dataand show impressive performance, particularly in node classification. Recently,many methods have studied the representations of GNNs from the perspective ofoptimization goals and spectral graph theory. However, the feature space thatdominates representation learning has not been systematically studied in graphneural networks. In this paper, we propose to fill this gap by analyzing thefeature space of both spatial and spectral models. We decompose graph neuralnetworks into determined feature spaces and trainable weights, providing theconvenience of studying the feature space explicitly using matrix spaceanalysis. In particular, we theoretically find that the feature space tends tobe linearly correlated due to repeated aggregations. Motivated by thesefindings, we propose 1) feature subspaces flattening and 2) structuralprincipal components to expand the feature space. Extensive experiments verifythe effectiveness of our proposed more comprehensive feature space, withcomparable inference time to the baseline, and demonstrate its efficientconvergence capability.</description><author>Jiaqi Sun, Lin Zhang, Guangyi Chen, Kun Zhang, Peng XU, Yujiu Yang</author><pubDate>Wed, 10 May 2023 14:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06142v1</guid></item><item><title>Active Semantic Localization with Graph Neural Embedding</title><link>http://arxiv.org/abs/2305.06141v1</link><description>Semantic localization, i.e., robot self-localization with semantic imagemodality, is critical in recently emerging embodied AI applications such aspoint-goal navigation, object-goal navigation and vision language navigation.However, most existing works on semantic localization focus on passive visiontasks without viewpoint planning, or rely on additional rich modalities (e.g.,depth measurements). Thus, the problem is largely unsolved. In this work, weexplore a lightweight, entirely CPU-based, domain-adaptive semanticlocalization framework, called graph neural localizer.Our approach is inspiredby two recently emerging technologies: (1) Scene graph, which combines theviewpoint- and appearance- invariance of local and global features; (2) Graphneural network, which enables direct learning/recognition of graph data (i.e.,non-vector data). Specifically, a graph convolutional neural network is firsttrained as a scene graph classifier for passive vision, and then its knowledgeis transferred to a reinforcement-learning planner for active vision.Experiments on two scenarios, self-supervised learning and unsupervised domainadaptation, using a photo-realistic Habitat simulator validate theeffectiveness of the proposed method.</description><author>Mitsuki Yoshida, Kanji Tanaka, Ryogo Yamamoto, Daiki Iwata</author><pubDate>Wed, 10 May 2023 14:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06141v1</guid></item><item><title>CrudeBERT: Applying Economic Theory towards fine-tuning Transformer-based Sentiment Analysis Models to the Crude Oil Market</title><link>http://arxiv.org/abs/2305.06140v1</link><description>Predicting market movements based on the sentiment of news media has a longtradition in data analysis. With advances in natural language processing,transformer architectures have emerged that enable contextually aware sentimentclassification. Nevertheless, current methods built for the general financialmarket such as FinBERT cannot distinguish asset-specific value-driving factors.This paper addresses this shortcoming by presenting a method that identifiesand classifies events that impact supply and demand in the crude oil marketswithin a large corpus of relevant news headlines. We then introduce CrudeBERT,a new sentiment analysis model that draws upon these events to contextualizeand fine-tune FinBERT, thereby yielding improved sentiment classifications forheadlines related to the crude oil futures market. An extensive evaluationdemonstrates that CrudeBERT outperforms proprietary and open-source solutionsin the domain of crude oil.</description><author>Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun</author><pubDate>Wed, 10 May 2023 14:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06140v1</guid></item><item><title>A Neural Emulator for Uncertainty Estimation of Fire Propagation</title><link>http://arxiv.org/abs/2305.06139v1</link><description>Wildfire propagation is a highly stochastic process where small changes inenvironmental conditions (such as wind speed and direction) can lead to largechanges in observed behaviour. A traditional approach to quantify uncertaintyin fire-front progression is to generate probability maps via ensembles ofsimulations. However, use of ensembles is typically computationally expensive,which can limit the scope of uncertainty analysis. To address this, we explorethe use of a spatio-temporal neural-based modelling approach to directlyestimate the likelihood of fire propagation given uncertainty in inputparameters. The uncertainty is represented by deliberately perturbing the inputweather forecast during model training. The computational load is concentratedin the model training process, which allows larger probability spaces to beexplored during deployment. Empirical evaluations indicate that the proposedmodel achieves comparable fire boundaries to those produced by the traditionalSPARK simulation platform, with an overall Jaccard index (similarity score) of67.4% on a set of 35 simulated fires. When compared to a related neural model(emulator) which was employed to generate probability maps via ensembles ofemulated fires, the proposed approach produces competitive Jaccard similarityscores while being approximately an order of magnitude faster.</description><author>Andrew Bolt, Conrad Sanderson, Joel Janek Dabrowski, Carolyn Huston, Petra Kuhnert</author><pubDate>Wed, 10 May 2023 14:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06139v1</guid></item><item><title>AttentionMixer: An Accurate and Interpretable Framework for Process Monitoring</title><link>http://arxiv.org/abs/2302.10426v2</link><description>An accurate and explainable automatic monitoring system is critical for thesafety of high efficiency energy conversion plants that operate under extremeworking condition. Nonetheless, currently available data-driven monitoringsystems often fall short in meeting the requirements for either high-accuracyor interpretability, which hinders their application in practice. To overcomethis limitation, a data-driven approach, AttentionMixer, is proposed under ageneralized message passing framework, with the goal of establishing anaccurate and interpretable radiation monitoring framework for energy conversionplants. To improve the model accuracy, the first technical contributioninvolves the development of spatial and temporal adaptive message passingblocks, which enable the capture of spatial and temporal correlations,respectively; the two blocks are cascaded through a mixing operator. To enhancethe model interpretability, the second technical contribution involves theimplementation of a sparse message passing regularizer, which eliminatesspurious and noisy message passing routes. The effectiveness of theAttentionMixer approach is validated through extensive evaluations on amonitoring benchmark collected from the national radiation monitoring networkfor nuclear power plants, resulting in enhanced monitoring accuracy andinterpretability in practice.</description><author>Hao Wang, Zhiyu Wang, Yunlong Niu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, Xinggao Liu</author><pubDate>Wed, 10 May 2023 14:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10426v2</guid></item><item><title>A proof of convergence of inverse reinforcement learning for multi-objective optimization</title><link>http://arxiv.org/abs/2305.06137v1</link><description>We show the convergence of Wasserstein inverse reinforcement learning (WIRL)for multi-objective optimizations with the projective subgradient method byformulating an inverse problem of the optimization problem that is equivalentto WIRL for multi-objective optimizations. In addition, we prove convergence of inverse reinforcement learning (maximumentropy inverse reinforcement learning, guid cost learning) for multi-objectiveoptimization with the projective subgradient method.</description><author>Akira Kitaoka, Riki Eto</author><pubDate>Wed, 10 May 2023 14:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06137v1</guid></item><item><title>Large Language Models Need Holistically Thought in Medical Conversational QA</title><link>http://arxiv.org/abs/2305.05410v2</link><description>The medical conversational question answering (CQA) system aims at providinga series of professional medical services to improve the efficiency of medicalcare. Despite the success of large language models (LLMs) in complex reasoningtasks in various fields, such as mathematics, logic, and commonsense QA, theystill need to improve with the increased complexity and specialization of themedical field. This is because medical CQA tasks require not only strongmedical reasoning, but also the ability to think broadly and deeply. In thispaper, to address these challenges in medical CQA tasks that need to beconsidered and understood in many aspects, we propose the Holistically Thought(HoT) method, which is designed to guide the LLMs to perform the diffused andfocused thinking for generating high-quality medical responses. The proposedHoT method has been evaluated through automated and manual assessments in threedifferent medical CQA datasets containing the English and Chinese languages.The extensive experimental results show that our method can produce morecorrectness, professional, and considerate answers than severalstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code inhttps://github.com/WENGSYX/HoT.</description><author>Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bin Sun, Shizhu He, Kang Liu, Jun Zhao</author><pubDate>Wed, 10 May 2023 14:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05410v2</guid></item><item><title>Real-time motion amplification on mobile devices</title><link>http://arxiv.org/abs/2206.08422v2</link><description>A simple motion amplification algorithm suitable for real-time applicationson mobile devices, including smartphones, is presented. It is based on motionenhancement by moving average differencing (MEMAD), a temporal high-pass filterfor video streams. MEMAD can amplify small moving objects or subtle motion inlarger objects. It is computationally sufficiently simple to be implemented inreal time on smartphones. In the specific implementation as an Android phoneapp, MEMAD is demonstrated on examples chosen such as to motivate applicationsin the engineering, biological, and medical sciences.</description><author>Henning U. Voss</author><pubDate>Wed, 10 May 2023 14:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08422v2</guid></item><item><title>When ChatGPT for Computer Vision Will Come? From 2D to 3D</title><link>http://arxiv.org/abs/2305.06133v1</link><description>ChatGPT and its improved variant GPT4 have revolutionized the NLP field witha single model solving almost all text related tasks. However, such a model forcomputer vision does not exist, especially for 3D vision. This article firstprovides a brief view on the progress of deep learning in text, image and 3Dfields from the model perspective. Moreover, this work further discusses howAIGC evolves from the data perspective. On top of that, this work presents anoutlook on the development of AIGC in 3D from the data perspective.</description><author>Chenghao Li, Chaoning Zhang</author><pubDate>Wed, 10 May 2023 14:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06133v1</guid></item><item><title>Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era</title><link>http://arxiv.org/abs/2305.06131v1</link><description>Generative AI (AIGC, a.k.a. AI generated content) has made remarkableprogress in the past few years, among which text-guided content generation isthe most practical one since it enables the interaction between humaninstruction and AIGC. Due to the development in text-to-image as well 3Dmodeling technologies (like NeRF), text-to-3D has become a newly emerging yethighly active research field. Our work conducts the first yet comprehensivesurvey on text-to-3D to help readers interested in this direction quickly catchup with its fast development. First, we introduce 3D data representations,including both Euclidean data and non-Euclidean data. On top of that, weintroduce various foundation technologies as well as summarize how recent workscombine those foundation technologies to realize satisfactory text-to-3D.Moreover, we summarize how text-to-3D technology is used in variousapplications, including avatar generation, texture generation, shapetransformation, and scene generation.</description><author>Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, Choong Seon Hong</author><pubDate>Wed, 10 May 2023 14:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06131v1</guid></item><item><title>Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement</title><link>http://arxiv.org/abs/2305.05354v2</link><description>Spinal fusion surgery requires highly accurate implantation of pedicle screwimplants, which must be conducted in critical proximity to vital structureswith a limited view of anatomy. Robotic surgery systems have been proposed toimprove placement accuracy, however, state-of-the-art systems suffer from thelimitations of open-loop approaches, as they follow traditional concepts ofpreoperative planning and intraoperative registration, without real-timerecalculation of the surgical plan. In this paper, we propose an intraoperativeplanning approach for robotic spine surgery that leverages real-timeobservation for drill path planning based on Safe Deep Reinforcement Learning(DRL). The main contributions of our method are (1) the capability to guaranteesafe actions by introducing an uncertainty-aware distance-based safety filter;and (2) the ability to compensate for incomplete intraoperative anatomicalinformation, by encoding a-priori knowledge about anatomical structures with anetwork pre-trained on high-fidelity anatomical models. Planning quality wasassessed by quantitative comparison with the gold standard (GS) drill planning.In experiments with 5 models derived from real magnetic resonance imaging (MRI)data, our approach was capable of achieving 90% bone penetration with respectto the GS while satisfying safety requirements, even under observation andmotion uncertainty. To the best of our knowledge, our approach is the firstsafe DRL approach focusing on orthopedic surgeries.</description><author>Yunke Ao, Hooman Esfandiari, Fabio Carrillo, Yarden As, Mazda Farshad, Benjamin F. Grewe, Andreas Krause, Philipp Fuernstahl</author><pubDate>Wed, 10 May 2023 14:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05354v2</guid></item><item><title>FedDWA: Personalized Federated Learning with Online Weight Adjustment</title><link>http://arxiv.org/abs/2305.06124v1</link><description>Different from conventional federated learning, personalized federatedlearning (PFL) is able to train a customized model for each individual clientaccording to its unique requirement. The mainstream approach is to adopt a kindof weighted aggregation method to generate personalized models, in whichweights are determined by the loss value or model parameters among differentclients. However, such kinds of methods require clients to download others'models. It not only sheer increases communication traffic but also potentiallyinfringes data privacy. In this paper, we propose a new PFL algorithm called\emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to addressthe above problem, which leverages the parameter server (PS) to computepersonalized aggregation weights based on collected models from clients. Inthis way, FedDWA can capture similarities between clients with much lesscommunication overhead. More specifically, we formulate the PFL problem as anoptimization problem by minimizing the distance between personalized models andguidance models, so as to customize aggregation weights for each client.Guidance models are obtained by the local one-step ahead adaptation onindividual clients. Finally, we conduct extensive experiments using five realdatasets and the results demonstrate that FedDWA can significantly reduce thecommunication traffic and achieve much higher model accuracy than thestate-of-the-art approaches.</description><author>Jiahao Liu, Jiang Wu, Jinyu Chen, Miao Hu, Yipeng Zhou, Di Wu</author><pubDate>Wed, 10 May 2023 14:12:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06124v1</guid></item><item><title>Training neural network ensembles via trajectory sampling</title><link>http://arxiv.org/abs/2209.11116v2</link><description>In machine learning, there is renewed interest in neural network ensembles(NNEs), whereby predictions are obtained as an aggregate from a diverse set ofsmaller models, rather than from a single larger model. Here, we show how todefine and train a NNE using techniques from the study of rare trajectories instochastic systems. We define an NNE in terms of the trajectory of the modelparameters under a simple, and discrete in time, diffusive dynamics, and trainthe NNE by biasing these trajectories towards a small time-integrated loss, ascontrolled by appropriate counting fields which act as hyperparameters. Wedemonstrate the viability of this technique on a range of simple supervisedlearning tasks. We discuss potential advantages of our trajectory samplingapproach compared with more conventional gradient based methods.</description><author>Jamie F. Mair, Dominic C. Rose, Juan P. Garrahan</author><pubDate>Wed, 10 May 2023 14:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11116v2</guid></item><item><title>Transformer-based model for monocular visual odometry: a video understanding approach</title><link>http://arxiv.org/abs/2305.06121v1</link><description>Estimating the camera pose given images of a single camera is a traditionaltask in mobile robots and autonomous vehicles. This problem is called monocularvisual odometry and it often relies on geometric approaches that requireengineering effort for a specific scenario. Deep learning methods have shown tobe generalizable after proper training and a considerable amount of availabledata. Transformer-based architectures have dominated the state-of-the-art innatural language processing and computer vision tasks, such as image and videounderstanding. In this work, we deal with the monocular visual odometry as avideo understanding task to estimate the 6-DoF camera's pose. We contribute bypresenting the TSformer-VO model based on spatio-temporal self-attentionmechanisms to extract features from clips and estimate the motions in anend-to-end manner. Our approach achieved competitive state-of-the-artperformance compared with geometry-based and deep learning-based methods on theKITTI visual odometry dataset, outperforming the DeepVO implementation highlyaccepted in the visual odometry community.</description><author>André O. Françani, Marcos R. O. A. Maximo</author><pubDate>Wed, 10 May 2023 14:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06121v1</guid></item><item><title>NeRF$^\textbf{2}$: Neural Radio-Frequency Radiance Fields</title><link>http://arxiv.org/abs/2305.06118v1</link><description>Although Maxwell discovered the physical laws of electromagnetic waves 160years ago, how to precisely model the propagation of an RF signal in anelectrically large and complex environment remains a long-standing problem. Thedifficulty is in the complex interactions between the RF signal and theobstacles (e.g., reflection, diffraction, etc.). Inspired by the great successof using a neural network to describe the optical field in computer vision, wepropose a neural radio-frequency radiance field, NeRF$^\textbf{2}$, whichrepresents a continuous volumetric scene function that makes sense of an RFsignal's propagation. Particularly, after training with a few signalmeasurements, NeRF$^\textbf{2}$ can tell how/what signal is received at anyposition when it knows the position of a transmitter. As a physical-layerneural network, NeRF$^\textbf{2}$ can take advantage of the learned statisticmodel plus the physical model of ray tracing to generate a synthetic datasetthat meets the training demands of application-layer artificial neural networks(ANNs). Thus, we can boost the performance of ANNs by the proposedturbo-learning, which mixes the true and synthetic datasets to intensify thetraining. Our experiment results show that turbo-learning can enhanceperformance with an approximate 50% increase. We also demonstrate the power ofNeRF$^\textbf{2}$ in the field of indoor localization and 5G MIMO.</description><author>Xiaopeng Zhao, Zhenlin An, Qingrui Pan, Lei Yang</author><pubDate>Wed, 10 May 2023 14:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06118v1</guid></item><item><title>VTPNet for 3D deep learning on point cloud</title><link>http://arxiv.org/abs/2305.06115v1</link><description>Recently, Transformer-based methods for point cloud learning have achievedgood results on various point cloud learning benchmarks. However, since theattention mechanism needs to generate three feature vectors of query, key, andvalue to calculate attention features, most of the existing Transformer-basedpoint cloud learning methods usually consume a large amount of computationaltime and memory resources when calculating global attention. To address thisproblem, we propose a Voxel-Transformer-Point (VTP) Block for extracting localand global features of point clouds. VTP combines the advantages ofvoxel-based, point-based and Transformer-based methods, which consists ofVoxel-Based Branch (V branch), Point-Based Transformer Branch (PT branch) andPoint-Based Branch (P branch). The V branch extracts the coarse-grainedfeatures of the point cloud through low voxel resolution; the PT branch obtainsthe fine-grained features of the point cloud by calculating the self-attentionin the local neighborhood and the inter-neighborhood cross-attention; the Pbranch uses a simplified MLP network to generate the global locationinformation of the point cloud. In addition, to enrich the local features ofpoint clouds at different scales, we set the voxel scale in the V branch andthe neighborhood sphere scale in the PT branch to one large and one small(large voxel scale \&amp; small neighborhood sphere scale or small voxel scale \&amp;large neighborhood sphere scale). Finally, we use VTP as the feature extractionnetwork to construct a VTPNet for point cloud learning, and performs shapeclassification, part segmentation, and semantic segmentation tasks on theModelNet40, ShapeNet Part, and S3DIS datasets. The experimental resultsindicate that VTPNet has good performance in 3D point cloud learning.</description><author>Wei Zhou, Weiwei Jin, Qian Wang, Yifan Wang, Dekui Wang, Xingxing Hao, Yongxiang Yu</author><pubDate>Wed, 10 May 2023 14:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06115v1</guid></item><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>The Compositional Structure of Bayesian Inference</title><link>http://arxiv.org/abs/2305.06112v1</link><description>Bayes' rule tells us how to invert a causal process in order to update ourbeliefs in light of new evidence. If the process is believed to have a complexcompositional structure, we may observe that the inversion of the whole can becomputed piecewise in terms of the component processes. We study the structureof this compositional rule, noting that it relates to the lens pattern infunctional programming. Working in a suitably general axiomatic presentation ofa category of Markov kernels, we see how we can think of Bayesian inversion asa particular instance of a state-dependent morphism in a fibred category. Wediscuss the compositional nature of this, formulated as a functor on theunderlying category and explore how this can used for a more type-drivenapproach to statistical inference.</description><author>Dylan Braithwaite, Jules Hedges, Toby St Clere Smithe</author><pubDate>Wed, 10 May 2023 13:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06112v1</guid></item><item><title>QuaLA-MiniLM: a Quantized Length Adaptive MiniLM</title><link>http://arxiv.org/abs/2210.17114v3</link><description>Limited computational budgets often prevent transformers from being used inproduction and from having their high accuracy utilized. A knowledgedistillation approach addresses the computational efficiency by self-distillingBERT into a smaller transformer representation having fewer layers and smallerinternal embedding. However, the performance of these models drops as we reducethe number of layers, notably in advanced NLP tasks such as span questionanswering. In addition, a separate model must be trained for each inferencescenario with its distinct computational budget. Dynamic-TinyBERT tackles bothlimitations by partially implementing the Length Adaptive Transformer (LAT)technique onto TinyBERT, achieving x3 speedup over BERT-base with minimalaccuracy loss. In this work, we expand the Dynamic-TinyBERT approach togenerate a much more highly efficient model. We use MiniLM distillation jointlywith the LAT method, and we further enhance the efficiency by applying low-bitquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) istrained only once, dynamically fits any inference scenario, and achieves anaccuracy-efficiency trade-off superior to any other efficient approaches perany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with &lt;1%accuracy loss). The code to reproduce this work is publicly available onGithub.</description><author>Shira Guskin, Moshe Wasserblat, Chang Wang, Haihao Shen</author><pubDate>Wed, 10 May 2023 13:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17114v3</guid></item><item><title>Linguistic More: Taking a Further Step toward Efficient and Accurate Scene Text Recognition</title><link>http://arxiv.org/abs/2305.05140v2</link><description>Vision model have gained increasing attention due to their simplicity andefficiency in Scene Text Recognition (STR) task. However, due to lacking theperception of linguistic knowledge and information, recent vision models sufferfrom two problems: (1) the pure vision-based query results in attention drift,which usually causes poor recognition and is summarized as linguisticinsensitive drift (LID) problem in this paper. (2) the visual feature issuboptimal for the recognition in some vision-missing cases (e.g. occlusion,etc.). To address these issues, we propose a $\textbf{L}$inguistic$\textbf{P}$erception $\textbf{V}$ision model (LPV), which explores thelinguistic capability of vision model for accurate text recognition. Toalleviate the LID problem, we introduce a Cascade Position Attention (CPA)mechanism that obtains high-quality and accurate attention maps throughstep-wise optimization and linguistic information mining. Furthermore, a GlobalLinguistic Reconstruction Module (GLRM) is proposed to improve therepresentation of visual features by perceiving the linguistic information inthe visual space, which gradually converts visual features into semanticallyrich ones during the cascade process. Different from previous methods, ourmethod obtains SOTA results while keeping low complexity (92.4% accuracy withonly 8.11M parameters). Code is available athttps://github.com/CyrilSterling/LPV.</description><author>Boqiang Zhang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Yongdong Zhang</author><pubDate>Wed, 10 May 2023 13:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05140v2</guid></item><item><title>Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification</title><link>http://arxiv.org/abs/2305.06110v1</link><description>This paper proposes a feedback mechanism to 'break bad habits' using thePavlok device. Pavlok utilises beeps, vibration and shocks as a mode ofaversion technique to help individuals with behaviour modification. While thedevice can be useful in certain periodic daily life situations, like alarms andexercise notifications, the device relies on manual operations that limit itsusage. To this end, we design a user interface to generate an automaticfeedback mechanism that integrates Pavlok and a deep learning based model todetect certain behaviours via an integrated user interface i.e. mobile ordesktop application. Our proposed solution is implemented and verified in thecontext of snoring, which first detects audio from the environment following aprediction of whether the audio content is a snore or not. Based on theprediction of the deep learning model, we use Pavlok to alert users forpreventive measures. We believe that this simple solution can help people tochange their atomic habits, which may lead to long-term benefits.</description><author>Shreya Ghosh, Rakibul Hasan, Pradyumna Agrawal, Zhixi Cai, Susannah Soon, Abhinav Dhall, Tom Gedeon</author><pubDate>Wed, 10 May 2023 13:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06110v1</guid></item><item><title>XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients</title><link>http://arxiv.org/abs/2305.06109v1</link><description>Heart attack remain one of the greatest contributors to mortality in theUnited States and globally. Patients admitted to the intensive care unit (ICU)with diagnosed heart attack (myocardial infarction or MI) are at higher risk ofdeath. In this study, we use two retrospective cohorts extracted from the eICUand MIMIC-IV databases, to develop a novel pseudo-dynamic machine learningframework for mortality prediction in the ICU with interpretability andclinical risk analysis. The method provides accurate prediction for ICUpatients up to 24 hours before the event and provide time-resolvedinterpretability results. The performance of the framework relying on extremegradient boosting was evaluated on a held-out test set from eICU, andexternally validated on the MIMIC-IV cohort using the most important featuresidentified by time-resolved Shapley values achieving AUCs of 91.0 (balancedaccuracy of 82.3) for 6-hour prediction of mortality respectively. We show thatour framework successfully leverages time-series physiological measurements bytranslating them into stacked static prediction problems to be robustlypredictive through time in the ICU stay and can offer clinical insight fromtime-resolved interpretability</description><author>Munib Mesinovic, Peter Watkinson, Tingting Zhu</author><pubDate>Wed, 10 May 2023 13:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06109v1</guid></item><item><title>Few-shot Link Prediction on N-ary Facts</title><link>http://arxiv.org/abs/2305.06104v1</link><description>N-ary facts composed of a primary triple (head entity, relation, tail entity)and an arbitrary number of auxiliary attribute-value pairs, are prevalent inreal-world knowledge graphs (KGs). Link prediction on n-ary facts is to predicta missing element in an n-ary fact. This helps populate and enrich KGs andfurther promotes numerous downstream applications. Previous studies usuallyrequire a substantial amount of high-quality data to understand the elements inn-ary facts. However, these studies overlook few-shot relations, which havelimited labeled instances, yet are common in real-world scenarios. Thus, thispaper introduces a new task, few-shot link prediction on n-ary facts. It aimsto predict a missing entity in an n-ary fact with limited labeled instances. Wefurther propose a model for Few-shot Link prEdict on N-ary facts, thus calledFLEN, which consists of three modules: the relation learning, support-specificadjusting, and query inference modules. FLEN captures relation meta informationfrom limited instances to predict a missing entity in a query instance. Tovalidate the effectiveness of FLEN, we construct three datasets based onexisting benchmark data. Our experimental results show that FLEN significantlyoutperforms existing related models in both few-shot link prediction on n-aryfacts and binary facts.</description><author>Jiyao Wei, Saiping Guan, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</author><pubDate>Wed, 10 May 2023 13:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06104v1</guid></item><item><title>Towards Better Graph Representation Learning with Parameterized Decomposition &amp; Filtering</title><link>http://arxiv.org/abs/2305.06102v1</link><description>Proposing an effective and flexible matrix to represent a graph is afundamental challenge that has been explored from multiple perspectives, e.g.,filtering in Graph Fourier Transforms. In this work, we develop a novel andgeneral framework which unifies many existing GNN models from the view ofparameterized decomposition and filtering, and show how it helps to enhance theflexibility of GNNs while alleviating the smoothness and amplification issuesof existing models. Essentially, we show that the extensively studied spectralgraph convolutions with learnable polynomial filters are constrained variantsof this formulation, and releasing these constraints enables our model toexpress the desired decomposition and filtering simultaneously. Based on thisgeneralized framework, we develop models that are simple in implementation butachieve significant improvements and computational efficiency on a variety ofgraph learning tasks. Code is available at https://github.com/qslim/PDF.</description><author>Mingqi Yang, Wenjie Feng, Yanming Shen, Bryan Hooi</author><pubDate>Wed, 10 May 2023 13:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06102v1</guid></item><item><title>PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information</title><link>http://arxiv.org/abs/2305.06099v1</link><description>The MultiCoNER II task aims to detect complex, ambiguous, and fine-grainednamed entities in low-context situations and noisy scenarios like the presenceof spelling mistakes and typos for multiple languages. The task posessignificant challenges due to the scarcity of contextual information, the highgranularity of the entities(up to 33 classes), and the interference of noisydata. To address these issues, our team {\bf PAI} proposes a universal NamedEntity Recognition (NER) system that integrates external entity information toimprove performance. Specifically, our system retrieves entities withproperties from the knowledge base (i.e. Wikipedia) for a given text, thenconcatenates entity information with the input sentence and feeds it intoTransformer-based models. Finally, our system wins 2 first places, 4 secondplaces, and 1 third place out of 13 tracks. The code is publicly available at\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.</description><author>Long Ma, Kai Lu, Tianbo Che, Hailong Huang, Weiguo Gao, Xuan Li</author><pubDate>Wed, 10 May 2023 13:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06099v1</guid></item><item><title>Extracting Cultural Commonsense Knowledge at Scale</title><link>http://arxiv.org/abs/2210.07763v3</link><description>Structured knowledge is important for many AI applications. Commonsenseknowledge, which is crucial for robust human-centric AI, is covered by a smallnumber of structured knowledge projects. However, they lack knowledge abouthuman traits and behaviors conditioned on socio-cultural contexts, which iscrucial for situative AI. This paper presents CANDLE, an end-to-end methodologyfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.CANDLE extracts CCSK assertions from a huge web corpus and organizes them intocoherent clusters, for 3 domains of subjects (geography, religion, occupation)and several cultural facets (food, drinks, clothing, traditions, rituals,behaviors). CANDLE includes judicious techniques for classification-basedfiltering and scoring of interestingness. Experimental evaluations show thesuperiority of the CANDLE CCSK collection over prior works, and an extrinsicuse case demonstrates the benefits of CCSK for the GPT-3 language model. Codeand data can be accessed at https://candle.mpi-inf.mpg.de/.</description><author>Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, Gerhard Weikum</author><pubDate>Wed, 10 May 2023 13:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07763v3</guid></item><item><title>SMAuC -- The Scientific Multi-Authorship Corpus</title><link>http://arxiv.org/abs/2211.02477v2</link><description>The rapidly growing volume of scientific publications offers an interestingchallenge for research on methods for analyzing the authorship of documentswith one or more authors. However, most existing datasets lack scientificdocuments or the necessary metadata for constructing new experiments and testcases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored toscientific authorship analysis. Comprising over 3 million publications acrossvarious disciplines from over 5 million authors, SMAuC is the largest openlyaccessible corpus for this purpose. It encompasses scientific texts fromhumanities and natural sciences, accompanied by extensive, curated metadata,including unambiguous author IDs. SMAuC aims to significantly advance thedomain of authorship analysis in scientific texts.</description><author>Janek Bevendorff, Philipp Sauer, Lukas Gienapp, Wolfgang Kircheis, Erik Körner, Benno Stein, Martin Potthast</author><pubDate>Wed, 10 May 2023 13:21:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02477v2</guid></item><item><title>Super Vision Transformer</title><link>http://arxiv.org/abs/2205.11397v4</link><description>We attempt to reduce the computational costs in vision transformers (ViTs),which increase quadratically in the token number. We present a novel trainingparadigm that trains only one ViT model at a time, but is capable of providingimproved image recognition performance with various computational costs. Here,the trained ViT model, termed super vision transformer (SuperViT), is empoweredwith the versatile ability to solve incoming patches of multiple sizes as wellas preserve informative tokens with multiple keeping rates (the ratio ofkeeping tokens) to achieve good hardware efficiency for inference, given thatthe available hardware resources often change from time to time. Experimentalresults on ImageNet demonstrate that our SuperViT can considerably reduce thecomputational costs of ViT models with even performance increase. For example,we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existingstudies on efficient vision transformers. For example, when consuming the sameamount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SOTA) EViTby 1.1% when using DeiT-S as their backbones. The project of this work is madepublicly available at https://github.com/lmbxmu/SuperViT.</description><author>Mingbao Lin, Mengzhao Chen, Yuxin Zhang, Chunhua Shen, Rongrong Ji, Liujuan Cao</author><pubDate>Wed, 10 May 2023 13:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11397v4</guid></item><item><title>XTab: Cross-table Pretraining for Tabular Transformers</title><link>http://arxiv.org/abs/2305.06090v1</link><description>The success of self-supervised learning in computer vision and naturallanguage processing has motivated pretraining methods on tabular data. However,most existing tabular self-supervised learning models fail to leverageinformation across multiple data tables and cannot generalize to new tables. Inthis work, we introduce XTab, a framework for cross-table pretraining oftabular transformers on datasets from various domains. We address the challengeof inconsistent column types and quantities among tables by utilizingindependent featurizers and using federated learning to pretrain the sharedcomponent. Tested on 84 tabular prediction tasks from the OpenML-AutoMLBenchmark (AMLB), we show that (1) XTab consistently boosts thegeneralizability, learning speed, and performance of multiple tabulartransformers, (2) by pretraining FT-Transformer via XTab, we achieve superiorperformance than other state-of-the-art tabular deep learning models on varioustasks such as regression, binary, and multiclass classification.</description><author>Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, Mahsa Shoaran</author><pubDate>Wed, 10 May 2023 13:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06090v1</guid></item><item><title>Building Interoperable Electronic Health Records as Purpose-Driven Knowledge Graphs</title><link>http://arxiv.org/abs/2305.06088v1</link><description>When building a new application we are increasingly confronted with the needof reusing and integrating pre-existing knowledge. Nevertheless, it is a factthat this prior knowledge is virtually impossible to reuse as-is. This is truealso in domains, e.g., eHealth, where a lot of effort has been put intodeveloping high-quality standards and reference ontologies, e.g. FHIR1. In thispaper, we propose an integrated methodology, called iTelos, which enables dataand knowledge reuse towards the construction of Interoperable Electronic HealthRecords (iEHR). The key intuition is that the data level and the schema levelof an application should be developed independently, thus allowing for maximumflexibility in the reuse of the prior knowledge, but under the overall guidanceof the needs to be satisfied, formalized as competence queries. This intuitionis implemented by codifying all the requirements, including those concerningreuse, as part of a purpose defined a priori, which is then used to drive amiddle-out development process where the application schema and data arecontinuously aligned. The proposed methodology is validated through itsapplication to a large-scale case study.</description><author>Simone Bocca, Alessio Zamboni, Gabor Bella, Yamini Chandrashekar, Mayukh Bagchi, Gabriel Kuper, Paolo Bouquet, Fausto Giunchiglia</author><pubDate>Wed, 10 May 2023 13:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06088v1</guid></item></channel></rss>