<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 09 Dec 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Scaling Laws of Synthetic Images for Model Training ... for Now</title><link>http://arxiv.org/abs/2312.04567v1</link><description>Recent significant advances in text-to-image models unlock the possibility oftraining vision systems using synthetic images, potentially overcoming thedifficulty of collecting curated data at scale. It is unclear, however, howthese models behave at scale, as more synthetic data is added to the trainingset. In this paper we study the scaling laws of synthetic images generated bystate of the art text-to-image models, for the training of supervised models:image classifiers with label supervision, and CLIP with language supervision.We identify several factors, including text prompts, classifier-free guidancescale, and types of text-to-image models, that significantly affect scalingbehavior. After tuning these factors, we observe that synthetic imagesdemonstrate a scaling trend similar to, but slightly less effective than, realimages in CLIP training, while they significantly underperform in scaling whentraining supervised image classifiers. Our analysis indicates that the mainreason for this underperformance is the inability of off-the-shelftext-to-image models to generate certain concepts, a limitation thatsignificantly impairs the training of image classifiers. Our findings alsosuggest that scaling synthetic data can be particularly effective in scenariossuch as: (1) when there is a limited supply of real images for a supervisedproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when theevaluation dataset diverges significantly from the training data, indicatingthe out-of-distribution scenario, or (3) when synthetic data is used inconjunction with real images, as demonstrated in the training of CLIP models.</description><author>Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian</author><pubDate>Thu, 07 Dec 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04567v1</guid></item><item><title>Gen2Det: Generate to Detect</title><link>http://arxiv.org/abs/2312.04566v1</link><description>Recently diffusion models have shown improvement in synthetic image qualityas well as better control in generation. We motivate and present Gen2Det, asimple modular pipeline to create synthetic training data for object detectionfor free by leveraging state-of-the-art grounded image generation methods.Unlike existing works which generate individual object instances, requireidentifying foreground followed by pasting on other images, we simplify todirectly generating scene-centric images. In addition to the synthetic data,Gen2Det also proposes a suite of techniques to best utilize the generated data,including image-level filtering, instance-level filtering, and better trainingrecipe to account for imperfections in the generation. Using Gen2Det, we showhealthy improvements on object detection and segmentation tasks under varioussettings and agnostic to detection methods. In the long-tailed detectionsetting on LVIS, Gen2Det improves the performance on rare categories by a largemargin while also significantly improving the performance on other categories,e.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just trainingon real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO,Gen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. Inthe most general detection setting, Gen2Det still demonstrates robustperformance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and0.32 points.</description><author>Saksham Suri, Fanyi Xiao, Animesh Sinha, Sean Chang Culatana, Raghuraman Krishnamoorthi, Chenchen Zhu, Abhinav Shrivastava</author><pubDate>Thu, 07 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04566v1</guid></item><item><title>MuRF: Multi-Baseline Radiance Fields</title><link>http://arxiv.org/abs/2312.04565v1</link><description>We present Multi-Baseline Radiance Fields (MuRF), a general feed-forwardapproach to solving sparse view synthesis under multiple different baselinesettings (small and large baselines, and different number of input views). Torender a target novel view, we discretize the 3D space into planes parallel tothe target image plane, and accordingly construct a target view frustum volume.Such a target volume representation is spatially aligned with the target view,which effectively aggregates relevant information from the input views forhigh-quality rendering. It also facilitates subsequent radiance fieldregression with a convolutional network thanks to its axis-aligned nature. The3D context modeled by the convolutional network enables our method to synthesissharper scene structures than prior works. Our MuRF achieves state-of-the-artperformance across multiple different baseline settings and diverse scenariosranging from simple objects (DTU) to complex indoor and outdoor scenes(RealEstate10K and LLFF). We also show promising zero-shot generalizationabilities on the Mip-NeRF 360 dataset, demonstrating the general applicabilityof MuRF.</description><author>Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, Fisher Yu</author><pubDate>Thu, 07 Dec 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04565v1</guid></item><item><title>EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS</title><link>http://arxiv.org/abs/2312.04564v1</link><description>Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-viewscene synthesis. It addresses the challenges of lengthy training times and slowrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,differentiable rasterization of 3D Gaussians, 3D-GS achieves real-timerendering and accelerated training. They, however, demand substantial memoryresources for both training and storage, as they require millions of Gaussiansin their point cloud representation for each scene. We present a techniqueutilizing quantized embeddings to significantly reduce memory storagerequirements and a coarse-to-fine training strategy for a faster and morestable optimization of the Gaussian point clouds. Our approach results in scenerepresentations with fewer Gaussians and quantized representations, leading tofaster training times and rendering speeds for real-time rendering of highresolution scenes. We reduce memory by more than an order of magnitude allwhile maintaining the reconstruction quality. We validate the effectiveness ofour approach on a variety of datasets and scenes preserving the visual qualitywhile consuming 10-20x less memory and faster training/inference speed. Projectpage and code is available https://efficientgaussian.github.io</description><author>Sharath Girish, Kamal Gupta, Abhinav Shrivastava</author><pubDate>Thu, 07 Dec 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04564v1</guid></item><item><title>Visual Geometry Grounded Deep Structure From Motion</title><link>http://arxiv.org/abs/2312.04563v1</link><description>Structure-from-motion (SfM) is a long-standing problem in the computer visioncommunity, which aims to reconstruct the camera poses and 3D structure of ascene from a set of unconstrained 2D images. Classical frameworks solve thisproblem in an incremental manner by detecting and matching keypoints,registering images, triangulating 3D points, and conducting bundle adjustment.Recent research efforts have predominantly revolved around harnessing the powerof deep learning techniques to enhance specific elements (e.g., keypointmatching), but are still based on the original, non-differentiable pipeline.Instead, we propose a new deep pipeline VGGSfM, where each component is fullydifferentiable and thus can be trained in an end-to-end manner. To this end, weintroduce new mechanisms and simplifications. First, we build on recentadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,which eliminates the need for chaining pairwise matches. Furthermore, werecover all cameras simultaneously based on the image and track featuresinstead of gradually registering cameras. Finally, we optimise the cameras andtriangulate 3D points via a differentiable bundle adjustment layer. We attainstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,and ETH3D.</description><author>Jianyuan Wang, Nikita Karaev, Christian Rupprecht, David Novotny</author><pubDate>Thu, 07 Dec 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04563v1</guid></item><item><title>NeRFiller: Completing Scenes via Generative 3D Inpainting</title><link>http://arxiv.org/abs/2312.04560v1</link><description>We propose NeRFiller, an approach that completes missing portions of a 3Dcapture via generative 3D inpainting using off-the-shelf 2D visual generativemodels. Often parts of a captured 3D scene or object are missing due to meshreconstruction failures or a lack of observations (e.g., contact regions, suchas the bottom of objects, or hard-to-reach areas). We approach this challenging3D inpainting problem by leveraging a 2D inpainting diffusion model. Weidentify a surprising behavior of these models, where they generate more 3Dconsistent inpaints when images form a 2$\times$2 grid, and show how togeneralize this behavior to more than four images. We then present an iterativeframework to distill these inpainted regions into a single consistent 3D scene.In contrast to related works, we focus on completing scenes rather thandeleting foreground objects, and our approach does not require tight 2D objectmasks or text. We compare our approach to relevant baselines adapted to oursetting on a variety of scenes, where NeRFiller creates the most 3D consistentand plausible scene completions. Our project page is athttps://ethanweber.me/nerfiller.</description><author>Ethan Weber, Aleksander Hołyński, Varun Jampani, Saurabh Saxena, Noah Snavely, Abhishek Kar, Angjoo Kanazawa</author><pubDate>Thu, 07 Dec 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04560v1</guid></item><item><title>GenDeF: Learning Generative Deformation Field for Video Generation</title><link>http://arxiv.org/abs/2312.04561v1</link><description>We offer a new perspective on approaching the task of video generation.Instead of directly synthesizing a sequence of frames, we propose to render avideo by warping one static image with a generative deformation field (GenDeF).Such a pipeline enjoys three appealing advantages. First, we can sufficientlyreuse a well-trained image generator to synthesize the static image (alsocalled canonical image), alleviating the difficulty in producing a video andthereby resulting in better visual quality. Second, we can easily convert adeformation field to optical flows, making it possible to apply explicitstructural regularizations for motion modeling, leading to temporallyconsistent results. Third, the disentanglement between content and motionallows users to process a synthesized video through processing itscorresponding static image without any tuning, facilitating many applicationslike video editing, keypoint tracking, and video segmentation. Both qualitativeand quantitative results on three common video generation benchmarksdemonstrate the superiority of our GenDeF method.</description><author>Wen Wang, Kecheng Zheng, Qiuyu Wang, Hao Chen, Zifan Shi, Ceyuan Yang, Yujun Shen, Chunhua Shen</author><pubDate>Thu, 07 Dec 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04561v1</guid></item><item><title>PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation</title><link>http://arxiv.org/abs/2312.04559v1</link><description>We present PrimDiffusion, the first diffusion-based framework for 3D humangeneration. Devising diffusion models for 3D human generation is difficult dueto the intensive computational cost of 3D representations and the articulatedtopology of 3D humans. To tackle these challenges, our key insight is operatingthe denoising diffusion process directly on a set of volumetric primitives,which models the human body as a number of small volumes with radiance andkinematic information. This volumetric primitives representation marries thecapacity of volumetric representations with the efficiency of primitive-basedrendering. Our PrimDiffusion framework has three appealing properties: 1)compact and expressive parameter space for the diffusion model, 2) flexible 3Drepresentation that incorporates human prior, and 3) decoder-free rendering forefficient novel-view and novel-pose synthesis. Extensive experiments validatethat PrimDiffusion outperforms state-of-the-art methods in 3D human generation.Notably, compared to GAN-based methods, our PrimDiffusion supports real-timerendering of high-quality 3D humans at a resolution of $512\times512$ once thedenoising process is done. We also demonstrate the flexibility of our frameworkon training-free conditional generation such as texture transfer and 3Dinpainting.</description><author>Zhaoxi Chen, Fangzhou Hong, Haiyi Mei, Guangcong Wang, Lei Yang, Ziwei Liu</author><pubDate>Thu, 07 Dec 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04559v1</guid></item><item><title>MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar</title><link>http://arxiv.org/abs/2312.04558v1</link><description>The ability to animate photo-realistic head avatars reconstructed frommonocular portrait video sequences represents a crucial step in bridging thegap between the virtual and real worlds. Recent advancements in head avatartechniques, including explicit 3D morphable meshes (3DMM), point clouds, andneural implicit representation have been exploited for this ongoing research.However, 3DMM-based methods are constrained by their fixed topologies,point-based approaches suffer from a heavy training burden due to the extensivequantity of points involved, and the last ones suffer from limitations indeformation flexibility and rendering efficiency. In response to thesechallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based HeadAvatar), a novel approach that harnesses 3D Gaussian point representationcoupled with a Gaussian deformation field to learn explicit head avatars frommonocular portrait videos. We define our head avatars with Gaussian pointscharacterized by adaptable shapes, enabling flexible topology. These pointsexhibit movement with a Gaussian deformation field in alignment with the targetpose and expression of a person, facilitating efficient deformation.Additionally, the Gaussian points have controllable shape, size, color, andopacity combined with Gaussian splatting, allowing for efficient training andrendering. Experiments demonstrate the superior performance of our method,which achieves state-of-the-art results among previous methods.</description><author>Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu</author><pubDate>Thu, 07 Dec 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04558v1</guid></item><item><title>GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation</title><link>http://arxiv.org/abs/2312.04557v1</link><description>In this study, we explore Transformer-based diffusion models for image andvideo generation. Despite the dominance of Transformer architectures in variousfields due to their flexibility and scalability, the visual generative domainprimarily utilizes CNN-based U-Net architectures, particularly indiffusion-based models. We introduce GenTron, a family of Generative modelsemploying Transformer-based diffusion, to address this gap. Our initial stepwas to adapt Diffusion Transformers (DiTs) from class to text conditioning, aprocess involving thorough empirical exploration of the conditioning mechanism.We then scale GenTron from approximately 900M to over 3B parameters, observingsignificant improvements in visual quality. Furthermore, we extend GenTron totext-to-video generation, incorporating novel motion-free guidance to enhancevideo quality. In human evaluations against SDXL, GenTron achieves a 51.1% winrate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in textalignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,underscoring its strengths in compositional generation. We believe this workwill provide meaningful insights and serve as a valuable reference for futureresearch.</description><author>Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, Juan-Manuel Perez-Rua</author><pubDate>Thu, 07 Dec 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04557v1</guid></item><item><title>Large Language Models for Mathematicians</title><link>http://arxiv.org/abs/2312.04556v1</link><description>Large language models (LLMs) such as ChatGPT have received immense interestfor their general-purpose language understanding and, in particular, theirability to generate high-quality text or computer code. For many professions,LLMs represent an invaluable tool that can speed up and improve the quality ofwork. In this note, we discuss to what extent they can aid professionalmathematicians. We first provide a mathematical description of the transformermodel used in all modern language models. Based on recent studies, we thenoutline best practices and potential issues and report on the mathematicalabilities of language models. Finally, we shed light on the potential of LMMsto change how mathematicians work.</description><author>Simon Frieder, Julius Berner, Philipp Petersen, Thomas Lukasiewicz</author><pubDate>Thu, 07 Dec 2023 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04556v1</guid></item><item><title>Improved Visual Grounding through Self-Consistent Explanations</title><link>http://arxiv.org/abs/2312.04554v1</link><description>Vision-and-language models trained to match images with text can be combinedwith visual explanation methods to point to the locations of specific objectsin an image. Our work shows that the localization --"grounding"-- abilities ofthese models can be further improved by finetuning for self-consistent visualexplanations. We propose a strategy for augmenting existing text-image datasetswith paraphrases using a large language model, and SelfEQ, a weakly-supervisedstrategy on visual explanation maps for paraphrases that encouragesself-consistency. Specifically, for an input textual phrase, we attempt togenerate a paraphrase and finetune the model so that the phrase and paraphrasemap to the same region in the image. We posit that this both expands thevocabulary that the model is able to handle, and improves the quality of theobject locations highlighted by gradient-based visual explanation methods (e.g.GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,ReferIt, and RefCOCO+ over a strong baseline method and several prior works.Particularly, comparing to other methods that do not use any type of boxannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% onRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% onaverage).</description><author>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez</author><pubDate>Thu, 07 Dec 2023 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04554v1</guid></item><item><title>SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing</title><link>http://arxiv.org/abs/2312.04553v1</link><description>Can we capture shape and reflectance in stealth? Such capability would bevaluable for many application domains in vision, xR, robotics, and HCI. Weintroduce Structured Polarization, the first depth and reflectance sensingmethod using patterns of polarized light (SPIDeRS). The key idea is to modulatethe angle of linear polarization (AoLP) of projected light at each pixel. Theuse of polarization makes it invisible and lets us recover not only depth butalso directly surface normals and even reflectance. We implement SPIDeRS with aliquid crystal spatial light modulator (SLM) and a polarimetric camera. Wederive a novel method for robustly extracting the projected structuredpolarization pattern from the polarimetric object appearance. We evaluate theeffectiveness of SPIDeRS by applying it to a number of real-world objects. Theresults show that our method successfully reconstructs object shapes of variousmaterials and is robust to diffuse reflection and ambient light. We alsodemonstrate relighting using recovered surface normals and reflectance. Webelieve SPIDeRS opens a new avenue of polarization use in visual sensing.</description><author>Tomoki Ichikawa, Shohei Nobuhara, Ko Nishino</author><pubDate>Thu, 07 Dec 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04553v1</guid></item><item><title>Generating Illustrated Instructions</title><link>http://arxiv.org/abs/2312.04552v1</link><description>We introduce the new task of generating Illustrated Instructions, i.e.,visual instructions customized to a user's needs. We identify desiderata uniqueto this task, and formalize it through a suite of automatic and humanevaluation metrics, designed to measure the validity, consistency, and efficacyof the generations. We combine the power of large language models (LLMs)together with strong text-to-image generation diffusion models to propose asimple approach called StackedDiffusion, which generates such illustratedinstructions given text as input. The resulting model strongly outperformsbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,users even prefer it to human-generated articles. Most notably, it enablesvarious new and exciting applications far beyond what static articles on theweb can provide, such as personalized instructions complete with intermediatesteps and pictures in response to a user's individual situation.</description><author>Sachit Menon, Ishan Misra, Rohit Girdhar</author><pubDate>Thu, 07 Dec 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04552v1</guid></item><item><title>Free3D: Consistent Novel View Synthesis without 3D Representation</title><link>http://arxiv.org/abs/2312.04551v1</link><description>We introduce Free3D, a simple approach designed for open-set novel viewsynthesis (NVS) from a single image. Similar to Zero-1-to-3, we start from apre-trained 2D image generator for generalization, and fine-tune it for NVS.Compared to recent and concurrent works, we obtain significant improvementswithout resorting to an explicit 3D representation, which is slow andmemory-consuming or training an additional 3D network. We do so by encodingbetter the target camera pose via a new per-pixel ray conditioningnormalization (RCN) layer. The latter injects pose information in theunderlying 2D image generator by telling each pixel its specific viewingdirection. We also improve multi-view consistency via a light-weight multi-viewattention layer and multi-view noise sharing. We train Free3D on the Objaversedataset and demonstrate excellent generalization to various new categories inseveral new datasets, including OminiObject3D and GSO. We hope our simple andeffective approach will serve as a solid baseline and help future research inNVS with more accuracy pose. The project page is available athttps://chuanxiaz.com/free3d/.</description><author>Chuanxia Zheng, Andrea Vedaldi</author><pubDate>Thu, 07 Dec 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04551v1</guid></item><item><title>PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play</title><link>http://arxiv.org/abs/2312.04549v1</link><description>Learning from unstructured and uncurated data has become the dominantparadigm for generative approaches in language and vision. Such unstructuredand unguided behavior data, commonly known as play, is also easier to collectin robotics but much more difficult to learn from due to its inherentlymultimodal, noisy, and suboptimal nature. In this paper, we study this problemof learning goal-directed skill policies from unstructured play data which islabeled with language in hindsight. Specifically, we leverage advances indiffusion models to learn a multi-task diffusion model to extract roboticskills from play data. Using a conditional denoising diffusion process in thespace of states and actions, we can gracefully handle the complexity andmultimodality of play data and generate diverse and interesting robotbehaviors. To make diffusion models more useful for skill learning, weencourage robotic agents to acquire a vocabulary of skills by introducingdiscrete bottlenecks into the conditional behavior generation process. In ourexperiments, we demonstrate the effectiveness of our approach across a widevariety of environments in both simulation and the real world. Resultsvisualizations and videos at https://play-fusion.github.io</description><author>Lili Chen, Shikhar Bahl, Deepak Pathak</author><pubDate>Thu, 07 Dec 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04549v1</guid></item><item><title>Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?</title><link>http://arxiv.org/abs/2312.04548v1</link><description>Despite the commercial abundance of UAVs, aerial data acquisition remainschallenging, and the existing Asia and North America-centric open-source UAVdatasets are small-scale or low-resolution and lack diversity in scenecontextuality. Additionally, the color content of the scenes, solar-zenithangle, and population density of different geographies influence the datadiversity. These two factors conjointly render suboptimal aerial-visualperception of the deep neural network (DNN) models trained primarily on theground-view data, including the open-world foundational models. To pave the way for a transformative era of aerial detection, we presentMultiview Aerial Visual RECognition or MAVREC, a video dataset where we recordsynchronized scenes from different perspectives -- ground camera anddrone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard2.7K resolution video sequences, more than 0.5 million frames, and 1.1 millionannotated bounding boxes. This makes MAVREC the largest ground and aerial-viewdataset, and the fourth largest among all drone-based datasets across allmodalities and tasks. Through our extensive benchmarking on MAVREC, werecognize that augmenting object detectors with ground-view images from thecorresponding geographical location is a superior pre-training strategy foraerial detection. Building on this strategy, we benchmark MAVREC with acurriculum-based semi-supervised object detection approach that leverageslabeled (ground and aerial) and unlabeled (only aerial) images to enhance theaerial detection. We publicly release the MAVREC dataset:https://mavrec.github.io.</description><author>Aritra Dutta, Srijan Das, Jacob Nielsen, Rajatsubhra Chakraborty, Mubarak Shah</author><pubDate>Thu, 07 Dec 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04548v1</guid></item><item><title>Digital Life Project: Autonomous 3D Characters with Social Intelligence</title><link>http://arxiv.org/abs/2312.04547v1</link><description>In this work, we present Digital Life Project, a framework utilizing languageas the universal medium to build autonomous 3D characters, who are capable ofengaging in social interactions and expressing with articulated body motions,thereby simulating life in a digital environment. Our framework comprises twoprimary components: 1) SocioMind: a meticulously crafted digital brain thatmodels personalities with systematic few-shot exemplars, incorporates areflection process based on psychology principles, and emulates autonomy byinitiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesisparadigm for controlling the character's digital body. It integrates motionmatching, a proven industry technique to ensure motion quality, withcutting-edge advancements in motion generation for diversity. Extensiveexperiments demonstrate that each module achieves state-of-the-art performancein its respective domain. Collectively, they enable virtual characters toinitiate and sustain dialogues autonomously, while evolving theirsocio-psychological states. Concurrently, these characters can performcontextually relevant bodily movements. Additionally, a motion captioningmodule further allows the virtual character to recognize and appropriatelyrespond to human players' actions. Homepage: https://digital-life-project.com/</description><author>Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Xiangyu Fan, Han Du, Liang Pan, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, Ziwei Liu</author><pubDate>Thu, 07 Dec 2023 18:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04547v1</guid></item><item><title>Adversarial Learning for Feature Shift Detection and Correction</title><link>http://arxiv.org/abs/2312.04546v1</link><description>Data shift is a phenomenon present in many real-world applications, and whilethere are multiple methods attempting to detect shifts, the task of localizingand correcting the features originating such shifts has not been studied indepth. Feature shifts can occur in many datasets, including in multi-sensordata, where some sensors are malfunctioning, or in tabular and structured data,including biomedical, financial, and survey data, where faulty standardizationand data processing pipelines can lead to erroneous features. In this work, weexplore using the principles of adversarial learning, where the informationfrom several discriminators trained to distinguish between two distributions isused to both detect the corrupted features and fix them in order to remove thedistribution shift between datasets. We show that mainstream supervisedclassifiers, such as random forest or gradient boosting trees, combined withsimple iterative heuristics, can localize and correct feature shifts,outperforming current statistical and neural network-based techniques. The codeis available at https://github.com/AI-sandbox/DataFix.</description><author>Miriam Barrabes, Daniel Mas Montserrat, Margarita Geleta, Xavier Giro-i-Nieto, Alexander G. Ioannidis</author><pubDate>Thu, 07 Dec 2023 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04546v1</guid></item><item><title>SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</title><link>http://arxiv.org/abs/2302.01330v3</link><description>In this work, we present SceneDreamer, an unconditional generative model forunbounded 3D scenes, which synthesizes large-scale 3D landscapes from randomnoise. Our framework is learned from in-the-wild 2D image collections only,without any 3D annotations. At the core of SceneDreamer is a principledlearning paradigm comprising 1) an efficient yet expressive 3D scenerepresentation, 2) a generative scene parameterization, and 3) an effectiverenderer that can leverage the knowledge from 2D images. Our approach beginswith an efficient bird's-eye-view (BEV) representation generated from simplexnoise, which includes a height field for surface elevation and a semantic fieldfor detailed scene semantics. This BEV scene representation enables 1)representing a 3D scene with quadratic complexity, 2) disentangled geometry andsemantics, and 3) efficient training. Moreover, we propose a novel generativeneural hash grid to parameterize the latent space based on 3D positions andscene semantics, aiming to encode generalizable features across various scenes.Lastly, a neural volumetric renderer, learned from 2D image collections throughadversarial training, is employed to produce photorealistic images. Extensiveexperiments demonstrate the effectiveness of SceneDreamer and superiority overstate-of-the-art methods in generating vivid yet diverse unbounded 3D worlds.</description><author>Zhaoxi Chen, Guangcong Wang, Ziwei Liu</author><pubDate>Thu, 07 Dec 2023 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01330v3</guid></item><item><title>HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image</title><link>http://arxiv.org/abs/2312.04543v1</link><description>3D content creation from a single image is a long-standing yet highlydesirable task. Recent advances introduce 2D diffusion priors, yieldingreasonable results. However, existing methods are not hyper-realistic enoughfor post-generation usage, as users cannot view, render and edit the resulting3D content from a full range. To address these challenges, we introduceHyperDreamer with several key designs and appealing properties: 1) Viewable:360 degree mesh modeling with high-resolution textures enables the creation ofvisually compelling 3D models from a full range of observation points. 2)Renderable: Fine-grained semantic segmentation and data-driven priors areincorporated as guidance to learn reasonable albedo, roughness, and specularproperties of the materials, enabling semantic-aware arbitrary materialestimation. 3) Editable: For a generated model or their own data, users caninteractively select any region via a few clicks and efficiently edit thetexture with text-based guidance. Extensive experiments demonstrate theeffectiveness of HyperDreamer in modeling region-aware materials withhigh-resolution textures and enabling user-friendly editing. We believe thatHyperDreamer holds promise for advancing 3D content creation and findingapplications in various domains.</description><author>Tong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xinggang Pan, Jiaqi Wang, Dahua Lin, Ziwei Liu</author><pubDate>Thu, 07 Dec 2023 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04543v1</guid></item><item><title>SoK: Unintended Interactions among Machine Learning Defenses and Risks</title><link>http://arxiv.org/abs/2312.04542v1</link><description>Machine learning (ML) models cannot neglect risks to security, privacy, andfairness. Several defenses have been proposed to mitigate such risks. When adefense is effective in mitigating one risk, it may correspond to increased ordecreased susceptibility to other risks. Existing research lacks an effectiveframework to recognize and explain these unintended interactions. We presentsuch a framework, based on the conjecture that overfitting and memorizationunderlie unintended interactions. We survey existing literature on unintendedinteractions, accommodating them within our framework. We use our framework toconjecture on two previously unexplored interactions, and empirically validateour conjectures.</description><author>Vasisht Duddu, Sebastian Szyller, N. Asokan</author><pubDate>Thu, 07 Dec 2023 18:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04542v1</guid></item><item><title>Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations</title><link>http://arxiv.org/abs/2312.04540v1</link><description>Modeling spatial-temporal interactions among neighboring agents is at theheart of multi-agent problems such as motion forecasting and crowd navigation.Despite notable progress, it remains unclear to which extent modernrepresentations can capture the causal relationships behind agent interactions.In this work, we take an in-depth look at the causal awareness of theserepresentations, from computational formalism to real-world practice. First, wecast doubt on the notion of non-causal robustness studied in the recentCausalAgents benchmark. We show that recent representations are alreadypartially resilient to perturbations of non-causal agents, and yet modelingindirect causal effects involving mediator agents remains challenging. Toaddress this challenge, we introduce a metric learning approach thatregularizes latent representations with causal annotations. Our controlledexperiments show that this approach not only leads to higher degrees of causalawareness but also yields stronger out-of-distribution robustness. To furtheroperationalize it in practice, we propose a sim-to-real causal transfer methodvia cross-domain multi-task learning. Experiments on pedestrian datasets showthat our method can substantially boost generalization, even in the absence ofreal-world causal annotations. We hope our work provides a new perspective onthe challenges and potential pathways towards causally-aware representations ofmulti-agent interactions. Our code is available athttps://github.com/socialcausality.</description><author>Yuejiang Liu, Ahmad Rahimi, Po-Chien Luan, Frano Rajič, Alexandre Alahi</author><pubDate>Thu, 07 Dec 2023 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04540v1</guid></item><item><title>Self-Guided Open-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04539v1</link><description>Vision-Language Models (VLMs) have emerged as promising tools for open-endedimage understanding tasks, including open vocabulary segmentation. Yet, directapplication of such VLMs to segmentation is non-trivial, since VLMs are trainedwith image-text pairs and naturally lack pixel-level granularity. Recent workshave made advancements in bridging this gap, often by leveraging the sharedimage-text space in which the image and a provided text prompt are represented.In this paper, we challenge the capabilities of VLMs further and tackleopen-vocabulary segmentation without the need for any textual input. To thisend, we propose a novel Self-Guided Semantic Segmentation (Self-Seg) framework.Self-Seg is capable of automatically detecting relevant class names fromclustered BLIP embeddings and using these for accurate semantic segmentation.In addition, we propose an LLM-based Open-Vocabulary Evaluator (LOVE) toeffectively assess predicted open-vocabulary class names. We achievestate-of-the-art results on Pascal VOC, ADE20K and CityScapes foropen-vocabulary segmentation without given class names, as well as competitiveperformance with methods where class names are given. All code and data will bereleased.</description><author>Osman Ülger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald</author><pubDate>Thu, 07 Dec 2023 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04539v1</guid></item><item><title>Trajeglish: Learning the Language of Driving Scenarios</title><link>http://arxiv.org/abs/2312.04535v1</link><description>A longstanding challenge for self-driving development is simulating dynamicdriving scenarios seeded from recorded driving logs. In pursuit of thisfunctionality, we apply tools from discrete sequence modeling to model howvehicles, pedestrians and cyclists interact in driving scenarios. Using asimple data-driven tokenization scheme, we discretize trajectories tocentimeter-level resolution using a small vocabulary. We then model themulti-agent sequence of motion tokens with a GPT-like encoder-decoder that isautoregressive in time and takes into account intra-timestep interactionbetween agents. Scenarios sampled from our model exhibit state-of-the-artrealism; our model tops the Waymo Sim Agents Benchmark, surpassing prior workalong the realism meta metric by 3.3% and along the interaction metric by 9.9%.We ablate our modeling choices in full autonomy and partial autonomy settings,and show that the representations learned by our model can quickly be adaptedto improve performance on nuScenes. We additionally evaluate the scalability ofour model with respect to parameter count and dataset size, and use densityestimates from our model to quantify the saliency of context length andintra-timestep interaction for the traffic modeling task.</description><author>Jonah Philion, Xue Bin Peng, Sanja Fidler</author><pubDate>Thu, 07 Dec 2023 18:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04535v1</guid></item><item><title>PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns</title><link>http://arxiv.org/abs/2312.04534v1</link><description>In this paper, we propose a novel virtual try-on from unconstrained designs(ucVTON) task to enable photorealistic synthesis of personalized compositeclothing on input human images. Unlike prior arts constrained by specific inputtypes, our method allows flexible specification of style (text or image) andtexture (full garment, cropped sections, or texture patches) conditions. Toaddress the entanglement challenge when using full garment images asconditions, we develop a two-stage pipeline with explicit disentanglement ofstyle and texture. In the first stage, we generate a human parsing mapreflecting the desired style conditioned on the input. In the second stage, wecomposite textures onto the parsing map areas based on the texture input. Torepresent complex and non-stationary textures that have never been achieved inprevious fashion editing works, we first propose extracting hierarchical andbalanced CLIP features and applying position encoding in VTON. Experimentsdemonstrate superior synthesis quality and personalization enabled by ourmethod. The flexible control over style and texture mixing brings virtualtry-on to a new level of user experience for online shopping and fashiondesign.</description><author>Shuliang Ning, Duomin Wang, Yipeng Qin, Zirong Jin, Baoyuan Wang, Xiaoguang Han</author><pubDate>Thu, 07 Dec 2023 18:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04534v1</guid></item><item><title>Plotting Behind the Scenes: Towards Learnable Game Engines</title><link>http://arxiv.org/abs/2303.13472v2</link><description>Neural video game simulators emerged as powerful tools to generate and editvideos. Their idea is to represent games as the evolution of an environment'sstate driven by the actions of its agents. While such a paradigm enables usersto play a game action-by-action, its rigidity precludes more semantic forms ofcontrol. To overcome this limitation, we augment game models with promptsspecified as a set of natural language actions and desired states. The result-aPromptable Game Model (PGM)-makes it possible for a user to play the game byprompting it with high- and low-level action sequences. Most captivatingly, ourPGM unlocks the director's mode, where the game is played by specifying goalsfor the agents in the form of a prompt. This requires learning "game AI",encapsulated by our animation model, to navigate the scene using high-levelconstraints, play against an adversary, and devise a strategy to win a point.To render the resulting state, we use a compositional NeRF representationencapsulated in our synthesis model. To foster future research, we presentnewly collected, annotated and calibrated Tennis and Minecraft datasets. Ourmethod significantly outperforms existing neural video game simulators in termsof rendering quality and unlocks applications beyond the capabilities of thecurrent state of the art. Our framework, data, and models are available athttps://snap-research.github.io/promptable-game-models/.</description><author>Willi Menapace, Aliaksandr Siarohin, Stéphane Lathuilière, Panos Achlioptas, Vladislav Golyanik, Sergey Tulyakov, Elisa Ricci</author><pubDate>Thu, 07 Dec 2023 18:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13472v2</guid></item><item><title>Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models</title><link>http://arxiv.org/abs/2312.04533v1</link><description>We introduce Dream2Real, a robotics framework which integratesvision-language models (VLMs) trained on 2D data into a 3D object rearrangementpipeline. This is achieved by the robot autonomously constructing a 3Drepresentation of the scene, where objects can be rearranged virtually and animage of the resulting arrangement rendered. These renders are evaluated by aVLM, so that the arrangement which best satisfies the user instruction isselected and recreated in the real world with pick-and-place. This enableslanguage-conditioned rearrangement to be performed zero-shot, without needingto collect a training dataset of example arrangements. Results on a series ofreal-world tasks show that this framework is robust to distractors,controllable by language, capable of understanding complex multi-objectrelations, and readily applicable to both tabletop and 6-DoF rearrangementtasks.</description><author>Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, Edward Johns</author><pubDate>Thu, 07 Dec 2023 18:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04533v1</guid></item><item><title>Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware Road-Scene Depth Estimation</title><link>http://arxiv.org/abs/2312.04530v1</link><description>Monocular depth estimators either require explicit scale supervision throughauxiliary sensors or suffer from scale ambiguity, which renders them difficultto deploy in downstream applications. A possible source of scale is the sizesof objects found in the scene, but inaccurate localization makes them difficultto exploit. In this paper, we introduce a novel scale-aware monocular depthestimation method called StableCamH that does not require any auxiliary sensoror supervision. The key idea is to exploit prior knowledge of object heights inthe scene but aggregate the height cues into a single invariant measure commonto all frames in a road video sequence, namely the camera height. Byformulating monocular depth estimation as camera height optimization, weachieve robust and accurate unsupervised end-to-end training. To realizeStableCamH, we devise a novel learning-based size prior that can directlyconvert car appearance into its dimensions. Extensive experiments on KITTI andCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracycompared with related methods, and its generalizability. The training frameworkof StableCamH can be used for any monocular depth estimation method and willhopefully become a fundamental building block for further work.</description><author>Genki Kinoshita, Ko Nishino</author><pubDate>Thu, 07 Dec 2023 18:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04530v1</guid></item><item><title>Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance</title><link>http://arxiv.org/abs/2312.04529v1</link><description>Reflectance bounds the frequency spectrum of illumination in the objectappearance. In this paper, we introduce the first stochastic inverse renderingmethod, which recovers the full frequency spectrum of an illumination jointlywith the object reflectance from a single image. Our key idea is to solve thisblind inverse problem in the reflectance map, an appearance representationinvariant to the underlying geometry, by learning to reverse the imageformation with a novel diffusion model which we refer to as the DiffusionReflectance Map Network (DRMNet). Given an observed reflectance map convertedand completed from the single input image, DRMNet generates a reflectance mapcorresponding to a perfect mirror sphere while jointly estimating thereflectance. The forward process can be understood as gradually filtering anatural illumination with lower and lower frequency reflectance and additiveGaussian noise. DRMNet learns to invert this process with two subnetworks,IllNet and RefNet, which work in concert towards this joint estimation. Thenetwork is trained on an extensive synthetic dataset and is demonstrated togeneralize to real images, showing state-of-the-art accuracy on establisheddatasets.</description><author>Yuto Enyo, Ko Nishino</author><pubDate>Thu, 07 Dec 2023 18:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04529v1</guid></item><item><title>Evaluation of Active Feature Acquisition Methods for Time-varying Feature Settings</title><link>http://arxiv.org/abs/2312.01530v2</link><description>Machine learning methods often assume input features are available at nocost. However, in domains like healthcare, where acquiring features could beexpensive or harmful, it is necessary to balance a feature's acquisition costagainst its predictive value. The task of training an AI agent to decide whichfeatures to acquire is called active feature acquisition (AFA). By deploying anAFA agent, we effectively alter the acquisition strategy and trigger adistribution shift. To safely deploy AFA agents under this distribution shift,we present the problem of active feature acquisition performance evaluation(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, statingthat acquisitions don't affect the underlying feature values; and ii) a nounobserved confounding (NUC) assumption, stating that retrospective featureacquisition decisions were only based on observed features. We show that onecan apply offline reinforcement learning under the NUC assumption and missingdata methods under the NDE assumption. When NUC and NDE hold, we propose anovel semi-offline reinforcement learning framework, which requires a weakerpositivity assumption and yields more data-efficient estimators. We introducethree novel estimators: a direct method (DM), an inverse probability weighting(IPW), and a double reinforcement learning (DRL) estimator.</description><author>Henrik von Kleist, Alireza Zamanian, Ilya Shpitser, Narges Ahmidi</author><pubDate>Thu, 07 Dec 2023 18:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01530v2</guid></item><item><title>Using Large Language Models for Hyperparameter Optimization</title><link>http://arxiv.org/abs/2312.04528v1</link><description>This paper studies using foundational large language models (LLMs) to makedecisions during hyperparameter optimization (HPO). Empirical evaluationsdemonstrate that in settings with constrained search budgets, LLMs can performcomparably or better than traditional HPO methods like random search andBayesian optimization on standard benchmarks. Furthermore, we propose to treatthe code specifying our model as a hyperparameter, which the LLM outputs, goingbeyond the capabilities of existing HPO approaches. Our findings suggest thatLLMs are a promising tool for improving efficiency in the traditionaldecision-making problem of hyperparameter optimization.</description><author>Michael R. Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, Jimmy Ba</author><pubDate>Thu, 07 Dec 2023 18:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04528v1</guid></item><item><title>Correspondences of the Third Kind: Camera Pose Estimation from Object Reflection</title><link>http://arxiv.org/abs/2312.04527v1</link><description>Computer vision has long relied on two kinds of correspondences: pixelcorrespondences in images and 3D correspondences on object surfaces. Is thereanother kind, and if there is, what can they do for us? In this paper, weintroduce correspondences of the third kind we call reflection correspondencesand show that they can help estimate camera pose by just looking at objectswithout relying on the background. Reflection correspondences are pointcorrespondences in the reflected world, i.e., the scene reflected by the objectsurface. The object geometry and reflectance alters the scene geometrically andradiometrically, respectively, causing incorrect pixel correspondences.Geometry recovered from each image is also hampered by distortions, namelygeneralized bas-relief ambiguity, leading to erroneous 3D correspondences. Weshow that reflection correspondences can resolve the ambiguities arising fromthese distortions. We introduce a neural correspondence estimator and a RANSACalgorithm that fully leverages all three kinds of correspondences for robustand accurate joint camera pose and object shape estimation just from the objectappearance. The method expands the horizon of numerous downstream tasks,including camera pose estimation for appearance modeling (e.g., NeRF) andmotion estimation of reflective objects (e.g., cars on the road), to name afew, as it relieves the requirement of overlapping background.</description><author>Kohei Yamashita, Vincent Lepetit, Ko Nishino</author><pubDate>Thu, 07 Dec 2023 18:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04527v1</guid></item><item><title>Evaluation of Active Feature Acquisition Methods for Static Feature Settings</title><link>http://arxiv.org/abs/2312.03619v2</link><description>Active feature acquisition (AFA) agents, crucial in domains like healthcarewhere acquiring features is often costly or harmful, determine the optimal setof features for a subsequent classification task. As deploying an AFA agentintroduces a shift in missingness distribution, it's vital to assess itsexpected performance at deployment using retrospective data. In a companionpaper, we introduce a semi-offline reinforcement learning (RL) framework foractive feature acquisition performance evaluation (AFAPE) where features areassumed to be time-dependent. Here, we study and extend the AFAPE problem tocover static feature settings, where features are time-invariant, and henceprovide more flexibility to the AFA agents in deciding the order of theacquisitions. In this static feature setting, we derive and adapt new inverseprobability weighting (IPW), direct method (DM), and double reinforcementlearning (DRL) estimators within the semi-offline RL framework. Theseestimators can be applied when the missingness in the retrospective datasetfollows a missing-at-random (MAR) pattern. They also can be applied tomissing-not-at-random (MNAR) patterns in conjunction with appropriate existingmissing data techniques. We illustrate the improved data efficiency offered bythe semi-offline RL estimators in synthetic and real-world data experimentsunder synthetic MAR and MNAR missingness.</description><author>Henrik von Kleist, Alireza Zamanian, Ilya Shpitser, Narges Ahmidi</author><pubDate>Thu, 07 Dec 2023 18:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03619v2</guid></item><item><title>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</title><link>http://arxiv.org/abs/2312.04524v1</link><description>Recent advancements in diffusion-based models have demonstrated significantsuccess in generating images from text. However, video editing models have notyet reached the same level of visual quality and user control. To address this,we introduce RAVE, a zero-shot video editing method that leverages pre-trainedtext-to-image diffusion models without additional training. RAVE takes an inputvideo and a text prompt to produce high-quality videos while preserving theoriginal motion and semantic structure. It employs a novel noise shufflingstrategy, leveraging spatio-temporal interactions between frames, to producetemporally consistent videos faster than existing methods. It is also efficientin terms of memory requirements, allowing it to handle longer videos. RAVE iscapable of a wide range of edits, from local attribute modifications to shapetransformations. In order to demonstrate the versatility of RAVE, we create acomprehensive video evaluation dataset ranging from object-focused scenes tocomplex human activities like dancing and typing, and dynamic scenes featuringswimming fish and boats. Our qualitative and quantitative experiments highlightthe effectiveness of RAVE in diverse video editing scenarios compared toexisting methods. Our code, dataset and videos can be found inhttps://rave-video.github.io.</description><author>Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, Pinar Yanardag</author><pubDate>Thu, 07 Dec 2023 18:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04524v1</guid></item><item><title>Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping</title><link>http://arxiv.org/abs/2312.04521v1</link><description>The paper explores the industrial multimodal Anomaly Detection (AD) task,which exploits point clouds and RGB images to localize anomalies. We introducea novel light and fast framework that learns to map features from one modalityto the other on nominal samples. At test time, anomalies are detected bypinpointing inconsistencies between observed and mapped features. Extensiveexperiments show that our approach achieves state-of-the-art detection andsegmentation performance in both the standard and few-shot settings on theMVTec 3D-AD dataset while achieving faster inference and occupying less memorythan previous multimodal AD methods. Moreover, we propose a layer-pruningtechnique to improve memory and time efficiency with a marginal sacrifice inperformance.</description><author>Alex Costanzino, Pierluigi Zama Ramirez, Giuseppe Lisanti, Luigi Di Stefano</author><pubDate>Thu, 07 Dec 2023 18:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04521v1</guid></item><item><title>Bootstrapping Autonomous Radars with Self-Supervised Learning</title><link>http://arxiv.org/abs/2312.04519v1</link><description>The perception of autonomous vehicles using radars has attracted increasedresearch interest due its ability to operate in fog and bad weather. However,training radar models is hindered by the cost and difficulty of annotatinglarge-scale radar data. To overcome this bottleneck, we propose aself-supervised learning framework to leverage the large amount of unlabeledradar data to pre-train radar-only embeddings for self-driving perceptiontasks. The proposed method combines radar-to-radar and radar-to-visioncontrastive losses to learn a general representation from unlabeled radarheatmaps paired with their corresponding camera images. When used fordownstream object detection, we demonstrate that the proposed self-supervisionframework can improve the accuracy of state-of-the-art supervised baselines by5.8% in mAP.</description><author>Yiduo Hao, Sohrab Madani, Junfeng Guan, Mohammed Alloulah, Saurabh Gupta, Haitham Hassanieh</author><pubDate>Thu, 07 Dec 2023 18:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04519v1</guid></item><item><title>Can Large Language Models Transform Computational Social Science?</title><link>http://arxiv.org/abs/2305.03514v2</link><description>Large Language Models (LLMs) are capable of successfully performing manylanguage processing tasks zero-shot (without training data). If zero-shot LLMscan also reliably classify and explain social phenomena like persuasiveness andpolitical ideology, then LLMs could augment the Computational Social Science(CSS) pipeline in important ways. This work provides a road map for using LLMsas CSS tools. Towards this end, we contribute a set of prompting best practicesand an extensive evaluation pipeline to measure the zero-shot performance of 13language models on 25 representative English CSS benchmarks. On taxonomiclabeling tasks (classification), LLMs fail to outperform the best fine-tunedmodels but still achieve fair levels of agreement with humans. On free-formcoding tasks (generation), LLMs produce explanations that often exceed thequality of crowdworkers' gold references. We conclude that the performance oftoday's LLMs can augment the CSS research pipeline in two ways: (1) serving aszero-shot data annotators on human annotation teams, and (2) bootstrappingchallenging creative generation tasks (e.g., explaining the underlyingattributes of a text). In summary, LLMs are posed to meaningfully participatein} social science analysis in partnership with humans.</description><author>Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang</author><pubDate>Thu, 07 Dec 2023 18:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03514v2</guid></item><item><title>Preserving privacy in domain transfer of medical AI models comes at no performance costs: The integral role of differential privacy</title><link>http://arxiv.org/abs/2306.06503v2</link><description>Developing robust and effective artificial intelligence (AI) models inmedicine requires access to large amounts of patient data. The use of AI modelssolely trained on large multi-institutional datasets can help with this, yetthe imperative to ensure data privacy remains, particularly as membershipinference risks breaching patient confidentiality. As a proposed remedy, weadvocate for the integration of differential privacy (DP). We specificallyinvestigate the performance of models trained with DP as compared to modelstrained without DP on data from institutions that the model had not seen duringits training (i.e., external validation) - the situation that is reflective ofthe clinical use of AI models. By leveraging more than 590,000 chestradiographs from five institutions, we evaluated the efficacy of DP-enhanceddomain transfer (DP-DT) in diagnosing cardiomegaly, pleural effusion,pneumonia, atelectasis, and in identifying healthy subjects. We juxtaposedDP-DT with non-DP-DT and examined diagnostic accuracy and demographic fairnessusing the area under the receiver operating characteristic curve (AUC) as themain metric, as well as accuracy, sensitivity, and specificity. Our resultsshow that DP-DT, even with exceptionally high privacy levels (epsilon around1), performs comparably to non-DP-DT (P&gt;0.119 across all domains). Furthermore,DP-DT led to marginal AUC differences - less than 1% - for nearly allsubgroups, relative to non-DP-DT. Despite consistent evidence suggesting thatDP models induce significant performance degradation for on-domainapplications, we show that off-domain performance is almost not affected.Therefore, we ardently advocate for the adoption of DP in training diagnosticmedical AI models, given its minimal impact on performance.</description><author>Soroosh Tayebi Arasteh, Mahshad Lotfinia, Teresa Nolte, Marwin Saehn, Peter Isfort, Christiane Kuhl, Sven Nebelung, Georgios Kaissis, Daniel Truhn</author><pubDate>Thu, 07 Dec 2023 18:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06503v2</guid></item><item><title>Efficient Monotonic Multihead Attention</title><link>http://arxiv.org/abs/2312.04515v1</link><description>We introduce the Efficient Monotonic Multihead Attention (EMMA), astate-of-the-art simultaneous translation model with numerically-stable andunbiased monotonic alignment estimation. In addition, we present improvedtraining and inference strategies, including simultaneous fine-tuning from anoffline translation model and reduction of monotonic alignment variance. Theexperimental results demonstrate that the proposed model attainsstate-of-the-art performance in simultaneous speech-to-text translation on theSpanish and English translation task.</description><author>Xutai Ma, Anna Sun, Siqi Ouyang, Hirofumi Inaguma, Paden Tomasello</author><pubDate>Thu, 07 Dec 2023 18:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04515v1</guid></item><item><title>An LLM Compiler for Parallel Function Calling</title><link>http://arxiv.org/abs/2312.04511v1</link><description>Large Language Models (LLMs) have shown remarkable results on various complexreasoning benchmarks. The reasoning capabilities of LLMs enable them to executefunction calls, using user-provided functions to overcome their inherentlimitations, such as knowledge cutoffs, poor arithmetic skills, or lack ofaccess to private data. This development has expanded LLMs' scope to includemulti-function calling, where LLMs are equipped with a variety of functions andselect the proper functions based on the context. Multi-function callingabilities of LLMs have catalyzed LLM-based software development, allowing themto tackle more complex problems. However, current methods for multi-functioncalling often require sequential reasoning and acting for each function whichcan result in high latency, cost, and sometimes inaccurate behavior. To addressthis, we introduce LLMCompiler, which executes functions in parallel toefficiently orchestrate multi-function calling. Drawing from the principles ofclassical compilers, LLMCompiler streamlines parallel function calling withthree components: (i) an LLM Planner, formulating execution strategies anddependencies; (ii) a Task Fetching Unit, dispatching function calling tasks;and (iii) an Executor, executing these tasks in parallel. LLMCompilerautomatically computes an optimized orchestration for the function calls andcan be used with open-source models such as LLaMA-2. We have benchmarkedLLMCompiler on a range of tasks including cases with non-trivialinter-dependency between function calls, as well as cases that require dynamicreplanning based on intermediate results. We observe consistent latency speedupof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35xlatency gain over OpenAI's recent parallel function calling, while achievingsimilar accuracy.</description><author>Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</author><pubDate>Thu, 07 Dec 2023 18:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04511v1</guid></item><item><title>A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation</title><link>http://arxiv.org/abs/2312.04510v1</link><description>Recent work has shown that energy-based language modeling is an effectiveframework for controllable text generation because it enables flexibleintegration of arbitrary discriminators. However, because energy-based LMs areglobally normalized, approximate techniques like Metropolis-Hastings (MH) arerequired for inference. Past work has largely explored simple proposaldistributions that modify a single token at a time, like in Gibbs sampling. Inthis paper, we develop a novel MH sampler that, in contrast, proposes re-writesof the entire sequence in each step via iterative prompting of a large languagemodel. Our new sampler (a) allows for more efficient and accurate sampling froma target distribution and (b) allows generation length to be determined throughthe sampling procedure rather than fixed in advance, as past work has required.We perform experiments on two controlled generation tasks, showing bothdownstream performance gains and more accurate target distribution sampling incomparison with single-token proposal techniques.</description><author>Jarad Forristal, Niloofar Mireshghallah, Greg Durrett, Taylor Berg-Kirkpatrick</author><pubDate>Thu, 07 Dec 2023 18:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04510v1</guid></item><item><title>LAVA: Data Valuation without Pre-Specified Learning Algorithms</title><link>http://arxiv.org/abs/2305.00054v2</link><description>Traditionally, data valuation (DV) is posed as a problem of equitablysplitting the validation performance of a learning algorithm among the trainingdata. As a result, the calculated data values depend on many design choices ofthe underlying learning algorithm. However, this dependence is undesirable formany DV use cases, such as setting priorities over different data sources in adata acquisition process and informing pricing mechanisms in a datamarketplace. In these scenarios, data needs to be valued before the actualanalysis and the choice of the learning algorithm is still undetermined then.Another side-effect of the dependence is that to assess the value of individualpoints, one needs to re-run the learning algorithm with and without a point,which incurs a large computation burden. This work leapfrogs over the currentlimits of data valuation methods by introducing a new framework that can valuetraining data in a way that is oblivious to the downstream learning algorithm.Our main results are as follows. (1) We develop a proxy for the validationperformance associated with a training set based on a non-conventionalclass-wise Wasserstein distance between training and validation sets. We showthat the distance characterizes the upper bound of the validation performancefor any given model under certain Lipschitz conditions. (2) We develop a novelmethod to value individual data based on the sensitivity analysis of theclass-wise Wasserstein distance. Importantly, these values can be directlyobtained for free from the output of off-the-shelf optimization solvers whencomputing the distance. (3) We evaluate our new data valuation framework overvarious use cases related to detecting low-quality data and show that,surprisingly, the learning-agnostic feature of our framework enables asignificant improvement over SOTA performance while being orders of magnitudefaster.</description><author>Hoang Anh Just, Feiyang Kang, Jiachen T. Wang, Yi Zeng, Myeongseob Ko, Ming Jin, Ruoxi Jia</author><pubDate>Thu, 07 Dec 2023 18:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00054v2</guid></item><item><title>Coordination-free Decentralised Federated Learning on Complex Networks: Overcoming Heterogeneity</title><link>http://arxiv.org/abs/2312.04504v1</link><description>Federated Learning (FL) is a well-known framework for successfully performinga learning task in an edge computing scenario where the devices involved havelimited resources and incomplete data representation. The basic assumption ofFL is that the devices communicate directly or indirectly with a parameterserver that centrally coordinates the whole process, overcoming severalchallenges associated with it. However, in highly pervasive edge scenarios, thepresence of a central controller that oversees the process cannot always beguaranteed, and the interactions (i.e., the connectivity graph) between devicesmight not be predetermined, resulting in a complex network structure. Moreover,the heterogeneity of data and devices further complicates the learning process.This poses new challenges from a learning standpoint that we address byproposing a communication-efficient Decentralised Federated Learning (DFL)algorithm able to cope with them. Our solution allows devices communicatingonly with their direct neighbours to train an accurate model, overcoming theheterogeneity induced by data and different training histories. Our resultsshow that the resulting local models generalise better than those trained withcompeting approaches, and do so in a more communication-efficient way.</description><author>Lorenzo Valerio, Chiara Boldrini, Andrea Passarella, János Kertész, Márton Karsai, Gerardo Iñiguez</author><pubDate>Thu, 07 Dec 2023 18:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04504v1</guid></item><item><title>Graph Metanetworks for Processing Diverse Neural Architectures</title><link>http://arxiv.org/abs/2312.04501v1</link><description>Neural networks efficiently encode learned information within theirparameters. Consequently, many tasks can be unified by treating neural networksthemselves as input data. When doing so, recent studies demonstrated theimportance of accounting for the symmetries and geometry of parameter spaces.However, those works developed architectures tailored to specific networks suchas MLPs and CNNs without normalization layers, and generalizing sucharchitectures to other types of networks can be challenging. In this work, weovercome these challenges by building new metanetworks - neural networks thattake weights from other neural networks as input. Put simply, we carefullybuild graphs representing the input neural networks and process the graphsusing graph neural networks. Our approach, Graph Metanetworks (GMNs),generalizes to neural architectures where competing methods struggle, such asmulti-head attention layers, normalization layers, convolutional layers, ResNetblocks, and group-equivariant linear layers. We prove that GMNs are expressiveand equivariant to parameter permutation symmetries that leave the input neuralnetwork functions unchanged. We validate the effectiveness of our method onseveral metanetwork tasks over diverse neural network architectures.</description><author>Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas</author><pubDate>Thu, 07 Dec 2023 18:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04501v1</guid></item><item><title>AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making</title><link>http://arxiv.org/abs/2312.04494v1</link><description>With recent advances in multi-modal foundation models, the previouslytext-only large language models (LLM) have evolved to incorporate visual input,opening up unprecedented opportunities for various applications invisualization. Our work explores the utilization of the visual perceptionability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs)that can interpret and accomplish user-defined visualization objectives throughnatural language. We propose the first framework for the design of AVAs andpresent several usage scenarios intended to demonstrate the generalapplicability of the proposed paradigm. The addition of visual perceptionallows AVAs to act as the virtual visualization assistant for domain expertswho may lack the knowledge or expertise in fine-tuning visualization outputs.Our preliminary exploration and proof-of-concept agents suggest that thisapproach can be widely applicable whenever the choices of appropriatevisualization parameters require the interpretation of previous visual output.Feedback from unstructured interviews with experts in AI research, medicalvisualization, and radiology has been incorporated, highlighting thepracticality and potential of AVAs. Our study indicates that AVAs represent ageneral paradigm for designing intelligent visualization systems that canachieve high-level visualization goals, which pave the way for developingexpert-level visualization agents in the future.</description><author>Shusen Liu, Haichao Miao, Zhimin Li, Matthew Olson, Valerio Pascucci, Peer-Timo Bremer</author><pubDate>Thu, 07 Dec 2023 18:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04494v1</guid></item><item><title>A Stability Analysis of Fine-Tuning a Pre-Trained Model</title><link>http://arxiv.org/abs/2301.09820v2</link><description>Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,etc.) has proven to be one of the most promising paradigms in recent NLPresearch. However, numerous recent works indicate that fine-tuning suffers fromthe instability problem, i.e., tuning the same model under the same settingresults in significantly different performance. Many recent works have proposeddifferent methods to solve this problem, but there is no theoreticalunderstanding of why and how these methods work. In this paper, we propose anovel theoretical stability analysis of fine-tuning that focuses on twocommonly used settings, namely, full fine-tuning and head tuning. We define thestability under each setting and prove the corresponding stability bounds. Thetheoretical bounds explain why and how several existing methods can stabilizethe fine-tuning procedure. In addition to being able to explain most of theobserved empirical discoveries, our proposed theoretical analysis framework canalso help in the design of effective and provable methods. Based on our theory,we propose three novel strategies to stabilize the fine-tuning procedure,namely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and SelfUnsupervised Re-Training (SURT). We extensively evaluate our proposedapproaches on 11 widely used real-world benchmark datasets, as well as hundredsof synthetic classification datasets. The experiment results show that ourproposed methods significantly stabilize the fine-tuning procedure and alsocorroborate our theoretical analysis.</description><author>Zihao Fu, Anthony Man-Cho So, Nigel Collier</author><pubDate>Thu, 07 Dec 2023 18:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09820v2</guid></item><item><title>Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</title><link>http://arxiv.org/abs/2305.03907v2</link><description>Egocentric gaze anticipation serves as a key building block for the emergingcapability of Augmented Reality. Notably, gaze behavior is driven by bothvisual cues and audio signals during daily activities. Motivated by thisobservation, we introduce the first model that leverages both the video andaudio modalities for egocentric gaze anticipation. Specifically, we propose aContrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts twomodules to separately capture audio-visual correlations in spatial and temporaldimensions, and applies a contrastive loss on the re-weighted audio-visualfeatures from fusion modules for representation learning. We conduct extensiveablation studies and thorough analysis using two egocentric video datasets:Ego4D and Aria, to validate our model design. We also demonstrate our modeloutperforms prior state-of-the-art methods by at least +1.9% and +1.6%.Moreover, we provide visualizations to show the gaze anticipation results andprovide additional insights into audio-visual representation learning.</description><author>Bolin Lai, Fiona Ryan, Wenqi Jia, Miao Liu, James M. Rehg</author><pubDate>Thu, 07 Dec 2023 18:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03907v2</guid></item><item><title>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation</title><link>http://arxiv.org/abs/2312.04484v1</link><description>LiDAR segmentation is crucial for autonomous driving systems. The recentrange-view approaches are promising for real-time processing. However, theysuffer inevitably from corrupted contextual information and rely heavily onpost-processing techniques for prediction refinement. In this work, we proposea simple yet powerful FRNet that restores the contextual information of therange image pixels with corresponding frustum LiDAR points. Firstly, a frustumfeature encoder module is used to extract per-point features within the frustumregion, which preserves scene consistency and is crucial for point-levelpredictions. Next, a frustum-point fusion module is introduced to updateper-point features hierarchically, which enables each point to extract moresurrounding information via the frustum features. Finally, a head fusion moduleis used to fuse features at different levels for final semantic prediction.Extensive experiments on four popular LiDAR segmentation benchmarks undervarious task setups demonstrate our superiority. FRNet achieves competitiveperformance while maintaining high efficiency. The code is publicly available.</description><author>Xiang Xu, Lingdong Kong, Hui Shuai, Qingshan Liu</author><pubDate>Thu, 07 Dec 2023 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04484v1</guid></item><item><title>Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</title><link>http://arxiv.org/abs/2312.04483v1</link><description>Despite diffusion models having shown powerful abilities to generatephotorealistic images, generating videos that are realistic and diverse stillremains in its infancy. One of the key reasons is that current methodsintertwine spatial content and temporal dynamics together, leading to a notablyincreased complexity of text-to-video generation (T2V). In this work, wepropose HiGen, a diffusion model-based method that improves performance bydecoupling the spatial and temporal factors of videos from two perspectives,i.e., structure level and content level. At the structure level, we decomposethe T2V task into two steps, including spatial reasoning and temporalreasoning, using a unified denoiser. Specifically, we generate spatiallycoherent priors using text during spatial reasoning and then generatetemporally coherent motions from these priors during temporal reasoning. At thecontent level, we extract two subtle cues from the content of the input videothat can express motion and appearance changes, respectively. These two cuesthen guide the model's training for generating videos, enabling flexiblecontent variations and enhancing temporal stability. Through the decoupledparadigm, HiGen can effectively reduce the complexity of this task and generaterealistic videos with semantics accuracy and motion stability. Extensiveexperiments demonstrate the superior performance of HiGen over thestate-of-the-art T2V methods.</description><author>Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, Nong Sang</author><pubDate>Thu, 07 Dec 2023 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04483v1</guid></item><item><title>If your data distribution shifts, use self-learning</title><link>http://arxiv.org/abs/2104.12928v4</link><description>We demonstrate that self-learning techniques like entropy minimization andpseudo-labeling are simple and effective at improving performance of a deployedcomputer vision model under systematic domain shifts. We conduct a wide rangeof large-scale experiments and show consistent improvements irrespective of themodel architecture, the pre-training technique or the type of distributionshift. At the same time, self-learning is simple to use in practice because itdoes not require knowledge or access to the original training data or scheme,is robust to hyperparameter choices, is straight-forward to implement andrequires only a few adaptation epochs. This makes self-learning techniqueshighly attractive for any practitioner who applies machine learning algorithmsin the real world. We present state-of-the-art adaptation results on CIFAR10-C(8.5% error), ImageNet-C (22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A(14.8% error), theoretically study the dynamics of self-supervised adaptationmethods and propose a new classification dataset (ImageNet-D) which ischallenging even with adaptation.</description><author>Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Gehler, Oliver Bringmann, Wieland Brendel, Matthias Bethge</author><pubDate>Thu, 07 Dec 2023 17:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.12928v4</guid></item><item><title>Distributed Bayesian Estimation in Sensor Networks: Consensus on Marginal Densities</title><link>http://arxiv.org/abs/2312.01227v2</link><description>In this paper, we aim to design and analyze distributed Bayesian estimationalgorithms for sensor networks. The challenges we address are to (i) derive adistributed provably-correct algorithm in the functional space of probabilitydistributions over continuous variables, and (ii) leverage these results toobtain new distributed estimators restricted to subsets of variables observedby individual agents. This relates to applications such as cooperativelocalization and federated learning, where the data collected at any agentdepends on a subset of all variables of interest. We present Bayesian densityestimation algorithms using data from non-linear likelihoods at agents incentralized, distributed, and marginal distributed settings. After setting up adistributed estimation objective, we prove almost-sure convergence to theoptimal set of pdfs at each agent. Then, we prove the same for a storage-awarealgorithm estimating densities only over relevant variables at each agent.Finally, we present a Gaussian version of these algorithms and implement it ina mapping problem using variational inference to handle non-linear likelihoodmodels associated with LiDAR sensing.</description><author>Parth Paritosh, Nikolay Atanasov, Sonia Martinez</author><pubDate>Thu, 07 Dec 2023 17:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01227v2</guid></item><item><title>GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction</title><link>http://arxiv.org/abs/2312.04479v1</link><description>Pedestrian trajectory prediction, vital for selfdriving cars andsocially-aware robots, is complicated due to intricate interactions betweenpedestrians, their environment, and other Vulnerable Road Users. This paperpresents GSGFormer, an innovative generative model adept at predictingpedestrian trajectories by considering these complex interactions and offeringa plethora of potential modal behaviors. We incorporate a heterogeneous graphneural network to capture interactions between pedestrians, semantic maps, andpotential destinations. The Transformer module extracts temporal features,while our novel CVAE-Residual-GMM module promotes diverse behavioral modalitygeneration. Through evaluations on multiple public datasets, GSGFormer not onlyoutperforms leading methods with ample data but also remains competitive whendata is limited.</description><author>Zhongchang Luo, Marion Robin, Pavan Vasishta</author><pubDate>Thu, 07 Dec 2023 17:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04479v1</guid></item><item><title>Chain of Code: Reasoning with a Language Model-Augmented Code Emulator</title><link>http://arxiv.org/abs/2312.04474v1</link><description>Code provides a general syntactic structure to build complex programs andperform precise computations when paired with a code interpreter -- wehypothesize that language models (LMs) can leverage code-writing to improveChain of Thought reasoning not only for logic and arithmetic tasks, but alsofor linguistic ones (and in particular, those that are a mix of both). Forexample, consider prompting an LM to write code that counts the number of timesit detects sarcasm in an essay: the LM may struggle to write an implementationfor "detect_sarcasm(string)" that can be executed by the interpreter (handlingthe edge cases would be insurmountable). However, LMs may still produce a validsolution if they are used not only to write the code, but also to selectively"emulate" the interpreter by generating the expected output of"detect_sarcasm(string)" and other lines of code (e.g., that the interpretercould not compile). In this work, we propose Chain of Code (CoT), a simple yetsurprisingly effective extension that improves LM code-driven reasoning. Thekey idea is to encourage LMs to format linguistic sub-tasks in a program asflexible pseudocode that the compiler can explicitly catch undefined behaviorsand hand off to simulate with an LM (as an "LMulator"). Experiments demonstratethat Chain of Code outperforms Chain of Thought and other baselines across avariety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of12% over Chain of Thought. CoT scales well with large and small models alike,and broadens the scope of reasoning questions that LMs can correctly answer by"thinking in code". Project webpage: https://chain-of-code.github.io/.</description><author>Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter</author><pubDate>Thu, 07 Dec 2023 17:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04474v1</guid></item><item><title>ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation</title><link>http://arxiv.org/abs/2306.00971v2</link><description>Personalized text-to-image generation using diffusion models has recentlyemerged and garnered significant interest. This task learns a novel concept(e.g., a unique toy), illustrated in a handful of images, into a generativemodel that captures fine visual details and generates photorealistic imagesbased on textual embeddings. In this paper, we present ViCo, a novellightweight plug-and-play method that seamlessly integrates visual conditioninto personalized text-to-image generation. ViCo stands out for its uniquefeature of not requiring any fine-tuning of the original diffusion modelparameters, thereby facilitating more flexible and scalable model deployment.This key advantage distinguishes ViCo from most existing models thatnecessitate partial or full diffusion fine-tuning. ViCo incorporates an imageattention module that conditions the diffusion process on patch-wise visualsemantics, and an attention-based object mask that comes at no extra cost fromthe attention module. Despite only requiring light parameter training (~6%compared to the diffusion U-Net), ViCo delivers performance that is on parwith, or even surpasses, all state-of-the-art models, both qualitatively andquantitatively. This underscores the efficacy of ViCo, making it a highlypromising solution for personalized text-to-image generation without the needfor diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo</description><author>Shaozhe Hao, Kai Han, Shihao Zhao, Kwan-Yee K. Wong</author><pubDate>Thu, 07 Dec 2023 17:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00971v2</guid></item><item><title>On the Learnability of Watermarks for Language Models</title><link>http://arxiv.org/abs/2312.04469v1</link><description>Watermarking of language model outputs enables statistical detection ofmodel-generated text, which has many applications in the responsible deploymentof language models. Existing watermarking strategies operate by altering thedecoder of an existing language model, and the ability for a language model todirectly learn to generate the watermark would have significant implicationsfor the real-world deployment of watermarks. First, learned watermarks could beused to build open models that naturally generate watermarked text, allowingfor open models to benefit from watermarking. Second, if watermarking is usedto determine the provenance of generated text, an adversary can hurt thereputation of a victim model by spoofing its watermark and generating damagingwatermarked text. To investigate the learnability of watermarks, we proposewatermark distillation, which trains a student model to behave like a teachermodel that uses decoding-based watermarking. We test our approach on threedistinct decoding-based watermarking strategies and various hyperparametersettings, finding that models can learn to generate watermarked text with highdetectability. We also find limitations to learnability, including the loss ofwatermarking capabilities under fine-tuning on normal text and high samplecomplexity when learning low-distortion watermarks.</description><author>Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto</author><pubDate>Thu, 07 Dec 2023 17:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04469v1</guid></item><item><title>Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion</title><link>http://arxiv.org/abs/2312.04466v1</link><description>Existing methods for synthesizing 3D human gestures from speech have shownpromising results, but they do not explicitly model the impact of emotions onthe generated gestures. Instead, these methods directly output animations fromspeech without control over the expressed emotion. To address this limitation,we present AMUSE, an emotional speech-driven body animation model based onlatent diffusion. Our observation is that content (i.e., gestures related tospeech rhythm and word utterances), emotion, and personal style are separable.To account for this, AMUSE maps the driving audio to three disentangled latentvectors: one for content, one for emotion, and one for personal style. A latentdiffusion model, trained to generate gesture motion sequences, is thenconditioned on these latent vectors. Once trained, AMUSE synthesizes 3D humangestures directly from speech with control over the expressed emotions andstyle by combining the content from the driving speech with the emotion andstyle of another speech sequence. Randomly sampling the noise of the diffusionmodel further generates variations of the gesture with the same emotionalexpressivity. Qualitative, quantitative, and perceptual evaluations demonstratethat AMUSE outputs realistic gesture sequences. Compared to the state of theart, the generated gestures are better synchronized with the speech content andbetter represent the emotion expressed by the input speech. Our project websiteis amuse.is.tue.mpg.de.</description><author>Kiran Chhatre, Radek Daněček, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart</author><pubDate>Thu, 07 Dec 2023 17:39:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04466v1</guid></item><item><title>FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models</title><link>http://arxiv.org/abs/2312.04465v1</link><description>The remarkable progress in 3D face reconstruction has resulted in high-detailand photorealistic facial representations. Recently, Diffusion Models haverevolutionized the capabilities of generative methods by achieving far betterperformance than GANs. In this work, we present FitDiff, a diffusion-based 3Dfacial avatar generative model. This model accurately generates relightablefacial avatars, utilizing an identity embedding extracted from an "in-the-wild"2D facial image. Our multi-modal diffusion model concurrently outputs facialreflectance maps (diffuse and specular albedo and normals) and shapes,showcasing great generalization capabilities. It is solely trained on anannotated subset of a public facial dataset, paired with 3D reconstructions. Werevisit the typical 3D facial fitting approach by guiding a reverse diffusionprocess using perceptual and face recognition losses. Being the first LDMconditioned on face recognition embeddings, FitDiff reconstructs relightablehuman avatars, that can be used as-is in common rendering engines, startingonly from an unconstrained facial image, and achieving state-of-the-artperformance.</description><author>Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou</author><pubDate>Thu, 07 Dec 2023 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04465v1</guid></item><item><title>Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement Learning with General Function Approximation</title><link>http://arxiv.org/abs/2312.04464v1</link><description>To tackle long planning horizon problems in reinforcement learning withgeneral function approximation, we propose the first algorithm, termed asUCRL-WVTR, that achieves both \emph{horizon-free} and\emph{instance-dependent}, since it eliminates the polynomial dependency on theplanning horizon. The derived regret bound is deemed \emph{sharp}, as itmatches the minimax lower bound when specialized to linear mixture MDPs up tologarithmic factors. Furthermore, UCRL-WVTR is \emph{computationally efficient}with access to a regression oracle. The achievement of such a horizon-free,instance-dependent, and sharp regret bound hinges upon (i) novel algorithmdesigns: weighted value-targeted regression and a high-order moment estimatorin the context of general function approximation; and (ii) fine-grainedanalyses: a novel concentration bound of weighted non-linear least squares anda refined analysis which leads to the tight instance-dependent bound. We alsoconduct comprehensive experiments to corroborate our theoretical findings.</description><author>Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang</author><pubDate>Thu, 07 Dec 2023 17:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04464v1</guid></item><item><title>PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</title><link>http://arxiv.org/abs/2312.04461v1</link><description>Recent advances in text-to-image generation have made remarkable progress insynthesizing realistic human photos conditioned on given text prompts. However,existing personalized generation methods cannot simultaneously satisfy therequirements of high efficiency, promising identity (ID) fidelity, and flexibletext controllability. In this work, we introduce PhotoMaker, an efficientpersonalized text-to-image generation method, which mainly encodes an arbitrarynumber of input ID images into a stack ID embedding for preserving IDinformation. Such an embedding, serving as a unified ID representation, can notonly encapsulate the characteristics of the same input ID comprehensively, butalso accommodate the characteristics of different IDs for subsequentintegration. This paves the way for more intriguing and practically valuableapplications. Besides, to drive the training of our PhotoMaker, we propose anID-oriented data construction pipeline to assemble the training data. Under thenourishment of the dataset constructed through the proposed pipeline, ourPhotoMaker demonstrates better ID preservation ability than test-timefine-tuning based methods, yet provides significant speed improvements,high-quality generation results, strong generalization capabilities, and a widerange of applications. Our project page is available athttps://photo-maker.github.io/</description><author>Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, Ying Shan</author><pubDate>Thu, 07 Dec 2023 17:32:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04461v1</guid></item><item><title>Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use</title><link>http://arxiv.org/abs/2312.04455v1</link><description>Recent advancements in large language models (LLMs) have significantlyexpanded their functionality and skills as tool agents. In this paper, we arguethat a waveform pattern in the model's attention allocation has an impact onthe tool use performance, which degrades when the position of essentialinformation hits the trough zone. To address this issue, we propose a novelinference method named Attention Buckets. This approach enables LLMs to handlecontext by conducting parallel processes, each featuring a unique RoPE anglebase that shapes the attention waveform. Attention Buckets ensures that anattention trough of a particular process can be compensated with an attentionpeak of another run, reducing the risk of the LLM missing essential informationresiding within the attention trough. Our extensive experiments on the widelyrecognized tool use benchmark demonstrate the efficacy of our approach, where a7B-parameter open-source model enhanced by Attention Buckets achieves SOTAperformance on par with GPT-4.</description><author>Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, Rui Yan</author><pubDate>Thu, 07 Dec 2023 17:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04455v1</guid></item><item><title>Privacy-preserving quantum federated learning via gradient hiding</title><link>http://arxiv.org/abs/2312.04447v1</link><description>Distributed quantum computing, particularly distributed quantum machinelearning, has gained substantial prominence for its capacity to harness thecollective power of distributed quantum resources, transcending the limitationsof individual quantum nodes. Meanwhile, the critical concern of privacy withindistributed computing protocols remains a significant challenge, particularlyin standard classical federated learning (FL) scenarios where data ofparticipating clients is susceptible to leakage via gradient inversion attacksby the server. This paper presents innovative quantum protocols with quantumcommunication designed to address the FL problem, strengthen privacy measures,and optimize communication efficiency. In contrast to previous works thatleverage expressive variational quantum circuits or differential privacytechniques, we consider gradient information concealment using quantum statesand propose two distinct FL protocols, one based on private inner-productestimation and the other on incremental learning. These protocols offersubstantial advancements in privacy preservation with low communicationresources, forging a path toward efficient quantum communication-assisted FLprotocols and contributing to the development of secure distributed quantummachine learning, thus addressing critical privacy concerns in the quantumcomputing era.</description><author>Changhao Li, Niraj Kumar, Zhixin Song, Shouvanik Chakrabarti, Marco Pistoia</author><pubDate>Thu, 07 Dec 2023 17:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04447v1</guid></item><item><title>Training Deep 3D Convolutional Neural Networks to Extract BSM Physics Parameters Directly from HEP Data: a Proof-of-Concept Study Using Monte Carlo Simulations</title><link>http://arxiv.org/abs/2311.13060v2</link><description>We report on a novel application of computer vision techniques to extractbeyond the Standard Model (BSM) parameters directly from high energy physics(HEP) flavor data. We develop a method of transforming angular and kinematicdistributions into "quasi-images" that can be used to train a convolutionalneural network to perform regression tasks, similar to fitting. This contrastswith the usual classification functions performed using ML/AI in HEP. As aproof-of-concept, we train a 34-layer Residual Neural Network to regress onthese images and determine the Wilson Coefficient $C_{9}$ in MC (Monte Carlo)simulations of $B \rightarrow K^{*}\mu^{+}\mu^{-}$ decays. The techniquedescribed here can be generalized and may find applicability across various HEPexperiments and elsewhere.</description><author>S. Dubey, T. E. Browder, S. Kohani, R. Mandal, A. Sibidanov, R. Sinha</author><pubDate>Thu, 07 Dec 2023 17:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13060v2</guid></item><item><title>OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization</title><link>http://arxiv.org/abs/2312.04440v1</link><description>The performance of automatic summarization models has improved dramaticallyin recent years. Yet, there is still a gap in meeting specific informationneeds of users in real-world scenarios, particularly when a targeted summary issought, such as in the useful aspect-based summarization setting targeted inthis paper. Previous datasets and studies for this setting have predominantlyconcentrated on a limited set of pre-defined aspects, focused solely on singledocument inputs, or relied on synthetic data. To advance research on morerealistic scenarios, we introduce OpenAsp, a benchmark for multi-document\textit{open} aspect-based summarization. This benchmark is created using anovel and cost-effective annotation protocol, by which an open aspect datasetis derived from existing generic multi-document summarization datasets. Weanalyze the properties of OpenAsp showcasing its high-quality content. Further,we show that the realistic open-aspect setting realized in OpenAsp poses achallenge for current state-of-the-art summarization models, as well as forlarge language models.</description><author>Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, Ido Dagan</author><pubDate>Thu, 07 Dec 2023 17:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04440v1</guid></item><item><title>DreamVideo: Composing Your Dream Videos with Customized Subject and Motion</title><link>http://arxiv.org/abs/2312.04433v1</link><description>Customized generation using diffusion models has made impressive progress inimage generation, but remains unsatisfactory in the challenging videogeneration task, as it requires the controllability of both subjects andmotions. To that end, we present DreamVideo, a novel approach to generatingpersonalized videos from a few static images of the desired subject and a fewvideos of target motion. DreamVideo decouples this task into two stages,subject learning and motion learning, by leveraging a pre-trained videodiffusion model. The subject learning aims to accurately capture the fineappearance of the subject from provided images, which is achieved by combiningtextual inversion and fine-tuning of our carefully designed identity adapter.In motion learning, we architect a motion adapter and fine-tune it on the givenvideos to effectively model the target motion pattern. Combining these twolightweight and efficient adapters allows for flexible customization of anysubject with any motion. Extensive experimental results demonstrate thesuperior performance of our DreamVideo over the state-of-the-art methods forcustomized video generation. Our project page is athttps://dreamvideo-t2v.github.io.</description><author>Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan</author><pubDate>Thu, 07 Dec 2023 16:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04433v1</guid></item><item><title>FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning</title><link>http://arxiv.org/abs/2312.04432v1</link><description>Federated learning (FL) is a collaborative learning paradigm allowingmultiple clients to jointly train a model without sharing their training data.However, FL is susceptible to poisoning attacks, in which the adversary injectsmanipulated model updates into the federated model aggregation process tocorrupt or destroy predictions (untargeted poisoning) or implant hiddenfunctionalities (targeted poisoning or backdoors). Existing defenses againstpoisoning attacks in FL have several limitations, such as relying on specificassumptions about attack types and strategies or data distributions or notsufficiently robust against advanced injection techniques and strategies andsimultaneously maintaining the utility of the aggregated model. To address thedeficiencies of existing defenses, we take a generic and completely differentapproach to detect poisoning (targeted and untargeted) attacks. We presentFreqFed, a novel aggregation mechanism that transforms the model updates (i.e.,weights) into the frequency domain, where we can identify the core frequencycomponents that inherit sufficient information about weights. This allows us toeffectively filter out malicious updates during local training on the clients,regardless of attack types, strategies, and clients' data distributions. Weextensively evaluate the efficiency and effectiveness of FreqFed in differentapplication domains, including image classification, word prediction, IoTintrusion detection, and speech recognition. We demonstrate that FreqFed canmitigate poisoning attacks effectively with a negligible impact on the utilityof the aggregated model.</description><author>Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi</author><pubDate>Thu, 07 Dec 2023 16:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04432v1</guid></item><item><title>Approximate Caching for Efficiently Serving Diffusion Models</title><link>http://arxiv.org/abs/2312.04429v1</link><description>Text-to-image generation using diffusion models has seen explosive popularityowing to their ability in producing high quality images adhering to textprompts. However, production-grade diffusion model serving is a resourceintensive task that not only require high-end GPUs which are expensive but alsoincurs considerable latency. In this paper, we introduce a technique calledapproximate-caching that can reduce such iterative denoising steps for an imagegeneration based on a prompt by reusing intermediate noise states createdduring a prior image generation for similar prompts. Based on this idea, wepresent an end to end text-to-image system, Nirvana, that uses theapproximate-caching with a novel cache management-policy Least ComputationallyBeneficial and Frequently Used (LCBFU) to provide % GPU compute savings, 19.8%end-to-end latency reduction and 19% dollar savings, on average, on two realproduction workloads. We further present an extensive characterization of realproduction text-to-image prompts from the perspective of caching, popularityand reuse of intermediate states in a large production environment.</description><author>Shubham Agarwal, Subrata Mitra, Sarthak Chakraborty, Srikrishna Karanam, Koyel Mukherjee, Shiv Saini</author><pubDate>Thu, 07 Dec 2023 16:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04429v1</guid></item><item><title>Kernel quadrature with randomly pivoted Cholesky</title><link>http://arxiv.org/abs/2306.03955v3</link><description>This paper presents new quadrature rules for functions in a reproducingkernel Hilbert space using nodes drawn by a sampling algorithm known asrandomly pivoted Cholesky. The resulting computational procedure comparesfavorably to previous kernel quadrature methods, which either achieve lowaccuracy or require solving a computationally challenging sampling problem.Theoretical and numerical results show that randomly pivoted Cholesky is fastand achieves comparable quadrature error rates to more computationallyexpensive quadrature schemes based on continuous volume sampling, thinning, andrecombination. Randomly pivoted Cholesky is easily adapted to complicatedgeometries with arbitrary kernels, unlocking new potential for kernelquadrature.</description><author>Ethan N. Epperly, Elvira Moreno</author><pubDate>Thu, 07 Dec 2023 16:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03955v3</guid></item><item><title>Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views</title><link>http://arxiv.org/abs/2312.04424v1</link><description>Synthesizing multi-view 3D from one single image is a significant andchallenging task. For this goal, Zero-1-to-3 methods aim to extend a 2D latentdiffusion model to the 3D scope. These approaches generate the target-viewimage with a single-view source image and the camera pose as conditioninformation. However, the one-to-one manner adopted in Zero-1-to-3 incurschallenges for building geometric and visual consistency across views,especially for complex objects. We propose a cascade generation frameworkconstructed with two Zero-1-to-3 models, named Cascade-Zero123, to tackle thisissue, which progressively extracts 3D information from the source image.Specifically, a self-prompting mechanism is designed to generate several nearbyviews at first. These views are then fed into the second-stage model along withthe source image as generation conditions. With self-prompted multiple views asthe supplementary information, our Cascade-Zero123 generates more highlyconsistent novel-view images than Zero-1-to-3. The promotion is significant forvarious complex and challenging scenes, involving insects, humans, transparentobjects, and stacked multiple objects etc. The project page is athttps://cascadezero123.github.io/.</description><author>Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, Qi Tian</author><pubDate>Thu, 07 Dec 2023 16:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04424v1</guid></item><item><title>Scalable Knowledge Graph Construction and Inference on Human Genome Variants</title><link>http://arxiv.org/abs/2312.04423v1</link><description>Real-world knowledge can be represented as a graph consisting of entities andrelationships between the entities. The need for efficient and scalablesolutions arises when dealing with vast genomic data, like RNA-sequencing.Knowledge graphs offer a powerful approach for various tasks in suchlarge-scale genomic data, such as analysis and inference. In this work,variant-level information extracted from the RNA-sequences of vaccine-na\"iveCOVID-19 patients have been represented as a unified, large knowledge graph.Variant call format (VCF) files containing the variant-level information wereannotated to include further information for each variant. The data records inthe annotated files were then converted to Resource Description Framework (RDF)triples. Each VCF file obtained had an associated CADD scores file thatcontained the raw and Phred-scaled scores for each variant. An ontology wasdefined for the VCF and CADD scores files. Using this ontology and theextracted information, a large, scalable knowledge graph was created. Availablegraph storage was then leveraged to query and create datasets for furtherdownstream tasks. We also present a case study using the knowledge graph andperform a classification task using graph machine learning. We also drawcomparisons between different Graph Neural Networks (GNNs) for the case study.</description><author>Shivika Prasanna, Deepthi Rao, Eduardo Simoes, Praveen Rao</author><pubDate>Thu, 07 Dec 2023 16:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04423v1</guid></item><item><title>MapFormer: Boosting Change Detection by Using Pre-change Information</title><link>http://arxiv.org/abs/2303.17859v4</link><description>Change detection in remote sensing imagery is essential for a variety ofapplications such as urban planning, disaster management, and climate research.However, existing methods for identifying semantically changed areas overlookthe availability of semantic information in the form of existing mapsdescribing features of the earth's surface. In this paper, we leverage thisinformation for change detection in bi-temporal images. We show that the simpleintegration of the additional information via concatenation of latentrepresentations suffices to significantly outperform state-of-the-art changedetection methods. Motivated by this observation, we propose the new task of*Conditional Change Detection*, where pre-change semantic information is usedas input next to bi-temporal images. To fully exploit the extra information, wepropose *MapFormer*, a novel architecture based on a multi-modal feature fusionmodule that allows for feature processing conditioned on the available semanticinformation. We further employ a supervised, cross-modal contrastive loss toguide the learning of visual representations. Our approach outperforms existingchange detection methods by an absolute 11.7\% and 18.4\% in terms of binarychange IoU on DynamicEarthNet and HRSCD, respectively. Furthermore, wedemonstrate the robustness of our approach to the quality of the pre-changesemantic information and the absence pre-change imagery. The code is availableat https://github.com/mxbh/mapformer.</description><author>Maximilian Bernhard, Niklas Strauß, Matthias Schubert</author><pubDate>Thu, 07 Dec 2023 16:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17859v4</guid></item><item><title>Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications</title><link>http://arxiv.org/abs/2301.00752v4</link><description>This study demonstrates the feasibility of point cloud-based proactive linkquality prediction for millimeter-wave (mmWave) communications. Previousstudies have proposed machine learning-based methods to predict received signalstrength for future time periods using time series of depth images to mitigatethe line-of-sight (LOS) path blockage by pedestrians in mmWave communication.However, these image-based methods have limited applicability due to privacyconcerns as camera images may contain sensitive information. This studyproposes a point cloud-based method for mmWave link quality prediction anddemonstrates its feasibility through experiments. Point clouds representthree-dimensional (3D) spaces as a set of points and are sparser and lesslikely to contain sensitive information than camera images. Additionally, pointclouds provide 3D position and motion information, which is necessary forunderstanding the radio propagation environment involving pedestrians. Thisstudy designs the mmWave link quality prediction method and conducts realisticindoor experiments, where the link quality fluctuates significantly due tohuman blockage, using commercially available IEEE 802.11ad-based 60 GHzwireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 lightdetection and ranging (LiDAR) for point cloud acquisition. The experimentalresults showed that our proposed method can predict future large attenuation ofmmWave received signal strength and throughput induced by the LOS path blockageby pedestrians with comparable or superior accuracy to image-based predictionmethods. Hence, our point cloud-based method can serve as a viable alternativeto image-based methods.</description><author>Shoki Ohta, Takayuki Nishio, Riichi Kudo, Kahoko Takahashi, Hisashi Nagata</author><pubDate>Thu, 07 Dec 2023 16:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00752v4</guid></item><item><title>Temporal Fairness in Multiwinner Voting</title><link>http://arxiv.org/abs/2312.04417v1</link><description>Multiwinner voting captures a wide variety of settings, from parliamentaryelections in democratic systems to product placement in online shoppingplatforms. There is a large body of work dealing with axiomaticcharacterizations, computational complexity, and algorithmic analysis ofmultiwinner voting rules. Although many challenges remain, significant progresshas been made in showing existence of fair and representative outcomes as wellas efficient algorithmic solutions for many commonly studied settings. However,much of this work focuses on single-shot elections, even though in numerousreal-world settings elections are held periodically and repeatedly. Hence, itis imperative to extend the study of multiwinner voting to temporal settings.Recently, there have been several efforts to address this challenge. However,these works are difficult to compare, as they model multi-period voting in verydifferent ways. We propose a unified framework for studying temporal fairnessin this domain, drawing connections with various existing bodies of work, andconsolidating them within a general framework. We also identify gaps inexisting literature, outline multiple opportunities for future work, and putforward a vision for the future of multiwinner voting in temporal settings.</description><author>Edith Elkind, Svetlana Obratzsova, Nicholas Teh</author><pubDate>Thu, 07 Dec 2023 16:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04417v1</guid></item><item><title>Monitoring Sustainable Global Development Along Shared Socioeconomic Pathways</title><link>http://arxiv.org/abs/2312.04416v1</link><description>Sustainable global development is one of the most prevalent challenges facingthe world today, hinging on the equilibrium between socioeconomic growth andenvironmental sustainability. We propose approaches to monitor and quantifysustainable development along the Shared Socioeconomic Pathways (SSPs),including mathematically derived scoring algorithms, and machine learningmethods. These integrate socioeconomic and environmental datasets, to producean interpretable metric for SSP alignment. An initial study demonstratespromising results, laying the groundwork for the application of differentmethods to the monitoring of sustainable global development.</description><author>Michelle W. L. Wan, Jeffrey N. Clark, Edward A. Small, Elena Fillola Mayoral, Raúl Santos-Rodríguez</author><pubDate>Thu, 07 Dec 2023 16:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04416v1</guid></item><item><title>MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds</title><link>http://arxiv.org/abs/2212.07207v5</link><description>The sensing process of large-scale LiDAR point clouds inevitably causes largeblind spots, i.e. regions not visible to the sensor. We demonstrate how theseinherent sampling properties can be effectively utilized for self-supervisedrepresentation learning by designing a highly effective pre-training frameworkthat considerably reduces the need for tedious 3D annotations to trainstate-of-the-art object detectors. Our Masked AutoEncoder for LiDAR pointclouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in boththe encoder and decoder during reconstruction. This results in more expressiveand useful initialization, which can be directly applied to downstreamperception tasks, such as 3D object detection or semantic segmentation forautonomous driving. In a novel reconstruction approach, MAELi distinguishesbetween empty and occluded space and employs a new masking strategy thattargets the LiDAR's inherent spherical projection. Thereby, without any groundtruth whatsoever and trained on single frames only, MAELi obtains anunderstanding of the underlying 3D scene geometry and semantics. To demonstratethe potential of MAELi, we pre-train backbones in an end-to-end manner and showthe effectiveness of our unsupervised pre-trained weights on the tasks of 3Dobject detection and semantic segmentation.</description><author>Georg Krispel, David Schinagl, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof</author><pubDate>Thu, 07 Dec 2023 16:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07207v5</guid></item><item><title>DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics</title><link>http://arxiv.org/abs/2306.03088v2</link><description>Functional brain dynamics is supported by parallel and overlapping functionalnetwork modes that are associated with specific neural circuits. Decomposingthese network modes from fMRI data and finding their temporal characteristicsis challenging due to their time-varying nature and the non-linearity of thefunctional dynamics. Dynamic Mode Decomposition (DMD) algorithms have beenquite popular for solving this decomposition problem in recent years. In thiswork, we apply GraphDMD -- an extension of the DMD for network data -- toextract the dynamic network modes and their temporal characteristics from thefMRI time series in an interpretable manner. GraphDMD, however, regards theunderlying system as a linear dynamical system that is sub-optimal forextracting the network modes from non-linear functional data. In this work, wedevelop a generalized version of the GraphDMD algorithm -- DeepGraphDMD --applicable to arbitrary non-linear graph dynamical systems. DeepGraphDMD is anautoencoder-based deep learning model that learns Koopman eigenfunctions forgraph data and embeds the non-linear graph dynamics into a latent linear space.We show the effectiveness of our method in both simulated data and the HCPresting-state fMRI data. In the HCP data, DeepGraphDMD provides novel insightsinto cognitive brain functions by discovering two major network modes relatedto fluid and crystallized intelligence.</description><author>Md Asadullah Turja, Martin Styner, Guorong Wu</author><pubDate>Thu, 07 Dec 2023 16:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03088v2</guid></item><item><title>Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models</title><link>http://arxiv.org/abs/2312.04410v1</link><description>Recently, diffusion models have made remarkable progress in text-to-image(T2I) generation, synthesizing images with high fidelity and diverse contents.Despite this advancement, latent space smoothness within diffusion modelsremains largely unexplored. Smooth latent spaces ensure that a perturbation onan input latent corresponds to a steady change in the output image. Thisproperty proves beneficial in downstream tasks, including image interpolation,inversion, and editing. In this work, we expose the non-smoothness of diffusionlatent spaces by observing noticeable visual fluctuations resulting from minorlatent variations. To tackle this issue, we propose Smooth Diffusion, a newcategory of diffusion models that can be simultaneously high-performing andsmooth. Specifically, we introduce Step-wise Variation Regularization toenforce the proportion between the variations of an arbitrary input latent andthat of the output image is a constant at any diffusion training step. Inaddition, we devise an interpolation standard deviation (ISTD) metric toeffectively assess the latent space smoothness of a diffusion model. Extensivequantitative and qualitative experiments demonstrate that Smooth Diffusionstands out as a more desirable solution not only in T2I generation but alsoacross various downstream tasks. Smooth Diffusion is implemented as aplug-and-play Smooth-LoRA to work with various community models. Code isavailable at https://github.com/SHI-Labs/Smooth-Diffusion.</description><author>Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, Humphrey Shi</author><pubDate>Thu, 07 Dec 2023 16:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04410v1</guid></item><item><title>On the Impact of Multi-dimensional Local Differential Privacy on Fairness</title><link>http://arxiv.org/abs/2312.04404v1</link><description>Automated decision systems are increasingly used to make consequentialdecisions in people's lives. Due to the sensitivity of the manipulated data aswell as the resulting decisions, several ethical concerns need to be addressedfor the appropriate use of such technologies, in particular, fairness andprivacy. Unlike previous work, which focused on centralized differentialprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,we examine the impact of LDP in the presence of several sensitive attributes(i.e., multi-dimensional data) on fairness. Detailed empirical analysis onsynthetic and benchmark datasets revealed very relevant observations. Inparticular, (1) multi-dimensional LDP is an efficient approach to reducedisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)matters only at low privacy guarantees, and (3) the outcome Y distribution hasan important effect on which group is more sensitive to the obfuscation. Last,we summarize our findings in the form of recommendations to guide practitionersin adopting effective privacy-preserving practices while maintaining fairnessand utility in ML applications.</description><author>karima Makhlouf, Heber H. Arcolezi, Sami Zhioua, Ghassen Ben Brahim, Catuscia Palamidessi</author><pubDate>Thu, 07 Dec 2023 16:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04404v1</guid></item><item><title>OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization</title><link>http://arxiv.org/abs/2312.04403v1</link><description>Vision-language pre-training (VLP) models demonstrate impressive abilities inprocessing both images and text. However, they are vulnerable to multi-modaladversarial examples (AEs). Investigating the generation ofhigh-transferability adversarial examples is crucial for uncovering VLP models'vulnerabilities in practical scenarios. Recent works have indicated thatleveraging data augmentation and image-text modal interactions can enhance thetransferability of adversarial examples for VLP models significantly. However,they do not consider the optimal alignment problem between dataaugmentedimage-text pairs. This oversight leads to adversarial examples that are overlytailored to the source model, thus limiting improvements in transferability. Inour research, we first explore the interplay between image sets producedthrough data augmentation and their corresponding text sets. We find thataugmented image samples can align optimally with certain texts while exhibitingless relevance to others. Motivated by this, we propose an OptimalTransport-based Adversarial Attack, dubbed OT-Attack. The proposed methodformulates the features of image and text sets as two distinct distributionsand employs optimal transport theory to determine the most efficient mappingbetween them. This optimal mapping informs our generation of adversarialexamples to effectively counteract the overfitting issues. Extensiveexperiments across various network architectures and datasets in image-textmatching tasks reveal that our OT-Attack outperforms existing state-of-the-artmethods in terms of adversarial transferability.</description><author>Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, Xiaochun Cao</author><pubDate>Thu, 07 Dec 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04403v1</guid></item><item><title>Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning</title><link>http://arxiv.org/abs/2312.04402v1</link><description>Semantic segmentation enables robots to perceive and reason about theirenvironments beyond geometry. Most of such systems build upon deep learningapproaches. As autonomous robots are commonly deployed in initially unknownenvironments, pre-training on static datasets cannot always capture the varietyof domains and limits the robot's perception performance during missions.Recently, self-supervised and fully supervised active learning methods emergedto improve a robot's vision. These approaches rely on large in-domainpre-training datasets or require substantial human labelling effort. We proposea planning method for semi-supervised active learning of semantic segmentationthat substantially reduces human labelling requirements compared to fullysupervised approaches. We leverage an adaptive map-based planner guided towardsthe frontiers of unexplored space with high model uncertainty collectingtraining data for human labelling. A key aspect of our approach is to combinethe sparse high-quality human labels with pseudo labels automatically extractedfrom highly certain environment map areas. Experimental results show that ourmethod reaches segmentation performance close to fully supervised approacheswith drastically reduced human labelling effort while outperformingself-supervised approaches.</description><author>Julius Rückin, Federico Magistri, Cyrill Stachniss, Marija Popović</author><pubDate>Thu, 07 Dec 2023 16:16:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04402v1</guid></item><item><title>Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control</title><link>http://arxiv.org/abs/2210.12583v3</link><description>Model-based control requires an accurate model of the system dynamics forprecisely and safely controlling the robot in complex and dynamic environments.Moreover, in the presence of variations in the operating conditions, the modelshould be continuously refined to compensate for dynamics changes. In thispaper, we present a self-supervised learning approach that actively models thedynamics of nonlinear robotic systems. We combine offline learning from pastexperience and online learning from current robot interaction with the unknownenvironment. These two ingredients enable a highly sample-efficient andadaptive learning process, capable of accurately inferring model dynamics inreal-time even in operating regimes that greatly differ from the trainingdistribution. Moreover, we design an uncertainty-aware model predictivecontroller that is heuristically conditioned to the aleatoric (data)uncertainty of the learned dynamics. This controller actively chooses theoptimal control actions that (i) optimize the control performance and (ii)improve the efficiency of online learning sample collection. We demonstrate theeffectiveness of our method through a series of challenging real-worldexperiments using a quadrotor system. Our approach showcases high resilienceand generalization capabilities by consistently adapting to unseen flightconditions, while it significantly outperforms classical and adaptive controlbaselines.</description><author>Alessandro Saviolo, Jonathan Frey, Abhishek Rathod, Moritz Diehl, Giuseppe Loianno</author><pubDate>Thu, 07 Dec 2023 16:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12583v3</guid></item><item><title>Sem@$K$: Is my knowledge graph embedding model semantic-aware?</title><link>http://arxiv.org/abs/2301.05601v2</link><description>Using knowledge graph embedding models (KGEMs) is a popular approach forpredicting links in knowledge graphs (KGs). Traditionally, the performance ofKGEMs for link prediction is assessed using rank-based metrics, which evaluatetheir ability to give high scores to ground-truth entities. However, theliterature claims that the KGEM evaluation procedure would benefit from addingsupplementary dimensions to assess. That is why, in this paper, we extend ourpreviously introduced metric Sem@K that measures the capability of models topredict valid entities w.r.t. domain and range constraints. In particular, weconsider a broad range of KGs and take their respective characteristics intoaccount to propose different versions of Sem@K. We also perform an extensivestudy to qualify the abilities of KGEMs as measured by our metric. Ourexperiments show that Sem@K provides a new perspective on KGEM quality. Itsjoint analysis with rank-based metrics offers different conclusions on thepredictive power of models. Regarding Sem@K, some KGEMs are inherently betterthan others, but this semantic superiority is not indicative of theirperformance w.r.t. rank-based metrics. In this work, we generalize conclusionsabout the relative performance of KGEMs w.r.t. rank-based and semantic-orientedmetrics at the level of families of models. The joint analysis of theaforementioned metrics gives more insight into the peculiarities of each model.This work paves the way for a more comprehensive evaluation of KGEM adequacyfor specific downstream tasks.</description><author>Nicolas Hubert, Pierre Monnin, Armelle Brun, Davy Monticolo</author><pubDate>Thu, 07 Dec 2023 16:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05601v2</guid></item><item><title>Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning</title><link>http://arxiv.org/abs/2312.04398v1</link><description>The burgeoning navigation services using digital maps provide greatconvenience to drivers. Nevertheless, the presence of anomalies in lanerendering map images occasionally introduces potential hazards, as suchanomalies can be misleading to human drivers and consequently contribute tounsafe driving conditions. In response to this concern and to accurately andeffectively detect the anomalies, this paper transforms lane rendering imageanomaly detection into a classification problem and proposes a four-phasepipeline consisting of data pre-processing, self-supervised pre-training withthe masked image modeling (MiM) method, customized fine-tuning usingcross-entropy based loss with label smoothing, and post-processing to tackle itleveraging state-of-the-art deep learning techniques, especially thoseinvolving Transformer models. Various experiments verify the effectiveness ofthe proposed pipeline. Results indicate that the proposed pipeline exhibitssuperior performance in lane rendering image anomaly detection, and notably,the self-supervised pre-training with MiM can greatly enhance the detectionaccuracy while significantly reducing the total training time. For instance,employing the Swin Transformer with Uniform Masking as self-supervisedpretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and animproved Area Under The Curve (AUC) score of 0.9743 compared with the pure SwinTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and anAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from theoriginal 280. In conclusion, the proposed pipeline, with its incorporation ofself-supervised pre-training using MiM and other advanced deep learningtechniques, emerges as a robust solution for enhancing the accuracy andefficiency of lane rendering image anomaly detection in digital navigationsystems.</description><author>Yongqi Dong, Xingmin Lu, Ruohan Li, Wei Song, Bart van Arem, Haneen Farah</author><pubDate>Thu, 07 Dec 2023 16:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04398v1</guid></item><item><title>PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction</title><link>http://arxiv.org/abs/2312.04393v1</link><description>Humans interact with objects all the time. Enabling a humanoid to learnhuman-object interaction (HOI) is a key step for future smart animation andintelligent robotics systems. However, recent progress in physics-based HOIrequires carefully designed task-specific rewards, making the system unscalableand labor-intensive. This work focuses on dynamic HOI imitation: teachinghumanoid dynamic interaction skills through imitating kinematic HOIdemonstrations. It is quite challenging because of the complexity of theinteraction between body parts and objects and the lack of dynamic HOI data. Tohandle the above issues, we present PhysHOI, the first physics-based whole-bodyHOI imitation approach without task-specific reward designs. Except for thekinematic HOI representations of humans and objects, we introduce the contactgraph to model the contact relations between body parts and objects explicitly.A contact graph reward is also designed, which proved to be critical forprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverseHOI tasks simply yet effectively without prior knowledge. To make up for thelack of dynamic HOI scenarios in this area, we introduce the BallPlay datasetthat contains eight whole-body basketball skills. We validate PhysHOI ondiverse HOI tasks, including whole-body grasping and basketball skills.</description><author>Yinhuai Wang, Jing Lin, Ailing Zeng, Zhengyi Luo, Jian Zhang, Lei Zhang</author><pubDate>Thu, 07 Dec 2023 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04393v1</guid></item><item><title>Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case</title><link>http://arxiv.org/abs/2306.00857v2</link><description>The Classification Tree (CT) is one of the most common models ininterpretable machine learning. Although such models are usually built withgreedy strategies, in recent years, thanks to remarkable advances inMixer-Integer Programming (MIP) solvers, several exact formulations of thelearning problem have been developed. In this paper, we argue that some of themost relevant ones among these training models can be encapsulated within ageneral framework, whose instances are shaped by the specification of lossfunctions and regularizers. Next, we introduce a novel realization of thisframework: specifically, we consider the logistic loss, handled in the MIPsetting by a linear piece-wise approximation, and couple it with$\ell_1$-regularization terms. The resulting Optimal Logistic Tree modelnumerically proves to be able to induce trees with enhanced interpretabilityfeatures and competitive generalization capabilities, compared to thestate-of-the-art MIP-based approaches.</description><author>Tommaso Aldinucci, Matteo Lapucci</author><pubDate>Thu, 07 Dec 2023 16:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00857v2</guid></item><item><title>Conversational Semantic Parsing using Dynamic Context Graphs</title><link>http://arxiv.org/abs/2305.06164v2</link><description>In this paper we consider the task of conversational semantic parsing overgeneral purpose knowledge graphs (KGs) with millions of entities, and thousandsof relation-types. We focus on models which are capable of interactivelymapping user utterances into executable logical forms (e.g., Sparql) in thecontext of the conversational history. Our key idea is to represent informationabout an utterance and its context via a subgraph which is created dynamically,i.e., the number of nodes varies per utterance. Rather than treating thesubgraph as a sequence, we exploit its underlying structure and encode it witha graph neural network which further allows us to represent a large number of(unseen) nodes. Experimental results show that dynamic context modeling issuperior to static approaches, delivering performance improvements across theboard (i.e., for simple and complex questions). Our results further confirmthat modeling the structure of context is better at processing discourseinformation, (i.e., at handling ellipsis and resolving coreference) and longerinteractions.</description><author>Parag Jain, Mirella Lapata</author><pubDate>Thu, 07 Dec 2023 15:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06164v2</guid></item><item><title>Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization</title><link>http://arxiv.org/abs/2312.04386v1</link><description>We consider the problem of quantifying uncertainty over expected cumulativerewards in model-based reinforcement learning. In particular, we focus oncharacterizing the variance over values induced by a distribution over MDPs.Previous work upper bounds the posterior variance over values by solving aso-called uncertainty Bellman equation (UBE), but the over-approximation mayresult in inefficient exploration. We propose a new UBE whose solutionconverges to the true posterior variance over values and leads to lower regretin tabular exploration problems. We identify challenges to apply the UBE theorybeyond tabular problems and propose a suitable approximation. Based on thisapproximation, we introduce a general-purpose policy optimization algorithm,Q-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for eitherrisk-seeking or risk-averse policy optimization with minimal changes.Experiments in both online and offline RL demonstrate improved performancecompared to other uncertainty estimation methods.</description><author>Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</author><pubDate>Thu, 07 Dec 2023 15:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04386v1</guid></item><item><title>AniRes2D: Anisotropic Residual-enhanced Diffusion for 2D MR Super-Resolution</title><link>http://arxiv.org/abs/2312.04385v1</link><description>Anisotropic low-resolution (LR) magnetic resonance (MR) images are fast toobtain but hinder automated processing. We propose to use denoising diffusionprobabilistic models (DDPMs) to super-resolve these 2D-acquired LR MR slices.This paper introduces AniRes2D, a novel approach combining DDPM with a residualprediction for 2D super-resolution (SR). Results demonstrate that AniRes2Doutperforms several other DDPM-based models in quantitative metrics, visualquality, and out-of-domain evaluation. We use a trained AniRes2D tosuper-resolve 3D volumes slice by slice, where comparative quantitative resultsand reduced skull aliasing are achieved compared to a recent state-of-the-artself-supervised 3D super-resolution method. Furthermore, we explored the use ofnoise conditioning augmentation (NCA) as an alternative augmentation techniquefor DDPM-based SR models, but it was found to reduce performance. Our findingscontribute valuable insights to the application of DDPMs for SR of anisotropicMR images.</description><author>Zejun Wu, Samuel W. Remedios, Blake E. Dewey, Aaron Carass, Jerry L. Prince</author><pubDate>Thu, 07 Dec 2023 15:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04385v1</guid></item><item><title>Adversarial Denoising Diffusion Model for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2312.04382v1</link><description>In this paper, we propose the Adversarial Denoising Diffusion Model (ADDM).The ADDM is based on the Denoising Diffusion Probabilistic Model (DDPM) butcomplementarily trained by adversarial learning. The proposed adversariallearning is achieved by classifying model-based denoised samples and samples towhich random Gaussian noise is added to a specific sampling step. With theaddition of explicit adversarial learning on data samples, ADDM can learn thesemantic characteristics of the data more robustly during training, whichachieves a similar data sampling performance with much fewer sampling stepsthan DDPM. We apply ADDM to anomaly detection in unsupervised MRI images.Experimental results show that the proposed ADDM outperformed existinggenerative model-based unsupervised anomaly detection methods. In particular,compared to other DDPM-based anomaly detection methods, the proposed ADDM showsbetter performance with the same number of sampling steps and similarperformance with 50% fewer sampling steps.</description><author>Jongmin Yu, Hyeontaek Oh, Jinhong Yang</author><pubDate>Thu, 07 Dec 2023 15:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04382v1</guid></item><item><title>How much informative is your XAI? A decision-making assessment task to objectively measure the goodness of explanations</title><link>http://arxiv.org/abs/2312.04379v1</link><description>There is an increasing consensus about the effectiveness of user-centredapproaches in the explainable artificial intelligence (XAI) field. Indeed, thenumber and complexity of personalised and user-centred approaches to XAI haverapidly grown in recent years. Often, these works have a two-fold objective:(1) proposing novel XAI techniques able to consider the users and (2) assessingthe \textit{goodness} of such techniques with respect to others. From these newworks, it emerged that user-centred approaches to XAI positively affect theinteraction between users and systems. However, so far, the goodness of XAIsystems has been measured through indirect measures, such as performance. Inthis paper, we propose an assessment task to objectively and quantitativelymeasure the goodness of XAI systems in terms of their \textit{informationpower}, which we intended as the amount of information the system provides tothe users during the interaction. Moreover, we plan to use our task toobjectively compare two XAI techniques in a human-robot decision-making task tounderstand deeper whether user-centred approaches are more informative thanclassical ones.</description><author>Marco Matarese, Francesco Rea, Alessandra Sciutti</author><pubDate>Thu, 07 Dec 2023 15:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04379v1</guid></item><item><title>Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Informed Neural Network for Autonomous Racing</title><link>http://arxiv.org/abs/2312.04374v1</link><description>Autonomous racing is a critical research area for autonomous driving,presenting significant challenges in vehicle dynamics modeling, such asbalancing model precision and computational efficiency at high speeds(&gt;280kmph), where minor errors in modeling have severe consequences. Existingphysics-based models for vehicle dynamics require elaborate testing setups andtuning, which are hard to implement, time-intensive, and cost-prohibitive.Conversely, purely data-driven approaches do not generalize well and cannotadequately ensure physical constraints on predictions. This paper introducesDeep Dynamics, a physics-informed neural network (PINN) for vehicle dynamicsmodeling of an autonomous racecar. It combines physics coefficient estimationand dynamical equations to accurately predict vehicle states at high speeds andincludes a unique Physics Guard layer to ensure internal coefficient estimatesremain within their nominal physical ranges. Open-loop and closed-loopperformance assessments, using a physics-based simulator and full-scaleautonomous Indy racecar data, highlight Deep Dynamics as a promising approachfor modeling racecar vehicle dynamics.</description><author>John Chrosniak, Jingyun Ning, Madhur Behl</author><pubDate>Thu, 07 Dec 2023 15:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04374v1</guid></item><item><title>LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs</title><link>http://arxiv.org/abs/2312.04372v1</link><description>We present LaMPilot, a novel framework for planning in the field ofautonomous driving, rethinking the task as a code-generation process thatleverages established behavioral primitives. This approach aims to address thechallenge of interpreting and executing spontaneous user instructions such as"overtake the car ahead," which have typically posed difficulties for existingframeworks. We introduce the LaMPilot benchmark specifically designed toquantitatively evaluate the efficacy of Large Language Models (LLMs) intranslating human directives into actionable driving policies. We then evaluatea wide range of state-of-the-art code generation language models on tasks fromthe LaMPilot Benchmark. The results of the experiments showed that GPT-4, withhuman feedback, achieved an impressive task completion rate of 92.7% and aminimal collision rate of 0.9%. To encourage further investigation in thisarea, our code and dataset will be made available.</description><author>Yunsheng Ma, Can Cui, Xu Cao, Wenqian Ye, Peiran Liu, Juanwu Lu, Amr Abdelraouf, Rohit Gupta, Kyungtae Han, Aniket Bera, James M. Rehg, Ziran Wang</author><pubDate>Thu, 07 Dec 2023 15:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04372v1</guid></item><item><title>A Scalable Network-Aware Multi-Agent Reinforcement Learning Framework for Decentralized Inverter-based Voltage Control</title><link>http://arxiv.org/abs/2312.04371v1</link><description>This paper addresses the challenges associated with decentralized voltagecontrol in power grids due to an increase in distributed generations (DGs).Traditional model-based voltage control methods struggle with the rapid energyfluctuations and uncertainties of these DGs. While multi-agent reinforcementlearning (MARL) has shown potential for decentralized secondary control,scalability issues arise when dealing with a large number of DGs. This problemlies in the dominant centralized training and decentralized execution (CTDE)framework, where the critics take global observations and actions. To overcomethese challenges, we propose a scalable network-aware (SNA) framework thatleverages network structure to truncate the input to the critic's Q-function,thereby improving scalability and reducing communication costs during training.Further, the SNA framework is theoretically grounded with provableapproximation guarantee, and it can seamlessly integrate with multiplemulti-agent actor-critic algorithms. The proposed SNA framework is successfullydemonstrated in a system with 114 DGs, providing a promising solution fordecentralized voltage control in increasingly complex power grid systems.</description><author>Han Xu, Jialin Zheng, Guannan Qu</author><pubDate>Thu, 07 Dec 2023 15:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04371v1</guid></item><item><title>Investigating the Design Space of Diffusion Models for Speech Enhancement</title><link>http://arxiv.org/abs/2312.04370v1</link><description>Diffusion models are a new class of generative models that have shownoutstanding performance in image generation literature. As a consequence,studies have attempted to apply diffusion models to other tasks, such as speechenhancement. A popular approach in adapting diffusion models to speechenhancement consists in modelling a progressive transformation between theclean and noisy speech signals. However, one popular diffusion model frameworkpreviously laid in image generation literature did not account for such atransformation towards the system input, which prevents from relating theexisting diffusion-based speech enhancement systems with the aforementioneddiffusion model framework. To address this, we extend this framework to accountfor the progressive transformation between the clean and noisy speech signals.This allows us to apply recent developments from image generation literature,and to systematically investigate design aspects of diffusion models thatremain largely unexplored for speech enhancement, such as the neural networkpreconditioning, the training loss weighting, the stochastic differentialequation (SDE), or the amount of stochasticity injected in the reverse process.We show that the performance of previous diffusion-based speech enhancementsystems cannot be attributed to the progressive transformation between theclean and noisy speech signals. Moreover, we show that a proper choice ofpreconditioning, training loss weighting, SDE and sampler allows to outperforma popular diffusion-based speech enhancement system in terms of perceptualmetrics while using fewer sampling steps, thus reducing the computational costby a factor of four.</description><author>Philippe Gonzalez, Zheng-Hua Tan, Jan Østergaard, Jesper Jensen, Tommy Sonne Alstrøm, Tobias May</author><pubDate>Thu, 07 Dec 2023 15:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04370v1</guid></item><item><title>SingingHead: A Large-scale 4D Dataset for Singing Head Animation</title><link>http://arxiv.org/abs/2312.04369v1</link><description>Singing, as a common facial movement second only to talking, can be regardedas a universal language across ethnicities and cultures, plays an importantrole in emotional communication, art, and entertainment. However, it is oftenoverlooked in the field of audio-driven facial animation due to the lack ofsinging head datasets and the domain gap between singing and talking in rhythmand amplitude. To this end, we collect a high-quality large-scale singing headdataset, SingingHead, which consists of more than 27 hours of synchronizedsinging video, 3D facial motion, singing audio, and background music from 76individuals and 8 types of music. Along with the SingingHead dataset, we arguethat 3D and 2D facial animation tasks can be solved together, and propose aunified singing facial animation framework named UniSinger to achieve bothsinging audio-driven 3D singing head animation and 2D singing portrait videosynthesis. Extensive comparative experiments with both SOTA 3D facial animationand 2D portrait animation methods demonstrate the necessity of singing-specificdatasets in singing head animation tasks and the promising performance of ourunified facial animation framework.</description><author>Sijing Wu, Yunhao Li, Weitian Zhang, Jun Jia, Yucheng Zhu, Yichao Yan, Guangtao Zhai</author><pubDate>Thu, 07 Dec 2023 15:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04369v1</guid></item><item><title>DemoCaricature: Democratising Caricature Generation with a Rough Sketch</title><link>http://arxiv.org/abs/2312.04364v1</link><description>In this paper, we democratise caricature generation, empowering individualsto effortlessly craft personalised caricatures with just a photo and aconceptual sketch. Our objective is to strike a delicate balance betweenabstraction and identity, while preserving the creativity and subjectivityinherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editingalongside single-image personalisation, selectively applying nuanced edits tocross-attention layers for a seamless merge of identity and style.Additionally, we propose Random Mask Reconstruction to enhance robustness,directing the model to focus on distinctive identity and style features.Crucially, our aim is not to replace artists but to eliminate accessibilitybarriers, allowing enthusiasts to engage in the artistry.</description><author>Dar-Yen Chen, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Ayan Kumar Bhunia, Yi-Zhe Song</author><pubDate>Thu, 07 Dec 2023 15:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04364v1</guid></item><item><title>LLMs for Science: Usage for Code Generation and Data Analysis</title><link>http://arxiv.org/abs/2311.16733v3</link><description>Large language models (LLMs) have been touted to enable increasedproductivity in many areas of today's work life. Scientific research as an areaof work is no exception: the potential of LLM-based tools to assist in thedaily work of scientists has become a highly discussed topic acrossdisciplines. However, we are only at the very onset of this subject of study.It is still unclear how the potential of LLMs will materialise in researchpractice. With this study, we give first empirical evidence on the use of LLMsin the research process. We have investigated a set of use cases for LLM-basedtools in scientific research, and conducted a first study to assess to whichdegree current tools are helpful. In this paper we report specifically on usecases related to software engineering, such as generating application code anddeveloping scripts for data analytics. While we studied seemingly simple usecases, results across tools differ significantly. Our results highlight thepromise of LLM-based tools in general, yet we also observe various issues,particularly regarding the integrity of the output these tools provide.</description><author>Mohamed Nejjar, Luca Zacharias, Fabian Stiehle, Ingo Weber</author><pubDate>Thu, 07 Dec 2023 15:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16733v3</guid></item><item><title>PCoQA: Persian Conversational Question Answering Dataset</title><link>http://arxiv.org/abs/2312.04362v1</link><description>Humans seek information regarding a specific topic through performing aconversation containing a series of questions and answers. In the pursuit ofconversational question answering research, we introduce the PCoQA, the first\textbf{P}ersian \textbf{Co}nversational \textbf{Q}uestion \textbf{A}nsweringdataset, a resource comprising information-seeking dialogs encompassing a totalof 9,026 contextually-driven questions. Each dialog involves a questioner, aresponder, and a document from the Wikipedia; The questioner asks severalinter-connected questions from the text and the responder provides a span ofthe document as the answer for each question. PCoQA is designed to presentnovel challenges compared to previous question answering datasets includinghaving more open-ended non-factual answers, longer answers, and fewer lexicaloverlaps. This paper not only presents the comprehensive PCoQA dataset but alsoreports the performance of various benchmark models. Our models includebaseline models and pre-trained models, which are leveraged to boost theperformance of the model. The dataset and benchmarks are available at ourGithub page.</description><author>Hamed Hematian Hemati, Atousa Toghyani, Atena Souri, Sayed Hesam Alavian, Hossein Sameti, Hamid Beigy</author><pubDate>Thu, 07 Dec 2023 15:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04362v1</guid></item><item><title>A Machine Learning Approach to Two-Stage Adaptive Robust Optimization</title><link>http://arxiv.org/abs/2307.12409v2</link><description>We propose an approach based on machine learning to solve two-stage linearadaptive robust optimization (ARO) problems with binary here-and-now variablesand polyhedral uncertainty sets. We encode the optimal here-and-now decisions,the worst-case scenarios associated with the optimal here-and-now decisions,and the optimal wait-and-see decisions into what we denote as the strategy. Wesolve multiple similar ARO instances in advance using the column and constraintgeneration algorithm and extract the optimal strategies to generate a trainingset. We train a machine learning model that predicts high-quality strategiesfor the here-and-now decisions, the worst-case scenarios associated with theoptimal here-and-now decisions, and the wait-and-see decisions. We alsointroduce an algorithm to reduce the number of different target classes themachine learning algorithm needs to be trained on. We apply the proposedapproach to the facility location, the multi-item inventory control and theunit commitment problems. Our approach solves ARO problems drastically fasterthan the state-of-the-art algorithms with high accuracy.</description><author>Dimitris Bertsimas, Cheol Woo Kim</author><pubDate>Thu, 07 Dec 2023 15:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12409v2</guid></item><item><title>NeuJeans: Private Neural Network Inference with Joint Optimization of Convolution and Bootstrapping</title><link>http://arxiv.org/abs/2312.04356v1</link><description>Fully homomorphic encryption (FHE) is a promising cryptographic primitive forrealizing private neural network inference (PI) services by allowing a clientto fully offload the inference task to a cloud server while keeping the clientdata oblivious to the server. This work proposes NeuJeans, an FHE-basedsolution for the PI of deep convolutional neural networks (CNNs). NeuJeanstackles the critical problem of the enormous computational cost for the FHEevaluation of convolutional layers (conv2d), mainly due to the high cost ofdata reordering and bootstrapping. We first propose an encoding methodintroducing nested structures inside encoded vectors for FHE, which enables usto develop efficient conv2d algorithms with reduced data reordering costs.However, the new encoding method also introduces additional computations forconversion between encoding methods, which could negate its advantages. Wediscover that fusing conv2d with bootstrapping eliminates such computationswhile reducing the cost of bootstrapping. Then, we devise optimized executionflows for various types of conv2d and apply them to end-to-end implementationof CNNs. NeuJeans accelerates the performance of conv2d by up to 5.68 timescompared to state-of-the-art FHE-based PI work and performs the PI of a CNN atthe scale of ImageNet (ResNet18) within a mere few seconds</description><author>Jae Hyung Ju, Jaiyoung Park, Jongmin Kim, Donghwan Kim, Jung Ho Ahn</author><pubDate>Thu, 07 Dec 2023 15:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04356v1</guid></item></channel></rss>