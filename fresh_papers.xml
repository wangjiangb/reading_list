<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 Jul 2024 13:00:16 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Floating No More: Object-Ground Reconstruction from a Single Image</title><link>http://arxiv.org/abs/2407.18914v1</link><description>Recent advancements in 3D object reconstruction from single images haveprimarily focused on improving the accuracy of object shapes. Yet, thesetechniques often fail to accurately capture the inter-relation between theobject, ground, and camera. As a result, the reconstructed objects often appearfloating or tilted when placed on flat surfaces. This limitation significantlyaffects 3D-aware image editing applications like shadow rendering and objectpose manipulation. To address this issue, we introduce ORG (ObjectReconstruction with Ground), a novel task aimed at reconstructing 3D objectgeometry in conjunction with the ground surface. Our method uses two compactpixel-level representations to depict the relationship between camera, object,and ground. Experiments show that the proposed ORG model can effectivelyreconstruct object-ground geometry on unseen data, significantly enhancing thequality of shadow generation and pose manipulation compared to conventionalsingle-image 3D reconstruction techniques.</description><author>Yunze Man, Yichen Sheng, Jianming Zhang, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Fri, 26 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18914v1</guid></item><item><title>SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments</title><link>http://arxiv.org/abs/2407.18913v1</link><description>This work compares ways of extending Reinforcement Learning algorithms toPartially Observed Markov Decision Processes (POMDPs) with options. One view ofoptions is as temporally extended action, which can be realized as a memorythat allows the agent to retain historical information beyond the policy'scontext window. While option assignment could be handled using heuristics andhand-crafted objectives, learning temporally consistent options and associatedsub-policies without explicit supervision is a challenge. Two algorithms, PPOEMand SOAP, are proposed and studied in depth to address this problem. PPOEMapplies the forward-backward algorithm (for Hidden Markov Models) to optimizethe expected returns for an option-augmented policy. However, this learningapproach is unstable during on-policy rollouts. It is also unsuited forlearning causal policies without the knowledge of future trajectories, sinceoption assignments are optimized for offline sequences where the entire episodeis available. As an alternative approach, SOAP evaluates the policy gradientfor an optimal option assignment. It extends the concept of the generalizedadvantage estimation (GAE) to propagate option advantages through time, whichis an analytical equivalent to performing temporal back-propagation of optionpolicy gradients. This option policy is only conditional on the history of theagent, not future actions. Evaluated against competing baselines, SOAPexhibited the most robust performance, correctly discovering options for POMDPcorridor environments, as well as on standard benchmarks including Atari andMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. Theopen-sourced code is available at https://github.com/shuishida/SoapRL.</description><author>Shu Ishida, Jo√£o F. Henriques</author><pubDate>Fri, 26 Jul 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18913v1</guid></item><item><title>HRP: Human Affordances for Robotic Pre-Training</title><link>http://arxiv.org/abs/2407.18911v1</link><description>In order to *generalize* to various tasks in the wild, robotic agents willneed a suitable representation (i.e., vision network) that enables the robot topredict optimal actions given high dimensional vision inputs. However, learningsuch a representation requires an extreme amount of diverse training data,which is prohibitively expensive to collect on a real robot. How can weovercome this problem? Instead of collecting more robot data, this paperproposes using internet-scale, human videos to extract "affordances," both atthe environment and agent level, and distill them into a pre-trainedrepresentation. We present a simple framework for pre-training representationson hand, object, and contact "affordance labels" that highlight relevantobjects in images and how to interact with them. These affordances areautomatically extracted from human video data (with the help of off-the-shelfcomputer vision modules) and used to fine-tune existing representations. Ourapproach can efficiently fine-tune *any* existing representation, and resultsin models with stronger downstream robotic performance across the board. Weexperimentally demonstrate (using 3000+ robot trials) that this affordancepre-training scheme boosts performance by a minimum of 15% on 5 real-worldtasks, which consider three diverse robot morphologies (including a dexteroushand). Unlike prior works in the space, these representations improveperformance across 3 different camera views. Quantitatively, we find that ourapproach leads to higher levels of generalization in out-of-distributionsettings. For code, weights, and data check: https://hrp-robot.github.io</description><author>Mohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, Abhinav Gupta</author><pubDate>Fri, 26 Jul 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18911v1</guid></item><item><title>Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation</title><link>http://arxiv.org/abs/2407.18910v1</link><description>The efficiency and scalability of graph convolution networks (GCNs) intraining recommender systems (RecSys) have been persistent concerns, hinderingtheir deployment in real-world applications. This paper presents a criticalexamination of the necessity of graph convolutions during the training phaseand introduces an innovative alternative: the Light Post-Training GraphOrdinary-Differential-Equation (LightGODE). Our investigation reveals that thebenefits of GCNs are more pronounced during testing rather than training.Motivated by this, LightGODE utilizes a novel post-training graph convolutionmethod that bypasses the computation-intensive message passing of GCNs andemploys a non-parametric continuous graph ordinary-differential-equation (ODE)to dynamically model node representations. This approach drastically reducestraining time while achieving fine-grained post-training graph convolution toavoid the distortion of the original training embedding space, termed theembedding discrepancy issue. We validate our model across several real-worlddatasets of different scales, demonstrating that LightGODE not only outperformsGCN-based models in terms of efficiency and effectiveness but alsosignificantly mitigates the embedding discrepancy commonly associated withdeeper graph convolution layers. Our LightGODE challenges the prevailingparadigms in RecSys training and suggests re-evaluating the role of graphconvolutions, potentially guiding future developments of efficient large-scalegraph-based RecSys.</description><author>Weizhi Zhang, Liangwei Yang, Zihe Song, Henry Peng Zou, Ke Xu, Henry Peng Zou, Liancheng Fang, Philip S. Yu</author><pubDate>Fri, 26 Jul 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18910v1</guid></item><item><title>Hybrid summary statistics: neural weak lensing inference beyond the power spectrum</title><link>http://arxiv.org/abs/2407.18909v1</link><description>In inference problems, we often have domain knowledge which allows us todefine summary statistics that capture most of the information content in adataset. In this paper, we present a hybrid approach, where such physics-basedsummaries are augmented by a set of compressed neural summary statistics thatare optimised to extract the extra information that is not captured by thepredefined summaries. The resulting statistics are very powerful inputs tosimulation-based or implicit inference of model parameters. We apply thisgeneralisation of Information Maximising Neural Networks (IMNNs) to parameterconstraints from tomographic weak gravitational lensing convergence maps tofind summary statistics that are explicitly optimised to complement angularpower spectrum estimates. We study several dark matter simulation resolutionsin low- and high-noise regimes. We show that i) the information-updateformalism extracts at least $3\times$ and up to $8\times$ as much informationas the angular power spectrum in all noise regimes, ii) the network summariesare highly complementary to existing 2-point summaries, and iii) our formalismallows for networks with smaller, physically-informed architectures to matchmuch larger regression networks with far fewer simulations needed to obtainasymptotically optimal inference.</description><author>T. Lucas Makinen, Tom Charnock, Natalia Porqueres, Axel Lapel, Alan Heavens, Benjamin D. Wandelt</author><pubDate>Fri, 26 Jul 2024 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18909v1</guid></item><item><title>UGG: Unified Generative Grasping</title><link>http://arxiv.org/abs/2311.16917v2</link><description>Dexterous grasping aims to produce diverse grasping postures with a highgrasping success rate. Regression-based methods that directly predict graspingparameters given the object may achieve a high success rate but often lackdiversity. Generation-based methods that generate grasping postures conditionedon the object can often produce diverse grasping, but they are insufficient forhigh grasping success due to lack of discriminative information. To mitigate,we introduce a unified diffusion-based dexterous grasp generation model, dubbedthe name UGG, which operates within the object point cloud and hand parameterspaces. Our all-transformer architecture unifies the information from theobject, the hand, and the contacts, introducing a novel representation ofcontact points for improved contact modeling. The flexibility and quality ofour model enable the integration of a lightweight discriminator, benefitingfrom simulated discriminative data, which pushes for a high success rate whilepreserving high diversity. Beyond grasp generation, our model can also generateobjects based on hand information, offering valuable insights into objectdesign and studying how the generative model perceives objects. Our modelachieves state-of-the-art dexterous grasping on the large-scale DexGraspNetdataset while facilitating human-centric object design, marking a significantadvancement in dexterous grasping research. Our project page ishttps://jiaxin-lu.github.io/ugg/.</description><author>Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang, Qixing Huang, Gang Hua</author><pubDate>Fri, 26 Jul 2024 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16917v2</guid></item><item><title>Wolf: Captioning Everything with a World Summarization Framework</title><link>http://arxiv.org/abs/2407.18908v1</link><description>We propose Wolf, a WOrLd summarization Framework for accurate videocaptioning. Wolf is an automated captioning framework that adopts amixture-of-experts approach, leveraging complementary strengths of VisionLanguage Models (VLMs). By utilizing both image and video models, our frameworkcaptures different levels of information and summarizes them efficiently. Ourapproach can be applied to enhance video understanding, auto-labeling, andcaptioning. To evaluate caption quality, we introduce CapScore, an LLM-basedmetric to assess the similarity and quality of generated captions compared tothe ground truth captions. We further build four human-annotated datasets inthree domains: autonomous driving, general scenes, and robotics, to facilitatecomprehensive comparisons. We show that Wolf achieves superior captioningperformance compared to state-of-the-art approaches from the research community(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). Forinstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wiseby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,we establish a benchmark for video captioning and introduce a leaderboard,aiming to accelerate advancements in video understanding, captioning, and dataalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.</description><author>Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, Xinshuo Weng, Fuzhao Xue, Andrew Tao, Ming-Yu Liu, Sanja Fidler, Boris Ivanovic, Trevor Darrell, Jitendra Malik, Song Han, Marco Pavone</author><pubDate>Fri, 26 Jul 2024 17:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18908v1</guid></item><item><title>SHIC: Shape-Image Correspondences with no Keypoint Supervision</title><link>http://arxiv.org/abs/2407.18907v1</link><description>Canonical surface mapping generalizes keypoint detection by assigning eachpixel of an object to a corresponding point in a 3D template. Popularised byDensePose for the analysis of humans, authors have since attempted to apply theconcept to more categories, but with limited success due to the high cost ofmanual supervision. In this work, we introduce SHIC, a method to learncanonical maps without manual supervision which achieves better results thansupervised methods for most categories. Our idea is to leverage foundationcomputer vision models such as DINO and Stable Diffusion that are open-endedand thus possess excellent priors over natural categories. SHIC reduces theproblem of estimating image-to-template correspondences to predictingimage-to-image correspondences using features from the foundation models. Thereduction works by matching images of the object to non-photorealistic rendersof the template, which emulates the process of collecting manual annotationsfor this task. These correspondences are then used to supervise high-qualitycanonical maps for any object of interest. We also show that image generatorscan further improve the realism of the template views, which provide anadditional source of supervision for the model.</description><author>Aleksandar Shtedritski, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Fri, 26 Jul 2024 17:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18907v1</guid></item><item><title>A Scalable Quantum Non-local Neural Network for Image Classification</title><link>http://arxiv.org/abs/2407.18906v1</link><description>Non-local operations play a crucial role in computer vision enabling thecapture of long-range dependencies through weighted sums of features across theinput, surpassing the constraints of traditional convolution operations thatfocus solely on local neighborhoods. Non-local operations typically requirecomputing pairwise relationships between all elements in a set, leading toquadratic complexity in terms of time and memory. Due to the high computationaland memory demands, scaling non-local neural networks to large-scale problemscan be challenging. This article introduces a hybrid quantum-classical scalablenon-local neural network, referred to as Quantum Non-Local Neural Network(QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies oninherent quantum parallelism to allow the simultaneous processing of a largenumber of input features enabling more efficient computations inquantum-enhanced feature space and involving pairwise relationships throughquantum entanglement. We benchmark our proposed QNL-Net with other quantumcounterparts to binary classification with datasets MNIST and CIFAR-10. Thesimulation findings showcase our QNL-Net achieves cutting-edge accuracy levelsin binary image classification among quantum classifiers while utilizing fewerqubits.</description><author>Sparsh Gupta, Debanjan Konar, Vaneet Aggarwal</author><pubDate>Fri, 26 Jul 2024 17:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18906v1</guid></item><item><title>Lessons from Learning to Spin "Pens"</title><link>http://arxiv.org/abs/2407.18902v1</link><description>In-hand manipulation of pen-like objects is an important skill in our dailylives, as many tools such as hammers and screwdrivers are similarly shaped.However, current learning-based methods struggle with this task due to a lackof high-quality demonstrations and the significant gap between simulation andthe real world. In this work, we push the boundaries of learning-based in-handmanipulation systems by demonstrating the capability to spin pen-like objects.We first use reinforcement learning to train an oracle policy with privilegedinformation and generate a high-fidelity trajectory dataset in simulation. Thisserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)conducting open-loop trajectory replay in the real world. We then fine-tune thesensorimotor policy using these real-world trajectories to adapt it to the realworld dynamics. With less than 50 trajectories, our policy learns to rotatemore than ten pen-like objects with different physical properties for multiplerevolutions. We present a comprehensive analysis of our design choices andshare the lessons learned during development.</description><author>Jun Wang, Ying Yuan, Haichuan Che, Haozhi Qi, Yi Ma, Jitendra Malik, Xiaolong Wang</author><pubDate>Fri, 26 Jul 2024 17:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18902v1</guid></item><item><title>AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents</title><link>http://arxiv.org/abs/2407.18901v1</link><description>Autonomous agents that address day-to-day digital tasks (e.g., orderinggroceries for a household), must not only operate multiple apps (e.g., notes,messaging, shopping app) via APIs, but also generate rich code with complexcontrol flow in an iterative manner based on their interaction with theenvironment. However, existing benchmarks for tool use are inadequate, as theyonly cover tasks that require a simple sequence of API calls. To remedy this gap, we built $\textbf{AppWorld Engine}$, a high-qualityexecution environment (60K lines of code) of 9 day-to-day apps operable via 457APIs and populated with realistic digital activities simulating the lives of~100 fictitious users. We then created $\textbf{AppWorld Benchmark}$ (40K linesof code), a suite of 750 natural, diverse, and challenging autonomous agenttasks requiring rich and interactive code generation. It supports robustprogrammatic evaluation with state-based unit tests, allowing for differentways of completing a task while also checking for unexpected changes, i.e.,collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least16% fewer. This highlights the benchmark's difficulty and AppWorld's potentialto push the frontiers of interactive coding agents. The project website isavailable at https://appworld.dev/.</description><author>Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian</author><pubDate>Fri, 26 Jul 2024 17:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18901v1</guid></item><item><title>Physics-Guided Actor-Critic Reinforcement Learning for Swimming in Turbulence</title><link>http://arxiv.org/abs/2406.10242v2</link><description>Turbulent diffusion causes particles placed in proximity to separate. Weinvestigate the required swimming efforts to maintain a particle close to itspassively advected counterpart. We explore optimally balancing these effortswith the intended goal by developing and comparing a novel Physics-InformedReinforcement Learning (PIRL) strategy with prescribed control (PC) andstandard physics-agnostic Reinforcement Learning strategies. Our PIRL scheme,coined the Actor-Physicist, is an adaptation of the Actor-Critic algorithm inwhich the Neural Network parameterized Critic is replaced with an analyticallyderived physical heuristic function (the physicist). This strategy is thencompared with an analytically computed optimal PC policy derived from astochastic optimal control formulation and standard physics-agnosticActor-Critic type algorithms.</description><author>Christopher Koh, Laurent Pagnier, Michael Chertkov</author><pubDate>Fri, 26 Jul 2024 17:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10242v2</guid></item><item><title>Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits</title><link>http://arxiv.org/abs/2403.14791v2</link><description>General purpose AI, such as ChatGPT, seems to have lowered the barriers forthe public to use AI and harness its power. However, the governance anddevelopment of AI still remain in the hands of a few, and the pace ofdevelopment is accelerating without a comprehensive assessment of risks. As afirst step towards democratic risk assessment and design of general purpose AI,we introduce PARTICIP-AI, a carefully designed framework for laypeople tospeculate and assess AI use cases and their impacts. Our framework allows us tostudy more nuanced and detailed public opinions on AI through collecting usecases, surfacing diverse harms through risk assessment under alternatescenarios (i.e., developing and not developing a use case), and illuminatingtensions over AI development through making a concluding choice on itsdevelopment. To showcase the promise of our framework towards informingdemocratic AI development, we run a medium-scale study with inputs from 295demographically diverse participants. Our analyses show that participants'responses emphasize applications for personal life and society, contrastingwith most current AI development's business focus. We also surface diverse setof envisioned harms such as distrust in AI and institutions, complementary tothose defined by experts. Furthermore, we found that perceived impact of notdeveloping use cases significantly predicted participants' judgements ofwhether AI use cases should be developed, and highlighted lay users' concernsof techno-solutionism. We conclude with a discussion on how frameworks likePARTICIP-AI can further guide democratic AI development and governance.</description><author>Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, Maarten Sap</author><pubDate>Fri, 26 Jul 2024 17:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14791v2</guid></item><item><title>Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence</title><link>http://arxiv.org/abs/2407.18899v1</link><description>Domain Adaptation (DA) facilitates knowledge transfer from a source domain toa related target domain. This paper investigates a practical DA paradigm,namely Source data-Free Active Domain Adaptation (SFADA), where source databecomes inaccessible during adaptation, and a minimum amount of annotationbudget is available in the target domain. Without referencing the source data,new challenges emerge in identifying the most informative target samples forlabeling, establishing cross-domain alignment during adaptation, and ensuringcontinuous performance improvements through the iterative query-and-adaptationprocess. In response, we present learn from the learnt (LFTL), a novel paradigmfor SFADA to leverage the learnt knowledge from the source pretrained model andactively iterated models without extra overhead. We propose Contrastive ActiveSampling to learn from the hypotheses of the preceding model, thereby queryingtarget samples that are both informative to the current model and persistentlychallenging throughout active learning. During adaptation, we learn fromfeatures of actively selected anchors obtained from previous intermediatemodels, so that the Visual Persistence-guided Adaptation can facilitate featuredistribution alignment and active sample exploitation. Extensive experiments onthree widely-used benchmarks show that our LFTL achieves state-of-the-artperformance, superior computational efficiency and continuous improvements asthe annotation budget increases. Our code is available athttps://github.com/lyumengyao/lftl.</description><author>Mengyao Lyu, Tianxiang Hao, Xinhao Xu, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding</author><pubDate>Fri, 26 Jul 2024 17:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18899v1</guid></item><item><title>Small Molecule Optimization with Large Language Models</title><link>http://arxiv.org/abs/2407.18897v1</link><description>Recent advancements in large language models have opened new possibilitiesfor generative molecular drug design. We present Chemlactica and Chemma, twolanguage models fine-tuned on a novel corpus of 110M molecules with computedproperties, totaling 40B tokens. These models demonstrate strong performance ingenerating molecules with specified properties and predicting new molecularcharacteristics from limited samples. We introduce a novel optimizationalgorithm that leverages our language models to optimize molecules forarbitrary properties given limited access to a black box oracle. Our approachcombines ideas from genetic algorithms, rejection sampling, and promptoptimization. It achieves state-of-the-art performance on multiple molecularoptimization benchmarks, including an 8% improvement on Practical MolecularOptimization compared to previous methods. We publicly release the trainingcorpus, the language models and the optimization algorithm.</description><author>Philipp Guevorguian, Menua Bedrosian, Tigran Fahradyan, Gayane Chilingaryan, Hrant Khachatrian, Armen Aghajanyan</author><pubDate>Fri, 26 Jul 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18897v1</guid></item><item><title>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</title><link>http://arxiv.org/abs/2407.18219v2</link><description>A central piece in enabling intelligent agentic behavior in foundation modelsis to make them capable of introspecting upon their behavior, reasoning, andcorrecting their mistakes as more computation or interaction is available. Eventhe strongest proprietary large language models (LLMs) do not quite exhibit theability of continually improving their responses sequentially, even inscenarios where they are explicitly told that they are making a mistake. Inthis paper, we develop RISE: Recursive IntroSpEction, an approach forfine-tuning LLMs to introduce this capability, despite prior work hypothesizingthat this capability may not be possible to attain. Our approach prescribes aniterative fine-tuning procedure, which attempts to teach the model how to alterits response after having executed previously unsuccessful attempts to solve ahard test-time problem, with optionally additional environment feedback. RISEposes fine-tuning for a single-turn prompt as solving a multi-turn Markovdecision process (MDP), where the initial state is the prompt. Inspired byprinciples in online imitation learning and reinforcement learning, we proposestrategies for multi-turn data collection and training so as to imbue an LLMwith the capability to recursively detect and correct its previous mistakes insubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,and Mistral models to improve themselves with more turns on math reasoningtasks, outperforming several single-turn strategies given an equal amount ofinference-time computation. We also find that RISE scales well, often attaininglarger benefits with more capable models. Our analysis shows that RISE makesmeaningful improvements to responses to arrive at the correct solution forchallenging prompts, without disrupting one-turn abilities as a result ofexpressing more complex distributions.</description><author>Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar</author><pubDate>Fri, 26 Jul 2024 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18219v2</guid></item><item><title>SHANGUS: Deep Reinforcement Learning Meets Heuristic Optimization for Speedy Frontier-Based Exploration of Autonomous Vehicles in Unknown Spaces</title><link>http://arxiv.org/abs/2407.18892v1</link><description>This paper introduces SHANGUS, an advanced framework combining DeepReinforcement Learning (DRL) with heuristic optimization to improvefrontier-based exploration efficiency in unknown environments, particularly forintelligent vehicles in autonomous air services, search and rescue operations,and space exploration robotics. SHANGUS harnesses DRL's adaptability andheuristic prioritization, markedly enhancing exploration efficiency, reducingcompletion time, and minimizing travel distance. The strategy involves afrontier selection node to identify unexplored areas and a DRL navigation nodeusing the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm forrobust path planning and dynamic obstacle avoidance. Extensive experiments inROS2 and Gazebo simulation environments show SHANGUS surpasses representativetraditional methods like the Nearest Frontier (NF), Novel Frontier-BasedExploration Algorithm (CFE), and Goal-Driven Autonomous Exploration (GDAE)algorithms, especially in complex scenarios, excelling in completion time,travel distance, and exploration rate. This scalable solution is suitable forreal-time autonomous navigation in fields such as industrial automation,autonomous driving, household robotics, and space exploration. Future researchwill integrate additional sensory inputs and refine heuristic functions tofurther boost SHANGUS's efficiency and robustness.</description><author>Seunghyeop Nam, Tuan Anh Nguyen, Eunmi Choi, Dugki Min</author><pubDate>Fri, 26 Jul 2024 17:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18892v1</guid></item><item><title>On the Pros and Cons of Active Learning for Moral Preference Elicitation</title><link>http://arxiv.org/abs/2407.18889v1</link><description>Computational preference elicitation methods are tools used to learn people'spreferences quantitatively in a given context. Recent works on preferenceelicitation advocate for active learning as an efficient method to iterativelyconstruct queries (framed as comparisons between context-specific cases) thatare likely to be most informative about an agent's underlying preferences. Inthis work, we argue that the use of active learning for moral preferenceelicitation relies on certain assumptions about the underlying moralpreferences, which can be violated in practice. Specifically, we highlight thefollowing common assumptions (a) preferences are stable over time and notsensitive to the sequence of presented queries, (b) the appropriate hypothesisclass is chosen to model moral preferences, and (c) noise in the agent'sresponses is limited. While these assumptions can be appropriate for preferenceelicitation in certain domains, prior research on moral psychology suggeststhey may not be valid for moral judgments. Through a synthetic simulation ofpreferences that violate the above assumptions, we observe that active learningcan have similar or worse performance than a basic random query selectionmethod in certain settings. Yet, simulation results also demonstrate thatactive learning can still be viable if the degree of instability or noise isrelatively small and when the agent's preferences can be approximatelyrepresented with the hypothesis class used for learning. Our study highlightsthe nuances associated with effective moral preference elicitation in practiceand advocates for the cautious use of active learning as a methodology to learnmoral preferences.</description><author>Vijay Keswani, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong</author><pubDate>Fri, 26 Jul 2024 17:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18889v1</guid></item><item><title>Embedding And Clustering Your Data Can Improve Contrastive Pretraining</title><link>http://arxiv.org/abs/2407.18887v1</link><description>Recent studies of large-scale contrastive pretraining in the text embeddingdomain show that using single-source minibatches, rather than mixed-sourceminibatches, can substantially improve overall model accuracy. In this work, weexplore extending training data stratification beyond source granularity byleveraging a pretrained text embedding model and the classic k-means clusteringalgorithm to further split training data apart by the semantic clusters withineach source. Experimentally, we observe a notable increase in NDCG@10 whenpretraining a BERT-based text embedding model on query-passage pairs from theMSMARCO passage retrieval dataset. Additionally, we conceptually connect ourclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-Bmethodology and the nearest-neighbor-based hard-negative mining aspect of theANCE methodology and discuss how this unified view motivates future lines ofresearch on the organization of contrastive pretraining data.</description><author>Luke Merrick</author><pubDate>Fri, 26 Jul 2024 17:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18887v1</guid></item><item><title>Regression prediction algorithm for energy consumption regression in cloud computing based on horned lizard algorithm optimised convolutional neural network-bidirectional gated recurrent unit</title><link>http://arxiv.org/abs/2407.14575v2</link><description>For this paper, a prediction study of cloud computing energy consumption wasconducted by optimising the data regression algorithm based on the hornedlizard optimisation algorithm for Convolutional Neural Networks-Bi-DirectionalGated Recurrent Units. Firstly, through Spearman correlation analysis of CPU,usage, memory usage, network traffic, power consumption, number of instructionsexecuted, execution time and energy efficiency, we found that power consumptionhas the highest degree of positive correlation with energy efficiency, whileCPU usage has the highest degree of negative correlation with energyefficiency. In our experiments, we introduced a random forest model and anoptimisation model based on the horned lizard optimisation algorithm fortesting, and the results show that the optimisation algorithm has betterprediction results compared to the random forest model. Specifically, the meansquare error (MSE) of the optimisation algorithm is 0.01 smaller than that ofthe random forest model, and the mean absolute error (MAE) is 0.01 smaller thanthat of the random forest.3 The results of the combined metrics show that theoptimisation algorithm performs more accurately and reliably in predictingenergy efficiency. This research result provides new ideas and methods toimprove the energy efficiency of cloud computing systems. This research notonly expands the scope of application in the field of cloud computing, but alsoprovides a strong support for improving the energy use efficiency of thesystem.</description><author>Feiyang Li, Zinan Cao, Qixuan Yu, Xirui Tang</author><pubDate>Fri, 26 Jul 2024 17:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14575v2</guid></item><item><title>How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities</title><link>http://arxiv.org/abs/2407.08112v2</link><description>Long sequences occur in abundance within real-world scenarios, hence properlymodelling them opens numerous down-stream use-cases. Deep neural networks,however, have often struggled with these for a variety of reasons. Recentadvances, both in system engineering as well as model design, have enabled thescaling up of model that are purported to support extended context length. Inparticular, the state-space and linear recurrent neural network families ofmodels hypothetically can entend to infinite sequence lenth. However, is thistoo good to be true? We conduct an evaluation to show that while such claimsmay be sound theoretically, there remain large practical gaps that areempirically observed. In particular, recurrent models still suffer in the samesettings as long-context LLMs with attention. We further show that differentinductive biases have inconsistent extrapolation capabilities, highlighting theneed to further study such paradigms and investigate why long-context modelsseemingly fail to behave as one might expect.</description><author>Jerry Huang</author><pubDate>Fri, 26 Jul 2024 17:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08112v2</guid></item><item><title>Variational Inference via Smoothed Particle Hydrodynamics</title><link>http://arxiv.org/abs/2407.09186v2</link><description>A new variational inference method, SPH-ParVI, based on smoothed particlehydrodynamics (SPH), is proposed for sampling partially known densities (e.g.up to a constant) or sampling using gradients. SPH-ParVI simulates the flow ofa fluid under external effects driven by the target density; transient orsteady state of the fluid approximates the target density. The continuum fluidis modelled as an interacting particle system (IPS) via SPH, where eachparticle carries smoothed properties, interacts and evolves as per theNavier-Stokes equations. This mesh-free, Lagrangian simulation method offersfast, flexible, scalable and deterministic sampling and inference for a classof probabilistic models such as those encountered in Bayesian inference andgenerative modelling.</description><author>Yongchao Huang</author><pubDate>Fri, 26 Jul 2024 17:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09186v2</guid></item><item><title>Utilizing TTS Synthesized Data for Efficient Development of Keyword Spotting Model</title><link>http://arxiv.org/abs/2407.18879v1</link><description>This paper explores the use of TTS synthesized training data for KWS (keywordspotting) task while minimizing development cost and time. Keyword spottingmodels require a huge amount of training data to be accurate, and obtainingsuch training data can be costly. In the current state of the art, TTS modelscan generate large amounts of natural-sounding data, which can help reducingcost and time for KWS model development. Still, TTS generated data can belacking diversity compared to real data. To pursue maximizing KWS modelaccuracy under the constraint of limited resources and current TTS capability,we explored various strategies to mix TTS data and real human speech data, witha focus on minimizing real data use and maximizing diversity of TTS output. Ourexperimental results indicate that relatively small amounts of real audio datawith speaker diversity (100 speakers, 2k utterances) and large amounts of TTSsynthesized data can achieve reasonably high accuracy (within 3x error rate ofbaseline), compared to the baseline (trained with 3.8M real positiveutterances).</description><author>Hyun Jin Park, Dhruuv Agarwal, Neng Chen, Rentao Sun, Kurt Partridge, Justin Chen, Harry Zhang, Pai Zhu, Jacob Bartel, Kyle Kastner, Gary Wang, Andrew Rosenberg, Quan Wang</author><pubDate>Fri, 26 Jul 2024 17:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18879v1</guid></item><item><title>An Accelerated Multi-level Monte Carlo Approach for Average Reward Reinforcement Learning with General Policy Parametrization</title><link>http://arxiv.org/abs/2407.18878v1</link><description>In our study, we delve into average-reward reinforcement learning withgeneral policy parametrization. Within this domain, current guarantees eitherfall short with suboptimal guarantees or demand prior knowledge of mixing time.To address these issues, we introduce Randomized Accelerated Natural ActorCritic, a method that integrates Multi-level Monte-Carlo and Natural ActorCritic. Our approach is the first to achieve global convergence rate of$\tilde{\mathcal{O}}(1/\sqrt{T})$ without requiring knowledge of mixing time,significantly surpassing the state-of-the-art bound of$\tilde{\mathcal{O}}(1/T^{1/4})$.</description><author>Swetha Ganesh, Vaneet Aggarwal</author><pubDate>Fri, 26 Jul 2024 17:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18878v1</guid></item><item><title>Generative Adversarial Networks for Imputing Sparse Learning Performance</title><link>http://arxiv.org/abs/2407.18875v1</link><description>Learning performance data, such as correct or incorrect responses toquestions in Intelligent Tutoring Systems (ITSs) is crucial for tracking andassessing the learners' progress and mastery of knowledge. However, the issueof data sparsity, characterized by unexplored questions and missing attempts,hampers accurate assessment and the provision of tailored, personalizedinstruction within ITSs. This paper proposes using the Generative AdversarialImputation Networks (GAIN) framework to impute sparse learning performancedata, reconstructed into a three-dimensional (3D) tensor representation acrossthe dimensions of learners, questions and attempts. Our customized GAIN-basedmethod computational process imputes sparse data in a 3D tensor space,significantly enhanced by convolutional neural networks for its input andoutput layers. This adaptation also includes the use of a least squares lossfunction for optimization and aligns the shapes of the input and output withthe dimensions of the questions-attempts matrices along the learners'dimension. Through extensive experiments on six datasets from various ITSs,including AutoTutor, ASSISTments and MATHia, we demonstrate that the GAINapproach generally outperforms existing methods such as tensor factorizationand other generative adversarial network (GAN) based approaches in terms ofimputation accuracy. This finding enhances comprehensive learning data modelingand analytics in AI-based education.</description><author>Liang Zhang, Mohammed Yeasin, Jionghao Lin, Felix Havugimana, Xiangen Hu</author><pubDate>Fri, 26 Jul 2024 17:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18875v1</guid></item><item><title>Engaging with Children's Artwork in Mixed Visual-Ability Families</title><link>http://arxiv.org/abs/2407.18874v1</link><description>We present two studies exploring how blind or low-vision (BLV) family membersengage with their sighted children's artwork, strategies to supportunderstanding and interpretation, and the potential role of technology, such asAI, therein. Our first study involved 14 BLV individuals, and the secondincluded five groups of BLV individuals with their children. Throughsemi-structured interviews with AI descriptions of children's artwork andmulti-sensory design probes, we found that BLV family members value artworkengagement as a bonding opportunity, preferring the child's storytelling andinterpretation over other nonvisual representations. Additionally, despite someinaccuracies, BLV family members felt that AI-generated descriptions couldfacilitate dialogue with their children and aid self-guided art discovery. Weclose with specific design considerations for supporting artwork engagement inmixed visual-ability families, including enabling artwork access throughvarious methods, supporting children's corrections of AI output, anddistinctions in context vs. content and interpretation vs. description ofchildren's artwork.</description><author>Arnavi Chheda-Kothary, Jacob O. Wobbrock, Jon E. Froehlich</author><pubDate>Fri, 26 Jul 2024 17:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18874v1</guid></item><item><title>Distilling Multi-Scale Knowledge for Event Temporal Relation Extraction</title><link>http://arxiv.org/abs/2209.00568v3</link><description>Event Temporal Relation Extraction (ETRE) is paramount but challenging.Within a discourse, event pairs are situated at different distances or theso-called proximity bands. The temporal ordering communicated about event pairswhere at more remote (i.e., ``long'') or less remote (i.e., ``short'')proximity bands are encoded differently. SOTA models have tended to performwell on events situated at either short or long proximity bands, but not both.Nonetheless, real-world, natural texts contain all types of temporalevent-pairs. In this paper, we present MulCo: Distilling Multi-Scale Knowledgevia Contrastive Learning, a knowledge co-distillation approach that sharesknowledge across multiple event pair proximity bands to improve performance onall types of temporal datasets. Our experimental results show that MulCosuccessfully integrates linguistic cues pertaining to temporal reasoning acrossboth short and long proximity bands and achieves new state-of-the-art resultson several ETRE benchmark datasets.</description><author>Hao-Ren Yao, Luke Breitfeller, Aakanksha Naik, Chunxiao Zhou, Carolyn Rose</author><pubDate>Fri, 26 Jul 2024 17:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00568v3</guid></item><item><title>Downlink CCM Estimation via Representation Learning with Graph Regularization</title><link>http://arxiv.org/abs/2407.18865v1</link><description>In this paper, we propose an algorithm for downlink (DL) channel covariancematrix (CCM) estimation for frequency division duplexing (FDD) massivemultiple-input multiple-output (MIMO) communication systems with base station(BS) possessing a uniform linear array (ULA) antenna structure. We make use ofthe inherent similarity between the uplink (UL) CCM and the DL CCM due toangular reciprocity. We consider a setting where the UL CCM is mapped to DL CCMby a mapping function. We first present a theoretical error analysis oflearning a nonlinear embedding by constructing a mapping function, which pointsto the importance of the Lipschitz regularity of the mapping function forachieving high estimation performance. Then, based on the theoretical ground,we propose a representation learning algorithm as a solution for the estimationproblem, where Gaussian RBF kernel interpolators are chosen to map UL CCMs totheir DL counterparts. The proposed algorithm is based on the optimization ofan objective function that fits a regression model between the DL CCM and ULCCM samples in the training dataset and preserves the local geometric structureof the data in the UL CCM space, while explicitly regulating the Lipschitzcontinuity of the mapping function in light of our theoretical findings. Theproposed algorithm surpasses benchmark methods in terms of three error metricsas shown by simulations.</description><author>Melih Can Zerin, Elif Vural, Ali √ñzg√ºr Yƒ±lmaz</author><pubDate>Fri, 26 Jul 2024 16:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18865v1</guid></item><item><title>Semantic Prototypes: Enhancing Transparency Without Black Boxes</title><link>http://arxiv.org/abs/2407.15871v2</link><description>As machine learning (ML) models and datasets increase in complexity, thedemand for methods that enhance explainability and interpretability becomesparamount. Prototypes, by encapsulating essential characteristics within data,offer insights that enable tactical decision-making and enhance transparency.Traditional prototype methods often rely on sub-symbolic raw data and opaquelatent spaces, reducing explainability and increasing the risk ofmisinterpretations. This paper presents a novel framework that utilizessemantic descriptions to define prototypes and provide clear explanations,effectively addressing the shortcomings of conventional methods. Our approachleverages concept-based descriptions to cluster data on the semantic level,ensuring that prototypes not only represent underlying properties intuitivelybut are also straightforward to interpret. Our method simplifies theinterpretative process and effectively bridges the gap between complex datastructures and human cognitive processes, thereby enhancing transparency andfostering trust. Our approach outperforms existing widely-used prototypemethods in facilitating human understanding and informativeness, as validatedthrough a user survey.</description><author>Orfeas Menis-Mastromichalakis, Giorgos Filandrianos, Jason Liartis, Edmund Dervakos, Giorgos Stamou</author><pubDate>Fri, 26 Jul 2024 16:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15871v2</guid></item><item><title>Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment</title><link>http://arxiv.org/abs/2407.18854v1</link><description>Image classification models often demonstrate unstable performance inreal-world applications due to variations in image information, driven bydiffering visual perspectives of subject objects and lighting discrepancies. Tomitigate these challenges, existing studies commonly incorporate additionalmodal information matching the visual data to regularize the model's learningprocess, enabling the extraction of high-quality visual features from compleximage regions. Specifically, in the realm of multimodal learning, cross-modalalignment is recognized as an effective strategy, harmonizing different modalinformation by learning a domain-consistent latent feature space for visual andsemantic features. However, this approach may face limitations due to theheterogeneity between multimodal information, such as differences in featuredistribution and structure. To address this issue, we introduce a MultimodalAlignment and Reconstruction Network (MARNet), designed to enhance the model'sresistance to visual noise. Importantly, MARNet includes a cross-modaldiffusion reconstruction module for smoothly and stably blending informationacross different domains. Experiments conducted on two benchmark datasets,Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improvesthe quality of image information extracted by the model. It is a plug-and-playframework that can be rapidly integrated into various image classificationframeworks, boosting model performance.</description><author>Yuze Zheng, Zixuan Li, Xiangxian Li, Jinxing Liu, Yuqing Wang, Xiangxu Meng, Lei Meng</author><pubDate>Fri, 26 Jul 2024 16:30:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18854v1</guid></item><item><title>On TinyML and Cybersecurity: Electric Vehicle Charging Infrastructure Use Case</title><link>http://arxiv.org/abs/2404.16894v3</link><description>As technology advances, the use of Machine Learning (ML) in cybersecurity isbecoming increasingly crucial to tackle the growing complexity of cyberthreats. While traditional ML models can enhance cybersecurity, their highenergy and resource demands limit their applications, leading to the emergenceof Tiny Machine Learning (TinyML) as a more suitable solution forresource-constrained environments. TinyML is widely applied in areas such assmart homes, healthcare, and industrial automation. TinyML focuses onoptimizing ML algorithms for small, low-power devices, enabling intelligentdata processing directly on edge devices. This paper provides a comprehensivereview of common challenges of TinyML techniques, such as power consumption,limited memory, and computational constraints; it also explores potentialsolutions to these challenges, such as energy harvesting, computationaloptimization techniques, and transfer learning for privacy preservation. On theother hand, this paper discusses TinyML's applications in advancingcybersecurity for Electric Vehicle Charging Infrastructures (EVCIs) as arepresentative use case. It presents an experimental case study that enhancescybersecurity in EVCI using TinyML, evaluated against traditional ML in termsof reduced delay and memory usage, with a slight trade-off in accuracy.Additionally, the study includes a practical setup using the ESP32microcontroller in the PlatformIO environment, which provides a hands-onassessment of TinyML's application in cybersecurity for EVCI.</description><author>Fatemeh Dehrouyeh, Li Yang, Firouz Badrkhani Ajaei, Abdallah Shami</author><pubDate>Fri, 26 Jul 2024 16:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16894v3</guid></item><item><title>MxT: Mamba x Transformer for Image Inpainting</title><link>http://arxiv.org/abs/2407.16126v2</link><description>Image inpainting, or image completion, is a crucial task in computer visionthat aims to restore missing or damaged regions of images with semanticallycoherent content. This technique requires a precise balance of local texturereplication and global contextual understanding to ensure the restored imageintegrates seamlessly with its surroundings. Traditional methods usingConvolutional Neural Networks (CNNs) are effective at capturing local patternsbut often struggle with broader contextual relationships due to the limitedreceptive fields. Recent advancements have incorporated transformers,leveraging their ability to understand global interactions. However, thesemethods face computational inefficiencies and struggle to maintain fine-graineddetails. To overcome these challenges, we introduce MxT composed of theproposed Hybrid Module (HM), which combines Mamba with the transformer in asynergistic manner. Mamba is adept at efficiently processing long sequenceswith linear computational costs, making it an ideal complement to thetransformer for handling long-scale data interactions. Our HM facilitatesdual-level interaction learning at both pixel and patch levels, greatlyenhancing the model to reconstruct images with high quality and contextualaccuracy. We evaluate MxT on the widely-used CelebA-HQ and Places2-standarddatasets, where it consistently outperformed existing state-of-the-art methods.</description><author>Shuang Chen, Amir Atapour-Abarghouei, Haozheng Zhang, Hubert P. H. Shum</author><pubDate>Fri, 26 Jul 2024 16:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16126v2</guid></item><item><title>Repairing Networks of $\mathcal{EL_\perp}$ Ontologies using Weakening and Completing -- Extended version</title><link>http://arxiv.org/abs/2407.18848v1</link><description>The quality of ontologies and their alignments is crucial for developinghigh-quality semantics-based applications. Traditional debugging techniquesrepair ontology networks by removing unwanted axioms and mappings, but maythereby remove consequences that are correct in the domain of the ontologynetwork. In this paper we propose a framework for repairing ontology networksthat deals with this issue. It defines basic operations such as debugging,weakening and completing. Further, it defines combination operators thatreflect choices in how and when to use the basic operators, as well as choicesregarding the autonomy level of the ontologies and alignments in the ontologynetwork. We show the influence of the combination operators on the quality ofthe repaired network and present an implemented tool. By using our frameworktogether with existing algorithms for debugging, weakening and completing, weessentially provide a blueprint for extending previous work and systems.</description><author>Ying Li, Patrick Lambrix</author><pubDate>Fri, 26 Jul 2024 16:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18848v1</guid></item><item><title>Enhancing material property prediction with ensemble deep graph convolutional networks</title><link>http://arxiv.org/abs/2407.18847v1</link><description>Machine learning (ML) models have emerged as powerful tools for acceleratingmaterials discovery and design by enabling accurate predictions of propertiesfrom compositional and structural data. These capabilities are vital fordeveloping advanced technologies across fields such as energy, electronics, andbiomedicine, potentially reducing the time and resources needed for newmaterial exploration and promoting rapid innovation cycles. Recent efforts havefocused on employing advanced ML algorithms, including deep learning - basedgraph neural network, for property prediction. Additionally, ensemble modelshave proven to enhance the generalizability and robustness of ML and DL.However, the use of such ensemble strategies in deep graph networks formaterial property prediction remains underexplored. Our research provides anin-depth evaluation of ensemble strategies in deep learning - based graphneural network, specifically targeting material property prediction tasks. Bytesting the Crystal Graph Convolutional Neural Network (CGCNN) and itsmultitask version, MT-CGCNN, we demonstrated that ensemble techniques,especially prediction averaging, substantially improve precision beyondtraditional metrics for key properties like formation energy per atom ($\DeltaE^{f}$), band gap ($E_{g}$) and density ($\rho$) in 33,990 stable inorganicmaterials. These findings support the broader application of ensemble methodsto enhance predictive accuracy in the field.</description><author>Chowdhury Mohammad Abid Rahman, Ghadendra Bhandari, Nasser M Nasrabadi, Aldo H. Romero, Prashnna K. Gyawali</author><pubDate>Fri, 26 Jul 2024 16:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18847v1</guid></item><item><title>On The Expressive Power of Knowledge Graph Embedding Methods</title><link>http://arxiv.org/abs/2407.16326v2</link><description>Knowledge Graph Embedding (KGE) is a popular approach, which aims torepresent entities and relations of a knowledge graph in latent spaces. Theirrepresentations are known as embeddings. To measure the plausibility oftriplets, score functions are defined over embedding spaces. Despite widedissemination of KGE in various tasks, KGE methods have limitations inreasoning abilities. In this paper we propose a mathematical framework tocompare reasoning abilities of KGE methods. We show that STransE has a highercapability than TransComplEx, and then present new STransCoRe method, whichimproves the STransE by combining it with the TransCoRe insights, which canreduce the STransE space complexity.</description><author>Jiexing Gao, Dmitry Rodin, Vasily Motolygin, Denis Zaytsev</author><pubDate>Fri, 26 Jul 2024 16:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16326v2</guid></item><item><title>CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks</title><link>http://arxiv.org/abs/2402.17363v2</link><description>Dynamic graphs are extensively employed for detecting anomalous behavior innodes within the Internet of Things (IoT). Generative models are often used toaddress the issue of imbalanced node categories in dynamic graphs.Nevertheless, the constraints it faces include the monotonicity of adjacencyrelationships, the difficulty in constructing multi-dimensional features fornodes, and the lack of a method for end-to-end generation of multiplecategories of nodes. This paper presents a novel graph generation model, calledCGGM, designed specifically to generate a larger number of nodes belonging tothe minority class. The mechanism for generating an adjacency matrix, throughadaptive sparsity, enhances flexibility in its structure. The featuregeneration module, called multidimensional features generator (MFG) to generatenode features along with topological information. Labels are transformed intoembedding vectors, serving as conditional constraints to control the generationof synthetic data across multiple categories. Using a multi-stage loss, thedistribution of synthetic data is adjusted to closely resemble that of realdata. In extensive experiments, we show that CGGM's synthetic data outperformsstate-of-the-art methods across various metrics. Our results demonstrateefficient generation of diverse data categories, robustly enhancingmulti-category classification model performance.</description><author>Xianshi Su, Munan Li, Tongbang Jiang, Hao Long</author><pubDate>Fri, 26 Jul 2024 16:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17363v2</guid></item><item><title>QT-TDM: Planning with Transformer Dynamics Model and Autoregressive Q-Learning</title><link>http://arxiv.org/abs/2407.18841v1</link><description>Inspired by the success of the Transformer architecture in natural languageprocessing and computer vision, we investigate the use of Transformers inReinforcement Learning (RL), specifically in modeling the environment'sdynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilitiesof TDMs for continuous control in real-time planning scenarios with ModelPredictive Control (MPC). While Transformers excel in long-horizon prediction,their tokenization mechanism and autoregressive nature lead to costly planningover long horizons, especially as the environment's dimensionality increases.To alleviate this issue, we use a TDM for short-term planning, and learn anautoregressive discrete Q-function using a separate Q-Transformer (QT) model toestimate a long-term return beyond the short-horizon planning. Our proposedmethod, QT-TDM, integrates the robust predictive capabilities of Transformersas dynamics models with the efficacy of a model-free Q-Transformer to mitigatethe computational burden associated with real-time planning. Experiments indiverse state-based continuous control tasks show that QT-TDM is superior inperformance and sample efficiency compared to existing Transformer-based RLmodels while achieving fast and computationally efficient inference.</description><author>Mostafa Kotb, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter</author><pubDate>Fri, 26 Jul 2024 16:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18841v1</guid></item><item><title>The Cross-environment Hyperparameter Setting Benchmark for Reinforcement Learning</title><link>http://arxiv.org/abs/2407.18840v1</link><description>This paper introduces a new empirical methodology, the Cross-environmentHyperparameter Setting Benchmark, that compares RL algorithms acrossenvironments using a single hyperparameter setting, encouraging algorithmicdevelopment which is insensitive to hyperparameters. We demonstrate that thisbenchmark is robust to statistical noise and obtains qualitatively similarresults across repeated applications, even when using few samples. Thisrobustness makes the benchmark computationally cheap to apply, allowingstatistically sound insights at low cost. We demonstrate two exampleinstantiations of the CHS, on a set of six small control environments (SC-CHS)and on the entire DM Control suite of 28 environments (DMC-CHS). Finally, toillustrate the applicability of the CHS to modern RL algorithms on challengingenvironments, we conduct a novel empirical study of an open question in thecontinuous control literature. We show, with high confidence, that there is nomeaningful difference in performance between Ornstein-Uhlenbeck noise anduncorrelated Gaussian noise for exploration with the DDPG algorithm on theDMC-CHS.</description><author>Andrew Patterson, Samuel Neumann, Raksha Kumaraswamy, Martha White, Adam White</author><pubDate>Fri, 26 Jul 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18840v1</guid></item><item><title>Scalable Group Choreography via Variational Phase Manifold Learning</title><link>http://arxiv.org/abs/2407.18839v1</link><description>Generating group dance motion from the music is a challenging task withseveral industrial applications. Although several methods have been proposed totackle this problem, most of them prioritize optimizing the fidelity in dancingmovement, constrained by predetermined dancer counts in datasets. Thislimitation impedes adaptability to real-world applications. Our study addressesthe scalability problem in group choreography while preserving naturalness andsynchronization. In particular, we propose a phase-based variational generativemodel for group dance generation on learning a generative manifold. Our methodachieves high-fidelity group dance motion and enables the generation with anunlimited number of dancers while consuming only a minimal and constant amountof memory. The intensive experiments on two public datasets show that ourproposed method outperforms recent state-of-the-art approaches by a largemargin and is scalable to a great number of dancers beyond the training data.</description><author>Nhat Le, Khoa Do, Xuan Bui, Tuong Do, Erman Tjiputra, Quang D. Tran, Anh Nguyen</author><pubDate>Fri, 26 Jul 2024 16:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18839v1</guid></item><item><title>The Role of Temporal Hierarchy in Spiking Neural Networks</title><link>http://arxiv.org/abs/2407.18838v1</link><description>Spiking Neural Networks (SNNs) have the potential for rich spatio-temporalsignal processing thanks to exploiting both spatial and temporal parameters.The temporal dynamics such as time constants of the synapses and neurons anddelays have been recently shown to have computational benefits that help reducethe overall number of parameters required in the network and increase theaccuracy of the SNNs in solving temporal tasks. Optimizing such temporalparameters, for example, through gradient descent, gives rise to a temporalarchitecture for different problems. As has been shown in machine learning, toreduce the cost of optimization, architectural biases can be applied, in thiscase in the temporal domain. Such inductive biases in temporal parameters havebeen found in neuroscience studies, highlighting a hierarchy of temporalstructure and input representation in different layers of the cortex. Motivatedby this, we propose to impose a hierarchy of temporal representation in thehidden layers of SNNs, highlighting that such an inductive bias improves theirperformance. We demonstrate the positive effects of temporal hierarchy in thetime constants of feed-forward SNNs applied to temporal tasks (Multi-Time-ScaleXOR and Keyword Spotting, with a benefit of up to 4.1% in classificationaccuracy). Moreover, we show that such architectural biases, i.e. hierarchy oftime constants, naturally emerge when optimizing the time constants throughgradient descent, initialized as homogeneous values. We further pursue thisproposal in temporal convolutional SNNs, by introducing the hierarchical biasin the size and dilation of temporal kernels, giving rise to competitiveresults in popular temporal spike-based datasets.</description><author>Filippo Moro, Pau Vilimelis Aceituno, Laura Kriener, Melika Payvand</author><pubDate>Fri, 26 Jul 2024 16:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18838v1</guid></item><item><title>Learning to Visually Connect Actions and their Effects</title><link>http://arxiv.org/abs/2401.10805v3</link><description>We introduce the novel concept of visually Connecting Actions and TheirEffects (CATE) in video understanding. CATE can have applications in areas liketask planning and learning from demonstration. We identify and explore twodifferent aspects of the concept of CATE: Action Selection (AS) andEffect-Affinity Assessment (EAA), where video understanding models connectactions and effects at semantic and fine-grained levels, respectively. Wedesign various baseline models for AS and EAA. Despite the intuitive nature ofthe task, we observe that models struggle, and humans outperform them by alarge margin. Our experiments show that in solving AS and EAA, models learnintuitive properties like object tracking and pose encoding without explicitsupervision. We demonstrate that CATE can be an effective self-supervised taskfor learning video representations from unlabeled videos. The study aims toshowcase the fundamental nature and versatility of CATE, with the hope ofinspiring advanced formulations and models.</description><author>Paritosh Parmar, Eric Peh, Basura Fernando</author><pubDate>Fri, 26 Jul 2024 16:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10805v3</guid></item><item><title>Selective Vision-Language Subspace Projection for Few-shot CLIP</title><link>http://arxiv.org/abs/2407.16977v2</link><description>Vision-language models such as CLIP are capable of mapping the differentmodality data into a unified feature space, enabling zero/few-shot inference bymeasuring the similarity of given images and texts. However, most existingmethods overlook modality gaps in CLIP's encoded features, which is shown asthe text and image features lie far apart from each other, resulting in limitedclassification performance. To tackle this issue, we introduce a method calledSelective Vision-Language Subspace Projection (SSP), which incorporates localimage features and utilizes them as a bridge to enhance the alignment betweenimage-text pairs. Specifically, our SSP framework comprises two parallelmodules: a vision projector and a language projector. Both projectors utilizelocal image features to span the respective subspaces for image and texts,thereby projecting the image and text features into their respective subspacesto achieve alignment. Moreover, our approach entails only training-free matrixcalculations and can be seamlessly integrated into advanced CLIP-based few-shotlearning frameworks. Extensive experiments on 11 datasets have demonstratedSSP's superior text-image alignment capabilities, outperforming thestate-of-the-art alignment methods. The code is available athttps://github.com/zhuhsingyuu/SSP</description><author>Xingyu Zhu, Beier Zhu, Yi Tan, Shuo Wang, Yanbin Hao, Hanwang Zhang</author><pubDate>Fri, 26 Jul 2024 15:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16977v2</guid></item><item><title>Autonomous Multi-Objective Optimization Using Large Language Model</title><link>http://arxiv.org/abs/2406.08987v2</link><description>Multi-objective optimization problems (MOPs) are ubiquitous in real-worldapplications, presenting a complex challenge of balancing multiple conflictingobjectives. Traditional evolutionary algorithms (EAs), though effective, oftenrely on domain-specific expertise and iterative fine-tuning, hinderingadaptability to unseen MOPs. In recent years, the advent of Large LanguageModels (LLMs) has revolutionized software engineering by enabling theautonomous generation and refinement of programs. Leveraging this breakthrough,we propose a new LLM-based framework that autonomously designs EA operators forsolving MOPs. The proposed framework includes a robust testing module to refinethe generated EA operator through error-driven dialogue with LLMs, a dynamicselection strategy along with informative prompting-based crossover andmutation to fit textual optimization pipeline. Our approach facilitates thedesign of EA operators without the extensive demands for expert intervention,thereby speeding up the innovation of EA operators. Empirical studies acrossvarious MOP categories validate the robustness and superior performance of ourproposed framework.</description><author>Yuxiao Huang, Shenghao Wu, Wenjie Zhang, Jibin Wu, Liang Feng, Kay Chen Tan</author><pubDate>Fri, 26 Jul 2024 15:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08987v2</guid></item><item><title>Weyl Calculus and Exactly Solvable Schr√∂dinger Bridges with Quadratic State Cost</title><link>http://arxiv.org/abs/2407.15245v2</link><description>Schr\"{o}dinger bridge--a stochastic dynamical generalization of optimal masstransport--exhibits a learning-control duality. Viewed as a stochastic controlproblem, the Schr\"{o}dinger bridge finds an optimal control policy that steersa given joint state statistics to another while minimizing the total controleffort subject to controlled diffusion and deadline constraints. Viewed as astochastic learning problem, the Schr\"{o}dinger bridge finds the most-likelydistribution-valued trajectory connecting endpoint distributional observations,i.e., solves the two point boundary-constrained maximum likelihood problem overthe manifold of probability distributions. Recent works have shown that solvingthe Schr\"{o}dinger bridge problem with state cost requires finding the Markovkernel associated with a reaction-diffusion PDE where the state cost appears asa state-dependent reaction rate. We explain how ideas from Weyl calculus inquantum mechanics, specifically the Weyl operator and the Weyl symbol, can helpdetermine such Markov kernels. We illustrate these ideas by explicitly findingthe Markov kernel for the case of quadratic state cost via Weyl calculus,recovering our earlier results but avoiding tedious computation with Hermitepolynomials.</description><author>Alexis M. H. Teter, Wenqing Wang, Abhishek Halder</author><pubDate>Fri, 26 Jul 2024 15:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15245v2</guid></item><item><title>Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models</title><link>http://arxiv.org/abs/2407.18827v1</link><description>Data-driven research in Additive Manufacturing (AM) has gained significantsuccess in recent years. This has led to a plethora of scientific literature toemerge. The knowledge in these works consists of AM and Artificial Intelligence(AI) contexts that have not been mined and formalized in an integrated way. Itrequires substantial effort and time to extract scientific information fromthese works. AM domain experts have contributed over two dozen review papers tosummarize these works. However, information specific to AM and AI contextsstill requires manual effort to extract. The recent success of foundationmodels such as BERT (Bidirectional Encoder Representations for Transformers) orGPT (Generative Pre-trained Transformers) on textual data has opened thepossibility of expediting scientific information extraction. We propose aframework that enables collaboration between AM and AI experts to continuouslyextract scientific information from data-driven AM literature. A demonstrationtool is implemented based on the proposed framework and a case study isconducted to extract information relevant to the datasets, modeling, sensing,and AM system categories. We show the ability of LLMs (Large Language Models)to expedite the extraction of relevant information from data-driven AMliterature. In the future, the framework can be used to extract informationfrom the broader design and manufacturing literature in the engineeringdiscipline.</description><author>Mutahar Safdar, Jiarui Xie, Andrei Mircea, Yaoyao Fiona Zhao</author><pubDate>Fri, 26 Jul 2024 15:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18827v1</guid></item><item><title>Diffusion MRI with Machine Learning</title><link>http://arxiv.org/abs/2402.00019v2</link><description>Diffusion-weighted magnetic resonance imaging (dMRI) offers uniquecapabilities including noninvasive probing of brain's tissue microstructure andstructural connectivity. It is widely used for clinical assessment of brainpathologies and for neuroscience research. Analyzing the dMRI data to extractuseful information for medical and scientific purposes can be challenging. ThedMRI measurements often suffer from strong noise and artifacts, there isusually high inter-session and inter-scanner variability in the data, andconsiderable inter-subject heterogeneity in brain structure. Moreover, therelationship between measurements and the phenomena of interest can be highlycomplex. Recent years have witnessed increasing use of machine learning methodsfor dMRI analysis. This manuscript aims to assess these efforts, with a focuson methods that have addressed data preprocessing and harmonization,microstructure mapping, tractography, and white matter tract analysis. We studythe main findings, strengths, and weaknesses of the existing methods andsuggest topics for future research. We find that machine learning may beexceptionally suited to tackle some of the difficult tasks in dMRI analysis.However, for this to happen, several shortcomings of existing methods andcritical unresolved issues need to be addressed. These include deficientevaluation practices, lack of rich training datasets and validation benchmarks,as well as model generalizability, reliability, and explainability concerns.</description><author>Davood Karimi</author><pubDate>Fri, 26 Jul 2024 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00019v2</guid></item><item><title>Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic</title><link>http://arxiv.org/abs/2407.18129v2</link><description>Recent advancements have significantly enhanced the capabilities ofMultimodal Large Language Models (MLLMs) in generating and understandingimage-to-text content. Despite these successes, progress is predominantlylimited to English due to the scarcity of high quality multimodal resources inother languages. This limitation impedes the development of competitive modelsin languages such as Arabic. To alleviate this situation, we introduce anefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advancedlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallahdemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuningsix Arabic dialects, Dallah showcases its capability to handle complexdialectal interactions incorporating both textual and visual elements. Themodel excels in two benchmark tests: one evaluating its performance on ModernStandard Arabic (MSA) and another specifically designed to assess dialectalresponses. Beyond its robust performance in multimodal interaction tasks,Dallah has the potential to pave the way for further development ofdialect-aware Arabic MLLMs.</description><author>Fakhraddin Alwajih, Gagan Bhatia, Muhammad Abdul-Mageed</author><pubDate>Fri, 26 Jul 2024 15:34:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18129v2</guid></item><item><title>Deep Companion Learning: Enhancing Generalization Through Historical Consistency</title><link>http://arxiv.org/abs/2407.18821v1</link><description>We propose Deep Companion Learning (DCL), a novel training method for DeepNeural Networks (DNNs) that enhances generalization by penalizing inconsistentmodel predictions compared to its historical performance. To achieve this, wetrain a deep-companion model (DCM), by using previous versions of the model toprovide forecasts on new inputs. This companion model deciphers a meaningfullatent semantic structure within the data, thereby providing targetedsupervision that encourages the primary model to address the scenarios it findsmost challenging. We validate our approach through both theoretical analysisand extensive experimentation, including ablation studies, on a variety ofbenchmark datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1K) using diversearchitectural models (ShuffleNetV2, ResNet, Vision Transformer, etc.),demonstrating state-of-the-art performance.</description><author>Ruizhao Zhu, Venkatesh Saligrama</author><pubDate>Fri, 26 Jul 2024 15:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18821v1</guid></item><item><title>Online Planning in POMDPs with State-Requests</title><link>http://arxiv.org/abs/2407.18812v1</link><description>In key real-world problems, full state information is sometimes available butonly at a high cost, like activating precise yet energy-intensive sensors orconsulting humans, thereby compelling the agent to operate under partialobservability. For this scenario, we propose AEMS-SR (Anytime ErrorMinimization Search with State Requests), a principled online planningalgorithm tailored for POMDPs with state requests. By representing the searchspace as a graph instead of a tree, AEMS-SR avoids the exponential growth ofthe search space originating from state requests. Theoretical analysisdemonstrates AEMS-SR's $\varepsilon$-optimality, ensuring solution quality,while empirical evaluations illustrate its effectiveness compared with AEMS andPOMCP, two SOTA online planning algorithms. AEMS-SR enables efficient planningin domains characterized by partial observability and costly state requestsoffering practical benefits across various applications.</description><author>Raphael Avalos, Eugenio Bargiacchi, Ann Now√©, Diederik M. Roijers, Frans A. Oliehoek</author><pubDate>Fri, 26 Jul 2024 15:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18812v1</guid></item><item><title>Interpreting artificial neural networks to detect genome-wide association signals for complex traits</title><link>http://arxiv.org/abs/2407.18811v1</link><description>Investigating the genetic architecture of complex diseases is challenging dueto the highly polygenic and interactive landscape of genetic and environmentalfactors. Although genome-wide association studies (GWAS) have identifiedthousands of variants for multiple complex phenotypes, conventional statisticalapproaches can be limited by simplified assumptions such as linearity and lackof epistasis models. In this work, we trained artificial neural networks forpredicting complex traits using both simulated and real genotype/phenotypedatasets. We extracted feature importance scores via different post hocinterpretability methods to identify potentially associated loci (PAL) for thetarget phenotype. Simulations we performed with various parameters demonstratedthat associated loci can be detected with good precision using strict selectioncriteria, but downstream analyses are required for fine-mapping the exactvariants due to linkage disequilibrium, similarly to conventional GWAS. Byapplying our approach to the schizophrenia cohort in the Estonian Biobank, wewere able to detect multiple PAL related to this highly polygenic and heritabledisorder. We also performed enrichment analyses with PAL in genic regions,which predominantly identified terms associated with brain morphology. Withfurther improvements in model optimization and confidence measures, artificialneural networks can enhance the identification of genomic loci associated withcomplex diseases, providing a more comprehensive approach for GWAS and servingas initial screening tools for subsequent functional studies. Keywords: Deep learning, interpretability, genome-wide association studies,complex diseases</description><author>Burak Yelmen, Maris Alver, Estonian Biobank Research Team, Flora Jay, Lili Milani</author><pubDate>Fri, 26 Jul 2024 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18811v1</guid></item><item><title>Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs</title><link>http://arxiv.org/abs/2407.18808v1</link><description>The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for onlineprediction of generic (possibly non-Markovian) stochastic processes withirregular (in time) and potentially incomplete (with respect to coordinates)observations. It is a model for which convergence to the $L^2$-optimalpredictor, which is given by the conditional expectation, is establishedtheoretically. Thereby, the training of the model is solely based on a datasetof realizations of the underlying stochastic process, without the need ofknowledge of the law of the process. In the case where the underlying processis deterministic, the conditional expectation coincides with the processitself. Therefore, this framework can equivalently be used to learn thedynamics of ODE or PDE systems solely from realizations of the dynamical systemwith different initial conditions. We showcase the potential of our method byapplying it to the chaotic system of a double pendulum. When training thestandard PD-NJ-ODE method, we see that the prediction starts to diverge fromthe true path after about half of the evaluation time. In this work we enhancethe model with two novel ideas, which independently of each other improve theperformance of our modelling setup. The resulting dynamics match the truedynamics of the chaotic system very closely. The same enhancements can be usedto provably enable the PD-NJ-ODE to learn long-term predictions for generalstochastic datasets, where the standard model fails. This is verified inseveral experiments.</description><author>Florian Krach, Josef Teichmann</author><pubDate>Fri, 26 Jul 2024 15:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18808v1</guid></item><item><title>Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit</title><link>http://arxiv.org/abs/2407.18807v1</link><description>The infinite width limit of random neural networks is known to result inNeural Networks as Gaussian Process (NNGP) (Lee et al. [2018]), characterizedby task-independent kernels. It is widely accepted that larger network widthscontribute to improved generalization (Park et al. [2019]). However, this workchallenges this notion by investigating the narrow width limit of the BayesianParallel Branching Graph Neural Network (BPB-GNN), an architecture thatresembles residual networks. We demonstrate that when the width of a BPB-GNN issignificantly smaller compared to the number of training examples, each branchexhibits more robust learning due to a symmetry breaking of branches in kernelrenormalization. Surprisingly, the performance of a BPB-GNN in the narrow widthlimit is generally superior or comparable to that achieved in the wide widthlimit in bias-limited scenarios. Furthermore, the readout norms of each branchin the narrow width limit are mostly independent of the architecturalhyperparameters but generally reflective of the nature of the data. Our resultscharacterize a newly defined narrow-width regime for parallel branchingnetworks in general.</description><author>Zechen Zhang, Haim Sompolinsky</author><pubDate>Fri, 26 Jul 2024 15:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18807v1</guid></item><item><title>EHR-SeqSQL : A Sequential Text-to-SQL Dataset For Interactively Exploring Electronic Health Records</title><link>http://arxiv.org/abs/2406.00019v2</link><description>In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQLdataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed toaddress critical yet underexplored aspects in text-to-SQL parsing:interactivity, compositionality, and efficiency. To the best of our knowledge,EHR-SeqSQL is not only the largest but also the first medical text-to-SQLdataset benchmark to include sequential and contextual questions. We provide adata split and the new test set designed to assess compositional generalizationability. Our experiments demonstrate the superiority of a multi-turn approachover a single-turn approach in learning compositionality. Additionally, ourdataset integrates specially crafted tokens into SQL queries to improveexecution efficiency. With EHR-SeqSQL, we aim to bridge the gap betweenpractical needs and academic research in the text-to-SQL domain. EHR-SeqSQL isavailable \href{https://github.com/seonhee99/EHR-SeqSQL}{at this https URL}.</description><author>Jaehee Ryu, Seonhee Cho, Gyubok Lee, Edward Choi</author><pubDate>Fri, 26 Jul 2024 15:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00019v2</guid></item><item><title>Metadata-enhanced contrastive learning from retinal optical coherence tomography images</title><link>http://arxiv.org/abs/2208.02529v3</link><description>Deep learning has potential to automate screening, monitoring and grading ofdisease in medical images. Pretraining with contrastive learning enables modelsto extract robust and generalisable features from natural image datasets,facilitating label-efficient downstream image analysis. However, the directapplication of conventional contrastive methods to medical datasets introducestwo domain-specific issues. Firstly, several image transformations which havebeen shown to be crucial for effective contrastive learning do not translatefrom the natural image to the medical image domain. Secondly, the assumptionmade by conventional methods, that any two images are dissimilar, issystematically misleading in medical datasets depicting the same anatomy anddisease. This is exacerbated in longitudinal image datasets that repeatedlyimage the same patient cohort to monitor their disease progression over time.In this paper we tackle these issues by extending conventional contrastiveframeworks with a novel metadata-enhanced strategy. Our approach employs widelyavailable patient metadata to approximate the true set of inter-imagecontrastive relationships. To this end we employ records for patient identity,eye position (i.e. left or right) and time series information. In experimentsusing two large longitudinal datasets containing 170,427 retinal OCT images of7,912 patients with age-related macular degeneration (AMD), we evaluate theutility of using metadata to incorporate the temporal dynamics of diseaseprogression into pretraining. Our metadata-enhanced approach outperforms bothstandard contrastive methods and a retinal image foundation model in five outof six image-level downstream tasks related to AMD. Due to its modularity, ourmethod can be quickly and cost-effectively tested to establish the potentialbenefits of including available metadata in contrastive pretraining.</description><author>Robbie Holland, Oliver Leingang, Hrvoje Bogunoviƒá, Sophie Riedl, Lars Fritsche, Toby Prevost, Hendrik P. N. Scholl, Ursula Schmidt-Erfurth, Sobha Sivaprasad, Andrew J. Lotery, Daniel Rueckert, Martin J. Menten</author><pubDate>Fri, 26 Jul 2024 15:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.02529v3</guid></item><item><title>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</title><link>http://arxiv.org/abs/2310.05140v4</link><description>Empathetic dialogue is an indispensable part of building harmonious socialrelationships and contributes to the development of a helpful AI. Previousapproaches are mainly based on fine small-scale language models. With theadvent of ChatGPT, the application effect of large language models (LLMs) inthis field has attracted great attention. This work empirically investigatesthe performance of LLMs in generating empathetic responses and proposes threeimprovement methods of semantically similar in-context learning, two-stageinteractive generation, and combination with the knowledge base. Extensiveexperiments show that LLMs can significantly benefit from our proposed methodsand is able to achieve state-of-the-art performance in both automatic and humanevaluations. Additionally, we explore the possibility of GPT-4 simulating humanevaluators.</description><author>Yushan Qian, Wei-Nan Zhang, Ting Liu</author><pubDate>Fri, 26 Jul 2024 15:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05140v4</guid></item><item><title>Log-Concave Coupling for Sampling Neural Net Posteriors</title><link>http://arxiv.org/abs/2407.18802v1</link><description>In this work, we present a sampling algorithm for single hidden layer neuralnetworks. This algorithm is built upon a recursive series of Bayesianposteriors using a method we call Greedy Bayes. Sampling of the Bayesianposterior for neuron weight vectors $w$ of dimension $d$ is challenging becauseof its multimodality. Our algorithm to tackle this problem is based on acoupling of the posterior density for $w$ with an auxiliary random variable$\xi$. The resulting reverse conditional $w|\xi$ of neuron weights given auxiliaryrandom variable is shown to be log concave. In the construction of theposterior distributions we provide some freedom in the choice of the prior. Inparticular, for Gaussian priors on $w$ with suitably small variance, theresulting marginal density of the auxiliary variable $\xi$ is proven to bestrictly log concave for all dimensions $d$. For a uniform prior on the unit$\ell_1$ ball, evidence is given that the density of $\xi$ is again strictlylog concave for sufficiently large $d$. The score of the marginal density of the auxiliary random variable $\xi$ isdetermined by an expectation over $w|\xi$ and thus can be computed by variousrapidly mixing Markov Chain Monte Carlo methods. Moreover, the computation ofthe score of $\xi$ permits methods of sampling $\xi$ by a stochastic diffusion(Langevin dynamics) with drift function built from this score. With suchdynamics, information-theoretic methods pioneered by Bakry and Emery show thataccurate sampling of $\xi$ is obtained rapidly when its density is indeedstrictly log-concave. After which, one more draw from $w|\xi$, produces neuronweights $w$ whose marginal distribution is from the desired posterior.</description><author>Curtis McDonald, Andrew R Barron</author><pubDate>Fri, 26 Jul 2024 15:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18802v1</guid></item><item><title>Benchmarking Dependence Measures to Prevent Shortcut Learning in Medical Imaging</title><link>http://arxiv.org/abs/2407.18792v1</link><description>Medical imaging cohorts are often confounded by factors such as acquisitiondevices, hospital sites, patient backgrounds, and many more. As a result, deeplearning models tend to learn spurious correlations instead of causally relatedfeatures, limiting their generalizability to new and unseen data. This problemcan be addressed by minimizing dependence measures between intermediaterepresentations of task-related and non-task-related variables. These measuresinclude mutual information, distance correlation, and the performance ofadversarial classifiers. Here, we benchmark such dependence measures for thetask of preventing shortcut learning. We study a simplified setting usingMorpho-MNIST and a medical imaging task with CheXpert chest radiographs. Ourresults provide insights into how to mitigate confounding factors in medicalimaging.</description><author>Sarah M√ºller, Louisa Fay, Lisa M. Koch, Sergios Gatidis, Thomas K√ºstner, Philipp Berens</author><pubDate>Fri, 26 Jul 2024 14:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18792v1</guid></item><item><title>Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation</title><link>http://arxiv.org/abs/2407.18789v1</link><description>Applying differential privacy (DP) by means of the DP-SGD algorithm toprotect individual data points during training is becoming increasingly popularin NLP. However, the choice of granularity at which DP is applied is oftenneglected. For example, neural machine translation (NMT) typically operates onthe sentence-level granularity. From the perspective of DP, this setup assumesthat each sentence belongs to a single person and any two sentences in thetraining dataset are independent. This assumption is however violated in manyreal-world NMT datasets, e.g. those including dialogues. For proper applicationof DP we thus must shift from sentences to entire documents. In this paper, weinvestigate NMT at both the sentence and document levels, analyzing theprivacy/utility trade-off for both scenarios, and evaluating the risks of notusing the appropriate privacy granularity in terms of leaking personallyidentifiable information (PII). Our findings indicate that the document-levelNMT system is more resistant to membership inference attacks, emphasizing thesignificance of using the appropriate granularity when working with DP.</description><author>Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal</author><pubDate>Fri, 26 Jul 2024 14:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18789v1</guid></item><item><title>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</title><link>http://arxiv.org/abs/2402.17372v2</link><description>Point cloud matching, a crucial technique in computer vision, medical androbotics fields, is primarily concerned with finding correspondences betweenpairs of point clouds or voxels. In some practical scenarios, emphasizing localdifferences is crucial for accurately identifying a correct match, therebyenhancing the overall robustness and reliability of the matching process.Commonly used shape descriptors have several limitations and often fail toprovide meaningful local insights about the paired geometries. In this work, wepropose a new technique, based on graph Laplacian eigenmaps, to match pointclouds by taking into account fine local structures. To deal with the order andsign ambiguity of Laplacian eigenmaps, we introduce a new operator, calledCoupled Laplacian (https://github.com/matteo-bastico/CoupLap), that allows toeasily generate aligned eigenspaces for multiple registered geometries. We showthat the similarity between those aligned high-dimensional spaces provides alocally meaningful score to match shapes. We firstly evaluate the performanceof the proposed technique in a point-wise manner, focusing on the task ofobject anomaly localization on the MVTec 3D-AD dataset. Additionally, we definea new medical task, called automatic Bone Side Estimation (BSE), which weaddress through a global similarity score derived from coupled eigenspaces. Inorder to test it, we propose a benchmark collecting bone surface structuresfrom various public datasets. Our matching technique, based on CoupledLaplacian, outperforms other methods by reaching an impressive accuracy on bothtasks.</description><author>Matteo Bastico, Etienne Decenci√®re, Laurent Cort√©, Yannick Tillier, David Ryckelynck</author><pubDate>Fri, 26 Jul 2024 14:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17372v2</guid></item><item><title>The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs</title><link>http://arxiv.org/abs/2407.18786v1</link><description>This paper studies gender bias in machine translation through the lens ofLarge Language Models (LLMs). Four widely-used test sets are employed tobenchmark various base LLMs, comparing their translation quality and genderbias against state-of-the-art Neural Machine Translation (NMT) models forEnglish to Catalan (En $\rightarrow$ Ca) and English to Spanish (En$\rightarrow$ Es) translation directions. Our findings reveal pervasive genderbias across all models, with base LLMs exhibiting a higher degree of biascompared to NMT models. To combat this bias, we explore prompting engineeringtechniques applied to an instruction-tuned LLM. We identify a prompt structurethat significantly reduces gender bias by up to 12% on the WinoMT evaluationdataset compared to more straightforward prompts. These results significantlyreduce the gender bias accuracy gap between LLMs and traditional NMT systems.</description><author>Aleix Sant, Carlos Escolano, Audrey Mash, Francesca De Luca Fornaciari, Maite Melero</author><pubDate>Fri, 26 Jul 2024 14:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18786v1</guid></item><item><title>Understanding XAI Through the Philosopher's Lens: A Historical Perspective</title><link>http://arxiv.org/abs/2407.18782v1</link><description>Despite explainable AI (XAI) has recently become a hot topic and severaldifferent approaches have been developed, there is still a widespread beliefthat it lacks a convincing unifying foundation. On the other hand, over thepast centuries, the very concept of explanation has been the subject ofextensive philosophical analysis in an attempt to address the fundamentalquestion of "why" in the context of scientific law. However, this discussionhas rarely been connected with XAI. This paper tries to fill in this gap andaims to explore the concept of explanation in AI through an epistemologicallens. By comparing the historical development of both the philosophy of scienceand AI, an intriguing picture emerges. Specifically, we show that a gradualprogression has independently occurred in both domains from logical-deductiveto statistical models of explanation, thereby experiencing in both cases aparadigm shift from deterministic to nondeterministic and probabilisticcausality. Interestingly, we also notice that similar concepts haveindependently emerged in both realms such as, for example, the relation betweenexplanation and understanding and the importance of pragmatic factors. Ourstudy aims to be the first step towards understanding the philosophicalunderpinnings of the notion of explanation in AI, and we hope that our findingswill shed some fresh light on the elusive nature of XAI.</description><author>Martina Mattioli, Antonio Emanuele Cin√†, Marcello Pelillo</author><pubDate>Fri, 26 Jul 2024 14:44:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18782v1</guid></item><item><title>Learning production functions for supply chains with graph neural networks</title><link>http://arxiv.org/abs/2407.18772v1</link><description>The global economy relies on the flow of goods over supply chain networks,with nodes as firms and edges as transactions between firms. While we mayobserve these external transactions, they are governed by unseen productionfunctions, which determine how firms internally transform the input productsthey receive into output products that they sell. In this setting, it can beextremely valuable to infer these production functions, to better understandand improve supply chains, and to forecast future transactions more accurately.However, existing graph neural networks (GNNs) cannot capture these hiddenrelationships between nodes' inputs and outputs. Here, we introduce a new classof models for this setting, by combining temporal GNNs with a novel inventorymodule, which learns production functions via attention weights and a specialloss function. We evaluate our models extensively on real supply chains data,along with data generated from our new open-source simulator, SupplySim. Ourmodels successfully infer production functions, with a 6-50% improvement overbaselines, and forecast future transactions on real and synthetic data,outperforming baselines by 11-62%.</description><author>Serina Chang, Zhiyin Lin, Benjamin Yan, Swapnil Bembde, Qi Xiu, Chi Heem Wong, Yu Qin, Frank Kloster, Alex Luo, Raj Palleti, Jure Leskovec</author><pubDate>Fri, 26 Jul 2024 14:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18772v1</guid></item><item><title>Any four real numbers are on all fours with analogy</title><link>http://arxiv.org/abs/2407.18770v1</link><description>This work presents a formalization of analogy on numbers that relies ongeneralized means. It is motivated by recent advances in artificialintelligence and applications of machine learning, where the notion of analogyis used to infer results, create data and even as an assessment tool of objectrepresentations, or embeddings, that are basically collections of numbers(vectors, matrices, tensors). This extended analogy use asks for mathematicalfoundations and clear understanding of the notion of analogy between numbers.We propose a unifying view of analogies that relies on generalized meansdefined in terms of a power parameter. In particular, we show that any fourincreasing positive real numbers is an analogy in a unique suitable power. Inaddition, we show that any such analogy can be reduced to an equivalentarithmetic analogy and that any analogical equation has a solution forincreasing numbers, which generalizes without restriction to complex numbers.These foundational results provide a better understanding of analogies in areaswhere representations are numerical.</description><author>Yves Lepage, Miguel Couceiro</author><pubDate>Fri, 26 Jul 2024 14:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18770v1</guid></item><item><title>TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals</title><link>http://arxiv.org/abs/2407.18764v1</link><description>Efforts directed towards promoting Open Government Data (OGD) have gainedsignificant traction across various governmental tiers since the mid-2000s. Asmore datasets are published on OGD portals, finding specific data becomesharder, leading to information overload. Complete and accurate documentation ofdatasets, including association of proper tags with datasets is key toimproving dataset findability and accessibility. Analysis conducted on theEstonian Open Data Portal, revealed that 11% datasets have no associated tags,while 26% had only one tag assigned to them, which underscores challenges indata findability and accessibility within the portal, which, according to therecent Open Data Maturity Report, is considered trend-setter. The aim of thisstudy is to propose an automated solution to tagging datasets to improve datafindability on OGD portals. This paper presents Tagify - a prototype of tagginginterface that employs large language models (LLM) such as GPT-3.5-turbo andGPT-4 to automate dataset tagging, generating tags for datasets in English andEstonian, thereby augmenting metadata preparation by data publishers andimproving data findability on OGD portals by data users. The developed solutionwas evaluated by users and their feedback was collected to define an agenda forfuture prototype improvements.</description><author>Kevin Kliimask, Anastasija Nikiforova</author><pubDate>Fri, 26 Jul 2024 14:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18764v1</guid></item><item><title>Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification</title><link>http://arxiv.org/abs/2405.06468v2</link><description>The task of medical image recognition is notably complicated by the presenceof varied and multiple pathological indications, presenting a unique challengein multi-label classification with unseen labels. This complexity underlinesthe need for computer-aided diagnosis methods employing multi-label zero-shotlearning. Recent advancements in pre-trained vision-language models (VLMs) haveshowcased notable zero-shot classification abilities on medical images.However, these methods have limitations on leveraging extensive pre-trainedknowledge from broader image datasets, and often depend on manual promptconstruction by expert radiologists. By automating the process of prompttuning, prompt learning techniques have emerged as an efficient way to adaptVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short inperforming class-specific prompts on unseen categories, limitinggeneralizability in fine-grained scenarios. To overcome these constraints, weintroduce a novel prompt generation approach inspirited by text generation innatural language processing (NLP). Our method, named Pseudo-Prompt Generating(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuringa RNN-based decoder, PsPG autoregressively generates class-tailored embeddingvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-labelchest radiograph datasets affirm the superiority of our approach againstleading medical vision-language and multi-label prompt learning methods. Thesource code is available at https://github.com/fallingnight/PsPG</description><author>Yaoqin Ye, Junjie Zhang, Hongwei Shi</author><pubDate>Fri, 26 Jul 2024 14:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06468v2</guid></item><item><title>Unsupervised Reservoir Computing for Multivariate Denoising of Severely Contaminated Signals</title><link>http://arxiv.org/abs/2407.18759v1</link><description>The interdependence and high dimensionality of multivariate signals presentsignificant challenges for denoising, as conventional univariate methods oftenstruggle to capture the complex interactions between variables. A successfulapproach must consider not only the multivariate dependencies of the desiredsignal but also the multivariate dependencies of the interfering noise. In ourprevious research, we introduced a method using machine learning to extract themaximum portion of ``predictable information" from univariate signal. We extendthis approach to multivariate signals, with the key idea being to properlyincorporate the interdependencies of the noise back into the interdependentreconstruction of the signal. The method works successfully for variousmultivariate signals, including chaotic signals and highly oscillatingsinusoidal signals which are corrupted by spatially correlated intensive noise.It consistently outperforms other existing multivariate denoising methodsacross a wide range of scenarios.</description><author>Jaesung Choi, Pilwon Kim</author><pubDate>Fri, 26 Jul 2024 14:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18759v1</guid></item><item><title>Large Language Model for Table Processing: A Survey</title><link>http://arxiv.org/abs/2402.05121v2</link><description>Tables, typically two-dimensional and structured to store large amounts ofdata, are essential in daily activities like database queries, spreadsheetmanipulations, web table question answering, and image table informationextraction. Automating these table-centric tasks with Large Language Models(LLMs) or Visual Language Models (VLMs) offers significant public benefits,garnering interest from academia and industry. This survey provides acomprehensive overview of table-related tasks, examining both user scenariosand technical aspects. It covers traditional tasks like table questionanswering as well as emerging fields such as spreadsheet manipulation and tabledata analysis. We summarize the training techniques for LLMs and VLMs tailoredfor table processing. Additionally, we discuss prompt engineering, particularlythe use of LLM-powered agents, for various table-related tasks. Finally, wehighlight several challenges, including processing implicit user intentions andextracting information from various table sources.</description><author>Weizheng Lu, Jing Zhang, Ju Fan, Zihao Fu, Yueguo Chen, Xiaoyong Du</author><pubDate>Fri, 26 Jul 2024 14:12:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05121v2</guid></item><item><title>Evaluating Human Trajectory Prediction with Metamorphic Testing</title><link>http://arxiv.org/abs/2407.18756v1</link><description>The prediction of human trajectories is important for planning in autonomoussystems that act in the real world, e.g. automated driving or mobile robots.Human trajectory prediction is a noisy process, and no prediction doesprecisely match any future trajectory. It is therefore approached as astochastic problem, where the goal is to minimise the error between the trueand the predicted trajectory. In this work, we explore the application ofmetamorphic testing for human trajectory prediction. Metamorphic testing isdesigned to handle unclear or missing test oracles. It is well-designed forhuman trajectory prediction, where there is no clear criterion of correct orincorrect human behaviour. Metamorphic relations rely on transformations oversource test cases and exploit invariants. A setting well-designed for humantrajectory prediction where there are many symmetries of expected humanbehaviour under variations of the input, e.g. mirroring and rescaling of theinput data. We discuss how metamorphic testing can be applied to stochastichuman trajectory prediction and introduce the Wasserstein Violation Criterionto statistically assess whether a follow-up test case violates alabel-preserving metamorphic relation.</description><author>Helge Spieker, Nassim Belmecheri, Arnaud Gotlieb, Nadjib Lazaar</author><pubDate>Fri, 26 Jul 2024 14:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18756v1</guid></item><item><title>Unsqueeze [CLS] Bottleneck to Learn Rich Representations</title><link>http://arxiv.org/abs/2407.17671v2</link><description>Distillation-based self-supervised learning typically leads to morecompressed representations due to its radical clustering process and theimplementation of a sharper target distribution. To overcome this limitationand preserve more information from input, we introduce UDI, conceptualized asUnsqueezed Distillation-based self-supervised learning (SSL). UDI enriches thelearned representation by encouraging multimodal prediction distilled from aconsolidated profile of local predictions that are derived via stratifiedsampling. Our evaluations show that UDI not only promotes semanticallymeaningful representations at instance level, delivering superior orcompetitive results to state-of-the-art SSL methods in image classification,but also effectively preserves the nuisance of input, which yields significantimprovement in dense prediction tasks, including object detection andsegmentation. Additionally, UDI performs competitively in low-shot imageclassification, improving the scalability of joint-embedding pipelines. Variousvisualizations and ablation studies are presented to further elucidate themechanisms behind UDI. Our source code is available athttps://github.com/ISL-CV/udi.</description><author>Qing Su, Shihao Ji</author><pubDate>Fri, 26 Jul 2024 14:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17671v2</guid></item><item><title>Score matching through the roof: linear, nonlinear, and latent variables causal discovery</title><link>http://arxiv.org/abs/2407.18755v1</link><description>Causal discovery from observational data holds great promise, but existingmethods rely on strong assumptions about the underlying causal structure, oftenrequiring full observability of all relevant variables. We tackle thesechallenges by leveraging the score function $\nabla \log p(X)$ of observedvariables for causal discovery and propose the following contributions. First,we generalize the existing results of identifiability with the score toadditive noise models with minimal requirements on the causal mechanisms.Second, we establish conditions for inferring causal relations from the scoreeven in the presence of hidden variables; this result is two-faced: wedemonstrate the score's potential as an alternative to conditional independencetests to infer the equivalence class of causal graphs with hidden variables,and we provide the necessary conditions for identifying direct causes in latentvariable models. Building on these insights, we propose a flexible algorithmfor causal discovery across linear, nonlinear, and latent variable models,which we empirically validate.</description><author>Francesco Montagna, Philipp M. Faller, Patrick Bloebaum, Elke Kirschbaum, Francesco Locatello</author><pubDate>Fri, 26 Jul 2024 14:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18755v1</guid></item><item><title>Attacks on fairness in Federated Learning</title><link>http://arxiv.org/abs/2311.12715v2</link><description>Federated Learning is an important emerging distributed training paradigmthat keeps data private on clients. It is now well understood that bycontrolling only a small subset of FL clients, it is possible to introduce abackdoor to a federated learning model, in the presence of certain attributes.In this paper, we present a new type of attack that compromises the fairness ofthe trained model. Fairness is understood to be the attribute-level performancedistribution of a trained model. It is particularly salient in domains where,for example, skewed accuracy discrimination between subpopulations could havedisastrous consequences. We find that by employing a threat model similar tothat of a backdoor attack, an attacker is able to influence the aggregatedmodel to have an unfair performance distribution between any given set ofattributes. Furthermore, we find that this attack is possible by controllingonly a single client. While combating naturally induced unfairness in FL haspreviously been discussed in depth, its artificially induced kind has beenneglected. We show that defending against attacks on fairness should be acritical consideration in any situation where unfairness in a trained modelcould benefit a user who participated in its training.</description><author>Joseph Rance, Filip Svoboda</author><pubDate>Fri, 26 Jul 2024 14:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12715v2</guid></item><item><title>Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery</title><link>http://arxiv.org/abs/2407.18752v1</link><description>Causal discovery aims to estimate causal structures among variables based onobservational data. Large Language Models (LLMs) offer a fresh perspective totackle the causal discovery problem by reasoning on the metadata associatedwith variables rather than their actual data values, an approach referred to asknowledge-based causal discovery. In this paper, we investigate thecapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1billion parameters) with prompt-based learning for knowledge-based causaldiscovery. Specifically, we present KG Structure as Prompt, a novel approachfor integrating structural information from a knowledge graph, such as commonneighbor nodes and metapaths, into prompt-based learning to enhance thecapabilities of SLMs. Experimental results on three types of biomedical andopen-domain datasets under few-shot settings demonstrate the effectiveness ofour approach, surpassing most baselines and even conventional fine-tuningapproaches trained on full datasets. Our findings further highlight the strongcapabilities of SLMs: in combination with knowledge graphs and prompt-basedlearning, SLMs demonstrate the potential to surpass LLMs with larger number ofparameters. Our code and datasets are available on GitHub.</description><author>Yuni Susanti, Michael F√§rber</author><pubDate>Fri, 26 Jul 2024 14:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18752v1</guid></item><item><title>FLUE: Federated Learning with Un-Encrypted model weights</title><link>http://arxiv.org/abs/2407.18750v1</link><description>Federated Learning enables diverse devices to collaboratively train a sharedmodel while keeping training data locally stored, avoiding the need forcentralized cloud storage. Despite existing privacy measures, concerns arisefrom potential reverse engineering of gradients, even with added noise,revealing private data. To address this, recent research emphasizes usingencrypted model parameters during training. This paper introduces a novelfederated learning algorithm, leveraging coded local gradients withoutencryption, exchanging coded proxies for model parameters, and injectingsurplus noise for enhanced privacy. Two algorithm variants are presented,showcasing convergence and learning rates adaptable to coding schemes and rawdata characteristics. Two encryption-free implementations with fixed and randomcoding matrices are provided, demonstrating promising simulation results fromboth federated optimization and machine learning perspectives.</description><author>Elie Atallah</author><pubDate>Fri, 26 Jul 2024 14:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18750v1</guid></item><item><title>Multi-Robot System Architecture design in SysML and BPMN</title><link>http://arxiv.org/abs/2407.18749v1</link><description>Multi-Robot System (MRS) is a complex system that contains many differentsoftware and hardware components. This main problem addressed in this articleis the MRS design complexity. The proposed solution provides a modular modelingand simulation technique that is based on formal system engineering method,therefore the MRS design complexity is decomposed and reduced. Modeling the MRShas been achieved via two formal Architecture Description Languages (ADLs),which are Systems Modeling Language (SysML) and Business Process Model andNotation (BPMN), to design the system blueprints. By using those abstractdesign ADLs, the implementation of the project becomes technology agnostic.This allows to transfer the design concept from on programming language toanother. During the simulation phase, a multi-agent environment is used tosimulate the MRS blueprints. The simulation has been implemented in Java AgentDevelopment (JADE) middleware. Therefore, its results can be used to analysisand verify the proposed MRS model in form of performance evaluation matrix.</description><author>Ahmed R. Sadik, Christian Goerick</author><pubDate>Fri, 26 Jul 2024 14:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18749v1</guid></item><item><title>FairAIED: Navigating Fairness, Bias, and Ethics in Educational AI Applications</title><link>http://arxiv.org/abs/2407.18745v1</link><description>The integration of Artificial Intelligence (AI) into education hastransformative potential, providing tailored learning experiences and creativeinstructional approaches. However, the inherent biases in AI algorithms hinderthis improvement by unintentionally perpetuating prejudice against specificdemographics, especially in human-centered applications like education. Thissurvey delves deeply into the developing topic of algorithmic fairness ineducational contexts, providing a comprehensive evaluation of the diverseliterature on fairness, bias, and ethics in AI-driven educational applications.It identifies the common forms of biases, such as data-related, algorithmic,and user-interaction, that fundamentally undermine the accomplishment offairness in AI teaching aids. By outlining existing techniques for mitigatingthese biases, ranging from varied data gathering to algorithmic fairnessinterventions, the survey emphasizes the critical role of ethicalconsiderations and legal frameworks in shaping a more equitable educationalenvironment. Furthermore, it guides readers through the complexities offairness measurements, methods, and datasets, shedding light on the way to biasreduction. Despite these gains, this survey highlights long-standing issues,such as achieving a balance between fairness and accuracy, as well as the needfor diverse datasets. Overcoming these challenges and ensuring the ethical andfair use of AI's promise in education call for a collaborative,interdisciplinary approach.</description><author>Sribala Vidyadhari Chinta, Zichong Wang, Zhipeng Yin, Nhat Hoang, Matthew Gonzalez, Tai Le Quy, Wenbin Zhang</author><pubDate>Fri, 26 Jul 2024 13:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18745v1</guid></item><item><title>Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems</title><link>http://arxiv.org/abs/2407.15810v2</link><description>Facial Recognition Systems (FRSs) are being developed and deployed globallyat unprecedented rates. Most platforms are designed in a limited set ofcountries but deployed in worldwide, without adequate checkpoints. This isespecially problematic for Global South countries which lack strong legislationto safeguard persons facing disparate performance of these systems. Acombination of unavailability of datasets, lack of understanding of FRSfunctionality and low-resource bias mitigation measures accentuate the problem.In this work, we propose a new face dataset composed of 6,579 unique male andfemale sportspersons from eight countries around the world. More than 50% ofthe dataset comprises individuals from the Global South countries and isdemographically diverse. To aid adversarial audits and robust model training,each image has four adversarial variants, totaling over 40,000 images. We alsobenchmark five popular FRSs, both commercial and open-source, for the task ofgender prediction (and country prediction for one of the open-source models asan example of red-teaming). Experiments on industrial FRSs reveal accuraciesranging from 98.2%--38.1%, with a large disparity between males and females inthe Global South (max difference of 38.5%). Biases are also observed in allFRSs between females of the Global North and South (max difference of ~50%).Grad-CAM analysis identifies the nose, forehead and mouth as the regions ofinterest on one of the open-source FRSs. Utilizing this insight, we designsimple, low-resource bias mitigation solutions using few-shot and novelcontrastive learning techniques significantly improving the accuracy withdisparity between males and females reducing from 50% to 1.5% in one of thesettings. In the red-teaming experiment with the open-source Deepface model,contrastive learning proves more effective than simple fine-tuning.</description><author>Siddharth D Jaiswal, Animesh Ganai, Abhisek Dash, Saptarshi Ghosh, Animesh Mukherjee</author><pubDate>Fri, 26 Jul 2024 13:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15810v2</guid></item><item><title>Towards Effective and Efficient Continual Pre-training of Large Language Models</title><link>http://arxiv.org/abs/2407.18743v1</link><description>Continual pre-training (CPT) has been an important approach for adaptinglanguage models to specific domains or tasks. To make the CPT approach moretraceable, this paper presents a technical report for continually pre-trainingLlama-3 (8B), which significantly enhances the Chinese language ability andscientific reasoning ability of the backbone model. To enhance the newabilities while retaining the original abilities, we design specific datamixture and curriculum strategies by utilizing existing datasets andsynthesizing high-quality datasets. Specifically, we synthesizemultidisciplinary scientific question and answer (QA) pairs based on relatedweb pages, and subsequently incorporate these synthetic data to improve thescientific reasoning ability of Llama-3. We refer to the model after CPT asLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuningexperiments with a relatively small model -- TinyLlama, and employ the derivedfindings to train the backbone model. Extensive experiments on a number ofevaluation benchmarks show that our approach can largely improve theperformance of the backbone models, including both the general abilities (+8.81on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 onMATH and +4.13 on SciEval), without hurting the original capacities. Our model,data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.</description><author>Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ji-Rong Wen</author><pubDate>Fri, 26 Jul 2024 13:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18743v1</guid></item><item><title>MUVO: A Multimodal World Model with Spatial Representations for Autonomous Driving</title><link>http://arxiv.org/abs/2311.11762v3</link><description>Learning unsupervised world models for autonomous driving has the potentialto improve the reasoning capabilities of today's systems dramatically. However,most work neglects the physical attributes of the world and focuses on sensordata alone. We propose MUVO, a MUltimodal World Model with spatial VOxelrepresentations, to address this challenge. We utilize raw camera and lidardata to learn a sensor-agnostic geometric representation of the world. Wedemonstrate multimodal future predictions and show that our spatialrepresentation improves the prediction quality of both camera images and lidarpoint clouds.</description><author>Daniel Bogdoll, Yitian Yang, Tim Joseph, J. Marius Z√∂llner</author><pubDate>Fri, 26 Jul 2024 13:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11762v3</guid></item><item><title>Towards Generalized Offensive Language Identification</title><link>http://arxiv.org/abs/2407.18738v1</link><description>The prevalence of offensive content on the internet, encompassing hate speechand cyberbullying, is a pervasive issue worldwide. Consequently, it hasgarnered significant attention from the machine learning (ML) and naturallanguage processing (NLP) communities. As a result, numerous systems have beendeveloped to automatically identify potentially harmful content and mitigateits impact. These systems can follow two approaches; (1) Use publicly availablemodels and application endpoints, including prompting large language models(LLMs) (2) Annotate datasets and train ML models on them. However, bothapproaches lack an understanding of how generalizable they are. Furthermore,the applicability of these systems is often questioned in off-domain andpractical environments. This paper empirically evaluates the generalizabilityof offensive language detection models and datasets across a novel generalizedbenchmark. We answer three research questions on generalizability. Our findingswill be useful in creating robust real-world offensive language detectionsystems.</description><author>Alphaeus Dmonte, Tejas Arya, Tharindu Ranasinghe, Marcos Zampieri</author><pubDate>Fri, 26 Jul 2024 13:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18738v1</guid></item><item><title>Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents</title><link>http://arxiv.org/abs/2403.04202v4</link><description>Growing concerns about safety and alignment of AI systems highlight theimportance of embedding moral capabilities in artificial agents: a promisingsolution is the use of learning from experience, i.e., Reinforcement Learning.In multi-agent (social) environments, complex population-level phenomena mayemerge from interactions between individual learning agents. Many of theexisting studies rely on simulated social dilemma environments to study theinteractions of independent learning agents; however, they tend to ignore themoral heterogeneity that is likely to be present in societies of agents inpractice. For example, at different points in time a single learning agent mayface opponents who are consequentialist (i.e., focused on maximizing outcomesover time), norm-based (i.e., conforming to specific norms), or virtue-based(i.e., considering a combination of different virtues). The extent to whichagents' co-development may be impacted by such moral heterogeneity inpopulations is not well understood. In this paper, we present a study of thelearning dynamics of morally heterogeneous populations interacting in a socialdilemma setting. Using an Iterated Prisoner's Dilemma environment with apartner selection mechanism, we investigate the extent to which the prevalenceof diverse moral agents in populations affects individual agents' learningbehaviors and emergent population-level outcomes. We observe several types ofnon-trivial interactions between pro-social and anti-social agents, and findthat certain types of moral agents are able to steer selfish agents towardsmore cooperative behavior.</description><author>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</author><pubDate>Fri, 26 Jul 2024 13:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04202v4</guid></item><item><title>Outlier detection by ensembling uncertainty with negative objectness</title><link>http://arxiv.org/abs/2402.15374v3</link><description>Outlier detection is an essential capability in safety-critical applicationsof supervised visual recognition. Most of the existing methods deliver bestresults by encouraging standard closed-set models to produce low-confidencepredictions in negative training data. However, that approach conflatesprediction uncertainty with recognition of the negative class. We thereforereconsider direct prediction of K+1 logits that correspond to K groundtruthclasses and one outlier class. This setup allows us to formulate a novelanomaly score as an ensemble of in-distribution uncertainty and the posteriorof the outlier class which we term negative objectness. Now outliers can beindependently detected due to i) high prediction uncertainty or ii) similaritywith negative data. We embed our method into a dense prediction architecturewith mask-level recognition over K+2 classes. The training procedure encouragesthe novel K+2-th class to learn negative objectness at pasted negativeinstances. Our models outperform the current state-of-the art on standardbenchmarks for image-wide and pixel-level outlier detection with and withouttraining on real negative data.</description><author>Anja Deliƒá, Matej Grciƒá, Sini≈°a ≈†egviƒá</author><pubDate>Fri, 26 Jul 2024 13:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15374v3</guid></item><item><title>Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities</title><link>http://arxiv.org/abs/2402.07039v3</link><description>Harm reporting in Artificial Intelligence (AI) currently lacks a structuredprocess for disclosing and addressing algorithmic flaws, relying largely on anad-hoc approach. This contrasts sharply with the well-established CoordinatedVulnerability Disclosure (CVD) ecosystem in software security. While globalefforts to establish frameworks for AI transparency and collaboration areunderway, the unique challenges presented by machine learning (ML) modelsdemand a specialized approach. To address this gap, we propose implementing aCoordinated Flaw Disclosure (CFD) framework tailored to the complexities of MLand AI issues. This paper reviews the evolution of ML disclosure practices,from ad hoc reporting to emerging participatory auditing methods, and comparesthem with cybersecurity norms. Our framework introduces innovations such asextended model cards, dynamic scope expansion, an independent adjudicationpanel, and an automated verification process. We also outline a forthcomingreal-world pilot of CFD. We argue that CFD could significantly enhance publictrust in AI systems. By balancing organizational and community interests, CFDaims to improve AI accountability in a rapidly evolving technologicallandscape.</description><author>Sven Cattell, Avijit Ghosh, Lucie-Aim√©e Kaffee</author><pubDate>Fri, 26 Jul 2024 13:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07039v3</guid></item><item><title>AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning</title><link>http://arxiv.org/abs/2407.18735v1</link><description>In this paper, we introduce AutoRDF2GML, a framework designed to convert RDFdata into data representations tailored for graph machine learning tasks.AutoRDF2GML enables, for the first time, the creation of both content-basedfeatures -- i.e., features based on RDF datatype properties -- andtopology-based features -- i.e., features based on RDF object properties.Characterized by automated feature extraction, AutoRDF2GML makes it possibleeven for users less familiar with RDF and SPARQL to generate datarepresentations ready for graph machine learning tasks, such as linkprediction, node classification, and graph classification. Furthermore, wepresent four new benchmark datasets for graph machine learning, created fromlarge RDF knowledge graphs using our framework. These datasets serve asvaluable resources for evaluating graph machine learning approaches, such asgraph neural networks. Overall, our framework effectively bridges the gapbetween the Graph Machine Learning and Semantic Web communities, paving the wayfor RDF-based machine learning applications.</description><author>Michael F√§rber, David Lamprecht, Yuni Susanti</author><pubDate>Fri, 26 Jul 2024 13:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18735v1</guid></item><item><title>A Physics-Informed Neural Network-Based Approach for the Spatial Upsampling of Spherical Microphone Arrays</title><link>http://arxiv.org/abs/2407.18732v1</link><description>Spherical microphone arrays are convenient tools for capturing the spatialcharacteristics of a sound field. However, achieving superior spatialresolution requires arrays with numerous capsules, consequently leading toexpensive devices. To address this issue, we present a method for spatiallyupsampling spherical microphone arrays with a limited number of capsules. Ourapproach exploits a physics-informed neural network with Rowdy activationfunctions, leveraging physical constraints to provide high-order microphonearray signals, starting from low-order devices. Results show that, within itsdomain of application, our approach outperforms a state of the art method basedon signal processing for spherical microphone arrays upsampling.</description><author>Federico Miotello, Ferdinando Terminiello, Mirco Pezzoli, Alberto Bernardini, Fabio Antonacci, Augusto Sarti</author><pubDate>Fri, 26 Jul 2024 13:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18732v1</guid></item><item><title>Using representation balancing to learn conditional-average dose responses from clustered data</title><link>http://arxiv.org/abs/2309.03731v2</link><description>Estimating a unit's responses to interventions with an associated dose, the"conditional average dose response" (CADR), is relevant in a variety ofdomains, from healthcare to business, economics, and beyond. Such a responsetypically needs to be estimated from observational data, which introducesseveral challenges. That is why the machine learning (ML) community hasproposed several tailored CADR estimators. Yet, the proposal of most of thesemethods requires strong assumptions on the distribution of data and theassignment of interventions, which go beyond the standard assumptions in causalinference. Whereas previous works have so far focused on smooth shifts incovariate distributions across doses, in this work, we will study estimatingCADR from clustered data and where different doses are assigned to differentsegments of a population. On a novel benchmarking dataset, we show the impactsof clustered data on model performance and propose an estimator, CBRNet, thatlearns cluster-agnostic and hence dose-agnostic covariate representationsthrough representation balancing for unbiased CADR inference. We run extensiveexperiments to illustrate the workings of our method and compare it with thestate of the art in ML for CADR estimation.</description><author>Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, Wouter Verbeke</author><pubDate>Fri, 26 Jul 2024 13:33:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03731v2</guid></item><item><title>SCB-dataset: A Dataset for Detecting Student Classroom Behavior</title><link>http://arxiv.org/abs/2304.02488v2</link><description>The use of deep learning methods for automatic detection of students'classroom behavior is a promising approach to analyze their class performanceand enhance teaching effectiveness. However, the lack of publicly availabledatasets on student behavior poses a challenge for researchers in this field.To address this issue, we propose a Student Classroom Behavior dataset(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248labels and 4,003 images, with a focus on hand-raising behavior. We evaluatedthe dataset using the YOLOv7 algorithm, achieving a mean average precision(map) of up to 85.3%. We believe that our dataset can serve as a robustfoundation for future research in the field of student behavior detection andpromote further advancements in this area.Our SCB-dataset can be downloadedfrom: https://github.com/Whiffe/SCB-dataset</description><author>Fan Yang</author><pubDate>Fri, 26 Jul 2024 13:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02488v2</guid></item><item><title>Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of Shakespeare and Milton</title><link>http://arxiv.org/abs/2407.18730v1</link><description>In this work we present a corpus of poems by William Shakespeare and JohnMilton that have been enriched with readings from the public domain. We havealigned all the lines with their respective audio segments, at the line, word,syllable and phone level, and we have included their scansion. We make a basicvisualization platform for these poems and we conclude by conjecturing possiblefuture directions.</description><author>Manex Agirrezabal</author><pubDate>Fri, 26 Jul 2024 13:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18730v1</guid></item><item><title>MMPolymer: A Multimodal Multitask Pretraining Framework for Polymer Property Prediction</title><link>http://arxiv.org/abs/2406.04727v2</link><description>Polymers are high-molecular-weight compounds constructed by the covalentbonding of numerous identical or similar monomers so that their 3D structuresare complex yet exhibit unignorable regularity. Typically, the properties of apolymer, such as plasticity, conductivity, bio-compatibility, and so on, arehighly correlated with its 3D structure. However, existing polymer propertyprediction methods heavily rely on the information learned from polymer SMILESsequences (P-SMILES strings) while ignoring crucial 3D structural information,resulting in sub-optimal performance. In this work, we propose MMPolymer, anovel multimodal multitask pretraining framework incorporating polymer 1Dsequential and 3D structural information to encourage downstream polymerproperty prediction tasks. Besides, considering the scarcity of polymer 3Ddata, we further introduce the "Star Substitution" strategy to extract 3Dstructural information effectively. During pretraining, in addition topredicting masked tokens and recovering clear 3D coordinates, MMPolymerachieves the cross-modal alignment of latent representations. Then we furtherfine-tune the pretrained MMPolymer for downstream polymer property predictiontasks in the supervised learning paradigm. Experiments show that MMPolymerachieves state-of-the-art performance in downstream property prediction tasks.Moreover, given the pretrained MMPolymer, utilizing merely a single modality inthe fine-tuning phase can also outperform existing methods, showcasing theexceptional capability of MMPolymer in polymer feature extraction andutilization.</description><author>Fanmeng Wang, Wentao Guo, Minjie Cheng, Shen Yuan, Hongteng Xu, Zhifeng Gao</author><pubDate>Fri, 26 Jul 2024 13:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04727v2</guid></item><item><title>LLASP: Fine-tuning Large Language Models for Answer Set Programming</title><link>http://arxiv.org/abs/2407.18723v1</link><description>Recently, Large Language Models (LLMs) have showcased their potential invarious natural language processing tasks, including code generation. However,while significant progress has been made in adapting LLMs to generate code forseveral imperative programming languages and tasks, there remains a notable gapin their application to declarative formalisms, such as Answer Set Programming(ASP). In this paper, we move a step towards exploring the capabilities of LLMsfor ASP code generation. First, we perform a systematic evaluation of severalstate-of-the-art LLMs. Despite their power in terms of number of parameters,training data and computational resources, empirical results demonstrateinadequate performances in generating correct ASP programs. Therefore, wepropose LLASP, a fine-tuned lightweight model specifically trained to encodefundamental ASP program patterns. To this aim, we create an ad-hoc datasetcovering a wide variety of fundamental problem specifications that can beencoded in ASP. Our experiments demonstrate that the quality of ASP programsgenerated by LLASP is remarkable. This holds true not only when compared to thenon-fine-tuned counterpart but also when compared to the majority of eager LLMcandidates, particularly from a semantic perspective. All the code and dataused to perform the experiments are publicly available athttps://anonymous.4open.science/r/LLASP-D86C/.</description><author>Erica Coppolillo, Francesco Calimeri, Giuseppe Manco, Simona Perri, Francesco Ricca</author><pubDate>Fri, 26 Jul 2024 13:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18723v1</guid></item><item><title>Relightable Neural Actor with Intrinsic Decomposition and Pose Control</title><link>http://arxiv.org/abs/2312.11587v2</link><description>Creating a controllable and relightable digital avatar from multi-view videowith fixed illumination is a very challenging problem since humans are highlyarticulated, creating pose-dependent appearance effects, and skin as well asclothing require space-varying BRDF modeling. Existing works on creatinganimatible avatars either do not focus on relighting at all, require controlledillumination setups, or try to recover a relightable avatar from very low costsetups, i.e. a single RGB video, at the cost of severely limited resultquality, e.g. shadows not even being modeled. To address this, we proposeRelightable Neural Actor, a new video-based method for learning a pose-drivenneural human model that can be relighted, allows appearance editing, and modelspose-dependent effects such as wrinkles and self-shadows. Importantly, fortraining, our method solely requires a multi-view recording of the human undera known, but static lighting condition. To tackle this challenging problem, weleverage an implicit geometry representation of the actor with a drivabledensity field that models pose-dependent deformations and derive a dynamicmapping between 3D and UV spaces, where normal, visibility, and materials areeffectively encoded. To evaluate our approach in real-world scenarios, wecollect a new dataset with four identities recorded under different lightconditions, indoors and outdoors, providing the first benchmark of its kind forhuman relighting, and demonstrating state-of-the-art relighting results fornovel human poses.</description><author>Diogo Luvizon, Vladislav Golyanik, Adam Kortylewski, Marc Habermann, Christian Theobalt</author><pubDate>Fri, 26 Jul 2024 13:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11587v2</guid></item><item><title>Neurosymbolic AI for Enhancing Instructability in Generative AI</title><link>http://arxiv.org/abs/2407.18722v1</link><description>Generative AI, especially via Large Language Models (LLMs), has transformedcontent creation across text, images, and music, showcasing capabilities infollowing instructions through prompting, largely facilitated by instructiontuning. Instruction tuning is a supervised fine-tuning method where LLMs aretrained on datasets formatted with specific tasks and correspondinginstructions. This method systematically enhances the model's ability tocomprehend and execute the provided directives. Despite these advancements,LLMs still face challenges in consistently interpreting complex, multi-stepinstructions and generalizing them to novel tasks, which are essential forbroader applicability in real-world scenarios. This article explores whyneurosymbolic AI offers a better path to enhance the instructability of LLMs.We explore the use a symbolic task planner to decompose high-level instructionsinto structured tasks, a neural semantic parser to ground these tasks intoexecutable actions, and a neuro-symbolic executor to implement these actionswhile dynamically maintaining an explicit representation of state. We also seekto show that neurosymbolic approach enhances the reliability andcontext-awareness of task execution, enabling LLMs to dynamically interpret andrespond to a wider range of instructional contexts with greater precision andflexibility.</description><author>Amit Sheth, Vishal Pallagani, Kaushik Roy</author><pubDate>Fri, 26 Jul 2024 13:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18722v1</guid></item><item><title>ChatSchema: A pipeline of extracting structured information with Large Multimodal Models based on schema</title><link>http://arxiv.org/abs/2407.18716v1</link><description>Objective: This study introduces ChatSchema, an effective method forextracting and structuring information from unstructured data in medical paperreports using a combination of Large Multimodal Models (LMMs) and OpticalCharacter Recognition (OCR) based on the schema. By integrating predefinedschema, we intend to enable LMMs to directly extract and standardizeinformation according to the schema specifications, facilitating further dataentry. Method: Our approach involves a two-stage process, includingclassification and extraction for categorizing report scenarios and structuringinformation. We established and annotated a dataset to verify the effectivenessof ChatSchema, and evaluated key extraction using precision, recall, F1-score,and accuracy metrics. Based on key extraction, we further assessed valueextraction. We conducted ablation studies on two LMMs to illustrate theimprovement of structured information extraction with different input modalsand methods. Result: We analyzed 100 medical reports from Peking UniversityFirst Hospital and established a ground truth dataset with 2,945 key-valuepairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found ahigher overall performance of GPT-4o. The results are as follows: For theresult of key extraction, key-precision was 98.6%, key-recall was 98.5%,key-F1-score was 98.6%. For the result of value extraction based on correct keyextraction, the overall accuracy was 97.2%, precision was 95.8%, recall was95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchemaachieved significantly higher overall accuracy and overall F1-score ofkey-value extraction, compared to the Baseline, with increases of 26.9% overallaccuracy and 27.4% overall F1-score, respectively.</description><author>Fei Wang, Yuewen Zheng, Qin Li, Jingyi Wu, Pengfei Li, Luxia Zhang</author><pubDate>Fri, 26 Jul 2024 13:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18716v1</guid></item><item><title>Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer</title><link>http://arxiv.org/abs/2407.12322v2</link><description>Recently, transformers have demonstrated great potential for modelinglong-term dependencies from skeleton sequences and thereby gainedever-increasing attention in skeleton action recognition. However, the existingtransformer-based approaches heavily rely on the naive attention mechanism forcapturing the spatiotemporal features, which falls short in learningdiscriminative representations that exhibit similar motion patterns. To addressthis challenge, we introduce the Frequency-aware Mixed Transformer(FreqMixFormer), specifically designed for recognizing similar skeletal actionswith subtle discriminative motions. First, we introduce a frequency-awareattention module to unweave skeleton frequency representations by embeddingjoint features into frequency attention maps, aiming to distinguish thediscriminative movements based on their frequency coefficients. Subsequently,we develop a mixed transformer architecture to incorporate spatial featureswith frequency features to model the comprehensive frequency-spatial patterns.Additionally, a temporal transformer is proposed to extract the globalcorrelations across frames. Extensive experiments show that FreqMiXFormeroutperforms SOTA on 3 popular skeleton action recognition datasets, includingNTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.</description><author>Wenhan Wu, Ce Zheng, Zihao Yang, Chen Chen, Srijan Das, Aidong Lu</author><pubDate>Fri, 26 Jul 2024 13:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12322v2</guid></item><item><title>BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation</title><link>http://arxiv.org/abs/2407.18715v1</link><description>Scene Graph Generation (SGG) remains a challenging task due to itscompositional property. Previous approaches improve prediction efficiency bylearning in an end-to-end manner. However, these methods exhibit limitedperformance as they assume unidirectional conditioning between entities andpredicates, leading to insufficient information interaction. To address thislimitation, we propose a novel bidirectional conditioning factorization forSGG, introducing efficient interaction between entities and predicates.Specifically, we develop an end-to-end scene graph generation model,Bidirectional Conditioning Transformer (BCTR), to implement our factorization.BCTR consists of two key modules. First, the Bidirectional ConditioningGenerator (BCG) facilitates multi-stage interactive feature augmentationbetween entities and predicates, enabling mutual benefits between the twopredictions. Second, Random Feature Alignment (RFA) regularizes the featurespace by distilling multi-modal knowledge from pre-trained models, enhancingBCTR's ability on tailed categories without relying on statistical priors. Weconduct a series of experiments on Visual Genome and Open Image V6,demonstrating that BCTR achieves state-of-the-art performance on bothbenchmarks. The code will be available upon acceptance of the paper.</description><author>Peng Hao, Xiaobing Wang, Yingying Jiang, Hanchao Jia, Xiaoshuai Hao</author><pubDate>Fri, 26 Jul 2024 13:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18715v1</guid></item><item><title>Merit-based Fair Combinatorial Semi-Bandit with Unrestricted Feedback Delays</title><link>http://arxiv.org/abs/2407.15439v2</link><description>We study the stochastic combinatorial semi-bandit problem with unrestrictedfeedback delays under merit-based fairness constraints. This is motivated byapplications such as crowdsourcing, and online advertising, where immediatefeedback is not immediately available and fairness among different choices (orarms) is crucial. We consider two types of unrestricted feedback delays:reward-independent delays where the feedback delays are independent of therewards, and reward-dependent delays where the feedback delays are correlatedwith the rewards. Furthermore, we introduce merit-based fairness constraints toensure a fair selection of the arms. We define the reward regret and thefairness regret and present new bandit algorithms to select arms underunrestricted feedback delays based on their merits. We prove that ouralgorithms all achieve sublinear expected reward regret and expected fairnessregret, with a dependence on the quantiles of the delay distribution. We alsoconduct extensive experiments using synthetic and real-world data and show thatour algorithms can fairly select arms with different feedback delays.</description><author>Ziqun Chen, Kechao Cai, Zhuoyue Chen, Jinbei Zhang, John C. S. Lui</author><pubDate>Fri, 26 Jul 2024 13:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15439v2</guid></item><item><title>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</title><link>http://arxiv.org/abs/2407.13623v2</link><description>Research on scaling large language models (LLMs) has primarily focused onmodel parameters and training data size, overlooking the role of vocabularysize. We investigate how vocabulary size impacts LLM scaling laws by trainingmodels ranging from 33M to 3B parameters on up to 500B characters with variousvocabulary configurations. We propose three complementary approaches forpredicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivativeestimation, and parametric fit of the loss function. Our approaches converge onthe same result that the optimal vocabulary size depends on the availablecompute budget and that larger models deserve larger vocabularies. However,most LLMs use too small vocabulary sizes. For example, we predict that theoptimal vocabulary size of Llama2-70B should have been at least 216K, 7 timeslarger than its vocabulary of 32K. We validate our predictions empirically bytraining models with 3B parameters across different FLOPs budgets. Adopting ourpredicted optimal vocabulary size consistently improves downstream performanceover commonly used vocabulary sizes. By increasing the vocabulary size from theconventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointlyconsidering model parameters and vocabulary size for efficient scaling.</description><author>Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong</author><pubDate>Fri, 26 Jul 2024 12:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13623v2</guid></item><item><title>Cluster-norm for Unsupervised Probing of Knowledge</title><link>http://arxiv.org/abs/2407.18712v1</link><description>The deployment of language models brings challenges in generating reliableinformation, especially when these models are fine-tuned using humanpreferences. To extract encoded knowledge without (potentially) biased humanlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)have been developed (Burns et al., 2022). However, salient but unrelatedfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).Addressing this, we propose a cluster normalization method to minimize theimpact of such features by clustering and normalizing activations of contrastpairs before applying unsupervised probing techniques. While this approach doesnot address the issue of differentiating between knowledge in general andsimulated knowledge - a major issue in the literature of latent knowledgeelicitation (Christiano et al., 2021) - it significantly improves the abilityof unsupervised probes to identify the intended knowledge amidst distractions.</description><author>Walter Laurito, Sharan Maiya, Gr√©goire Dhimo√Øla, Owen, Yeung, Kaarel H√§nni</author><pubDate>Fri, 26 Jul 2024 12:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18712v1</guid></item><item><title>AEP$n$P: A Less-constrained EP$n$P Solver for Pose Estimation with Anisotropic Scaling</title><link>http://arxiv.org/abs/2310.09982v4</link><description>Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for poseestimation in various applications. In this paper, we present a new approach tothe P$n$P problem with relaxed constraints, eliminating the need for precise 3Dcoordinates, which is especially suitable for object pose estimation wherecorresponding object models may not be available in practice. Built upon theclassical EP$n$P solver, we refer to it as AEP$n$P due to its ability to handleunknown anisotropic scaling factors in addition to the common 6Dtransformation. Through a few algebraic manipulations and a well-chosen frameof reference, this new problem can be boiled down to a simple linear null-spaceproblem followed by point registration-based identification of a similaritytransformation. Experimental results on both simulated and real datasetsdemonstrate the effectiveness of AEP$n$P as a flexible and practical solutionto object pose estimation. Code: https://github.com/goldoak/AEPnP.</description><author>Jiaxin Wei, Stefan Leutenegger, Laurent Kneip</author><pubDate>Fri, 26 Jul 2024 12:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09982v4</guid></item><item><title>Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification</title><link>http://arxiv.org/abs/2407.17688v2</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities inexecuting tasks based on natural language queries. However, these models,trained on curated datasets, inherently embody biases ranging from racial tonational and gender biases. It remains uncertain whether these biases impactthe performance of LLMs for certain tasks. In this study, we investigate thepolitical biases of LLMs within the stance classification task, specificallyexamining whether these models exhibit a tendency to more accurately classifypolitically-charged stances. Utilizing three datasets, seven LLMs, and fourdistinct prompting schemes, we analyze the performance of LLMs on politicallyoriented statements and targets. Our findings reveal a statisticallysignificant difference in the performance of LLMs across various politicallyoriented stance classification tasks. Furthermore, we observe that thisdifference primarily manifests at the dataset level, with models and promptingschemes showing statistically similar performances across different stanceclassification datasets. Lastly, we observe that when there is greaterambiguity in the target the statement is directed towards, LLMs have poorerstance classification accuracy. Code &amp; Dataset: http://doi.org/10.5281/zenodo.12938478</description><author>Lynnette Hui Xian Ng, Iain Cruickshank, Roy Ka-Wei Lee</author><pubDate>Fri, 26 Jul 2024 12:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17688v2</guid></item><item><title>Finite Neural Networks as Mixtures of Gaussian Processes: From Provable Error Bounds to Prior Selection</title><link>http://arxiv.org/abs/2407.18707v1</link><description>Infinitely wide or deep neural networks (NNs) with independent andidentically distributed (i.i.d.) parameters have been shown to be equivalent toGaussian processes. Because of the favorable properties of Gaussian processes,this equivalence is commonly employed to analyze neural networks and has led tovarious breakthroughs over the years. However, neural networks and Gaussianprocesses are equivalent only in the limit; in the finite case there arecurrently no methods available to approximate a trained neural network with aGaussian model with bounds on the approximation error. In this work, we presentan algorithmic framework to approximate a neural network of finite width anddepth, and with not necessarily i.i.d. parameters, with a mixture of Gaussianprocesses with error bounds on the approximation error. In particular, weconsider the Wasserstein distance to quantify the closeness betweenprobabilistic models and, by relying on tools from optimal transport andGaussian processes, we iteratively approximate the output distribution of eachlayer of the neural network as a mixture of Gaussian processes. Crucially, forany NN and $\epsilon &gt;0$ our approach is able to return a mixture of Gaussianprocesses that is $\epsilon$-close to the NN at a finite set of input points.Furthermore, we rely on the differentiability of the resulting error bound toshow how our approach can be employed to tune the parameters of a NN to mimicthe functional behavior of a given Gaussian process, e.g., for prior selectionin the context of Bayesian inference. We empirically investigate theeffectiveness of our results on both regression and classification problemswith various neural network architectures. Our experiments highlight how ourresults can represent an important step towards understanding neural networkpredictions and formally quantifying their uncertainty.</description><author>Steven Adams, Patan√®, Morteza Lahijanian, Luca Laurenti</author><pubDate>Fri, 26 Jul 2024 12:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18707v1</guid></item></channel></rss>