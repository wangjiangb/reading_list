<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 20 Aug 2024 01:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking for Sequential Recommendation</title><link>http://arxiv.org/abs/2408.08686v2</link><description>Language Models (LMs) are increasingly employed in recommendation systems dueto their advanced language understanding and generation capabilities. Recentrecommender systems based on generative retrieval have leveraged theinferential abilities of LMs to directly generate the index tokens of the nextitem, based on item sequences within the user's interaction history. Previousstudies have mostly focused on item indices based solely on textual semantic orcollaborative information. However, although the standalone effectiveness ofthese aspects has been demonstrated, the integration of this information hasremained unexplored. Our in-depth analysis finds that there is a significantdifference in the knowledge captured by the model from heterogeneous itemindices and diverse input prompts, which can have a high potential forcomplementarity. In this paper, we propose SC-Rec, a unified recommender systemthat learns diverse preference knowledge from two distinct item indices andmultiple prompt templates. Furthermore, SC-Rec adopts a novel rerankingstrategy that aggregates a set of ranking results, inferred based on differentindices and prompts, to achieve the self-consistency of the model. Ourempirical evaluation on three real-world datasets demonstrates that SC-Recconsiderably outperforms the state-of-the-art methods for sequentialrecommendation, effectively incorporating complementary knowledge from variedoutputs of the model.</description><author>Tongyoung Kim, Soojin Yoon, Seongku Kang, Jinyoung Yeo, Dongha Lee</author><pubDate>Mon, 19 Aug 2024 04:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08686v2</guid></item><item><title>PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars</title><link>http://arxiv.org/abs/2408.08869v2</link><description>Self-ensembling techniques with diverse reasoning paths such asSelf-Consistency have demonstrated remarkable performance gains in textgeneration with Large Language Models (LLMs). However, such techniques dependon the availability of an accurate answer extraction process to aggregateacross multiple outputs. Moreover, they acquire higher inference cost, incomparison to Greedy Decoding, due to generation of relatively higher number ofoutput tokens. Research has shown that the free form text outputs fromSelf-Consistency can be aggregated reliably using LLMs to produce the finaloutput. Additionally, recent advancements in LLM inference have demonstratedthat usage of diverse exemplars in prompts have the ability to induce diversityin the LLM outputs. Such proven techniques can be easily extended toself-ensembling based approaches to achieve enhanced results in textgeneration. In this paper, we introduce PEDAL (Prompts based on ExemplarDiversity Aggregated using LLMs), a hybrid self-ensembling approach, thatcombines the strengths of diverse exemplar based prompts and LLM basedaggregation to achieve improvement in overall performance. On the publiclyavailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achievebetter accuracy than Greedy Decoding based strategies with lower inference costcompared to Self Consistency based approaches.</description><author>Sumanth Prabhu</author><pubDate>Mon, 19 Aug 2024 04:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08869v2</guid></item><item><title>Beyond Full Label: Single-Point Prompt for Infrared Small Target Label Generation</title><link>http://arxiv.org/abs/2408.08191v3</link><description>In this work, we make the first attempt to construct a learning-basedsingle-point annotation paradigm for infrared small target label generation(IRSTLG). Our intuition is that label generation requires just one more pointprompt than target detection: IRSTLG can be regarded as an infrared smalltarget detection (IRSTD) task with the target location hint. Based on thisinsight, we introduce an energy double guided single-point prompt (EDGSP)framework, which adeptly transforms the target detection network into a refinedlabel generation method. Specifically, the proposed EDGSP includes: 1) targetenergy initialization (TEI) to create a foundational outline for sufficientshape evolution of pseudo label, 2) double prompt embedding (DPE) for rapidlocalization of interested regions and reinforcement of individual differencesto avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminatefalse alarms. Experimental results show that pseudo labels generated by threebaselines equipped with EDGSP achieve 100% object-level probability ofdetection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1kdatasets, with a pixel-level intersection over union (IoU) improvement of13.28% over state-of-the-art (SOTA) label generation methods. In the practicalapplication of downstream IRSTD, EDGSP realizes, for the first time, asingle-point generated pseudo mask beyond the full label. Even with coarsesingle-point annotations, it still achieves 99.5% performance of full labeling.</description><author>Shuai Yuan, Hanlin Qin, Renke Kou, Xiang Yan, Zechuan Li, Chenxu Peng, Abd-Krim Seghouane</author><pubDate>Mon, 19 Aug 2024 01:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08191v3</guid></item><item><title>Accelerating Giant Impact Simulations with Machine Learning</title><link>http://arxiv.org/abs/2408.08873v1</link><description>Constraining planet formation models based on the observed exoplanetpopulation requires generating large samples of synthetic planetary systems,which can be computationally prohibitive. A significant bottleneck issimulating the giant impact phase, during which planetary embryos evolvegravitationally and combine to form planets, which may themselves experiencelater collisions. To accelerate giant impact simulations, we present a machinelearning (ML) approach to predicting collisional outcomes in multiplanetsystems. Trained on more than 500,000 $N$-body simulations of three-planetsystems, we develop an ML model that can accurately predict which two planetswill experience a collision, along with the state of the post-collisionplanets, from a short integration of the system's initial conditions. Our modelgreatly improves on non-ML baselines that rely on metrics from dynamics theory,which struggle to accurately predict which pair of planets will experience acollision. By combining with a model for predicting long-term stability, wecreate an efficient ML-based giant impact emulator, which can predict theoutcomes of giant impact simulations with a speedup of up to four orders ofmagnitude. We expect our model to enable analyses that would not otherwise becomputationally feasible. As such, we release our full training code, alongwith an easy-to-use API for our collision outcome model and giant impactemulator.</description><author>Caleb Lammers, Miles Cranmer, Sam Hadden, Shirley Ho, Norman Murray, Daniel Tamayo</author><pubDate>Fri, 16 Aug 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08873v1</guid></item><item><title>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</title><link>http://arxiv.org/abs/2408.08872v1</link><description>This report introduces xGen-MM (also known as BLIP-3), a framework fordeveloping Large Multimodal Models (LMMs). The framework comprises meticulouslycurated datasets, a training recipe, model architectures, and a resulting suiteof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGeninitiative on foundation AI models. Our models undergo rigorous evaluationacross a range of tasks, including both single and multi-image benchmarks. Ourpre-trained base model exhibits strong in-context learning capabilities and theinstruction-tuned model demonstrates competitive performance among open-sourceLMMs with similar model sizes. In addition, we introduce a safety-tuned modelwith DPO, aiming to mitigate harmful behaviors such as hallucinations andimprove safety. We open-source our models, curated large-scale datasets, andour fine-tuning codebase to facilitate further advancements in LMM research.Associated resources will be available on our project page above.</description><author>Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu</author><pubDate>Fri, 16 Aug 2024 17:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08872v1</guid></item><item><title>SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation</title><link>http://arxiv.org/abs/2408.08870v1</link><description>Image segmentation plays an important role in vision understanding. Recently,the emerging vision foundation models continuously achieved superiorperformance on various tasks. Following such success, in this paper, we provethat the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shapedsegmentation models. We propose a simple but effective framework, termedSAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts theHiera backbone of SAM2 as the encoder, while the decoder uses the classicU-shaped design. Additionally, adapters are inserted into the encoder to allowparameter-efficient fine-tuning. Preliminary experiments on various downstreamtasks, such as camouflaged object detection, salient object detection, marineanimal segmentation, mirror detection, and polyp segmentation, demonstrate thatour SAM2-UNet can simply beat existing specialized state-of-the-art methodswithout bells and whistles. Project page:\url{https://github.com/WZH0120/SAM2-UNet}.</description><author>Xinyu Xiong, Zihuang Wu, Shuangyi Tan, Wenxue Li, Feilong Tang, Ying Chen, Siying Li, Jie Ma, Guanbin Li</author><pubDate>Fri, 16 Aug 2024 17:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08870v1</guid></item><item><title>Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning</title><link>http://arxiv.org/abs/2405.17243v2</link><description>Both entropy-minimizing and entropy-maximizing (curiosity) objectives forunsupervised reinforcement learning (RL) have been shown to be effective indifferent environments, depending on the environment's level of naturalentropy. However, neither method alone results in an agent that willconsistently learn intelligent behavior across environments. In an effort tofind a single entropy-based method that will encourage emergent behaviors inany environment, we propose an agent that can adapt its objective online,depending on the entropy conditions by framing the choice as a multi-armedbandit problem. We devise a novel intrinsic feedback signal for the bandit,which captures the agent's ability to control the entropy in its environment.We demonstrate that such agents can learn to control entropy and exhibitemergent behaviors in both high- and low-entropy regimes and can learn skillfulbehaviors in benchmark tasks. Videos of the trained agents and summarizedfindings can be found on our project pagehttps://sites.google.com/view/surprise-adaptive-agents</description><author>Adriana Hugessen, Roger Creus Castanyer, Faisal Mohamed, Glen Berseth</author><pubDate>Fri, 16 Aug 2024 17:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17243v2</guid></item><item><title>PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars</title><link>http://arxiv.org/abs/2408.08869v1</link><description>Self-ensembling techniques with diverse reasoning paths such asSelf-Consistency have demonstrated remarkable gains in accuracy for LargeLanguage Models (LLMs). However, such techniques depend on the availability ofan accurate answer extraction process to aggregate across multiple outputs.Moreover, they acquire higher inference cost, in comparison to Greedy Decoding,due to generation of relatively higher number of output tokens. Research hasshown that the free form text outputs from Self-Consistency can be aggregatedreliably using LLMs to produce the final output. Additionally, recentadvancements in LLM inference have demonstrated that usage of diverse exemplarsin prompts have the ability to induce diversity in the LLM outputs. Such proventechniques can be easily extended to self-ensembling based approaches toachieve enhanced results in text generation. In this paper, we introduce PEDAL(Prompts based on Exemplar Diversity Aggregated using LLMs), a hybridself-ensembling approach, that combines the strengths of diverse exemplar basedprompts and LLM based aggregation to achieve improvement in overallperformance. On the publicly available SVAMP and ARC datasets, our experimentsreveal that PEDAL can achieve better accuracy than Greedy Decoding basedstrategies with lower inference cost compared to Self Consistency basedapproaches.</description><author>Sumanth Prabhu</author><pubDate>Fri, 16 Aug 2024 17:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08869v1</guid></item><item><title>A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs</title><link>http://arxiv.org/abs/2408.08868v1</link><description>The state-of-the-art for training on-device language models for mobilekeyboard applications combines federated learning (FL) with differentialprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Twovariants of DP-FTRL are used in practice, tree aggregation and matrixfactorization. However, tree aggregation suffers from significantly suboptimalprivacy/utility tradeoffs, while matrix mechanisms require expensiveoptimization parameterized by hard-to-estimate-in-advance constants, and highruntime memory costs.This paper extends the recently introduced Buffered LinearToeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRLmaintains the ease-of-use advantages of tree aggregation, while essentiallymatching matrix factorization in terms of utility and privacy. We evaluateBLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulationbenchmark, and across four on-device language model tasks in a production FLsystem. Our empirical results highlight the advantages of the BLT mechanism andelevate the practicality and effectiveness of DP in real-world scenarios.</description><author>H. Brendan McMahan, Zheng Xu, Yanxiang Zhang</author><pubDate>Fri, 16 Aug 2024 17:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08868v1</guid></item><item><title>Visual Agents as Fast and Slow Thinkers</title><link>http://arxiv.org/abs/2408.08862v1</link><description>Achieving human-level intelligence requires refining cognitive distinctionsbetween System 1 and System 2 thinking. While contemporary AI, driven by largelanguage models, demonstrates human-like traits, it falls short of genuinecognition. Transitioning from structured benchmarks to real-world scenariospresents challenges for visual agents, often leading to inaccurate and overlyconfident responses. To address the challenge, we introduce FaST, whichincorporates the Fast and Slow Thinking mechanism into visual agents. FaSTemploys a switch adapter to dynamically select between System 1/2 modes,tailoring the problem-solving approach to different task complexity. It tacklesuncertain and unseen objects by adjusting model confidence and integrating newcontextual data. With this novel design, we advocate a flexible system,hierarchical reasoning capabilities, and a transparent decision-makingpipeline, all of which contribute to its ability to emulate human-likecognitive processes in visual intelligence. Empirical results demonstrate thatFaST outperforms various well-known baselines, achieving 80.8% accuracy overVQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg forreasoning segmentation, demonstrate FaST's superior performance. Extensivetesting validates the efficacy and robustness of FaST's core components,showcasing its potential to advance the development of cognitive visual agentsin AI systems.</description><author>Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, Dongfang Liu</author><pubDate>Fri, 16 Aug 2024 17:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08862v1</guid></item><item><title>CoDefeater: Using LLMs To Find Defeaters in Assurance Cases</title><link>http://arxiv.org/abs/2407.13717v2</link><description>Constructing assurance cases is a widely used, and sometimes required,process toward demonstrating that safety-critical systems will operate safelyin their planned environment. To mitigate the risk of errors and missing edgecases, the concept of defeaters - arguments or evidence that challenge claimsin an assurance case - has been introduced. Defeaters can provide timelydetection of weaknesses in the arguments, prompting further investigation andtimely mitigations. However, capturing defeaters relies on expert judgment,experience, and creativity and must be done iteratively due to evolvingrequirements and regulations. This paper proposes CoDefeater, an automatedprocess to leverage large language models (LLMs) for finding defeaters. Initialresults on two systems show that LLMs can efficiently find known and unforeseenfeasible defeaters to support safety analysts in enhancing the completeness andconfidence of assurance cases.</description><author>Usman Gohar, Michael C. Hunter, Robyn R. Lutz, Myra B. Cohen</author><pubDate>Fri, 16 Aug 2024 17:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13717v2</guid></item><item><title>Stochastic Bandits Robust to Adversarial Attacks</title><link>http://arxiv.org/abs/2408.08859v1</link><description>This paper investigates stochastic multi-armed bandit algorithms that arerobust to adversarial attacks, where an attacker can first observe thelearner's action and {then} alter their reward observation. We study two casesof this model, with or without the knowledge of an attack budget $C$, definedas an upper bound of the summation of the difference between the actual andaltered rewards. For both cases, we devise two types of algorithms with regretbounds having additive or multiplicative $C$ dependence terms. For the knownattack budget case, we prove our algorithms achieve the regret bound of${O}((K/\Delta)\log T + KC)$ and $\tilde{O}(\sqrt{KTC})$ for the additive andmultiplicative $C$ terms, respectively, where $K$ is the number of arms, $T$ isthe time horizon, $\Delta$ is the gap between the expected rewards of theoptimal arm and the second-best arm, and $\tilde{O}$ hides the logarithmicfactors. For the unknown case, we prove our algorithms achieve the regret boundof $\tilde{O}(\sqrt{KT} + KC^2)$ and $\tilde{O}(KC\sqrt{T})$ for the additiveand multiplicative $C$ terms, respectively. In addition to these upper boundresults, we provide several lower bounds showing the tightness of our boundsand the optimality of our algorithms. These results delineate an intrinsicseparation between the bandits with attacks and corruption models [Lykouris etal., 2018].</description><author>Xuchuang Wang, Jinhang Zuo, Xutong Liu, John C. S. Lui, Mohammad Hajiesmaili</author><pubDate>Fri, 16 Aug 2024 17:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08859v1</guid></item><item><title>DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models</title><link>http://arxiv.org/abs/2408.08855v1</link><description>Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential inzero-shot image classification. However, adapting these models to new domainsremains challenging, especially in unsupervised settings where labelled data isunavailable. Recent research has proposed pseudo-labelling approaches to adaptCLIP in an unsupervised manner using unlabelled target data. Nonetheless, thesemethods struggle due to noisy pseudo-labels resulting from the misalignmentbetween CLIP's visual and textual representations. This study introduces DPA,an unsupervised domain adaptation method for VLMs. DPA introduces the conceptof dual prototypes, acting as distinct classifiers, along with the convexcombination of their outputs, thereby leading to accurate pseudo-labelconstruction. Next, it ranks pseudo-labels to facilitate robust self-training,particularly during early training. Finally, it addresses visual-textualmisalignment by aligning textual prototypes with image prototypes to furtherimprove the adaptation performance. Experiments on 13 downstream vision tasksdemonstrate that DPA significantly outperforms zero-shot CLIP and thestate-of-the-art unsupervised adaptation baselines.</description><author>Eman Ali, Sathira Silva, Muhammad Haris Khan</author><pubDate>Fri, 16 Aug 2024 17:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08855v1</guid></item><item><title>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation</title><link>http://arxiv.org/abs/2310.02304v3</link><description>Several recent advances in AI systems solve problems by providing a"scaffolding" program that structures multiple calls to language models (LMs)to generate better outputs. A scaffolding program is written in a programminglanguage such as Python. In this work, we use a language-model-infusedscaffolding program to improve itself. We start with a seed "improver" thatimproves an input program according to a given utility function by querying anLM several times and returning the best solution. We then run this seedimprover to improve itself. Across a small set of downstream tasks, theresulting improved improver generates programs with significantly betterperformance than its seed improver. A variety of self-improvement strategiesare proposed by the language model, including beam search, genetic algorithms,and simulated annealing. Since the language models themselves are not altered,this is not full recursive self-improvement. Nonetheless, it demonstrates thata modern language model, GPT-4 in our experiments, is capable of writing codethat can call itself to improve itself. We consider concerns around thedevelopment of self-improving technologies and evaluate the frequency withwhich the generated code bypasses a sandbox.</description><author>Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai</author><pubDate>Fri, 16 Aug 2024 17:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02304v3</guid></item><item><title>GeoTransformer: Enhancing Urban Forecasting with Geospatial Attention Mechanisms</title><link>http://arxiv.org/abs/2408.08852v1</link><description>Recent advancements have focused on encoding urban spatial information intohigh-dimensional spaces, with notable efforts dedicated to integratingsociodemographic data and satellite imagery. These efforts have establishedfoundational models in this field. However, the effective utilization of thesespatial representations for urban forecasting applications remainsunder-explored. To address this gap, we introduce GeoTransformer, a novelstructure that synergizes the Transformer architecture with geospatialstatistics prior. GeoTransformer employs an innovative geospatial attentionmechanism to incorporate extensive urban information and spatial dependenciesinto a unified predictive model. Specifically, we compute geospatial weightedattention scores between the target region and surrounding regions and leveragethe integrated urban information for predictions. Extensive experiments on GDPand ride-share demand prediction tasks demonstrate that GeoTransformersignificantly outperforms existing baseline models, showcasing its potential toenhance urban forecasting tasks.</description><author>Yuhao Jia, Zile Wu, Shengao Yi, Yifei Sun</author><pubDate>Fri, 16 Aug 2024 17:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08852v1</guid></item><item><title>Potion: Towards Poison Unlearning</title><link>http://arxiv.org/abs/2406.09173v2</link><description>Adversarial attacks by malicious actors on machine learning systems, such asintroducing poison triggers into training datasets, pose significant risks. Thechallenge in resolving such an attack arises in practice when only a subset ofthe poisoned data can be identified. This necessitates the development ofmethods to remove, i.e. unlearn, poison triggers from already trained modelswith only a subset of the poison data available. The requirements for this tasksignificantly deviate from privacy-focused unlearning where all of the data tobe forgotten by the model is known. Previous work has shown that theundiscovered poisoned samples lead to a failure of established unlearningmethods, with only one method, Selective Synaptic Dampening (SSD), showinglimited success. Even full retraining, after the removal of the identifiedpoison, cannot address this challenge as the undiscovered poison samples leadto a reintroduction of the poison trigger in the model. Our work addresses twokey challenges to advance the state of the art in poison unlearning. First, weintroduce a novel outlier-resistant method, based on SSD, that significantlyimproves model protection and unlearning performance. Second, we introducePoison Trigger Neutralisation (PTN) search, a fast, parallelisable,hyperparameter search that utilises the characteristic "unlearning versus modelprotection" trade-off to find suitable hyperparameters in settings where theforget set size is unknown and the retain set is contaminated. We benchmark ourcontributions using ResNet-9 on CIFAR10 and WideResNet-28x10 on CIFAR100.Experimental results show that our method heals 93.72% of poison compared toSSD with 83.41% and full retraining with 40.68%. We achieve this while alsolowering the average model accuracy drop caused by unlearning from 5.68% (SSD)to 1.41% (ours).</description><author>Stefan Schoepf, Jack Foster, Alexandra Brintrup</author><pubDate>Fri, 16 Aug 2024 17:24:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09173v2</guid></item><item><title>S-BDT: Distributed Differentially Private Boosted Decision Trees</title><link>http://arxiv.org/abs/2309.12041v3</link><description>We introduce S-BDT: a novel $(\varepsilon,\delta)$-differentially privatedistributed gradient boosted decision tree (GBDT) learner that improves theprotection of single training data points (privacy) while achieving meaningfullearning goals, such as accuracy or regression error (utility). S-BDT uses lessnoise by relying on non-spherical multivariate Gaussian noise, for which weshow tight subsampling bounds for privacy amplification and incorporate thatinto a R\'enyi filter for individual privacy accounting. We experimentallyreach the same utility while saving $50\%$ in terms of epsilon for $\varepsilon\le 0.5$ on the Abalone regression dataset (dataset size $\approx 4K$), saving$30\%$ in terms of epsilon for $\varepsilon \le 0.08$ for the Adultclassification dataset (dataset size $\approx 50K$), and saving $30\%$ in termsof epsilon for $\varepsilon\leq0.03$ for the Spambase classification dataset(dataset size $\approx 5K$). Moreover, we show that for situations where a GBDTis learning a stream of data that originates from different subpopulations(non-IID), S-BDT improves the saving of epsilon even further.</description><author>Thorsten Peinemann, Moritz Kirschte, Joshua Stock, Carlos Cotrini, Esfandiar Mohammadi</author><pubDate>Fri, 16 Aug 2024 17:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12041v3</guid></item><item><title>PsychoLex: Unveiling the Psychological Mind of Large Language Models</title><link>http://arxiv.org/abs/2408.08848v1</link><description>This paper explores the intersection of psychology and artificialintelligence through the development and evaluation of specialized LargeLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designedto enhance LLMs' proficiency in psychological tasks in both Persian andEnglish. Key contributions include the PsychoLexQA dataset for instructionalcontent and the PsychoLexEval dataset for rigorous evaluation of LLMs incomplex psychological scenarios. Additionally, we present the PsychoLexLLaMAmodel, optimized specifically for psychological applications, demonstratingsuperior performance compared to general-purpose models. The findingsunderscore the potential of tailored LLMs for advancing psychological researchand applications, while also highlighting areas for further refinement. Thisresearch offers a foundational step towards integrating LLMs into specializedpsychological domains, with implications for future advancements in AI-drivenpsychological practice.</description><author>Mohammad Amin Abbasi, Farnaz Sadat Mirnezami, Hassan Naderi</author><pubDate>Fri, 16 Aug 2024 17:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08848v1</guid></item><item><title>HistoGym: A Reinforcement Learning Environment for Histopathological Image Analysis</title><link>http://arxiv.org/abs/2408.08847v1</link><description>In pathological research, education, and clinical practice, thedecision-making process based on pathological images is critically important.This significance extends to digital pathology image analysis: its adequacy isdemonstrated by the extensive information contained within tissue structures,which is essential for accurate cancer classification and grading.Additionally, its necessity is highlighted by the inherent requirement forinterpretability in the conclusions generated by algorithms. For humans,determining tumor type and grade typically involves multi-scale analysis, whichpresents a significant challenge for AI algorithms. Traditional patch-basedmethods are inadequate for modeling such complex structures, as they fail tocapture the intricate, multi-scale information inherent in whole slide images.Consequently, there is a pressing need for advanced AI techniques capable ofefficiently and accurately replicating this complex analytical process. Toaddress this issue, we introduce HistoGym, an open-source reinforcementlearning environment for histopathological image analysis. Following OpenAI GymAPIs, HistoGym aims to foster whole slide image diagnosis by mimicking thereal-life processes of doctors. Leveraging the pyramid feature of WSIs and theOpenSlide API, HistoGym provides a unified framework for various clinicaltasks, including tumor detection and classification. We detail the observation,action, and reward specifications tailored for the histopathological imageanalysis domain and provide an open-source Python-based interface for bothclinicians and researchers. To accommodate different clinical demands, we offervarious scenarios for different organs and cancers, including both WSI-basedand selected region-based scenarios, showcasing several noteworthy results.</description><author>Zhi-Bo Liu, Xiaobo Pang, Jizhao Wang, Shuai Liu, Chen Li</author><pubDate>Fri, 16 Aug 2024 17:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08847v1</guid></item><item><title>DivCon: Divide and Conquer for Progressive Text-to-Image Generation</title><link>http://arxiv.org/abs/2403.06400v2</link><description>Diffusion-driven text-to-image (T2I) generation has achieved remarkableadvancements. To further improve T2I models' capability in numerical andspatial reasoning, the layout is employed as an intermedium to bridge largelanguage models and layout-based diffusion models. However, these methods stillstruggle with generating images from textural prompts with multiple objects andcomplicated spatial relationships. To tackle this challenge, we introduce adivide-and-conquer approach which decouples the T2I generation task into simplesubtasks. Our approach divides the layout prediction stage into numerical &amp;spatial reasoning and bounding box prediction. Then, the layout-to-imagegeneration stage is conducted in an iterative manner to reconstruct objectsfrom easy ones to difficult ones. We conduct experiments on the HRS and NSR-1Kbenchmarks and our approach outperforms previous state-of-the-art models withnotable margins. In addition, visual results demonstrate that our approachsignificantly improves the controllability and consistency in generatingmultiple objects from complex textural prompts.</description><author>Yuhao Jia, Wenhan Tan</author><pubDate>Fri, 16 Aug 2024 17:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06400v2</guid></item><item><title>Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams</title><link>http://arxiv.org/abs/2403.15455v2</link><description>The proliferation of textual data on the Internet presents a uniqueopportunity for institutions and companies to monitor public opinion abouttheir services and products. Given the rapid generation of such data, the textstream mining setting, which handles sequentially arriving, potentiallyinfinite text streams, is often more suitable than traditional batch learning.While pre-trained language models are commonly employed for their high-qualitytext vectorization capabilities in streaming contexts, they face challengesadapting to concept drift - the phenomenon where the data distribution changesover time, adversely affecting model performance. Addressing the issue ofconcept drift, this study explores the efficacy of seven text sampling methodsdesigned to selectively fine-tune language models, thereby mitigatingperformance degradation. We precisely assess the impact of these methods onfine-tuning the SBERT model using four different loss functions. Ourevaluation, focused on Macro F1-score and elapsed time, employs two text streamdatasets and an incremental SVM classifier to benchmark performance. Ourfindings indicate that Softmax loss and Batch All Triplets loss areparticularly effective for text stream classification, demonstrating thatlarger sample sizes generally correlate with improved macro F1-scores. Notably,our proposed WordPieceToken ratio sampling method significantly enhancesperformance with the identified loss functions, surpassing baseline results.</description><author>Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr, Jean Paul Barddal</author><pubDate>Fri, 16 Aug 2024 17:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15455v2</guid></item><item><title>DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers</title><link>http://arxiv.org/abs/2408.03291v2</link><description>Vision transformers (ViTs) have garnered significant attention for theirperformance in vision tasks, but the high computational cost and significantlatency issues have hindered widespread adoption. Post-training quantization(PTQ), a promising method for model compression, still faces accuracydegradation challenges with ViTs. There are two reasons for this: the existingquantization paradigm does not fit the power-law distribution of post-Softmaxactivations well, and accuracy inevitably decreases after reparameterizingpost-LayerNorm activations. We propose a Distribution-Friendly andOutlier-Aware Post-training Quantization method for Vision Transformers, namedDopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers andintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses moreon values near 1, more accurately preserving the power-law distribution ofpost-Softmax activations, and achieves favorable results. Besides, during thereparameterization of post-LayerNorm activations from channel-wise tolayer-wise quantization, the accuracy degradation is mainly due to thesignificant impact of outliers in the scaling factors. Therefore, DopQ-ViTproposes a method to select Median as the Optimal Scaling Factor, denoted asMOSF, which compensates for the influence of outliers and preserves theperformance of the quantization model. DopQ-ViT has been extensively validatedand significantly improves the performance of quantization models, especiallyin low-bit settings.</description><author>Lianwei Yang, Haisong Gong, Qingyi Gu</author><pubDate>Fri, 16 Aug 2024 17:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03291v2</guid></item><item><title>Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People</title><link>http://arxiv.org/abs/2403.03640v4</link><description>Despite the vast repository of global medical knowledge predominantly beingin English, local languages are crucial for delivering tailored healthcareservices, particularly in areas with limited medical resources. To extend thereach of medical AI advancements to a broader population, we aim to developmedical LLMs across the six most widely spoken languages, encompassing a globalpopulation of 6.1 billion. This effort culminates in the creation of theApolloCorpora multilingual medical dataset and the XMedBench benchmark. In themultilingual medical benchmark, the released Apollo models, at variousrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the bestperformance among models of equivalent size. Especially, Apollo-7B is thestate-of-the-art multilingual medical LLMs up to 70B. Additionally, these litemodels could be used to improve the multi-lingual medical capabilities oflarger models without fine-tuning in a proxy-tuning fashion. We willopen-source training corpora, code, model weights and evaluation benchmark.</description><author>Xidong Wang, Nuo Chen, Junyin Chen, Yidong Wang, Guorui Zhen, Yan Hu, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang</author><pubDate>Fri, 16 Aug 2024 17:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03640v4</guid></item><item><title>Shapley Marginal Surplus for Strong Models</title><link>http://arxiv.org/abs/2408.08845v1</link><description>Shapley values have seen widespread use in machine learning as a way toexplain model predictions and estimate the importance of covariates. Accuratelyexplaining models is critical in real-world models to both aid in decisionmaking and to infer the properties of the true data-generating process (DGP).In this paper, we demonstrate that while model-based Shapley values might beaccurate explainers of model predictions, machine learning models themselvesare often poor explainers of the DGP even if the model is highly accurate.Particularly in the presence of interrelated or noisy variables, the output ofa highly predictive model may fail to account for these relationships. Thisimplies explanations of a trained model's behavior may fail to providemeaningful insight into the DGP. In this paper we introduce a novel variableimportance algorithm, Shapley Marginal Surplus for Strong Models, that samplesthe space of possible models to come up with an inferential measure of featureimportance. We compare this method to other popular feature importance methods,both Shapley-based and non-Shapley based, and demonstrate significantoutperformance in inferential capabilities relative to other methods.</description><author>Daniel de Marchi, Michael Kosorok, Scott de Marchi</author><pubDate>Fri, 16 Aug 2024 17:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08845v1</guid></item><item><title>AI-as-exploration: Navigating intelligence space</title><link>http://arxiv.org/abs/2401.07964v3</link><description>Artificial Intelligence is a field that lives many lives, and the term hascome to encompass a motley collection of scientific and commercial endeavours.In this paper, I articulate the contours of a rather neglected but centralscientific role that AI has to play, which I dub `AI-as-exploration'.The basicthrust of AI-as-exploration is that of creating and studying systems that canreveal candidate building blocks of intelligence that may differ from the formsof human and animal intelligence we are familiar with. In other words, Isuggest that AI is one of the best tools we have for exploring intelligencespace, namely the space of possible intelligent systems. I illustrate the valueof AI-as-exploration by focusing on a specific case study, i.e., recent work onthe capacity to combine novel and invented concepts in humans and LargeLanguage Models. I show that the latter, despite showing human-level accuracyin such a task, probably solve it in ways radically different, but no lessrelevant to intelligence research, to those hypothesised for humans.</description><author>Dimitri Coelho Mollo</author><pubDate>Fri, 16 Aug 2024 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07964v3</guid></item><item><title>FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats</title><link>http://arxiv.org/abs/2408.08841v1</link><description>The table reasoning task aims to answer the question according to the giventable. Currently, using Large Language Models (LLMs) is the predominant methodfor table reasoning. Most existing methods employ a fixed tabular format torepresent the table, which could limit the performance. Given that eachinstance requires different capabilities and models possess varying abilities,we assert that different instances and models suit different tabular formats.We prove the aforementioned claim through quantitative analysis of experimentalresults, where different instances and models achieve different performancesusing various tabular formats. Building on this discussion, we proposeFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance byemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains aclassifier to predict the most suitable tabular format based on the instanceand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.Our experiments on WikiTableQuestions and TabFact reveal significantimprovements, with average gains of 2.3% and 4.8% compared to the bestperformance achieved using a fixed tabular format with greedy decoding andself-consistency decoding, thereby validating the effectiveness of our methods.</description><author>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Baoxin Wang, Dayong Wu, Qingfu Zhu, Wanxiang Che</author><pubDate>Fri, 16 Aug 2024 17:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08841v1</guid></item><item><title>ChemVLM: Exploring the Power of Multimodal Large Language Models in Chemistry Area</title><link>http://arxiv.org/abs/2408.07246v2</link><description>Large Language Models (LLMs) have achieved remarkable success and have beenapplied across various scientific fields, including chemistry. However, manychemical tasks require the processing of visual information, which cannot besuccessfully handled by existing chemical LLMs. This brings a growing need formodels capable of integrating multimodal information in the chemical domain. Inthis paper, we introduce \textbf{ChemVLM}, an open-source chemical multimodallarge language model specifically designed for chemical applications. ChemVLMis trained on a carefully curated bilingual multimodal dataset that enhancesits ability to understand both textual and visual chemical information,including molecular structures, reactions, and chemistry examination questions.We develop three datasets for comprehensive evaluation, tailored to ChemicalOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), andMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a rangeof open-source and proprietary multimodal large language models on varioustasks. Experimental results demonstrate that ChemVLM achieves competitiveperformance across all evaluated tasks. Our model can be found athttps://huggingface.co/AI4Chem/ChemVLM-26B.</description><author>Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Wei Li, Shufei Zhang, Mao Su, Wanli Ouyang, Yuqiang Li, Dongzhan Zhou</author><pubDate>Fri, 16 Aug 2024 16:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07246v2</guid></item><item><title>Entropy Coding of Unordered Data Structures</title><link>http://arxiv.org/abs/2408.08837v1</link><description>We present shuffle coding, a general method for optimal compression ofsequences of unordered objects using bits-back coding. Data structures that canbe compressed using shuffle coding include multisets, graphs, hypergraphs, andothers. We release an implementation that can easily be adapted to differentdata types and statistical models, and demonstrate that our implementationachieves state-of-the-art compression rates on a range of graph datasetsincluding molecular data.</description><author>Julius Kunze, Daniel Severo, Giulio Zani, Jan-Willem van de Meent, James Townsend</author><pubDate>Fri, 16 Aug 2024 16:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08837v1</guid></item><item><title>Active Learning with Weak Supervision for Gaussian Processes</title><link>http://arxiv.org/abs/2204.08335v3</link><description>Annotating data for supervised learning can be costly. When the annotationbudget is limited, active learning can be used to select and annotate thoseobservations that are likely to give the most gain in model performance. Wepropose an active learning algorithm that, in addition to selecting whichobservation to annotate, selects the precision of the annotation that isacquired. Assuming that annotations with low precision are cheaper to obtain,this allows the model to explore a larger part of the input space, with thesame annotation budget. We build our acquisition function on the previouslyproposed BALD objective for Gaussian Processes, and empirically demonstrate thegains of being able to adjust the annotation precision in the active learningloop.</description><author>Amanda Olmin, Jakob Lindqvist, Lennart Svensson, Fredrik Lindsten</author><pubDate>Fri, 16 Aug 2024 16:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08335v3</guid></item><item><title>Bee-yond the Plateau: Training QNNs with Swarm Algorithms</title><link>http://arxiv.org/abs/2408.08836v1</link><description>In the quest to harness the power of quantum computing, training quantumneural networks (QNNs) presents a formidable challenge. This study introducesan innovative approach, integrating the Bees Optimization Algorithm (BOA) toovercome one of the most significant hurdles -- barren plateaus. Ourexperiments across varying qubit counts and circuit depths demonstrate theBOA's superior performance compared to the Adam algorithm. Notably, BOAachieves faster convergence, higher accuracy, and greater computationalefficiency. This study confirms BOA's potential in enhancing the applicabilityof QNNs in complex quantum computations.</description><author>Rubén Darío Guerrero</author><pubDate>Fri, 16 Aug 2024 16:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08836v1</guid></item><item><title>Prediction Instability in Machine Learning Ensembles</title><link>http://arxiv.org/abs/2407.03194v4</link><description>In machine learning ensembles predictions from multiple models areaggregated. Despite widespread use and strong performance of ensembles inapplied problems little is known about the mathematical properties ofaggregating models and associated consequences for safe, explainable use ofsuch models. In this paper we prove a theorem that shows that any ensemble willexhibit at least one of the following forms of prediction instability. It willeither ignore agreement among all underlying models, change its mind when noneof the underlying models have done so, or be manipulable through inclusion orexclusion of options it would never actually predict. As a consequence,ensemble aggregation procedures will always need to balance the benefits ofinformation use against the risk of these prediction instabilities. Thisanalysis also sheds light on what specific forms of prediction instability toexpect from particular ensemble algorithms; for example popular tree ensembleslike random forest, or xgboost will violate basic, intuitive fairnessproperties. Finally, we show that this can be ameliorated by using consistentmodels in asymptotic conditions.</description><author>Jeremy Kedziora</author><pubDate>Fri, 16 Aug 2024 16:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03194v4</guid></item><item><title>Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning</title><link>http://arxiv.org/abs/2311.00201v2</link><description>Federated reinforcement learning (RL) enables collaborative decision makingof multiple distributed agents without sharing local data trajectories. In thiswork, we consider a multi-task setting, in which each agent has its own privatereward function corresponding to different tasks, while sharing the sametransition kernel of the environment. Focusing on infinite-horizon Markovdecision processes, the goal is to learn a globally optimal policy thatmaximizes the sum of the discounted total rewards of all the agents in adecentralized manner, where each agent only communicates with its neighborsover some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient(NPG) methods in the tabular setting under softmax parameterization, wheregradient tracking is applied to estimate the global Q-function to mitigate theimpact of imperfect information sharing. We establish non-asymptotic globalconvergence guarantees under exact policy evaluation, where the rates arenearly independent of the size of the state-action space and illuminate theimpacts of network size and connectivity. To the best of our knowledge, this isthe first time that near dimension-free global convergence is established forfederated multi-task RL using policy optimization. We further go beyond thetabular setting by proposing a federated natural actor critic (NAC) method formulti-task RL with function approximation, and establish its finite-time samplecomplexity taking the errors of function approximation into account.</description><author>Tong Yang, Shicong Cen, Yuting Wei, Yuxin Chen, Yuejie Chi</author><pubDate>Fri, 16 Aug 2024 16:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00201v2</guid></item><item><title>Self-Supervised Multimodal Learning: A Survey</title><link>http://arxiv.org/abs/2304.01008v3</link><description>Multimodal learning, which aims to understand and analyze information frommultiple modalities, has achieved substantial progress in the supervised regimein recent years. However, the heavy dependence on data paired with expensivehuman annotations impedes scaling up models. Meanwhile, given the availabilityof large-scale unannotated data in the wild, self-supervised learning hasbecome an attractive strategy to alleviate the annotation bottleneck. Buildingon these two directions, self-supervised multimodal learning (SSML) providesways to learn from raw multimodal data. In this survey, we provide acomprehensive review of the state-of-the-art in SSML, in which we elucidatethree major challenges intrinsic to self-supervised learning with multimodaldata: (1) learning representations from multimodal data without labels, (2)fusion of different modalities, and (3) learning with unaligned data. We thendetail existing solutions to these challenges. Specifically, we consider (1)objectives for learning from multimodal unlabeled data via self-supervision,(2) model architectures from the perspective of different multimodal fusionstrategies, and (3) pair-free learning strategies for coarse-grained andfine-grained alignment. We also review real-world applications of SSMLalgorithms in diverse fields such as healthcare, remote sensing, and machinetranslation. Finally, we discuss challenges and future directions for SSML. Acollection of related resources can be found at:https://github.com/ys-zong/awesome-self-supervised-multimodal-learning.</description><author>Yongshuo Zong, Oisin Mac Aodha, Timothy Hospedales</author><pubDate>Fri, 16 Aug 2024 16:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01008v3</guid></item><item><title>RGBT Tracking via All-layer Multimodal Interactions with Progressive Fusion Mamba</title><link>http://arxiv.org/abs/2408.08827v1</link><description>Existing RGBT tracking methods often design various interaction models toperform cross-modal fusion of each layer, but can not execute the featureinteractions among all layers, which plays a critical role in robust multimodalrepresentation, due to large computational burden. To address this issue, thispaper presents a novel All-layer multimodal Interaction Network, named AINet,which performs efficient and effective feature interactions of all modalitiesand layers in a progressive fusion Mamba, for robust RGBT tracking. Even thoughmodality features in different layers are known to contain different cues, itis always challenging to build multimodal interactions in each layer due tostruggling in balancing interaction capabilities and efficiency. Meanwhile,considering that the feature discrepancy between RGB and thermal modalitiesreflects their complementary information to some extent, we design aDifference-based Fusion Mamba (DFM) to achieve enhanced fusion of differentmodalities with linear complexity. When interacting with features from alllayers, a huge number of token sequences (3840 tokens in this work) areinvolved and the computational burden is thus large. To handle this problem, wedesign an Order-dynamic Fusion Mamba (OFM) to execute efficient and effectivefeature interactions of all layers by dynamically adjusting the scan order ofdifferent layers in Mamba. Extensive experiments on four public RGBT trackingdatasets show that AINet achieves leading performance against existingstate-of-the-art methods.</description><author>Andong Lu, Wanyu Wang, Chenglong Li, Jin Tang, Bin Luo</author><pubDate>Fri, 16 Aug 2024 16:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08827v1</guid></item><item><title>Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov</title><link>http://arxiv.org/abs/2401.11325v3</link><description>Many Reinforcement Learning algorithms assume a Markov reward function toguarantee optimality. However, not all reward functions are Markov. This paperproposes a framework for mapping non-Markov reward functions into equivalentMarkov ones by learning specialized reward automata, Reward Machines. Unlikethe general practice of learning Reward Machines, we do not require a set ofhigh-level propositional symbols from which to learn. Rather, we learn hiddentriggers, directly from data, that construct them. We demonstrate theimportance of learning Reward Machines over their Deterministic Finite-StateAutomata counterparts given their ability to model reward dependencies. Weformalize this distinction in our learning objective. Our mapping process isconstructed as an Integer Linear Programming problem. We prove that ourmappings form a suitable proxy for maximizing reward expectations. Weempirically validate our approach by learning black-box, non-Markov rewardfunctions in the Officeworld domain. Additionally, we demonstrate theeffectiveness of learning reward dependencies in a new domain, Breakfastworld.</description><author>Gregory Hyde, Eugene Santos Jr</author><pubDate>Fri, 16 Aug 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11325v3</guid></item><item><title>LEVIS: Large Exact Verifiable Input Spaces for Neural Networks</title><link>http://arxiv.org/abs/2408.08824v1</link><description>The robustness of neural networks is paramount in safety-criticalapplications. While most current robustness verification methods assess theworst-case output under the assumption that the input space is known,identifying a verifiable input space $\mathcal{C}$, where no adversarialexamples exist, is crucial for effective model selection, robustnessevaluation, and the development of reliable control strategies. To address thischallenge, we introduce a novel framework, $\texttt{LEVIS}$, comprising$\texttt{LEVIS}$-$\alpha$ and $\texttt{LEVIS}$-$\beta$.$\texttt{LEVIS}$-$\alpha$ locates the largest possible verifiable ball withinthe central region of $\mathcal{C}$ that intersects at least two boundaries. Incontrast, $\texttt{LEVIS}$-$\beta$ integrates multiple verifiable balls toencapsulate the entirety of the verifiable space comprehensively. Ourcontributions are threefold: (1) We propose $\texttt{LEVIS}$ equipped withthree pioneering techniques that identify the maximum verifiable ball and thenearest adversarial point along collinear or orthogonal directions. (2) Weoffer a theoretical analysis elucidating the properties of the verifiable ballsacquired through $\texttt{LEVIS}$-$\alpha$ and $\texttt{LEVIS}$-$\beta$. (3) Wevalidate our methodology across diverse applications, including electricalpower flow regression and image classification, showcasing performanceenhancements and visualizations of the searching characteristics.</description><author>Mohamad Fares El Hajj Chehade, Brian Wesley Bell, Russell Bent, Hao Zhu, Wenting Li</author><pubDate>Fri, 16 Aug 2024 16:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08824v1</guid></item><item><title>Optimal Symmetries in Binary Classification</title><link>http://arxiv.org/abs/2408.08823v1</link><description>We explore the role of group symmetries in binary classification tasks,presenting a novel framework that leverages the principles of Neyman-Pearsonoptimality. Contrary to the common intuition that larger symmetry groups leadto improved classification performance, our findings show that selecting theappropriate group symmetries is crucial for optimising generalisation andsample efficiency. We develop a theoretical foundation for designing groupequivariant neural networks that align the choice of symmetries with theunderlying probability distributions of the data. Our approach provides aunified methodology for improving classification accuracy across a broad rangeof applications by carefully tailoring the symmetry group to the specificcharacteristics of the problem. Theoretical analysis and experimental resultsdemonstrate that optimal classification performance is not always associatedwith the largest equivariant groups possible in the domain, even when thelikelihood ratio is invariant under one of its proper subgroups, but ratherwith those subgroups themselves. This work offers insights and practicalguidelines for constructing more effective group equivariant architectures indiverse machine-learning contexts.</description><author>Vishal S. Ngairangbam, Michael Spannowsky</author><pubDate>Fri, 16 Aug 2024 16:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08823v1</guid></item><item><title>PFDiff: Training-free Acceleration of Diffusion Models through the Gradient Guidance of Past and Future</title><link>http://arxiv.org/abs/2408.08822v1</link><description>Diffusion Probabilistic Models (DPMs) have shown remarkable potential inimage generation, but their sampling efficiency is hindered by the need fornumerous denoising steps. Most existing solutions accelerate the samplingprocess by proposing fast ODE solvers. However, the inevitable discretizationerrors of the ODE solvers are significantly magnified when the number offunction evaluations (NFE) is fewer. In this work, we propose PFDiff, a noveltraining-free and orthogonal timestep-skipping strategy, which enables existingfast ODE solvers to operate with fewer NFE. Based on two key observations: asignificant similarity in the model's outputs at time step size that is notexcessively large during the denoising process of existing ODE solvers, and ahigh resemblance between the denoising process and SGD. PFDiff, by employinggradient replacement from past time steps and foresight updates inspired byNesterov momentum, rapidly updates intermediate states, thereby reducingunnecessary NFE while correcting for discretization errors inherent infirst-order ODE solvers. Experimental results demonstrate that PFDiff exhibitsflexible applicability across various pre-trained DPMs, particularly excellingin conditional DPMs and surpassing previous state-of-the-art training-freemethods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE)compared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance,and 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale.</description><author>Guangyi Wang, Yuren Cai, Lijiang Li, Wei Peng, Songzhi Su</author><pubDate>Fri, 16 Aug 2024 16:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08822v1</guid></item><item><title>EasyRec: Simple yet Effective Language Models for Recommendation</title><link>http://arxiv.org/abs/2408.08821v1</link><description>Deep neural networks have become a powerful technique for learningrepresentations from user-item interaction data in collaborative filtering (CF)for recommender systems. However, many existing methods heavily rely on uniqueuser and item IDs, which limits their ability to perform well in practicalzero-shot learning scenarios where sufficient training data may be unavailable.Inspired by the success of language models (LMs) and their stronggeneralization capabilities, a crucial question arises: How can we harness thepotential of language models to empower recommender systems and elevate itsgeneralization capabilities to new heights? In this study, we propose EasyRec -an effective and easy-to-use approach that seamlessly integrates text-basedsemantic understanding with collaborative signals. EasyRec employs atext-behavior alignment framework, which combines contrastive learning withcollaborative language model tuning, to ensure a strong alignment between thetext-enhanced semantic space and the collaborative behavior information.Extensive empirical evaluations across diverse real-world datasets demonstratethe superior performance of EasyRec compared to state-of-the-art alternativemodels, particularly in the challenging text-based zero-shot recommendationscenarios. Furthermore, the study highlights the potential of seamlesslyintegrating EasyRec as a plug-and-play component into text-enhancedcollaborative filtering frameworks, thereby empowering existing recommendersystems to elevate their recommendation performance and adapt to the evolvinguser preferences in dynamic environments. For better result reproducibility ofour EasyRec framework, the model implementation details, source code, anddatasets are available at the link: https://github.com/HKUDS/EasyRec.</description><author>Xubin Ren, Chao Huang</author><pubDate>Fri, 16 Aug 2024 16:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08821v1</guid></item><item><title>Agentic Skill Discovery</title><link>http://arxiv.org/abs/2405.15019v2</link><description>Language-conditioned robotic skills make it possible to apply the high-levelreasoning of Large Language Models (LLMs) to low-level robotic control. Aremaining challenge is to acquire a diverse set of fundamental skills. Existingapproaches either manually decompose a complex task into atomic robotic actionsin a top-down fashion, or bootstrap as many combinations as possible in abottom-up fashion to cover a wider range of task possibilities. Thesedecompositions or combinations, however, require an initial skill library. Forexample, a ``grasping'' capability can never emerge from a skill librarycontaining only diverse ``pushing'' skills. Existing skill discovery techniqueswith reinforcement learning acquire skills by an exhaustive exploration butoften yield non-meaningful behaviors. In this study, we introduce a novelframework for skill discovery that is entirely driven by LLMs. The frameworkbegins with an LLM generating task proposals based on the provided scenedescription and the robot's configurations, aiming to incrementally acquire newskills upon task completion. For each proposed task, a series of reinforcementlearning processes are initiated, utilizing reward and success determinationfunctions sampled by the LLM to develop the corresponding policy. Thereliability and trustworthiness of learned behaviors are further ensured by anindependent vision-language model. We show that starting with zero skill, theskill library emerges and expands to more and more meaningful and reliableskills, enabling the robot to efficiently further propose and complete advancedtasks. Project page: \url{https://agentic-skill-discovery.github.io}.</description><author>Xufeng Zhao, Cornelius Weber, Stefan Wermter</author><pubDate>Fri, 16 Aug 2024 15:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15019v2</guid></item><item><title>An Empirical Examination of Balancing Strategy for Counterfactual Estimation on Time Series</title><link>http://arxiv.org/abs/2408.08815v1</link><description>Counterfactual estimation from observations represents a critical endeavor innumerous application fields, such as healthcare and finance, with the primarychallenge being the mitigation of treatment bias. The balancing strategy aimedat reducing covariate disparities between different treatment groups serves asa universal solution. However, when it comes to the time series data, theeffectiveness of balancing strategies remains an open question, with a thoroughanalysis of the robustness and applicability of balancing strategies stilllacking. This paper revisits counterfactual estimation in the temporal settingand provides a brief overview of recent advancements in balancing strategies.More importantly, we conduct a critical empirical examination for theeffectiveness of the balancing strategies within the realm of temporalcounterfactual estimation in various settings on multiple datasets. Ourfindings could be of significant interest to researchers and practitioners andcall for a reexamination of the balancing strategy in time series settings.</description><author>Qiang Huang, Chuizheng Meng, Defu Cao, Biwei Huang, Yi Chang, Yan Liu</author><pubDate>Fri, 16 Aug 2024 15:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08815v1</guid></item><item><title>SLAM for Visually Impaired People: a Survey</title><link>http://arxiv.org/abs/2212.04745v6</link><description>In recent decades, several assistive technologies have been developed toimprove the ability of blind and visually impaired (BVI) individuals tonavigate independently and safely. At the same time, simultaneous localizationand mapping (SLAM) techniques have become sufficiently robust and efficient tobe adopted in developing these assistive technologies. We present the firstsystematic literature review of 54 recent studies on SLAM-based solutions forblind and visually impaired people, focusing on literature published from 2017onward. This review explores various localization and mapping techniquesemployed in this context. We systematically identified and categorized diverseSLAM approaches and analyzed their localization and mapping techniques, sensortypes, computing resources, and machine-learning methods. We discuss theadvantages and limitations of these techniques for blind and visually impairednavigation. Moreover, we examine the major challenges described across studies,including practical challenges and considerations that affect usability andadoption. Our analysis also evaluates the effectiveness of these SLAM-basedsolutions in real-world scenarios and user satisfaction, providing insightsinto their practical impact on BVI mobility. The insights derived from thisreview identify critical gaps and opportunities for future research activities,particularly in addressing the challenges presented by dynamic and complexenvironments. We explain how SLAM technology offers the potential to improvethe ability of visually impaired individuals to navigate effectively. Finally,we present future opportunities and challenges in this domain.</description><author>Marziyeh Bamdad, Davide Scaramuzza, Alireza Darvishy</author><pubDate>Fri, 16 Aug 2024 15:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04745v6</guid></item><item><title>Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models</title><link>http://arxiv.org/abs/2408.08813v1</link><description>Medical image segmentation is crucial for clinical decision-making, but thescarcity of annotated data presents significant challenges. Few-shotsegmentation (FSS) methods show promise but often require retraining on thetarget domain and struggle to generalize across different modalities.Similarly, adapting foundation models like the Segment Anything Model (SAM) formedical imaging has limitations, including the need for finetuning anddomain-specific adaptation. To address these issues, we propose a novel methodthat adapts DINOv2 and Segment Anything Model 2 (SAM 2) for retrieval-augmentedfew-shot medical image segmentation. Our approach uses DINOv2's feature asquery to retrieve similar samples from limited annotated data, which are thenencoded as memories and stored in memory bank. With the memory attentionmechanism of SAM 2, the model leverages these memories as conditions togenerate accurate segmentation of the target image. We evaluated our frameworkon three medical image segmentation tasks, demonstrating superior performanceand generalizability across various modalities without the need for anyretraining or finetuning. Overall, this method offers a practical and effectivesolution for few-shot medical image segmentation and holds significantpotential as a valuable annotation tool in clinical applications.</description><author>Lin Zhao, Xiao Chen, Eric Z. Chen, Yikang Liu, Terrence Chen, Shanhui Sun</author><pubDate>Fri, 16 Aug 2024 15:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08813v1</guid></item><item><title>CAT: Caution Aware Transfer in Reinforcement Learning via Distributional Risk</title><link>http://arxiv.org/abs/2408.08812v1</link><description>Transfer learning in reinforcement learning (RL) has become a pivotalstrategy for improving data efficiency in new, unseen tasks by utilizingknowledge from previously learned tasks. This approach is especially beneficialin real-world deployment scenarios where computational resources areconstrained and agents must adapt rapidly to novel environments. However,current state-of-the-art methods often fall short in ensuring safety during thetransfer process, particularly when unforeseen risks emerge in the deploymentphase. In this work, we address these limitations by introducing a novelCaution-Aware Transfer Learning (CAT) framework. Unlike traditional approachesthat limit risk considerations to mean-variance, we define "caution" as a moregeneralized and comprehensive notion of risk. Our core innovation lies inoptimizing a weighted sum of reward return and caution-based on state-actionoccupancy measures-during the transfer process, allowing for a richrepresentation of diverse risk factors. To the best of our knowledge, this isthe first work to explore the optimization of such a generalized risk notionwithin the context of transfer RL. Our contributions are threefold: (1) Wepropose a Caution-Aware Transfer (CAT) framework that evaluates source policieswithin the test environment and constructs a new policy that balances rewardmaximization and caution. (2) We derive theoretical sub-optimality bounds forour method, providing rigorous guarantees of its efficacy. (3) We empiricallyvalidate CAT, demonstrating that it consistently outperforms existing methodsby delivering safer policies under varying risk conditions in the test tasks.</description><author>Mohamad Fares El Hajj Chehade, Amrit Singh Bedi, Amy Zhang, Hao Zhu</author><pubDate>Fri, 16 Aug 2024 15:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08812v1</guid></item><item><title>Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge</title><link>http://arxiv.org/abs/2408.08808v1</link><description>Large Language Models (LLMs) have revolutionized the landscape of machinelearning, yet current benchmarks often fall short in capturing the diversebehavior of these models in real-world applications. A benchmark's usefulnessis determined by its ability to clearly differentiate between models of varyingcapabilities (separability) and closely align with human preferences. Existingframeworks like Alpaca-Eval 2.0 LC\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\cite{li2024crowdsourced} are limited by their focus on general-purpose queriesand lack of diversity across domains such as law, medicine, and multilingualcontexts. In this paper, we address these limitations by introducing a noveldata pipeline that curates diverse, domain-specific evaluation sets tailoredfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manualcuration, semi-supervised learning to generate clusters, and stratifiedsampling to ensure balanced representation across a wide range of domains andlanguages. The resulting evaluation set, which includes 1573 samples across 14categories, demonstrates high separability (84\%) across ten top-ranked models,and agreement (84\%) with Chatbot Arena and (0.915) Spearman correlation. Theagreement values are 9\% better than Arena Hard and 20\% better than AlpacaEval2.0 LC, while the Spearman coefficient is 0.7 more than the next bestbenchmark, showcasing a significant improvement in the usefulness of thebenchmark. We further provide an open-source evaluation tool that enablesfine-grained analysis of model performance across user-defined categories,offering valuable insights for practitioners. This work contributes to theongoing effort to enhance the transparency, diversity, and effectiveness of LLMevaluation methodologies.</description><author>Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakkar</author><pubDate>Fri, 16 Aug 2024 15:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08808v1</guid></item><item><title>Multi-Hop Table Retrieval for Open-Domain Text-to-SQL</title><link>http://arxiv.org/abs/2402.10666v3</link><description>Open-domain text-to-SQL is an important task that retrieves question-relevanttables from massive databases and then generates SQL. However, existingretrieval methods that retrieve in a single hop do not pay attention to thetext-to-SQL challenge of schema linking, which is aligning the entities in thequestion with table entities, reflected in two aspects: similar irrelevantentity and domain mismatch entity. Therefore, we propose our method, themulti-hop table retrieval with rewrite and beam search (Murre). To reduce theeffect of the similar irrelevant entity, our method focuses on unretrievedentities at each hop and considers the low-ranked tables by beam search. Toalleviate the limitation of domain mismatch entity, Murre rewrites the questionbased on retrieved tables in multiple hops, decreasing the domain gap withrelevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reachingnew state-of-the-art results with an average improvement of 6.38%.</description><author>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che</author><pubDate>Fri, 16 Aug 2024 15:37:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10666v3</guid></item><item><title>Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification</title><link>http://arxiv.org/abs/2406.08660v2</link><description>Generative AI offers a simple, prompt-based alternative to fine-tuningsmaller BERT-style LLMs for text classification tasks. This promises toeliminate the need for manually labeled training data and task-specific modeltraining. However, it remains an open question whether tools like ChatGPT candeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs(still) consistently and significantly outperform larger, zero-shot promptedmodels in text classification. We compare three major generative AI models(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMsacross a diverse set of classification tasks (sentiment, approval/disapproval,emotions, party positions) and text categories (news, tweets, speeches). Wefind that fine-tuning with application-specific training data achieves superiorperformance in all cases. To make this approach more accessible to a broaderaudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,accompanied by non-technical step-by-step guidance, enables users to select andfine-tune BERT-like LLMs for any classification task with minimal technical andcomputational effort.</description><author>Martin Juan José Bucher, Marco Martini</author><pubDate>Fri, 16 Aug 2024 15:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08660v2</guid></item><item><title>CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems</title><link>http://arxiv.org/abs/2408.08805v1</link><description>In this study, we introduce CIKMar, an efficient approach to educationaldialogue systems powered by the Gemma Language model. By leveraging aDual-Encoder ranking system that incorporates both BERT and SBERT model, wehave designed CIKMar to deliver highly relevant and accurate responses, evenwith the constraints of a smaller language model size. Our evaluation revealsthat CIKMar achieves a robust recall and F1-score of 0.70 using BERTScoremetrics. However, we have identified a significant challenge: the Dual-Encodertends to prioritize theoretical responses over practical ones. These findingsunderscore the potential of compact and efficient models like Gemma indemocratizing access to advanced educational AI systems, ensuring effective andcontextually appropriate responses.</description><author>Joanito Agili Lopo, Marina Indah Prasasti, Alma Permatasari</author><pubDate>Fri, 16 Aug 2024 15:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08805v1</guid></item><item><title>Dataset-learning duality and emergent criticality</title><link>http://arxiv.org/abs/2405.17391v2</link><description>In artificial neural networks, the activation dynamics of non-trainablevariables is strongly coupled to the learning dynamics of trainable variables.During the activation pass, the boundary neurons (e.g., input neurons) aremapped to the bulk neurons (e.g., hidden neurons), and during the learningpass, both bulk and boundary neurons are mapped to changes in trainablevariables (e.g., weights and biases). For example, in feed-forward neuralnetworks, forward propagation is the activation pass and backward propagationis the learning pass. We show that a composition of the two maps establishes aduality map between a subspace of non-trainable boundary variables (e.g.,dataset) and a tangent subspace of trainable variables (i.e., learning). Ingeneral, the dataset-learning duality is a complex non-linear map betweenhigh-dimensional spaces, but in a learning equilibrium, the problem can belinearized and reduced to many weakly coupled one-dimensional problems. We usethe duality to study the emergence of criticality, or the power-lawdistributions of fluctuations of the trainable variables. In particular, weshow that criticality can emerge in the learning system even from the datasetin a non-critical state, and that the power-law distribution can be modified bychanging either the activation function or the loss function.</description><author>Ekaterina Kukleva, Vitaly Vanchurin</author><pubDate>Fri, 16 Aug 2024 15:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17391v2</guid></item><item><title>Leveraging FourierKAN Classification Head for Pre-Trained Transformer-based Text Classification</title><link>http://arxiv.org/abs/2408.08803v1</link><description>For many years, transformer-based pre-trained models with Multi-layerPerceptron (MLP) heads have been the standard for text classification tasks.However, the fixed non-linear functions employed by MLPs often fall short ofcapturing the intricacies of the contextualized embeddings produced bypre-trained encoders. Furthermore, MLPs usually require a significant number oftraining parameters, which can be computationally expensive. In this work, weintroduce FourierKAN (FR-KAN), a variant of the promising MLP alternativecalled Kolmogorov-Arnold Networks (KANs), as classification heads fortransformer-based encoders. Our studies reveal an average increase of 10% inaccuracy and 11% in F1-score when incorporating FR-KAN heads instead oftraditional MLP heads for several transformer-based pre-trained models acrossmultiple text classification tasks. Beyond improving model accuracy, FR-KANheads train faster and require fewer parameters. Our research opens new groundsfor broader applications of KAN across several Natural Language Processing(NLP) tasks.</description><author>Abdullah Al Imran, Md Farhan Ishmam</author><pubDate>Fri, 16 Aug 2024 15:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08803v1</guid></item><item><title>PriorMapNet: Enhancing Online Vectorized HD Map Construction with Priors</title><link>http://arxiv.org/abs/2408.08802v1</link><description>Online vectorized High-Definition (HD) map construction is crucial forsubsequent prediction and planning tasks in autonomous driving. Following MapTRparadigm, recent works have made noteworthy achievements. However, referencepoints are randomly initialized in mainstream methods, leading to unstablematching between predictions and ground truth. To address this issue, weintroduce PriorMapNet to enhance online vectorized HD map construction withpriors. We propose the PPS-Decoder, which provides reference points withposition and structure priors. Fitted from the map elements in the dataset,prior reference points lower the learning difficulty and achieve stablematching. Furthermore, we propose the PF-Encoder to enhance the image-to-BEVtransformation with BEV feature priors. Besides, we propose the DMDcross-attention, which decouples cross-attention along multi-scale andmulti-sample respectively to achieve efficiency. Our proposed PriorMapNetachieves state-of-the-art performance in the online vectorized HD mapconstruction task on nuScenes and Argoverse2 datasets. The code will bereleased publicly soon.</description><author>Rongxuan Wang, Xin Lu, Xiaoyang Liu, Xiaoyi Zou, Tongyi Cao, Ying Li</author><pubDate>Fri, 16 Aug 2024 15:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08802v1</guid></item><item><title>CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images</title><link>http://arxiv.org/abs/2311.03967v4</link><description>The ultra-widefield (UWF) fundus image is an attractive 3D biomarker inAI-aided myopia screening because it provides much richer myopia-relatedinformation. Though axial length (AL) has been acknowledged to be highlyrelated to the two key targets of myopia screening, Spherical Equivalence (SE)measurement and high myopia diagnosis, its prediction based on the UWF fundusimage is rarely considered. To save the high expense and time costs ofmeasuring SE and AL, we propose the Copula-enhanced Convolutional NeuralNetwork (CeCNN), a one-stop UWF-based ophthalmic AI framework to jointlypredict SE, AL, and myopia status. The CeCNN formulates a multiresponseregression that relates multiple dependent discrete-continuous responses andthe image covariate, where the nonlinearity of the association is modeled by abackbone CNN. To thoroughly describe the dependence structure among theresponses, we model and incorporate the conditional dependence among responsesin a CNN through a new copula-likelihood loss. We provide statisticalinterpretations of the conditional dependence among responses, and reveal thatsuch dependence is beyond the dependence explained by the image covariate. Weheuristically justify that the proposed loss can enhance the estimationefficiency of the CNN weights. We apply the CeCNN to the UWF dataset collectedby us and demonstrate that the CeCNN sharply enhances the predictive capabilityof various backbone CNNs. Our study evidences the ophthalmology view thatbesides SE, AL is also an important measure to myopia.</description><author>Chong Zhong, Yang Li, Danjuan Yang, Meiyan Li, Xingyao Zhou, Bo Fu, Catherine C. Liu, A. H. Welsh</author><pubDate>Fri, 16 Aug 2024 15:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03967v4</guid></item><item><title>Representation Learning of Geometric Trees</title><link>http://arxiv.org/abs/2408.08799v1</link><description>Geometric trees are characterized by their tree-structured layout andspatially constrained nodes and edges, which significantly impacts theirtopological attributes. This inherent hierarchical structure plays a crucialrole in domains such as neuron morphology and river geomorphology, buttraditional graph representation methods often overlook these specificcharacteristics of tree structures. To address this, we introduce a newrepresentation learning framework tailored for geometric trees. It firstfeatures a unique message passing neural network, which is both provablygeometrical structure-recoverable and rotation-translation invariant. Toaddress the data label scarcity issue, our approach also includes twoinnovative training targets that reflect the hierarchical ordering andgeometric structure of these geometric trees. This enables fullyself-supervised learning without explicit labels. We validate our method'seffectiveness on eight real-world datasets, demonstrating its capability torepresent geometric trees.</description><author>Zheng Zhang, Allen Zhang, Ruth Nelson, Giorgio Ascoli, Liang Zhao</author><pubDate>Fri, 16 Aug 2024 15:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08799v1</guid></item><item><title>Improving Task Instructions for Data Annotators: How Clear Rules and Higher Pay Increase Performance in Data Annotation in the AI Economy</title><link>http://arxiv.org/abs/2312.14565v2</link><description>The global surge in AI applications is transforming industries, leading todisplacement and complementation of existing jobs, while also giving rise tonew employment opportunities. Data annotation, encompassing the labelling ofimages or annotating of texts by human workers, crucially influences thequality of a dataset directly influences the quality of AI models trained onit. This paper delves into the economics of data annotation, with a specificfocus on the impact of task instruction design (that is, the choice betweenrules and standards as theorised in law and economics) and monetary incentiveson data quality and costs. An experimental study involving 307 data annotatorsexamines six groups with varying task instructions (norms) and monetaryincentives. Results reveal that annotators provided with clear rules exhibithigher accuracy rates, outperforming those with vague standards by 14%.Similarly, annotators receiving an additional monetary incentive performsignificantly better, with the highest accuracy rate recorded in the groupworking with both clear rules and incentives (87.5% accuracy). In addition, ourresults show that rules are perceived as being more helpful by annotators thanstandards and reduce annotators' difficulty in annotating images. Theseempirical findings underscore the double benefit of rule-based instructions onboth data quality and worker wellbeing. Our research design allows us to revealthat, in our study, rules are more cost-efficient in increasing accuracy thanmonetary incentives. The paper contributes experimental insights to discussionson the economical, ethical, and legal considerations of AI technologies.Addressing policymakers and practitioners, we emphasise the need for a balancedapproach in optimising data annotation processes for efficient and ethical AIdevelopment and usage.</description><author>Johann Laux, Fabian Stephany, Alice Liefgreen</author><pubDate>Fri, 16 Aug 2024 15:12:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14565v2</guid></item><item><title>Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation</title><link>http://arxiv.org/abs/2408.06945v2</link><description>By using an parametric value function to replace the Monte-Carlo rollouts forvalue estimation, the actor-critic (AC) algorithms can reduce the variance ofstochastic policy gradient so that to improve the convergence rate. Whileexisting works mainly focus on analyzing convergence rate of AC algorithmsunder Markovian noise, the impacts of momentum on AC algorithms remain largelyunexplored. In this work, we first propose a heavy-ball momentum basedadvantage actor-critic (\mbox{HB-A2C}) algorithm by integrating the heavy-ballmomentum into the critic recursion that is parameterized by a linear function.When the sample trajectory follows a Markov decision process, we quantitativelycertify the acceleration capability of the proposed HB-A2C algorithm. Ourtheoretical results demonstrate that the proposed HB-A2C finds an$\epsilon$-approximate stationary point with $\oo{\epsilon^{-2}}$ iterationsfor reinforcement learning tasks with Markovian noise. Moreover, we also revealthe dependence of learning rates on the length of the sample trajectory. Bycarefully selecting the momentum factor of the critic recursion, the proposedHB-A2C can balance the errors introduced by the initialization and thestoschastic approximation.</description><author>Yanjie Dong, Haijun Zhang, Gang Wang, Shisheng Cui, Xiping Hu</author><pubDate>Fri, 16 Aug 2024 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06945v2</guid></item><item><title>Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer</title><link>http://arxiv.org/abs/2408.08793v1</link><description>Visual retrieval systems face significant challenges when updating modelswith improved representations due to misalignment between the old and newrepresentations. The costly and resource-intensive backfilling process involvesrecalculating feature vectors for images in the gallery set whenever a newmodel is introduced. To address this, prior research has exploredbackward-compatible training methods that enable direct comparisons between newand old representations without backfilling. Despite these advancements,achieving a balance between backward compatibility and the performance ofindependently trained models remains an open problem. In this paper, we addressit by expanding the representation space with additional dimensions andlearning an orthogonal transformation to achieve compatibility with old modelsand, at the same time, integrate new information. This transformation preservesthe original feature space's geometry, ensuring that our model aligns withprevious versions while also learning new data. Our Orthogonal CompatibleAligned (OCA) approach eliminates the need for re-indexing during model updatesand ensures that features can be compared directly across different modelupdates without additional mapping functions. Experimental results on CIFAR-100and ImageNet-1k demonstrate that our method not only maintains compatibilitywith previous models but also achieves state-of-the-art accuracy, outperformingseveral existing methods.</description><author>Simone Ricci, Niccolò Biondi, Federico Pernici, Alberto Del Bimbo</author><pubDate>Fri, 16 Aug 2024 15:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08793v1</guid></item><item><title>Assessing Generalization Capabilities of Malaria Diagnostic Models from Thin Blood Smears</title><link>http://arxiv.org/abs/2408.08792v1</link><description>Malaria remains a significant global health challenge, necessitating rapidand accurate diagnostic methods. While computer-aided diagnosis (CAD) toolsutilizing deep learning have shown promise, their generalization to diverseclinical settings remains poorly assessed. This study evaluates thegeneralization capabilities of a CAD model for malaria diagnosis from thinblood smear images across four sites. We explore strategies to enhancegeneralization, including fine-tuning and incremental learning. Our resultsdemonstrate that incorporating site-specific data significantly improves modelperformance, paving the way for broader clinical application.</description><author>Louise Guillon, Soheib Biga, Axel Puyo, Grégoire Pasquier, Valentin Foucher, Yendoubé E. Kantchire, Stéphane E. Sossou, Ameyo M. Dorkenoo, Laurent Bonnardot, Marc Thellier, Laurence Lachaud, Renaud Piarroux</author><pubDate>Fri, 16 Aug 2024 15:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08792v1</guid></item><item><title>A Disease-Specific Foundation Model Using Over 100K Fundus Images: Release and Validation for Abnormality and Multi-Disease Classification on Downstream Tasks</title><link>http://arxiv.org/abs/2408.08790v1</link><description>Artificial intelligence applied to retinal images offers significantpotential for recognizing signs and symptoms of retinal conditions andexpediting the diagnosis of eye diseases and systemic disorders. However,developing generalized artificial intelligence models for medical data oftenrequires a large number of labeled images representing various disease signs,and most models are typically task-specific, focusing on major retinaldiseases. In this study, we developed a Fundus-Specific Pretrained Model(Image+Fundus), a supervised artificial intelligence model trained to detectabnormalities in fundus images. A total of 57,803 images were used to developthis pretrained model, which achieved superior performance across variousdownstream tasks, indicating that our proposed model outperforms other generalmethods. Our Image+Fundus model offers a generalized approach to improve modelperformance while reducing the number of labeled datasets required.Additionally, it provides more disease-specific insights into fundus images,with visualizations generated by our model. These disease-specific foundationmodels are invaluable in enhancing the performance and efficiency of deeplearning models in the field of fundus imaging.</description><author>Boa Jang, Youngbin Ahn, Eun Kyung Choe, Chang Ki Yoon, Hyuk Jin Choi, Young-Gon Kim</author><pubDate>Fri, 16 Aug 2024 15:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08790v1</guid></item><item><title>Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning</title><link>http://arxiv.org/abs/2406.14322v3</link><description>Large language models (LLMs) have emerged as powerful tools for tacklingcomplex tasks across diverse domains, but they also raise privacy concerns whenfine-tuned on sensitive data due to potential memorization. While differentialprivacy (DP) offers a promising solution by ensuring models are 'almostindistinguishable' with or without any particular privacy unit, currentevaluations on LLMs mostly treat each example (text record) as the privacyunit. This leads to uneven user privacy guarantees when contributions per uservary. We therefore study user-level DP motivated by applications where itnecessary to ensure uniform privacy protection across users. We present asystematic evaluation of user-level DP for LLM fine-tuning on natural languagegeneration tasks. Focusing on two mechanisms for achieving user-level DPguarantees, Group Privacy and User-wise DP-SGD, we investigate design choiceslike data selection strategies and parameter tuning for the bestprivacy-utility tradeoff.</description><author>Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</author><pubDate>Fri, 16 Aug 2024 15:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14322v3</guid></item><item><title>Neighbor Overlay-Induced Graph Attention Network</title><link>http://arxiv.org/abs/2408.08788v1</link><description>Graph neural networks (GNNs) have garnered significant attention due to theirability to represent graph data. Among various GNN variants, graph attentionnetwork (GAT) stands out since it is able to dynamically learn the importanceof different nodes. However, present GATs heavily rely on the smoothed nodefeatures to obtain the attention coefficients rather than graph structuralinformation, which fails to provide crucial contextual cues for noderepresentations. To address this issue, this study proposes a neighboroverlay-induced graph attention network (NO-GAT) with the following two-foldideas: a) learning favorable structural information, i.e., overlaid neighbors,outside the node feature propagation process from an adjacency matrix; b)injecting the information of overlaid neighbors into the node featurepropagation process to compute the attention coefficient jointly. Empiricalstudies on graph benchmark datasets indicate that the proposed NO-GATconsistently outperforms state-of-the-art models.</description><author>Tiqiao Wei, Ye Yuan</author><pubDate>Fri, 16 Aug 2024 15:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08788v1</guid></item><item><title>Centralized and Federated Heart Disease Classification Models Using UCI Dataset and their Shapley-value Based Interpretability</title><link>http://arxiv.org/abs/2408.06183v2</link><description>Cardiovascular diseases are a leading cause of mortality worldwide,highlighting the need for accurate diagnostic methods. This study benchmarkscentralized and federated machine learning algorithms for heart diseaseclassification using the UCI dataset which includes 920 patient records fromfour hospitals in the USA, Hungary and Switzerland. Our benchmark is supportedby Shapley-value interpretability analysis to quantify features' importance forclassification. In the centralized setup, various binary classificationalgorithms are trained on pooled data, with a support vector machine (SVM)achieving the highest testing accuracy of 83.3\%, surpassing the establishedbenchmark of 78.7\% with logistic regression. Additionally, federated learningalgorithms with four clients (hospitals) are explored, leveraging the dataset'snatural partition to enhance privacy without sacrificing accuracy. FederatedSVM, an uncommon approach in the literature, achieves a top testing accuracy of73.8\%. Our interpretability analysis aligns with existing medical knowledge ofheart disease indicators. Overall, this study establishes a benchmark forefficient and interpretable pre-screening tools for heart disease whilemaintaining patients' privacy. This work is available athttps://github.com/padillma1/Heart-Disease-Classification-on-UCI-dataset-and-Shapley-Interpretability-Analysis.</description><author>Mario Padilla Rodriguez, Mohamed Nafea</author><pubDate>Fri, 16 Aug 2024 14:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06183v2</guid></item><item><title>A Transparency Paradox? Investigating the Impact of Explanation Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers</title><link>http://arxiv.org/abs/2408.08785v1</link><description>Transparency in automated systems could be afforded through the provision ofintelligible explanations. While transparency is desirable, might it lead tocatastrophic outcomes (such as anxiety), that could outweigh its benefits? It'squite unclear how the specificity of explanations (level of transparency)influences recipients, especially in autonomous driving (AD). In this work, weexamined the effects of transparency mediated through varying levels ofexplanation specificity in AD. We first extended a data-driven explainer modelby adding a rule-based option for explanation generation in AD, and thenconducted a within-subject lab study with 39 participants in an immersivedriving simulator to study the effect of the resulting explanations.Specifically, our investigation focused on: (1) how different types ofexplanations (specific vs. abstract) affect passengers' perceived safety,anxiety, and willingness to take control of the vehicle when the vehicleperception system makes erroneous predictions; and (2) the relationship betweenpassengers' behavioural cues and their feelings during the autonomous drives.Our findings showed that passengers felt safer with specific explanations whenthe vehicle's perception system had minimal errors, while abstract explanationsthat hid perception errors led to lower feelings of safety. Anxiety levelsincreased when specific explanations revealed perception system errors (hightransparency). We found no significant link between passengers' visual patternsand their anxiety levels. Our study suggests that passengers prefer clear andspecific explanations (high transparency) when they originate from autonomousvehicles (AVs) with optimal perceptual accuracy.</description><author>Daniel Omeiza, Raunak Bhattacharyya, Marina Jirotka, Nick Hawes, Lars Kunze</author><pubDate>Fri, 16 Aug 2024 14:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08785v1</guid></item><item><title>Transformers and Cortical Waves: Encoders for Pulling In Context Across Time</title><link>http://arxiv.org/abs/2401.14267v3</link><description>The capabilities of transformer networks such as ChatGPT and other LargeLanguage Models (LLMs) have captured the world's attention. The crucialcomputational mechanism underlying their performance relies on transforming acomplete input sequence - for example, all the words in a sentence - into along "encoding vector" that allows transformers to learn long-range temporaldependencies in naturalistic sequences. Specifically, "self-attention" appliedto this encoding vector enhances temporal context in transformers by computingassociations between pairs of words in the input sequence. We suggest thatwaves of neural activity traveling across single cortical areas or multipleregions at the whole-brain scale could implement a similar encoding principle.By encapsulating recent input history into a single spatial pattern at eachmoment in time, cortical waves may enable temporal context to be extracted fromsequences of sensory inputs, the same computational principle used intransformers.</description><author>Lyle Muller, Patricia S. Churchland, Terrence J. Sejnowski</author><pubDate>Fri, 16 Aug 2024 14:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14267v3</guid></item><item><title>Multi-task Learning Approach for Intracranial Hemorrhage Prognosis</title><link>http://arxiv.org/abs/2408.08784v1</link><description>Prognosis after intracranial hemorrhage (ICH) is influenced by a complexinterplay between imaging and tabular data. Rapid and reliable prognosis arecrucial for effective patient stratification and informed treatmentdecision-making. In this study, we aim to enhance image-based prognosis bylearning a robust feature representation shared between prognosis and theclinical and demographic variables most highly correlated with it. Our approachmimics clinical decision-making by reinforcing the model to learn valuableprognostic data embedded in the image. We propose a 3D multi-task image modelto predict prognosis, Glasgow Coma Scale and age, improving accuracy andinterpretability. Our method outperforms current state-of-the-art baselineimage models, and demonstrates superior performance in ICH prognosis comparedto four board-certified neuroradiologists using only CT scans as input. Wefurther validate our model with interpretability saliency maps. Code isavailable at https://github.com/MiriamCobo/MultitaskLearning_ICH_Prognosis.git.</description><author>Miriam Cobo, Amaia Pérez del Barrio, Pablo Menéndez Fernández-Miranda, Pablo Sanz Bellón, Lara Lloret Iglesias, Wilson Silva</author><pubDate>Fri, 16 Aug 2024 14:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08784v1</guid></item><item><title>EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics</title><link>http://arxiv.org/abs/2408.08782v1</link><description>Designing emotionally intelligent conversational systems to provide comfortand advice to people experiencing distress is a compelling area of research.Previous efforts have focused on developing modular dialogue systems that treatsocio-emotional strategy prediction as an auxiliary task and generatestrategy-conditioned responses with customized decoders. Recently, withadvancements in large language models (LLMs), end-to-end dialogue agentswithout explicit socio-emotional strategy prediction steps have becomeprevalent. However, despite their excellence in language generation, recentstudies show that LLMs' inherent preference bias towards certainsocio-emotional strategies hinders the delivery of high-quality emotionalsupport. To address this challenge, we propose decoupling strategy predictionfrom language generation, and introduce a novel dialogue strategy predictor,EmoDynamiX, which models the discourse dynamics between user emotions andsystem strategies using a heterogeneous graph. Additionally, we make use of theEmotion Recognition in Conversations (ERC) task and design a flexiblemixed-emotion module to capture fine-grained emotional states of the user.Experimental results on two ESC datasets show EmoDynamiX outperforms previousstate-of-the-art methods with a significant margin.</description><author>Chenwei Wan, Matthieu Labeau, Chloé Clavel</author><pubDate>Fri, 16 Aug 2024 14:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08782v1</guid></item><item><title>Learning Diffusion Priors from Observations by Expectation Maximization</title><link>http://arxiv.org/abs/2405.13712v3</link><description>Diffusion models recently proved to be remarkable priors for Bayesian inverseproblems. However, training these models typically requires access to largeamounts of clean data, which could prove difficult in some settings. In thiswork, we present a novel method based on the expectation-maximization algorithmfor training diffusion models from incomplete and noisy observations only.Unlike previous works, our method leads to proper diffusion models, which iscrucial for downstream tasks. As part of our method, we propose and motivate animproved posterior sampling scheme for unconditional diffusion models. Wepresent empirical evidence supporting the effectiveness of our method.</description><author>François Rozet, Gérôme Andry, François Lanusse, Gilles Louppe</author><pubDate>Fri, 16 Aug 2024 14:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13712v3</guid></item><item><title>Multi-task Image Restoration Guided By Robust DINO Features</title><link>http://arxiv.org/abs/2312.01677v3</link><description>Multi-task image restoration has gained significant interest due to itsinherent versatility and efficiency compared to its single-task counterpart.However, performance decline is observed with an increase in the number oftasks, primarily attributed to the restoration model's challenge in handlingdifferent tasks with distinct natures at the same time. Thus, a perspectiveemerged aiming to explore the degradation-insensitive semantic commonalitiesamong different degradation tasks. In this paper, we observe that the featuresof DINOv2 can effectively model semantic information and are independent ofdegradation factors. Motivated by this observation, we propose\mbox{\textbf{DINO-IR}}, a multi-task image restoration approach leveragingrobust features extracted from DINOv2 to solve multi-task image restorationsimultaneously. We first propose a pixel-semantic fusion (PSF) module todynamically fuse DINOV2's shallow features containing pixel-level informationand deep features containing degradation-independent semantic information. Toguide the restoration model with the features of DINOv2, we develop aDINO-Restore adaption and fusion module to adjust the channel of fused featuresfrom PSF and then integrate them with the features from the restoration model.By formulating these modules into a unified deep model, we propose a DINOperception contrastive loss to constrain the model training. Extensiveexperimental results demonstrate that our DINO-IR performs favorably againstexisting multi-task image restoration approaches in various tasks by a largemargin. The source codes and trained models will be made available.</description><author>Xin Lin, Jingtong Yue, Kelvin C. K. Chan, Lu Qi, Chao Ren, Jinshan Pan, Ming-Hsuan Yang</author><pubDate>Fri, 16 Aug 2024 14:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01677v3</guid></item><item><title>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</title><link>http://arxiv.org/abs/2408.08781v1</link><description>LLMs-as-a-judge is a recently popularized method which replaces humanjudgements in task evaluation (Zheng et al. 2024) with automatic evaluationusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from HumanFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to havestrong alignment with human preferences when prompted for a quality judgement,such as the coherence of a text. While this seems beneficial, it is not clearwhether the assessments by an LLM-as-a-judge constitute only an evaluationbased on the instructions in the prompts, or reflect its preference forhigh-quality data similar to its fine-tune data. To investigate how muchinfluence prompting the LLMs-as-a-judge has on the alignment of AI judgementsto human judgements, we analyze prompts with increasing levels of instructionsabout the target quality of an evaluation, for several LLMs-as-a-judge.Further, we compare to a prompt-free method using model perplexity as a qualitymeasure instead. We aggregate a taxonomy of quality criteria commonly usedacross state-of-the-art evaluations with LLMs and provide this as a rigorousbenchmark of models as judges. Overall, we show that the LLMs-as-a-judgebenefit only little from highly detailed instructions in prompts and thatperplexity can sometimes align better with human judgements than prompting,especially on textual quality.</description><author>Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana Negreanu, Chris Parnin, Advait Sarkar</author><pubDate>Fri, 16 Aug 2024 14:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08781v1</guid></item><item><title>Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions</title><link>http://arxiv.org/abs/2408.08780v1</link><description>With the help of in-context learning (ICL), large language models (LLMs) haveachieved impressive performance across various tasks. However, the function ofdescriptive instructions during ICL remains under-explored. In this work, wepropose an ensemble prompt framework to describe the selection criteria ofmultiple in-context examples, and preliminary experiments on machinetranslation (MT) across six translation directions confirm that this frameworkboosts ICL perfromance. But to our surprise, LLMs might not necessarily carewhat the descriptions actually say, and the performance gain is primarilycaused by the ensemble format, since the framework could lead to improvementeven with random descriptive nouns. We further apply this new ensemble prompton a range of commonsense, math, logical reasoning and hallucination tasks withthree LLMs and achieve promising results, suggesting again that designing aproper prompt format would be much more effective and efficient than payingeffort into specific descriptions. Our code will be publicly available oncethis paper is published.</description><author>Chenming Tang, Zhixiang Wang, Yunfang Wu</author><pubDate>Fri, 16 Aug 2024 14:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08780v1</guid></item><item><title>Automated Contrastive Learning Strategy Search for Time Series</title><link>http://arxiv.org/abs/2403.12641v2</link><description>In recent years, Contrastive Learning (CL) has become a predominantrepresentation learning paradigm for time series. Most existing methodsmanually build specific CL Strategies (CLS) by human heuristics for certaindatasets and tasks. However, manually developing CLS usually requires excessiveprior knowledge about the data, and massive experiments to determine thedetailed CL configurations. In this paper, we present an Automated MachineLearning (AutoML) practice at Microsoft, which automatically learns CLS fortime series datasets and tasks, namely Automated Contrastive Learning (AutoCL).We first construct a principled search space of size over $3\times10^{12}$,covering data augmentation, embedding transformation, contrastive pairconstruction, and contrastive losses. Further, we introduce an efficientreinforcement learning algorithm, which optimizes CLS from the performance onthe validation tasks, to obtain effective CLS within the space. Experimentalresults on various real-world datasets demonstrate that AutoCL couldautomatically find the suitable CLS for the given dataset and task. From thecandidate CLS found by AutoCL on several public datasets/tasks, we compose atransferable Generally Good Strategy (GGS), which has a strong performance forother datasets. We also provide empirical analysis as a guide for the futuredesign of CLS.</description><author>Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, Kan Ren</author><pubDate>Fri, 16 Aug 2024 14:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12641v2</guid></item><item><title>Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation</title><link>http://arxiv.org/abs/2403.11960v2</link><description>Spatiotemporal time series are usually collected via monitoring sensorsplaced at different locations, which usually contain missing values due tovarious mechanical failures. Imputing the missing values is crucial foranalyzing time series. When recovering a specific data point, most existingmethods consider all the information relevant to that point regardless of thecause-and-effect relationship. During data collection, it is inevitable thatsome unknown confounders are included, e.g., background noise in time seriesand non-causal shortcut edges in the constructed sensor network. Theseconfounders could open backdoor paths and establish non-causal correlationsbetween the input and output. Over-exploiting these non-causal correlationscould cause overfitting. In this paper, we first revisit spatiotemporal timeseries imputation from a causal perspective and show how to block theconfounders via the frontdoor adjustment. Based on the results of frontdooradjustment, we introduce a novel Causality-Aware Spatiotemporal Graph NeuralNetwork (Casper), which contains a novel Prompt Based Decoder (PBD) and aSpatiotemporal Causal Attention (SCA). PBD could reduce the impact ofconfounders and SCA could discover the sparse causal relationships amongembeddings. Theoretical analysis reveals that SCA discovers causalrelationships based on the values of gradients. We evaluate Casper on threereal-world datasets, and the experimental results show that Casper couldoutperform the baselines and could effectively discover causal relationships.</description><author>Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang</author><pubDate>Fri, 16 Aug 2024 14:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11960v2</guid></item><item><title>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</title><link>http://arxiv.org/abs/2408.01826v2</link><description>Speech-driven talking head generation is an important but challenging taskfor many downstream applications such as augmented reality. Existing methodshave achieved remarkable performance by utilizing autoregressive models ordiffusion models. However, most still suffer from modality inconsistencies,specifically the misalignment between audio and mesh modalities, which causesinconsistencies in motion diversity and lip-sync accuracy. To address thisissue, this paper introduces GLDiTalker, a novel speech-driven 3D facialanimation model that employs a Graph Latent Diffusion Transformer. The coreidea behind GLDiTalker is that the audio-mesh modality misalignment can beresolved by diffusing the signal in a latent quantilized spatial-temporalspace. To achieve this, GLDiTalker builds upon a quantilized space-timediffusion training pipeline, which consists of a Graph Enhanced QuantilizedSpace Learning Stage and a Space-Time Powered Latent Diffusion Stage. The firststage ensures lip-sync accuracy, while the second stage enhances motiondiversity. Together, these stages enable GLDiTalker to generate temporally andspatially stable, realistic models. Extensive evaluations on several widelyused benchmarks demonstrate that our method achieves superior performancecompared to existing methods.</description><author>Yihong Lin, Zhaoxin Fan, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Xianjia Wu, Songju Lei, Huang Xu</author><pubDate>Fri, 16 Aug 2024 14:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01826v2</guid></item><item><title>DAC: Decomposed Automation Correction for Text-to-SQL</title><link>http://arxiv.org/abs/2408.08779v1</link><description>Text-to-SQL is an important task that helps people obtain information fromdatabases by automatically generating SQL queries. Considering the brilliantperformance, approaches based on Large Language Models (LLMs) become themainstream for text-to-SQL. Among these approaches, automated correction is aneffective approach that further enhances performance by correcting the mistakesin the generated results. The existing correction methods require LLMs todirectly correct with generated SQL, while previous research shows that LLMs donot know how to detect mistakes, leading to poor performance. Therefore, inthis paper, we propose to employ the decomposed correction to enhancetext-to-SQL performance. We first demonstrate that decomposed correctionoutperforms direct correction since detecting and fixing mistakes with theresults of the decomposed sub-tasks is easier than with SQL. Based on thisanalysis, we introduce Decomposed Automation Correction (DAC), which correctsSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DACfirst generates the entity and skeleton corresponding to the question and thencompares the differences between the initial SQL and the generated entities andskeleton as feedback for correction. Experimental results show that our methodimproves performance by $3.7\%$ on average of Spider, Bird, and KaggleDBQAcompared with the baseline method, demonstrating the effectiveness of DAC.</description><author>Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che</author><pubDate>Fri, 16 Aug 2024 14:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08779v1</guid></item><item><title>NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance</title><link>http://arxiv.org/abs/2408.08776v1</link><description>Artificial neural networks have been shown to be state-of-the-art machinelearning models in a wide variety of applications, including natural languageprocessing and image recognition. However, building a performant neural networkis a laborious task and requires substantial computing power. NeuralArchitecture Search (NAS) addresses this issue by an automatic selection of theoptimal network from a set of potential candidates. While many NAS methodsstill require training of (some) neural networks, zero-cost proxies promise toidentify the optimal network without training. In this work, we propose thezero-cost proxy Network Expressivity by Activation Rank (NEAR). It is based onthe effective rank of the pre- and post-activation matrix, i.e., the values ofa neural network layer before and after applying its activation function. Wedemonstrate the cutting-edge correlation between this network score and themodel accuracy on NAS-Bench-101 and NATS-Bench-SSS/TSS. In addition, we presenta simple approach to estimate the optimal layer sizes in multi-layerperceptrons. Furthermore, we show that this score can be utilized to selecthyperparameters such as the activation function and the neural network weightinitialization scheme.</description><author>Raphael T. Husistein, Markus Reiher, Marco Eckhoff</author><pubDate>Fri, 16 Aug 2024 14:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08776v1</guid></item><item><title>Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction</title><link>http://arxiv.org/abs/2404.06128v2</link><description>Within colorectal cancer diagnostics, conventional colonoscopy techniquesface critical limitations, including a limited field of view and a lack ofdepth information, which can impede the detection of precancerous lesions.Current methods struggle to provide comprehensive and accurate 3Dreconstructions of the colonic surface which can help minimize the missingregions and reinspection for pre-cancerous polyps. Addressing this, weintroduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting(3D GS) combined with a Recurrent Neural Network-based SimultaneousLocalization and Mapping (RNNSLAM) system. By introducing geometric and depthregularization into the 3D GS framework, our approach ensures more accuratealignment of Gaussians with the colon surface, resulting in smoother 3Dreconstructions with novel viewing of detailed textures and structures.Evaluations across three diverse datasets show that Gaussian Pancakes enhancesnovel view synthesis quality, surpassing current leading methods with a 18%boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X fasterrendering and more than 10X shorter training times, making it a practical toolfor real-time applications. Hence, this holds promise for achieving clinicaltranslation for better detection and diagnosis of colorectal cancer.</description><author>Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano</author><pubDate>Fri, 16 Aug 2024 14:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06128v2</guid></item><item><title>Speckle Noise Analysis for Synthetic Aperture Radar (SAR) Space Data</title><link>http://arxiv.org/abs/2408.08774v1</link><description>This research tackles the challenge of speckle noise in Synthetic ApertureRadar (SAR) space data, a prevalent issue that hampers the clarity and utilityof SAR images. The study presents a comparative analysis of six distinctspeckle noise reduction techniques: Lee Filtering, Frost Filtering, KuanFiltering, Gaussian Filtering, Median Filtering, and Bilateral Filtering. Thesemethods, selected for their unique approaches to noise reduction and imagepreservation, were applied to SAR datasets sourced from the Alaska SatelliteFacility (ASF). The performance of each technique was evaluated using acomprehensive set of metrics, including Peak Signal-to-Noise Ratio (PSNR), MeanSquared Error (MSE), Structural Similarity Index (SSIM), Equivalent Number ofLooks (ENL), and Speckle Suppression Index (SSI). The study concludes that boththe Lee and Kuan Filters are effective, with the choice of filter depending onthe specific application requirements for image quality and noise suppression.This work provides valuable insights into optimizing SAR image processing, withsignificant implications for remote sensing, environmental monitoring, andgeological surveying.</description><author>Sanjjushri Varshini R, Rohith Mahadevan, Bagiya Lakshmi S, Mathivanan Periasamy, Raja CSP Raman, Lokesh M</author><pubDate>Fri, 16 Aug 2024 14:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08774v1</guid></item><item><title>Pessimistic Iterative Planning for Robust POMDPs</title><link>http://arxiv.org/abs/2408.08770v1</link><description>Robust partially observable Markov decision processes (robust POMDPs) extendclassical POMDPs to handle additional uncertainty on the transition andobservation probabilities via so-called uncertainty sets. Policies for robustPOMDPs must not only be memory-based to account for partial observability butalso robust against model uncertainty to account for the worst-case instancesfrom the uncertainty sets. We propose the pessimistic iterative planning (PIP)framework, which finds robust memory-based policies for robust POMDPs. PIPalternates between two main steps: (1) selecting an adversarial (non-robust)POMDP via worst-case probability instances from the uncertainty sets; and (2)computing a finite-state controller (FSC) for this adversarial POMDP. Weevaluate the performance of this FSC on the original robust POMDP and use thisevaluation in step (1) to select the next adversarial POMDP. Within PIP, wepropose the rFSCNet algorithm. In each iteration, rFSCNet finds an FSC througha recurrent neural network trained using supervision policies optimized for theadversarial POMDP. The empirical evaluation in four benchmark environmentsshowcases improved robustness against a baseline method in an ablation studyand competitive performance compared to a state-of-the-art robust POMDP solver.</description><author>Maris F. L. Galesloot, Marnix Suilen, Thiago D. Simão, Steven Carr, Matthijs T. J. Spaan, Ufuk Topcu, Nils Jansen</author><pubDate>Fri, 16 Aug 2024 14:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08770v1</guid></item><item><title>Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused</title><link>http://arxiv.org/abs/2408.08769v1</link><description>Large Language Models (LLMs) have demonstrated exceptional performance acrossvarious natural language processing tasks, yet they occasionally tend to yieldcontent that factually inaccurate or discordant with the expected output, aphenomenon empirically referred to as "hallucination". To tackle this issue,recent works have investigated contrastive decoding between the original modeland an amateur model with induced hallucination, which has shown promisingresults. Nonetheless, this method may undermine the output distribution of theoriginal LLM caused by its coarse contrast and simplistic subtractionoperation, potentially leading to errors in certain cases. In this paper, weintroduce a novel contrastive decoding framework termed LOL (LOwer LayerMatters). Our approach involves concatenating the contrastive decoding of boththe final and lower layers between the original model and the amateur model,thereby achieving multi-layer fusion to aid in the mitigation of hallucination.Additionally, we incorporate a truthfulness refocused module that leveragescontextual guidance to enhance factual encoding, further capturing truthfulnessduring contrastive decoding. Extensive experiments conducted on two publiclyavailable datasets illustrate that our proposed LOL framework can substantiallyalleviate hallucination while surpassing existing baselines in most cases.Compared with the best baseline, we improve by average 4.5 points on allmetrics of TruthfulQA. The source code is coming soon.</description><author>Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Ruifeng Xu, Min Yang, Chengming Li</author><pubDate>Fri, 16 Aug 2024 14:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08769v1</guid></item><item><title>VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction</title><link>http://arxiv.org/abs/2408.08766v1</link><description>Implicit surfaces via neural radiance fields (NeRF) have shown surprisingaccuracy in surface reconstruction. Despite their success in reconstructingrichly textured surfaces, existing methods struggle with planar regions withweak textures, which account for the majority of indoor scenes. In this paper,we address indoor dense surface reconstruction by revisiting key aspects ofNeRF in order to use the recently proposed Vector Field (VF) as the implicitrepresentation. VF is defined by the unit vector directed to the nearestsurface point. It therefore flips direction at the surface and equals to theexplicit surface normals. Except for this flip, VF remains constant alongplanar surfaces and provides a strong inductive bias in representing planarsurfaces. Concretely, we develop a novel density-VF relationship and a trainingscheme that allows us to learn VF via volume rendering By doing this, VF-NeRFcan model large planar surfaces and sharp corners accurately. We show that,when depth cues are available, our method further improves and achievesstate-of-the-art results in reconstructing indoor scenes and rendering novelviews. We extensively evaluate VF-NeRF on indoor datasets and run ablations ofits components.</description><author>Albert Gassol Puigjaner, Edoardo Mello Rella, Erik Sandström, Ajad Chhatkuli, Luc Van Gool</author><pubDate>Fri, 16 Aug 2024 14:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08766v1</guid></item><item><title>SYMPOL: Symbolic Tree-Based On-Policy Reinforcement Learning</title><link>http://arxiv.org/abs/2408.08761v1</link><description>Reinforcement learning (RL) has seen significant success across variousdomains, but its adoption is often limited by the black-box nature of neuralnetwork policies, making them difficult to interpret. In contrast, symbolicpolicies allow representing decision-making strategies in a compact andinterpretable way. However, learning symbolic policies directly withinon-policy methods remains challenging. In this paper, we introduce SYMPOL, anovel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-basedmodel integrated with a policy gradient method, enabling the agent to learn andadapt its actions while maintaining a high level of interpretability. Weevaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiorityover alternative tree-based RL approaches in terms of performance andinterpretability. To the best of our knowledge, this is the first method, thatallows a gradient-based end-to-end learning of interpretable, axis-aligneddecision trees on-policy. Therefore, SYMPOL can become the foundation for a newclass of interpretable RL based on decision trees. Our implementation isavailable under: https://github.com/s-marton/SYMPOL</description><author>Sascha Marton, Tim Grams, Florian Vogt, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt</author><pubDate>Fri, 16 Aug 2024 14:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08761v1</guid></item><item><title>SE-SGformer: A Self-Explainable Signed Graph Transformer for Link Sign Prediction</title><link>http://arxiv.org/abs/2408.08754v1</link><description>Signed Graph Neural Networks (SGNNs) have been shown to be effective inanalyzing complex patterns in real-world situations where positive and negativelinks coexist. However, SGNN models suffer from poor explainability, whichlimit their adoptions in critical scenarios that require understanding therationale behind predictions. To the best of our knowledge, there is currentlyno research work on the explainability of the SGNN models. Our goal is toaddress the explainability of decision-making for the downstream task of linksign prediction specific to signed graph neural networks. Since post-hocexplanations are not derived directly from the models, they may be biased andmisrepresent the true explanations. Therefore, in this paper we introduce aSelf-Explainable Signed Graph transformer (SE-SGformer) framework, which cannot only outputs explainable information while ensuring high predictionaccuracy. Specifically, We propose a new Transformer architecture for signedgraphs and theoretically demonstrate that using positional encoding based onsigned random walks has greater expressive power than current SGNN methods andother positional encoding graph Transformer-based approaches. We constructs anovel explainable decision process by discovering the $K$-nearest (farthest)positive (negative) neighbors of a node to replace the neural network-baseddecoder for predicting edge signs. These $K$ positive (negative) neighborsrepresent crucial information about the formation of positive (negative) edgesbetween nodes and thus can serve as important explanatory information in thedecision-making process. We conducted experiments on several real-worlddatasets to validate the effectiveness of SE-SGformer, which outperforms thestate-of-the-art methods by improving 2.2\% prediction accuracy and 73.1\%explainablity accuracy in the best-case scenario.</description><author>Lu Li, Jiale Liu, Xingyu Ji, Maojun Wang, Zeyu Zhang</author><pubDate>Fri, 16 Aug 2024 13:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08754v1</guid></item><item><title>PCP-MAE: Learning to Predict Centers for Point Masked Autoencoders</title><link>http://arxiv.org/abs/2408.08753v1</link><description>Masked autoencoder has been widely explored in point cloud self-supervisedlearning, whereby the point cloud is generally divided into visible and maskedparts. These methods typically include an encoder accepting visible patches(normalized) and corresponding patch centers (position) as input, with thedecoder accepting the output of the encoder and the centers (position) of themasked parts to reconstruct each point in the masked patches. Then, thepre-trained encoders are used for downstream tasks. In this paper, we show amotivating empirical result that when directly feeding the centers of maskedpatches to the decoder without information from the encoder, it stillreconstructs well. In other words, the centers of patches are important and thereconstruction objective does not necessarily rely on representations of theencoder, thus preventing the encoder from learning semantic representations.Based on this key observation, we propose a simple yet effective method, i.e.,learning to Predict Centers for Point Masked AutoEncoders (PCP-MAE) whichguides the model to learn to predict the significant centers and use thepredicted centers to replace the directly provided centers. Specifically, wepropose a Predicting Center Module (PCM) that shares parameters with theoriginal encoder with extra cross-attention to predict centers. Our method isof high pre-training efficiency compared to other alternatives and achievesgreat improvement over Point-MAE, particularly outperforming it by 5.50%,6.03%, and 5.17% on three variants of ScanObjectNN. The code will be madepublicly available.</description><author>Xiangdong Zhang, Shaofeng Zhang, Junchi Yan</author><pubDate>Fri, 16 Aug 2024 13:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08753v1</guid></item><item><title>Multi-marginal Schrödinger Bridges with Iterative Reference Refinement</title><link>http://arxiv.org/abs/2408.06277v2</link><description>Practitioners frequently aim to infer an unobserved population trajectoryusing sample snapshots at multiple time points. For instance, in single-cellsequencing, scientists would like to learn how gene expression evolves overtime. But sequencing any cell destroys that cell. So we cannot access anycell's full trajectory, but we can access snapshot samples from many cells.Stochastic differential equations are commonly used to analyze systems withfull individual-trajectory access; since here we have only sample snapshots,these methods are inapplicable. The deep learning community has recentlyexplored using Schr\"odinger bridges (SBs) and their extensions to estimatethese dynamics. However, these methods either (1) interpolate between just twotime points or (2) require a single fixed reference dynamic within the SB,which is often just set to be Brownian motion. But learning piecewise fromadjacent time points can fail to capture long-term dependencies. Andpractitioners are typically able to specify a model class for the referencedynamic but not the exact values of the parameters within it. So we propose anew method that (1) learns the unobserved trajectories from sample snapshotsacross multiple time points and (2) requires specification only of a class ofreference dynamics, not a single fixed one. In particular, we suggest aniterative projection method inspired by Schr\"odinger bridges; we alternatebetween learning a piecewise SB on the unobserved trajectories and using thelearned SB to refine our best guess for the dynamics within the referenceclass. We demonstrate the advantages of our method via a well-known simulatedparametric model from ecology, simulated and real data from systems biology,and real motion-capture data.</description><author>Yunyi Shen, Renato Berlinghieri, Tamara Broderick</author><pubDate>Fri, 16 Aug 2024 13:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06277v2</guid></item><item><title>Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion</title><link>http://arxiv.org/abs/2408.08751v1</link><description>This paper examines three major generative modelling frameworks: VariationalAutoencoders (VAEs), Generative Adversarial Networks (GANs), and StableDiffusion models. VAEs are effective at learning latent representations butfrequently yield blurry results. GANs can generate realistic images but faceissues such as mode collapse. Stable Diffusion models, while producinghigh-quality images with strong semantic coherence, are demanding in terms ofcomputational resources. Additionally, the paper explores how incorporatingGrounding DINO and Grounded SAM with Stable Diffusion improves image accuracyby utilising sophisticated segmentation and inpainting techniques. The analysisguides on selecting suitable models for various applications and highlightsareas for further research.</description><author>Sanchayan Vivekananthan</author><pubDate>Fri, 16 Aug 2024 13:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08751v1</guid></item><item><title>ML Study of MaliciousTransactions in Ethereum</title><link>http://arxiv.org/abs/2408.08749v1</link><description>Smart contracts are a major tool in Ethereum transactions. Therefore hackerscan exploit them by adding code vulnerabilities to their sources and usingthese vulnerabilities for performing malicious transactions. This paperpresents two successful approaches for detecting malicious contracts: one usesopcode and relies on GPT2 and the other uses the Solidity source and a LORAfine-tuned CodeLlama. Finally, we present an XGBOOST model that combines gasproperties and Hexa-decimal signatures for detecting malicious transactions.This approach relies on early assumptions that maliciousness is manifested bythe uncommon usage of the contracts' functions and the effort to pursue thetransaction.</description><author>Natan Katz</author><pubDate>Fri, 16 Aug 2024 13:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08749v1</guid></item><item><title>MicroSSIM: Improved Structural Similarity for Comparing Microscopy Data</title><link>http://arxiv.org/abs/2408.08747v1</link><description>Microscopy is routinely used to image biological structures of interest. Dueto imaging constraints, acquired images are typically low-SNR and containnoise. Over the last few years, regression-based tasks like unsuperviseddenoising and splitting have found utility in working with such noisymicrographs. For evaluation, Structural Similarity (SSIM) is one of the mostpopular measures used in the field. For such tasks, the best evaluation wouldbe when both low-SNR noisy images and corresponding high-SNR clean images areobtained directly from a microscope. However, due to the following threepeculiar properties of the microscopy data, we observe that SSIM is not wellsuited to this data regime: (a) high-SNR micrographs have higher intensitypixels as compared to low SNR micrographs, (b) high-SNR micrographs have higherintensity pixels than found in natural images, images for which SSIM wasdeveloped, and (c) a digitally configurable offset is added by the detectorpresent inside the microscope. We show that SSIM components behave unexpectedlywhen the prediction generated from low-SNR input is compared with thecorresponding high-SNR data. We explain this behavior by introducing thephenomenon of saturation, where the value of SSIM components becomes lesssensitive to (dis)similarity between the images. We introduce microSSIM, avariant of SSIM, which overcomes the above-discussed issues. We justify thesoundness and utility of microSSIM using theoretical and empirical argumentsand show the utility of microSSIM on two tasks: unsupervised denoising andjoint image splitting with unsupervised denoising. Since our formulation can beapplied to a broad family of SSIM-based measures, we also introduce MicroMS3IM,a microscopy-specific variation of MS-SSIM. The source code and python packageis available at https://github.com/juglab/MicroSSIM.</description><author>Ashesh Ashesh, Joran Deschamps, Florian Jug</author><pubDate>Fri, 16 Aug 2024 13:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08747v1</guid></item><item><title>A lifted Bregman strategy for training unfolded proximal neural network Gaussian denoisers</title><link>http://arxiv.org/abs/2408.08742v1</link><description>Unfolded proximal neural networks (PNNs) form a family of methods thatcombines deep learning and proximal optimization approaches. They consist indesigning a neural network for a specific task by unrolling a proximalalgorithm for a fixed number of iterations, where linearities can be learnedfrom prior training procedure. PNNs have shown to be more robust thantraditional deep learning approaches while reaching at least as goodperformances, in particular in computational imaging. However, training PNNsstill depends on the efficiency of available training algorithms. In this work,we propose a lifted training formulation based on Bregman distances forunfolded PNNs. Leveraging the deterministic mini-batch block-coordinateforward-backward method, we design a bespoke computational strategy beyondtraditional back-propagation methods for solving the resulting learning problemefficiently. We assess the behaviour of the proposed training approach for PNNsthrough numerical simulations on image denoising, considering a denoising PNNwhose structure is based on dual proximal-gradient iterations.</description><author>Xiaoyu Wang, Martin Benning, Audrey Repetti</author><pubDate>Fri, 16 Aug 2024 13:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08742v1</guid></item><item><title>ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks at Scale</title><link>http://arxiv.org/abs/2408.08739v1</link><description>ASVspoof 5 is the fifth edition in a series of challenges that promote thestudy of speech spoofing and deepfake attacks, and the design of detectionsolutions. Compared to previous challenges, the ASVspoof 5 database is builtfrom crowdsourced data collected from a vastly greater number of speakers indiverse acoustic conditions. Attacks, also crowdsourced, are generated andtested using surrogate detection models, while adversarial attacks areincorporated for the first time. New metrics support the evaluation ofspoofing-robust automatic speaker verification (SASV) as well as stand-alonedetection solutions, i.e., countermeasures without ASV. We describe the twochallenge tracks, the new database, the evaluation metrics, baselines, and theevaluation platform, and present a summary of the results. Attackssignificantly compromise the baseline systems, while submissions bringsubstantial improvements.</description><author>Xin Wang, Hector Delgado, Hemlata Tak, Jee-weon Jung, Hye-jin Shim, Massimiliano Todisco, Ivan Kukanov, Xuechen Liu, Md Sahidullah, Tomi Kinnunen, Nicholas Evans, Kong Aik Lee, Junichi Yamagishi</author><pubDate>Fri, 16 Aug 2024 13:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08739v1</guid></item><item><title>Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image Super-Resolution</title><link>http://arxiv.org/abs/2408.08736v1</link><description>Arbitrary-scale super-resolution (ASSR) aims to learn a single model forimage super-resolution at arbitrary magnifying scales. Existing ASSR networkstypically comprise an off-the-shelf scale-agnostic feature extractor and anarbitrary scale upsampler. These feature extractors often use fixed networkarchitectures to address different ASSR inference tasks, each of which ischaracterized by an input image and an upsampling scale. However, thisoverlooks the difficulty variance of super-resolution on different inferencescenarios, where simple images or small SR scales could be resolved with lesscomputational effort than difficult images or large SR scales. To tackle thisdifficulty variability, in this paper, we propose a Task-Aware DynamicTransformer (TADT) as an input-adaptive feature extractor for efficient imageASSR. Our TADT consists of a multi-scale feature extraction backbone built upongroups of Multi-Scale Transformer Blocks (MSTBs) and a Task-Aware RoutingController (TARC). The TARC predicts the inference paths within featureextraction backbone, specifically selecting MSTBs based on the input images andSR scales. The prediction of inference path is guided by a new loss function totrade-off the SR accuracy and efficiency. Experiments demonstrate that, whenworking with three popular arbitrary-scale upsamplers, our TADT achievesstate-of-the-art ASSR performance when compared with mainstream featureextractors, but with relatively fewer computational costs. The code will bepublicly released.</description><author>Tianyi Xu, Yiji Zhou, Xiaotao Hu, Kai Zhang, Anran Zhang, Xingye Qiu, Jun Xu</author><pubDate>Fri, 16 Aug 2024 13:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08736v1</guid></item><item><title>Symbolic Parameter Learning in Probabilistic Answer Set Programming</title><link>http://arxiv.org/abs/2408.08732v1</link><description>Parameter learning is a crucial task in the field of Statistical RelationalArtificial Intelligence: given a probabilistic logic program and a set ofobservations in the form of interpretations, the goal is to learn theprobabilities of the facts in the program such that the probabilities of theinterpretations are maximized. In this paper, we propose two algorithms tosolve such a task within the formalism of Probabilistic Answer Set Programming,both based on the extraction of symbolic equations representing theprobabilities of the interpretations. The first solves the task using anoff-the-shelf constrained optimization solver while the second is based on animplementation of the Expectation Maximization algorithm. Empirical resultsshow that our proposals often outperform existing approaches based on projectedanswer set enumeration in terms of quality of the solution and in terms ofexecution time. The paper has been accepted at the ICLP2024 conference and isunder consideration in Theory and Practice of Logic Programming (TPLP).</description><author>Damiano Azzolini, Elisabetta Gentili, Fabrizio Riguzzi</author><pubDate>Fri, 16 Aug 2024 13:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08732v1</guid></item><item><title>ConcateNet: Dialogue Separation Using Local And Global Feature Concatenation</title><link>http://arxiv.org/abs/2408.08729v1</link><description>Dialogue separation involves isolating a dialogue signal from a mixture, suchas a movie or a TV program. This can be a necessary step to enable dialogueenhancement for broadcast-related applications. In this paper, ConcateNet fordialogue separation is proposed, which is based on a novel approach forprocessing local and global features aimed at better generalization forout-of-domain signals. ConcateNet is trained using a noise reduction-focused,publicly available dataset and evaluated using three datasets: two noisereduction-focused datasets (in-domain), which show competitive performance forConcateNet, and a broadcast-focused dataset (out-of-domain), which verifies thebetter generalization performance for the proposed architecture compared toconsidered state-of-the-art noise-reduction methods.</description><author>Mhd Modar Halimeh, Matteo Torcoli, Emanuël Habets</author><pubDate>Fri, 16 Aug 2024 13:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08729v1</guid></item><item><title>ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target Language</title><link>http://arxiv.org/abs/2408.08724v1</link><description>Although large language models(LLMs) show amazing capabilities, among variousexciting applications discovered for LLMs fall short in other low-resourcelanguages. Besides, most existing methods depend on large-scale dialoguecorpora and thus building systems for dialogue generation in a zero-shotscenario remains a considerable challenge. To address this challenge, wepropose a novel end-to-end zero-shot dialogue generation model ChatZero basedon cross-lingual code-switching method. First, we construct code-switchinglanguage and pseudo-target language with placeholders. Then for cross-lingualsemantic transfer, we employ unsupervised contrastive learning to minimize thesemantics gap of the source language, code-switching language, andpseudo-target language that are mutually positive examples in the highdimensional semantic space. Experiments on the multilingual DailyDialog andDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\% of theoriginal performance under the zero-shot case compared to supervised learning,and achieve state-of-the-art performance compared with other baselines.</description><author>Yongkang Liu, Feng Shi, Daling Wang, Yifei Zhang, Hinrich Schütze</author><pubDate>Fri, 16 Aug 2024 13:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08724v1</guid></item><item><title>Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS</title><link>http://arxiv.org/abs/2408.08723v1</link><description>Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processedcamera poses--referred to as SfM-free methods--is crucial for promoting rapidresponse capabilities and enhancing robustness against variable operatingconditions. Recent SfM-free methods have integrated pose optimization,designing end-to-end frameworks for joint camera pose estimation and NVS.However, most existing works rely on per-pixel image loss functions, such as L2loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,which, under the constraints of per-pixel image loss functions, results inexcessive gradients, causing unstable optimization and poor convergence forNVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussiansplatting for NVS. We use correspondences between the target and the renderedresult to achieve better pixel alignment, facilitating the optimization ofrelative poses between frames. We then apply the learned poses to optimize theentire scene. Each 2D screen-space pixel is associated with its corresponding3D Gaussians through approximated surface rendering to facilitate gradient backpropagation. Experimental results underline the superior performance and timeefficiency of the proposed approach compared to the state-of-the-art baselines.</description><author>Wei Sun, Xiaosong Zhang, Fang Wan, Yanzhao Zhou, Yuan Li, Qixiang Ye, Jianbin Jiao</author><pubDate>Fri, 16 Aug 2024 13:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08723v1</guid></item><item><title>Kernel Density Estimators in Large Dimensions</title><link>http://arxiv.org/abs/2408.05807v2</link><description>This paper studies Kernel density estimation for a high-dimensionaldistribution $\rho(x)$. Traditional approaches have focused on the limit oflarge number of data points $n$ and fixed dimension $d$. We analyze instead theregime where both the number $n$ of data points $y_i$ and their dimensionality$d$ grow with a fixed ratio $\alpha=(\log n)/d$. Our study reveals threedistinct statistical regimes for the kernel-based estimate of the density $\hat\rho_h^{\mathcal {D}}(x)=\frac{1}{n h^d}\sum_{i=1}^nK\left(\frac{x-y_i}{h}\right)$, depending on the bandwidth $h$: a classicalregime for large bandwidth where the Central Limit Theorem (CLT) holds, whichis akin to the one found in traditional approaches. Below a certain value ofthe bandwidth, $h_{CLT}(\alpha)$, we find that the CLT breaks down. Thestatistics of $\hat \rho_h^{\mathcal {D}}(x)$ for a fixed $x$ drawn from$\rho(x)$ is given by a heavy-tailed distribution (an alpha-stabledistribution). In particular below a value $h_G(\alpha)$, we find that $\hat\rho_h^{\mathcal {D}}(x)$ is governed by extreme value statistics: only a fewpoints in the database matter and give the dominant contribution to the densityestimator. We provide a detailed analysis for high-dimensional multivariateGaussian data. We show that the optimal bandwidth threshold based onKullback-Leibler divergence lies in the new statistical regime identified inthis paper. Our findings reveal limitations of classical approaches, show therelevance of these new statistical regimes, and offer new insights for Kerneldensity estimation in high-dimensional settings.</description><author>Giulio Biroli, Marc Mézard</author><pubDate>Fri, 16 Aug 2024 13:03:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05807v2</guid></item><item><title>Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction</title><link>http://arxiv.org/abs/2408.08713v1</link><description>Modeling feature interactions is crucial for click-through rate (CTR)prediction, particularly when it comes to high-order explicit interactions.Traditional methods struggle with this task because they often predefine amaximum interaction order, which relies heavily on prior knowledge and canlimit the model's effectiveness. Additionally, modeling high-order interactionstypically leads to increased computational costs. Therefore, the challenge liesin adaptively modeling high-order feature interactions while maintainingefficiency. To address this issue, we introduce Kolmogorov-Arnold RepresentedSparse Efficient Interaction Network (KarSein), designed to optimize bothpredictive accuracy and computational efficiency. We firstly identifylimitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR andthen introduce KarSein to overcome these issues. It features a novelarchitecture that reduces the computational costs of KAN and supports embeddingvectors as feature inputs. Additionally, KarSein employs guided symbolicregression to address the challenge of KAN in spontaneously learningmultiplicative relationships. Extensive experiments demonstrate KarSein'ssuperior performance, achieving significant predictive accuracy with minimalcomputational overhead. Furthermore, KarSein maintains strong globalexplainability while enabling the removal of redundant features, resulting in asparse network structure. These advantages also position KarSein as a promisingmethod for efficient inference.</description><author>Yunxiao Shi, Wujiang Wu, Mingyu Jin, Haimin Zhang, Qiang Wu, Yongfeng Zhang, Min Xu</author><pubDate>Fri, 16 Aug 2024 12:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08713v1</guid></item><item><title>Resilience in Online Federated Learning: Mitigating Model-Poisoning Attacks via Partial Sharing</title><link>http://arxiv.org/abs/2403.13108v2</link><description>Federated learning (FL) allows training machine learning models ondistributed data without compromising privacy. However, FL is vulnerable tomodel-poisoning attacks where malicious clients tamper with their local modelsto manipulate the global model. In this work, we investigate the resilience ofthe partial-sharing online FL (PSO-Fed) algorithm against such attacks. PSO-Fedreduces communication overhead by allowing clients to share only a fraction oftheir model updates with the server. We demonstrate that this partial sharingmechanism has the added advantage of enhancing PSO-Fed's robustness tomodel-poisoning attacks. Through theoretical analysis, we show that PSO-Fedmaintains convergence even under Byzantine attacks, where malicious clientsinject noise into their updates. Furthermore, we derive a formula for PSO-Fed'smean square error, considering factors like stepsize, attack probability, andthe number of malicious clients. Interestingly, we find a non-trivial optimalstepsize that maximizes PSO-Fed's resistance to these attacks. Extensivenumerical experiments confirm our theoretical findings and showcase PSO-Fed'ssuperior performance against model-poisoning attacks compared to other leadingFL algorithms.</description><author>Ehsan Lari, Reza Arablouei, Vinay Chakravarthi Gogineni, Stefan Werner</author><pubDate>Fri, 16 Aug 2024 12:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13108v2</guid></item><item><title>A Medical Data-Effective Learning Benchmark for Highly Efficient Pre-training of Foundation Models</title><link>http://arxiv.org/abs/2401.17542v3</link><description>Foundation models, pre-trained on massive datasets, have achievedunprecedented generalizability. However, is it truly necessary to involve suchvast amounts of data in pre-training, consuming extensive computationalresources? This paper introduces data-effective learning, aiming to use data inthe most impactful way to pre-train foundation models. This involves strategiesthat focus on data quality rather than quantity, ensuring the data used fortraining has high informational value. Data-effective learning plays a profoundrole in accelerating foundation model training, reducing computational costs,and saving data storage, which is very important as the volume of medical datain recent years has grown beyond many people's expectations. However, due tothe lack of standards and comprehensive benchmarks, research on medicaldata-effective learning is poorly studied. To address this gap, our paperintroduces a comprehensive benchmark specifically for evaluating data-effectivelearning in the medical field. This benchmark includes a dataset with millionsof data samples from 31 medical centers (DataDEL), a baseline method forcomparison (MedDEL), and a new evaluation metric (NormDEL) to objectivelymeasure data-effective learning performance. Our extensive experimental resultsshow the baseline MedDEL can achieve performance comparable to the originallarge dataset with only 5% of the data. Establishing such an opendata-effective learning benchmark is crucial for the medical foundation modelresearch community because it facilitates efficient data use, promotescollaborative breakthroughs, and fosters the development of cost-effective,scalable, and impactful healthcare solutions.</description><author>Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan</author><pubDate>Fri, 16 Aug 2024 12:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17542v3</guid></item><item><title>Decoupling Feature Representations of Ego and Other Modalities for Incomplete Multi-modal Brain Tumor Segmentation</title><link>http://arxiv.org/abs/2408.08708v1</link><description>Multi-modal brain tumor segmentation typically involves four magneticresonance imaging (MRI) modalities, while incomplete modalities significantlydegrade performance. Existing solutions employ explicit or implicit modalityadaptation, aligning features across modalities or learning a fused featurerobust to modality incompleteness. They share a common goal of encouraging eachmodality to express both itself and the others. However, the two expressionabilities are entangled as a whole in a seamless feature space, resulting inprohibitive learning burdens. In this paper, we propose DeMoSeg to enhance themodality adaptation by Decoupling the task of representing the ego and otherModalities for robust incomplete multi-modal Segmentation. The decoupling issuper lightweight by simply using two convolutions to map each modality ontofour feature sub-spaces. The first sub-space expresses itself (Self-feature),while the remaining sub-spaces substitute for other modalities(Mutual-features). The Self- and Mutual-features interactively guide each otherthrough a carefully-designed Channel-wised Sparse Self-Attention (CSSA). Afterthat, a Radiologist-mimic Cross-modality expression Relationships (RCR) isintroduced to have available modalities provide Self-feature and also `lend'their Mutual-features to compensate for the absent ones by exploiting theclinical prior knowledge. The benchmark results on BraTS2020, BraTS2018 andBraTS2015 verify the DeMoSeg's superiority thanks to the alleviated modalityadaptation difficulty. Concretely, for BraTS2020, DeMoSeg increases Dice by atleast 0.92%, 2.95% and 4.95% on whole tumor, tumor core and enhanced tumorregions, respectively, compared to other state-of-the-arts. Codes are athttps://github.com/kk42yy/DeMoSeg</description><author>Kaixiang Yang, Wenqi Shan, Xudong Li, Xuan Wang, Xikai Yang, Xi Wang, Pheng-Ann Heng, Qiang Li, Zhiwei Wang</author><pubDate>Fri, 16 Aug 2024 12:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08708v1</guid></item><item><title>Beam Prediction based on Large Language Models</title><link>http://arxiv.org/abs/2408.08707v1</link><description>Millimeter-wave (mmWave) communication is promising for next-generationwireless networks but suffers from significant path loss, requiring extensiveantenna arrays and frequent beam training. Traditional deep learning models,such as long short-term memory (LSTM), enhance beam tracking accuracy howeverare limited by poor robustness and generalization. In this letter, we use largelanguage models (LLMs) to improve the robustness of beam prediction. Byconverting time series data into text-based representations and employing thePrompt-as-Prefix (PaP) technique for contextual enrichment, our approachunleashes the strength of LLMs for time series forecasting. Simulation resultsdemonstrate that our LLM-based method offers superior robustness andgeneralization compared to LSTM-based models, showcasing the potential of LLMsin wireless communications.</description><author>Yucheng Sheng, Kai Huang, Le Liang, Peng Liu, Shi Jin, Geoffrey Ye Li</author><pubDate>Fri, 16 Aug 2024 12:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08707v1</guid></item><item><title>Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing</title><link>http://arxiv.org/abs/2408.02558v3</link><description>With the EU AI Act effective from 1 August 2024, high-risk applications likecredit scoring must adhere to stringent transparency and quality standards,including algorithmic fairness evaluations. Consequently, developing tools forauditing algorithmic fairness has become crucial. This paper addresses a keyquestion: how can we scientifically audit algorithmic fairness? It is vital todetermine whether adverse decisions result from algorithmic discrimination orthe subjects' inherent limitations. We introduce a novel auditing framework,``peer-induced fairness'', leveraging counterfactual fairness and advancedcausal inference techniques within credit approval systems. Our approachassesses fairness at the individual level through peer comparisons, independentof specific AI methodologies. It effectively tackles challenges like datascarcity and imbalance, common in traditional models, particularly in creditapproval. Model-agnostic and flexible, the framework functions as both aself-audit tool for stakeholders and an external audit tool for regulators,offering ease of integration. It also meets the EU AI Act's transparencyrequirements by providing clear feedback on whether adverse decisions stem frompersonal capabilities or discrimination. We demonstrate the framework'susefulness by applying it to SME credit approval, revealing significant bias:41.51% of micro-firms face discrimination compared to non-micro firms. Thesefindings highlight the framework's potential for diverse AI applications.</description><author>Shiqi Fang, Zexun Chen, Jake Ansell</author><pubDate>Fri, 16 Aug 2024 12:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02558v3</guid></item></channel></rss>