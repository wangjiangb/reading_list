<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 30 Dec 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views</title><link>http://arxiv.org/abs/2312.17250v1</link><description>We present iFusion, a novel 3D object reconstruction framework that requiresonly two views with unknown camera poses. While single-view reconstructionyields visually appealing results, it can deviate significantly from the actualobject, especially on unseen sides. Additional views improve reconstructionfidelity but necessitate known camera poses. However, assuming the availabilityof pose may be unrealistic, and existing pose estimators fail in sparse viewscenarios. To address this, we harness a pre-trained novel view synthesisdiffusion model, which embeds implicit knowledge about the geometry andappearance of diverse objects. Our strategy unfolds in three steps: (1) Weinvert the diffusion model for camera pose estimation instead of synthesizingnovel views. (2) The diffusion model is fine-tuned using provided views andestimated poses, turned into a novel view synthesizer tailored for the targetobject. (3) Leveraging registered views and the fine-tuned diffusion model, wereconstruct the 3D object. Experiments demonstrate strong performance in bothpose estimation and novel view synthesis. Moreover, iFusion seamlesslyintegrates with various reconstruction methods and enhances them.</description><author>Chin-Hsuan Wu, Yen-Chun Chen, Bolivar Solarte, Lu Yuan, Min Sun</author><pubDate>Thu, 28 Dec 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17250v1</guid></item><item><title>Do Androids Know They're Only Dreaming of Electric Sheep?</title><link>http://arxiv.org/abs/2312.17249v1</link><description>We design probes trained on the internal representations of a transformerlanguage model that are predictive of its hallucinatory behavior on in-contextgeneration tasks. To facilitate this detection, we create a span-annotateddataset of organic and synthetic hallucinations over several tasks. We findthat probes trained on the force-decoded states of synthetic hallucinations aregenerally ecologically invalid in organic hallucination detection. Furthermore,hidden state information about hallucination appears to be task anddistribution-dependent. Intrinsic and extrinsic hallucination saliency variesacross layers, hidden state types, and tasks; notably, extrinsic hallucinationstend to be more salient in a transformer's internal representations.Outperforming multiple contemporary baselines, we show that probing is afeasible and efficient alternative to language model hallucination evaluationwhen model states are available.</description><author>Sky CH-Wang, Benjamin Van Durme, Jason Eisner, Chris Kedzie</author><pubDate>Thu, 28 Dec 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17249v1</guid></item><item><title>Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity</title><link>http://arxiv.org/abs/2312.17248v1</link><description>Reinforcement Learning (RL) encompasses diverse paradigms, includingmodel-based RL, policy-based RL, and value-based RL, each tailored toapproximate the model, optimal policy, and optimal value function,respectively. This work investigates the potential hierarchy of representationcomplexity -- the complexity of functions to be represented -- among these RLparadigms. We first demonstrate that, for a broad class of Markov decisionprocesses (MDPs), the model can be represented by constant-depth circuits withpolynomial size or Multi-Layer Perceptrons (MLPs) with constant layers andpolynomial hidden dimension. However, the representation of the optimal policyand optimal value proves to be $\mathsf{NP}$-complete and unattainable byconstant-layer MLPs with polynomial size. This demonstrates a significantrepresentation complexity gap between model-based RL and model-free RL, whichincludes policy-based RL and value-based RL. To further explore therepresentation complexity hierarchy between policy-based RL and value-based RL,we introduce another general class of MDPs where both the model and optimalpolicy can be represented by constant-depth circuits with polynomial size orconstant-layer MLPs with polynomial size. In contrast, representing the optimalvalue is $\mathsf{P}$-complete and intractable via a constant-layer MLP withpolynomial hidden dimension. This accentuates the intricate representationcomplexity associated with value-based RL compared to policy-based RL. Insummary, we unveil a potential representation complexity hierarchy within RL --representing the model emerges as the easiest task, followed by the optimalpolicy, while representing the optimal value function presents the mostintricate challenge.</description><author>Guhao Feng, Han Zhong</author><pubDate>Thu, 28 Dec 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17248v1</guid></item><item><title>Amodal Ground Truth and Completion in the Wild</title><link>http://arxiv.org/abs/2312.17247v1</link><description>The problem we study in this paper is amodal image segmentation: predictingentire object segmentation masks including both visible and invisible(occluded) parts. In previous work, the amodal segmentation ground truth onreal images is usually predicted by manual annotaton and thus is subjective. Incontrast, we use 3D data to establish an automatic pipeline to determineauthentic ground truth amodal masks for partially occluded objects in realimages. This pipeline is used to construct an amodal completion evaluationbenchmark, MP3D-Amodal, consisting of a variety of object categories andlabels. To better handle the amodal completion task in the wild, we explore twoarchitecture variants: a two-stage model that first infers the occluder,followed by amodal mask completion; and a one-stage model that exploits therepresentation power of Stable Diffusion for amodal segmentation across manycategories. Without bells and whistles, our method achieves a newstate-of-the-art performance on Amodal segmentation datasets that cover a largevariety of objects, including COCOA and our new MP3D-Amodal dataset. Thedataset, model, and code are available athttps://www.robots.ox.ac.uk/~vgg/research/amodal/.</description><author>Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman</author><pubDate>Thu, 28 Dec 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17247v1</guid></item><item><title>The LLM Surgeon</title><link>http://arxiv.org/abs/2312.17244v1</link><description>State-of-the-art language models are becoming increasingly large in an effortto achieve the highest performance on large corpora of available textual data.However, the sheer size of the Transformer architectures makes it difficult todeploy models within computational, environmental or device-specificconstraints. We explore data-driven compression of existing pretrained modelsas an alternative to training smaller models from scratch. To do so, we scaleKronecker-factored curvature approximations of the target loss landscape tolarge language models. In doing so, we can compute both the dynamic allocationof structures that can be removed as well as updates of remaining weights thataccount for the removal. We provide a general framework for unstructured,semi-structured and structured pruning and improve upon weight updates tocapture more correlations between weights, while remaining computationallyefficient. Experimentally, our method can prune rows and columns from a rangeof OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,and achieve state-of-the-art results in unstructured and semi-structuredpruning of large language models.</description><author>Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki M. Asano, Tijmen Blankevoort</author><pubDate>Thu, 28 Dec 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17244v1</guid></item><item><title>Unsupervised Universal Image Segmentation</title><link>http://arxiv.org/abs/2312.17243v1</link><description>Several unsupervised image segmentation approaches have been proposed whicheliminate the need for dense manually-annotated segmentation masks; currentmodels separately handle either semantic segmentation (e.g., STEGO) orclass-agnostic instance segmentation (e.g., CutLER), but not both (i.e.,panoptic segmentation). We propose an Unsupervised Universal Segmentation model(U2Seg) adept at performing various image segmentation tasks -- instance,semantic and panoptic -- using a novel unified framework. U2Seg generatespseudo semantic labels for these segmentation tasks via leveragingself-supervised models followed by clustering; each cluster representsdifferent semantic and/or instance membership of pixels. We then self-train themodel on these pseudo semantic labels, yielding substantial performance gainsover specialized methods tailored to each task: a +2.6 AP$^{\text{box}}$ boostvs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAccincrease (vs. STEGO) in unsupervised semantic segmentation on COCOStuff.Moreover, our method sets up a new baseline for unsupervised panopticsegmentation, which has not been previously explored. U2Seg is also a strongpretrained model for few-shot segmentation, surpassing CutLER by +5.0AP$^{\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCOlabels. We hope our simple yet effective method can inspire more research onunsupervised universal image segmentation.</description><author>Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, Trevor Darrell</author><pubDate>Thu, 28 Dec 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17243v1</guid></item><item><title>Learning to Generate Text in Arbitrary Writing Styles</title><link>http://arxiv.org/abs/2312.17242v1</link><description>Prior work in style-controlled text generation has focused on tasks such asemulating the style of prolific literary authors, producing formal or informaltext, and the degree of toxicity of generated text. Plentiful demonstrations ofthese styles are available, and as a result modern language models are oftenable to emulate them, either via prompting or discriminative control. However,in applications such as writing assistants, it is desirable for language modelsto produce text in an author-specific style on the basis of a small writingsample. We find that instruction-tuned language models can struggle toreproduce author-specific style demonstrated in a prompt. Instead, we proposeto guide a language model to generate text in a target style usingcontrastively-trained representations that capture stylometric features. Acentral challenge in doing so is that an author's writing is characterized bysurprising token choices under a generic language model. To reconcile thistension, we combine generative re-scoring to achieve an author-specific model,with discriminative control to ensure style consistency at the sequence-level.The combination of these approaches is found to be particularly effective atadhering to an author-specific style in a variety of conditions, includingunconditional generation and style transfer, and is applicable to anyunderlying language model without requiring fine-tuning.</description><author>Aleem Khan, Andrew Wang, Sophia Hager, Nicholas Andrews</author><pubDate>Thu, 28 Dec 2023 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17242v1</guid></item><item><title>Compact Neural Graphics Primitives with Learned Hash Probing</title><link>http://arxiv.org/abs/2312.17241v1</link><description>Neural graphics primitives are faster and achieve higher quality when theirneural networks are augmented by spatial data structures that hold trainablefeatures arranged in a grid. However, existing feature grids either come with alarge memory footprint (dense or factorized grids, trees, and hash tables) orslow performance (index learning and vector quantization). In this paper, weshow that a hash table with learned probes has neither disadvantage, resultingin a favorable combination of size and speed. Inference is faster than unprobedhash tables at equal quality while training is only 1.2-2.6x slower,significantly outperforming prior index learning approaches. We arrive at thisformulation by casting all feature grids into a common framework: they eachcorrespond to a lookup function that indexes into a table of feature vectors.In this framework, the lookup functions of existing data structures can becombined by simple arithmetic combinations of their indices, resulting inPareto optimal compression and speed.</description><author>Towaki Takikawa, Thomas Müller, Merlin Nimier-David, Alex Evans, Sanja Fidler, Alec Jacobson, Alexander Keller</author><pubDate>Thu, 28 Dec 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17241v1</guid></item><item><title>An Improved Baseline for Reasoning Segmentation with Large Language Model</title><link>http://arxiv.org/abs/2312.17240v1</link><description>While LISA effectively bridges the gap between segmentation and largelanguage models to enable reasoning segmentation, it poses certain limitations:unable to distinguish different instances of the target region, and constrainedby the pre-defined textual response formats. In this work, we introduce LISA++,an update to the existing LISA model, focusing on improving corefunctionalities while keeping the base architecture intact. The mainenhancements in LISA++ include: \textbf{1) Enhanced Segmentation}: The instancesegmentation ability has been added, providing a more detailed scene analysisalong with the existing multi-region semantic segmentation. \textbf{2) MoreNatural Conversation}: Improved capability for multi-turn dialogue, with theability to incorporate segmentation results directly into text responses, i.e.,Segmentation in Dialogue (SiD). These improvements are achieved by curating theexisting samples of generic segmentation datasets, aimed specifically atenhancing the segmentation and conversational skills without structural changeand additional data sources. Comparative analysis with the original LISA modelshows significant advancements in these areas, positioning LISA++ as a notableupgrade in visual understanding and interaction. LISA++'s adaptability andimproved features highlight the versatility of the mask-as-embedding paradigmproposed by LISA, and the potential as a foundational model for diverseapplications.</description><author>Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, Jiaya Jia</author><pubDate>Thu, 28 Dec 2023 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17240v1</guid></item><item><title>Fast Inference of Mixture-of-Experts Language Models with Offloading</title><link>http://arxiv.org/abs/2312.17238v1</link><description>With the widespread adoption of Large Language Models (LLMs), many deeplearning practitioners are looking for strategies of running these models moreefficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - atype of model architectures where only a fraction of model layers are activefor any given input. This property allows MoE-based language models to generatetokens faster than their dense counterparts, but it also increases model sizedue to having multiple experts. Unfortunately, this makes state-of-the-art MoElanguage models difficult to run without high-end GPUs. In this work, we studythe problem of running large MoE language models on consumer hardware withlimited accelerator memory. We build upon parameter offloading algorithms andpropose a novel strategy that accelerates offloading by taking advantage ofinnate properties of MoE LLMs. Using this strategy, we build can runMixtral-8x7B with mixed quantization on desktop hardware and free-tier GoogleColab instances.</description><author>Artyom Eliseev, Denis Mazur</author><pubDate>Thu, 28 Dec 2023 18:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17238v1</guid></item><item><title>A Simple LLM Framework for Long-Range Video Question-Answering</title><link>http://arxiv.org/abs/2312.17235v1</link><description>We present LLoVi, a language-based framework for long-range videoquestion-answering (LVQA). Unlike prior long-range video understanding methods,which are often costly and require specialized long-range video modeling design(e.g., memory queues, state-space layers, etc.), our approach uses aframe/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with aLarge Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisinglyeffective LVQA framework. Specifically, we decompose short and long-rangemodeling aspects of LVQA into two stages. First, we use a short-term visualcaptioner to generate textual descriptions of short video clips (0.5-8s inlength) densely sampled from a long input video. Afterward, an LLM aggregatesthe densely extracted short-term captions to perform long-range temporalreasoning needed to understand the whole video and answer a question. Toanalyze what makes our simple framework so effective, we thoroughly evaluatevarious components of our system. Our empirical analysis reveals that thechoice of the visual captioner and LLM is critical for good LVQA performance.Furthermore, we show that a specialized prompt that asks the LLM first tosummarize the noisy short-term visual captions and then answer a given inputquestion leads to a significant LVQA performance boost. On EgoSchema, which isbest known as a very long-form video question-answering benchmark, our methodachieves 50.3% accuracy, outperforming the previous best-performing approach by18.1% (absolute gain). In addition, our approach outperforms the previousstate-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVito grounded LVQA and show that it outperforms all prior methods on the NeXT-GQAdataset. We will release our code at https://github.com/CeeZh/LLoVi.</description><author>Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, Gedas Bertasius</author><pubDate>Thu, 28 Dec 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17235v1</guid></item><item><title>Personalized Restoration via Dual-Pivot Tuning</title><link>http://arxiv.org/abs/2312.17234v1</link><description>Generative diffusion models can serve as a prior which ensures that solutionsof image restoration systems adhere to the manifold of natural images. However,for restoring facial images, a personalized prior is necessary to accuratelyrepresent and reconstruct unique facial features of a given individual. In thispaper, we propose a simple, yet effective, method for personalized restoration,called Dual-Pivot Tuning - a two-stage approach that personalize a blindrestoration system while maintaining the integrity of the general prior and thedistinct role of each component. Our key observation is that for optimalpersonalization, the generative model should be tuned around a fixed textpivot, while the guiding network should be tuned in a generic(non-personalized) manner, using the personalized generative model as a fixed``pivot". This approach ensures that personalization does not interfere withthe restoration process, resulting in a natural appearance with high fidelityto the person's identity and the attributes of the degraded image. We evaluatedour approach both qualitatively and quantitatively through extensiveexperiments with images of widely recognized individuals, comparing it againstrelevant baselines. Surprisingly, we found that our personalized prior not onlyachieves higher fidelity to identity with respect to the person's identity, butalso outperforms state-of-the-art generic priors in terms of general imagequality. Project webpage: https://personalized-restoration.github.io</description><author>Pradyumna Chari, Sizhuo Ma, Daniil Ostashev, Achuta Kadambi, Gurunandan Krishnan, Jian Wang, Kfir Aberman</author><pubDate>Thu, 28 Dec 2023 18:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17234v1</guid></item><item><title>Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels</title><link>http://arxiv.org/abs/2312.17232v1</link><description>Current 3D scene segmentation methods are heavily dependent on manuallyannotated 3D training datasets. Such manual annotations are labor-intensive,and often lack fine-grained details. Importantly, models trained on this datatypically struggle to recognize object classes beyond the annotated classes,i.e., they do not generalize well to unseen domains and require additionaldomain-specific annotations. In contrast, 2D foundation models demonstratestrong generalization and impressive zero-shot abilities, inspiring us toincorporate these characteristics from 2D models into 3D models. Therefore, weexplore the use of image segmentation foundation models to automaticallygenerate training labels for 3D segmentation. We propose Segment3D, a methodfor class-agnostic 3D scene segmentation that produces high-quality 3Dsegmentation masks. It improves over existing 3D segmentation models(especially on fine-grained masks), and enables easily adding new training datato further boost the segmentation performance -- all without the need formanual training labels.</description><author>Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, Francis Engelmann</author><pubDate>Thu, 28 Dec 2023 18:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17232v1</guid></item><item><title>Think Before You Duel: Understanding Complexities of Preference Learning under Constrained Resources</title><link>http://arxiv.org/abs/2312.17229v1</link><description>We consider the problem of reward maximization in the dueling bandit setupalong with constraints on resource consumption. As in the classic duelingbandits, at each round the learner has to choose a pair of items from a set of$K$ items and observe a relative feedback for the current pair. Additionally,for both items, the learner also observes a vector of resource consumptions.The objective of the learner is to maximize the cumulative reward, whileensuring that the total consumption of any resource is within the allocatedbudget. We show that due to the relative nature of the feedback, the problem ismore difficult than its bandit counterpart and that without further assumptionsthe problem is not learnable from a regret minimization perspective.Thereafter, by exploiting assumptions on the available budget, we provide anEXP3 based dueling algorithm that also considers the associated consumptionsand show that it achieves an$\tilde{\mathcal{O}}\left({\frac{OPT^{(b)}}{B}}K^{1/3}T^{2/3}\right)$ regret,where $OPT^{(b)}$ is the optimal value and $B$ is the available budget.Finally, we provide numerical simulations to demonstrate the efficacy of ourproposed method.</description><author>Rohan Deb, Aadirupa Saha</author><pubDate>Thu, 28 Dec 2023 18:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17229v1</guid></item><item><title>Gradient-based Planning with World Models</title><link>http://arxiv.org/abs/2312.17227v1</link><description>The enduring challenge in the field of artificial intelligence has been thecontrol of systems to achieve desired behaviours. While for systems governed bystraightforward dynamics equations, methods like Linear Quadratic Regulation(LQR) have historically proven highly effective, most real-world tasks, whichrequire a general problem-solver, demand world models with dynamics that cannotbe easily described by simple equations. Consequently, these models must belearned from data using neural networks. Most model predictive control (MPC)algorithms designed for visual world models have traditionally exploredgradient-free population-based optimisation methods, such as Cross Entropy andModel Predictive Path Integral (MPPI) for planning. However, we present anexploration of a gradient-based alternative that fully leverages thedifferentiability of the world model. In our study, we conduct a comparativeanalysis between our method and other MPC-based alternatives, as well aspolicy-based algorithms. In a sample-efficient setting, our method achieves onpar or superior performance compared to the alternative approaches in mosttasks. Additionally, we introduce a hybrid model that combines policy networksand gradient-based MPC, which outperforms pure policy based methods therebyholding promise for Gradient-based planning with world models in complexreal-world tasks.</description><author>Jyothir S V, Siddhartha Jalagam, Yann LeCun, Vlad Sobal</author><pubDate>Thu, 28 Dec 2023 18:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17227v1</guid></item><item><title>Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models</title><link>http://arxiv.org/abs/2312.10835v2</link><description>Knowledge distillation methods have recently shown to be a promisingdirection to speedup the synthesis of large-scale diffusion models by requiringonly a few inference steps. While several powerful distillation methods wererecently proposed, the overall quality of student samples is typically lowercompared to the teacher ones, which hinders their practical usage. In thiswork, we investigate the relative quality of samples produced by the teachertext-to-image diffusion model and its distilled student version. As our mainempirical finding, we discover that a noticeable portion of student samplesexhibit superior fidelity compared to the teacher ones, despite the``approximate'' nature of the student. Based on this finding, we propose anadaptive collaboration between student and teacher diffusion models foreffective text-to-image synthesis. Specifically, the distilled model producesthe initial sample, and then an oracle decides whether it needs furtherimprovements with a slow teacher model. Extensive experiments demonstrate thatthe designed pipeline surpasses state-of-the-art text-to-image alternatives forvarious inference budgets in terms of human preference. Furthermore, theproposed approach can be naturally used in popular applications such astext-guided image editing and controllable generation.</description><author>Nikita Starodubcev, Artem Fedorov, Artem Babenko, Dmitry Baranchuk</author><pubDate>Thu, 28 Dec 2023 18:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10835v2</guid></item><item><title>4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency</title><link>http://arxiv.org/abs/2312.17225v1</link><description>Aided by text-to-image and text-to-video diffusion models, existing 4Dcontent creation pipelines utilize score distillation sampling to optimize theentire dynamic 3D scene. However, as these pipelines generate 4D content fromtext or image inputs, they incur significant time and effort in promptengineering through trial and error. This work introduces 4DGen, a novel,holistic framework for grounded 4D content creation that decomposes the 4Dgeneration task into multiple stages. We identify static 3D assets andmonocular video sequences as key components in constructing the 4D content. Ourpipeline facilitates conditional 4D generation, enabling users to specifygeometry (3D assets) and motion (monocular videos), thus offering superiorcontrol over content creation. Furthermore, we construct our 4D representationusing dynamic 3D Gaussians, which permits efficient, high-resolutionsupervision through rendering during training, thereby facilitatinghigh-quality 4D generation. Additionally, we employ spatial-temporal pseudolabels on anchor frames, along with seamless consistency priors implementedthrough 3D-aware score distillation sampling and smoothness regularizations.Compared to existing baselines, our approach yields competitive results infaithfully reconstructing input signals and realistically inferring renderingsfrom novel viewpoints and timesteps. Most importantly, our method supportsgrounded generation, offering users enhanced control, a feature difficult toachieve with previous methods. Project page:https://vita-group.github.io/4DGen/</description><author>Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, Yunchao Wei</author><pubDate>Thu, 28 Dec 2023 18:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17225v1</guid></item><item><title>SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks</title><link>http://arxiv.org/abs/2312.17216v1</link><description>Spiking Neural Networks (SNNs) are biologically-inspired models that arecapable of processing information in streams of action potentials. However,simulating and training SNNs is computationally expensive due to the need tosolve large systems of coupled differential equations. In this paper, weintroduce SparseProp, a novel event-based algorithm for simulating and trainingsparse SNNs. Our algorithm reduces the computational cost of both the forwardand backward pass operations from O(N) to O(log(N)) per network spike, therebyenabling numerically exact simulations of large spiking networks and theirefficient training using backpropagation through time. By leveraging thesparsity of the network, SparseProp eliminates the need to iterate through allneurons at each spike, employing efficient state updates instead. Wedemonstrate the efficacy of SparseProp across several classicalintegrate-and-fire neuron models, including a simulation of a sparse SNN withone million LIF neurons. This results in a speed-up exceeding four orders ofmagnitude relative to previous event-based implementations. Our work providesan efficient and exact solution for training large-scale spiking neuralnetworks and opens up new possibilities for building more sophisticatedbrain-inspired models.</description><author>Rainer Engelken</author><pubDate>Thu, 28 Dec 2023 18:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17216v1</guid></item><item><title>Continual Learning via Sequential Function-Space Variational Inference</title><link>http://arxiv.org/abs/2312.17210v1</link><description>Sequential Bayesian inference over predictive functions is a naturalframework for continual learning from streams of data. However, applying it toneural networks has proved challenging in practice. Addressing the drawbacks ofexisting techniques, we propose an optimization objective derived byformulating continual learning as sequential function-space variationalinference. In contrast to existing methods that regularize neural networkparameters directly, this objective allows parameters to vary widely duringtraining, enabling better adaptation to new tasks. Compared to objectives thatdirectly regularize neural network predictions, the proposed objective allowsfor more flexible variational distributions and more effective regularization.We demonstrate that, across a range of task sequences, neural networks trainedvia sequential function-space variational inference achieve better predictiveaccuracy than networks trained with related methods while depending less onmaintaining a set of representative points from previous tasks.</description><author>Tim G. J. Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, Yarin Gal</author><pubDate>Thu, 28 Dec 2023 18:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17210v1</guid></item><item><title>EFHQ: Multi-purpose ExtremePose-Face-HQ dataset</title><link>http://arxiv.org/abs/2312.17205v1</link><description>The existing facial datasets, while having plentiful images at near frontalviews, lack images with extreme head poses, leading to the downgradedperformance of deep learning models when dealing with profile or pitched faces.This work aims to address this gap by introducing a novel dataset named ExtremePose Face High-Quality Dataset (EFHQ), which includes a maximum of 450khigh-quality images of faces at extreme poses. To produce such a massivedataset, we utilize a novel and meticulous dataset processing pipeline tocurate two publicly available datasets, VFHQ and CelebV-HQ, which contain manyhigh-resolution face videos captured in various settings. Our dataset cancomplement existing datasets on various facial-related tasks, such as facialsynthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,and face reenactment. Specifically, training with EFHQ helps models generalizewell across diverse poses, significantly improving performance in scenariosinvolving extreme views, confirmed by extensive experiments. Additionally, weutilize EFHQ to define a challenging cross-view face verification benchmark, inwhich the performance of SOTA face recognition models drops 5-37\% compared tofrontal-to-frontal scenarios, aiming to stimulate studies on face recognitionunder severe pose conditions in the wild.</description><author>Trung Tuan Dao, Duc Hong Vu, Cuong Pham, Anh Tran</author><pubDate>Thu, 28 Dec 2023 18:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17205v1</guid></item><item><title>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</title><link>http://arxiv.org/abs/2312.02366v3</link><description>The integration of deep learning systems into healthcare has been hindered bythe resource-intensive process of data annotation and the inability of thesesystems to generalize to different data distributions. Foundation models, whichare models pre-trained on large datasets, have emerged as a solution to reducereliance on annotated data and enhance model generalizability and robustness.DINOv2 is an open-source foundation model pre-trained with self-supervisedlearning on 142 million curated natural images that exhibits promisingcapabilities across various vision tasks. Nevertheless, a critical questionremains unanswered regarding DINOv2's adaptability to radiological imaging, andwhether its features are sufficiently general to benefit radiology imageanalysis. Therefore, this study comprehensively evaluates DINOv2 for radiology,conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI).To measure the effectiveness and generalizability of DINOv2's featurerepresentations, we analyze the model across medical image analysis tasksincluding disease classification and organ segmentation on both 2D and 3Dimages, and under different settings like kNN, few-shot learning,linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning.Comparative analyses with established supervised, self-supervised, andweakly-supervised models reveal DINOv2's superior performance and cross-taskgeneralizability. The findings contribute insights to potential avenues foroptimizing pre-training strategies for medical imaging and enhancing thebroader understanding of DINOv2's role in bridging the gap between natural andradiological image analysis. Our code is available athttps://github.com/MohammedSB/DINOv2ForRadiology</description><author>Mohammed Baharoon, Waseem Qureshi, Jiahong Ouyang, Yanwu Xu, Abdulrhman Aljouie, Wei Peng</author><pubDate>Thu, 28 Dec 2023 18:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02366v3</guid></item><item><title>Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic Simulations of a Plasma Sheet Model</title><link>http://arxiv.org/abs/2310.17646v2</link><description>We explore the possibility of fully replacing a plasma physics kineticsimulator with a graph neural network-based simulator. We focus on this classof surrogate models given the similarity between their message-passing updatemechanism and the traditional physics solver update, and the possibility ofenforcing known physical priors into the graph construction and update. We showthat our model learns the kinetic plasma dynamics of the one-dimensional plasmamodel, a predecessor of contemporary kinetic plasma simulation codes, andrecovers a wide range of well-known kinetic plasma processes, including plasmathermalization, electrostatic fluctuations about thermal equilibrium, and thedrag on a fast sheet and Landau damping. We compare the performance against theoriginal plasma model in terms of run-time, conservation laws, and temporalevolution of key physical quantities. The limitations of the model arepresented and possible directions for higher-dimensional surrogate models forkinetic plasmas are discussed.</description><author>Diogo D Carvalho, Diogo R Ferreira, Luis O Silva</author><pubDate>Thu, 28 Dec 2023 18:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17646v2</guid></item><item><title>Tractable Function-Space Variational Inference in Bayesian Neural Networks</title><link>http://arxiv.org/abs/2312.17199v1</link><description>Reliable predictive uncertainty estimation plays an important role inenabling the deployment of neural networks to safety-critical settings. Apopular approach for estimating the predictive uncertainty of neural networksis to define a prior distribution over the network parameters, infer anapproximate posterior distribution, and use it to make stochastic predictions.However, explicit inference over neural network parameters makes it difficultto incorporate meaningful prior information about the data-generating processinto the model. In this paper, we pursue an alternative approach. Recognizingthat the primary object of interest in most settings is the distribution overfunctions induced by the posterior distribution over neural network parameters,we frame Bayesian inference in neural networks explicitly as inferring aposterior distribution over functions and propose a scalable function-spacevariational inference method that allows incorporating prior information andresults in reliable predictive uncertainty estimates. We show that the proposedmethod leads to state-of-the-art uncertainty estimation and predictiveperformance on a range of prediction tasks and demonstrate that it performswell on a challenging safety-critical medical diagnosis task in which reliableuncertainty estimation is essential.</description><author>Tim G. J. Rudner, Zonghao Chen, Yee Whye Teh, Yarin Gal</author><pubDate>Thu, 28 Dec 2023 18:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17199v1</guid></item><item><title>Resilient Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2312.17194v1</link><description>We study a class of constrained reinforcement learning (RL) problems in whichmultiple constraint specifications are not identified before training. It ischallenging to identify appropriate constraint specifications due to theundefined trade-off between the reward maximization objective and theconstraint satisfaction, which is ubiquitous in constrained decision-making. Totackle this issue, we propose a new constrained RL approach that searches forpolicy and constraint specifications together. This method features theadaptation of relaxing the constraint according to a relaxation cost introducedin the learning objective. Since this feature mimics how ecological systemsadapt to disruptions by altering operation, our approach is termed as resilientconstrained RL. Specifically, we provide a set of sufficient conditions thatbalance the constraint satisfaction and the reward maximization in notion ofresilient equilibrium, propose a tractable formulation of resilient constrainedpolicy optimization that takes this equilibrium as an optimal solution, andadvocate two resilient constrained policy search algorithms with non-asymptoticconvergence guarantees on the optimality gap and constraint satisfaction.Furthermore, we demonstrate the merits and the effectiveness of our approach incomputational experiments.</description><author>Dongsheng Ding, Zhengyan Huan, Alejandro Ribeiro</author><pubDate>Thu, 28 Dec 2023 18:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17194v1</guid></item><item><title>Predicting Transcription Factor Binding Sites using Transformer based Capsule Network</title><link>http://arxiv.org/abs/2310.15202v2</link><description>Prediction of binding sites for transcription factors is important tounderstand how they regulate gene expression and how this regulation can bemodulated for therapeutic purposes. Although in the past few years there aresignificant works addressing this issue, there is still space for improvement.In this regard, a transformer based capsule network viz. DNABERT-Cap isproposed in this work to predict transcription factor binding sites miningChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained withlarge number of genomic DNA sequences, empowered with a capsule layerresponsible for the final prediction. The proposed model builds a predictor fortranscription factor binding sites using the joint optimisation of featuresencompassing both bidirectional encoder and capsule layer, along withconvolutional and bidirectional long-short term memory layers. To evaluate theefficiency of the proposed approach, we use a benchmark ChIP-seq datasets offive cell lines viz. A549, GM12878, Hep-G2, H1-hESC and Hela, available in theENCODE repository. The results show that the average area under the receiveroperating characteristic curve score exceeds 0.91 for all such five cell lines.DNABERT-Cap is also compared with existing state-of-the-art deep learning basedpredictors viz. DeepARC, DeepTF, CNN-Zeng and DeepBind, and is seen tooutperform them.</description><author>Nimisha Ghosh, Daniele Santoni, Indrajit Saha, Giovanni Felici</author><pubDate>Thu, 28 Dec 2023 18:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15202v2</guid></item><item><title>HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human Reconstruction</title><link>http://arxiv.org/abs/2312.17192v1</link><description>Neural reconstruction and rendering strategies have demonstratedstate-of-the-art performances due, in part, to their ability to preserve highlevel shape details. Existing approaches, however, either represent objects asimplicit surface functions or neural volumes and still struggle to recovershapes with heterogeneous materials, in particular human skin, hair or clothes.To this aim, we present a new hybrid implicit surface representation to modelhuman shapes. This representation is composed of two surface layers thatrepresent opaque and translucent regions on the clothed human body. We segmentdifferent regions automatically using visual cues and learn to reconstruct twosigned distance functions (SDFs). We perform surface-based rendering on opaqueregions (e.g., body, face, clothes) to preserve high-fidelity surface normalsand volume rendering on translucent regions (e.g., hair). Experimentsdemonstrate that our approach obtains state-of-the-art results on 3D humanreconstructions, and also shows competitive performances on other objects.</description><author>Angtian Wang, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Edmond Boyer, Alan Yuille, Tony Tung</author><pubDate>Thu, 28 Dec 2023 18:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17192v1</guid></item><item><title>Why Do Probabilistic Clinical Models Fail To Transport Between Sites?</title><link>http://arxiv.org/abs/2311.04787v2</link><description>The rising popularity of artificial intelligence in healthcare ishighlighting the problem that a computational model achieving super-humanclinical performance at its training sites may perform substantially worse atnew sites. In this perspective, we present common sources for this failure totransport, which we divide into sources under the control of the experimenterand sources inherent to the clinical data-generating process. Of the inherentsources we look a little deeper into site-specific clinical practices that canaffect the data distribution, and propose a potential solution intended toisolate the imprint of those practices on the data from the patterns of diseasecause and effect that are the usual target of probabilistic clinical models.</description><author>Thomas A. Lasko, Eric V. Strobl, William W. Stead</author><pubDate>Thu, 28 Dec 2023 18:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04787v2</guid></item><item><title>One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</title><link>http://arxiv.org/abs/2312.17183v1</link><description>In this study, we focus on building up a model that can Segment Anything inmedical scenarios, driven by Text prompts, termed as SAT. Our maincontributions are three folds: (i) on data construction, we combine multipleknowledge sources to construct a multi-modal medical knowledge tree; Then webuild up a large-scale segmentation dataset for training, by collecting over11K 3D medical image scans from 31 segmentation datasets with carefulstandardization on both visual scans and label space; (ii) on model training,we formulate a universal segmentation model, that can be prompted by inputtingmedical terminologies in text form. We present a knowledge-enhancedrepresentation learning framework, and a series of strategies for effectivelytraining on the combination of a large number of datasets; (iii) on modelevaluation, we train a SAT-Nano with only 107M parameters, to segment 31different segmentation datasets with text prompt, resulting in 362 categories.We thoroughly evaluate the model from three aspects: averaged by body regions,averaged by classes, and average by datasets, demonstrating comparableperformance to 36 specialist nnUNets, i.e., we train nnUNet models on eachdataset/subset, resulting in 36 nnUNets with around 1000M parameters for the 31datasets. We will release all the codes, and models used in this report, i.e.,SAT-Nano. Moreover, we will offer SAT-Ultra in the near future, which istrained with model of larger size, on more diverse datasets. Webpage URL:https://zhaoziheng.github.io/MedUniSeg.</description><author>Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 28 Dec 2023 18:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17183v1</guid></item><item><title>Virtual Scientific Companion for Synchrotron Beamlines: A Prototype</title><link>http://arxiv.org/abs/2312.17180v1</link><description>The extraordinarily high X-ray flux and specialized instrumentation atsynchrotron beamlines have enabled versatile in-situ and high throughputstudies that are impossible elsewhere. Dexterous and efficient control ofexperiments are thus crucial for efficient beamline operation. Artificialintelligence and machine learning methods are constantly being developed toenhance facility performance, but the full potential of these developments canonly be reached with efficient human-computer-interaction. Natural language isthe most intuitive and efficient way for humans to communicate. However, thelow credibility and reproducibility of existing large language models and toolsdemand extensive development to be made for robust and reliable performance forscientific purposes. In this work, we introduce the prototype of virtualscientific companion (VISION) and demonstrate that it is possible to controlbasic beamline operations through natural language with open-source languagemodel and the limited computational resources at beamline. The human-AI natureof VISION leverages existing automation systems and data framework atsynchrotron beamlines.</description><author>Daniel Potemkin, Carlos Soto, Ruipeng Li, Kevin Yager, Esther Tsai</author><pubDate>Thu, 28 Dec 2023 18:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17180v1</guid></item><item><title>Attributing Learned Concepts in Neural Networks to Training Data</title><link>http://arxiv.org/abs/2310.03149v4</link><description>By now there is substantial evidence that deep learning models learn certainhuman-interpretable features as part of their internal representations of data.As having the right (or wrong) concepts is critical to trustworthy machinelearning systems, it is natural to ask which inputs from the model's originaltraining set were most important for learning a concept at a given layer. Toanswer this, we combine data attribution methods with methods for probing theconcepts learned by a model. Training network and probe ensembles for twoconcept datasets on a range of network layers, we use the recently developedTRAK method for large-scale data attribution. We find some evidence forconvergence, where removing the 10,000 top attributing images for a concept andretraining the model does not change the location of the concept in the networknor the probing sparsity of the concept. This suggests that rather than beinghighly dependent on a few specific examples, the features that inform thedevelopment of a concept are spread in a more diffuse manner across itsexemplars, implying robustness in concept formation.</description><author>Nicholas Konz, Charles Godfrey, Madelyn Shapiro, Jonathan Tu, Henry Kvinge, Davis Brown</author><pubDate>Thu, 28 Dec 2023 18:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03149v4</guid></item><item><title>Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution</title><link>http://arxiv.org/abs/2312.17174v1</link><description>Vision-language pretrained models have seen remarkable success, but theirapplication to safety-critical settings is limited by their lack ofinterpretability. To improve the interpretability of vision-language modelssuch as CLIP, we propose a multi-modal information bottleneck (M2IB) approachthat learns latent representations that compress irrelevant information whilepreserving relevant visual and textual features. We demonstrate how M2IB can beapplied to attribution analysis of vision-language pretrained models,increasing attribution accuracy and improving the interpretability of suchmodels when applied to safety-critical domains such as healthcare. Crucially,unlike commonly used unimodal attribution methods, M2IB does not require groundtruth labels, making it possible to audit representations of vision-languagepretrained models when multiple modalities but no ground-truth data isavailable. Using CLIP as an example, we demonstrate the effectiveness of M2IBattribution and show that it outperforms gradient-based, perturbation-based,and attention-based attribution methods both qualitatively and quantitatively.</description><author>Ying Wang, Tim G. J. Rudner, Andrew Gordon Wilson</author><pubDate>Thu, 28 Dec 2023 18:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17174v1</guid></item><item><title>An Adaptive Tangent Feature Perspective of Neural Networks</title><link>http://arxiv.org/abs/2308.15478v2</link><description>In order to better understand feature learning in neural networks, we proposea framework for understanding linear models in tangent feature space where thefeatures are allowed to be transformed during training. We consider lineartransformations of features, resulting in a joint optimization over parametersand transformations with a bilinear interpolation constraint. We show that thisoptimization problem has an equivalent linearly constrained optimization withstructured regularization that encourages approximately low rank solutions.Specializing to neural network structure, we gain insights into how thefeatures and thus the kernel function change, providing additional nuance tothe phenomenon of kernel alignment when the target function is poorlyrepresented using tangent features. In addition to verifying our theoreticalobservations in real neural networks on a simple regression problem, weempirically show that an adaptive feature implementation of tangent featureclassification has an order of magnitude lower sample complexity than the fixedtangent feature model on MNIST and CIFAR-10.</description><author>Daniel LeJeune, Sina Alemohammad</author><pubDate>Thu, 28 Dec 2023 18:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15478v2</guid></item><item><title>Non-Vacuous Generalization Bounds for Large Language Models</title><link>http://arxiv.org/abs/2312.17173v1</link><description>Modern language models can contain billions of parameters, raising thequestion of whether they can generalize beyond the training data or simplyregurgitate their training corpora. We provide the first non-vacuousgeneralization bounds for pretrained large language models (LLMs), indicatingthat language models are capable of discovering regularities that generalize tounseen data. In particular, we derive a compression bound that is valid for theunbounded log-likelihood loss using prediction smoothing, and we extend thebound to handle subsampling, accelerating bound computation on massivedatasets. To achieve the extreme level of compression required for non-vacuousgeneralization bounds, we devise SubLoRA, a low-dimensional non-linearparameterization. Using this approach, we find that larger models have bettergeneralization bounds and are more compressible than smaller models.</description><author>Sanae Lotfi, Marc Finzi, Yilun Kuang, Tim G. J. Rudner, Micah Goldblum, Andrew Gordon Wilson</author><pubDate>Thu, 28 Dec 2023 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17173v1</guid></item><item><title>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</title><link>http://arxiv.org/abs/2312.17172v1</link><description>We present Unified-IO 2, the first autoregressive multimodal model that iscapable of understanding and generating image, text, audio, and action. Tounify different modalities, we tokenize inputs and outputs -- images, text,audio, action, bounding boxes, etc., into a shared semantic space and thenprocess them with a single encoder-decoder transformer model. Since trainingwith such diverse modalities is challenging, we propose various architecturalimprovements to stabilize model training. We train our model from scratch on alarge multimodal pre-training corpus from diverse sources with a multimodalmixture of denoisers objective. To learn an expansive set of skills, such asfollowing multimodal instructions, we construct and finetune on an ensemble of120 datasets with prompts and augmentations. With a single unified model,Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark andstrong results in more than 35 benchmarks, including image generation andunderstanding, natural language understanding, video and audio understanding,and robotic manipulation. We release all our models to the research community.</description><author>Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi</author><pubDate>Thu, 28 Dec 2023 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17172v1</guid></item><item><title>Can Active Sampling Reduce Causal Confusion in Offline Reinforcement Learning?</title><link>http://arxiv.org/abs/2312.17168v1</link><description>Causal confusion is a phenomenon where an agent learns a policy that reflectsimperfect spurious correlations in the data. Such a policy may falsely appearto be optimal during training if most of the training data contain suchspurious correlations. This phenomenon is particularly pronounced in domainssuch as robotics, with potentially large gaps between the open- and closed-loopperformance of an agent. In such settings, causally confused models may appearto perform well according to open-loop metrics during training but failcatastrophically when deployed in the real world. In this paper, we studycausal confusion in offline reinforcement learning. We investigate whetherselectively sampling appropriate points from a dataset of demonstrations mayenable offline reinforcement learning agents to disambiguate the underlyingcausal mechanisms of the environment, alleviate causal confusion in offlinereinforcement learning, and produce a safer model for deployment. To answerthis question, we consider a set of tailored offline reinforcement learningdatasets that exhibit causal ambiguity and assess the ability of activesampling techniques to reduce causal confusion at evaluation. We provideempirical evidence that uniform and active sampling techniques are able toconsistently reduce causal confusion as training progresses and that activesampling is able to do so significantly more efficiently than uniform sampling.</description><author>Gunshi Gupta, Tim G. J. Rudner, Rowan Thomas McAllister, Adrien Gaidon, Yarin Gal</author><pubDate>Thu, 28 Dec 2023 17:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17168v1</guid></item><item><title>Securing NextG Systems against Poisoning Attacks on Federated Learning: A Game-Theoretic Solution</title><link>http://arxiv.org/abs/2312.17164v1</link><description>This paper studies the poisoning attack and defense interactions in afederated learning (FL) system, specifically in the context of wireless signalclassification using deep learning for next-generation (NextG) communications.FL collectively trains a global model without the need for clients to exchangetheir data samples. By leveraging geographically dispersed clients, the trainedglobal model can be used for incumbent user identification, facilitatingspectrum sharing. However, in this distributed learning system, the presence ofmalicious clients introduces the risk of poisoning the training data tomanipulate the global model through falsified local model exchanges. To addressthis challenge, a proactive defense mechanism is employed in this paper to makeinformed decisions regarding the admission or rejection of clientsparticipating in FL systems. Consequently, the attack-defense interactions aremodeled as a game, centered around the underlying admission and poisoningdecisions. First, performance bounds are established, encompassing the best andworst strategies for attackers and defenders. Subsequently, the attack anddefense utilities are characterized within the Nash equilibrium, where noplayer can unilaterally improve its performance given the fixed strategies ofothers. The results offer insights into novel operational modes that safeguardFL systems against poisoning attacks by quantifying the performance of bothattacks and defenses in the context of NextG communications.</description><author>Yalin E. Sagduyu, Tugba Erpek, Yi Shi</author><pubDate>Thu, 28 Dec 2023 17:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17164v1</guid></item><item><title>Roots and Requirements for Collaborative AIs</title><link>http://arxiv.org/abs/2303.12040v6</link><description>The vision of AI collaborators is a staple of mythology and science fiction,where artificial agents with special talents assist human partners and teams.In this dream, sophisticated AIs understand nuances of collaboration and humancommunication. The AI as collaborator dream is different from computer toolsthat augment human intelligence (IA) or intermediate human collaboration. Suchtools have their roots in the 1960s and helped to drive an informationtechnology revolution. They can be useful but they are not intelligent and donot collaborate as effectively as skilled people. With the increase of hybridand remote work since the COVID pandemic, the benefits and requirements forbetter coordination, collaboration, and communication are becoming a hot topicin the workplace. Employers and workers face choices and trade-offs as theynegotiate the options for working from home versus working at the office. Manyfactors such as the high costs of homes near employers are impeding a massreturn to the office. Government advisory groups and leaders in AI haveadvocated for years that AIs should be transparent and effective collaborators.Nonetheless, robust AIs that collaborate like talented people remain out ofreach. Are AI teammates part of a solution? How artificially intelligent (AI)could and should they be? This position paper reviews the arc of technology andpublic calls for human-machine teaming. It draws on earlier research inpsychology and the social sciences about what human-like collaborationrequires. This paper sets a context for a second science-driven paper thatadvocates a radical shift in technology and methodology for creating resilient,intelligent, and human-compatible AIs (Stefik &amp; Price, 2023). The aspirationalgoal is that such AIs would learn, share what they learn, and collaborate toachieve high capabilities.</description><author>Mark Stefik</author><pubDate>Thu, 28 Dec 2023 17:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12040v6</guid></item><item><title>FENet: Focusing Enhanced Network for Lane Detection</title><link>http://arxiv.org/abs/2312.17163v1</link><description>Inspired by human driving focus, this research pioneers networks augmentedwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPNarchitecture and Directional IoU Loss - targeted innovations addressingobstacles to precise lane detection for autonomous driving. Experimentsdemonstrate our Focusing Sampling strategy, emphasizing vital distant detailsunlike uniform approaches, significantly boosts both benchmark and practicalcurved/distant lane recognition accuracy essential for safety. While FENetV1achieves state-of-the-art conventional metric performance via enhancementsisolating perspective-aware contexts mimicking driver vision, FENetV2 provesmost reliable on the proposed Partial Field analysis. Hence we specificallyrecommend V2 for practical lane navigation despite fractional degradation onstandard entire-image measures. Future directions include collecting on-roaddata and integrating complementary dual frameworks to further breakthroughsguided by human perception principles. Code will be made available.</description><author>Liman Wang, Hanyang Zhong</author><pubDate>Thu, 28 Dec 2023 17:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17163v1</guid></item><item><title>Function-Space Regularization in Neural Networks: A Probabilistic Perspective</title><link>http://arxiv.org/abs/2312.17162v1</link><description>Parameter-space regularization in neural network optimization is afundamental tool for improving generalization. However, standardparameter-space regularization methods make it challenging to encode explicitpreferences about desired predictive functions into neural network training. Inthis work, we approach regularization in neural networks from a probabilisticperspective and show that by viewing parameter-space regularization asspecifying an empirical prior distribution over the model parameters, we canderive a probabilistically well-motivated regularization technique that allowsexplicitly encoding information about desired predictive functions into neuralnetwork training. This method -- which we refer to as function-space empiricalBayes (FSEB) -- includes both parameter- and function-space regularization, ismathematically simple, easy to implement, and incurs only minimal computationaloverhead compared to standard regularization techniques. We evaluate theutility of this regularization technique empirically and demonstrate that theproposed method leads to near-perfect semantic shift detection,highly-calibrated predictive uncertainty estimates, successful task adaptionfrom pre-trained models, and improved generalization under covariate shift.</description><author>Tim G. J. Rudner, Sanyam Kapoor, Shikai Qiu, Andrew Gordon Wilson</author><pubDate>Thu, 28 Dec 2023 17:50:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17162v1</guid></item><item><title>Restoration by Generation with Constrained Priors</title><link>http://arxiv.org/abs/2312.17161v1</link><description>The inherent generative power of denoising diffusion models makes themwell-suited for image restoration tasks where the objective is to find theoptimal high-quality image within the generative space that closely resemblesthe input image. We propose a method to adapt a pretrained diffusion model forimage restoration by simply adding noise to the input image to be restored andthen denoise. Our method is based on the observation that the space of agenerative model needs to be constrained. We impose this constraint byfinetuning the generative model with a set of anchor images that capture thecharacteristics of the input image. With the constrained space, we can thenleverage the sampling strategy used for generation to do image restoration. Weevaluate against previous methods and show superior performances on multiplereal-world restoration datasets in preserving identity and image quality. Wealso demonstrate an important and practical application on personalizedrestoration, where we use a personal album as the anchor images to constrainthe generative space. This approach allows us to produce results thataccurately preserve high-frequency details, which previous works are unable todo. Project webpage: https://gen2res.github.io.</description><author>Zheng Ding, Xuaner Zhang, Zhuowen Tu, Zhihao Xia</author><pubDate>Thu, 28 Dec 2023 17:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17161v1</guid></item><item><title>The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion</title><link>http://arxiv.org/abs/2107.09133v4</link><description>In this work we explore the limiting dynamics of deep neural networks trainedwith stochastic gradient descent (SGD). As observed previously, long afterperformance has converged, networks continue to move through parameter space bya process of anomalous diffusion in which distance travelled grows as a powerlaw in the number of gradient updates with a nontrivial exponent. We reveal anintricate interaction between the hyperparameters of optimization, thestructure in the gradient noise, and the Hessian matrix at the end of trainingthat explains this anomalous diffusion. To build this understanding, we firstderive a continuous-time model for SGD with finite learning rates and batchsizes as an underdamped Langevin equation. We study this equation in thesetting of linear regression, where we can derive exact, analytic expressionsfor the phase space dynamics of the parameters and their instantaneousvelocities from initialization to stationarity. Using the Fokker-Planckequation, we show that the key ingredient driving these dynamics is not theoriginal training loss, but rather the combination of a modified loss, whichimplicitly regularizes the velocity, and probability currents, which causeoscillations in phase space. We identify qualitative and quantitativepredictions of this theory in the dynamics of a ResNet-18 model trained onImageNet. Through the lens of statistical physics, we uncover a mechanisticorigin for the anomalous limiting dynamics of deep neural networks trained withSGD.</description><author>Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie, Eshed Margalit, Hidenori Tanaka, Surya Ganguli, Daniel L. K. Yamins</author><pubDate>Thu, 28 Dec 2023 17:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.09133v4</guid></item><item><title>Replica Tree-based Federated Learning using Limited Data</title><link>http://arxiv.org/abs/2312.17159v1</link><description>Learning from limited data has been extensively studied in machine learning,considering that deep neural networks achieve optimal performance when trainedusing a large amount of samples. Although various strategies have been proposedfor centralized training, the topic of federated learning with small datasetsremains largely unexplored. Moreover, in realistic scenarios, such as settingswhere medical institutions are involved, the number of participating clients isalso constrained. In this work, we propose a novel federated learningframework, named RepTreeFL. At the core of the solution is the concept of areplica, where we replicate each participating client by copying its modelarchitecture and perturbing its local data distribution. Our approach enableslearning from limited data and a small number of clients by aggregating alarger number of models with diverse data distributions. Furthermore, weleverage the hierarchical structure of the client network (both original andvirtual), alongside the model diversity across replicas, and introduce adiversity-based tree aggregation, where replicas are combined in a tree-likemanner and the aggregation weights are dynamically updated based on the modeldiscrepancy. We evaluated our method on two tasks and two types of data, graphgeneration and image classification (binary and multi-class), with bothhomogeneous and heterogeneous model architectures. Experimental resultsdemonstrate the effectiveness and outperformance of RepTreeFL in settings whereboth data and clients are limited. Our code is available athttps://github.com/basiralab/RepTreeFL.</description><author>Ramona Ghilea, Islem Rekik</author><pubDate>Thu, 28 Dec 2023 17:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17159v1</guid></item><item><title>A Latent Space Correlation-Aware Autoencoder for Anomaly Detection in Skewed Data</title><link>http://arxiv.org/abs/2301.00462v2</link><description>Unsupervised learning-based anomaly detection in latent space has gainedimportance since discriminating anomalies from normal data becomes difficult inhigh-dimensional space. Both density estimation and distance-based methods todetect anomalies in latent space have been explored in the past. These methodsprove that retaining valuable properties of input data in latent space helps inthe better reconstruction of test data. Moreover, real-world sensor data isskewed and non-Gaussian in nature, making mean-based estimators unreliable forskewed data. Again, anomaly detection methods based on reconstruction errorrely on Euclidean distance, which does not consider useful correlationinformation in the feature space and also fails to accurately reconstruct thedata when it deviates from the training distribution. In this work, we addressthe limitations of reconstruction error-based autoencoders and propose akernelized autoencoder that leverages a robust form of Mahalanobis distance(MD) to measure latent dimension correlation to effectively detect both nearand far anomalies. This hybrid loss is aided by the principle of maximizing themutual information gain between the latent dimension and the high-dimensionalprior data space by maximizing the entropy of the latent space while preservinguseful correlation information of the original data in the low-dimensionallatent space. The multi-objective function has two goals -- it measurescorrelation information in the latent feature space in the form of robust MDdistance and simultaneously tries to preserve useful correlation informationfrom the original data space in the latent space by maximizing mutualinformation between the prior and latent space.</description><author>Padmaksha Roy</author><pubDate>Thu, 28 Dec 2023 17:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00462v2</guid></item><item><title>DreamGaussian4D: Generative 4D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.17142v1</link><description>Remarkable progress has been made in 4D content generation recently. However,existing methods suffer from long optimization time, lack of motioncontrollability, and a low level of detail. In this paper, we introduceDreamGaussian4D, an efficient 4D generation framework that builds on 4DGaussian Splatting representation. Our key insight is that the explicitmodeling of spatial transformations in Gaussian Splatting makes it moresuitable for the 4D generation setting compared with implicit representations.DreamGaussian4D reduces the optimization time from several hours to just a fewminutes, allows flexible control of the generated 3D motion, and producesanimated meshes that can be efficiently rendered in 3D engines.</description><author>Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu</author><pubDate>Thu, 28 Dec 2023 17:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17142v1</guid></item><item><title>InsActor: Instruction-driven Physics-based Characters</title><link>http://arxiv.org/abs/2312.17135v1</link><description>Generating animation of physics-based characters with intuitive control haslong been a desirable task with numerous applications. However, generatingphysically simulated animations that reflect high-level human instructionsremains a difficult problem due to the complexity of physical environments andthe richness of human language. In this paper, we present InsActor, aprincipled generative framework that leverages recent advancements indiffusion-based human motion models to produce instruction-driven animations ofphysics-based characters. Our framework empowers InsActor to capture complexrelationships between high-level human instructions and character motions byemploying diffusion policies for flexibly conditioned motion planning. Toovercome invalid states and infeasible state transitions in planned motions,InsActor discovers low-level skills and maps plans to latent skill sequences ina compact latent space. Extensive experiments demonstrate that InsActorachieves state-of-the-art results on various tasks, includinginstruction-driven motion generation and instruction-driven waypoint heading.Notably, the ability of InsActor to generate physically simulated animationsusing high-level human instructions makes it a valuable tool, particularly inexecuting long-horizon tasks with a rich set of instructions.</description><author>Jiawei Ren, Mingyuan Zhang, Cunjun Yu, Xiao Ma, Liang Pan, Ziwei Liu</author><pubDate>Thu, 28 Dec 2023 17:10:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17135v1</guid></item><item><title>ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe</title><link>http://arxiv.org/abs/2312.17133v1</link><description>We present ARTrackV2, which integrates two pivotal aspects of tracking:determining where to look (localization) and how to describe (appearanceanalysis) the target object across video frames. Building on the foundation ofits predecessor, ARTrackV2 extends the concept by introducing a unifiedgenerative framework to "read out" object's trajectory and "retell" itsappearance in an autoregressive manner. This approach fosters a time-continuousmethodology that models the joint evolution of motion and visual features,guided by previous estimates. Furthermore, ARTrackV2 stands out for itsefficiency and simplicity, obviating the less efficient intra-frameautoregression and hand-tuned parameters for appearance updates. Despite itssimplicity, ARTrackV2 achieves state-of-the-art performance on prevailingbenchmark datasets while demonstrating remarkable efficiency improvement. Inparticular, ARTrackV2 achieves AO score of 79.5\% on GOT-10k, and AUC of 86.1\%on TrackingNet while being $3.6 \times$ faster than ARTrack. The code will bereleased.</description><author>Yifan Bai, Zeyang Zhao, Yihong Gong, Xing Wei</author><pubDate>Thu, 28 Dec 2023 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17133v1</guid></item><item><title>Large Language Model for Causal Decision Making</title><link>http://arxiv.org/abs/2312.17122v1</link><description>Large Language Models (LLMs) have shown their success in languageunderstanding and reasoning on general topics. However, their capability toinference based on user-specified structured data and knowledge in corpus-rareconcepts like causal decision-making is still limited. In this work, we explorethe possibility of fine-tuning an open-sourced LLM into LLM4Causal, which canidentify the causal task, execute a corresponding function, and interpret itsnumerical results based on users' queries and the provided dataset. Meanwhile,we propose a data generation process for more controllable GPT prompting andpresent two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causalproblem identification and input parameter extraction for causal functioncalling and (2) Causal-Interpret-Bench for in-context causal interpretation.With three case studies, we showed that LLM4Causal can deliver end-to-endsolutions for causal problems and provide easy-to-understand answers. Numericalstudies also reveal that it has a remarkable ability to identify the correctcausal task given a query.</description><author>Haitao Jiang, Lin Ge, Yuhe Gao, Jianian Wang, Rui Song</author><pubDate>Thu, 28 Dec 2023 16:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17122v1</guid></item><item><title>Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math</title><link>http://arxiv.org/abs/2312.17120v1</link><description>High-quality, large-scale corpora are the cornerstone of building foundationmodels. In this work, we introduce \textsc{MathPile}, a diverse andhigh-quality math-centric corpus comprising about 9.5 billion tokens.Throughout its creation, we adhered to the principle of ``\emph{less ismore}'', firmly believing in the supremacy of data quality over quantity, evenin the pre-training phase. Our meticulous data collection and processingefforts included a complex suite of preprocessing, prefiltering, languageidentification, cleaning, filtering, and deduplication, ensuring the highquality of our corpus. Furthermore, we performed data contamination detectionon downstream benchmark test sets to eliminate duplicates. We hope our\textsc{MathPile} can help to enhance the mathematical reasoning abilities oflanguage models. We plan to open-source different versions of \mathpile withthe scripts used for processing, to facilitate future developments in thisfield.</description><author>Zengzhi Wang, Rui Xia, Pengfei Liu</author><pubDate>Thu, 28 Dec 2023 16:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17120v1</guid></item><item><title>Fully Sparse 3D Panoptic Occupancy Prediction</title><link>http://arxiv.org/abs/2312.17118v1</link><description>Occupancy prediction plays a pivotal role in the realm of autonomous driving.Previous methods typically constructs a dense 3D volume, neglecting theinherent sparsity of the scene, which results in a high computational cost.Furthermore, these methods are limited to semantic occupancy and fail todifferentiate between distinct instances. To exploit the sparsity property andensure instance-awareness, we introduce a novel fully sparse panoptic occupancynetwork, termed SparseOcc. SparseOcc initially reconstructs a sparse 3Drepresentation from visual inputs. Subsequently, it employs sparse instancequeries to predict each object instance from the sparse 3D representation.These instance queries interact with 2D features via mask-guided sparsesampling, thereby circumventing the need for costly dense features or globalattention. Additionally, we have established the first-ever vision-centricpanoptic occupancy benchmark. SparseOcc demonstrates its efficacy on theOcc3D-nus dataset by achieving a mean Intersection over Union (mIoU) of 26.0,while maintaining a real-time inference speed of 25.4 FPS. By incorporatingtemporal modeling from the preceding 8 frames, SparseOcc further improves itsperformance, achieving 30.9 mIoU without whistles and bells. Code will be madeavailable.</description><author>Haisong Liu, Haiguang Wang, Yang Chen, Zetong Yang, Jia Zeng, Li Chen, Limin Wang</author><pubDate>Thu, 28 Dec 2023 16:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17118v1</guid></item><item><title>Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos</title><link>http://arxiv.org/abs/2312.17117v1</link><description>Temporal Sentence Grounding (TSG), which aims to localize moments from videosbased on the given natural language queries, has attracted widespreadattention. Existing works are mainly designed for short videos, failing tohandle TSG in long videos, which poses two challenges: i) complicated contextsin long videos require temporal reasoning over longer moment sequences, and ii)multiple modalities including textual speech with rich information requirespecial designs for content understanding in long videos. To tackle thesechallenges, in this work we propose a Grounding-Prompter method, which iscapable of conducting TSG in long videos through prompting LLM with multimodalinformation. In detail, we first transform the TSG task and its multimodalinputs including speech and visual, into compressed task textualization.Furthermore, to enhance temporal reasoning under complicated contexts, aBoundary-Perceptive Prompting strategy is proposed, which contains three folds:i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combineglobal and local semantics with noise filtering step by step, ii) we set upvalidity principles capable of constraining LLM to generate reasonablepredictions following specific formats, and iii) we introduce one-shotIn-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLMin TSG task understanding. Experiments demonstrate the state-of-the-artperformance of our Grounding-Prompter method, revealing the benefits ofprompting LLM with multimodal information for TSG in long videos.</description><author>Houlun Chen, Xin Wang, Hong Chen, Zihan Song, Jia Jia, Wenwu Zhu</author><pubDate>Thu, 28 Dec 2023 16:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17117v1</guid></item><item><title>Generalizable Visual Reinforcement Learning with Segment Anything Model</title><link>http://arxiv.org/abs/2312.17116v1</link><description>Learning policies that can generalize to unseen environments is a fundamentalchallenge in visual reinforcement learning (RL). While most current methodsfocus on acquiring robust visual representations through auxiliary supervision,pre-training, or data augmentation, the potential of modern vision foundationmodels remains underleveraged. In this work, we introduce Segment AnythingModel for Generalizable visual RL (SAM-G), a novel framework that leverages thepromptable segmentation ability of Segment Anything Model (SAM) to enhance thegeneralization capabilities of visual RL agents. We utilize image features fromDINOv2 and SAM to find correspondence as point prompts to SAM, and then SAMproduces high-quality masked images for agents directly. Evaluated across 8DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visualgeneralization ability without altering the RL agents' architecture but merelytheir observations. Notably, SAM-G achieves 44% and 29% relative improvementson the challenging video hard setting on DMControl and Adroit respectively,compared to state-of-the-art methods. Video and code:https://yanjieze.com/SAM-G/</description><author>Ziyu Wang, Yanjie Ze, Yifei Sun, Zhecheng Yuan, Huazhe Xu</author><pubDate>Thu, 28 Dec 2023 16:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17116v1</guid></item><item><title>How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation</title><link>http://arxiv.org/abs/2312.17115v1</link><description>Human behavior simulation of AI agents necessitates the agents to possess aquality of believability, which is crucial as it facilitates users inestablishing trust toward the agents and streamlines the fulfillment of theagents' goal. While recent advancements in Large Language Model (LLM) basedagents have improved human behavior simulation, challenges inherent to LLMs(e.g., long context modeling) can undermine their believability. Consequently,evaluating AI agent believability becomes imperative. Unfortunately, priorresearch often neglects the negative impacts of LLM deficiencies. To addressthese gaps, we introduce two metrics for assessing LLM-based agentbelievability: consistency, and robustness, together with a benchmark,SimulateBench, with which, we evaluate the consistency and robustness of agentsimplemented with popular LLMs. We find that agents (i) struggle to accuratelydepict character information when presented with lengthy profile inputs; (ii)exhibit vulnerability to profile perturbations; and (iii) are significantlyaffected by certain key factors that impact their overall believability. Codeand SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.</description><author>Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, Pengfei Liu</author><pubDate>Thu, 28 Dec 2023 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17115v1</guid></item><item><title>Symmetry Breaking in Symmetric Tensor Decomposition</title><link>http://arxiv.org/abs/2103.06234v2</link><description>In this note, we consider the highly nonconvex optimization problemassociated with computing the rank decomposition of symmetric tensors. Weformulate the invariance properties of the loss function and show that criticalpoints detected by standard gradient based methods are \emph{symmetry breaking}with respect to the target tensor. The phenomena, seen for different choices oftarget tensors and norms, make possible the use of recently developed analyticand algebraic tools for studying nonconvex optimization landscapes exhibitingsymmetry breaking phenomena of similar nature.</description><author>Yossi Arjevani, Joan Bruna, Michael Field, Joe Kileel, Matthew Trager, Francis Williams</author><pubDate>Thu, 28 Dec 2023 16:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.06234v2</guid></item><item><title>Is ChatGPT a Good Personality Recognizer? A Preliminary Study</title><link>http://arxiv.org/abs/2307.03952v3</link><description>In recent years, personality has been regarded as a valuable personal factorbeing incorporated into numerous tasks such as sentiment analysis and productrecommendation. This has led to widespread attention to text-based personalityrecognition task, which aims to identify an individual's personality based ongiven text. Considering that ChatGPT has recently exhibited remarkableabilities on various natural language processing tasks, we provide apreliminary evaluation of ChatGPT on text-based personality recognition taskfor generating effective personality data. Concretely, we employ a variety ofprompting strategies to explore ChatGPT's ability in recognizing personalityfrom given text, especially the level-oriented prompting strategy we designedfor guiding ChatGPT in analyzing given text at a specified level. Theexperimental results on two representative real-world datasets reveal thatChatGPT with zero-shot chain-of-thought prompting exhibits impressivepersonality recognition ability and is capable to provide natural languageexplanations through text-based logical reasoning. Furthermore, by employingthe level-oriented prompting strategy to optimize zero-shot chain-of-thoughtprompting, the performance gap between ChatGPT and correspondingstate-of-the-art model has been narrowed even more. However, we observe thatChatGPT shows unfairness towards certain sensitive demographic attributes suchas gender and age. Additionally, we discover that eliciting the personalityrecognition ability of ChatGPT helps improve its performance onpersonality-related downstream tasks such as sentiment classification andstress prediction.</description><author>Yu Ji, Wen Wu, Hong Zheng, Yi Hu, Xi Chen, Liang He</author><pubDate>Thu, 28 Dec 2023 16:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03952v3</guid></item><item><title>Online Tensor Inference</title><link>http://arxiv.org/abs/2312.17111v1</link><description>Recent technological advances have led to contemporary applications thatdemand real-time processing and analysis of sequentially arriving tensor data.Traditional offline learning, involving the storage and utilization of all datain each computational iteration, becomes impractical for high-dimensionaltensor data due to its voluminous size. Furthermore, existing low-rank tensormethods lack the capability for statistical inference in an online fashion,which is essential for real-time predictions and informed decision-making. Thispaper addresses these challenges by introducing a novel online inferenceframework for low-rank tensor learning. Our approach employs StochasticGradient Descent (SGD) to enable efficient real-time data processing withoutextensive memory requirements, thereby significantly reducing computationaldemands. We establish a non-asymptotic convergence result for the onlinelow-rank SGD estimator, nearly matches the minimax optimal rate of estimationerror in offline models that store all historical data. Building upon thisfoundation, we propose a simple yet powerful online debiasing approach forsequential statistical inference in low-rank tensor learning. The entire onlineprocedure, covering both estimation and inference, eliminates the need for datasplitting or storing historical data, making it suitable for on-the-flyhypothesis testing. Given the sequential nature of our data collection,traditional analyses relying on offline methods and sample splitting areinadequate. In our analysis, we control the sum of constructedsuper-martingales to ensure estimates along the entire solution path remainwithin the benign region. Additionally, a novel spectral representation tool isemployed to address statistical dependencies among iterative estimates,establishing the desired asymptotic normality.</description><author>Xin Wen, Will Wei Sun, Yichen Zhang</author><pubDate>Thu, 28 Dec 2023 16:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17111v1</guid></item><item><title>Cumulative Regret Analysis of the Piyavskii--Shubert Algorithm and Its Variants for Global Optimization</title><link>http://arxiv.org/abs/2108.10859v2</link><description>We study the problem of global optimization, where we analyze the performanceof the Piyavskii--Shubert algorithm and its variants. For any given timeduration $T$, instead of the extensively studied simple regret (which is thedifference of the losses between the best estimate up to $T$ and the globalminimum), we study the cumulative regret up to time $T$. For $L$-Lipschitzcontinuous functions, we show that the cumulative regret is $O(L\log T)$. For$H$-Lipschitz smooth functions, we show that the cumulative regret is $O(H)$.We analytically extend our results for functions with Holder continuousderivatives, which cover both the Lipschitz continuous and the Lipschitz smoothfunctions, individually. We further show that a simpler variant of thePiyavskii-Shubert algorithm performs just as well as the traditional variantsfor the Lipschitz continuous or the Lipschitz smooth functions. We furtherextend our results to broader classes of functions, and show that, ouralgorithm efficiently determines its queries; and achieves nearly minimaxoptimal (up to log factors) cumulative regret, for general convex or evenconcave regularity conditions on the extrema of the objective (whichencompasses many preceding regularities). We consider further extensions byinvestigating the performance of the Piyavskii-Shubert variants in thescenarios with unknown regularity, noisy evaluation and multivariate domain.</description><author>Kaan Gokcesu, Hakan Gokcesu</author><pubDate>Thu, 28 Dec 2023 16:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.10859v2</guid></item><item><title>MIVC: Multiple Instance Visual Component for Visual-Language Models</title><link>http://arxiv.org/abs/2312.17109v1</link><description>Vision-language models have been widely explored across a wide range of tasksand achieve satisfactory performance. However, it's under-explored how toconsolidate entity understanding through a varying number of images and toalign it with the pre-trained language models for generative tasks. In thispaper, we propose MIVC, a general multiple instance visual component to bridgethe gap between various image inputs with off-the-shelf vision-language modelsby aggregating visual representations in a permutation-invariant fashionthrough a neural network. We show that MIVC could be plugged into thevisual-language models to improve the model performance consistently on visualquestion answering, classification and captioning tasks on a public availablee-commerce dataset with multiple images per product. Furthermore, we show thatthe component provides insight into the contribution of each image to thedownstream tasks.</description><author>Wenyi Wu, Qi Li, Wenliang Zhong, Junzhou Huang</author><pubDate>Thu, 28 Dec 2023 16:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17109v1</guid></item><item><title>Geometry-Biased Transformer for Robust Multi-View 3D Human Pose Reconstruction</title><link>http://arxiv.org/abs/2312.17106v1</link><description>We address the challenges in estimating 3D human poses from multiple viewsunder occlusion and with limited overlapping views. We approach multi-view,single-person 3D human pose reconstruction as a regression problem and proposea novel encoder-decoder Transformer architecture to estimate 3D poses frommulti-view 2D pose sequences. The encoder refines 2D skeleton joints detectedacross different views and times, fusing multi-view and temporal informationthrough global self-attention. We enhance the encoder by incorporating ageometry-biased attention mechanism, effectively leveraging geometricrelationships between views. Additionally, we use detection scores provided bythe 2D pose detector to further guide the encoder's attention based on thereliability of the 2D detections. The decoder subsequently regresses the 3Dpose sequence from these refined tokens, using pre-defined queries for eachjoint. To enhance the generalization of our method to unseen scenes and improveresilience to missing joints, we implement strategies including scenecentering, synthetic views, and token dropout. We conduct extensive experimentson three benchmark public datasets, Human3.6M, CMU Panoptic andOcclusion-Persons. Our results demonstrate the efficacy of our approach,particularly in occluded scenes and when few views are available, which aretraditionally challenging scenarios for triangulation-based methods.</description><author>Olivier Moliner, Sangxia Huang, Kalle Åström</author><pubDate>Thu, 28 Dec 2023 16:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17106v1</guid></item><item><title>Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes</title><link>http://arxiv.org/abs/2305.08777v2</link><description>Background: Injection drug use (IDU) is a dangerous health behavior thatincreases mortality and morbidity. Identifying IDU early and initiating harmreduction interventions can benefit individuals at risk. However, extractingIDU behaviors from patients' electronic health records (EHR) is difficultbecause there is no International Classification of Disease (ICD) code and theonly place IDU information can be indicated is unstructured free-text clinicalnotes. Although natural language processing can efficiently extract thisinformation from unstructured data, there are no validated tools. Methods: Toaddress this gap in clinical information, we design and demonstrate aquestion-answering (QA) framework to extract information on IDU from clinicalnotes. Our framework involves two main steps: (1) generating a gold-standard QAdataset and (2) developing and testing the QA model. We utilize 2323 clinicalnotes of 1145 patients sourced from the VA Corporate Data Warehouse toconstruct the gold-standard dataset for developing and evaluating the QA model.We also demonstrate the QA model's ability to extract IDU-related informationon temporally out-of-distribution data. Results: Here we show that for a strictmatch between gold-standard and predicted answers, the QA model achieves 51.65%F1 score. For a relaxed match between the gold-standard and predicted answers,the QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%Recall scores. Moreover, the QA model demonstrates consistent performance whensubjected to temporally out-of-distribution data. Conclusions: Our studyintroduces a QA framework designed to extract IDU information from clinicalnotes, aiming to enhance the accurate and efficient detection of people whoinject drugs, extract relevant information, and ultimately facilitate informedpatient care.</description><author>Maria Mahbub, Ian Goethert, Ioana Danciu, Kathryn Knight, Sudarshan Srinivasan, Suzanne Tamang, Karine Rozenberg-Ben-Dror, Hugo Solares, Susana Martins, Jodie Trafton, Edmon Begoli, Gregory Peterson</author><pubDate>Thu, 28 Dec 2023 16:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08777v2</guid></item><item><title>TSPP: A Unified Benchmarking Tool for Time-series Forecasting</title><link>http://arxiv.org/abs/2312.17100v1</link><description>Recently there has been increasing interest in developing and deploying deepgraph learning algorithms for many tasks, such as fraud detection andrecommender systems. Albeit, there is a limited number of publicly availablegraph-structured datasets, most of which are tiny compared to production-sizedapplications or are limited in their application domain. This work tackles thisshortcoming by proposing a scalable synthetic graph generation tool to scalethe datasets to production-size graphs with trillions of edges and billions ofnodes. The tool learns a series of parametric models from proprietary datasetsthat can be released to researchers to study various graph methods on thesynthetic data increasing prototype development and novel applications. Wedemonstrate the generalizability of the framework across a series of datasets,mimicking structural and feature distributions as well as the ability to scalethem across varying sizes demonstrating their usefulness for benchmarking andmodel development.</description><author>Jan Bączek, Dmytro Zhylko, Gilberto Titericz, Sajad Darabi, Jean-Francois Puget, Izzy Putterman, Dawid Majchrowski, Anmol Gupta, Kyle Kranen, Pawel Morkisz</author><pubDate>Thu, 28 Dec 2023 16:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17100v1</guid></item><item><title>Spectral Persistent Homology: Persistence Signals</title><link>http://arxiv.org/abs/2312.17093v1</link><description>In this paper, we present a novel family of descriptors for persistencediagrams, reconceptualizing them as signals in $\mathbb R^2_+$. This marks asignificant advancement in Topological Data Analysis. Our methodologytransforms persistence diagrams into a finite-dimensional vector space throughfunctionals of the discrete measures induced by these diagrams. While our focusis primarily on frequency-based transformations, we do not restrict ourapproach exclusively to this types of techniques. We term this family oftransformations as $Persistence$ $Signals$ and prove stability for some membersof this family against the 1-$Kantorovitch$-$Rubinstein$ metric, ensuring itsresponsiveness to subtle data variations. Extensive comparative analysisreveals that our descriptor performs competitively with the currentstate-of-art from the topological data analysis literature, and oftensurpasses, the existing methods. This research not only introduces agroundbreaking perspective for data scientists but also establishes afoundation for future innovations in applying persistence diagrams in dataanalysis and machine learning.</description><author>Michael Etienne Van Huffel, Matteo Palo</author><pubDate>Thu, 28 Dec 2023 16:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17093v1</guid></item><item><title>Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels</title><link>http://arxiv.org/abs/2312.17090v1</link><description>The explosion of visual content available online underscores the requirementfor an accurate machine assessor to robustly evaluate scores across diversetypes of visual contents. While recent studies have demonstrated theexceptional potentials of large multi-modality models (LMMs) on a wide range ofrelated fields, in this work, we explore how to teach them for visual ratingaligned with human opinions. Observing that human raters only learn and judgediscrete text-defined levels in subjective studies, we propose to emulate thissubjective process and teach LMMs with text-defined rating levels instead ofscores. The proposed Q-Align achieves state-of-the-art performance on imagequality assessment (IQA), image aesthetic assessment (IAA), as well as videoquality assessment (VQA) tasks under the original LMM structure. With thesyllabus, we further unify the three tasks into one model, termed the OneAlign.In our experiments, we demonstrate the advantage of the discrete-level-basedsyllabus over direct-score-based variants for LMMs. Our code and thepre-trained weights are released at https://github.com/Q-Future/Q-Align.</description><author>Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, Weisi Lin</author><pubDate>Thu, 28 Dec 2023 16:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17090v1</guid></item><item><title>SVIT: Scaling up Visual Instruction Tuning</title><link>http://arxiv.org/abs/2307.04087v3</link><description>Thanks to the emerging of foundation models, the large language and visionmodels are integrated to acquire the multimodal ability of visual captioning,question answering, etc. Although existing multimodal models present impressiveperformance of visual understanding and reasoning, their limits are stilllargely under-explored due to the scarcity of high-quality instruction tuningdata. To push the limits of multimodal capability, we Scale up VisualInstruction Tuning (SVIT) by constructing a dataset of 4.2 million visualinstruction tuning data including 1.6M conversation question-answer (QA) pairs,1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailedimage descriptions. Besides the volume, the proposed dataset is also featuredby the high quality and rich diversity, which is generated by prompting GPT-4with the abundant manual annotations of images. We also propose a new datarecipe to select subset with better diversity and balance, which evokes model'ssuperior capabilities. Extensive experiments verify that SVIT-v1.5, trained onthe proposed dataset, outperforms state-of-the-art Multimodal Large LanguageModels on popular benchmarks. The data and code are publicly available athttps://github.com/BAAI-DCAI/Visual-Instruction-Tuning.</description><author>Bo Zhao, Boya Wu, Muyang He, Tiejun Huang</author><pubDate>Thu, 28 Dec 2023 16:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04087v3</guid></item><item><title>Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs</title><link>http://arxiv.org/abs/2312.17080v1</link><description>In this work, we introduce a novel evaluation paradigm for Large LanguageModels, one that challenges them to engage in meta-reasoning. This approachaddresses critical shortcomings in existing math problem-solving benchmarks,traditionally used to evaluate the cognitive capabilities of agents. Ourparadigm shifts the focus from result-oriented assessments, which oftenoverlook the reasoning process, to a more holistic evaluation that effectivelydifferentiates the cognitive capabilities among models. For example, in ourbenchmark, GPT-4 demonstrates a performance ten times more accurate thanGPT3-5. The significance of this new paradigm lies in its ability to revealpotential cognitive deficiencies in LLMs that current benchmarks, such asGSM8K, fail to uncover due to their saturation and lack of effectivedifferentiation among varying reasoning abilities. Our comprehensive analysisincludes several state-of-the-art math models from both open-source andclosed-source communities, uncovering fundamental deficiencies in theirtraining and evaluation approaches. This paper not only advocates for aparadigm shift in the assessment of LLMs but also contributes to the ongoingdiscourse on the trajectory towards Artificial General Intelligence (AGI). Bypromoting the adoption of meta-reasoning evaluation methods similar to ours, weaim to facilitate a more accurate assessment of the true cognitive abilities ofLLMs.</description><author>Zhongshen Zeng, Pengguang Chen, Haiyun Jiang, Jiaya Jia</author><pubDate>Thu, 28 Dec 2023 15:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17080v1</guid></item><item><title>Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding</title><link>http://arxiv.org/abs/2306.08832v3</link><description>Vision-Language Models (VLMs), such as CLIP, exhibit strong image-textcomprehension abilities, facilitating advances in several downstream tasks suchas zero-shot image classification, image-text retrieval, and text-to-imagegeneration. However, the compositional reasoning abilities of existing VLMsremains subpar. The root of this limitation lies in the inadequate alignmentbetween the images and captions in the pretraining datasets. Additionally, thecurrent contrastive learning objective fails to focus on fine-grained groundingcomponents like relations, actions, and attributes, resulting in "bag-of-words"representations. We introduce a simple and effective method to improvecompositional reasoning in VLMs. Our method better leverages available datasetsby refining and expanding the standard image-text contrastive learningframework. Our approach does not require specific annotations and does notincur extra parameters. When integrated with CLIP, our technique yields notableimprovement over state-of-the-art baselines across five vision-languagecompositional benchmarks. We open-source our code athttps://github.com/lezhang7/Enhance-FineGrained.</description><author>Le Zhang, Rabiul Awal, Aishwarya Agrawal</author><pubDate>Thu, 28 Dec 2023 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08832v3</guid></item><item><title>A Comprehensive Evaluation of Tool-Assisted Generation Strategies</title><link>http://arxiv.org/abs/2310.10062v2</link><description>A growing area of research investigates augmenting language models with tools(e.g., search engines, calculators) to overcome their shortcomings (e.g.,missing or incorrect knowledge, incorrect logical inferences). Various few-shottool-usage strategies have been proposed. However, there is no systematic andfair comparison across different strategies, or between these strategies andstrong baselines that do not leverage tools. We conduct an extensive empiricalanalysis, finding that (1) across various datasets, example difficulty levels,and models, strong no-tool baselines are competitive to tool-assistedstrategies, implying that effectively using tools with in-contextdemonstrations is a difficult unsolved problem; (2) for knowledge-retrievaltasks, strategies that *refine* incorrect outputs with tools outperformstrategies that retrieve relevant information *ahead of* or *duringgeneration*; (3) tool-assisted strategies are expensive in the number of tokensthey require to work -- incurring additional costs by orders of magnitude --which does not translate into significant improvement in performance. Overall,our findings suggest that few-shot tool integration is still an open challenge,emphasizing the need for comprehensive evaluations of future strategies toaccurately assess their *benefits* and *costs*.</description><author>Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva</author><pubDate>Thu, 28 Dec 2023 15:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10062v2</guid></item><item><title>Approximating nonlinear functions with latent boundaries in low-rank excitatory-inhibitory spiking networks</title><link>http://arxiv.org/abs/2307.09334v3</link><description>Deep feedforward and recurrent rate-based neural networks have becomesuccessful functional models of the brain, but they neglect obvious biologicaldetails such as spikes and Dale's law. Here we argue that these details arecrucial in order to understand how real neural circuits operate. Towards thisaim, we put forth a new framework for spike-based computation in low-rankexcitatory-inhibitory spiking networks. By considering populations with rank-1connectivity, we cast each neuron's spiking threshold as a boundary in alow-dimensional input-output space. We then show how the combined thresholds ofa population of inhibitory neurons form a stable boundary in this space, andthose of a population of excitatory neurons form an unstable boundary.Combining the two boundaries results in a rank-2 excitatory-inhibitory (EI)network with inhibition-stabilized dynamics at the intersection of the twoboundaries. The computation of the resulting networks can be understood as thedifference of two convex functions and is thereby capable of approximatingarbitrary non-linear input-output mappings. We demonstrate several propertiesof these networks, including noise suppression and amplification, irregularactivity and synaptic balance, as well as how they relate to rate networkdynamics in the limit that the boundary becomes soft. Finally, while our workfocuses on small networks (5-50 neurons), we discuss potential avenues forscaling up to much larger networks. Overall, our work proposes a newperspective on spiking networks that may serve as a starting point for amechanistic understanding of biological spike-based computation.</description><author>William F. Podlaski, Christian K. Machens</author><pubDate>Thu, 28 Dec 2023 15:40:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09334v3</guid></item><item><title>An Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation</title><link>http://arxiv.org/abs/2312.17072v1</link><description>Online to offline recommendation strongly correlates with the user andservice's spatiotemporal information, therefore calling for a higher degree ofmodel personalization. The traditional methodology is based on a uniform modelstructure trained by collected centralized data, which is unlikely to captureall user patterns over different geographical areas or time periods. To tacklethis challenge, we propose a geographical group-specific modeling method calledGeoGrouse, which simultaneously studies the common knowledge as well asgroup-specific knowledge of user preferences. An automatic grouping paradigm isemployed and verified based on users' geographical grouping indicators. Offlineand online experiments are conducted to verify the effectiveness of ourapproach, and substantial business improvement is achieved.</description><author>Luo Ji, Jiayu Mao, Hailong Shi, Qian Li, Yunfei Chu, Hongxia Yang</author><pubDate>Thu, 28 Dec 2023 15:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17072v1</guid></item><item><title>SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation</title><link>http://arxiv.org/abs/2312.17071v1</link><description>Recent real-time semantic segmentation methods usually adopt an additionalsemantic branch to pursue rich long-range context. However, the additionalbranch incurs undesirable computational overhead and slows inference speed. Toeliminate this dilemma, we propose SCTNet, a single branch CNN with transformersemantic information for real-time segmentation. SCTNet enjoys the richsemantic representations of an inference-free semantic branch while retainingthe high efficiency of lightweight single branch CNN. SCTNet utilizes atransformer as the training-only semantic branch considering its superb abilityto extract long-range context. With the help of the proposed transformer-likeCNN block CFBlock and the semantic information alignment module, SCTNet couldcapture the rich semantic information from the transformer branch in training.During the inference, only the single branch CNN needs to be deployed. Weconduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, andthe results show that our method achieves the new state-of-the-art performance.The code and model is available at https://github.com/xzz777/SCTNet</description><author>Zhengze Xu, Dongyue Wu, Changqian Yu, Xiangxiang Chu, Nong Sang, Changxin Gao</author><pubDate>Thu, 28 Dec 2023 15:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17071v1</guid></item><item><title>Large-scale Long-tailed Disease Diagnosis on Radiology Images</title><link>http://arxiv.org/abs/2312.16151v2</link><description>In this study, we aim to investigate the problem of large-scale,large-vocabulary disease classification for radiologic images, which can beformulated as a multi-modal, multi-anatomy, multi-label, long-tailedclassification. Our main contributions are three folds: (i), on datasetconstruction, we build up an academically accessible, large-scale diagnosticdataset that encompasses 5568 disorders linked with 930 unique ICD-10-CM codes,containing 39,026 cases (192,675 scans). (ii), on model design, we present anovel architecture that enables to process arbitrary number of input scans,from various imaging modalities, which is trained with knowledge enhancement toleverage the rich domain knowledge; (iii), on evaluation, we initialize a newbenchmark for multi-modal multi-anatomy long-tailed diagnosis. Our method showssuperior results on it. Additionally, our final model serves as a pre-trainedmodel, and can be finetuned to benefit diagnosis on various external datasets.</description><author>Qiaoyu Zheng, Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 28 Dec 2023 15:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16151v2</guid></item><item><title>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue</title><link>http://arxiv.org/abs/2308.03549v3</link><description>Recent advances in Large Language Models (LLMs) have achieved remarkablebreakthroughs in understanding and responding to user intents. However, theirperformance lag behind general use cases in some expertise domains, such asChinese medicine. Existing efforts to incorporate Chinese medicine into LLMsrely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialoguedata. These models lack the ability for doctor-like proactive inquiry andmulti-turn comprehension and cannot align responses with experts' intentions.In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLMthat implements an entire training pipeline from continuous pre-training, SFT,to Reinforcement Learning from Human Feedback (RLHF). Additionally, weconstruct a Chinese multi-turn medical dialogue dataset of 70,000 authenticdoctor-patient dialogues, CMtMedQA, which significantly enhances the model'scapability for complex dialogue and proactive inquiry initiation. We alsodefine a refined annotation rule and evaluation criteria given the uniquecharacteristics of the biomedical domain. Extensive experimental results showthat Zhongjing outperforms baselines in various capacities and matches theperformance of ChatGPT in some abilities, despite the 100x parameters. Ablationstudies also demonstrate the contributions of each component: pre-trainingenhances medical knowledge, and RLHF further improves instruction-followingability and safety. Our code, datasets, and models are available athttps://github.com/SupritYoung/Zhongjing.</description><author>Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan</author><pubDate>Thu, 28 Dec 2023 15:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03549v3</guid></item><item><title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</title><link>http://arxiv.org/abs/2310.10477v3</link><description>The rapid development of large language models (LLMs) has not only providednumerous opportunities but also presented significant challenges. This becomesparticularly evident when LLMs inadvertently generate harmful or toxic content,either unintentionally or because of intentional inducement. Existing alignmentmethods usually direct LLMs toward favorable outcomes by utilizinghuman-annotated, flawless instruction-response pairs. Conversely, this studyproposes a novel alignment technique based on mistake analysis, whichdeliberately exposes LLMs to erroneous content to learn the reasons formistakes and how to avoid them. In this case, mistakes are repurposed intovaluable data for alignment, effectively helping to avoid the production oferroneous responses. Without external models or human annotations, our methodleverages a model's intrinsic ability to discern undesirable mistakes andimproves the safety of its generated responses. Experimental results revealthat our method outperforms existing alignment approaches in enhancing modelsafety while maintaining the overall utility.</description><author>Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</author><pubDate>Thu, 28 Dec 2023 15:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10477v3</guid></item><item><title>United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit</title><link>http://arxiv.org/abs/2310.11077v2</link><description>Deep neural networks have become the method of choice for solving manyclassification tasks, largely because they can fit very complex functionsdefined over raw data. The downside of such powerful learners is the danger ofoverfit. In this paper, we introduce a novel ensemble classifier for deepnetworks that effectively overcomes overfitting by combining models generatedat specific intermediate epochs during training. Our method allows for theincorporation of useful knowledge obtained by the models during the overfittingphase without deterioration of the general performance, which is usually missedwhen early stopping is used. To motivate this approach, we begin with thetheoretical analysis of a regression model, whose prediction -- that thevariance among classifiers increases when overfit occurs -- is demonstratedempirically in deep networks in common use. Guided by these results, weconstruct a new ensemble-based prediction method, where the prediction isdetermined by the class that attains the most consensual prediction throughoutthe training epochs. Using multiple image and text classification datasets, weshow that when regular ensembles suffer from overfit, our method eliminates theharmful reduction in generalization due to overfit, and often even surpassesthe performance obtained by early stopping. Our method is easy to implement andcan be integrated with any training scheme and architecture, without additionalprior knowledge beyond the training set. It is thus a practical and useful toolto overcome overfit. Code is available athttps://github.com/uristern123/United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit.</description><author>Uri Stern, Daniel Shwartz, Daphna Weinshall</author><pubDate>Thu, 28 Dec 2023 15:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11077v2</guid></item><item><title>On the Robustness of Decision-Focused Learning</title><link>http://arxiv.org/abs/2311.16487v3</link><description>Decision-Focused Learning (DFL) is an emerging learning paradigm that tacklesthe task of training a machine learning (ML) model to predict missingparameters of an incomplete optimization problem, where the missing parametersare predicted. DFL trains an ML model in an end-to-end system, by integratingthe prediction and optimization tasks, providing better alignment of thetraining and testing objectives. DFL has shown a lot of promise and holds thecapacity to revolutionize decision-making in many real-world applications.However, very little is known about the performance of these models underadversarial attacks. We adopt ten unique DFL methods and benchmark theirperformance under two distinctly focused attacks adapted towards thePredict-then-Optimize problem setting. Our study proposes the hypothesis thatthe robustness of a model is highly correlated with its ability to findpredictions that lead to optimal decisions without deviating from theground-truth label. Furthermore, we provide insight into how to target themodels that violate this condition and show how these models responddifferently depending on the achieved optimality at the end of their trainingcycles.</description><author>Yehya Farhat</author><pubDate>Thu, 28 Dec 2023 15:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16487v3</guid></item><item><title>Improving In-context Learning via Bidirectional Alignment</title><link>http://arxiv.org/abs/2312.17055v1</link><description>Large language models (LLMs) have shown impressive few-shot generalization onmany tasks via in-context learning (ICL). Despite their success in showing suchemergent abilities, the scale and complexity of larger models also lead tounprecedentedly high computational demands and deployment challenges. Inreaction, researchers explore transferring the powerful capabilities of largermodels to more efficient and compact models by typically aligning the output ofsmaller models with that of larger models. Existing methods either trainsmaller models on the generated outputs of larger models or to imitate theirtoken-level probability distributions. However, these distillation methods paylittle to no attention to the input part, which also plays a crucial role inICL. Based on the finding that the performance of ICL is highly sensitive tothe selection of demonstration examples, we propose Bidirectional Alignment(BiAlign) to fully leverage the models' preferences for ICL examples to improvethe ICL abilities of smaller models. Specifically, we introduce the alignmentof input preferences between smaller and larger models by incorporating a novelranking loss, in addition to aligning the token-level output distribution. Withextensive experiments and analysis, we demonstrate that BiAlign canconsistently outperform existing baselines on a variety of tasks includinglanguage understanding, reasoning, and coding.</description><author>Chengwei Qin, Wenhan Xia, Fangkai Jiao, Shafiq Joty</author><pubDate>Thu, 28 Dec 2023 15:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17055v1</guid></item><item><title>AccidentGPT: Accident Analysis and Prevention from V2X Environmental Perception with Multi-modal Large Model</title><link>http://arxiv.org/abs/2312.13156v2</link><description>Traffic accidents, being a significant contributor to both human casualtiesand property damage, have long been a focal point of research for many scholarsin the field of traffic safety. However, previous studies, whether focusing onstatic environmental assessments or dynamic driving analyses, as well aspre-accident predictions or post-accident rule analyses, have typically beenconducted in isolation. There has been a lack of an effective framework fordeveloping a comprehensive understanding and application of traffic safety. Toaddress this gap, this paper introduces AccidentGPT, a comprehensive accidentanalysis and prevention multi-modal large model. AccidentGPT establishes amulti-modal information interaction framework grounded in multi-sensorperception, thereby enabling a holistic approach to accident analysis andprevention in the field of traffic safety. Specifically, our capabilities canbe categorized as follows: for autonomous driving vehicles, we providecomprehensive environmental perception and understanding to control the vehicleand avoid collisions. For human-driven vehicles, we offer proactive long-rangesafety warnings and blind-spot alerts while also providing safety drivingrecommendations and behavioral norms through human-machine dialogue andinteraction. Additionally, for traffic police and management agencies, ourframework supports intelligent and real-time analysis of traffic safety,encompassing pedestrian, vehicles, roads, and the environment throughcollaborative perception from multiple vehicles and road testing devices. Thesystem is also capable of providing a thorough analysis of accident causes andliability after vehicle collisions. Our framework stands as the first largemodel to integrate comprehensive scene understanding into traffic safetystudies. Project page: https://accidentgpt.github.io</description><author>Lening Wang, Han Jiang, Pinlong Cai, Daocheng Fu, Tianqi Wang, Zhiyong Cui, Yilong Ren, Haiyang Yu, Xuesong Wang, Hanchu Zhou, Helai Huang, Yinhai Wang</author><pubDate>Thu, 28 Dec 2023 15:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13156v2</guid></item><item><title>Multi-Attention Fusion Drowsy Driving Detection Model</title><link>http://arxiv.org/abs/2312.17052v1</link><description>Drowsy driving represents a major contributor to traffic accidents, and theimplementation of driver drowsy driving detection systems has been proven tosignificantly reduce the occurrence of such accidents. Despite the developmentof numerous drowsy driving detection algorithms, many of them impose specificprerequisites such as the availability of complete facial images, optimallighting conditions, and the use of RGB images. In our study, we introduce anovel approach called the Multi-Attention Fusion Drowsy Driving Detection Model(MAF). MAF is aimed at significantly enhancing classification performance,especially in scenarios involving partial facial occlusion and low lightingconditions. It accomplishes this by capitalizing on the local featureextraction capabilities provided by multi-attention fusion, thereby enhancingthe algorithm's overall robustness. To enhance our dataset, we collectedreal-world data that includes both occluded and unoccluded faces captured undernighttime and daytime lighting conditions. We conducted a comprehensive seriesof experiments using both publicly available datasets and our self-built data.The results of these experiments demonstrate that our proposed model achievesan impressive driver drowsiness detection accuracy of 96.8%.</description><author>Shulei QU, Zhenguo Gao, Xiaoxiao Wu, Yuanyuan Qiu</author><pubDate>Thu, 28 Dec 2023 14:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17052v1</guid></item><item><title>FILP-3D: Enhancing 3D Few-shot Class-incremental Learning with Pre-trained Vision-Language Models</title><link>http://arxiv.org/abs/2312.17051v1</link><description>Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophicforgetting issue when a model is incrementally trained on limited data. Whilethe Contrastive Vision-Language Pre-Training (CLIP) model has been effective inaddressing 2D few/zero-shot learning tasks, its direct application to 3D FSCILfaces limitations. These limitations arise from feature space misalignment andsignificant noise in real-world scanned 3D data. To address these challenges,we introduce two novel components: the Redundant Feature Eliminator (RFE) andthe Spatial Noise Compensator (SNC). RFE aligns the feature spaces of inputpoint clouds and their embeddings by performing a unique dimensionalityreduction on the feature space of pre-trained models (PTMs), effectivelyeliminating redundant information without compromising semantic integrity. Onthe other hand, SNC is a graph-based 3D model designed to capture robustgeometric information within point clouds, thereby augmenting the knowledgelost due to projection, particularly when processing real-world scanned data.Considering the imbalance in existing 3D datasets, we also propose newevaluation metrics that offer a more nuanced assessment of a 3D FSCIL model.Traditional accuracy metrics are proved to be biased; thus, our metrics focuson the model's proficiency in learning new classes while maintaining thebalance between old and new classes. Experimental results on both established3D FSCIL benchmarks and our dataset demonstrate that our approach significantlyoutperforms existing state-of-the-art methods.</description><author>Wan Xu, Tianyu Huang, Tianyu Qu, Guanglei Yang, Yiwen Guo, Wangmeng Zuo</author><pubDate>Thu, 28 Dec 2023 14:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17051v1</guid></item><item><title>KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching</title><link>http://arxiv.org/abs/2312.17050v1</link><description>Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)based SR by utilizing the telephoto image (Ref) to assist the super-resolutionof the low-resolution wide-angle image (LR input). Different from generalRefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)area. However, current dual-lens SR methods rarely utilize these specificcharacteristics and directly perform dense matching between the LR input andRef. Due to the resolution gap between LR and Ref, the matching may miss thebest-matched candidate and destroy the consistent structures in the overlappedFoV area. Different from them, we propose to first align the Ref with thecenter region (namely the overlapped FoV area) of the LR input by combiningglobal warping and local warping to make the aligned Ref be sharp andconsistent. Then, we formulate the aligned Ref and LR center as value-keypairs, and the corner region of the LR is formulated as queries. In this way,we propose a kernel-free matching strategy by matching between the LR-corner(query) and LR-center (key) regions, and the corresponding aligned Ref (value)can be warped to the corner region of the target. Our kernel-free matchingstrategy avoids the resolution gap between LR and Ref, which makes our networkhave better generalization ability. In addition, we construct a DuSR-Realdataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.Experiments on three datasets demonstrate that our method outperforms thesecond-best method by a large margin. Our code and dataset are available athttps://github.com/Craigie-Hill/KeDuSR.</description><author>Huanjing Yue, Zifan Cui, Kun Li, Jingyu Yang</author><pubDate>Thu, 28 Dec 2023 14:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17050v1</guid></item><item><title>Inconsistency of cross-validation for structure learning in Gaussian graphical models</title><link>http://arxiv.org/abs/2312.17047v1</link><description>Despite numerous years of research into the merits and trade-offs of variousmodel selection criteria, obtaining robust results that elucidate the behaviorof cross-validation remains a challenging endeavor. In this paper, we highlightthe inherent limitations of cross-validation when employed to discern thestructure of a Gaussian graphical model. We provide finite-sample bounds on theprobability that the Lasso estimator for the neighborhood of a node within aGaussian graphical model, optimized using a prediction oracle, misidentifiesthe neighborhood. Our results pertain to both undirected and directed acyclicgraphs, encompassing general, sparse covariance structures. To support ourtheoretical findings, we conduct an empirical investigation of thisinconsistency by contrasting our outcomes with other commonly used informationcriteria through an extensive simulation study. Given that many algorithmsdesigned to learn the structure of graphical models require hyperparameterselection, the precise calibration of this hyperparameter is paramount foraccurately estimating the inherent structure. Consequently, our observationsshed light on this widely recognized practical challenge.</description><author>Zhao Lyu, Wai Ming Tai, Mladen Kolar, Bryon Aragam</author><pubDate>Thu, 28 Dec 2023 14:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17047v1</guid></item><item><title>On the Principle of Least Symmetry Breaking in Shallow ReLU Models</title><link>http://arxiv.org/abs/1912.11939v3</link><description>We consider the optimization problem associated with fitting two-layer ReLUnetworks with respect to the squared loss, where labels are assumed to begenerated by a target network. Focusing first on standard Gaussian inputs, weshow that the structure of spurious local minima detected by stochasticgradient descent (SGD) is, in a well-defined sense, the \emph{least loss ofsymmetry} with respect to the target weights. A closer look at the analysisindicates that this principle of least symmetry breaking may apply to a broaderrange of settings. Motivated by this, we conduct a series of experiments whichcorroborate this hypothesis for different classes of non-isotropic non-productdistributions, smooth activation functions and networks with a few layers.</description><author>Yossi Arjevani, Michael Field</author><pubDate>Thu, 28 Dec 2023 14:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1912.11939v3</guid></item><item><title>Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding</title><link>http://arxiv.org/abs/2312.17044v1</link><description>Transformer has taken the natural language processing (NLP) field by stormsince birth, owing to its superior ability to model complex dependencies insequences. Despite the great success of pretrained language models (PLMs) basedon Transformer across almost all NLP tasks, they all suffer from a presetlength limit and thus can hardly extend this success to longer sequences beyondseen data, namely the length extrapolation problem. Length extrapolation hasaroused great interest among researchers, as it is the core feature of humanlanguage capacity. To enhance length extrapolation of Transformers, a plethoraof methods have been proposed, mostly focusing on extrapolatable positionencodings. In this article, we provide an organized and systematical review ofthese research efforts in a unified notation from a position encodingperspective, aiming to enable the reader to gain a deep understanding ofexisting methods and provide stimuli for future research.</description><author>Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, Ting Liu</author><pubDate>Thu, 28 Dec 2023 14:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17044v1</guid></item><item><title>Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs</title><link>http://arxiv.org/abs/2310.11094v2</link><description>The infrequent occurrence of overfit in deep neural networks is perplexing.On the one hand, theory predicts that as models get larger they shouldeventually become too specialized for a specific training set, with ensuingdecrease in generalization. In contrast, empirical results in imageclassification indicate that increasing the training time of deep models orusing bigger models almost never hurts generalization. Is it because the way wemeasure overfit is too limited? Here, we introduce a novel score forquantifying overfit, which monitors the forgetting rate of deep models onvalidation data. Presumably, this score indicates that even whilegeneralization improves overall, there are certain regions of the data spacewhere it deteriorates. When thus measured, we show that overfit can occur withand without a decrease in validation accuracy, and may be more common thanpreviously appreciated. This observation may help to clarify the aforementionedconfusing picture. We use our observations to construct a new ensemble method,based solely on the training history of a single network, which providessignificant improvement in performance without any additional cost in trainingtime. An extensive empirical evaluation with modern deep models shows ourmethod's utility on multiple datasets, neural networks architectures andtraining schemes, both when training from scratch and when using pre-trainednetworks in transfer learning. Notably, our method outperforms comparablemethods while being easier to implement and use, and further improves theperformance of competitive networks on Imagenet by 1%.</description><author>Uri Stern, Daphna Weinshall</author><pubDate>Thu, 28 Dec 2023 14:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11094v2</guid></item><item><title>Self-Supervised Learning for Few-Shot Bird Sound Classification</title><link>http://arxiv.org/abs/2312.15824v2</link><description>Self-supervised learning (SSL) in audio holds significant potential acrossvarious domains, particularly in situations where abundant, unlabeled data isreadily available at no cost. This is particularly pertinent in bioacoustics,where biologists routinely collect extensive sound datasets from the naturalenvironment. In this study, we demonstrate that SSL is capable of acquiringmeaningful representations of bird sounds from audio recordings without theneed for annotations. Our experiments showcase that these learnedrepresentations exhibit the capacity to generalize to new bird species infew-shot learning (FSL) scenarios. Additionally, we show that selecting windowswith high bird activation for self-supervised learning, using a pretrainedaudio neural network, significantly enhances the quality of the learnedrepresentations.</description><author>Ilyass Moummad, Romain Serizel, Nicolas Farrugia</author><pubDate>Thu, 28 Dec 2023 14:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15824v2</guid></item><item><title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title><link>http://arxiv.org/abs/2209.15224v2</link><description>Unsupervised learning has been widely used in many real-world applications.One of the simplest and most important unsupervised learning models is theGaussian mixture model (GMM). In this work, we study the multi-task learningproblem on GMMs, which aims to leverage potentially similar GMM parameterstructures among tasks to obtain improved learning performance compared tosingle-task learning. We propose a multi-task GMM learning procedure based onthe EM algorithm that not only can effectively utilize unknown similaritybetween related tasks but is also robust against a fraction of outlier tasksfrom arbitrary distributions. The proposed procedure is shown to achieveminimax optimal rate of convergence for both parameter estimation error and theexcess mis-clustering error, in a wide range of regimes. Moreover, wegeneralize our approach to tackle the problem of transfer learning for GMMs,where similar theoretical results are derived. Finally, we demonstrate theeffectiveness of our methods through simulations and real data examples. To thebest of our knowledge, this is the first work studying multi-task and transferlearning on GMMs with theoretical guarantees.</description><author>Ye Tian, Haolei Weng, Yang Feng</author><pubDate>Thu, 28 Dec 2023 14:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.15224v2</guid></item><item><title>AI Powered Road Network Prediction with Multi-Modal Data</title><link>http://arxiv.org/abs/2312.17040v1</link><description>This study presents an innovative approach for automatic road detection withdeep learning, by employing fusion strategies for utilizing bothlower-resolution satellite imagery and GPS trajectory data, a concept neverexplored before. We rigorously investigate both early and late fusionstrategies, and assess deep learning based road detection performance usingdifferent fusion settings. Our extensive ablation studies assess the efficacyof our framework under diverse model architectures, loss functions, andgeographic domains (Istanbul and Montreal). For an unbiased and completeevaluation of road detection results, we use both region-based andboundary-based evaluation metrics for road segmentation. The outcomes revealthat the ResUnet model outperforms U-Net and D-Linknet in road extractiontasks, achieving superior results over the benchmark study using low-resolutionSentinel-2 data. This research not only contributes to the field of automaticroad detection but also offers novel insights into the utilization of datafusion methods in diverse applications.</description><author>Necip Enes Gengec, Ergin Tari, Ulas Bagci</author><pubDate>Thu, 28 Dec 2023 14:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17040v1</guid></item><item><title>An Evaluation of Machine Learning Approaches for Early Diagnosis of Autism Spectrum Disorder</title><link>http://arxiv.org/abs/2309.11646v2</link><description>Autistic Spectrum Disorder (ASD) is a neurological disease characterized bydifficulties with social interaction, communication, and repetitive activities.While its primary origin lies in genetics, early detection is crucial, andleveraging machine learning offers a promising avenue for a faster and morecost-effective diagnosis. This study employs diverse machine learning methodsto identify crucial ASD traits, aiming to enhance and automate the diagnosticprocess. We study eight state-of-the-art classification models to determinetheir effectiveness in ASD detection. We evaluate the models using accuracy,precision, recall, specificity, F1-score, area under the curve (AUC), kappa,and log loss metrics to find the best classifier for these binary datasets.Among all the classification models, for the children dataset, the SVM and LRmodels achieve the highest accuracy of 100% and for the adult dataset, the LRmodel produces the highest accuracy of 97.14%. Our proposed ANN model providesthe highest accuracy of 94.24% for the new combined dataset whenhyperparameters are precisely tuned for each model. As almost allclassification models achieve high accuracy which utilize true labels, webecome interested in delving into five popular clustering algorithms tounderstand model behavior in scenarios without true labels. We calculateNormalized Mutual Information (NMI), Adjusted Rand Index (ARI), and SilhouetteCoefficient (SC) metrics to select the best clustering models. Our evaluationfinds that spectral clustering outperforms all other benchmarking clusteringmodels in terms of NMI and ARI metrics while demonstrating comparability to theoptimal SC achieved by k-means. The implemented code is available at GitHub.</description><author>Rownak Ara Rasul, Promy Saha, Diponkor Bala, S M Rakib Ul Karim, Md. Ibrahim Abdullah, Bishwajit Saha</author><pubDate>Thu, 28 Dec 2023 14:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11646v2</guid></item><item><title>Generalized Mask-aware IoU for Anchor Assignment for Real-time Instance Segmentation</title><link>http://arxiv.org/abs/2312.17031v1</link><description>This paper introduces Generalized Mask-aware Intersection-over-Union (GmaIoU)as a new measure for positive-negative assignment of anchor boxes duringtraining of instance segmentation methods. Unlike conventional IoU measure orits variants, which only consider the proximity of anchor and ground-truthboxes; GmaIoU additionally takes into account the segmentation mask. Thisenables GmaIoU to provide more accurate supervision during training. Wedemonstrate the effectiveness of GmaIoU by replacing IoU with our GmaIoU inATSS, a state-of-the-art (SOTA) assigner. Then, we train YOLACT, a real-timeinstance segmentation method, using our GmaIoU-based ATSS assigner. Theresulting YOLACT based on the GmaIoU assigner outperforms (i) ATSS with IoU by$\sim 1.0-1.5$ mask AP, (ii) YOLACT with a fixed IoU threshold assigner by$\sim 1.5-2$ mask AP over different image sizes and (iii) decreases theinference time by $25 \%$ owing to using less anchors. Taking advantage of thisefficiency, we further devise GmaYOLACT, a faster and $+7$ mask AP points moreaccurate detector than YOLACT. Our best model achieves $38.7$ mask AP at $26$fps on COCO test-dev establishing a new state-of-the-art for real-time instancesegmentation.</description><author>Barış Can Çam, Kemal Öksüz, Fehmi Kahraman, Zeynep Sonat Baltacı, Sinan Kalkan, Emre Akbaş</author><pubDate>Thu, 28 Dec 2023 14:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17031v1</guid></item><item><title>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</title><link>http://arxiv.org/abs/2304.05977v4</link><description>We present a comprehensive solution to learn and improve text-to-image modelsfrom human preference feedback. To begin with, we build ImageReward -- thefirst general-purpose text-to-image human preference reward model -- toeffectively encode human preferences. Its training is based on our systematicannotation pipeline including rating and ranking, which collects 137k expertcomparisons to date. In human evaluation, ImageReward outperforms existingscoring models and metrics, making it a promising automatic metric forevaluating text-to-image synthesis. On top of it, we propose Reward FeedbackLearning (ReFL), a direct tuning algorithm to optimize diffusion models againsta scorer. Both automatic and human evaluation support ReFL's advantages overcompared methods. All code and datasets are provided at\url{https://github.com/THUDM/ImageReward}.</description><author>Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong</author><pubDate>Thu, 28 Dec 2023 14:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05977v4</guid></item><item><title>Learning Multi-axis Representation in Frequency Domain for Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.17030v1</link><description>Recently, Visual Transformer (ViT) has been extensively used in medical imagesegmentation (MIS) due to applying self-attention mechanism in the spatialdomain to modeling global knowledge. However, many studies have focused onimproving models in the spatial domain while neglecting the importance offrequency domain information. Therefore, we propose Multi-axis External WeightsUNet (MEW-UNet) based on the U-shape architecture by replacing self-attentionin ViT with our Multi-axis External Weights block. Specifically, our blockperforms a Fourier transform on the three axes of the input features andassigns the external weight in the frequency domain, which is generated by ourExternal Weights Generator. Then, an inverse Fourier transform is performed tochange the features back to the spatial domain. We evaluate our model on fourdatasets, including Synapse, ACDC, ISIC17 and ISIC18 datasets, and our approachdemonstrates competitive performance, owing to its effective utilization offrequency domain information.</description><author>Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Suncheng Xiang</author><pubDate>Thu, 28 Dec 2023 14:12:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17030v1</guid></item><item><title>FedSDD: Scalable and Diversity-enhanced Distillation for Model Aggregation in Federated Learning</title><link>http://arxiv.org/abs/2312.17029v1</link><description>Recently, innovative model aggregation methods based on knowledgedistillation (KD) have been proposed for federated learning (FL). These methodsnot only improved the robustness of model aggregation over heterogeneouslearning environment, but also allowed training heterogeneous models on clientdevices. However, the scalability of existing methods is not satisfactory,because the training cost on the server increases with the number of clients,which limits their application in large scale systems. Furthermore, theensemble of existing methods is built from a set of client models initializedfrom the same checkpoint, causing low diversity. In this paper, we propose ascalable and diversity-enhanced federated distillation scheme, FedSDD, whichdecouples the training complexity from the number of clients to enhance thescalability, and builds the ensemble from a set of aggregated models withenhanced diversity. In particular, the teacher model in FedSDD is an ensemblebuilt by a small group of aggregated (global) models, instead of all clientmodels, such that the computation cost will not scale with the number ofclients. Furthermore, to enhance diversity, FedSDD only performs KD to enhanceone of the global models, i.e., the \textit{main global model}, which improvesthe performance of both the ensemble and the main global model. Whilepartitioning client model into more groups allow building an ensemble with moreaggregated models, the convergence of individual aggregated models will be slowdown. We introduce the temporal ensembling which leverage the issues, andprovide significant improvement with the heterogeneous settings. Experimentresults show that FedSDD outperforms other FL methods, including FedAvg andFedDF, on the benchmark datasets.</description><author>Ho Man Kwan, Shenghui Song</author><pubDate>Thu, 28 Dec 2023 14:10:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17029v1</guid></item><item><title>DSAC-T: Distributional Soft Actor-Critic with Three Refinements</title><link>http://arxiv.org/abs/2310.05858v4</link><description>Reinforcement learning (RL) has proven to be highly effective in tacklingcomplex decision-making and control tasks. However, prevalent model-free RLmethods often face severe performance degradation due to the well-knownoverestimation issue. In response to this problem, we recently introduced anoff-policy RL algorithm, called distributional soft actor-critic (DSAC orDSAC-v1), which can effectively improve the value estimation accuracy bylearning a continuous Gaussian value distribution. Nonetheless, standard DSAChas its own shortcomings, including occasionally unstable learning processesand the necessity for task-specific reward scaling, which may hinder itsoverall performance and adaptability in some special tasks. This paper furtherintroduces three important refinements to standard DSAC in order to addressthese shortcomings. These refinements consist of expected value substituting,twin value distribution learning, and variance-based critic gradient adjusting.The modified RL algorithm is named as DSAC with three refinements (DSAC-T orDSAC-v2), and its performances are systematically evaluated on a diverse set ofbenchmark tasks. Without any task-specific hyperparameter tuning, DSAC-Tsurpasses or matches a lot of mainstream model-free RL algorithms, includingSAC, TD3, DDPG, TRPO, and PPO, in all tested environments. Additionally,DSAC-T, unlike its standard version, ensures a highly stable learning processand delivers similar performance across varying reward scales.</description><author>Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, Shengbo Eben Li</author><pubDate>Thu, 28 Dec 2023 14:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05858v4</guid></item><item><title>DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2311.17812v3</link><description>Following language instructions to navigate in unseen environments is achallenging task for autonomous embodied agents. With strong representationcapabilities, pretrained vision-and-language models are widely used in VLN.However, most of them are trained on web-crawled general-purpose datasets,which incurs a considerable domain gap when used for VLN tasks. To address theproblem, we propose a novel and model-agnostic domain-aware prompt learning(DAP) framework. For equipping the pretrained models with specific object-leveland scene-level cross-modal alignment in VLN tasks, DAP applies a low-costprompt tuning paradigm to learn soft visual prompts for extracting in-domainimage semantics. Specifically, we first generate a set of in-domain image-textpairs with the help of the CLIP model. Then we introduce soft visual prompts inthe input space of the visual encoder in a pretrained model. DAP injectsin-domain visual knowledge into the visual encoder of the pretrained model inan efficient way. Experimental results on both R2R and REVERIE show thesuperiority of DAP compared to existing state-of-the-art methods.</description><author>Ting Liu, Yue Hu, Wansen Wu, Youkai Wang, Kai Xu, Quanjun Yin</author><pubDate>Thu, 28 Dec 2023 13:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17812v3</guid></item><item><title>Experiential Co-Learning of Software-Developing Agents</title><link>http://arxiv.org/abs/2312.17025v1</link><description>Recent advancements in large language models (LLMs) have brought significantchanges to various dimains, especially through LLM-driven autonomous agents.These agents are now capable of collaborating seamlessly, splitting tasks andenhancing accuracy, thus minimizing the need for human involvement. However,these agents often approach a diverse range of tasks in isolation, withoutbenefiting from past experiences. This isolation can lead to repeated mistakesand inefficient trials in task solving. To this end, this paper introducesExperiential Co-Learning, a novel framework in which instructor and assistantagents gather shortcut-oriented experiences from their historical trajectoriesand use these past experiences for mutual reasoning. This paradigm, enrichedwith previous experiences, equips agents to more effectively address unseentasks.</description><author>Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 28 Dec 2023 13:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17025v1</guid></item><item><title>Gasper: GrAph Signal ProcEssing in R</title><link>http://arxiv.org/abs/2007.10642v5</link><description>We present a short tutorial on to the use of the R gasper package. Gasper isa package dedicated to signal processing on graphs. It also provides aninterface to the SuiteSparse Matrix Collection.</description><author>Basile de Loynes, Fabien Navarro, Baptiste Olivier</author><pubDate>Thu, 28 Dec 2023 13:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.10642v5</guid></item><item><title>Efficient Learning of Long-Range and Equivariant Quantum Systems</title><link>http://arxiv.org/abs/2312.17019v1</link><description>In this work, we consider a fundamental task in quantum many-body physics -finding and learning ground states of quantum Hamiltonians and theirproperties. Recent works have studied the task of predicting the ground stateexpectation value of sums of geometrically local observables by learning fromdata. For short-range gapped Hamiltonians, a sample complexity that islogarithmic in the number of qubits and quasipolynomial in the error wasobtained. Here we extend these results beyond the local requirements on bothHamiltonians and observables, motivated by the relevance of long-rangeinteractions in molecular and atomic systems. For interactions decaying as apower law with exponent greater than twice the dimension of the system, werecover the same efficient logarithmic scaling with respect to the number ofqubits, but the dependence on the error worsens to exponential. Further, weshow that learning algorithms equivariant under the automorphism group of theinteraction hypergraph achieve a sample complexity reduction, leading inparticular to a constant number of samples for learning sums of localobservables in systems with periodic boundary conditions. We demonstrate theefficient scaling in practice by learning from DMRG simulations of $1$Dlong-range and disordered systems with up to $128$ qubits. Finally, we providean analysis of the concentration of expectation values of global observablesstemming from central limit theorem, resulting in increased predictionaccuracy.</description><author>Štěpán Šmíd, Roberto Bondesan</author><pubDate>Thu, 28 Dec 2023 13:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17019v1</guid></item><item><title>Learning Spatially Collaged Fourier Bases for Implicit Neural Representation</title><link>http://arxiv.org/abs/2312.17018v1</link><description>Existing approaches to Implicit Neural Representation (INR) can beinterpreted as a global scene representation via a linear combination ofFourier bases of different frequencies. However, such universal basis functionscan limit the representation capability in local regions where a specificcomponent is unnecessary, resulting in unpleasant artifacts. To this end, weintroduce a learnable spatial mask that effectively dispatches distinct Fourierbases into respective regions. This translates into collaging Fourier patches,thus enabling an accurate representation of complex signals. Comprehensiveexperiments demonstrate the superior reconstruction quality of the proposedapproach over existing baselines across various INR tasks, including imagefitting, video representation, and 3D shape representation. Our methodoutperforms all other baselines, improving the image fitting PSNR by over 3dBand 3D reconstruction to 98.81 IoU and 0.0011 Chamfer Distance.</description><author>Jason Chun Lok Li, Chang Liu, Binxiao Huang, Ngai Wong</author><pubDate>Thu, 28 Dec 2023 13:36:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17018v1</guid></item><item><title>Robust Multi-Modal Image Stitching for Improved Scene Understanding</title><link>http://arxiv.org/abs/2312.17010v1</link><description>Multi-modal image stitching can be a difficult feat. That's why, in thispaper, we've devised a unique and comprehensive image-stitching pipeline thattaps into OpenCV's stitching module. Our approach integrates feature-basedmatching, transformation estimation, and blending techniques to bring aboutpanoramic views that are of top-tier quality - irrespective of lighting, scaleor orientation differences between images. We've put our pipeline to the testwith a varied dataset and found that it's very effective in enhancing sceneunderstanding and finding real-world applications.</description><author>Aritra Dutta, Dr. G Suseela, Asmita Sood</author><pubDate>Thu, 28 Dec 2023 13:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17010v1</guid></item><item><title>Revisiting the Reliability of Psychological Scales on Large Language Models</title><link>http://arxiv.org/abs/2305.19926v3</link><description>Recent research has extended beyond assessing the performance of LargeLanguage Models (LLMs) to examining their characteristics from a psychologicalstandpoint, acknowledging the necessity of understanding their behavioralcharacteristics. The administration of personality tests to LLMs has emerged asa noteworthy area in this context. However, the suitability of employingpsychological scales, initially devised for humans, on LLMs is a matter ofongoing debate. Our study aims to determine the reliability of applyingpersonality assessments to LLMs, explicitly investigating whether LLMsdemonstrate consistent personality traits. Analyzing responses under 2,500settings reveals that gpt-3.5-turbo shows consistency in responses to the BigFive Inventory, indicating a high degree of reliability. Furthermore, ourresearch explores the potential of gpt-3.5-turbo to emulate diversepersonalities and represent various groups, which is a capability increasinglysought after in social sciences for substituting human participants with LLMsto reduce costs. Our findings reveal that LLMs have the potential to representdifferent personalities with specific prompt instructions. By shedding light onthe personalization of LLMs, our study endeavors to pave the way for futureexplorations in this field. We have made our experimental results and thecorresponding code openly accessible viahttps://github.com/CUHK-ARISE/LLMPersonality.</description><author>Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu</author><pubDate>Thu, 28 Dec 2023 13:21:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19926v3</guid></item><item><title>On the rate of convergence of an over-parametrized Transformer classifier learned by gradient descent</title><link>http://arxiv.org/abs/2312.17007v1</link><description>One of the most recent and fascinating breakthroughs in artificialintelligence is ChatGPT, a chatbot which can simulate human conversation.ChatGPT is an instance of GPT4, which is a language model based on generativegredictive gransformers. So if one wants to study from a theoretical point ofview, how powerful such artificial intelligence can be, one approach is toconsider transformer networks and to study which problems one can solve withthese networks theoretically. Here it is not only important what kind of modelsthese network can approximate, or how they can generalize their knowledgelearned by choosing the best possible approximation to a concrete data set, butalso how well optimization of such transformer network based on concrete dataset works. In this article we consider all these three different aspectssimultaneously and show a theoretical upper bound on the missclassificationprobability of a transformer network fitted to the observed data. Forsimplicity we focus in this context on transformer encoder networks which canbe applied to define an estimate in the context of a classification probleminvolving natural language.</description><author>Michael Kohler, Adam Krzyzak</author><pubDate>Thu, 28 Dec 2023 13:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17007v1</guid></item></channel></rss>