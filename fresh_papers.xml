<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 22 Nov 2023 06:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models</title><link>http://arxiv.org/abs/2311.12796v1</link><description>3D reconstruction of dynamic scenes is a long-standing problem in computergraphics and increasingly difficult the less information is available.Shape-from-Template (SfT) methods aim to reconstruct a template-based geometryfrom RGB images or video sequences, often leveraging just a single monocularcamera without depth information, such as regular smartphone recordings.Unfortunately, existing reconstruction methods are either unphysical and noisyor slow in optimization. To solve this problem, we propose a novel SfTreconstruction algorithm for cloth using a pre-trained neural surrogate modelthat is fast to evaluate, stable, and produces smooth reconstructions due to aregularizing physics simulation. Differentiable rendering of the simulated meshenables pixel-wise comparisons between the reconstruction and a target videosequence that can be used for a gradient-based optimization procedure toextract not only shape information but also physical parameters such asstretching, shearing, or bending stiffness of the cloth. This allows to retaina precise, stable, and smooth reconstructed geometry while reducing the runtimeby a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-basedSfT approach.</description><author>David Stotko, Nils Wandel, Reinhard Klein</author><pubDate>Tue, 21 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12796v1</guid></item><item><title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</title><link>http://arxiv.org/abs/2311.12793v1</link><description>In the realm of large multi-modal models (LMMs), efficient modality alignmentis crucial yet often constrained by the scarcity of high-quality image-textdata. To address this bottleneck, we introduce the ShareGPT4V dataset, apioneering large-scale resource featuring 1.2 million highly descriptivecaptions, which surpasses existing datasets in diversity and informationcontent, covering world knowledge, object properties, spatial relationships,and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated100K high-quality captions collected from advanced GPT4-Vision and has beenexpanded to 1.2M with a superb caption model trained on this subset. ShareGPT4Vfirst demonstrates its effectiveness for the Supervised Fine-Tuning (SFT)phase, by substituting an equivalent quantity of detailed captions in existingSFT datasets with a subset of our high-quality captions, significantlyenhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MMEand MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-trainingand SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simplearchitecture that has remarkable performance across a majority of themulti-modal benchmarks. This project is available athttps://ShareGPT4V.github.io to serve as a pivotal resource for advancing theLMMs community.</description><author>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin</author><pubDate>Tue, 21 Nov 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12793v1</guid></item><item><title>Intrinsic Image Decomposition via Ordinal Shading</title><link>http://arxiv.org/abs/2311.12792v1</link><description>Intrinsic decomposition is a fundamental mid-level vision problem that playsa crucial role in various inverse rendering and computational photographypipelines. Generating highly accurate intrinsic decompositions is an inherentlyunder-constrained task that requires precisely estimating continuous-valuedshading and albedo. In this work, we achieve high-resolution intrinsicdecomposition by breaking the problem into two parts. First, we present a denseordinal shading formulation using a shift- and scale-invariant loss in order toestimate ordinal shading cues without restricting the predictions to obey theintrinsic model. We then combine low- and high-resolution ordinal estimationsusing a second network to generate a shading estimate with both globalcoherency and local details. We encourage the model to learn an accuratedecomposition by computing losses on the estimated shading as well as thealbedo implied by the intrinsic model. We develop a straightforward method forgenerating dense pseudo ground truth using our model's predictions andmulti-illumination data, enabling generalization to in-the-wild imagery. Wepresent an exhaustive qualitative and quantitative analysis of our predictedintrinsic components against state-of-the-art methods. Finally, we demonstratethe real-world applicability of our estimations by performing otherwisedifficult editing tasks such as recoloring and relighting.</description><author>Chris Careaga, Yağız Aksoy</author><pubDate>Tue, 21 Nov 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12792v1</guid></item><item><title>MaGIC: Multi-modality Guided Image Completion</title><link>http://arxiv.org/abs/2305.11818v2</link><description>Vanilla image completion approaches exhibit sensitivity to large missingregions, attributed to the limited availability of reference information forplausible generation. To mitigate this, existing methods incorporate the extracue as a guidance for image completion. Despite improvements, these approachesare often restricted to employing a single modality (e.g., segmentation orsketch maps), which lacks scalability in leveraging multi-modality for moreplausible completion. In this paper, we propose a novel, simple yet effectivemethod for Multi-modal Guided Image Completion, dubbed MaGIC, which not onlysupports a wide range of single modality as the guidance (e.g., text, cannyedge, sketch, segmentation, depth, and pose), but also adapts to arbitrarilycustomized combination of these modalities (i.e., arbitrary multi-modality) forimage completion. For building MaGIC, we first introduce a modality-specificconditional U-Net (MCU-Net) that injects single-modal signal into a U-Netdenoiser for single-modal guided image completion. Then, we devise a consistentmodality blending (CMB) method to leverage modality signals encoded in multiplelearned MCU-Nets through gradient guidance in latent space. Our CMB istraining-free, thereby avoids the cumbersome joint re-training of differentmodalities, which is the secret of MaGIC to achieve exceptional flexibility inaccommodating new modalities for completion. Experiments show the superiorityof MaGIC over state-of-the-art methods and its generalization to variouscompletion tasks. Our project with code and models is available atyeates.github.io/MaGIC-Page/.</description><author>Yongsheng Yu, Hao Wang, Tiejian Luo, Heng Fan, Libo Zhang</author><pubDate>Tue, 21 Nov 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11818v2</guid></item><item><title>PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs</title><link>http://arxiv.org/abs/2202.10793v5</link><description>Networks are ubiquitous in many real-world applications (e.g., socialnetworks encoding trust/distrust relationships, correlation networks arisingfrom time series data). While many networks are signed or directed, or both,there is a lack of unified software packages on graph neural networks (GNNs)specially designed for signed and directed networks. In this paper, we presentPyTorch Geometric Signed Directed (PyGSD), a software package which fills thisgap. Along the way, we evaluate the implemented methods with experiments with aview to providing insights into which method to choose for a given task. Thedeep learning framework consists of easy-to-use GNN models, synthetic andreal-world data, as well as task-specific evaluation metrics and loss functionsfor signed and directed networks. As an extension library for PyG, our proposedsoftware is maintained with open-source releases, detailed documentation,continuous integration, unit tests and code coverage checks. The GitHubrepository of the library ishttps://github.com/SherylHYX/pytorch_geometric_signed_directed.</description><author>Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, Gesine Reinert</author><pubDate>Tue, 21 Nov 2023 18:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.10793v5</guid></item><item><title>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</title><link>http://arxiv.org/abs/2311.12786v1</link><description>Fine-tuning large pre-trained models has become the de facto strategy fordeveloping both task-specific and general-purpose machine learning systems,including developing models that are safe to deploy. Despite its clearimportance, there has been minimal work that explains how fine-tuning altersthe underlying capabilities learned by a model during pretraining: doesfine-tuning yield entirely novel capabilities or does it just modulate existingones? We address this question empirically in synthetic, controlled settingswhere we can use mechanistic interpretability tools (e.g., network pruning andprobing) to understand how the model's underlying capabilities are changing. Weperform an extensive analysis of the effects of fine-tuning in these settings,and show that: (i) fine-tuning rarely alters the underlying model capabilities;(ii) a minimal transformation, which we call a 'wrapper', is typically learnedon top of the underlying model capabilities, creating the illusion that theyhave been modified; and (iii) further fine-tuning on a task where such hiddencapabilities are relevant leads to sample-efficient 'revival' of thecapability, i.e., the model begins reusing these capability after only a fewgradient steps. This indicates that practitioners can unintentionally remove amodel's safety wrapper merely by fine-tuning it on a, e.g., superficiallyunrelated, downstream task. We additionally perform analysis on language modelstrained on the TinyStories dataset to support our claims in a more realisticsetup.</description><author>Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, David Scott Krueger</author><pubDate>Tue, 21 Nov 2023 18:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12786v1</guid></item><item><title>Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching</title><link>http://arxiv.org/abs/2311.01331v2</link><description>In real-world scenarios, arbitrary interactions with the environment canoften be costly, and actions of expert demonstrations are not always available.To reduce the need for both, Offline Learning from Observations (LfO) isextensively studied, where the agent learns to solve a task with only expertstates and \textit{task-agnostic} non-expert state-action pairs. Thestate-of-the-art DIstribution Correction Estimation (DICE) methods minimize thestate occupancy divergence between the learner and expert policies. However,they are limited to either $f$-divergences (KL and $\chi^2$) or Wassersteindistance with Rubinstein duality, the latter of which constrains the underlyingdistance metric crucial to the performance of Wasserstein-based solutions. Toaddress this problem, we propose Primal Wasserstein DICE (PW-DICE), whichminimizes the primal Wasserstein distance between the expert and learner stateoccupancies with a pessimistic regularizer and leverages a contrastivelylearned distance as the underlying metric for the Wasserstein distance.Theoretically, we prove that our framework is a generalization of thestate-of-the-art, SMODICE, and unifies $f$-divergence and Wassersteinminimization. Empirically, we find that PW-DICE improves upon severalstate-of-the-art methods on multiple testbeds.</description><author>Kai Yan, Alexander G. Schwing, Yu-xiong Wang</author><pubDate>Tue, 21 Nov 2023 18:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01331v2</guid></item><item><title>Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian, and Beyond $1+α$ Moments</title><link>http://arxiv.org/abs/2311.12784v1</link><description>There is growing interest in improving our algorithmic understanding offundamental statistical problems such as mean estimation, driven by the goal ofunderstanding the limits of what we can extract from valuable data. The stateof the art results for mean estimation in $\mathbb{R}$ are 1) the optimalsub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant forall distributions with finite but unknown variance, and 2) the analysis of themedian-of-means algorithm by [BCL13] and a lower bound by [DLLO16],characterizing the big-O optimal errors for distributions for which only a$1+\alpha$ moment exists for $\alpha \in (0,1)$. Both results, however, areoptimal only in the worst case. We initiate the fine-grained study of the meanestimation problem: Can algorithms leverage useful features of the inputdistribution to beat the sub-Gaussian rate, without explicit knowledge of suchfeatures? We resolve this question with an unexpectedly nuanced answer: "Yes in limitedregimes, but in general no". For any distribution $p$ with a finite mean, weconstruct a distribution $q$ whose mean is well-separated from $p$'s, yet $p$and $q$ are not distinguishable with high probability, and $q$ furtherpreserves $p$'s moments up to constants. The main consequence is that noreasonable estimator can asymptotically achieve better than the sub-Gaussianerror rate for any distribution, matching the worst-case result of [LV22]. Moregenerally, we introduce a new definitional framework to analyze thefine-grained optimality of algorithms, which we call "neighborhood optimality",interpolating between the unattainably strong "instance optimality" and thetrivially weak "admissibility" definitions. Applying the new framework, we showthat median-of-means is neighborhood optimal, up to constant factors. It isopen to find a neighborhood-optimal estimator without constant factorslackness.</description><author>Trung Dang, Jasper C. H. Lee, Maoyuan Song, Paul Valiant</author><pubDate>Tue, 21 Nov 2023 18:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12784v1</guid></item><item><title>Topological properties of basins of attraction and expressiveness of width bounded neural networks</title><link>http://arxiv.org/abs/2011.04923v5</link><description>In Radhakrishnan et al. [2020], the authors empirically show thatautoencoders trained with usual SGD methods shape out basins of attractionaround their training data. We consider network functions of width notexceeding the input dimension and prove that in this situation basins ofattraction are bounded and their complement cannot have bounded components. Ourconditions in these results are met in several experiments of the latter workand we thus address a question posed therein. We also show that under some morerestrictive conditions the basins of attraction are path-connected. Thetightness of the conditions in our results is demonstrated by means of severalexamples. Finally, the arguments used to prove the above results allow us toderive a root cause why scalar-valued neural network functions that fulfill ourbounded width condition are not dense in spaces of continuous functions.</description><author>Hans-Peter Beise, Steve Dias Da Cruz</author><pubDate>Tue, 21 Nov 2023 18:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.04923v5</guid></item><item><title>GEFF: Improving Any Clothes-Changing Person ReID Model using Gallery Enrichment with Face Features</title><link>http://arxiv.org/abs/2211.13807v2</link><description>In the Clothes-Changing Re-Identification (CC-ReID) problem, given a querysample of a person, the goal is to determine the correct identity based on alabeled gallery in which the person appears in different clothes. Severalmodels tackle this challenge by extracting clothes-independent features.However, the performance of these models is still lower for theclothes-changing setting compared to the same-clothes setting in which theperson appears with the same clothes in the labeled gallery. Asclothing-related features are often dominant features in the data, we propose anew process we call Gallery Enrichment, to utilize these features. In thisprocess, we enrich the original gallery by adding to it query samples based ontheir face features, using an unsupervised algorithm. Additionally, we showthat combining ReID and face feature extraction modules alongside an enrichedgallery results in a more accurate ReID model, even for query samples with newoutfits that do not include faces. Moreover, we claim that existing CC-ReIDbenchmarks do not fully represent real-world scenarios, and propose a new videoCC-ReID dataset called 42Street, based on a theater play that includes crowdedscenes and numerous clothes changes. When applied to multiple ReID models, ourmethod (GEFF) achieves an average improvement of 33.5% and 6.7% in the Top-1clothes-changing metric on the PRCC and LTCC benchmarks. Combined with thelatest ReID models, our method achieves new SOTA results on the PRCC, LTCC,CCVID, LaST and VC-Clothes benchmarks and the proposed 42Street dataset.</description><author>Daniel Arkushin, Bar Cohen, Shmuel Peleg, Ohad Fried</author><pubDate>Tue, 21 Nov 2023 18:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13807v2</guid></item><item><title>Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects</title><link>http://arxiv.org/abs/2311.12781v1</link><description>Automatic assessment of impairment and disease severity is a key challenge indata-driven medicine. We propose a novel framework to address this challenge,which leverages AI models trained exclusively on healthy individuals. TheCOnfidence-Based chaRacterization of Anomalies (COBRA) score exploits thedecrease in confidence of these models when presented with impaired or diseasedpatients to quantify their deviation from the healthy population. We appliedthe COBRA score to address a key limitation of current clinical evaluation ofupper-body impairment in stroke patients. The gold-standard Fugl-MeyerAssessment (FMA) requires in-person administration by a trained assessor for30-45 minutes, which restricts monitoring frequency and precludes physiciansfrom adapting rehabilitation protocols to the progress of each patient. TheCOBRA score, computed automatically in under one minute, is shown to bestrongly correlated with the FMA on an independent test cohort for twodifferent data modalities: wearable sensors ($\rho = 0.845$, 95% CI[0.743,0.908]) and video ($\rho = 0.746$, 95% C.I [0.594, 0.847]). Todemonstrate the generalizability of the approach to other conditions, the COBRAscore was also applied to quantify severity of knee osteoarthritis frommagnetic-resonance imaging scans, again achieving significant correlation withan independent clinical assessment ($\rho = 0.644$, 95% C.I [0.585,0.696]).</description><author>Boyang Yu, Aakash Kaku, Kangning Liu, Avinash Parnandi, Emily Fokas, Anita Venkatesan, Natasha Pandit, Rajesh Ranganath, Heidi Schambra, Carlos Fernandez-Granda</author><pubDate>Tue, 21 Nov 2023 18:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12781v1</guid></item><item><title>Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models</title><link>http://arxiv.org/abs/2305.12827v3</link><description>Task arithmetic has recently emerged as a cost-effective and scalableapproach to edit pre-trained models directly in weight space: By adding thefine-tuned weights of different tasks, the model's performance can be improvedon these tasks, while negating them leads to task forgetting. Yet, ourunderstanding of the effectiveness of task arithmetic and its underlyingprinciples remains limited. We present a comprehensive study of task arithmeticin vision-language models and show that weight disentanglement is the crucialfactor that makes it effective. This property arises during pre-training andmanifests when distinct directions in weight space govern separate, localizedregions in function space associated with the tasks. Notably, we show thatfine-tuning models in their tangent space by linearizing them amplifies weightdisentanglement. This leads to substantial performance improvements acrossmultiple task arithmetic benchmarks and diverse models. Building on thesefindings, we provide theoretical and empirical analyses of the neural tangentkernel (NTK) of these models and establish a compelling link between taskarithmetic and the spatial localization of the NTK eigenfunctions. Overall, ourwork uncovers novel insights into the fundamental mechanisms of task arithmeticand offers a more reliable and effective approach to edit pre-trained modelsthrough the NTK linearization.</description><author>Guillermo Ortiz-Jimenez, Alessandro Favero, Pascal Frossard</author><pubDate>Tue, 21 Nov 2023 18:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12827v3</guid></item><item><title>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</title><link>http://arxiv.org/abs/2311.12775v1</link><description>We propose a method to allow precise and extremely fast mesh extraction from3D Gaussian Splatting. Gaussian Splatting has recently become very popular asit yields realistic rendering while being significantly faster to train thanNeRFs. It is however challenging to extract a mesh from the millions of tiny 3Dgaussians as these gaussians tend to be unorganized after optimization and nomethod has been proposed so far. Our first key contribution is a regularizationterm that encourages the gaussians to align well with the surface of the scene.We then introduce a method that exploits this alignment to extract a mesh fromthe Gaussians using Poisson reconstruction, which is fast, scalable, andpreserves details, in contrast to the Marching Cubes algorithm usually appliedto extract meshes from Neural SDFs. Finally, we introduce an optionalrefinement strategy that binds gaussians to the surface of the mesh, andjointly optimizes these Gaussians and the mesh through Gaussian splattingrendering. This enables easy editing, sculpting, rigging, animating,compositing and relighting of the Gaussians using traditional softwares bymanipulating the mesh instead of the gaussians themselves. Retrieving such aneditable mesh for realistic rendering is done within minutes with our method,compared to hours with the state-of-the-art methods on neural SDFs, whileproviding a better rendering quality.</description><author>Antoine Guédon, Vincent Lepetit</author><pubDate>Tue, 21 Nov 2023 18:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12775v1</guid></item><item><title>Iris Presentation Attack: Assessing the Impact of Combining Vanadium Dioxide Films with Artificial Eyes</title><link>http://arxiv.org/abs/2311.12773v1</link><description>Iris recognition systems, operating in the near infrared spectrum (NIR), havedemonstrated vulnerability to presentation attacks, where an adversary usesartifacts such as cosmetic contact lenses, artificial eyes or printed irisimages in order to circumvent the system. At the same time, a number ofeffective presentation attack detection (PAD) methods have been developed.These methods have demonstrated success in detecting artificial eyes (e.g.,fake Van Dyke eyes) as presentation attacks. In this work, we seek to alter theoptical characteristics of artificial eyes by affixing Vanadium Dioxide (VO2)films on their surface in various spatial configurations. VO2 films can be usedto selectively transmit NIR light and can, therefore, be used to regulate theamount of NIR light from the object that is captured by the iris sensor. Westudy the impact of such images produced by the sensor on two state-of-the-artiris PA detection methods. We observe that the addition of VO2 films on thesurface of artificial eyes can cause the PA detection methods to misclassifythem as bonafide eyes in some cases. This represents a vulnerability that mustbe systematically analyzed and effectively addressed.</description><author>Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross</author><pubDate>Tue, 21 Nov 2023 18:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12773v1</guid></item><item><title>Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images</title><link>http://arxiv.org/abs/2111.11057v3</link><description>The task of instance segmentation in remote sensing images, aiming atperforming per-pixel labeling of objects at instance level, is of greatimportance for various civil applications. Despite previous successes, mostexisting instance segmentation methods designed for natural images encountersharp performance degradations when they are directly applied to top-viewremote sensing images. Through careful analysis, we observe that the challengesmainly come from the lack of discriminative object features due to severe scalevariations, low contrasts, and clustered distributions. In order to addressthese problems, a novel context aggregation network (CATNet) is proposed toimprove the feature extraction process. The proposed model exploits threelightweight plug-and-play modules, namely dense feature pyramid network(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interestextractor (HRoIE), to aggregate global visual context at feature, spatial, andinstance domains, respectively. DenseFPN is a multi-scale feature propagationmodule that establishes more flexible information flows by adopting inter-levelresidual connections, cross-level dense connections, and feature re-weightingstrategy. Leveraging the attention mechanism, SCP further augments the featuresby aggregating global spatial context into local regions. For each instance,HRoIE adaptively generates RoI features for different downstream tasks.Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, andHRSID datasets demonstrate that the proposed approach outperformsstate-of-the-arts under similar computational costs. Source code andpre-trained models are available at https://github.com/yeliudev/CATNet.</description><author>Ye Liu, Huifang Li, Chao Hu, Shuang Luo, Yan Luo, Chang Wen Chen</author><pubDate>Tue, 21 Nov 2023 18:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.11057v3</guid></item><item><title>Banach-Tarski Embeddings and Transformers</title><link>http://arxiv.org/abs/2311.09387v2</link><description>We introduce a new construction of embeddings of arbitrary recursive datastructures into high dimensional vectors. These embeddings provide aninterpretable model for the latent state vectors of transformers. Wedemonstrate that these embeddings can be decoded to the original data structurewhen the embedding dimension is sufficiently large. This decoding algorithm hasa natural implementation as a transformer. We also show that these embeddingvectors can be manipulated directly to perform computations on the underlyingdata without decoding. As an example we present an algorithm that constructsthe embedded parse tree of an embedded token sequence using only vectoroperations in embedding space.</description><author>Joshua Maher</author><pubDate>Tue, 21 Nov 2023 18:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09387v2</guid></item><item><title>Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions</title><link>http://arxiv.org/abs/2206.01255v5</link><description>High-dimensional Partial Differential Equations (PDEs) are a popularmathematical modelling tool, with applications ranging from finance tocomputational chemistry. However, standard numerical techniques for solvingthese PDEs are typically affected by the curse of dimensionality. In this work,we tackle this challenge while focusing on stationary diffusion equationsdefined over a high-dimensional domain with periodic boundary conditions.Inspired by recent progress in sparse function approximation in highdimensions, we propose a new method called compressive Fourier collocation.Combining ideas from compressive sensing and spectral collocation, our methodreplaces the use of structured collocation grids with Monte Carlo sampling andemploys sparse recovery techniques, such as orthogonal matching pursuit and$\ell^1$ minimization, to approximate the Fourier coefficients of the PDEsolution. We conduct a rigorous theoretical analysis showing that theapproximation error of the proposed method is comparable with the best $s$-termapproximation (with respect to the Fourier basis) to the solution. Using therecently introduced framework of random sampling in bounded Riesz systems, ouranalysis shows that the compressive Fourier collocation method mitigates thecurse of dimensionality with respect to the number of collocation points undersufficient conditions on the regularity of the diffusion coefficient. We alsopresent numerical experiments that illustrate the accuracy and stability of themethod for the approximation of sparse and compressible solutions.</description><author>Weiqi Wang, Simone Brugiapaglia</author><pubDate>Tue, 21 Nov 2023 18:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.01255v5</guid></item><item><title>Swift Parameter-free Attention Network for Efficient Super-Resolution</title><link>http://arxiv.org/abs/2311.12770v1</link><description>Single Image Super-Resolution (SISR) is a crucial task in low-level computervision, aiming to reconstruct high-resolution images from low-resolutioncounterparts. Conventional attention mechanisms have significantly improvedSISR performance but often result in complex network structures and largenumber of parameters, leading to slow inference speed and large model size. Toaddress this issue, we propose the Swift Parameter-free Attention Network(SPAN), a highly efficient SISR model that balances parameter count, inferencespeed, and image quality. SPAN employs a novel parameter-free attentionmechanism, which leverages symmetric activation functions and residualconnections to enhance high-contribution information and suppress redundantinformation. Our theoretical analysis demonstrates the effectiveness of thisdesign in achieving the attention mechanism's purpose. We evaluate SPAN onmultiple benchmarks, showing that it outperforms existing efficientsuper-resolution models in terms of both image quality and inference speed,achieving a significant quality-speed trade-off. This makes SPAN highlysuitable for real-world applications, particularly in resource-constrainedscenarios. Notably, our model attains the best PSNR of 27.09 dB, and the testruntime of our team is reduced by 7.08ms in the NTIRE 2023 efficientsuper-resolution challenge. Our code and models are made publicly available at\url{https://github.com/hongyuanyu/SPAN}.</description><author>Cheng Wan, Hongyuan Yu, Zhiqi Li, Yihang Chen, Yajun Zou, Yuqing Liu, Xuanwu Yin, Kunlong Zuo</author><pubDate>Tue, 21 Nov 2023 18:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12770v1</guid></item><item><title>Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection</title><link>http://arxiv.org/abs/2311.12764v1</link><description>Deep neural networks (DNNs) exhibit superior performance in various machinelearning tasks, e.g., image classification, speech recognition, biometricrecognition, object detection, etc. However, it is essential to analyze theirsensitivity to parameter perturbations before deploying them in real-worldapplications. In this work, we assess the sensitivity of DNNs againstperturbations to their weight and bias parameters. The sensitivity analysisinvolves three DNN architectures (VGG, ResNet, and DenseNet), three types ofparameter perturbations (Gaussian noise, weight zeroing, and weight scaling),and two settings (entire network and layer-wise). We perform experiments in thecontext of iris presentation attack detection and evaluate on two publiclyavailable datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on thesensitivity analysis, we propose improved models simply by perturbingparameters of the network without undergoing training. We further combine theseperturbed models at the score-level and at the parameter-level to improve theperformance over the original model. The ensemble at the parameter-level showsan average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% onthe LivDet-Iris-2020 dataset. The source code is available at\href{https://github.com/redwankarimsony/WeightPerturbation-MSU}{https://github.com/redwankarimsony/WeightPerturbation-MSU}.</description><author>Renu Sharma, Redwan Sony, Arun Ross</author><pubDate>Tue, 21 Nov 2023 18:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12764v1</guid></item><item><title>Editing Personality for LLMs</title><link>http://arxiv.org/abs/2310.02168v2</link><description>This paper introduces an innovative task focused on editing the personalitytraits of Large Language Models (LLMs). This task seeks to adjust the models'responses to opinion-related questions on specified topics since anindividual's personality often manifests in the form of their expressedopinions, thereby showcasing different personality traits. Specifically, weconstruct a new benchmark dataset PersonalityEdit to address this task. Drawingon the theory in Social Psychology, we isolate three representative traits,namely Neuroticism, Extraversion, and Agreeableness, as the foundation for ourbenchmark. We then gather data using GPT-4, generating responses that not onlyalign with a specified topic but also embody the targeted personality trait. Weconduct comprehensive experiments involving various baselines and discuss therepresentation of personality behavior in LLMs. Our intriguing findings uncoverpotential challenges of the proposed task, illustrating several remainingissues. We anticipate that our work can provide the NLP community withinsights. Code and datasets will be released athttps://github.com/zjunlp/EasyEdit.</description><author>Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</author><pubDate>Tue, 21 Nov 2023 18:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02168v2</guid></item><item><title>High-resolution Image-based Malware Classification using Multiple Instance Learning</title><link>http://arxiv.org/abs/2311.12760v1</link><description>This paper proposes a novel method of classifying malware into families usinghigh-resolution greyscale images and multiple instance learning to overcomeadversarial binary enlargement. Current methods of visualisation-based malwareclassification largely rely on lossy transformations of inputs such as resizingto handle the large, variable-sized images. Through empirical analysis andexperimentation, it is shown that these approaches cause crucial informationloss that can be exploited. The proposed solution divides the images intopatches and uses embedding-based multiple instance learning with aconvolutional neural network and an attention aggregation function forclassification. The implementation is evaluated on the Microsoft MalwareClassification dataset and achieves accuracies of up to $96.6\%$ onadversarially enlarged samples compared to the baseline of $22.8\%$. The Pythoncode is available online at https://github.com/timppeters/MIL-Malware-Images .</description><author>Tim Peters, Hikmat Farhat</author><pubDate>Tue, 21 Nov 2023 18:11:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12760v1</guid></item><item><title>VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch</title><link>http://arxiv.org/abs/2211.09945v7</link><description>AI's widespread integration has led to neural networks (NNs) deployment onedge and similar limited-resource platforms for safety-critical scenarios. Yet,NN's fragility raises concerns about reliable inference. Moreover, constrainedplatforms demand compact networks. This study introduces VeriCompress, a toolthat automates the search and training of compressed models with robustnessguarantees. These models are well-suited for safety-critical applications andadhere to predefined architecture and size limitations, making them deployableon resource-restricted platforms. The method trains models 2-3 times fasterthan the state-of-the-art approaches, surpassing relevant baseline approachesby average accuracy and robustness gains of 15.1 and 9.8 percentage points,respectively. When deployed on a resource-restricted generic platform, thesemodels require 5-8 times less memory and 2-4 times less inference time thanmodels used in verified robustness literature. Our comprehensive evaluationacross various model architectures and datasets, including MNIST, CIFAR, SVHN,and a relevant pedestrian detection dataset, showcases VeriCompress's capacityto identify compressed verified robust models with reduced computation overheadcompared to current standards. This underscores its potential as a valuabletool for end users, such as developers of safety-critical applications on edgeor Internet of Things platforms, empowering them to create suitable models forsafety-critical, resource-constrained platforms in their respective domains.</description><author>Sawinder Kaur, Yi Xiao, Asif Salekin</author><pubDate>Tue, 21 Nov 2023 18:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09945v7</guid></item><item><title>Digital Twin Framework for Optimal and Autonomous Decision-Making in Cyber-Physical Systems: Enhancing Reliability and Adaptability in the Oil and Gas Industry</title><link>http://arxiv.org/abs/2311.12755v1</link><description>The concept of creating a virtual copy of a complete Cyber-Physical Systemopens up numerous possibilities, including real-time assessments of thephysical environment and continuous learning from the system to providereliable and precise information. This process, known as the twinning processor the development of a digital twin (DT), has been widely adopted acrossvarious industries. However, challenges arise when considering thecomputational demands of implementing AI models, such as those employed indigital twins, in real-time information exchange scenarios. This work proposesa digital twin framework for optimal and autonomous decision-making applied toa gas-lift process in the oil and gas industry, focusing on enhancing therobustness and adaptability of the DT. The framework combines Bayesianinference, Monte Carlo simulations, transfer learning, online learning, andnovel strategies to confer cognition to the DT, including modelhyperdimensional reduction and cognitive tack. Consequently, creating aframework for efficient, reliable, and trustworthy DT identification waspossible. The proposed approach addresses the current gap in the literatureregarding integrating various learning techniques and uncertainty management indigital twin strategies. This digital twin framework aims to provide a reliableand efficient system capable of adapting to changing environments andincorporating prediction uncertainty, thus enhancing the overalldecision-making process in complex, real-world scenarios. Additionally, thiswork lays the foundation for further developments in digital twins for processsystems engineering, potentially fostering new advancements and applicationsacross various industrial sectors.</description><author>Carine Menezes Rebello, Johannes Jäschkea, Idelfonso B. R. Nogueira</author><pubDate>Tue, 21 Nov 2023 18:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12755v1</guid></item><item><title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2311.12754v1</link><description>3D occupancy prediction is an important task for the robustness ofvision-centric autonomous driving, which aims to predict whether each point isoccupied in the surrounding 3D space. Existing methods usually require 3Doccupancy labels to produce meaningful results. However, it is very laboriousto annotate the occupancy status of each voxel. In this paper, we proposeSelfOcc to explore a self-supervised way to learn 3D occupancy using only videosequences. We first transform the images into the 3D space (e.g., bird's eyeview) to obtain 3D representation of the scene. We directly impose constraintson the 3D representations by treating them as signed distance fields. We canthen render 2D images of previous and future frames as self-supervision signalsto learn the 3D representations. We propose an MVS-embedded strategy todirectly optimize the SDF-induced weights with multiple depth proposals. OurSelfOcc outperforms the previous best method SceneRF by 58.7% using a singleframe as input on SemanticKITTI and is the first self-supervised work thatproduces reasonable 3D occupancy for surround cameras on Occ3D. SelfOccproduces high-quality depth and achieves state-of-the-art results on noveldepth synthesis, monocular depth estimation, and surround-view depth estimationon the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:https://github.com/huang-yh/SelfOcc.</description><author>Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu</author><pubDate>Tue, 21 Nov 2023 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12754v1</guid></item><item><title>Unveiling the Pitfalls of Knowledge Editing for Large Language Models</title><link>http://arxiv.org/abs/2310.02129v2</link><description>As the cost associated with fine-tuning Large Language Models (LLMs)continues to rise, recent research efforts have pivoted towards developingmethodologies to edit implicit knowledge embedded within LLMs. Yet, there'sstill a dark cloud lingering overhead -- will knowledge editing triggerbutterfly effect? since it is still unclear whether knowledge editing mightintroduce side effects that pose potential risks or not. This paper pioneersthe investigation into the potential pitfalls associated with knowledge editingfor LLMs. To achieve this, we introduce new benchmark datasets and proposeinnovative evaluation metrics. Our results underline two pivotal concerns: (1)Knowledge Conflict: Editing groups of facts that logically clash can magnifythe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)Knowledge Distortion: Altering parameters with the aim of editing factualknowledge can irrevocably warp the innate knowledge structure of LLMs.Experimental results vividly demonstrate that knowledge editing mightinadvertently cast a shadow of unintended consequences on LLMs, which warrantattention and efforts for future works. Code is available athttps://github.com/zjunlp/PitfallsKnowledgeEditing.</description><author>Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</author><pubDate>Tue, 21 Nov 2023 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02129v2</guid></item><item><title>Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatially Relation Matching</title><link>http://arxiv.org/abs/2311.12751v1</link><description>Drone navigation through natural language commands remains a significantchallenge due to the lack of publicly available multi-modal datasets and theintricate demands of fine-grained visual-text alignment. In response to thispressing need, we present a new human-computer interaction annotation benchmarkcalled GeoText-1652, meticulously curated through a robust Large Language Model(LLM)-based data generation framework and the expertise of pre-trained visionmodels. This new dataset seamlessly extends the existing image dataset, \ie,University-1652, with spatial-aware text annotations, encompassing intricateimage-text-bounding box associations. Besides, we introduce a new optimizationobjective to leverage fine-grained spatial associations, called blendingspatial matching, for region-level spatial relation matching. Extensiveexperiments reveal that our approach maintains an exceptional recall rate undervarying description complexities. This underscores the promising potential ofour approach in elevating drone control and navigation through the seamlessintegration of natural language commands in real-world scenarios.</description><author>Meng Chu, Zhedong Zheng, Wei Ji, Tat-Seng Chua</author><pubDate>Tue, 21 Nov 2023 17:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12751v1</guid></item><item><title>Learning to Optimise Wind Farms with Graph Transformers</title><link>http://arxiv.org/abs/2311.12750v1</link><description>This work proposes a novel data-driven model capable of providing accuratepredictions for the power generation of all wind turbines in wind farms ofarbitrary layout, yaw angle configurations and wind conditions. The proposedmodel functions by encoding a wind farm into a fully-connected graph andprocessing the graph representation through a graph transformer. The graphtransformer surrogate is shown to generalise well and is able to uncover latentstructural patterns within the graph representation of wind farms. It isdemonstrated how the resulting surrogate model can be used to optimise yawangle configurations using genetic algorithms, achieving similar levels ofaccuracy to industrially-standard wind farm simulation tools while only takinga fraction of the computational cost.</description><author>Siyi Li, Arnaud Robert, A. Aldo Faisal, Matthew D. Piggott</author><pubDate>Tue, 21 Nov 2023 17:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12750v1</guid></item><item><title>An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer</title><link>http://arxiv.org/abs/2311.06185v2</link><description>Tumour-infiltrating lymphocytes (TILs) are considered as a valuableprognostic markers in both triple-negative and human epidermal growth factorreceptor 2 (HER2) positive breast cancer. In this study, we introduce aninnovative deep learning pipeline based on the Efficient-UNet architecture topredict the TILs score for breast cancer whole-slide images (WSIs). We firstsegment tumour and stromal regions in order to compute a tumour bulk mask. Wethen detect TILs within the tumour-associated stroma, generating a TILs scoreby closely mirroring the pathologist's workflow. Our method exhibitsstate-of-the-art performance in segmenting tumour/stroma areas and TILsdetection, as demonstrated by internal cross-validation on the TiGER Challengetraining dataset and evaluation on the final leaderboards. Additionally, ourTILs score proves competitive in predicting survival outcomes within the samechallenge, underscoring the clinical relevance and potential of our automatedTILs scoring pipeline as a breast cancer prognostic tool.</description><author>Adam J Shephard, Mostafa Jahanifar, Ruoyu Wang, Muhammad Dawood, Simon Graham, Kastytis Sidlauskas, Syed Ali Khurram, Nasir M Rajpoot, Shan E Ahmed Raza</author><pubDate>Tue, 21 Nov 2023 17:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06185v2</guid></item><item><title>Stable Adam Optimization for 16-bit Neural Networks Training</title><link>http://arxiv.org/abs/2307.16189v6</link><description>In this research, we address critical concerns related to the numericalinstability observed in 16-bit computations of machine learning models. Suchinstability, particularly when employing popular optimization algorithms likeAdam, often leads to unstable training of deep neural networks. This not onlydisrupts the learning process but also poses significant challenges indeploying dependable models in real-world applications. Our investigationidentifies the epsilon hyperparameter as the primary source of thisinstability. A nuanced exploration reveals that subtle adjustments to epsilonwithin 16-bit computations can enhance the numerical stability of Adam,enabling more stable training of 16-bit neural networks. We propose a novel,dependable approach that leverages updates from the Adam optimizer to bolsterthe stability of the learning process. Our contributions provide deeperinsights into optimization challenges in low-precision computations and offersolutions to ensure the stability of deep neural network training, paving theway for their dependable use in various applications.</description><author>Juyoung Yun</author><pubDate>Tue, 21 Nov 2023 17:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16189v6</guid></item><item><title>Image Transformation for IoT Time-Series Data: A Review</title><link>http://arxiv.org/abs/2311.12742v1</link><description>In the era of the Internet of Things (IoT), where smartphones, built-insystems, wireless sensors, and nearly every smart device connect through localnetworks or the internet, billions of smart things communicate with each otherand generate vast amounts of time-series data. As IoT time-series data ishigh-dimensional and high-frequency, time-series classification or regressionhas been a challenging issue in IoT. Recently, deep learning algorithms havedemonstrated superior performance results in time-series data classification inmany smart and intelligent IoT applications. However, it is hard to explore thehidden dynamic patterns and trends in time-series. Recent studies show thattransforming IoT data into images improves the performance of the learningmodel. In this paper, we present a review of these studies which use imagetransformation/encoding techniques in IoT domain. We examine the studiesaccording to their encoding techniques, data types, and application areas.Lastly, we emphasize the challenges and future dimensions of imagetransformation.</description><author>Duygu Altunkaya, Feyza Yildirim Okay, Suat Ozdemir</author><pubDate>Tue, 21 Nov 2023 17:31:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12742v1</guid></item><item><title>Content Augmented Graph Neural Networks</title><link>http://arxiv.org/abs/2311.12741v1</link><description>In recent years, graph neural networks (GNNs) have become a popular tool forsolving various problems over graphs. In these models, the link structure ofthe graph is typically exploited and nodes' embeddings are iteratively updatedbased on adjacent nodes. Nodes' contents are used solely in the form of featurevectors, served as nodes' first-layer embeddings. However, the filters orconvolutions, applied during iterations/layers to these initial embeddings leadto their impact diminish and contribute insignificantly to the finalembeddings. In order to address this issue, in this paper we propose augmentingnodes' embeddings by embeddings generating from their content, at higher GNNlayers. More precisely, we propose models wherein a structural embedding usinga GNN and a content embedding are computed for each node. These two arecombined using a combination layer to form the embedding of a node at a givenlayer. We suggest methods such as using an auto-encoder or building a contentgraph, to generate content embeddings. In the end, by conducting experimentsover several real-world datasets, we demonstrate the high accuracy andperformance of our models.</description><author>Fatemeh Gholamzadeh Nasrabadi, AmirHossein Kashani, Pegah Zahedi, Mostafa Haghir Chehreghani</author><pubDate>Tue, 21 Nov 2023 17:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12741v1</guid></item><item><title>BrainWash: A Poisoning Attack to Forget in Continual Learning</title><link>http://arxiv.org/abs/2311.11995v2</link><description>Continual learning has gained substantial attention within the deep learningcommunity, offering promising solutions to the challenging problem ofsequential learning. Yet, a largely unexplored facet of this paradigm is itssusceptibility to adversarial attacks, especially with the aim of inducingforgetting. In this paper, we introduce "BrainWash," a novel data poisoningmethod tailored to impose forgetting on a continual learner. By adding theBrainWash noise to a variety of baselines, we demonstrate how a trainedcontinual learner can be induced to forget its previously learned taskscatastrophically, even when using these continual learning baselines. Animportant feature of our approach is that the attacker requires no access toprevious tasks' data and is armed merely with the model's current parametersand the data belonging to the most recent task. Our extensive experimentshighlight the efficacy of BrainWash, showcasing degradation in performanceacross various regularization-based continual learning methods.</description><author>Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri</author><pubDate>Tue, 21 Nov 2023 17:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11995v2</guid></item><item><title>Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design</title><link>http://arxiv.org/abs/2311.03489v2</link><description>We present a new high-level synthesis methodology for using large languagemodel tools to generate hardware designs. The methodology uses exclusivelyopen-source tools excluding the large language model. As a case study, we useour methodology to generate a permuted congruential random number generatordesign with a wishbone interface. We verify the functionality and quality ofthe random number generator design using large language model-generatedsimulations and the Dieharder randomness test suite. We document all the largelanguage model chat logs, Python scripts, Verilog scripts, and simulationresults used in the case study. We believe that our method of hardware designgeneration coupled with the open source silicon 130 nm design tools willrevolutionize application-specific integrated circuit design. Our methodologysignificantly lowers the bar to entry when building domain-specific computingaccelerators for the Internet of Things and proof of concept prototypes forlater fabrication in more modern process nodes.</description><author>James T. Meech</author><pubDate>Tue, 21 Nov 2023 17:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03489v2</guid></item><item><title>Exploring Graph Classification Techniques Under Low Data Constraints: A Comprehensive Study</title><link>http://arxiv.org/abs/2311.12737v1</link><description>This survey paper presents a brief overview of recent research on graph dataaugmentation and few-shot learning. It covers various techniques for graph dataaugmentation, including node and edge perturbation, graph coarsening, and graphgeneration, as well as the latest developments in few-shot learning, such asmeta-learning and model-agnostic meta-learning. The paper explores these areasin depth and delves into further sub classifications. Rule based approaches andlearning based approaches are surveyed under graph augmentation techniques.Few-Shot Learning on graphs is also studied in terms of metric learningtechniques and optimization-based techniques. In all, this paper provides anextensive array of techniques that can be employed in solving graph processingproblems faced in low-data scenarios.</description><author>Kush Kothari, Bhavya Mehta, Reshmika Nambiar, Seema Shrawne</author><pubDate>Tue, 21 Nov 2023 17:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12737v1</guid></item><item><title>Using Scalable Computer Vision to Automate High-throughput Semiconductor Characterization</title><link>http://arxiv.org/abs/2304.14408v3</link><description>High-throughput materials synthesis methods have risen in popularity due totheir potential to accelerate the design and discovery of novel functionalmaterials, such as solution-processed semiconductors. After synthesis, keymaterial properties must be measured and characterized to validate discoveryand provide feedback to optimization cycles. However, with the boom indevelopment of high-throughput synthesis tools that champion production ratesup to $10^4$ samples per hour with flexible form factors, most samplecharacterization methods are either slow (conventional rates of $10^1$ samplesper hour, approximately 1000x slower) or rigid (e.g., designed forstandard-size microplates), resulting in a bottleneck that impedes thematerials-design process. To overcome this challenge, we propose a set ofautomated material property characterization (autocharacterization) tools thatleverage the adaptive, parallelizable, and scalable nature of computer visionto accelerate the throughput of characterization by 85x compared to thenon-automated workflow. We demonstrate a generalizable composition mapping toolfor high-throughput synthesized binary material systems as well as two scalableautocharacterization algorithms that (1) autonomously compute the band gap of200 unique compositions in 6 minutes and (2) autonomously compute the degree ofdegradation in 200 unique compositions in 20 minutes, generating ultra-highcompositional resolution trends of band gap and stability. We demonstrate thatthe developed band gap and degradation detection autocharacterization methodsachieve 98.5% accuracy and 96.9% accuracy, respectively, on theFA$_{1-x}$MA$_{x}$PbI$_3$, $0\leq x \leq 1$ perovskite semiconductor system.</description><author>Alexander E. Siemenn, Eunice Aissi, Fang Sheng, Armi Tiihonen, Hamide Kavak, Basita Das, Tonio Buonassisi</author><pubDate>Tue, 21 Nov 2023 17:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14408v3</guid></item><item><title>LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language</title><link>http://arxiv.org/abs/2311.12735v1</link><description>This paper describes the system of the LowResource Team for Task 2 ofBLP-2023, which involves conducting sentiment analysis on a dataset composed ofpublic posts and comments from diverse social media platforms. Our primary aimis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,using various strategies including fine-tuning, dropping random tokens, andusing several external datasets. Our final model is an ensemble of the threebest BanglaBert variations. Our system has achieved overall 3rd in the Test Setamong 30 participating teams with a score of 0.718. Additionally, we discussthe promising systems that didn't perform well namely task-adaptive pertainingand paraphrasing using BanglaT5. Training codes and external datasets which areused for our system are publicly available athttps://github.com/Aunabil4602/bnlp-workshop-task2-2023</description><author>Aunabil Chakma, Masum Hasan</author><pubDate>Tue, 21 Nov 2023 17:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12735v1</guid></item><item><title>Multi-Objective Optimization Using the R2 Utility</title><link>http://arxiv.org/abs/2305.11774v2</link><description>The goal of multi-objective optimization is to identify a collection ofpoints which describe the best possible trade-offs between the multipleobjectives. In order to solve this vector-valued optimization problem,practitioners often appeal to the use of scalarization functions in order totransform the multi-objective problem into a collection of single-objectiveproblems. This set of scalarized problems can then be solved using traditionalsingle-objective optimization techniques. In this work, we formalise thisconvention into a general mathematical framework. We show how this strategyeffectively recasts the original multi-objective optimization problem into asingle-objective optimization problem defined over sets. An appropriate classof objective functions for this new problem is the R2 utility function, whichis defined as a weighted integral over the scalarized optimization problems. Weshow that this utility function is a monotone and submodular set function,which can be optimised effectively using greedy optimization algorithms. Weanalyse the performance of these greedy algorithms both theoretically andempirically. Our analysis largely focusses on Bayesian optimization, which is apopular probabilistic framework for black-box optimization.</description><author>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</author><pubDate>Tue, 21 Nov 2023 17:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11774v2</guid></item><item><title>Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools</title><link>http://arxiv.org/abs/2311.10801v2</link><description>Portfolio management (PM) is a fundamental financial trading task, whichexplores the optimal periodical reallocation of capitals into different stocksto pursue long-term profits. Reinforcement learning (RL) has recently shown itspotential to train profitable agents for PM through interacting with financialmarkets. However, existing work mostly focuses on fixed stock pools, which isinconsistent with investors' practical demand. Specifically, the target stockpool of different investors varies dramatically due to their discrepancy onmarket states and individual investors may temporally adjust stocks they desireto trade (e.g., adding one popular stocks), which lead to customizable stockpools (CSPs). Existing RL methods require to retrain RL agents even with a tinychange of the stock pool, which leads to high computational cost and unstableperformance. To tackle this challenge, we propose EarnMore, a rEinforcementleARNing framework with Maskable stOck REpresentation to handle PM with CSPsthrough one-shot training in a global stock pool (GSP). Specifically, we firstintroduce a mechanism to mask out the representation of the stocks outside thetarget pool. Second, we learn meaningful stock representations through aself-supervised masking and reconstruction process. Third, a re-weightingmechanism is designed to make the portfolio concentrate on favorable stocks andneglect the stocks outside the target pool. Through extensive experiments on 8subset stock pools of the US stock market, we demonstrate that EarnMoresignificantly outperforms 14 state-of-the-art baselines in terms of 6 popularfinancial metrics with over 40% improvement on profit.</description><author>Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, Bo An</author><pubDate>Tue, 21 Nov 2023 17:11:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10801v2</guid></item><item><title>The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks</title><link>http://arxiv.org/abs/2306.17844v2</link><description>Do neural networks, trained on well-understood algorithmic tasks, reliablyrediscover known algorithms for solving those tasks? Several recent studies, ontasks ranging from group arithmetic to in-context linear regression, havesuggested that the answer is yes. Using modular addition as a prototypicalproblem, we show that algorithm discovery in neural networks is sometimes morecomplex. Small changes to model hyperparameters and initializations can inducethe discovery of qualitatively different algorithms from a fixed training set,and even parallel implementations of multiple such algorithms. Some networkstrained to perform modular addition implement a familiar Clock algorithm;others implement a previously undescribed, less intuitive, but comprehensibleprocedure which we term the Pizza algorithm, or a variety of even more complexprocedures. Our results show that even simple learning problems can admit asurprising diversity of solutions, motivating the development of new tools forcharacterizing the behavior of neural networks across their algorithmic phasespace.</description><author>Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas</author><pubDate>Tue, 21 Nov 2023 17:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17844v2</guid></item><item><title>XPert: Peripheral Circuit &amp; Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing</title><link>http://arxiv.org/abs/2303.17646v2</link><description>The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)implemented on In-memory Computing (IMC) architectures primarily depend on theDNN architecture and the peripheral circuit parameters. It is thereforeessential to holistically co-search the network and peripheral parameters toachieve optimal performance. To this end, we propose XPert, which co-searchesnetwork architecture in tandem with peripheral parameters such as the type andprecision of analog-to-digital converters, crossbar column sharing and thelayer-specific input precision using an optimization-based design spaceexploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lowerEDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper isavailable at https://github.com/Intelligent-Computing-Lab-Yale/XPert.</description><author>Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda</author><pubDate>Tue, 21 Nov 2023 17:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17646v2</guid></item><item><title>Soft Random Sampling: A Theoretical and Empirical Analysis</title><link>http://arxiv.org/abs/2311.12727v1</link><description>Soft random sampling (SRS) is a simple yet effective approach for efficienttraining of large-scale deep neural networks when dealing with massive data.SRS selects a subset uniformly at random with replacement from the full dataset in each epoch. In this paper, we conduct a theoretical and empiricalanalysis of SRS. First, we analyze its sampling dynamics including datacoverage and occupancy. Next, we investigate its convergence with non-convexobjective functions and give the convergence rate. Finally, we provide itsgeneralization performance. We empirically evaluate SRS for image recognitionon CIFAR10 and automatic speech recognition on Librispeech and an in-housepayload dataset to demonstrate its effectiveness. Compared to existingcoreset-based data selection methods, SRS offers a better accuracy-efficiencytrade-off. Especially on real-world industrial scale data sets, it is shown tobe a powerful training strategy with significant speedup and competitiveperformance with almost no additional computing cost.</description><author>Xiaodong Cui, Ashish Mittal, Songtao Lu, Wei Zhang, George Saon, Brian Kingsbury</author><pubDate>Tue, 21 Nov 2023 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12727v1</guid></item><item><title>Flexible variable selection in the presence of missing data</title><link>http://arxiv.org/abs/2202.12989v4</link><description>In many applications, it is of interest to identify a parsimonious set offeatures, or panel, from multiple candidates that achieves a desired level ofperformance in predicting a response. This task is often complicated inpractice by missing data arising from the sampling design or other randommechanisms. Most recent work on variable selection in missing data contextsrelies in some part on a finite-dimensional statistical model, e.g., ageneralized or penalized linear model. In cases where this model ismisspecified, the selected variables may not all be truly scientificallyrelevant and can result in panels with suboptimal classification performance.To address this limitation, we propose a nonparametric variable selectionalgorithm combined with multiple imputation to develop flexible panels in thepresence of missing-at-random data. We outline strategies based on the proposedalgorithm that achieve control of commonly used error rates. Throughsimulations, we show that our proposal has good operating characteristics andresults in panels with higher classification and variable selection performancecompared to several existing penalized regression approaches in cases where ageneralized linear model is misspecified. Finally, we use the proposed methodto develop biomarker panels for separating pancreatic cysts with differingmalignancy potential in a setting where complicated missingness in thebiomarkers arose due to limited specimen volumes.</description><author>B. D. Williamson, Y. Huang</author><pubDate>Tue, 21 Nov 2023 16:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12989v4</guid></item><item><title>A Unified Framework for Pattern Recovery in Penalized and Thresholded Estimation and its Geometry</title><link>http://arxiv.org/abs/2307.10158v3</link><description>We consider the framework of penalized estimation where the penalty term isgiven by a real-valued polyhedral gauge, which encompasses methods such asLASSO (and many variants thereof such as the generalized LASSO), SLOPE, OSCAR,PACS and others. Each of these estimators can uncover a different structure or``pattern'' of the unknown parameter vector. We define a general notion ofpatterns based on subdifferentials and formalize an approach to measure patterncomplexity. For pattern recovery, we provide a minimal condition for aparticular pattern to be detected by the procedure with positive probability,the so-called accessibility condition. Using our approach, we also introducethe stronger noiseless recovery condition. For the LASSO, it is well known thatthe irrepresentability condition is necessary for pattern recovery withprobability larger than $1/2$ and we show that the noiseless recovery playsexactly the same role, thereby extending and unifying the irrepresentabilitycondition of the LASSO to a broad class of penalized estimators. We also showthat the noiseless recovery condition can be relaxed when turning tothresholded penalized estimators, extending the idea of the thresholded LASSO:we prove that the accessibility condition is already sufficient (and necessary)for sure pattern recovery by thresholded penalized estimation provided that thesignal of the pattern is large enough. Throughout the article, we demonstratehow our findings can be interpreted through a geometrical lens.</description><author>Piotr Graczyk, Ulrike Schneider, Tomasz Skalski, Patrick Tardivel</author><pubDate>Tue, 21 Nov 2023 16:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10158v3</guid></item><item><title>Attacking Motion Planners Using Adversarial Perception Errors</title><link>http://arxiv.org/abs/2311.12722v1</link><description>Autonomous driving (AD) systems are often built and tested in a modularfashion, where the performance of different modules is measured usingtask-specific metrics. These metrics should be chosen so as to capture thedownstream impact of each module and the performance of the system as a whole.For example, high perception quality should enable prediction and planning tobe performed safely. Even though this is true in general, we show here that itis possible to construct planner inputs that score very highly on variousperception quality metrics but still lead to planning failures. In an analogyto adversarial attacks on image classifiers, we call such inputs\textbf{adversarial perception errors} and show they can be systematicallyconstructed using a simple boundary-attack algorithm. We demonstrate theeffectiveness of this algorithm by finding attacks for two different black-boxplanners in several urban and highway driving scenarios using the CARLAsimulator. Finally, we analyse the properties of these attacks and show thatthey are isolated in the input space of the planner, and discuss theirimplications for AD system deployment and testing.</description><author>Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller</author><pubDate>Tue, 21 Nov 2023 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12722v1</guid></item><item><title>Development of a Legal Document AI-Chatbot</title><link>http://arxiv.org/abs/2311.12719v1</link><description>With the exponential growth of digital data and the increasing complexity oflegal documentation, there is a pressing need for efficient and intelligenttools to streamline the handling of legal documents.With the recentdevelopments in the AI field, especially in chatbots, it cannot be ignored as avery compelling solution to this problem.An insight into the process ofcreating a Legal Documentation AI Chatbot with as many relevant features aspossible within the given time frame is presented.The development of eachcomponent of the chatbot is presented in detail.Each component's workings andfunctionality has been discussed.Starting from the build of the Android app andthe Langchain query processing code till the integration of both through aFlask backend and REST API methods.</description><author>Pranav Nataraj Devaraj, Rakesh Teja P V, Aaryav Gangrade, Manoj Kumar R</author><pubDate>Tue, 21 Nov 2023 16:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12719v1</guid></item><item><title>minimax: Efficient Baselines for Autocurricula in JAX</title><link>http://arxiv.org/abs/2311.12716v1</link><description>Unsupervised environment design (UED) is a form of automatic curriculumlearning for training robust decision-making agents to zero-shot transfer intounseen environments. Such autocurricula have received much interest from the RLcommunity. However, UED experiments, based on CPU rollouts and GPU modelupdates, have often required several weeks of training. This computerequirement is a major obstacle to rapid innovation for the field. This workintroduces the minimax library for UED training on accelerated hardware. UsingJAX to implement fully-tensorized environments and autocurriculum algorithms,minimax allows the entire training loop to be compiled for hardwareacceleration. To provide a petri dish for rapid experimentation, minimaxincludes a tensorized grid-world based on MiniGrid, in addition to reusableabstractions for conducting autocurricula in procedurally-generatedenvironments. With these components, minimax provides strong UED baselines,including new parallelized variants, which achieve over 120$\times$ speedups inwall time compared to previous implementations when training with equal batchsizes. The minimax library is available under the Apache 2.0 license athttps://github.com/facebookresearch/minimax.</description><author>Minqi Jiang, Michael Dennis, Edward Grefenstette, Tim Rocktäschel</author><pubDate>Tue, 21 Nov 2023 16:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12716v1</guid></item><item><title>Attacks of fairness in Federated Learning</title><link>http://arxiv.org/abs/2311.12715v1</link><description>Federated Learning is an important emerging distributed training paradigmthat keeps data private on clients. It is now well understood that bycontrolling only a small subset of FL clients, it is possible to introduce abackdoor to a federated learning model, in the presence of certain attributes.In this paper, we present a new type of attack that compromises the fairness ofthe trained model. Fairness is understood to be the attribute-level performancedistribution of a trained model. It is particularly salient in domains where,for example, skewed accuracy discrimination between subpopulations could havedisastrous consequences. We find that by employing a threat model similar tothat of a backdoor attack, an attacker is able to influence the aggregatedmodel to have an unfair performance distribution between any given set ofattributes. Furthermore, we find that this attack is possible by controllingonly a single client. While combating naturally induced unfairness in FL haspreviously been discussed in depth, its artificially induced kind has beenneglected. We show that defending against attacks on fairness should be acritical consideration in any situation where unfairness in a trained modelcould benefit a user who participated in its training.</description><author>Joseph Rance, Filip Svoboda</author><pubDate>Tue, 21 Nov 2023 16:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12715v1</guid></item><item><title>Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics</title><link>http://arxiv.org/abs/2311.12713v1</link><description>Machine learning with neural networks is now becoming a more and morepowerful tool for various tasks, such as natural language processing, imagerecognition, winning the game, and even for the issues of physics. Althoughthere are many studies on the application of machine learning to numericalcalculation and the assistance of experimental detection, the methods ofapplying machine learning to find the analytical method are poorly studied. Inthis paper, we propose the frameworks of developing analytical methods inphysics by using the symbolic regression with the Alpha Zero algorithm, that isAlpha Zero for physics (AZfP). As a demonstration, we show that AZfP can derivethe high-frequency expansion in the Floquet systems. AZfP may have thepossibility of developing a new theoretical framework in physics.</description><author>Yoshihiro Michishita</author><pubDate>Tue, 21 Nov 2023 16:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12713v1</guid></item><item><title>Relphormer: Relational Graph Transformer for Knowledge Graph Representations</title><link>http://arxiv.org/abs/2205.10852v6</link><description>Transformers have achieved remarkable performance in widespread fields,including natural language processing, computer vision and graph mining.However, vanilla Transformer architectures have not yielded promisingimprovements in the Knowledge Graph (KG) representations, where thetranslational distance paradigm dominates this area. Note that vanillaTransformer architectures struggle to capture the intrinsically heterogeneousstructural and semantic information of knowledge graphs. To this end, wepropose a new variant of Transformer for knowledge graph representations dubbedRelphormer. Specifically, we introduce Triple2Seq which can dynamically samplecontextualized sub-graph sequences as the input to alleviate the heterogeneityissue. We propose a novel structure-enhanced self-attention mechanism to encodethe relational information and keep the semantic information within entitiesand relations. Moreover, we utilize masked knowledge modeling for generalknowledge graph representation learning, which can be applied to variousKG-based tasks including knowledge graph completion, question answering, andrecommendation. Experimental results on six datasets show that Relphormer canobtain better performance compared with baselines. Code is available inhttps://github.com/zjunlp/Relphormer.</description><author>Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Feiyu Xiong, Ningyu Zhang</author><pubDate>Tue, 21 Nov 2023 16:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10852v6</guid></item><item><title>WEAR: An Outdoor Sports Dataset for Wearable and Egocentric Activity Recognition</title><link>http://arxiv.org/abs/2304.05088v3</link><description>Though research has shown the complementarity of camera- and inertial-baseddata, datasets which offer both egocentric video and inertial-based sensor dataremain scarce. In this paper, we introduce WEAR, an outdoor sports dataset forboth vision- and inertial-based human activity recognition (HAR). The datasetcomprises data from 18 participants performing a total of 18 different workoutactivities with untrimmed inertial (acceleration) and camera (egocentric video)data recorded at 10 different outside locations. Unlike previous egocentricdatasets, WEAR provides a challenging prediction scenario marked by purposelyintroduced activity variations as well as an overall small information overlapacross modalities. Benchmark results obtained using each modality separatelyshow that each modality interestingly offers complementary strengths andweaknesses in their prediction performance. Further, in light of the recentsuccess of temporal action localization models following the architecturedesign of the ActionFormer, we demonstrate their versatility by applying themin a plain fashion using vision, inertial and combined (vision + inertial)features as input. Results demonstrate both the applicability of vision-basedtemporal action localization models for inertial data and fusing bothmodalities by means of simple concatenation, with the combined approach (vision+ inertial features) being able to produce the highest mean average precisionand close-to-best F1-score. The dataset and code to reproduce experiments ispublicly available via: https://mariusbock.github.io/wear/</description><author>Marius Bock, Hilde Kuehne, Kristof Van Laerhoven, Michael Moeller</author><pubDate>Tue, 21 Nov 2023 16:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05088v3</guid></item><item><title>LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT</title><link>http://arxiv.org/abs/2306.17103v3</link><description>We introduce LyricWhiz, a robust, multilingual, and zero-shot automaticlyrics transcription method achieving state-of-the-art performance on variouslyrics transcription datasets, even in challenging genres such as rock andmetal. Our novel, training-free approach utilizes Whisper, a weakly supervisedrobust speech recognition model, and GPT-4, today's most performant chat-basedlarge language model. In the proposed method, Whisper functions as the "ear" bytranscribing the audio, while GPT-4 serves as the "brain," acting as anannotator with a strong performance for contextualized output selection andcorrection. Our experiments show that LyricWhiz significantly reduces WordError Rate compared to existing methods in English and can effectivelytranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz tocreate the first publicly available, large-scale, multilingual lyricstranscription dataset with a CC-BY-NC-SA copyright license, based onMTG-Jamendo, and offer a human-annotated subset for noise level estimation andevaluation. We anticipate that our proposed method and dataset will advance thedevelopment of multilingual lyrics transcription, a challenging and emergingtask.</description><author>Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo</author><pubDate>Tue, 21 Nov 2023 16:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17103v3</guid></item><item><title>Regression-Based Analysis of Multimodal Single-Cell Data Integration Strategies</title><link>http://arxiv.org/abs/2311.12711v1</link><description>Multimodal single-cell technologies enable the simultaneous collection ofdiverse data types from individual cells, enhancing our understanding ofcellular states. However, the integration of these datatypes and modeling theinterrelationships between modalities presents substantial computational andanalytical challenges in disease biomarker detection and drug discovery.Established practices rely on isolated methodologies to investigate individualmolecular aspects separately, often resulting in inaccurate analyses. Toaddress these obstacles, distinct Machine Learning Techniques are leveraged,each of its own kind to model the co-variation of DNA to RNA, and finally tosurface proteins in single cells during hematopoietic stem cell development,which simplifies understanding of underlying cellular mechanisms and immuneresponses. Experiments conducted on a curated subset of a 300,000-cell timecourse dataset, highlights the exceptional performance of Echo State Networks,boasting a remarkable state-of-the-art correlation score of 0.94 and 0.895 onMulti-omic and CiteSeq datasets. Beyond the confines of this study, thesefindings hold promise for advancing comprehension of cellular differentiationand function, leveraging the potential of Machine Learning.</description><author>Bhavya Mehta, Nirmit Deliwala, Madhav Chandane</author><pubDate>Tue, 21 Nov 2023 16:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12711v1</guid></item><item><title>Detect Every Thing with Few Examples</title><link>http://arxiv.org/abs/2309.12969v2</link><description>Open-set object detection aims at detecting arbitrary categories beyond thoseseen during training. Most recent advancements have adopted the open-vocabularyparadigm, utilizing vision-language backbones to represent categories withlanguage. In this paper, we introduce DE-ViT, an open-set object detector thatemploys vision-only DINOv2 backbones and learns new categories through exampleimages instead of language. To improve general detection ability, we transformmulti-classification tasks into binary classification tasks while bypassingper-class inference, and propose a novel region propagation technique forlocalization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shotobject detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms theopen-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViTsurpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot andone-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabularySoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available athttps://github.com/mlzxy/devit.</description><author>Xinyu Zhang, Yuting Wang, Abdeslam Boularias</author><pubDate>Tue, 21 Nov 2023 16:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12969v2</guid></item><item><title>Machine-learning-accelerated simulations to enable automatic surface reconstruction</title><link>http://arxiv.org/abs/2305.07251v2</link><description>Understanding material surfaces and interfaces is vital in applications likecatalysis or electronics. By combining energies from electronic structure withstatistical mechanics, ab initio simulations can in principle predict thestructure of material surfaces as a function of thermodynamic variables.However, accurate energy simulations are prohibitive when coupled to the vastphase space that must be statistically sampled. Here, we present a bi-facetedcomputational loop to predict surface phase diagrams of multi-componentmaterials that accelerates both the energy scoring and statistical samplingmethods. Fast, scalable, and data-efficient machine learning interatomicpotentials are trained on high-throughput density-functional theorycalculations through closed-loop active learning. Markov-chain Monte Carlosampling in the semi-grand canonical ensemble is enabled by using virtualsurface sites. The predicted surfaces for GaN(0001), Si(111), and SrTiO3(001)are in agreement with past work and suggest that the proposed strategy canmodel complex material surfaces and discover previously unreported surfaceterminations.</description><author>Xiaochen Du, James K. Damewood, Jaclyn R. Lunger, Reisel Millan, Bilge Yildiz, Lin Li, Rafael Gómez-Bombarelli</author><pubDate>Tue, 21 Nov 2023 16:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07251v2</guid></item><item><title>Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions</title><link>http://arxiv.org/abs/2311.12707v1</link><description>Standardized, validated questionnaires are vital tools in HCI research andhealthcare, offering dependable self-report data. However, their repeated usein longitudinal or pre-post studies can induce respondent fatigue, impactingdata quality via response biases and decreased response rates. We proposeutilizing large language models (LLMs) to generate diverse questionnaireversions while retaining good psychometric properties. In a longitudinal study,participants engaged with our agent system and responded daily for two weeks toeither a standardized depression questionnaire or one of two LLM-generatedquestionnaire variants, alongside a validated depression questionnaire.Psychometric testing revealed consistent covariation between the externalcriterion and the focal measure administered across the three conditions,demonstrating the reliability and validity of the LLM-generated variants.Participants found the repeated administration of the standardizedquestionnaire significantly more repetitive compared to the variants. Ourfindings highlight the potential of LLM-generated variants to invigoratequestionnaires, fostering engagement and interest without compromisingvalidity.</description><author>Hye Sun Yun, Mehdi Arjmand, Phillip Raymond Sherlock, Michael Paasche-Orlow, James W. Griffith, Timothy Bickmore</author><pubDate>Tue, 21 Nov 2023 16:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12707v1</guid></item><item><title>Cascade Learning Localises Discriminant Features in Visual Scene Classification</title><link>http://arxiv.org/abs/2311.12704v1</link><description>Lack of interpretability of deep convolutional neural networks (DCNN) is awell-known problem particularly in the medical domain as clinicians wanttrustworthy automated decisions. One way to improve trust is to demonstrate thelocalisation of feature representations with respect to expert labeled regionsof interest. In this work, we investigate the localisation of features learnedvia two varied learning paradigms and demonstrate the superiority of onelearning approach with respect to localisation. Our analysis on medical andnatural datasets show that the traditional end-to-end (E2E) learning strategyhas a limited ability to localise discriminative features across multiplenetwork layers. We show that a layer-wise learning strategy, namely cascadelearning (CL), results in more localised features. Considering localisationaccuracy, we not only show that CL outperforms E2E but that it is a promisingmethod of predicting regions. On the YOLO object detection framework, our bestresult shows that CL outperforms the E2E scheme by $2\%$ in mAP.</description><author>Junwen Wang, Katayoun Farrahi</author><pubDate>Tue, 21 Nov 2023 16:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12704v1</guid></item><item><title>Differentially Private Optimizers Can Learn Adversarially Robust Models</title><link>http://arxiv.org/abs/2211.08942v2</link><description>Machine learning models have shone in a variety of domains and attractedincreasing attention from both the security and the privacy communities. Oneimportant yet worrying question is: Will training models under the differentialprivacy (DP) constraint have an unfavorable impact on their adversarialrobustness? While previous works have postulated that privacy comes at the costof worse robustness, we give the first theoretical analysis to show that DPmodels can indeed be robust and accurate, even sometimes more robust than theirnaturally-trained non-private counterparts. We observe three key factors thatinfluence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DPoptimizers are critical; (2) pre-training on public data significantlymitigates the accuracy and robustness drop; (3) choice of DP optimizers makes adifference. With these factors set properly, we achieve 90\% natural accuracy,72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$attack, and 69\% robust accuracy ($+16\%$ than the non-private model) withpre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with$\epsilon=2$. In fact, we show both theoretically and empirically that DPmodels are Pareto optimal on the accuracy-robustness tradeoff. Empirically, therobustness of DP models is consistently observed across various datasets andmodels. We believe our encouraging results are a significant step towardstraining models that are private as well as robust.</description><author>Yuan Zhang, Zhiqi Bu</author><pubDate>Tue, 21 Nov 2023 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08942v2</guid></item><item><title>Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study</title><link>http://arxiv.org/abs/2311.12699v1</link><description>Large Language Models (LLMs) have garnered significant attention for theirpowerful ability in natural language understanding and reasoning. In thispaper, we present a comprehensive empirical study to explore the performance ofLLMs on misinformation detection tasks. This study stands as the pioneeringinvestigation into the understanding capabilities of multiple LLMs regardingboth content and propagation across social media platforms. Our empiricalstudies on five misinformation detection datasets show that LLMs with diverseprompts achieve comparable performance in text-based misinformation detectionbut exhibit notably constrained capabilities in comprehending propagationstructure compared to existing models in propagation-based misinformationdetection. Besides, we further design four instruction-tuned strategies toenhance LLMs for both content and propagation-based misinformation detection.These strategies boost LLMs to actively learn effective features from multipleinstances or hard instances, and eliminate irrelevant propagation structures,thereby achieving better detection performance. Extensive experiments furtherdemonstrate LLMs would play a better capacity in content and propagationstructure under these proposed strategies and achieve promising detectionperformance. These findings highlight the potential ability of LLMs to detectmisinformation.</description><author>Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, Songlin Hu</author><pubDate>Tue, 21 Nov 2023 16:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12699v1</guid></item><item><title>Low Dimensional Invariant Embeddings for Universal Geometric Learning</title><link>http://arxiv.org/abs/2205.02956v3</link><description>This paper studies separating invariants: mappings on $D$ dimensional domainswhich are invariant to an appropriate group action, and which separate orbits.The motivation for this study comes from the usefulness of separatinginvariants in proving universality of equivariant neural network architectures. We observe that in several cases the cardinality of separating invariantsproposed in the machine learning literature is much larger than the dimension$D$. As a result, the theoretical universal constructions based on theseseparating invariants is unrealistically large. Our goal in this paper is toresolve this issue. We show that when a continuous family of semi-algebraic separating invariantsis available, separation can be obtained by randomly selecting $2D+1 $ of theseinvariants. We apply this methodology to obtain an efficient scheme forcomputing separating invariants for several classical group actions which havebeen studied in the invariant learning literature. Examples include matrixmultiplication actions on point clouds by permutations, rotations, and variousother linear groups. Often the requirement of invariant separation is relaxed and only genericseparation is required. In this case, we show that only $D+1$ invariants arerequired. More importantly, generic invariants are often significantly easierto compute, as we illustrate by discussing generic and full separation forweighted graphs. Finally we outline an approach for proving that separatinginvariants can be constructed also when the random parameters have finiteprecision.</description><author>Nadav Dym, Steven J. Gortler</author><pubDate>Tue, 21 Nov 2023 15:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.02956v3</guid></item><item><title>Pairing-based graph neural network for simulating quantum materials</title><link>http://arxiv.org/abs/2311.02143v2</link><description>We develop a pairing-based graph neural network for simulating quantummany-body systems. Our architecture augments a BCS-type geminal wavefunctionwith a generalized pair amplitude parameterized by a graph neural network.Variational Monte Carlo with our neural network simultaneously provides anaccurate, flexible, and scalable method for simulating many-electron systems.We apply this method to two-dimensional semiconductor electron-hole bilayersand obtain accurate results on a variety of interaction-induced phases,including the exciton Bose-Einstein condensate, electron-hole superconductor,and bilayer Wigner crystal. Our study demonstrates the potential ofphysically-motivated neural network wavefunctions for quantum materialssimulations.</description><author>Di Luo, David D. Dai, Liang Fu</author><pubDate>Tue, 21 Nov 2023 15:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02143v2</guid></item><item><title>Fair Text Classification with Wasserstein Independence</title><link>http://arxiv.org/abs/2311.12689v1</link><description>Group fairness is a central research topic in text classification, wherereaching fair treatment between sensitive groups (e.g. women vs. men) remainsan open challenge. This paper presents a novel method for mitigating biases inneural text classification, agnostic to the model architecture. Considering thedifficulty to distinguish fair from unfair information in a text encoder, wetake inspiration from adversarial training to induce Wasserstein independencebetween representations learned to predict our target label and the oneslearned to predict some sensitive attribute. Our approach provides twosignificant advantages. Firstly, it does not require annotations of sensitiveattributes in both testing and training data. This is more suitable forreal-life scenarios compared to existing methods that require annotations ofsensitive attributes at train time. Second, our approach exhibits a comparableor better fairness-accuracy trade-off compared to existing methods.</description><author>Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier</author><pubDate>Tue, 21 Nov 2023 15:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12689v1</guid></item><item><title>On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning</title><link>http://arxiv.org/abs/2311.12688v1</link><description>Bayesian deep learning and conformal prediction are two methods that havebeen used to convey uncertainty and increase safety in machine learningsystems. We focus on combining Bayesian deep learning with split conformalprediction and how this combination effects out-of-distribution coverage;particularly in the case of multiclass image classification. We suggest that ifthe model is generally underconfident on the calibration set, then theresultant conformal sets may exhibit worse out-of-distribution coveragecompared to simple predictive credible sets. Conversely, if the model isoverconfident on the calibration set, the use of conformal prediction mayimprove out-of-distribution coverage. We evaluate prediction sets as a resultof combining split conformal methods and neural networks trained with (i)stochastic gradient descent, (ii) deep ensembles, and (iii) mean-fieldvariational inference. Our results suggest that combining Bayesian deeplearning models with split conformal prediction can, in some cases, causeunintended consequences such as reducing out-of-distribution coverage.</description><author>Paul Scemama, Ariel Kapusta</author><pubDate>Tue, 21 Nov 2023 15:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12688v1</guid></item><item><title>survex: an R package for explaining machine learning survival models</title><link>http://arxiv.org/abs/2308.16113v2</link><description>Due to their flexibility and superior performance, machine learning modelsfrequently complement and outperform traditional statistical survival models.However, their widespread adoption is hindered by a lack of user-friendly toolsto explain their internal operations and prediction rationales. To tackle thisissue, we introduce the survex R package, which provides a cohesive frameworkfor explaining any survival model by applying explainable artificialintelligence techniques. The capabilities of the proposed software encompassunderstanding and diagnosing survival models, which can lead to theirimprovement. By revealing insights into the decision-making process, such asvariable effects and importances, survex enables the assessment of modelreliability and the detection of biases. Thus, transparency and responsibilitymay be promoted in sensitive areas, such as biomedical research and healthcareapplications.</description><author>Mikołaj Spytek, Mateusz Krzyziński, Sophie Hanna Langbein, Hubert Baniecki, Marvin N. Wright, Przemysław Biecek</author><pubDate>Tue, 21 Nov 2023 15:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16113v2</guid></item><item><title>Managing ML-Based Application Non-Functional Behavior: A Multi-Model Approach</title><link>http://arxiv.org/abs/2311.12686v1</link><description>Modern applications are increasingly driven by Machine Learning (ML) modelswhose non-deterministic behavior is affecting the entire application life cyclefrom design to operation. The pervasive adoption of ML is urgently calling forapproaches that guarantee a stable non-functional behavior of ML-basedapplications over time and across model changes. To this aim, non-functionalproperties of ML models, such as privacy, confidentiality, fairness, andexplainability, must be monitored, verified, and maintained. This need is evenmore pressing when modern applications operate in the edge-cloud continuum,increasing their complexity and dynamicity. Existing approaches mostly focus oni) implementing classifier selection solutions according to the functionalbehavior of ML models, ii) finding new algorithmic solutions to this need, suchas continuous re-training. In this paper, we propose a multi-model approachbuilt on dynamic classifier selection, where multiple ML models showing similarnon-functional properties are made available to the application and one modelis selected over time according to (dynamic and unpredictable) contextualchanges. Our solution goes beyond the state of the art by providing anarchitectural and methodological approach that continuously guarantees a stablenon-functional behavior of ML-based applications, is applicable to different MLmodels, and is driven by non-functional properties assessed on the modelsthemselves. It consists of a two-step process working during applicationoperation, where model assessment verifies non-functional properties of MLmodels trained and selected at development time, and model substitutionguarantees a continuous and stable support of non-functional properties. Weexperimentally evaluate our solution in a real-world scenario focusing onnon-functional property fairness.</description><author>Marco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani, Paolo G. Panero</author><pubDate>Tue, 21 Nov 2023 15:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12686v1</guid></item><item><title>Adversarial Reweighting Guided by Wasserstein Distance for Bias Mitigation</title><link>http://arxiv.org/abs/2311.12684v1</link><description>The unequal representation of different groups in a sample population canlead to discrimination of minority groups when machine learning models makeautomated decisions. To address these issues, fairness-aware machine learningjointly optimizes two (or more) metrics aiming at predictive effectiveness andlow unfairness. However, the inherent under-representation of minorities in thedata makes the disparate treatment of subpopulations less noticeable anddifficult to deal with during learning. In this paper, we propose a noveladversarial reweighting method to address such \emph{representation bias}. Tobalance the data distribution between the majority and the minority groups, ourapproach deemphasizes samples from the majority group. To minimize empiricalrisk, our method prefers samples from the majority group that are close to theminority group as evaluated by the Wasserstein distance. Our theoreticalanalysis shows the effectiveness of our adversarial reweighting approach.Experiments demonstrate that our approach mitigates bias without sacrificingclassification accuracy, outperforming related state-of-the-art methods onimage and tabular benchmark datasets.</description><author>Xuan Zhao, Simone Fabbrizzi, Paula Reyero Lobo, Siamak Ghodsi, Klaus Broelemann, Steffen Staab, Gjergji Kasneci</author><pubDate>Tue, 21 Nov 2023 15:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12684v1</guid></item><item><title>Influencer Videos: Unboxing the Mystique</title><link>http://arxiv.org/abs/2012.12311v3</link><description>Influencer marketing has become a very popular tool to reach customers.Despite the rapid growth in influencer videos, there has been little researchon the effectiveness of their constituent features in explaining videoengagement. We study YouTube influencers and analyze their unstructured videodata across text, audio and images using an "interpretable deep learning"framework that accomplishes both goals of prediction and interpretation. Ourprediction-based approach analyzes unstructured data and finds that "what issaid" in words (text) is more influential than "how it is said" in imagery(images) or acoustics (audio). Our novel interpretation-based approach isimplemented after completion of model prediction by analyzing the same sourceof unstructured data to measure importance attributed to the video features. Weeliminate several spurious relationships in two steps, identifying a subset ofrelationships which are confirmed using theory. We uncover novel findings thatestablish distinct associations for measures of shallow and deep engagementbased on the dual-system framework of human thinking. Our approach is validatedusing simulated data, and we discuss the learnings from our findings forinfluencers and brands.</description><author>Prashant Rajaram, Puneet Manchanda</author><pubDate>Tue, 21 Nov 2023 15:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.12311v3</guid></item><item><title>Transferring to Real-World Layouts: A Depth-aware Framework for Scene Adaptation</title><link>http://arxiv.org/abs/2311.12682v1</link><description>Scene segmentation via unsupervised domain adaptation (UDA) enables thetransfer of knowledge acquired from source synthetic data to real-world targetdata, which largely reduces the need for manual pixel-level annotations in thetarget domain. To facilitate domain-invariant feature learning, existingmethods typically mix data from both the source domain and target domain bysimply copying and pasting the pixels. Such vanilla methods are usuallysub-optimal since they do not take into account how well the mixed layoutscorrespond to real-world scenarios. Real-world scenarios are with an inherentlayout. We observe that semantic categories, such as sidewalks, buildings, andsky, display relatively consistent depth distributions, and could be clearlydistinguished in a depth map. Based on such observation, we propose adepth-aware framework to explicitly leverage depth estimation to mix thecategories and facilitate the two complementary tasks, i.e., segmentation anddepth learning in an end-to-end manner. In particular, the framework contains aDepth-guided Contextual Filter (DCF) forndata augmentation and a cross-taskencoder for contextual learning. DCF simulates the real-world layouts, whilethe cross-task encoder further adaptively fuses the complementing featuresbetween two tasks. Besides, it is worth noting that several public datasets donot provide depth annotation. Therefore, we leverage the off-the-shelf depthestimation network to generate the pseudo depth. Extensive experiments showthat our proposed methods, even with pseudo depth, achieve competitiveperformance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapesand 69.3 mIoU on Synthia to Cityscapes.</description><author>Mu Chen, Zhedong Zheng, Yi Yang</author><pubDate>Tue, 21 Nov 2023 15:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12682v1</guid></item><item><title>BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</title><link>http://arxiv.org/abs/2311.12679v1</link><description>Capturing smooth motions from videos using markerless techniques typicallyinvolves complex processes such as temporal constraints, multiple stages withdata-driven regression and optimization, and bundle solving over temporalwindows. These processes can be inefficient and require tuning multipleobjectives across stages. In contrast, BundleMoCap introduces a novel andefficient approach to this problem. It solves the motion capture task in asingle stage, eliminating the need for temporal smoothness objectives whilestill delivering smooth motions. BundleMoCap outperforms the state-of-the-artwithout increasing complexity. The key concept behind BundleMoCap is manifoldinterpolation between latent keyframes. By relying on a local manifoldsmoothness assumption, we can efficiently solve a bundle of frames using asingle code. Additionally, the method can be implemented as a sliding windowoptimization and requires only the first frame to be properly initialized,reducing the overall computational burden. BundleMoCap's strength lies in itsability to achieve high-quality motion capture results with simplicity andefficiency. More details can be found at https://moverseai.github.io/bundle/.</description><author>Georgios Albanis, Nikolaos Zioulis, Kostas Kolomvatsos</author><pubDate>Tue, 21 Nov 2023 15:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12679v1</guid></item><item><title>Interpretation of the Transformer and Improvement of the Extractor</title><link>http://arxiv.org/abs/2311.12678v1</link><description>It has been over six years since the Transformer architecture was putforward. Surprisingly, the vanilla Transformer architecture is still widelyused today. One reason is that the lack of deep understanding and comprehensiveinterpretation of the Transformer architecture makes it more challenging toimprove the Transformer architecture. In this paper, we first interpret theTransformer architecture comprehensively in plain words based on ourunderstanding and experiences. The interpretations are further proved andverified. These interpretations also cover the Extractor, a family of drop-inreplacements for the multi-head self-attention in the Transformer architecture.Then, we propose an improvement on a type of the Extractor that outperforms theself-attention, without introducing additional trainable parameters.Experimental results demonstrate that the improved Extractor performs evenbetter, showing a way to improve the Transformer architecture.</description><author>Zhe Chen</author><pubDate>Tue, 21 Nov 2023 15:36:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12678v1</guid></item><item><title>Contrastive Left-Right Wearable Sensors (IMUs) Consistency Matching for HAR</title><link>http://arxiv.org/abs/2311.12674v1</link><description>Machine learning algorithms are improving rapidly, but annotating trainingdata remains a bottleneck for many applications. In this paper, we show howreal data can be used for self-supervised learning without any transformationsby taking advantage of the symmetry present in the activities. Our approachinvolves contrastive matching of two different sensors (left and right wrist orleg-worn IMUs) to make representations of co-occurring sensor data more similarand those of non-co-occurring sensor data more different. We test our approachon the Opportunity and MM-Fit datasets. In MM-Fit we show significantimprovement over the baseline supervised and self-supervised method SimCLR,while for Opportunity there is significant improvement over the supervisedbaseline and slight improvement when compared to SimCLR. Moreover, our methodimproves supervised baselines even when using only a small amount of the datafor training. Future work should explore under which conditions our method isbeneficial for human activity recognition systems and other relatedapplications.</description><author>Dominique Nshimyimana, Vitor Fortes Rey, Paul Lukowic</author><pubDate>Tue, 21 Nov 2023 15:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12674v1</guid></item><item><title>Predictive Density Combination Using a Tree-Based Synthesis Function</title><link>http://arxiv.org/abs/2311.12671v1</link><description>Bayesian predictive synthesis (BPS) provides a method for combining multiplepredictive distributions based on agent/expert opinion analysis theory andencompasses a range of existing density forecast pooling methods. The keyingredient in BPS is a ``synthesis'' function. This is typically specifiedparametrically as a dynamic linear regression. In this paper, we develop anonparametric treatment of the synthesis function using regression trees. Weshow the advantages of our tree-based approach in two macroeconomic forecastingapplications. The first uses density forecasts for GDP growth from the euroarea's Survey of Professional Forecasters. The second combines densityforecasts of US inflation produced by many regression models involvingdifferent predictors. Both applications demonstrate the benefits -- in terms ofimproved forecast accuracy and interpretability -- of modeling the synthesisfunction nonparametrically.</description><author>Tony Chernis, Niko Hauzenberger, Florian Huber, Gary Koop, James Mitchell</author><pubDate>Tue, 21 Nov 2023 15:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12671v1</guid></item><item><title>Towards a more inductive world for drug repurposing approaches</title><link>http://arxiv.org/abs/2311.12670v1</link><description>Drug-target interaction (DTI) prediction is a challenging, albeit essentialtask in drug repurposing. Learning on graph models have drawn special attentionas they can significantly reduce drug repurposing costs and time commitment.However, many current approaches require high-demanding additional informationbesides DTIs that complicates their evaluation process and usability.Additionally, structural differences in the learning architecture of currentmodels hinder their fair benchmarking. In this work, we first perform anin-depth evaluation of current DTI datasets and prediction models through arobust benchmarking process, and show that DTI prediction methods based ontransductive models lack generalization and lead to inflated performance whenevaluated as previously done in the literature, hence not being suited for drugrepurposing approaches. We then propose a novel biologically-driven strategyfor negative edge subsampling and show through in vitro validation that newlydiscovered interactions are indeed true. We envision this work as theunderpinning for future fair benchmarking and robust model design. Allgenerated resources and tools are publicly available as a python package.</description><author>Jesus de la Fuente, Guillermo Serrano, Uxía Veleiro, Mikel Casals, Laura Vera, Marija Pizurica, Antonio Pineda-Lucena, Idoia Ochoa, Silve Vicent, Olivier Gevaert, Mikel Hernaez</author><pubDate>Tue, 21 Nov 2023 15:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12670v1</guid></item><item><title>Decodable and Sample Invariant Continuous Object Encoder</title><link>http://arxiv.org/abs/2311.00187v2</link><description>We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of acontinuous object (e.g. a function), HDFE produces an explicit vectorrepresentation of the given object, invariant to the sample distribution anddensity. Sample distribution and density invariance enables HDFE toconsistently encode continuous objects regardless of their sampling, andtherefore allows neural networks to receive continuous objects as inputs formachine learning tasks, such as classification and regression. Besides, HDFEdoes not require any training and is proved to map the object into an organizedembedding space, which facilitates the training of the downstream tasks. Inaddition, the encoding is decodable, which enables neural networks to regresscontinuous objects by regressing their encodings. Therefore, HDFE serves as aninterface for processing continuous objects. We apply HDFE to function-to-function mapping, where vanilla HDFE achievescompetitive performance as the state-of-the-art algorithm. We apply HDFE topoint cloud surface normal estimation, where a simple replacement from PointNetto HDFE leads to immediate 12% and 15% error reductions in two benchmarks. Inaddition, by integrating HDFE into the PointNet-based SOTA network, we improvethe SOTA baseline by 2.5% and 1.7% in the same benchmarks.</description><author>Dehao Yuan, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos</author><pubDate>Tue, 21 Nov 2023 15:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00187v2</guid></item><item><title>Shortcut Learning in Deep Neural Networks</title><link>http://arxiv.org/abs/2004.07780v5</link><description>Deep learning has triggered the current rise of artificial intelligence andis the workhorse of today's machine intelligence. Numerous success stories haverapidly spread all over science, industry and society, but its limitations haveonly recently come into focus. In this perspective we seek to distill how manyof deep learning's problems can be seen as different symptoms of the sameunderlying problem: shortcut learning. Shortcuts are decision rules thatperform well on standard benchmarks but fail to transfer to more challengingtesting conditions, such as real-world scenarios. Related issues are known inComparative Psychology, Education and Linguistics, suggesting that shortcutlearning may be a common characteristic of learning systems, biological andartificial alike. Based on these observations, we develop a set ofrecommendations for model interpretation and benchmarking, highlighting recentadvances in machine learning to improve robustness and transferability from thelab to real-world applications.</description><author>Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann</author><pubDate>Tue, 21 Nov 2023 15:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2004.07780v5</guid></item><item><title>From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design</title><link>http://arxiv.org/abs/2311.12668v1</link><description>Engineering Design is undergoing a transformative shift with the advent ofAI, marking a new era in how we approach product, system, and service planning.Large language models have demonstrated impressive capabilities in enablingthis shift. Yet, with text as their only input modality, they cannot leveragethe large body of visual artifacts that engineers have used for centuries andare accustomed to. This gap is addressed with the release of multimodal visionlanguage models, such as GPT-4V, enabling AI to impact many more types oftasks. In light of these advancements, this paper presents a comprehensiveevaluation of GPT-4V, a vision language model, across a wide spectrum ofengineering design tasks, categorized into four main areas: Conceptual Design,System-Level and Detailed Design, Manufacturing and Inspection, and EngineeringEducation Tasks. Our study assesses GPT-4V's capabilities in design tasks suchas sketch similarity analysis, concept selection using Pugh Charts, materialselection, engineering drawing analysis, CAD generation, topology optimization,design for additive and subtractive manufacturing, spatial reasoningchallenges, and textbook problems. Through this structured evaluation, we notonly explore GPT-4V's proficiency in handling complex design and manufacturingchallenges but also identify its limitations in complex engineering designapplications. Our research establishes a foundation for future assessments ofvision language models, emphasizing their immense potential for innovating andenhancing the engineering design and manufacturing landscape. It alsocontributes a set of benchmark testing datasets, with more than 1000 queries,for ongoing advancements and applications in this field.</description><author>Cyril Picard, Kristen M. Edwards, Anna C. Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, Faez Ahmed</author><pubDate>Tue, 21 Nov 2023 15:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12668v1</guid></item><item><title>SSVEP-DAN: A Data Alignment Network for SSVEP-based Brain Computer Interfaces</title><link>http://arxiv.org/abs/2311.12666v1</link><description>Steady-state visual-evoked potential (SSVEP)-based brain-computer interfaces(BCIs) offer a non-invasive means of communication through high-speed spellersystems. However, their efficiency heavily relies on individual training dataobtained during time-consuming calibration sessions. To address the challengeof data insufficiency in SSVEP-based BCIs, we present SSVEP-DAN, the firstdedicated neural network model designed for aligning SSVEP data acrossdifferent domains, which can encompass various sessions, subjects, or devices.Our experimental results across multiple cross-domain scenarios demonstrateSSVEP-DAN's capability to transform existing source SSVEP data intosupplementary calibration data, significantly enhancing SSVEP decoding accuracyin scenarios with limited calibration data. We envision SSVEP-DAN as a catalystfor practical SSVEP-based BCI applications with minimal calibration. The sourcecodes in this work are available at: https://github.com/CECNL/SSVEP-DAN.</description><author>Sung-Yu Chen, Chi-Min Chang, Kuan-Jung Chiang, Chun-Shu Wei</author><pubDate>Tue, 21 Nov 2023 15:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12666v1</guid></item><item><title>Continual Learning: Applications and the Road Forward</title><link>http://arxiv.org/abs/2311.11908v2</link><description>Continual learning is a sub-field of machine learning, which aims to allowmachine learning models to continuously learn on new data, by accumulatingknowledge without forgetting what was learned in the past. In this work, wetake a step back, and ask: "Why should one care about continual learning in thefirst place?". We set the stage by surveying recent continual learning paperspublished at three major machine learning conferences, and show thatmemory-constrained settings dominate the field. Then, we discuss five openproblems in machine learning, and even though they seem unrelated to continuallearning at first sight, we show that continual learning will inevitably bepart of their solution. These problems are model-editing, personalization,on-device learning, faster (re-)training and reinforcement learning. Finally,by comparing the desiderata from these unsolved problems and the currentassumptions in continual learning, we highlight and discuss four futuredirections for continual learning research. We hope that this work offers aninteresting perspective on the future of continual learning, while displayingits potential value and the paths we have to pursue in order to make itsuccessful. This work is the result of the many discussions the authors had atthe Dagstuhl seminar on Deep Continual Learning, in March 2023.</description><author>Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven</author><pubDate>Tue, 21 Nov 2023 15:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11908v2</guid></item><item><title>The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change</title><link>http://arxiv.org/abs/2311.12664v1</link><description>We present the DURel tool that implements the annotation of semanticproximity between uses of words into an online, open source interface. The toolsupports standardized human annotation as well as computational annotation,building on recent advances with Word-in-Context models. Annotator judgmentsare clustered with automatic graph clustering techniques and visualized foranalysis. This allows to measure word senses with simple and intuitivemicro-task judgments between use pairs, requiring minimal preparation efforts.The tool offers additional functionalities to compare the agreement betweenannotators to guarantee the inter-subjectivity of the obtained judgments and tocalculate summary statistics giving insights into sense frequencydistributions, semantic variation or changes of senses over time.</description><author>Dominik Schlechtweg, Shafqat Mumtaz Virk, Pauline Sander, Emma Sköldberg, Lukas Theuer Linke, Tuo Zhang, Nina Tahmasebi, Jonas Kuhn, Sabine Schulte im Walde</author><pubDate>Tue, 21 Nov 2023 15:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12664v1</guid></item><item><title>Similar Document Template Matching Algorithm</title><link>http://arxiv.org/abs/2311.12663v1</link><description>This study outlines a comprehensive methodology for verifying medicaldocuments, integrating advanced techniques in template extraction, comparison,and fraud detection. It begins with template extraction using sophisticatedregion-of-interest (ROI) methods, incorporating contour analysis and edgeidentification. Pre-processing steps ensure template clarity throughmorphological operations and adaptive thresholding. The template comparisonalgorithm utilizes advanced feature matching with key points and descriptors,enhancing robustness through histogram-based analysis for accountingvariations. Fraud detection involves the SSIM computation and OCR for textualinformation extraction. The SSIM quantifies structural similarity, aiding inpotential match identification. OCR focuses on critical areas like patientdetails, provider information, and billing amounts. Extracted information iscompared with a reference dataset, and confidence thresholding ensures reliablefraud detection. Adaptive parameters enhance system flexibility for dynamicadjustments to varying document layouts. This methodology provides a robustapproach to medical document verification, addressing complexities in templateextraction, comparison, fraud detection, and adaptability to diverse documentstructures.</description><author>Harshitha Yenigalla, Bommareddy Revanth Srinivasa Reddy, Batta Venkata Rahul, Nannapuraju Hemanth Raju</author><pubDate>Tue, 21 Nov 2023 15:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12663v1</guid></item><item><title>Visually Guided Object Grasping</title><link>http://arxiv.org/abs/2311.12660v1</link><description>In this paper we present a visual servoing approach to the problem of objectgrasping and more generally, to the problem of aligning an end-effector with anobject. First we extend the method proposed by Espiau et al. [1] to the case ofa camera which is not mounted onto the robot being controlled and we stress theimportance of the real-time estimation of the image Jacobian. Second, we showhow to represent a grasp or more generally, an alignment between two solids in3-D projective space using an uncalibrated stereo rig. Such a 3-D projectiverepresentation is view-invariant in the sense that it can be easily mapped intoan image set-point without any knowledge about the camera parameters. Third, weperform an analysis of the performances of the visual servoing algorithm and ofthe grasping precision that can be expected from this type of approach.</description><author>Radu Horaud, Fadi Dornaika, Bernard Espiau</author><pubDate>Tue, 21 Nov 2023 15:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12660v1</guid></item><item><title>Carbohydrate NMR chemical shift predictions using E(3) equivariant graph neural networks</title><link>http://arxiv.org/abs/2311.12657v1</link><description>Carbohydrates, vital components of biological systems, are well-known fortheir structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy playsa crucial role in understanding their intricate molecular arrangements and isessential in assessing and verifying the molecular structure of organicmolecules. An important part of this process is to predict the NMR chemicalshift from the molecular structure. This work introduces a novel approach thatleverages E(3) equivariant graph neural networks to predict carbohydrate NMRspectra. Notably, our model achieves a substantial reduction in mean absoluteerror, up to threefold, compared to traditional models that rely solely ontwo-dimensional molecular structure. Even with limited data, the model excels,highlighting its robustness and generalization capabilities. The implicationsare far-reaching and go beyond an advanced understanding of carbohydratestructures and spectral interpretation. For example, it could accelerateresearch in pharmaceutical applications, biochemistry, and structural biology,offering a faster and more reliable analysis of molecular structures.Furthermore, our approach is a key step towards a new data-driven era inspectroscopy, potentially influencing spectroscopic techniques beyond NMR.</description><author>Maria Bånkestad, Keven M. Dorst, Göran Widmalm, Jerk Rönnols</author><pubDate>Tue, 21 Nov 2023 15:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12657v1</guid></item><item><title>Multi-channel Speech Separation Using Spatially Selective Deep Non-linear Filters</title><link>http://arxiv.org/abs/2304.12023v2</link><description>In a multi-channel separation task with multiple speakers, we aim to recoverall individual speech signals from the mixture. In contrast to single-channelapproaches, which rely on the different spectro-temporal characteristics of thespeech signals, multi-channel approaches should additionally utilize thedifferent spatial locations of the sources for a more powerful separationespecially when the number of sources increases. To enhance the spatialprocessing in a multi-channel source separation scenario, in this work, wepropose a deep neural network (DNN) based spatially selective filter (SSF) thatcan be spatially steered to extract the speaker of interest by initializing arecurrent neural network layer with the target direction. We compare theproposed SSF with a common end-to-end direct separation (DS) approach trainedusing utterance-wise permutation invariant training (PIT), which onlyimplicitly learns to perform spatial filtering. We show that the SSF has aclear advantage over a DS approach with the same underlying networkarchitecture when there are more than two speakers in the mixture, which can beattributed to a better use of the spatial information. Furthermore, we findthat the SSF generalizes much better to additional noise sources that were notseen during training and to scenarios with speakers positioned at a similarangle.</description><author>Kristina Tesch, Timo Gerkmann</author><pubDate>Tue, 21 Nov 2023 14:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12023v2</guid></item><item><title>Hand-Eye Calibration</title><link>http://arxiv.org/abs/2311.12655v1</link><description>Whenever a sensor is mounted on a robot hand it is important to know therelationship between the sensor and the hand. The problem of determining thisrelationship is referred to as hand-eye calibration, which is important in atleast two types of tasks: (i) map sensor centered measurements into the robotworkspace and (ii) allow the robot to precisely move the sensor. In the pastsome solutions were proposed in the particular case of a camera. With almost noexception, all existing solutions attempt to solve the homogeneous matrixequation AX=XB. First we show that there are two possible formulations of thehand-eye calibration problem. One formulation is the classical one that we justmentioned. A second formulation takes the form of the following homogeneousmatrix equation: MY=M'YB. The advantage of the latter is that the extrinsic andintrinsic camera parameters need not be made explicit. Indeed, this formulationdirectly uses the 3 by 4 perspective matrices (M and M') associated with twopositions of the camera. Moreover, this formulation together with the classicalone cover a wider range of camera-based sensors to be calibrated with respectto the robot hand. Second, we develop a common mathematical framework to solvefor the hand-eye calibration problem using either of the two formulations. Wepresent two methods, (i) a rotation then translation and (ii) a non-linearsolver for rotation and translation. Third, we perform a stability analysisboth for our two methods and for the classical linear method developed. In thelight of this comparison, the non-linear optimization method, that solves forrotation and translation simultaneously, seems to be the most robust one withrespect to noise and to measurement errors.</description><author>Radu Horaud, Fadi Dornaika</author><pubDate>Tue, 21 Nov 2023 14:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12655v1</guid></item><item><title>SCL-VI: Self-supervised Context Learning for Visual Inspection of Industrial Defects</title><link>http://arxiv.org/abs/2311.06504v2</link><description>The unsupervised visual inspection of defects in industrial products poses asignificant challenge due to substantial variations in product surfaces.Current unsupervised models struggle to strike a balance between detectingtexture and object defects, lacking the capacity to discern latentrepresentations and intricate features. In this paper, we present a novelself-supervised learning algorithm designed to derive an optimal encoder bytackling the renowned jigsaw puzzle. Our approach involves dividing the targetimage into nine patches, tasking the encoder with predicting the relativeposition relationships between any two patches to extract rich semantics.Subsequently, we introduce an affinity-augmentation method to accentuatedifferences between normal and abnormal latent representations. Leveraging theclassic support vector data description algorithm yields final detectionresults. Experimental outcomes demonstrate that our proposed method achievesoutstanding detection and segmentation performance on the widely used MVTec ADdataset, with rates of 95.8% and 96.8%, respectively, establishing astate-of-the-art benchmark for both texture and object defects. Comprehensiveexperimentation underscores the effectiveness of our approach in diverseindustrial applications.</description><author>Peng Wang, Haiming Yao, Wenyong Yu</author><pubDate>Tue, 21 Nov 2023 14:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06504v2</guid></item><item><title>PARK: Parkinson's Analysis with Remote Kinetic-tasks</title><link>http://arxiv.org/abs/2311.12654v1</link><description>We present a web-based framework to screen for Parkinson's disease (PD) byallowing users to perform neurological tests in their homes. Our web frameworkguides the users to complete three tasks involving speech, facial expression,and finger movements. The task videos are analyzed to classify whether theusers show signs of PD. We present the results in an easy-to-understand manner,along with personalized resources to further access to treatment and care. Ourframework is accessible by any major web browser, improving global access toneurological care.</description><author>Md Saiful Islam, Sangwu Lee, Abdelrahman Abdelkader, Sooyong Park, Ehsan Hoque</author><pubDate>Tue, 21 Nov 2023 14:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12654v1</guid></item><item><title>Attending to Graph Transformers</title><link>http://arxiv.org/abs/2302.04181v2</link><description>Recently, transformer architectures for graphs emerged as an alternative toestablished techniques for machine learning with graphs, such as(message-passing) graph neural networks. So far, they have shown promisingempirical results, e.g., on molecular prediction datasets, often attributed totheir ability to circumvent graph neural networks' shortcomings, such asover-smoothing and over-squashing. Here, we derive a taxonomy of graphtransformer architectures, bringing some order to this emerging field. Weoverview their theoretical properties, survey structural and positionalencodings, and discuss extensions for important graph classes, e.g., 3Dmolecular graphs. Empirically, we probe how well graph transformers can recovervarious graph properties, how well they can deal with heterophilic graphs, andto what extent they prevent over-squashing. Further, we outline open challengesand research direction to stimulate future work. Our code is available athttps://github.com/luis-mueller/probing-graph-transformers.</description><author>Luis Müller, Mikhail Galkin, Christopher Morris, Ladislav Rampášek</author><pubDate>Tue, 21 Nov 2023 14:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04181v2</guid></item><item><title>Computing Approximate $\ell_p$ Sensitivities</title><link>http://arxiv.org/abs/2311.04158v2</link><description>Recent works in dimensionality reduction for regression tasks have introducedthe notion of sensitivity, an estimate of the importance of a specificdatapoint in a dataset, offering provable guarantees on the quality of theapproximation after removing low-sensitivity datapoints via subsampling.However, fast algorithms for approximating $\ell_p$ sensitivities, which weshow is equivalent to approximate $\ell_p$ regression, are known for only the$\ell_2$ setting, in which they are termed leverage scores. In this work, we provide efficient algorithms for approximating $\ell_p$sensitivities and related summary statistics of a given matrix. In particular,for a given $n \times d$ matrix, we compute $\alpha$-approximation to its$\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations.For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$sensitivities), we provide an algorithm based on importance sampling of$\ell_p$ Lewis weights, which computes a constant factor approximation to thetotal sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivitycomputations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up toa $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize allthese results to $\ell_p$ norms for $p &gt; 1$. Lastly, we experimentally showthat for a wide class of matrices in real-world datasets, the total sensitivitycan be quickly approximated and is significantly smaller than the theoreticalprediction, demonstrating that real-world datasets have low intrinsic effectivedimensionality.</description><author>Swati Padmanabhan, David P. Woodruff, Qiuyi Zhang</author><pubDate>Tue, 21 Nov 2023 14:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04158v2</guid></item><item><title>FedDRO: Federated Compositional Optimization for Distributionally Robust Learning</title><link>http://arxiv.org/abs/2311.12652v1</link><description>Recently, compositional optimization (CO) has gained popularity because ofits applications in distributionally robust optimization (DRO) and many othermachine learning problems. Large-scale and distributed availability of datademands the development of efficient federated learning (FL) algorithms forsolving CO problems. Developing FL algorithms for CO is particularlychallenging because of the compositional nature of the objective. Moreover,current state-of-the-art methods to solve such problems rely on large batchgradients (depending on the solution accuracy) not feasible for most practicalsettings. To address these challenges, in this work, we propose efficientFedAvg-type algorithms for solving non-convex CO in the FL setting. We firstestablish that vanilla FedAvg is not suitable to solve distributed CO problemsbecause of the data heterogeneity in the compositional objective at each clientwhich leads to the amplification of bias in the local compositional gradientestimates. To this end, we propose a novel FL framework FedDRO that utilizesthe DRO problem structure to design a communication strategy that allows FedAvgto control the bias in the estimation of the compositional gradient. A keynovelty of our work is to develop solution accuracy-independent algorithms thatdo not require large batch gradients (and function evaluations) for solvingfederated CO problems. We establish $\mathcal{O}(\epsilon^{-2})$ sample and$\mathcal{O}(\epsilon^{-3/2})$ communication complexity in the FL setting whileachieving linear speedup with the number of clients. We corroborate ourtheoretical findings with empirical studies on large-scale DRO problems.</description><author>Prashant Khanduri, Chengyin Li, Rafi Ibn Sultan, Yao Qiang, Joerg Kliewer, Dongxiao Zhu</author><pubDate>Tue, 21 Nov 2023 14:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12652v1</guid></item><item><title>Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots</title><link>http://arxiv.org/abs/2311.12651v1</link><description>Precise and rapid delineation of sharp boundaries and robust semantics isessential for numerous downstream robotic tasks, such as robot grasping andmanipulation, real-time semantic mapping, and online sensor calibrationperformed on edge computing units. Although boundary detection and semanticsegmentation are complementary tasks, most studies focus on lightweight modelsfor semantic segmentation but overlook the critical role of boundary detection.In this work, we introduce Mobile-Seed, a lightweight, dual-task frameworktailored for simultaneous semantic segmentation and boundary detection. Ourframework features a two-stream encoder, an active fusion decoder (AFD) and adual-task regularization approach. The encoder is divided into two pathways:one captures category-aware semantic information, while the other discernsboundaries from multi-scale features. The AFD module dynamically adapts thefusion of semantic and boundary information by learning channel-wiserelationships, allowing for precise weight assignment of each channel.Furthermore, we introduce a regularization loss to mitigate the conflicts indual-task learning and deep diversity supervision. Compared to existingmethods, the proposed Mobile-Seed offers a lightweight framework tosimultaneously improve semantic segmentation performance and accurately locateobject boundaries. Experiments on the Cityscapes dataset have shown thatMobile-Seed achieves notable improvement over the state-of-the-art (SOTA)baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, whilemaintaining an online inference speed of 23.9 frames-per-second (FPS) with1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments onCamVid and PASCAL Context datasets confirm our method's generalizability. Codeand additional results are publicly available at\url{https://martin-liao.github.io/Mobile-Seed/}.</description><author>Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong, Bisheng Yang, Xieyuanli Chen</author><pubDate>Tue, 21 Nov 2023 14:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12651v1</guid></item><item><title>Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning</title><link>http://arxiv.org/abs/2310.19805v3</link><description>Offline-to-online RL can make full use of pre-collected offline datasets toinitialize policies, resulting in higher sample efficiency and betterperformance compared to only using online algorithms alone for policy training.However, direct fine-tuning of the pre-trained policy tends to result insub-optimal performance. A primary reason is that conservative offline RLmethods diminish the agent's capability of exploration, thereby impactingonline fine-tuning performance. To encourage agent's exploration during onlinefine-tuning and enhance the overall online fine-tuning performance, we proposea generalized reward augmentation method called Sample Efficient RewardAugmentation (SERA). Specifically, SERA encourages agent to explore bycomputing Q conditioned entropy as intrinsic reward. The advantage of SERA isthat it can extensively utilize offline pre-trained Q to encourage agentuniformly coverage of state space while considering the imbalance between thedistributions of high-value and low-value states. Additionally, SERA can beeffortlessly plugged into various RL algorithms to improve online fine-tuningand ensure sustained asymptotic improvement. Moreover, extensive experimentalresults demonstrate that when conducting offline-to-online problems, SERAconsistently and effectively enhances the performance of various offlinealgorithms.</description><author>Ziqi Zhang, Xiao Xiong, Zifeng Zhuang, Jinxin Liu, Donglin Wang</author><pubDate>Tue, 21 Nov 2023 14:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19805v3</guid></item><item><title>MathGloss: Building mathematical glossaries from text</title><link>http://arxiv.org/abs/2311.12649v1</link><description>MathGloss is a project to create a knowledge graph (KG) for undergraduatemathematics from text, automatically, using modern natural language processing(NLP) tools and resources already available on the web. MathGloss is a linkeddatabase of undergraduate concepts in mathematics. So far, it combines fiveresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graphhosted by the Wikimedia Foundation, (ii) terms covered in mathematics coursesat the University of Chicago, (iii) the syllabus of the French undergraduatemathematics curriculum which includes hyperlinks to the automated theoremprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated bymathematicians, and (v) the nLab, a wiki for category theory also curated bymathematicians. MathGloss's goal is to bring together resources for learningmathematics and to allow every mathematician to tailor their learning to theirown preferences. Moreover, by organizing different resources for learningundergraduate mathematics alongside those for learning formal mathematics, wehope to make it easier for mathematicians and formal tools (theorem provers,computer algebra systems, etc) experts to "understand" each other and breakdown some of the barriers to formal math.</description><author>Lucy Horowitz, Valeria de Paiva</author><pubDate>Tue, 21 Nov 2023 14:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12649v1</guid></item><item><title>Careful Selection and Thoughtful Discarding: Graph Explicit Pooling Utilizing Discarded Nodes</title><link>http://arxiv.org/abs/2311.12644v1</link><description>Graph pooling has been increasingly recognized as crucial for Graph NeuralNetworks (GNNs) to facilitate hierarchical graph representation learning.Existing graph pooling methods commonly consist of two stages: selectingtop-ranked nodes and discarding the remaining to construct coarsened graphrepresentations. However, this paper highlights two key issues with thesemethods: 1) The process of selecting nodes to discard frequently employsadditional Graph Convolutional Networks or Multilayer Perceptrons, lacking athorough evaluation of each node's impact on the final graph representation andsubsequent prediction tasks. 2) Current graph pooling methods tend to directlydiscard the noise segment (dropped) of the graph without accounting for thelatent information contained within these elements. To address the first issue,we introduce a novel Graph Explicit Pooling (GrePool) method, which selectsnodes by explicitly leveraging the relationships between the nodes and finalrepresentation vectors crucial for classification. The second issue isaddressed using an extended version of GrePool (i.e., GrePool+), which appliesa uniform loss on the discarded nodes. This addition is designed to augment thetraining process and improve classification accuracy. Furthermore, we conductcomprehensive experiments across 12 widely used datasets to validate ourproposed method's effectiveness, including the Open Graph Benchmark datasets.Our experimental results uniformly demonstrate that GrePool outperforms 14baseline methods for most datasets. Likewise, implementing GrePool+ enhancesGrePool's performance without incurring additional computational costs.</description><author>Chuang Liu, Wenhang Yu, Kuang Gao, Xueqi Ma, Yibing Zhan, Jia Wu, Bo Du, Wenbin Hu</author><pubDate>Tue, 21 Nov 2023 14:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12644v1</guid></item><item><title>Polyhedral Object Recognition by Indexing</title><link>http://arxiv.org/abs/2311.12641v1</link><description>In computer vision, the indexing problem is the problem of recognizing a fewobjects in a large database of objects while avoiding the help of the classicalimage-feature-to-object-feature matching paradigm. In this paper we address theproblem of recognizing 3-D polyhedral objects from 2-D images by indexing. Boththe objects to be recognized and the images are represented by weighted graphs.The indexing problem is therefore the problem of determining whether a graphextracted from the image is present or absent in a database of model graphs. Weintroduce a novel method for performing this graph indexing process which isbased both on polynomial characterization of binary and weighted graphs and onhashing. We describe in detail this polynomial characterization and then weshow how it can be used in the context of polyhedral object recognition. Nextwe describe a practical recognition-by-indexing system that includes theorganization of the database, the representation of polyhedral objects in termsof 2-D characteristic views, the representation of this views in terms ofweighted graphs, and the associated image processing. Finally, someexperimental results allow the evaluation of the system performance.</description><author>Radu Horaud, Humberto Sossa</author><pubDate>Tue, 21 Nov 2023 14:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12641v1</guid></item><item><title>KNVQA: A Benchmark for evaluation knowledge-based VQA</title><link>http://arxiv.org/abs/2311.12639v1</link><description>Within the multimodal field, large vision-language models (LVLMs) have madesignificant progress due to their strong perception and reasoning capabilitiesin the visual and language systems. However, LVLMs are still plagued by the twocritical issues of object hallucination and factual accuracy, which limit thepracticality of LVLMs in different scenarios. Furthermore, previous evaluationmethods focus more on the comprehension and reasoning of language content butlack a comprehensive evaluation of multimodal interactions, thereby resultingin potential limitations. To this end, we propose a novel KNVQA-Eval, which isdevoted to knowledge-based VQA task evaluation to reflect the factuality ofmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,we develop a new KNVQA dataset by incorporating human judgment and perception,aiming to evaluate the accuracy of standard answers relative to AI-generatedanswers in knowledge-based VQA. This work not only comprehensively evaluatesthe contextual information of LVLMs using reliable human annotations, but alsofurther analyzes the fine-grained capabilities of current methods to revealpotential avenues for subsequent optimization of LVLMs-based estimators. Ourproposed VQA-Eval and corresponding dataset KNVQA will facilitate thedevelopment of automatic evaluation tools with the advantages of low cost,privacy protection, and reproducibility. Our code will be released uponpublication.</description><author>Sirui Cheng, Siyu Zhang, Jiayi Wu, Muchen Lan</author><pubDate>Tue, 21 Nov 2023 14:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12639v1</guid></item><item><title>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</title><link>http://arxiv.org/abs/2311.10122v2</link><description>The Large Vision-Language Model (LVLM) has enhanced the performance ofvarious downstream tasks in visual-language understanding. Most existingapproaches encode images and videos into separate feature spaces, which arethen fed as inputs to large language models. However, due to the lack ofunified tokenization for images and videos, namely misalignment beforeprojection, it becomes challenging for a Large Language Model (LLM) to learnmulti-modal interactions from several poor projection layers. In this work, weunify visual representation into the language feature space to advance thefoundational LLM towards a unified LVLM. As a result, we establish a simple butrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of imagesand videos, mutually enhancing each other. Video-LLaVA achieves superiorperformances on a broad range of 9 image benchmarks across 5 imagequestion-answering datasets and 4 image benchmark toolkits. Additionally, ourVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% onMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensiveexperiments demonstrate that Video-LLaVA mutually benefits images and videoswithin a unified visual representation, outperforming models designedspecifically for images or videos. We aim for this work to provide modestinsights into the multi-modal inputs for the LLM.</description><author>Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan</author><pubDate>Tue, 21 Nov 2023 14:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10122v2</guid></item><item><title>GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning</title><link>http://arxiv.org/abs/2311.12631v1</link><description>Recent advances in text-to-video generation have harnessed the power ofdiffusion models to create visually compelling content conditioned on textprompts. However, they usually encounter high computational costs and oftenstruggle to produce videos with coherent physical motions. To tackle theseissues, we propose GPT4Motion, a training-free framework that leverages theplanning capability of large language models such as GPT, the physicalsimulation strength of Blender, and the excellent image generation ability oftext-to-image diffusion models to enhance the quality of video synthesis.Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on auser textual prompt, which commands Blender's built-in physics engine to craftfundamental scene components that encapsulate coherent physical motions acrossframes. Then these components are inputted into Stable Diffusion to generate avideo aligned with the textual prompt. Experimental results on three basicphysical motion scenarios, including rigid object drop and collision, clothdraping and swinging, and liquid flow, demonstrate that GPT4Motion can generatehigh-quality videos efficiently in maintaining motion coherency and entityconsistency. GPT4Motion offers new insights in text-to-video research,enhancing its quality and broadening its horizon for future explorations.</description><author>Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, Shifeng Chen</author><pubDate>Tue, 21 Nov 2023 14:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12631v1</guid></item><item><title>Hierarchical Joint Graph Learning and Multivariate Time Series Forecasting</title><link>http://arxiv.org/abs/2311.12630v1</link><description>Multivariate time series is prevalent in many scientific and industrialdomains. Modeling multivariate signals is challenging due to their long-rangetemporal dependencies and intricate interactions--both direct and indirect. Toconfront these complexities, we introduce a method of representing multivariatesignals as nodes in a graph with edges indicating interdependency between them.Specifically, we leverage graph neural networks (GNN) and attention mechanismsto efficiently learn the underlying relationships within the time series data.Moreover, we suggest employing hierarchical signal decompositions running overthe graphs to capture multiple spatial dependencies. The effectiveness of ourproposed model is evaluated across various real-world benchmark datasetsdesigned for long-term forecasting tasks. The results consistently showcase thesuperiority of our model, achieving an average 23\% reduction in mean squarederror (MSE) compared to existing models.</description><author>Juhyeon Kim, Hyungeun Lee, Seungwon Yu, Ung Hwang, Wooyul Jung, Miseon Park, Kijung Yoon</author><pubDate>Tue, 21 Nov 2023 14:24:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12630v1</guid></item><item><title>Neural Born Iteration Method For Solving Inverse Scattering Problems: 2D Cases</title><link>http://arxiv.org/abs/2112.09831v2</link><description>In this paper, we propose the neural Born iterative method (NeuralBIM) forsolving 2D inverse scattering problems (ISPs) by drawing on the scheme ofphysics-informed supervised residual learning (PhiSRL) to emulate the computingprocess of the traditional Born iterative method (TBIM). NeuralBIM employsindependent convolutional neural networks (CNNs) to learn the alternate updaterules of two different candidate solutions regarding the residuals. Twodifferent schemes are presented in this paper, including the supervised andunsupervised learning schemes. With the data set generated by the method ofmoments (MoM), supervised NeuralBIM are trained with the knowledge of totalfields and contrasts. Unsupervised NeuralBIM is guided by the physics-embeddedobjective function founding on the governing equations of ISPs, which resultsin no requirement of total fields and contrasts for training. Numerical andexperimental results further validate the efficacy of NeuralBIM.</description><author>Tao Shan, Zhichao Lin, Xiaoqian Song, Maokun Li, Fan Yang, Zhensheng Xu</author><pubDate>Tue, 21 Nov 2023 14:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09831v2</guid></item><item><title>Bridging Algorithmic Information Theory and Machine Learning: A New Approach to Kernel Learning</title><link>http://arxiv.org/abs/2311.12624v1</link><description>Machine Learning (ML) and Algorithmic Information Theory (AIT) look atComplexity from different points of view. We explore the interface between AITand Kernel Methods (that are prevalent in ML) by adopting an AIT perspective onthe problem of learning kernels from data, in kernel ridge regression, throughthe method of Sparse Kernel Flows. In particular, by looking at the differencesand commonalities between Minimal Description Length (MDL) and Regularizationin Machine Learning (RML), we prove that the method of Sparse Kernel Flows isthe natural approach to adopt to learn kernels from data. This paper shows thatit is not necessary to use the statistical route to derive Sparse Kernel Flowsand that one can directly work with code-lengths and complexities that areconcepts that show up in AIT.</description><author>Boumediene Hamzi, Marcus Hutter, Houman Owhadi</author><pubDate>Tue, 21 Nov 2023 14:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12624v1</guid></item><item><title>Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation</title><link>http://arxiv.org/abs/2311.12623v1</link><description>High Content Imaging (HCI) plays a vital role in modern drug discovery anddevelopment pipelines, facilitating various stages from hit identification tocandidate drug characterization. Applying machine learning models to thesedatasets can prove challenging as they typically consist of multiple batches,affected by experimental variation, especially if different imaging equipmenthave been used. Moreover, as new data arrive, it is preferable that they areanalyzed in an online fashion. To overcome this, we propose CODA, an onlineself-supervised domain adaptation approach. CODA divides the classifier's roleinto a generic feature extractor and a task-specific model. We adapt thefeature extractor's weights to the new domain using cross-batchself-supervision while keeping the task-specific model unchanged. Our resultsdemonstrate that this strategy significantly reduces the generalization gap,achieving up to a 300% improvement when applied to data from different labsutilizing different microscopes. CODA can be applied to new, unlabeledout-of-domain data sources of different sizes, from a single plate to multipleexperimental batches.</description><author>Johan Fredin Haslum, Christos Matsoukas, Karl-Johan Leuchowius, Kevin Smith</author><pubDate>Tue, 21 Nov 2023 14:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12623v1</guid></item></channel></rss>