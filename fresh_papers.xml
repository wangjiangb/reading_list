<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 23 May 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Contextualising Implicit Representations for Semantic Tasks</title><link>http://arxiv.org/abs/2305.13312v1</link><description>Prior works have demonstrated that implicit representations trained only forreconstruction tasks typically generate encodings that are not useful forsemantic tasks. In this work, we propose a method that contextualises theencodings of implicit representations, enabling their use in downstream tasks(e.g. semantic segmentation), without requiring access to the original trainingdata or encoding network. Using an implicit representation trained for areconstruction task alone, our contextualising module takes an encoding trainedfor reconstruction only and reveals meaningful semantic information that ishidden in the encodings, without compromising the reconstruction performance.With our proposed module, it becomes possible to pre-train implicitrepresentations on larger datasets, improving their reconstruction performancecompared to training on only a smaller labelled dataset, whilst maintainingtheir segmentation performance on the labelled dataset. Importantly, our methodallows for future foundation implicit representation models to be fine-tuned onunseen tasks, regardless of encoder or dataset availability.</description><author>Theo W. Costain, Kejie Li, Victor A. Prisacariu</author><pubDate>Mon, 22 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13312v1</guid></item><item><title>Change is Hard: A Closer Look at Subpopulation Shift</title><link>http://arxiv.org/abs/2302.12254v2</link><description>Machine learning models often perform poorly on subgroups that areunderrepresented in the training data. Yet, little is understood on thevariation in mechanisms that cause subpopulation shifts, and how algorithmsgeneralize across such diverse shifts at scale. In this work, we provide afine-grained analysis of subpopulation shift. We first propose a unifiedframework that dissects and explains common shifts in subgroups. We thenestablish a comprehensive benchmark of 20 state-of-the-art algorithms evaluatedon 12 real-world datasets in vision, language, and healthcare domains. Withresults obtained from training over 10,000 models, we reveal intriguingobservations for future progress in this space. First, existing algorithms onlyimprove subgroup robustness over certain types of shifts but not others.Moreover, while current algorithms rely on group-annotated validation data formodel selection, we find that a simple selection criterion based on worst-classaccuracy is surprisingly effective even without any group information. Finally,unlike existing works that solely aim to improve worst-group accuracy (WGA), wedemonstrate the fundamental tradeoff between WGA and other important metrics,highlighting the need to carefully choose testing metrics. Code and data areavailable at: https://github.com/YyzHarry/SubpopBench.</description><author>Yuzhe Yang, Haoran Zhang, Dina Katabi, Marzyeh Ghassemi</author><pubDate>Mon, 22 May 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12254v2</guid></item><item><title>VDT: An Empirical Study on Video Diffusion with Transformers</title><link>http://arxiv.org/abs/2305.13311v1</link><description>This work introduces Video Diffusion Transformer (VDT), which pioneers theuse of transformers in diffusion-based video generation. It featurestransformer blocks with modularized temporal and spatial attention modules,allowing separate optimization of each component and leveraging the richspatial-temporal representation inherited from transformers. VDT offers severalappealing benefits. 1) It excels at capturing temporal dependencies to producetemporally consistent video frames and even simulate the dynamics of 3D objectsover time. 2) It enables flexible conditioning information through simpleconcatenation in the token space, effectively unifying video generation andprediction tasks. 3) Its modularized design facilitates a spatial-temporaldecoupled training strategy, leading to improved efficiency. Extensiveexperiments on video generation, prediction, and dynamics modeling (i.e.,physics-based QA) tasks have been conducted to demonstrate the effectiveness ofVDT in various scenarios, including autonomous driving, human action, andphysics-based simulation. We hope our study on the capabilities of transformer-based video diffusion incapturing accurate temporal dependencies, handling conditioning information,and achieving efficient training will benefit future research and advance thefield. Codes and models are available at https://github.com/RERV/VDT.</description><author>Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding</author><pubDate>Mon, 22 May 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13311v1</guid></item><item><title>Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching</title><link>http://arxiv.org/abs/2305.13310v1</link><description>Powered by large-scale pre-training, vision foundation models exhibitsignificant potential in open-world image understanding. Even though individualmodels have limited capabilities, combining multiple such models properly canlead to positive synergies and unleash their full potential. In this work, wepresent Matcher, which segments anything with one shot by integrating anall-purpose feature extraction model and a class-agnostic segmentation model.Naively connecting the models results in unsatisfying performance, e.g., themodels tend to generate matching outliers and false-positive mask fragments. Toaddress these issues, we design a bidirectional matching strategy for accuratecross-image semantic dense matching and a robust prompt sampler for maskproposal generation. In addition, we propose a novel instance-level matchingstrategy for controllable mask merging. The proposed Matcher method deliversimpressive generalization performance across various segmentation tasks, allwithout training. For example, it achieves 52.7% mIoU on COCO-20$^i$ forone-shot semantic segmentation, surpassing the state-of-the-art specialistmodel by 1.6%. In addition, our visualization results show open-worldgenerality and flexibility on images in the wild. The code shall be released athttps://github.com/aim-uofa/Matcher.</description><author>Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen</author><pubDate>Mon, 22 May 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13310v1</guid></item><item><title>Evaluating Factual Consistency of Texts with Semantic Role Labeling</title><link>http://arxiv.org/abs/2305.13309v1</link><description>Automated evaluation of text generation systems has recently seen increasingattention, particularly checking whether generated text stays truthful to inputsources. Existing methods frequently rely on an evaluation using task-specificlanguage models, which in turn allows for little interpretability of generatedscores. We introduce SRLScore, a reference-free evaluation metric designed withtext summarization in mind. Our approach generates fact tuples constructed fromSemantic Role Labels, applied to both input and summary texts. A finalfactuality score is computed by an adjustable scoring mechanism, which allowsfor easy adaption of the method across domains. Correlation with humanjudgments on English summarization datasets shows that SRLScore is competitivewith state-of-the-art methods and exhibits stable generalization acrossdatasets without requiring further training or hyperparameter tuning. Weexperiment with an optional co-reference resolution step, but find that theperformance boost is mostly outweighed by the additional compute required. Ourmetric is available online at https://github.com/heyjing/SRLScore.</description><author>Jing Fan, Dennis Aumiller, Michael Gertz</author><pubDate>Mon, 22 May 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13309v1</guid></item><item><title>If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection</title><link>http://arxiv.org/abs/2305.13308v1</link><description>Despite their impressive capabilities, diffusion-based text-to-image (T2I)models can lack faithfulness to the text prompt, where generated images may notcontain all the mentioned objects, attributes or relations. To alleviate theseissues, recent works proposed post-hoc methods to improve model faithfulnesswithout costly retraining, by modifying how the model utilizes the inputprompt. In this work, we take a step back and show that large T2I diffusionmodels are more faithful than usually assumed, and can generate images faithfulto even complex prompts without the need to manipulate the generative process.Based on that, we show how faithfulness can be simply treated as a candidateselection problem instead, and introduce a straightforward pipeline thatgenerates candidate images for a text prompt and picks the best one accordingto an automatic scoring system that can leverage already existing T2Ievaluation metrics. Quantitative comparisons alongside user studies on diversebenchmarks show consistently improved faithfulness over post-hoc enhancementmethods, with comparable or lower computational cost. Code is available at\url{https://github.com/ExplainableML/ImageSelect}.</description><author>Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata</author><pubDate>Mon, 22 May 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13308v1</guid></item><item><title>UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers</title><link>http://arxiv.org/abs/2303.00807v2</link><description>Many information retrieval tasks require large labeled datasets forfine-tuning. However, such datasets are often unavailable, and their utilityfor real-world applications can diminish quickly due to domain shifts. Toaddress this challenge, we develop and motivate a method for using largelanguage models (LLMs) to generate large numbers of synthetic queries cheaply.The method begins by generating a small number of synthetic queries using anexpensive LLM. After that, a much less expensive one is used to create largenumbers of synthetic queries, which are used to fine-tune a family of rerankermodels. These rerankers are then distilled into a single efficient retrieverfor use in the target domain. We show that this technique boosts zero-shotaccuracy in long-tail domains, even where only 2K synthetic queries are usedfor fine-tuning, and that it achieves substantially lower latency than standardreranking methods. We make our end-to-end approach, including our syntheticdatasets and replication code, publicly available on Github:https://github.com/primeqa/primeqa.</description><author>Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, Christopher Potts</author><pubDate>Mon, 22 May 2023 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00807v2</guid></item><item><title>NeRFuser: Large-Scale Scene Representation by NeRF Fusion</title><link>http://arxiv.org/abs/2305.13307v1</link><description>A practical benefit of implicit visual representations like Neural RadianceFields (NeRFs) is their memory efficiency: large scenes can be efficientlystored and shared as small neural nets instead of collections of images.However, operating on these implicit visual data structures requires extendingclassical image-based vision techniques (e.g., registration, blending) fromimage sets to neural fields. Towards this goal, we propose NeRFuser, a novelarchitecture for NeRF registration and blending that assumes only access topre-generated NeRFs, and not the potentially large sets of images used togenerate them. We propose registration from re-rendering, a technique to inferthe transformation between NeRFs based on images synthesized from individualNeRFs. For blending, we propose sample-based inverse distance weighting toblend visual information at the ray-sample level. We evaluate NeRFuser onpublic benchmarks and a self-collected object-centric indoor dataset, showingthe robustness of our method, including to views that are challenging to renderfrom the individual source NeRFs.</description><author>Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, Matthew R. Walter</author><pubDate>Mon, 22 May 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13307v1</guid></item><item><title>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text</title><link>http://arxiv.org/abs/2305.13304v1</link><description>The fixed-size context of Transformer makes GPT models incapable ofgenerating arbitrarily long text. In this paper, we introduce RecurrentGPT, alanguage-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT isbuilt upon a large language model (LLM) such as ChatGPT and uses naturallanguage to simulate the Long Short-Term Memory mechanism in an LSTM. At eachtimestep, RecurrentGPT generates a paragraph of text and updates itslanguage-based long-short term memory stored on the hard drive and the prompt,respectively. This recurrence mechanism enables RecurrentGPT to generate textsof arbitrary length without forgetting. Since human users can easily observeand edit the natural language memories, RecurrentGPT is interpretable andenables interactive generation of long text. RecurrentGPT is an initial steptowards next-generation computer-assisted writing systems beyond local editingsuggestions. In addition to producing AI-generated content (AIGC), we alsodemonstrate the possibility of using RecurrentGPT as an interactive fictionthat directly interacts with consumers. We call this usage of generative modelsby ``AI As Contents'' (AIAC), which we believe is the next form of conventionalAIGC. We further demonstrate the possibility of using RecurrentGPT to createpersonalized interactive fiction that directly interacts with readers insteadof interacting with writers. More broadly, RecurrentGPT demonstrates theutility of borrowing ideas from popular model designs in cognitive science anddeep learning for prompting LLMs. Our code is available athttps://github.com/aiwaves-cn/RecurrentGPT and an online demo is available athttps://www.aiwaves.org/recurrentgpt.</description><author>Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, Mrinmaya Sachan</author><pubDate>Mon, 22 May 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13304v1</guid></item><item><title>Towards Unsupervised Recognition of Semantic Differences in Related Documents</title><link>http://arxiv.org/abs/2305.13303v1</link><description>Automatically highlighting words that cause semantic differences between twodocuments could be useful for a wide range of applications. We formulaterecognizing semantic differences (RSD) as a token-level regression task andstudy three unsupervised approaches that rely on a masked language model. Toassess the approaches, we begin with basic English sentences and gradually moveto more complex, cross-lingual document pairs. Our results show that anapproach based on word alignment and sentence-level contrastive learning has arobust correlation to gold labels. However, all unsupervised approaches stillleave a large margin of improvement. Code to reproduce our experiments isavailable at https://github.com/ZurichNLP/recognizing-semantic-differences</description><author>Jannis Vamvas, Rico Sennrich</author><pubDate>Mon, 22 May 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13303v1</guid></item><item><title>Language-Agnostic Bias Detection in Language Models</title><link>http://arxiv.org/abs/2305.13302v1</link><description>Pretrained language models (PLMs) are key components in NLP, but they containstrong social biases. Quantifying these biases is challenging because currentmethods focusing on fill-the-mask objectives are sensitive to slight changes ininput. To address this, we propose LABDet, a robust language-agnostic methodfor evaluating bias in PLMs. For nationality as a case study, we show thatLABDet "surfaces" nationality bias by training a classifier on top of a frozenPLM on non-nationality sentiment detection. Collaborating with politicalscientists, we find consistent patterns of nationality bias across monolingualPLMs in six languages that align with historical and political context. We alsoshow for English BERT that bias surfaced by LABDet correlates well with bias inthe pretraining data; thus, our work is one of the few studies that directlylinks pretraining data to PLM behavior. Finally, we verify LABDet's reliabilityand applicability to different templates and languages through an extensive setof robustness checks.</description><author>Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze</author><pubDate>Mon, 22 May 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13302v1</guid></item><item><title>Training Diffusion Models with Reinforcement Learning</title><link>http://arxiv.org/abs/2305.13301v1</link><description>Diffusion models are a class of flexible generative models trained with anapproximation to the log-likelihood objective. However, most use cases ofdiffusion models are not concerned with likelihoods, but instead withdownstream objectives such as human-perceived image quality or drugeffectiveness. In this paper, we investigate reinforcement learning methods fordirectly optimizing diffusion models for such objectives. We describe howposing denoising as a multi-step decision-making problem enables a class ofpolicy gradient algorithms, which we refer to as denoising diffusion policyoptimization (DDPO), that are more effective than alternative reward-weightedlikelihood approaches. Empirically, DDPO is able to adapt text-to-imagediffusion models to objectives that are difficult to express via prompting,such as image compressibility, and those derived from human feedback, such asaesthetic quality. Finally, we show that DDPO can improve prompt-imagealignment using feedback from a vision-language model without the need foradditional data collection or human annotation.</description><author>Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine</author><pubDate>Mon, 22 May 2023 18:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13301v1</guid></item><item><title>Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Conflicts</title><link>http://arxiv.org/abs/2305.13300v1</link><description>By providing external information to large language models (LLMs), toolaugmentation (including retrieval augmentation) has emerged as a promisingsolution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when theevidence conflicts with their parametric memory? We present the firstcomprehensive and controlled investigation into the behavior of LLMs whenencountering knowledge conflicts. We propose a systematic framework to elicithigh-quality parametric memory from LLMs and construct the correspondingcounter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs. On the onehand, different from prior wisdom, we find that LLMs can be highly receptive toexternal evidence even when that conflicts with their parametric memory, giventhat the external evidence is coherent and convincing. On the other hand, LLMsalso demonstrate a strong confirmation bias when the external evidence containssome information that is consistent with their parametric memory, despite beingpresented with conflicting evidence at the same time. These results poseimportant implications that are worth careful consideration for the furtherdevelopment and deployment of tool- and retrieval-augmented LLMs.</description><author>Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su</author><pubDate>Mon, 22 May 2023 18:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13300v1</guid></item><item><title>Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations</title><link>http://arxiv.org/abs/2305.13299v1</link><description>In-context learning (ICL) is an important paradigm for adapting largelanguage models (LLMs) to new tasks, but the generalization behavior of ICLremains poorly understood. We investigate the inductive biases of ICL from theperspective of feature bias: which feature ICL is more likely to use given aset of underspecified demonstrations in which two features are equallypredictive of the labels. First, we characterize the feature biases of GPT-3models by constructing underspecified demonstrations from a range of NLPdatasets and feature combinations. We find that LLMs exhibit clear featurebiases - for example, demonstrating a strong bias to predict labels accordingto sentiment rather than shallow lexical features, like punctuation. Second, weevaluate the effect of different interventions that are designed to impose aninductive bias in favor of a particular feature, such as adding a naturallanguage instruction or using semantically relevant label words. We find that,while many interventions can influence the learner to prefer a particularfeature, it can be difficult to overcome strong prior biases. Overall, ourresults provide a broader picture of the types of features that ICL may be morelikely to exploit and how to impose inductive biases that are better alignedwith the intended task.</description><author>Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, He He</author><pubDate>Mon, 22 May 2023 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13299v1</guid></item><item><title>DiffusionNER: Boundary Diffusion for Named Entity Recognition</title><link>http://arxiv.org/abs/2305.13298v1</link><description>In this paper, we propose DiffusionNER, which formulates the named entityrecognition task as a boundary-denoising diffusion process and thus generatesnamed entities from noisy spans. During training, DiffusionNER gradually addsnoises to the golden entity boundaries by a fixed forward diffusion process andlearns a reverse diffusion process to recover the entity boundaries. Ininference, DiffusionNER first randomly samples some noisy spans from a standardGaussian distribution and then generates the named entities by denoising themwith the learned reverse diffusion process. The proposed boundary-denoisingdiffusion process allows progressive refinement and dynamic sampling ofentities, empowering DiffusionNER with efficient and flexible entity generationcapability. Experiments on multiple flat and nested NER datasets demonstratethat DiffusionNER achieves comparable or even better performance than previousstate-of-the-art models.</description><author>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang</author><pubDate>Mon, 22 May 2023 18:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13298v1</guid></item><item><title>Parallel Attention and Feed-Forward Net Design for Pre-training and Inference on Transformers</title><link>http://arxiv.org/abs/2305.13297v1</link><description>In this paper, we introduce Parallel Attention and Feed-Forward Net Design(PAF) for transformer models. Transformer models are indisputably the backboneof all Natural Language Processing applications. Therefore, any efforts aimedat improving their efficiency are guaranteed to have an enormous impact.Transformer models consist of many layers and each layer has an attention blockfollowed by a feed-forward network (FFN) that processes the input based on theattention block's output. We refer to this standard design as Series Attentionand Feed-Forward Net Design (SAF). For each layer in our proposed PAF designfor transformer models, we make FFN block's computations independent of theoutput of the attention block. This decoupling allows FFN block of each layerto run in parallel to the attention block of that layer. We evaluate PAF designby training two large language models (RoBERTa-large and bert-large-uncased)and comparing them to their SAF counterparts on six tasks of the GeneralLanguage Understanding (GLUE) benchmark which test a multitude of semanticattributes. PAF models achieves nearly identical performance as their SAFcounterparts on all the six tasks. We also compare time complexities ofattention blocks with FFN blocks and find that running both blocks in parallelcan theoretically and in practice achieve upto 1.5x to 2x gains in speed. Weleave the development of fast and efficient libraries for implementation of PAFdesign for future work.</description><author>Shashank Sonkar, Richard G. Baraniuk</author><pubDate>Mon, 22 May 2023 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13297v1</guid></item><item><title>MEAL: Stable and Active Learning for Few-Shot Prompting</title><link>http://arxiv.org/abs/2211.08358v2</link><description>Few-shot classification has made great strides due to foundation models that,through priming and prompting, are highly effective few-shot learners. However,this approach has high variance both across different sets of few shots (dataselection) and across different finetuning runs (run variability). This isproblematic not only because it impedes the fair comparison of differentapproaches, but especially because it makes few-shot learning too unreliablefor many real-world applications. To alleviate these issues, we make twocontributions for more stable and effective few-shot learning: First, wepropose novel ensembling methods and show that they substantially reduce runvariability. Second, we introduce a new active learning (AL) criterion for dataselection and present the first AL-based approach specifically tailored towardsprompt-based learning. In our experiments, we show that our combined method,MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),improves overall performance of prompt-based finetuning by 2.3 points on fivediverse tasks.</description><author>Abdullatif Köksal, Timo Schick, Hinrich Schütze</author><pubDate>Mon, 22 May 2023 18:51:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08358v2</guid></item><item><title>Time Fairness in Online Knapsack Problems</title><link>http://arxiv.org/abs/2305.13293v1</link><description>The online knapsack problem is a classic problem in the field of onlinealgorithms. Its canonical version asks how to pack items of different valuesand weights arriving online into a capacity-limited knapsack so as to maximizethe total value of the admitted items. Although optimal competitive algorithmsare known for this problem, they may be fundamentally unfair, i.e., individualitems may be treated inequitably in different ways. Inspired by recentattention to fairness in online settings, we develop a natural andpractically-relevant notion of time fairness for the online knapsack problem,and show that the existing optimal algorithms perform poorly under this metric.We propose a parameterized deterministic algorithm where the parameterprecisely captures the Pareto-optimal trade-off between fairness andcompetitiveness. We show that randomization is theoretically powerful enough tobe simultaneously competitive and fair; however, it does not work well inpractice, using trace-driven experiments. To further improve the trade-offbetween fairness and competitiveness, we develop a fair, robust (competitive),and consistent learning-augmented algorithm with substantial performanceimprovement in trace-driven experiments.</description><author>Adam Lechowicz, Rik Sengupta, Bo Sun, Shahin Kamali, Mohammad Hajiesmaili</author><pubDate>Mon, 22 May 2023 18:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13293v1</guid></item><item><title>VideoLLM: Modeling Video Sequence with Large Language Models</title><link>http://arxiv.org/abs/2305.13292v1</link><description>With the exponential growth of video data, there is an urgent need forautomated technology to analyze and comprehend video content. However, existingvideo understanding models are often task-specific and lack a comprehensivecapability of handling diverse tasks. The success of large language models(LLMs) like GPT has demonstrated their impressive abilities in sequence causalreasoning. Building upon this insight, we propose a novel framework calledVideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMsfrom natural language processing (NLP) for video sequence understanding.VideoLLM incorporates a carefully designed Modality Encoder and SemanticTranslator, which convert inputs from various modalities into a unified tokensequence. This token sequence is then fed into a decoder-only LLM.Subsequently, with the aid of a simple task head, our VideoLLM yields aneffective unified framework for different kinds of video understanding tasks.To evaluate the efficacy of VideoLLM, we conduct extensive experiments usingmultiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight taskssourced from four different datasets. The experimental results demonstrate thatthe understanding and reasoning capabilities of LLMs can be effectivelytransferred to video understanding tasks.</description><author>Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, Limin Wang</author><pubDate>Mon, 22 May 2023 18:51:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13292v1</guid></item><item><title>Can Peanuts Fall in Love with Distributional Semantics?</title><link>http://arxiv.org/abs/2301.08731v2</link><description>Context changes expectations about upcoming words - following a storyinvolving an anthropomorphic peanut, comprehenders expect the sentence thepeanut was in love more than the peanut was salted, as indexed by N400amplitude (Nieuwland &amp; van Berkum, 2006). This updating of expectations hasbeen explained using Situation Models - mental representations of a describedevent. However, recent work showing that N400 amplitude is predictable fromdistributional information alone raises the question whether situation modelsare necessary for these contextual effects. We model the results of Nieuwlandand van Berkum (2006) using six computational language models and three sets ofword vectors, none of which have explicit situation models or semanticgrounding. We find that a subset of these can fully model the effect found byNieuwland and van Berkum (2006). Thus, at least some processing effectsnormally explained through situation models may not in fact require explicitsituation models.</description><author>James A. Michaelov, Seana Coulson, Benjamin K. Bergen</author><pubDate>Mon, 22 May 2023 18:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08731v2</guid></item><item><title>Materialistic: Selecting Similar Materials in Images</title><link>http://arxiv.org/abs/2305.13291v1</link><description>Separating an image into meaningful underlying components is a crucial firststep for both editing and understanding images. We present a method capable ofselecting the regions of a photograph exhibiting the same material as anartist-chosen area. Our proposed approach is robust to shading, specularhighlights, and cast shadows, enabling selection in real images. As we do notrely on semantic segmentation (different woods or metal should not be selectedtogether), we formulate the problem as a similarity-based grouping problembased on a user-provided image location. In particular, we propose to leveragethe unsupervised DINO features coupled with a proposed Cross-Similarity moduleand an MLP head to extract material similarities in an image. We train ourmodel on a new synthetic image dataset, that we release. We show that ourmethod generalizes well to real-world images. We carefully analyze our model'sbehavior on varying material properties and lighting. Additionally, we evaluateit against a hand-annotated benchmark of 50 real photographs. We furtherdemonstrate our model on a set of applications, including material editing,in-video selection, and retrieval of object photographs with similar materials.</description><author>Prafull Sharma, Julien Philip, Michaël Gharbi, William T. Freeman, Fredo Durand, Valentin Deschaintre</author><pubDate>Mon, 22 May 2023 18:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13291v1</guid></item><item><title>Uncertainty and Structure in Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2305.13290v1</link><description>Neural ordinary differential equations (ODEs) are an emerging class of deeplearning models for dynamical systems. They are particularly useful forlearning an ODE vector field from observed trajectories (i.e., inverseproblems). We here consider aspects of these models relevant for theirapplication in science and engineering. Scientific predictions generallyrequire structured uncertainty estimates. As a first contribution, we show thatbasic and lightweight Bayesian deep learning techniques like the Laplaceapproximation can be applied to neural ODEs to yield structured and meaningfuluncertainty quantification. But, in the scientific domain, availableinformation often goes beyond raw trajectories, and also includes mechanisticknowledge, e.g., in the form of conservation laws. We explore how mechanisticknowledge and uncertainty quantification interact on two recently proposedneural ODE frameworks - symplectic neural ODEs and physical models augmentedwith neural ODEs. In particular, uncertainty reflects the effect of mechanisticinformation more directly than the predictive power of the trained model could.And vice versa, structure can improve the extrapolation abilities of neuralODEs, a fact that can be best assessed in practice through uncertaintyestimates. Our experimental analysis demonstrates the effectiveness of theLaplace approach on both low dimensional ODE problems and a high dimensionalpartial differential equation.</description><author>Katharina Ott, Michael Tiemann, Philipp Hennig</author><pubDate>Mon, 22 May 2023 18:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13290v1</guid></item><item><title>Distributionally Robust Optimization Efficiently Solves Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2305.13289v1</link><description>Offline reinforcement learning aims to find the optimal policy from apre-collected dataset without active exploration. This problem is faced withmajor challenges, such as a limited amount of data and distribution shift.Existing studies employ the principle of pessimism in face of uncertainty, andpenalize rewards for less visited state-action pairs. In this paper, wedirectly model the uncertainty in the transition kernel using an uncertaintyset, and then employ the approach of distributionally robust optimization thatoptimizes the worst-case performance over the uncertainty set. We first designa Hoeffding-style uncertainty set, which guarantees that the true transitionkernel lies in the uncertainty set with high probability. We theoreticallyprove that it achieves an $\epsilon$-accuracy with a sample complexity of$\mathcal{O}\left((1-\gamma)^{-4}\epsilon^{-2}SC^{\pi^*} \right)$, where$\gamma$ is the discount factor, $C^{\pi^*}$ is the single-policyconcentrability for any comparator policy $\pi^*$, and $S$ is the number ofstates. We further design a Bernstein-style uncertainty set, which does notnecessarily guarantee the true transition kernel lies in the uncertainty set.We show an improved and near-optimal sample complexity of$\mathcal{O}\left((1-\gamma)^{-3}\epsilon^{-2}\left(SC^{\pi^*}+(\mu_{\min})^{-1}\right)\right)$, where $\mu_{\min}$ denotes the minimal non-zero entry of the behaviordistribution. In addition, the computational complexity of our algorithms isthe same as one of the LCB-based methods in the literature. Our resultsdemonstrate that distributionally robust optimization method can alsoefficiently solve offline reinforcement learning.</description><author>Yue Wang, Yuting Hu, Jinjun Xiong, Shaofeng Zou</author><pubDate>Mon, 22 May 2023 18:50:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13289v1</guid></item><item><title>How do languages influence each other? Studying cross-lingual data sharing during LLM fine-tuning</title><link>http://arxiv.org/abs/2305.13286v1</link><description>Multilingual large language models (MLLMs) are jointly trained on data frommany different languages such that representation of individual languages canbenefit from other languages' data. Impressive performance on zero-shotcross-lingual transfer shows that these models are capable of exploiting datafrom other languages. Yet, it remains unclear to what extent, and under whichconditions, languages rely on each other's data. In this study, we use TracIn(Pruthi et al., 2020), a training data attribution (TDA) method, to retrievethe most influential training samples seen during multilingual fine-tuning fora particular test language. This allows us to analyse cross-lingual sharingmechanisms of MLLMs from a new perspective. While previous work studiedcross-lingual sharing at the level of model parameters, we present the firstapproach to study cross-lingual sharing at the data level. We find that MLLMsrely on data from multiple languages from the early stages of fine-tuning andthat this reliance gradually increases as fine-tuning progresses. We furtherstudy how different fine-tuning languages influence model performance on agiven test language and find that they can both reinforce and complement theknowledge acquired from data of the test language itself.</description><author>Rochelle Choenni, Dan Garrette, Ekaterina Shutova</author><pubDate>Mon, 22 May 2023 18:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13286v1</guid></item><item><title>Personalized Subgraph Federated Learning</title><link>http://arxiv.org/abs/2206.10206v3</link><description>Subgraphs of a larger global graph may be distributed across multipledevices, and only locally accessible due to privacy restrictions, althoughthere may be links between subgraphs. Recently proposed subgraph FederatedLearning (FL) methods deal with those missing links across local subgraphswhile distributively training Graph Neural Networks (GNNs) on them. However,they have overlooked the inevitable heterogeneity between subgraphs comprisingdifferent communities of a global graph, consequently collapsing theincompatible knowledge from local GNN models. To this end, we introduce a newsubgraph FL problem, personalized subgraph FL, which focuses on the jointimprovement of the interrelated local GNNs rather than learning a single globalmodel, and propose a novel framework, FEDerated Personalized sUBgraph learning(FED-PUB), to tackle it. Since the server cannot access the subgraph in eachclient, FED-PUB utilizes functional embeddings of the local GNNs using randomgraphs as inputs to compute similarities between them, and use the similaritiesto perform weighted averaging for server-side aggregation. Further, it learns apersonalized sparse mask at each client to select and update only thesubgraph-relevant subset of the aggregated parameters. We validate our FED-PUBfor its subgraph FL performance on six datasets, considering bothnon-overlapping and overlapping subgraphs, on which it significantlyoutperforms relevant baselines. Our code is available athttps://github.com/JinheonBaek/FED-PUB.</description><author>Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, Sung Ju Hwang</author><pubDate>Mon, 22 May 2023 18:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10206v3</guid></item><item><title>Target-Aware Generative Augmentations for Single-Shot Adaptation</title><link>http://arxiv.org/abs/2305.13284v1</link><description>In this paper, we address the problem of adapting models from a source domainto a target domain, a task that has become increasingly important due to thebrittle generalization of deep neural networks. While several test-timeadaptation techniques have emerged, they typically rely on synthetic toolboxdata augmentations in cases of limited target data availability. We considerthe challenging setting of single-shot adaptation and explore the design ofaugmentation strategies. We argue that augmentations utilized by existingmethods are insufficient to handle large distribution shifts, and hence proposea new approach SiSTA, which first fine-tunes a generative model from the sourcedomain using a single-shot target, and then employs novel sampling strategiesfor curating synthetic target data. Using experiments on a variety ofbenchmarks, distribution shifts and image corruptions, we find that SiSTAproduces significantly improved generalization over existing baselines in faceattribute detection and multi-class object recognition. Furthermore, SiSTAperforms competitively to models obtained by training on larger targetdatasets. Our codes can be accessed at https://github.com/Rakshith-2905/SiSTA.</description><author>Kowshik Thopalli, Rakshith Subramanyam, Pavan Turaga, Jayaraman J. Thiagarajan</author><pubDate>Mon, 22 May 2023 18:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13284v1</guid></item><item><title>Approximating a RUM from Distributions on k-Slates</title><link>http://arxiv.org/abs/2305.13283v1</link><description>In this work we consider the problem of fitting Random Utility Models (RUMs)to user choices. Given the winner distributions of the subsets of size $k$ of auniverse, we obtain a polynomial-time algorithm that finds the RUM that bestapproximates the given distribution on average. Our algorithm is based on alinear program that we solve using the ellipsoid method. Given that itscorresponding separation oracle problem is NP-hard, we devise an approximateseparation oracle that can be viewed as a generalization of the weightedfeedback arc set problem to hypergraphs. Our theoretical result can also bemade practical: we obtain a heuristic that is effective and scales toreal-world datasets.</description><author>Flavio Chierichetti, Mirko Giacchini, Ravi Kumar, Alessandro Panconesi, Andrew Tomkins</author><pubDate>Mon, 22 May 2023 18:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13283v1</guid></item><item><title>Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection</title><link>http://arxiv.org/abs/2305.13282v1</link><description>Out-of-distribution (OOD) detection is a critical task for reliablepredictions over text. Fine-tuning with pre-trained language models has been ade facto procedure to derive OOD detectors with respect to in-distribution (ID)data. Despite its common use, the understanding of the role of fine-tuning andits necessity for OOD detection is largely unexplored. In this paper, we raisethe question: is fine-tuning necessary for OOD detection? We present a studyinvestigating the efficacy of directly leveraging pre-trained language modelsfor OOD detection, without any model fine-tuning on the ID data. We compare theapproach with several competitive fine-tuning objectives, and offer newinsights under various types of distributional shifts. Extensive evaluations on8 diverse ID-OOD dataset pairs demonstrate near-perfect OOD detectionperformance (with 0% FPR95 in many cases), strongly outperforming itsfine-tuned counterparts. We show that using distance-based detection methods,pre-trained language models are near-perfect OOD detectors when thedistribution shift involves a domain change. Furthermore, we study the effectof fine-tuning on OOD detection and identify how to balance ID accuracy withOOD detection performance. Our code is publically available athttps://github.com/Uppaal/lm-ood.</description><author>Rheeya Uppaal, Junjie Hu, Yixuan Li</author><pubDate>Mon, 22 May 2023 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13282v1</guid></item><item><title>LM vs LM: Detecting Factual Errors via Cross Examination</title><link>http://arxiv.org/abs/2305.13281v1</link><description>A prominent weakness of modern language models (LMs) is their tendency togenerate factually incorrect text, which hinders their usability. A naturalquestion is whether such factual errors can be detected automatically. Inspiredby truth-seeking mechanisms in law, we propose a factuality evaluationframework for LMs that is based on cross-examination. Our key idea is that anincorrect claim is likely to result in inconsistency with other claims that themodel generates. To discover such inconsistencies, we facilitate a multi-turninteraction between the LM that generated the claim and another LM (acting asan examiner) which introduces questions to discover inconsistencies. Weempirically evaluate our method on factual claims made by multiple recent LMson four benchmarks, finding that it outperforms existing methods and baselines,often by a large gap. Our results demonstrate the potential of usinginteracting LMs for capturing factual errors.</description><author>Roi Cohen, May Hamri, Mor Geva, Amir Globerson</author><pubDate>Mon, 22 May 2023 18:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13281v1</guid></item><item><title>User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks</title><link>http://arxiv.org/abs/2302.01471v2</link><description>The Metaverse is emerging as maturing technologies are empowering thedifferent facets. Virtual Reality (VR) technologies serve as the backbone ofthe virtual universe within the Metaverse to offer a highly immersive userexperience. As mobility is emphasized in the Metaverse context, VR devicesreduce their weights at the sacrifice of local computation abilities. In thispaper, for a system consisting of a Metaverse server and multiple VR users, weconsider two cases of (i) the server generating frames and transmitting them tousers, and (ii) users generating frames locally and thus consuming deviceenergy. Moreover, in our multi-user VR scenario for the Metaverse, users havedifferent characteristics and demands for Frames Per Second (FPS). Then thechannel access arrangement (including the decisions on frame generationlocation), and transmission powers for the downlink communications from theserver to the users are jointly optimized to improve the utilities of users.This joint optimization is addressed by deep reinforcement learning (DRL) withheterogeneous actions. Our proposed user-centric DRL algorithm is calledUser-centric Critic with Heterogenous Actors (UCHA). Extensive experimentsdemonstrate that our UCHA algorithm leads to remarkable results under variousrequirements and constraints.</description><author>Wenhan Yu, Terence Jie Chua, Jun Zhao</author><pubDate>Mon, 22 May 2023 18:42:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01471v2</guid></item><item><title>On Text-based Personality Computing: Challenges and Future Directions</title><link>http://arxiv.org/abs/2212.06711v4</link><description>Text-based personality computing (TPC) has gained many research interests inNLP. In this paper, we describe 15 challenges that we consider deserving theattention of the research community. These challenges are organized by thefollowing topics: personality taxonomies, measurement quality, datasets,performance evaluation, modelling choices, as well as ethics and fairness. Whenaddressing each challenge, not only do we combine perspectives from both NLPand social sciences, but also offer concrete suggestions. We hope to inspiremore valid and reliable TPC research.</description><author>Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri, Laura Boeschoten, Erik-Jan van Kesteren, Mahdi Shafiee Kamalabad, Daniel L Oberski</author><pubDate>Mon, 22 May 2023 18:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06711v4</guid></item><item><title>U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series</title><link>http://arxiv.org/abs/2305.13277v1</link><description>Satellite image time series in the optical and infrared spectrum suffer fromfrequent data gaps due to cloud cover, cloud shadows, and temporary sensoroutages. It has been a long-standing problem of remote sensing research how tobest reconstruct the missing pixel values and obtain complete, cloud-free imagesequences. We approach that problem from the perspective of representationlearning and develop U-TILISE, an efficient neural model that is able toimplicitly capture spatio-temporal patterns of the spectral intensities, andthat can therefore be trained to map a cloud-masked input sequence to acloud-free output sequence. The model consists of a convolutional spatialencoder that maps each individual frame of the input sequence to a latentencoding; an attention-based temporal encoder that captures dependenciesbetween those per-frame encodings and lets them exchange information along thetime dimension; and a convolutional spatial decoder that decodes the latentembeddings back into multi-spectral images. We experimentally evaluate theproposed model on EarthNet2021, a dataset of Sentinel-2 time series acquiredall over Europe, and demonstrate its superior ability to reconstruct themissing pixels. Compared to a standard interpolation baseline, it increases thePSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.</description><author>Corinne Stucker, Vivien Sainte Fare Garnot, Konrad Schindler</author><pubDate>Mon, 22 May 2023 18:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13277v1</guid></item><item><title>Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection</title><link>http://arxiv.org/abs/2305.13276v1</link><description>Hate speech is a severe issue that affects many online platforms. So far,several studies have been performed to develop robust hate speech detectionsystems. Large language models like ChatGPT have recently shown great potentialin performing several tasks, including hate speech detection. However, it iscrucial to comprehend the limitations of these models to build more robust hatespeech detection systems. Thus to bridge the gap, our study aims to evaluatethe weaknesses of the ChatGPT model in detecting hate speech at a granularlevel across 11 languages. In addition, we investigate the influence of complexemotions, such as the use of emojis in hate speech, on the performance of theChatGPT model. Through our analysis, we examine and investigate the errors madeby the model, shedding light on its shortcomings in detecting certain types ofhate speech and highlighting the need for further research and improvements inhate speech detection.</description><author>Mithun Das, Saurabh Kumar Pandey, Animesh Mukherjee</author><pubDate>Mon, 22 May 2023 18:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13276v1</guid></item><item><title>A Machine Learning Approach to Detect Dehydration in Afghan Children</title><link>http://arxiv.org/abs/2305.13275v1</link><description>Child dehydration is a significant health concern, especially among childrenunder 5 years of age who are more susceptible to diarrhea and vomiting. InAfghanistan, severe diarrhea contributes to child mortality due to dehydration.However, there is no evidence of research exploring the potential of machinelearning techniques in diagnosing dehydration in Afghan children under five. Tofill this gap, this study leveraged various classifiers such as Random Forest,Multilayer Perceptron, Support Vector Machine, J48, and Logistic Regression todevelop a predictive model using a dataset of sick children retrieved from theAfghanistan Demographic and Health Survey (ADHS). The primary objective was todetermine the dehydration status of children under 5 years. Among all theclassifiers, Random Forest proved to be the most effective, achieving anaccuracy of 91.46%, precision of 91%, and AUC of 94%. This model canpotentially assist healthcare professionals in promptly and accuratelyidentifying dehydration in under five children, leading to timelyinterventions, and reducing the risk of severe health complications. Our studydemonstrates the potential of machine learning techniques in improving theearly diagnosis of dehydration in Afghan children.</description><author>Ziaullah Momand, Debajyoti Pal, Pornchai Mongkolnam, Jonathan H. Chan</author><pubDate>Mon, 22 May 2023 18:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13275v1</guid></item><item><title>CLASS Meet SPOCK: An Education Tutoring Chatbot based on Learning Science Principles</title><link>http://arxiv.org/abs/2305.13272v1</link><description>We present a design framework called Conversational Learning with AnalyticalStep-by-Step Strategies (CLASS) for developing high-performance IntelligentTutoring Systems (ITS). The CLASS framework aims to empower ITS with with twocritical capabilities: imparting tutor-like step-by-step guidance and enablingtutor-like conversations in natural language to effectively engage learners. Toempower ITS with the aforementioned capabilities, the CLASS framework employstwo carefully curated synthetic datasets. The first scaffolding datasetencompasses a variety of elements, including problems, their correspondingsubproblems, hints, incorrect solutions, and tailored feedback. This datasetprovides ITS with essential problem-solving strategies necessary for guidingstudents through each step of the conversation. The second conversationaldataset contains simulated student-tutor conversations that involve theapplication of problem-solving strategies learned from the first dataset. Inthe second dataset, the tutoring system adheres to a pre-defined responsetemplate, which helps to maintain consistency and structure in ITS's responsesduring its interactions. This structured methodology facilitates seamlessintegration of user feedback and yields valuable insights into ITS's internaldecision-making process, allowing for continuous refinement and improvement ofthe system. We also present a proof-of-concept ITS, referred to as SPOCK,trained using the CLASS framework with a focus on college level introductorybiology content. A carefully constructed protocol was developed for SPOCK'spreliminary evaluation, examining aspects such as the factual accuracy andrelevance of its responses. Experts in the field of biology offered favorableremarks, particularly highlighting SPOCK's capability to break down questionsinto manageable subproblems and provide step-by-step guidance to students.</description><author>Shashank Sonkar, Lucy Liu, Debshila Basu Mallick, Richard G. Baraniuk</author><pubDate>Mon, 22 May 2023 18:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13272v1</guid></item><item><title>MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks</title><link>http://arxiv.org/abs/2305.13271v1</link><description>Despite their successful application to a variety of tasks, neural networksremain limited, like other machine learning methods, by their sensitivity toshifts in the data: their performance can be severely impacted by differencesin distribution between the data on which they were trained and that on whichthey are deployed. In this article, we propose a new family of representations,called MAGDiff, that we extract from any given neural network classifier andthat allows for efficient covariate data shift detection without the need totrain a new model dedicated to this task. These representations are computed bycomparing the activation graphs of the neural network for samples belonging tothe training distribution and to the target distribution, and yield powerfuldata- and task-adapted statistics for the two-sample tests commonly used fordata set shift detection. We demonstrate this empirically by measuring thestatistical powers of two-sample Kolmogorov-Smirnov (KS) tests on severaldifferent data sets and shift types, and showing that our novel representationsinduce significant improvements over a state-of-the-art baseline relying on thenetwork output.</description><author>Felix Hensel, Charles Arnal, Mathieu Carrière, Théo Lacombe, Hiroaki Kurihara, Yuichi Ike, Frédéric Chazal</author><pubDate>Mon, 22 May 2023 18:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13271v1</guid></item><item><title>Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases</title><link>http://arxiv.org/abs/2305.13269v1</link><description>We introduce Chain of Knowledge (CoK), a framework that augments largelanguage models with structured knowledge bases to improve factual correctnessand reduce hallucination. Compared to previous works which only retrieveunstructured texts, CoK leverages structured knowledge bases which supportcomplex queries and offer more direct factual statements. To assist largelanguage models to effectively query knowledge bases, we propose a querygenerator model with contrastive instruction-tuning. As the query generator isseparate from the frozen large language model, our framework is modular andthus easily adapted to various knowledge sources and models. Experiments showthat our framework significantly enhances the factual correctness of largelanguage models on knowledge-intensive tasks.</description><author>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, Soujanya Poria</author><pubDate>Mon, 22 May 2023 18:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13269v1</guid></item><item><title>Enhance Reasoning Ability of Visual-Language Models via Large Language Models</title><link>http://arxiv.org/abs/2305.13267v1</link><description>Pre-trained visual language models (VLM) have shown excellent performance inimage caption tasks. However, it sometimes shows insufficient reasoningability. In contrast, large language models (LLMs) emerge with powerfulreasoning capabilities. Therefore, we propose a method called TReE, whichtransfers the reasoning ability of a large language model to a visual languagemodel in zero-shot scenarios. TReE contains three stages: observation,thinking, and re-thinking. Observation stage indicates that VLM obtains theoverall information of the relative image. Thinking stage combines the imageinformation and task description as the prompt of the LLM, inference with therationals. Re-Thinking stage learns from rationale and then inference the finalresult through VLM.</description><author>Yueting Yang, Xintong Zhang, Wenjuan Han</author><pubDate>Mon, 22 May 2023 18:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13267v1</guid></item><item><title>Prompt-based methods may underestimate large language models' linguistic generalizations</title><link>http://arxiv.org/abs/2305.13264v1</link><description>Prompting is now a dominant method for evaluating the linguistic knowledge oflarge language models (LLMs). While other methods directly read out models'probability distributions over strings, prompting requires models to accessthis internal information by processing linguistic input, thereby implicitlytesting a new type of emergent ability: metalinguistic judgment. In this study,we compare metalinguistic prompting and direct probability measurements as waysof measuring models' knowledge of English. Broadly, we find that LLMs'metalinguistic judgments are inferior to quantities directly derived fromrepresentations. Furthermore, consistency gets worse as the prompt divergesfrom direct measurements of next-word probabilities. Our findings suggest thatnegative results relying on metalinguistic prompts cannot be taken asconclusive evidence that an LLM lacks a particular linguistic competence. Ourresults also highlight the lost value with the move to closed APIs where accessto probability distributions is limited.</description><author>Jennifer Hu, Roger Levy</author><pubDate>Mon, 22 May 2023 18:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13264v1</guid></item><item><title>Modulation Extraction for LFO-driven Audio Effects</title><link>http://arxiv.org/abs/2305.13262v1</link><description>Low frequency oscillator (LFO) driven audio effects such as phaser, flanger,and chorus, modify an input signal using time-varying filters and delays,resulting in characteristic sweeping or widening effects. It has been shownthat these effects can be modeled using neural networks when conditioned withthe ground truth LFO signal. However, in most cases, the LFO signal is notaccessible and measurement from the audio signal is nontrivial, hindering themodeling process. To address this, we propose a framework capable of extractingarbitrary LFO signals from processed audio across multiple digital audioeffects, parameter settings, and instrument configurations. Since our systemimposes no restrictions on the LFO signal shape, we demonstrate its ability toextract quasiperiodic, combined, and distorted modulation signals that arerelevant to effect modeling. Furthermore, we show how coupling the extractionmodel with a simple processing network enables training of end-to-end black-boxmodels of unseen analog or digital LFO-driven audio effects using only dry andwet audio pairs, overcoming the need to access the audio effect or internal LFOsignal. We make our code available and provide the trained audio effect modelsin a real-time VST plugin.</description><author>Christopher Mitcheltree, Christian J. Steinmetz, Marco Comunità, Joshua D. Reiss</author><pubDate>Mon, 22 May 2023 18:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13262v1</guid></item><item><title>SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers</title><link>http://arxiv.org/abs/2212.10325v5</link><description>Diffusion model, a new generative modelling paradigm, has achieved greatsuccess in image, audio, and video generation. However, considering thediscrete categorical nature of text, it is not trivial to extend continuousdiffusion models to natural language, and text diffusion models are lessstudied. Sequence-to-sequence text generation is one of the essential naturallanguage processing topics. In this work, we apply diffusion models to approachsequence-to-sequence text generation, and explore whether the superioritygeneration performance of diffusion model can transfer to natural languagedomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequencegeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture tomodel denoising function. In order to improve generation quality, SeqDiffuSeqcombines the self-conditioning technique and a newly proposed adaptive noiseschedule technique. The adaptive noise schedule has the difficulty of denoisingevenly distributed across time steps, and considers exclusive noise schedulesfor tokens at different positional order. Experiment results illustrate thegood performance on sequence-to-sequence generation in terms of text qualityand inference time.</description><author>Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang</author><pubDate>Mon, 22 May 2023 18:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10325v5</guid></item><item><title>NeSy4VRD: A Multifaceted Resource for Neurosymbolic AI Research using Knowledge Graphs in Visual Relationship Detection</title><link>http://arxiv.org/abs/2305.13258v1</link><description>NeSy4VRD is a multifaceted resource designed to support the development ofneurosymbolic AI (NeSy) research. NeSy4VRD re-establishes public access to theimages of the VRD dataset and couples them with an extensively revised,quality-improved version of the VRD visual relationship annotations. Crucially,NeSy4VRD provides a well-aligned, companion OWL ontology that describes thedataset domain.It comes with open source infrastructure that providescomprehensive support for extensibility of the annotations (which, in turn,facilitates extensibility of the ontology), and open source code for loadingthe annotations to/from a knowledge graph. We are contributing NeSy4VRD to thecomputer vision, NeSy and Semantic Web communities to help foster more NeSyresearch using OWL-based knowledge graphs.</description><author>David Herron, Ernesto Jiménez-Ruiz, Giacomo Tarroni, Tillman Weyde</author><pubDate>Mon, 22 May 2023 18:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13258v1</guid></item><item><title>TaskWeb: Selecting Better Source Tasks for Multi-task NLP</title><link>http://arxiv.org/abs/2305.13256v1</link><description>Recent work in NLP has shown promising results in training models on largeamounts of tasks to achieve better generalization. However, it is notwell-understood how tasks are related, and how helpful training tasks can bechosen for a new task. In this work, we investigate whether knowing taskrelationships via pairwise task transfer improves choosing one or more sourcetasks that help to learn a new target task. We provide TaskWeb, a large-scalebenchmark of pairwise task transfers for 22 NLP tasks using three differentmodel types, sizes, and adaptation methods, spanning about 25,000 experiments.Then, we design a new method TaskShop based on our analysis of TaskWeb.TaskShop uses TaskWeb to estimate the benefit of using a source task forlearning a new target, and to choose a subset of helpful training tasks formulti-task learning. Our method improves overall rankings and top-k precisionof source tasks by 12% and 29%, respectively. We also use TaskShop to buildsmaller multi-task training sets that improve zero-shot performances across 11different target tasks by at least 4.3%.</description><author>Joongwon Kim, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi</author><pubDate>Mon, 22 May 2023 18:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13256v1</guid></item><item><title>RRHF: Rank Responses to Align Language Models with Human Feedback without tears</title><link>http://arxiv.org/abs/2304.05302v2</link><description>Reinforcement Learning from Human Feedback (RLHF) facilitates the alignmentof large language models with human preferences, significantly enhancing thequality of interactions between humans and these models. InstructGPT implementsRLHF through several stages, including Supervised Fine-Tuning (SFT), rewardmodel training, and Proximal Policy Optimization (PPO). PPO, however, issensitive to hyperparameters and requires a minimum of four models in itsstandard implementation, which makes it hard to train. In contrast, we proposea novel learning paradigm called RRHF, which scores responses generated bydifferent sampling policies and learns to align them with human preferencesthrough ranking loss. RRHF can efficiently align language model outputprobabilities with human preferences as robust as fine-tuning and it only needs1 to 2 models during tuning. In addition, RRHF can be considered an extensionof SFT and reward models while being simpler than PPO in terms of coding, modelcounts, and hyperparameters. The entire alignment process can be accomplishedwithin a single RRHF training session. We evaluate RRHF using LLaMA and Alpacaon Helpful and Harmless data, demonstrating performance comparable to PPO.</description><author>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang</author><pubDate>Mon, 22 May 2023 18:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05302v2</guid></item><item><title>"According to ..." Prompting Language Models Improves Quoting from Pre-Training Data</title><link>http://arxiv.org/abs/2305.13252v1</link><description>Large Language Models (LLMs) may hallucinate and generate fake information,despite pre-training on factual data. Inspired by the journalistic device of"according to sources", we propose according-to prompting: directing LLMs toground responses against previously observed text. To quantify this grounding,we propose a novel evaluation metric (QUIP-Score) that measures the extent towhich model-produced answers are directly found in underlying text corpora. Weillustrate with experiments on Wikipedia that these prompts improve groundingunder our metrics, with the additional benefit of often improving end-taskperformance. Furthermore, prompts that ask the model to decrease grounding (orto ground to other corpora) decrease grounding, indicating the ability oflanguage models to increase or decrease grounded generations on request.</description><author>Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</author><pubDate>Mon, 22 May 2023 18:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13252v1</guid></item><item><title>Copy Recurrent Neural Network Structure Network</title><link>http://arxiv.org/abs/2305.13250v1</link><description>Electronic Health Record (EHR) coding involves automatically classifying EHRsinto diagnostic codes. While most previous research treats this as amulti-label classification task, generating probabilities for each code andselecting those above a certain threshold as labels, these approaches oftenoverlook the challenge of identifying complex diseases. In this study, ourfocus is on detecting complication diseases within EHRs. We propose a novel coarse-to-fine ICD path generation framework called theCopy Recurrent Neural Network Structure Network (CRNNet), which employs a PathGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs togenerate sequential outputs and incorporating a copy module, we efficientlyidentify complication diseases. Our method achieves a 57.30\% ratio of complexdiseases in predictions, outperforming state-of-the-art and previousapproaches. Additionally, through an ablation study, we demonstrate that the copymechanism plays a crucial role in detecting complex diseases.</description><author>Xiaofan Zhou, Xunzhu Tang</author><pubDate>Mon, 22 May 2023 18:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13250v1</guid></item><item><title>Bayesian Numerical Integration with Neural Networks</title><link>http://arxiv.org/abs/2305.13248v1</link><description>Bayesian probabilistic numerical methods for numerical integration offersignificant advantages over their non-Bayesian counterparts: they can encodeprior information about the integrand, and can quantify uncertainty overestimates of an integral. However, the most popular algorithm in this class,Bayesian quadrature, is based on Gaussian process models and is thereforeassociated with a high computational cost. To improve scalability, we proposean alternative approach based on Bayesian neural networks which we callBayesian Stein networks. The key ingredients are a neural network architecturebased on Stein operators, and an approximation of the Bayesian posterior basedon the Laplace approximation. We show that this leads to orders of magnitudespeed-ups on the popular Genz functions benchmark, and on challenging problemsarising in the Bayesian analysis of dynamical systems, and the prediction ofenergy production for a large-scale wind farm.</description><author>Katharina Ott, Michael Tiemann, Philipp Hennig, François-Xavier Briol</author><pubDate>Mon, 22 May 2023 18:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13248v1</guid></item><item><title>Interactive Natural Language Processing</title><link>http://arxiv.org/abs/2305.13246v1</link><description>Interactive Natural Language Processing (iNLP) has emerged as a novelparadigm within the field of NLP, aimed at addressing limitations in existingframeworks while aligning with the ultimate goals of artificial intelligence.This paradigm considers language models as agents capable of observing, acting,and receiving feedback iteratively from external entities. Specifically,language models in this context can: (1) interact with humans for betterunderstanding and addressing user needs, personalizing responses, aligning withhuman values, and improving the overall user experience; (2) interact withknowledge bases for enriching language representations with factual knowledge,enhancing the contextual relevance of responses, and dynamically leveragingexternal information to generate more accurate and informed responses; (3)interact with models and tools for effectively decomposing and addressingcomplex tasks, leveraging specialized expertise for specific subtasks, andfostering the simulation of social behaviors; and (4) interact withenvironments for learning grounded representations of language, and effectivelytackling embodied tasks such as reasoning, planning, and decision-making inresponse to environmental observations. This paper offers a comprehensivesurvey of iNLP, starting by proposing a unified definition and framework of theconcept. We then provide a systematic classification of iNLP, dissecting itsvarious components, including interactive objects, interaction interfaces, andinteraction methods. We proceed to delve into the evaluation methodologies usedin the field, explore its diverse applications, scrutinize its ethical andsafety issues, and discuss prospective research directions. This survey servesas an entry point for researchers who are interested in this rapidly evolvingarea and offers a broad view of the current landscape and future trajectory ofiNLP.</description><author>Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, Jie Fu</author><pubDate>Mon, 22 May 2023 18:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13246v1</guid></item><item><title>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</title><link>http://arxiv.org/abs/2305.13245v1</link><description>Multi-query attention (MQA), which only uses a single key-value head,drastically speeds up decoder inference. However, MQA can lead to qualitydegradation, and moreover it may not be desirable to train a separate modeljust for faster inference. We (1) propose a recipe for uptraining existingmulti-head language model checkpoints into models with MQA using 5% of originalpre-training compute, and (2) introduce grouped-query attention (GQA), ageneralization of multi-query attention which uses an intermediate (more thanone, less than number of query heads) number of key-value heads. We show thatuptrained GQA achieves quality close to multi-head attention with comparablespeed to MQA.</description><author>Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai</author><pubDate>Mon, 22 May 2023 18:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13245v1</guid></item><item><title>Chip-Chat: Challenges and Opportunities in Conversational Hardware Design</title><link>http://arxiv.org/abs/2305.13243v1</link><description>Modern hardware design starts with specifications provided in naturallanguage. These are then translated by hardware engineers into appropriateHardware Description Languages (HDLs) such as Verilog before synthesizingcircuit elements. Automating this translation could reduce sources of humanerror from the engineering process. But, it is only recently that artificialintelligence (AI) has demonstrated capabilities for machine-based end-to-enddesign translations. Commercially-available instruction-tuned Large LanguageModels (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able toproduce code in a variety of programming languages; but studies examining themfor hardware are still lacking. In this work, we thus explore the challengesfaced and opportunities presented when leveraging these recent advances in LLMsfor hardware design. Using a suite of 8 representative benchmarks, we examinedthe capabilities and limitations of the state of the art conversational LLMswhen producing Verilog for functional and verification purposes. Given that theLLMs performed best when used interactively, we then performed a longer fullyconversational case study where a hardware engineer co-designed a novel 8-bitaccumulator-based microprocessor architecture. We sent the benchmarks andprocessor to tapeout in a Skywater 130nm shuttle, meaning that these'Chip-Chats' resulted in what we believe to be the world's firstwholly-AI-written HDL for tapeout.</description><author>Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce</author><pubDate>Mon, 22 May 2023 18:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13243v1</guid></item><item><title>Deepfake Text Detection in the Wild</title><link>http://arxiv.org/abs/2305.13242v1</link><description>Recent advances in large language models have enabled them to reach a levelof text generation comparable to that of humans. These models show powerfulcapabilities across a wide range of content, including news article writing,story generation, and scientific writing. Such capability further narrows thegap between human-authored and machine-generated texts, highlighting theimportance of deepfake text detection to avoid potential risks such as fakenews propagation and plagiarism. However, previous work has been limited inthat they testify methods on testbed of specific domains or certain languagemodels. In practical scenarios, the detector faces texts from various domainsor LLMs without knowing their sources. To this end, we build a wild testbed bygathering texts from various human writings and deepfake texts generated bydifferent LLMs. Human annotators are only slightly better than random guessingat identifying machine-generated texts. Empirical results on automaticdetection methods further showcase the challenges of deepfake text detection ina wild testbed. In addition, out-of-distribution poses a greater challenge fora detector to be employed in realistic application scenarios. We release ourresources at https://github.com/yafuly/DeepfakeTextDetect.</description><author>Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang</author><pubDate>Mon, 22 May 2023 18:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13242v1</guid></item><item><title>Predicting municipalities in financial distress: a machine learning approach enhanced by domain expertise</title><link>http://arxiv.org/abs/2302.05780v2</link><description>Financial distress of municipalities, although comparable to bankruptcy ofprivate companies, has a far more serious impact on the well-being ofcommunities. For this reason, it is essential to detect deficits as soon aspossible. Predicting financial distress in municipalities can be a complextask, as it involves understanding a wide range of factors that can affect amunicipality's financial health. In this paper, we evaluate machine learningmodels to predict financial distress in Italian municipalities. Accountingjudiciary experts have specialized knowledge and experience in evaluating thefinancial performance, and they use a range of indicators to make theirassessments. By incorporating these indicators in the feature extractionprocess, we can ensure that the model is taking into account a wide range ofinformation that is relevant to the financial health of municipalities. Theresults of this study indicate that using machine learning models incombination with the knowledge of accounting judiciary experts can aid in theearly detection of financial distress, leading to better outcomes for thecommunities.</description><author>Dario Piermarini, Antonio M. Sudoso, Veronica Piccialli</author><pubDate>Mon, 22 May 2023 18:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05780v2</guid></item><item><title>Adaptive Gradient Prediction for DNN Training</title><link>http://arxiv.org/abs/2305.13236v1</link><description>Neural network training is inherently sequential where the layers finish theforward propagation in succession, followed by the calculation andback-propagation of gradients (based on a loss function) starting from the lastlayer. The sequential computations significantly slow down neural networktraining, especially the deeper ones. Prediction has been successfully used inmany areas of computer architecture to speed up sequential processing.Therefore, we propose ADA-GP, that uses gradient prediction adaptively to speedup deep neural network (DNN) training while maintaining accuracy. ADA-GP worksby incorporating a small neural network to predict gradients for differentlayers of a DNN model. ADA-GP uses a novel tensor reorganization to make itfeasible to predict a large number of gradients. ADA-GP alternates between DNNtraining using backpropagated gradients and DNN training using predictedgradients. ADA-GP adaptively adjusts when and for how long gradient predictionis used to strike a balance between accuracy and performance. Last but notleast, we provide a detailed hardware extension in a typical DNN accelerator torealize the speed up potential from gradient prediction. Our extensiveexperiments with fourteen DNN models show that ADA-GP can achieve an averagespeed up of 1.47x with similar or even higher accuracy than the baselinemodels. Moreover, it consumes, on average, 34% less energy due to reducedoff-chip memory accesses compared to the baseline hardware accelerator.</description><author>Vahid Janfaza, Shantanu Mandal, Farabi Mahmud, Abdullah Muzahid</author><pubDate>Mon, 22 May 2023 18:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13236v1</guid></item><item><title>SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations</title><link>http://arxiv.org/abs/2305.13235v1</link><description>Explaining the decisions of neural models is crucial for ensuring theirtrustworthiness at deployment time. Using Natural Language Explanations (NLEs)to justify a model's predictions has recently gained increasing interest.However, this approach usually demands large datasets of human-written NLEs forthe ground-truth answers, which are expensive and potentially infeasible forsome applications. For models to generate high-quality NLEs when only a fewNLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) inconjunction with prompt-based learning recently emerged. However, PLMstypically have billions of parameters, making fine-tuning expensive. We proposeSparseFit, a sparse few-shot fine-tuning strategy that leverages discreteprompts to jointly generate predictions and NLEs. We experiment with SparseFiton the T5 model and four datasets and compare it against state-of-the-artparameter-efficient fine-tuning techniques. We perform automatic and humanevaluations to assess the quality of the model-generated NLEs, finding thatfine-tuning only 6.8% of the model parameters leads to competitive results forboth the task performance and the quality of the NLEs.</description><author>Jesus Solano, Oana-Maria Camburu, Pasquale Minervini</author><pubDate>Mon, 22 May 2023 18:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13235v1</guid></item><item><title>Gibbs free energies via isobaric-isothermal flows</title><link>http://arxiv.org/abs/2305.13233v1</link><description>We present a machine-learning model based on normalizing flows that istrained to sample from the isobaric-isothermal (NPT) ensemble. In our approach,we approximate the joint distribution of a fully-flexible triclinic simulationbox and particle coordinates to achieve a desired internal pressure. We testour model on monatomic water in the cubic and hexagonal ice phases and findexcellent agreement of Gibbs free energies and other observables compared withestablished baselines.</description><author>Peter Wirnsberger, Borja Ibarz, George Papamakarios</author><pubDate>Mon, 22 May 2023 18:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13233v1</guid></item><item><title>Revisiting Data Augmentation in Model Compression: An Empirical and Comprehensive Study</title><link>http://arxiv.org/abs/2305.13232v1</link><description>The excellent performance of deep neural networks is usually accompanied by alarge number of parameters and computations, which have limited their usage onthe resource-limited edge devices. To address this issue, abundant methods suchas pruning, quantization and knowledge distillation have been proposed tocompress neural networks and achieved significant breakthroughs. However, mostof these compression methods focus on the architecture or the training methodof neural networks but ignore the influence from data augmentation. In thispaper, we revisit the usage of data augmentation in model compression and givea comprehensive study on the relation between model sizes and their optimaldata augmentation policy. To sum up, we mainly have the following threeobservations: (A) Models in different sizes prefer data augmentation withdifferent magnitudes. Hence, in iterative pruning, data augmentation withvarying magnitudes leads to better performance than data augmentation with aconsistent magnitude. (B) Data augmentation with a high magnitude maysignificantly improve the performance of large models but harm the performanceof small models. Fortunately, small models can still benefit from strong dataaugmentations by firstly learning them with "additional parameters" and thendiscard these "additional parameters" during inference. (C) The prediction of apre-trained large model can be utilized to measure the difficulty of dataaugmentation. Thus it can be utilized as a criterion to design better dataaugmentation policies. We hope this paper may promote more research on theusage of data augmentation in model compression.</description><author>Muzhou Yu, Linfeng Zhang, Kaisheng Ma</author><pubDate>Mon, 22 May 2023 18:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13232v1</guid></item><item><title>To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</title><link>http://arxiv.org/abs/2305.13230v1</link><description>Recent research has highlighted the importance of dataset size in scalinglanguage models. However, large language models (LLMs) are notoriouslytoken-hungry during pre-training, and high-quality text data on the web isapproaching its scaling limit for LLMs. To further enhance LLMs, astraightforward approach is to repeat the pre-training data for additionalepochs. In this study, we empirically investigate three key aspects under thisapproach. First, we explore the consequences of repeating pre-training data,revealing that the model is susceptible to overfitting, leading to multi-epochdegradation. Second, we examine the key factors contributing to multi-epochdegradation, finding that significant factors include dataset size, modelparameters, and training objectives, while less influential factors consist ofdataset quality and model FLOPs. Finally, we explore whether widely usedregularization can alleviate multi-epoch degradation. Most regularizationtechniques do not yield significant improvements, except for dropout, whichdemonstrates remarkable effectiveness but requires careful tuning when scalingup the model size. Additionally, we discover that leveraging mixture-of-experts(MoE) enables cost-effective and efficient hyper-parameter tuning forcomputationally intensive dense LLMs with comparable trainable parameters,potentially impacting efficient LLM development on a broader scale.</description><author>Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You</author><pubDate>Mon, 22 May 2023 18:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13230v1</guid></item><item><title>Sequential Transfer Learning to Decode Heard and Imagined Timbre from fMRI Data</title><link>http://arxiv.org/abs/2305.13226v1</link><description>We present a sequential transfer learning framework for transformers onfunctional Magnetic Resonance Imaging (fMRI) data and demonstrate itssignificant benefits for decoding musical timbre. In the first of two phases,we pre-train our stacked-encoder transformer architecture on Next ThoughtPrediction, a self-supervised task of predicting whether or not one sequence offMRI data follows another. This phase imparts a general understanding of thetemporal and spatial dynamics of neural activity, and can be applied to anyfMRI dataset. In the second phase, we fine-tune the pre-trained models andtrain additional fresh models on the supervised task of predicting whether ornot two sequences of fMRI data were recorded while listening to the samemusical timbre. The fine-tuned models achieve significantly higher accuracywith shorter training times than the fresh models, demonstrating the efficacyof our framework for facilitating transfer learning on fMRI data. Additionally,our fine-tuning task achieves a level of classification granularity beyondstandard methods. This work contributes to the growing literature ontransformer architectures for sequential transfer learning on fMRI data, andprovides evidence that our framework is an improvement over current methods fordecoding timbre.</description><author>Sean Paulsen, Michael Casey</author><pubDate>Mon, 22 May 2023 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13226v1</guid></item><item><title>Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance</title><link>http://arxiv.org/abs/2305.13225v1</link><description>ChatGPT and GPT-4 have attracted substantial interest from both academic andindustrial circles, owing to their remarkable few-shot (or even zero-shot)ability to handle various tasks. Recent work shows that, after being fine-tunedwith a few sets of instruction-driven data, the recently proposed LLM, LLaMa,exhibits an impressive capability to address a broad range of tasks. However,the zero-shot performance of LLMs does not consistently outperform that ofmodels fined-tuned for specific scenarios. To explore whether the capabilitiesof LLMs can be further enhanced for specific scenarios, we choose thewriting-assistance scenario as the testbed, including seven writing tasks. Wecollect training data for these tasks, reframe them in an instruction-followingformat, and subsequently refine LLaMa via instruction tuning. Experimentalresults show that continually fine-tuning LLaMa on writing instruction datasignificantly improves its ability on writing tasks. We also conduct moreexperiments and analyses to offer insights for future work on effectivelyfine-tuning LLaMa for specific scenarios.</description><author>Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, Wei Bi</author><pubDate>Mon, 22 May 2023 17:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13225v1</guid></item><item><title>RISE: Leveraging Retrieval Techniques for Summarization Evaluation</title><link>http://arxiv.org/abs/2212.08775v2</link><description>Evaluating automatically-generated text summaries is a challenging task.While there have been many interesting approaches, they still fall short ofhuman evaluations. We present RISE, a new approach for evaluating summaries byleveraging techniques from information retrieval. RISE is first trained as aretrieval task using a dual-encoder retrieval setup, and can then besubsequently utilized for evaluating a generated summary given an inputdocument, without gold reference summaries. RISE is especially well suited whenworking on new datasets where one may not have reference summaries availablefor evaluation. We conduct comprehensive experiments on the SummEval benchmark(Fabbri et al., 2021) and the results show that RISE has higher correlationwith human evaluations compared to many past approaches to summarizationevaluation. Furthermore, RISE also demonstrates data-efficiency andgeneralizability across languages.</description><author>David Uthus, Jianmo Ni</author><pubDate>Mon, 22 May 2023 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08775v2</guid></item><item><title>Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense Grids</title><link>http://arxiv.org/abs/2305.13220v1</link><description>Indoor scene reconstruction from monocular images has long been sought afterby augmented reality and robotics developers. Recent advances in neural fieldrepresentations and monocular priors have led to remarkable results inscene-level surface reconstructions. The reliance on Multilayer Perceptrons(MLP), however, significantly limits speed in training and rendering. In thiswork, we propose to directly use signed distance function (SDF) in sparse voxelblock grids for fast and accurate scene reconstruction without MLPs. Ourglobally sparse and locally dense data structure exploits surfaces' spatialsparsity, enables cache-friendly queries, and allows direct extensions tomulti-modal data such as color and semantic labels. To apply thisrepresentation to monocular scene reconstruction, we develop a scalecalibration algorithm for fast geometric initialization from monocular depthpriors. We apply differentiable volume rendering from this initialization torefine details with fast convergence. We also introduce efficienthigh-dimensional Continuous Random Fields (CRFs) to further exploit thesemantic-geometry consistency between scene objects. Experiments show that ourapproach is 10x faster in training and 100x faster in rendering while achievingcomparable accuracy to state-of-the-art neural implicit methods.</description><author>Wei Dong, Chris Choy, Charles Loop, Or Litany, Yuke Zhu, Anima Anandkumar</author><pubDate>Mon, 22 May 2023 17:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13220v1</guid></item><item><title>Sequence-to-Sequence Forecasting-aided State Estimation for Power Systems</title><link>http://arxiv.org/abs/2305.13215v1</link><description>Power system state forecasting has gained more attention in real-timeoperations recently. Unique challenges to energy systems are emerging with themassive deployment of renewable energy resources. As a result, power systemstate forecasting are becoming more crucial for monitoring, operating andsecuring modern power systems. This paper proposes an end-to-end deep learningframework to accurately predict multi-step power system state estimations inreal-time. In our model, we employ a sequence-to-sequence framework to allowfor multi-step forecasting. Bidirectional gated recurrent units (BiGRUs) areincorporated into the model to achieve high prediction accuracy. The dominantperformance of our model is validated using real dataset. Experimental resultsshow the superiority of our model in predictive power compared to existingalternatives.</description><author>Kamal Basulaiman, Masoud Barati</author><pubDate>Mon, 22 May 2023 17:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13215v1</guid></item><item><title>Logical Reasoning for Natural Language Inference Using Generated Facts as Atoms</title><link>http://arxiv.org/abs/2305.13214v1</link><description>State-of-the-art neural models can now reach human performance levels acrossvarious natural language understanding tasks. However, despite this impressiveperformance, models are known to learn from annotation artefacts at the expenseof the underlying task. While interpretability methods can identify influentialfeatures for each prediction, there are no guarantees that these features areresponsible for the model decisions. Instead, we introduce a model-agnosticlogical framework to determine the specific information in an input responsiblefor each model decision. This method creates interpretable Natural LanguageInference (NLI) models that maintain their predictive power. We achieve this bygenerating facts that decompose complex NLI observations into individuallogical atoms. Our model makes predictions for each atom and uses logical rulesto decide the class of the observation based on the predictions for each atom.We apply our method to the highly challenging ANLI dataset, where our frameworkimproves the performance of both a DeBERTa-base and BERT baseline. Our methodperforms best on the most challenging examples, achieving a newstate-of-the-art for the ANLI round 3 test set. We outperform every baseline ina reduced-data setting, and despite using no annotations for the generatedfacts, our model predictions for individual facts align with humanexpectations.</description><author>Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Oana-Maria Camburu, Marek Rei</author><pubDate>Mon, 22 May 2023 17:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13214v1</guid></item><item><title>Faster Differentially Private Convex Optimization via Second-Order Methods</title><link>http://arxiv.org/abs/2305.13209v1</link><description>Differentially private (stochastic) gradient descent is the workhorse of DPprivate machine learning in both the convex and non-convex settings. Withoutprivacy constraints, second-order methods, like Newton's method, convergefaster than first-order methods like gradient descent. In this work, weinvestigate the prospect of using the second-order information from the lossfunction to accelerate DP convex optimization. We first develop a privatevariant of the regularized cubic Newton method of Nesterov and Polyak, and showthat for the class of strongly convex loss functions, our algorithm hasquadratic convergence and achieves the optimal excess loss. We then design apractical second-order DP algorithm for the unconstrained logistic regressionproblem. We theoretically and empirically study the performance of ouralgorithm. Empirical results show our algorithm consistently achieves the bestexcess loss compared to other baselines and is 10-40x faster than DP-GD/DP-SGD.</description><author>Arun Ganesh, Mahdi Haghifam, Thomas Steinke, Abhradeep Thakurta</author><pubDate>Mon, 22 May 2023 17:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13209v1</guid></item><item><title>Know your Enemy: Investigating Monte-Carlo Tree Search with Opponent Models in Pommerman</title><link>http://arxiv.org/abs/2305.13206v1</link><description>In combination with Reinforcement Learning, Monte-Carlo Tree Search has shownto outperform human grandmasters in games such as Chess, Shogi and Go withlittle to no prior domain knowledge. However, most classical use cases onlyfeature up to two players. Scaling the search to an arbitrary number of playerspresents a computational challenge, especially if decisions have to be plannedover a longer time horizon. In this work, we investigate techniques thattransform general-sum multiplayer games into single-player and two-player gamesthat consider other agents to act according to given opponent models. For ourevaluation, we focus on the challenging Pommerman environment which involvespartial observability, a long time horizon and sparse rewards. In combinationwith our search methods, we investigate the phenomena of opponent modelingusing heuristics and self-play. Overall, we demonstrate the effectiveness ofour multiplayer search variants both in a supervised learning and reinforcementlearning setting.</description><author>Jannis Weil, Johannes Czech, Tobias Meuser, Kristian Kersting</author><pubDate>Mon, 22 May 2023 17:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13206v1</guid></item><item><title>Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters</title><link>http://arxiv.org/abs/2305.13204v1</link><description>To translate speech for automatic dubbing, machine translation needs to beisochronous, i.e. translated speech needs to be aligned with the source interms of speech durations. We introduce target factors in a transformer modelto predict durations jointly with target language phoneme sequences. We alsointroduce auxiliary counters to help the decoder to keep track of the timinginformation while generating target phonemes. We show that our model improvestranslation quality and isochrony compared to previous work where thetranslation model is instead trained to predict interleaved sequences ofphonemes and durations.</description><author>Proyag Pal, Brian Thompson, Yogesh Virkar, Prashant Mathur, Alexandra Chronopoulou, Marcello Federico</author><pubDate>Mon, 22 May 2023 17:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13204v1</guid></item><item><title>CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models</title><link>http://arxiv.org/abs/2205.12213v2</link><description>Methods for adapting language models (LMs) to new tasks and domains havetraditionally assumed white-box access to the model, and work by modifying itsparameters. However, this is incompatible with a recent trend in the field,where the highest quality models are only available as black-boxes throughinference APIs. Even when the model weights are available, the computationalcost of fine-tuning large LMs can be prohibitive for most practitioners. Inthis work, we present a lightweight method for adapting large LMs to newdomains and tasks, assuming no access to their weights or intermediateactivations. Our approach fine-tunes a small white-box LM and combines it withthe large black-box LM at the probability level through a small network,learned on a small validation set. We validate our approach by adapting a largeLM (OPT-30B) to several domains and a downstream task (machine translation),observing improved performance in all cases, of up to 9\%, while using a domainexpert 23x smaller.</description><author>Aitor Ormazabal, Mikel Artetxe, Eneko Agirre</author><pubDate>Mon, 22 May 2023 17:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12213v2</guid></item><item><title>Understanding accountability in algorithmic supply chains</title><link>http://arxiv.org/abs/2304.14749v2</link><description>Academic and policy proposals on algorithmic accountability often seek tounderstand algorithmic systems in their socio-technical context, recognisingthat they are produced by 'many hands'. Increasingly, however, algorithmicsystems are also produced, deployed, and used within a supply chain comprisingmultiple actors tied together by flows of data between them. In such cases, itis the working together of an algorithmic supply chain of different actors whocontribute to the production, deployment, use, and functionality that drivessystems and produces particular outcomes. We argue that algorithmicaccountability discussions must consider supply chains and the difficultimplications they raise for the governance and accountability of algorithmicsystems. In doing so, we explore algorithmic supply chains, locating them intheir broader technical and political economic context and identifying some keyfeatures that should be understood in future work on algorithmic governance andaccountability (particularly regarding general purpose AI services). Tohighlight ways forward and areas warranting attention, we further discuss someimplications raised by supply chains: challenges for allocating accountabilitystemming from distributed responsibility for systems between actors, limitedvisibility due to the accountability horizon, service models of use andliability, and cross-border supply chains and regulatory arbitrage</description><author>Jennifer Cobbe, Michael Veale, Jatinder Singh</author><pubDate>Mon, 22 May 2023 17:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14749v2</guid></item><item><title>Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision</title><link>http://arxiv.org/abs/2305.13199v1</link><description>Most existing task-oriented dialog (TOD) systems track dialog states in termsof slots and values and use them to query a database to get relevant knowledgeto generate responses. In real-life applications, user utterances are noisier,and thus it is more difficult to accurately track dialog states and correctlysecure relevant knowledge. Recently, a progress in question answering anddocument-grounded dialog systems is retrieval-augmented methods with aknowledge retriever. Inspired by such progress, we propose a retrieval-basedmethod to enhance knowledge selection in TOD systems, which significantlyoutperforms the traditional database query method for real-life dialogs.Further, we develop latent variable model based semi-supervised learning, whichcan work with the knowledge retriever to leverage both labeled and unlabeleddialog data. Joint Stochastic Approximation (JSA) algorithm is employed forsemi-supervised model training, and the whole system is referred to as thatJSA-KRTOD. Experiments are conducted on a real-life dataset from China MobileCustom-Service, called MobileCS, and show that JSA-KRTOD achieves superiorperformances in both labeled-only and semi-supervised settings.</description><author>Yucheng Cai, Hong Liu, Zhijian Ou, Yi Huang, Junlan Feng</author><pubDate>Mon, 22 May 2023 17:29:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13199v1</guid></item><item><title>Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale</title><link>http://arxiv.org/abs/2305.13198v1</link><description>We introduce a multilingual extension of the HOLISTICBIAS dataset, thelargest English template-based taxonomy of textual people references:MULTILINGUALHOLISTICBIAS. This extension consists of 20,459 sentences in 50languages distributed across all 13 demographic axes. Source sentences arebuilt from combinations of 118 demographic descriptors and three patterns,excluding nonsensical combinations. Multilingual translations includealternatives for gendered languages that cover gendered translations when thereis ambiguity in English. Our benchmark is intended to uncover demographicimbalances and be the tool to quantify mitigations towards them. Our initial findings show that translation quality for EN-to-XX translationsis an average of 8 spBLEU better when evaluating with the masculine humanreference compared to feminine. In the opposite direction, XX-to-EN, we comparethe robustness of the model when the source input only differs in gender(masculine or feminine) and masculine translations are an average of almost 4spBLEU better than feminine. When embedding sentences to a joint multilingualsentence representations space, we find that for most languages masculinetranslations are significantly closer to the English neutral sentences whenembedded.</description><author>Marta R. Costa-jussà, Pierre Andrews, Eric Smith, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Daniel Licht, Carleigh Wood</author><pubDate>Mon, 22 May 2023 17:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13198v1</guid></item><item><title>Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval</title><link>http://arxiv.org/abs/2305.13197v1</link><description>Recently, various studies have been directed towards exploring dense passageretrieval techniques employing pre-trained language models, among which themasked auto-encoder (MAE) pre-training architecture has emerged as the mostpromising. The conventional MAE framework relies on leveraging the passagereconstruction of decoder to bolster the text representation ability ofencoder, thereby enhancing the performance of resulting dense retrievalsystems. Within the context of building the representation ability of theencoder through passage reconstruction of decoder, it is reasonable topostulate that a ``more demanding'' decoder will necessitate a correspondingincrease in the encoder's ability. To this end, we propose a novel tokenimportance aware masking strategy based on pointwise mutual information tointensify the challenge of the decoder. Importantly, our approach can beimplemented in an unsupervised manner, without adding additional expenses tothe pre-training phase. Our experiments verify that the proposed method is botheffective and robust on large-scale supervised passage retrieval datasets andout-of-domain zero-shot retrieval benchmarks.</description><author>Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie</author><pubDate>Mon, 22 May 2023 17:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13197v1</guid></item><item><title>SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation</title><link>http://arxiv.org/abs/2305.13194v1</link><description>Reliable automatic evaluation of summarization systems is challenging due tothe multifaceted and subjective nature of the task. This is especially the casefor languages other than English, where human evaluations are scarce. In thiswork, we introduce SEAHORSE, a dataset for multilingual, multifacetedsummarization evaluation. SEAHORSE consists of 96K summaries with human ratingsalong 6 quality dimensions: comprehensibility, repetition, grammar,attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4datasets. As a result of its size and scope, SEAHORSE can serve both as abenchmark to evaluate learnt metrics, as well as a large-scale resource fortraining such metrics. We show that metrics trained with SEAHORSE achievestrong performance on the out-of-domain meta-evaluation benchmarks TRUE(Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make SEAHORSEpublicly available for future research on multilingual and multifacetedsummarization evaluation.</description><author>Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, Ankur P. Parikh</author><pubDate>Mon, 22 May 2023 17:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13194v1</guid></item><item><title>ImSimCSE: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives</title><link>http://arxiv.org/abs/2305.13192v1</link><description>This paper aims to improve contrastive learning for sentence embeddings fromtwo perspectives: handling dropout noise and addressing feature corruption.Specifically, for the first perspective, we identify that the dropout noisefrom negative pairs affects the model's performance. Therefore, we propose asimple yet effective method to deal with such type of noise. Secondly, wepinpoint the rank bottleneck of current solutions to feature corruption andpropose a dimension-wise contrastive learning objective to address this issue.Both proposed methods are generic and can be applied to any contrastivelearning based models for sentence embeddings. Experimental results on standardbenchmarks demonstrate that combining both proposed methods leads to a gain of1.8 points compared to the strong baseline SimCSE configured with BERT base.Furthermore, applying the proposed method to DiffCSE, another strongcontrastive learning based baseline, results in a gain of 1.4 points.</description><author>Jiahao Xu, Wei Shao, Lihui Chen, Lemao Liu</author><pubDate>Mon, 22 May 2023 17:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13192v1</guid></item><item><title>Taxonomy Expansion for Named Entity Recognition</title><link>http://arxiv.org/abs/2305.13191v1</link><description>Training a Named Entity Recognition (NER) model often involves fixing ataxonomy of entity types. However, requirements evolve and we might need theNER model to recognize additional entity types. A simple approach is tore-annotate entire dataset with both existing and additional entity types andthen train the model on the re-annotated dataset. However, this is an extremelylaborious task. To remedy this, we propose a novel approach called PartialLabel Model (PLM) that uses only partially annotated datasets. We experimentwith 6 diverse datasets and show that PLM consistently performs better thanmost other approaches (0.5 - 2.5 F1), including in novel settings for taxonomyexpansion not considered in prior work. The gap between PLM and all otherapproaches is especially large in settings where there is limited dataavailable for the additional entity types (as much as 11 F1), thus suggesting amore cost effective approaches to taxonomy expansion.</description><author>Karthikeyan K, Yogarshi Vyas, Jie Ma, Giovanni Paolini, Neha Anna John, Shuai Wang, Yassine Benajiba, Vittorio Castelli, Dan Roth, Miguel Ballesteros</author><pubDate>Mon, 22 May 2023 17:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13191v1</guid></item><item><title>An ASP Framework for the Refinement of Authorization and Obligation Policies</title><link>http://arxiv.org/abs/2305.13190v1</link><description>This paper introduces a framework for assisting policy authors in refiningand improving their policies. In particular, we focus on authorization andobligation policies that can be encoded in Gelfond and Lobo's AOPL language forpolicy specification. We propose a framework that detects the statements thatmake a policy inconsistent, underspecified, or ambiguous with respect to anaction being executed in a given state. We also give attention to issues thatarise at the intersection of authorization and obligation policies, forinstance when the policy requires an unauthorized action to be executed. Theframework is encoded in Answer Set Programming. Under consideration foracceptance in TPLP.</description><author>Daniela Inclezan</author><pubDate>Mon, 22 May 2023 17:23:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13190v1</guid></item><item><title>Unsupervised Anomaly Detection with Rejection</title><link>http://arxiv.org/abs/2305.13189v1</link><description>Anomaly detection aims at detecting unexpected behaviours in the data.Because anomaly detection is usually an unsupervised task, traditional anomalydetectors learn a decision boundary by employing heuristics based onintuitions, which are hard to verify in practice. This introduces someuncertainty, especially close to the decision boundary, that may reduce theuser trust in the detector's predictions. A way to combat this is by allowingthe detector to reject examples with high uncertainty (Learning to Reject).This requires employing a confidence metric that captures the distance to thedecision boundary and setting a rejection threshold to reject low-confidencepredictions. However, selecting a proper metric and setting the rejectionthreshold without labels are challenging tasks. In this paper, we solve thesechallenges by setting a constant rejection threshold on the stability metriccomputed by ExCeeD. Our insight relies on a theoretical analysis of such ametric. Moreover, setting a constant threshold results in strong guarantees: weestimate the test rejection rate, and derive a theoretical upper bound for boththe rejection rate and the expected prediction cost. Experimentally, we showthat our method outperforms some metric-based methods.</description><author>Lorenzo Perini, Jesse Davis</author><pubDate>Mon, 22 May 2023 17:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13189v1</guid></item><item><title>Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews</title><link>http://arxiv.org/abs/2305.11828v2</link><description>Medical systematic reviews are crucial for informing clinical decision makingand healthcare policy. But producing such reviews is onerous andtime-consuming. Thus, high-quality evidence synopses are not available for manyquestions and may be outdated even when they are available. Large languagemodels (LLMs) are now capable of generating long-form texts, suggesting thetantalizing possibility of automatically generating literature reviews ondemand. However, LLMs sometimes generate inaccurate (and potentiallymisleading) texts by hallucinating or omitting important information. In thehealthcare context, this may render LLMs unusable at best and dangerous atworst. Most discussion surrounding the benefits and risks of LLMs have beendivorced from specific applications. In this work, we seek to qualitativelycharacterize the potential utility and risks of LLMs for assisting inproduction of medical evidence reviews. We conducted 16 semi-structuredinterviews with international experts in systematic reviews, groundingdiscussion in the context of generating evidence reviews. Domain expertsindicated that LLMs could aid writing reviews, as a tool for drafting orcreating plain language summaries, generating templates or suggestions,distilling information, crosschecking, and synthesizing or interpreting textinputs. But they also identified issues with model outputs and expressedconcerns about potential downstream harms of confidently composed butinaccurate LLM outputs which might mislead. Other anticipated potentialdownstream harms included lessened accountability and proliferation ofautomatically generated reviews that might be of low quality. Informed by thisqualitative analysis, we identify criteria for rigorous evaluation ofbiomedical LLMs aligned with domain expert views.</description><author>Hye Sun Yun, Iain J. Marshall, Thomas Trikalinos, Byron C. Wallace</author><pubDate>Mon, 22 May 2023 17:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11828v2</guid></item><item><title>Synthetic ECG Signal Generation using Probabilistic Diffusion Models</title><link>http://arxiv.org/abs/2303.02475v4</link><description>Deep learning image processing models have had remarkable success in recentyears in generating high quality images. Particularly, the Improved DenoisingDiffusion Probabilistic Models (DDPM) have shown superiority in image qualityto the state-of-the-art generative models, which motivated us to investigatetheir capability in the generation of the synthetic electrocardiogram (ECG)signals. In this work, synthetic ECG signals are generated by the Improved DDPMand by the Wasserstein GAN with Gradient Penalty (WGAN-GP) models and thencompared. To this end, we devise a pipeline to utilize DDPM in its original$2D$ form. First, the $1D$ ECG time series data are embedded into the $2D$space, for which we employed the Gramian Angular Summation/Difference Fields(GASF/GADF) as well as Markov Transition Fields (MTF) to generate three $2D$matrices from each ECG time series, which when put together, form a $3$-channel$2D$ datum. Then $2D$ DDPM is used to generate $2D$ $3$-channel synthetic ECGimages. The $1D$ ECG signals are created by de-embedding the $2D$ generatedimage files back into the $1D$ space. This work focuses on unconditional modelsand the generation of \emph{Normal Sinus Beat} ECG signals exclusively, wherethe Normal Sinus Beat class from the MIT-BIH Arrhythmia dataset is used in thetraining phase. The \emph{quality}, \emph{distribution}, and the\emph{authenticity} of the generated ECG signals by each model arequantitatively evaluated and compared. Our results show that in the proposedpipeline and in the particular setting of this paper, the WGAN-GP model isconsistently superior to DDPM in all the considered metrics.</description><author>Edmond Adib, Amanda Fernandez, Fatemeh Afghah, John Jeff Prevost</author><pubDate>Mon, 22 May 2023 17:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02475v4</guid></item><item><title>Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities</title><link>http://arxiv.org/abs/2303.12706v3</link><description>One of the challenges of studying common neurological disorders is diseaseheterogeneity including differences in causes, neuroimaging characteristics,comorbidities, or genetic variation. Normative modelling has become a popularmethod for studying such cohorts where the 'normal' behaviour of aphysiological system is modelled and can be used at subject level to detectdeviations relating to disease pathology. For many heterogeneous diseases, weexpect to observe abnormalities across a range of neuroimaging and biologicalvariables. However, thus far, normative models have largely been developed forstudying a single imaging modality. We aim to develop a multi-modal normativemodelling framework where abnormality is aggregated across variables ofmultiple modalities and is better able to detect deviations than uni-modalbaselines. We propose two multi-modal VAE normative models to detect subjectlevel deviations across T1 and DTI data. Our proposed models were better ableto detect diseased individuals, capture disease severity, and correlate withpatient cognition than baseline approaches. We also propose a multivariatelatent deviation metric, measuring deviations from the joint latent space,which outperformed feature-based metrics.</description><author>Ana Lawry Aguila, James Chapman, Andre Altmann</author><pubDate>Mon, 22 May 2023 17:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12706v3</guid></item><item><title>SignSVRG: fixing SignSGD via variance reduction</title><link>http://arxiv.org/abs/2305.13187v1</link><description>We consider the problem of unconstrained minimization of finite sums offunctions. We propose a simple, yet, practical way to incorporate variancereduction techniques into SignSGD, guaranteeing convergence that is similar tothe full sign gradient descent. The core idea is first instantiated on theproblem of minimizing sums of convex and Lipschitz functions and is thenextended to the smooth case via variance reduction. Our analysis is elementaryand much simpler than the typical proof for variance reduction methods. We showthat for smooth functions our method gives $\mathcal{O}(1 / \sqrt{T})$ rate forexpected norm of the gradient and $\mathcal{O}(1/T)$ rate in the case of smoothconvex functions, recovering convergence results of deterministic methods,while preserving computational advantages of SignSGD.</description><author>Evgenii Chzhen, Sholom Schechtman</author><pubDate>Mon, 22 May 2023 17:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13187v1</guid></item><item><title>SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables</title><link>http://arxiv.org/abs/2305.13186v1</link><description>Scientific fact-checking is crucial for ensuring the accuracy, reliability,and trustworthiness of scientific claims. However, existing benchmarks arelimited in terms of their claim diversity, reliance on text-based evidence, andoversimplification of scientific reasoning. To address these gaps, we introduceSCITAB, a novel dataset comprising 1,225 challenging scientific claimsrequiring compositional reasoning with scientific tables. The claims in SCITABare derived from the actual scientific statements, and the evidence ispresented as tables, closely mirroring real-world fact-checking scenarios. Weestablish benchmarks on SCITAB using state-of-the-art models, revealing itsinherent difficulty and highlighting limitations in existing prompting methods.Our error analysis identifies unique challenges, including ambiguousexpressions and irrelevant claims, suggesting future research directions. Thecode and the data are publicly available athttps://github.com/XinyuanLu00/SciTab.</description><author>Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, Min-Yen Kan</author><pubDate>Mon, 22 May 2023 17:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13186v1</guid></item><item><title>Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice</title><link>http://arxiv.org/abs/2305.13185v1</link><description>Mirror descent value iteration (MDVI), an abstraction of Kullback-Leibler(KL) and entropy-regularized reinforcement learning (RL), has served as thebasis for recent high-performing practical RL algorithms. However, despite theuse of function approximation in practice, the theoretical understanding ofMDVI has been limited to tabular Markov decision processes (MDPs). We studyMDVI with linear function approximation through its sample complexity requiredto identify an $\varepsilon$-optimal policy with probability $1-\delta$ underthe settings of an infinite-horizon linear MDP, generative model, and G-optimaldesign. We demonstrate that least-squares regression weighted by the varianceof an estimated optimal value function of the next state is crucial toachieving minimax optimality. Based on this observation, we presentVariance-Weighted Least-Squares MDVI (VWLS-MDVI), the first theoreticalalgorithm that achieves nearly minimax optimal sample complexity forinfinite-horizon linear MDPs. Furthermore, we propose a practical VWLSalgorithm for value-based deep RL, Deep Variance Weighting (DVW). Ourexperiments demonstrate that DVW improves the performance of popularvalue-based deep RL algorithms on a set of MinAtar benchmarks.</description><author>Toshinori Kitamura, Tadashi Kozuno, Yunhao Tang, Nino Vieillard, Michal Valko, Wenhao Yang, Jincheng Mei, Pierre Ménard, Mohammad Gheshlaghi Azar, Rémi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesvári, Wataru Kumagai, Yutaka Matsuo</author><pubDate>Mon, 22 May 2023 17:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13185v1</guid></item><item><title>Teaching Probabilistic Logical Reasoning to Transformers</title><link>http://arxiv.org/abs/2305.13179v1</link><description>Recent research on transformer-based language models investigates theirreasoning ability over logical rules expressed in natural language text.However, their logic is not yet well-understood as we cannot explain theabstractions made by the models that help them in reasoning. These models arecriticized for merely memorizing complex patterns in the data, which oftencreates issues for their generalizability in unobserved situations. In thiswork, we analyze the use of probabilistic logical rules in transformer-basedlanguage models. In particular, we propose a new approach, ProbabilisticConstraint Training (PCT), that explicitly models probabilistic logicalreasoning by imposing the rules of reasoning as constraints during training. Wecreate a new QA benchmark for evaluating probabilistic reasoning over uncertaintextual rules, which creates instance-specific rules, unlike the only existingrelevant benchmark. Experimental results show that our proposed techniqueimproves the base language models' accuracy and explainability whenprobabilistic logical reasoning is required for question answering. Moreover,we show that the learned probabilistic reasoning abilities are transferable tonovel situations.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Mon, 22 May 2023 17:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13179v1</guid></item><item><title>Equilibrium and Learning in Fixed-Price Data Markets with Externality</title><link>http://arxiv.org/abs/2302.08012v2</link><description>We propose modeling real-world data markets, where sellers post fixed pricesand buyers are free to purchase from any set of sellers, as a simultaneous-movegame between the buyers. A key component of this model is the negativeexternality buyers induce on one another due to purchasing data with acompetitive advantage, a phenomenon exacerbated by data's easy replicability.We consider two settings. In the simpler complete-information setting, whereall buyers know their valuations, we characterize both the existence andwelfare properties of the pure-strategy Nash equilibrium in the presence ofbuyer externality. While this picture is bleak without any market intervention,reinforcing the limitations of current data markets, we prove that for astandard class of externality functions, market intervention in the form of atransaction cost can lead to a pure-strategy equilibrium with strong welfareguarantees. We next consider a more general setting where buyers start withunknown valuations and learn them over time through repeated data purchases.Our intervention is feasible in this regime as well, and we provide a learningalgorithm for buyers in this online scenario that under some naturalassumptions, achieves low regret with respect to both individual and cumulativeutility metrics. Lastly, we analyze the promise and shortfalls of thisintervention under a much richer model of externality. Our work paves the wayfor investigating simple interventions for existing data markets to addresstheir shortcoming and the unique challenges put forth by data products.</description><author>Yiling Chen, Safwan Hossain</author><pubDate>Mon, 22 May 2023 17:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08012v2</guid></item><item><title>Discovering Universal Geometry in Embeddings with ICA</title><link>http://arxiv.org/abs/2305.13175v1</link><description>This study employs Independent Component Analysis (ICA) to uncover universalproperties of embeddings of words or images. Our approach extracts independentsemantic components of embeddings, enabling each embedding to be represented asa composition of intrinsic interpretable axes. We demonstrate that embeddingscan be expressed as a combination of a few axes and that these semantic axesare consistent across different languages, modalities, and embeddingalgorithms. This discovery of universal properties in embeddings contributes tomodel interpretability, potentially facilitating the development of highlyinterpretable models and the compression of large-scale models.</description><author>Hiroaki Yamagiwa, Momose Oyama, Hidetoshi Shimodaira</author><pubDate>Mon, 22 May 2023 17:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13175v1</guid></item><item><title>Recovery Bounds on Class-Based Optimal Transport: A Sum-of-Norms Regularization Framework</title><link>http://arxiv.org/abs/1903.03850v3</link><description>We develop a novel theoretical framework for understating OT schemesrespecting a class structure. For this purpose, we propose a convex OT programwith a sum-of-norms regularization term, which provably recovers the underlyingclass structure under geometric assumptions. Furthermore, we derive anaccelerated proximal algorithm with a closed-form projection and proximaloperator scheme, thereby affording a more scalable algorithm for computingoptimal transport plans. We provide a novel argument for the uniqueness of theoptimum even in the absence of strong convexity. Our experiments show that thenew regularizer not only results in a better preservation of the classstructure in the data but also yields additional robustness to the datageometry, compared to previous regularizers.</description><author>Arman Rahbar, Ashkan Panahi, Morteza Haghir Chehreghani, Devdatt Dubhashi, Hamid Krim</author><pubDate>Mon, 22 May 2023 17:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1903.03850v3</guid></item><item><title>Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation</title><link>http://arxiv.org/abs/2305.13173v1</link><description>Zero-shot instance segmentation aims to detect and precisely segment objectsof unseen categories without any training samples. Since the model is trainedon seen categories, there is a strong bias that the model tends to classify allthe objects into seen categories. Besides, there is a natural confusion betweenbackground and novel objects that have never shown up in training. These twochallenges make novel objects hard to be raised in the final instancesegmentation results. It is desired to rescue novel objects from background anddominated seen categories. To this end, we propose D$^2$Zero withSemantic-Promoted Debiasing and Background Disambiguation to enhance theperformance of Zero-shot instance segmentation. Semantic-promoted debiasingutilizes inter-class semantic relationships to involve unseen categories invisual feature training and learns an input-conditional classifier to conductdynamical classification based on the input image. Background disambiguationproduces image-adaptive background representation to avoid mistaking novelobjects for background. Extensive experiments show that we significantlyoutperform previous state-of-the-art methods by a large margin, e.g., 16.86%improvement on COCO. Project page: https://henghuiding.github.io/D2Zero/</description><author>Shuting He, Henghui Ding, Wei Jiang</author><pubDate>Mon, 22 May 2023 17:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13173v1</guid></item><item><title>Editing Large Language Models: Problems, Methods, and Opportunities</title><link>http://arxiv.org/abs/2305.13172v1</link><description>Recent advancements in deep learning have precipitated the emergence of largelanguage models (LLMs) which exhibit an impressive aptitude for understandingand producing text akin to human language. Despite the ability to train highlycapable LLMs, the methodology for maintaining their relevancy and rectifyingerrors remains elusive. To that end, the past few years have witnessed a surgein techniques for editing LLMs, the objective of which is to alter the behaviorof LLMs within a specific domain without negatively impacting performanceacross other inputs. This paper embarks on a deep exploration of the problems,methods, and opportunities relating to model editing for LLMs. In particular,we provide an exhaustive overview of the task definition and challengesassociated with model editing, along with an in-depth empirical analysis of themost progressive methods currently at our disposal. We also build a newbenchmark dataset to facilitate a more robust evaluation and pinpoint enduringissues intrinsic to existing techniques. Our objective is to provide valuableinsights into the effectiveness and feasibility of each model editingtechnique, thereby assisting the research community in making informeddecisions when choosing the most appropriate method for a specific task orcontext. Code and datasets will be available athttps://github.com/zjunlp/EasyEdit.</description><author>Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 22 May 2023 17:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13172v1</guid></item><item><title>Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning</title><link>http://arxiv.org/abs/2305.13170v1</link><description>Federated Learning is an evolving machine learning paradigm, in whichmultiple clients perform computations based on their individual private data,interspersed by communication with a remote server. A common strategy tocurtail communication costs is Local Training, which consists in performingmultiple local stochastic gradient descent steps between successivecommunication rounds. However, the conventional approach to local trainingoverlooks the practical necessity for client-specific personalization, atechnique to tailor local models to individual needs. We introduce Scafflix, anovel algorithm that efficiently integrates explicit personalization with localtraining. This innovative approach benefits from these two techniques, therebyachieving doubly accelerated communication, as we demonstrate both in theoryand practice.</description><author>Kai Yi, Laurent Condat, Peter Richtárik</author><pubDate>Mon, 22 May 2023 16:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13170v1</guid></item><item><title>A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity</title><link>http://arxiv.org/abs/2305.13169v1</link><description>Pretraining is the preliminary and fundamental step in developing capablelanguage models (LM). Despite this, pretraining data design is criticallyunder-documented and often guided by empirically unsupported intuitions. Toaddress this, we pretrain 28 1.5B parameter decoder-only models, training ondata curated (1) at different times, (2) with varying toxicity and qualityfilters, and (3) with different domain compositions. First, we quantify theeffect of pretraining data age. A temporal shift between evaluation data andpretraining data leads to performance degradation, which is not overcome byfinetuning. Second, we explore the effect of quality and toxicity filters,showing a trade-off between performance on standard benchmarks and risk oftoxic generations. Our findings indicate there does not exist aone-size-fits-all solution to filtering training data. We also find that theeffects of different types of filtering are not predictable from text domaincharacteristics. Lastly, we empirically validate that the inclusion ofheterogeneous data sources, like books and web, is broadly beneficial andwarrants greater prioritization. These findings constitute the largest set ofexperiments to validate, quantify, and expose many undocumented intuitionsabout text pretraining, which we hope will help support more informeddata-centric decisions in LM development.</description><author>Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito</author><pubDate>Mon, 22 May 2023 16:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13169v1</guid></item><item><title>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities</title><link>http://arxiv.org/abs/2305.13168v1</link><description>This paper presents an exhaustive quantitative and qualitative evaluation ofLarge Language Models (LLMs) for Knowledge Graph (KG) construction andreasoning. We employ eight distinct datasets that encompass aspects includingentity, relation and event extraction, link prediction, and question answering.Empirically, our findings suggest that GPT-4 outperforms ChatGPT in themajority of tasks and even surpasses fine-tuned models in certain reasoning andquestion-answering datasets. Moreover, our investigation extends to thepotential generalization ability of LLMs for information extraction, whichculminates in the presentation of the Virtual Knowledge Extraction task and thedevelopment of the VINE dataset. Drawing on these empirical findings, wefurther propose AutoKG, a multi-agent-based approach employing LLMs for KGconstruction and reasoning, which aims to chart the future of this field andoffer exciting opportunities for advancement. We anticipate that our researchcan provide invaluable insights for future undertakings of KG\footnote{Code anddatasets will be available in https://github.com/zjunlp/AutoKG.</description><author>Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 22 May 2023 16:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13168v1</guid></item><item><title>Are Emergent Abilities of Large Language Models a Mirage?</title><link>http://arxiv.org/abs/2304.15004v2</link><description>Recent work claims that large language models display emergent abilities,abilities not present in smaller-scale models that are present in larger-scalemodels. What makes emergent abilities intriguing is two-fold: their sharpness,transitioning seemingly instantaneously from not present to present, and theirunpredictability, appearing at seemingly unforeseeable model scales. Here, wepresent an alternative explanation for emergent abilities: that for aparticular task and model family, when analyzing fixed model outputs, emergentabilities appear due to the researcher's choice of metric rather than due tofundamental changes in model behavior with scale. Specifically, nonlinear ordiscontinuous metrics produce apparent emergent abilities, whereas linear orcontinuous metrics produce smooth, continuous predictable changes in modelperformance. We present our alternative explanation in a simple mathematicalmodel, then test it in three complementary ways: we (1) make, test and confirmthree predictions on the effect of metric choice using the InstructGPT/GPT-3family on tasks with claimed emergent abilities; (2) make, test and confirm twopredictions about metric choices in a meta-analysis of emergent abilities onBIG-Bench; and (3) show to choose metrics to produce never-before-seenseemingly emergent abilities in multiple vision tasks across diverse deepnetworks. Via all three analyses, we provide evidence that alleged emergentabilities evaporate with different metrics or with better statistics, and maynot be a fundamental property of scaling AI models.</description><author>Rylan Schaeffer, Brando Miranda, Sanmi Koyejo</author><pubDate>Mon, 22 May 2023 16:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.15004v2</guid></item><item><title>VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending</title><link>http://arxiv.org/abs/2305.13167v1</link><description>Large-scale image-text contrastive pre-training models, such as CLIP, havebeen demonstrated to effectively learn high-quality multimodal representations.However, there is limited research on learning video-text representations forgeneral video multimodal tasks based on these powerful features. Towards thisgoal, we propose a novel video-text pre-training method dubbed VLAB: VideoLanguage pre-training by feature Adapting and Blending, which transfers CLIPrepresentations to video pre-training tasks and develops unified videomultimodal models for a wide range of video-text tasks. Specifically, VLAB isfounded on two key strategies: feature adapting and feature blending. In theformer, we introduce a new video adapter module to address CLIP's deficiency inmodeling temporal information and extend the model's capability to encompassboth contrastive and generative tasks. In the latter, we propose an end-to-endtraining method that further enhances the model's performance by exploiting thecomplementarity of image and video features. We validate the effectiveness andversatility of VLAB through extensive experiments on highly competitive videomultimodal tasks, including video text retrieval, video captioning, and videoquestion answering. Remarkably, VLAB outperforms competing methodssignificantly and sets new records in video question answering on MSRVTT, MSVD,and TGIF datasets. It achieves an accuracy of 49.6, 61.0, and 79.0,respectively. Codes and models will be released.</description><author>Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing Liu, Jiashi Feng</author><pubDate>Mon, 22 May 2023 16:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13167v1</guid></item><item><title>Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model</title><link>http://arxiv.org/abs/2305.13165v1</link><description>Neural collapse (NC) refers to the surprising structure of the last layer ofdeep neural networks in the terminal phase of gradient descent training.Recently, an increasing amount of experimental evidence has pointed to thepropagation of NC to earlier layers of neural networks. However, while the NCin the last layer is well studied theoretically, much less is known about itsmulti-layered counterpart - deep neural collapse (DNC). In particular, existingwork focuses either on linear layers or only on the last two layers at theprice of an extra assumption. Our paper fills this gap by generalizing theestablished analytical framework for NC - the unconstrained features model - tomultiple non-linear layers. Our key technical contribution is to show that, ina deep unconstrained features model, the unique global optimum for binaryclassification exhibits all the properties typical of DNC. This explains theexisting experimental evidence of DNC. We also empirically show that (i) byoptimizing deep unconstrained features models via gradient descent, theresulting solution agrees well with our theory, and (ii) trained networksrecover the unconstrained features suitable for the occurrence of DNC, thussupporting the validity of this modeling principle.</description><author>Peter Súkeník, Marco Mondelli, Christoph Lampert</author><pubDate>Mon, 22 May 2023 16:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13165v1</guid></item><item><title>INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search</title><link>http://arxiv.org/abs/2305.13164v1</link><description>Logic synthesis is the first and most vital step in chip design. This stepsconverts a chip specification written in a hardware description language (suchas Verilog) into an optimized implementation using Boolean logic gates.State-of-the-art logic synthesis algorithms have a large number of logicminimization heuristics, typically applied sequentially based on humanexperience and intuition. The choice of the order greatly impacts the quality(e.g., area and delay) of the synthesized circuit. In this paper, we proposeINVICTUS, a model-based offline reinforcement learning (RL) solution thatautomatically generates a sequence of logic minimization heuristics ("synthesisrecipe") based on a training dataset of previously seen designs. A keychallenge is that new designs can range from being very similar to past designs(e.g., adders and multipliers) to being completely novel (e.g., new processorinstructions). %Compared to prior work, INVICTUS is the first solution thatuses a mix of RL and search methods joint with an online out-of-distributiondetector to generate synthesis recipes over a wide range of benchmarks. Ourresults demonstrate significant improvement in area-delay product (ADP) ofsynthesized circuits with up to 30\% improvement over state-of-the-arttechniques. Moreover, INVICTUS achieves up to $6.3\times$ runtime reduction(iso-ADP) compared to the state-of-the-art.</description><author>Animesh Basak Chowdhury, Marco Romanelli, Benjamin Tan, Ramesh Karri, Siddharth Garg</author><pubDate>Mon, 22 May 2023 16:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13164v1</guid></item><item><title>Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning</title><link>http://arxiv.org/abs/2211.01576v2</link><description>We present a learning-enabled Task and Motion Planning (TAMP) algorithm forsolving mobile manipulation problems in environments with many articulated andmovable obstacles. Our idea is to bias the search procedure of a traditionalTAMP planner with a learned plan feasibility predictor. The core of ouralgorithm is PIGINet, a novel Transformer-based learning method that takes in atask plan, the goal, and the initial state, and predicts the probability offinding motion trajectories associated with the task plan. We integrate PIGINetwithin a TAMP planner that generates a diverse set of high-level task plans,sorts them by their predicted likelihood of feasibility, and refines them inthat order. We evaluate the runtime of our TAMP algorithm on seven families ofkitchen rearrangement problems, comparing its performance to that ofnon-learning baselines. Our experiments show that PIGINet substantiallyimproves planning efficiency, cutting down runtime by 80% on problems withsmall state spaces and 10%-50% on larger ones, after being trained on only150-600 problems. Finally, it also achieves zero-shot generalization toproblems with unseen object categories thanks to its visual encoding ofobjects. Project page https://piginet.github.io/.</description><author>Zhutian Yang, Caelan Reed Garrett, Tomás Lozano-Pérez, Leslie Kaelbling, Dieter Fox</author><pubDate>Mon, 22 May 2023 16:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01576v2</guid></item><item><title>Can ChatGPT Defend the Truth? Automatic Dialectical Evaluation Elicits LLMs' Deficiencies in Reasoning</title><link>http://arxiv.org/abs/2305.13160v1</link><description>We explore testing the reasoning ability of large language models (LLMs),such as ChatGPT, by engaging with them in a debate-like conversation thatprobes deeper into their understanding of the subject. Specifically, weformulate a new task where given a question, the LLM can generate a correctsolution while the user believes in a wrong solution in the beginning, and theyneed to discuss to make the correct decision through dialogue. Such a settingrequires the LLM to not only achieve the correct answer on its own (which couldbe done by shallow memorization), but also be able to defend the truth insteadof blindly believing or getting misled by the user's (invalid) arguments andcritiques, thus testing in greater depth whether the LLM grasps the essence ofthe reasoning required to solve the problem. To automate this evaluationframework and save human labor, we simulate the user using another LLMconditioned on a synthesized wrong solution. Across a range of complexreasoning benchmarks spanning math, commonsense, logic and tasks fromBIG-Bench, we find that despite being able to generate correct step-by-stepsolutions in the beginning, ChatGPT cannot maintain its belief in truth for asignificant portion of examples when challenged by often-time absurdly invalidarguments. Our work reveals LLMs' weaknesses not captured by conventionalbenchmarking, and also points to danger zones of aligning models with humanfeedback.</description><author>Boshi Wang, Xiang Yue, Huan Sun</author><pubDate>Mon, 22 May 2023 16:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13160v1</guid></item><item><title>Text Classification via Large Language Models</title><link>http://arxiv.org/abs/2305.08377v2</link><description>Despite the remarkable success of large-scale Language Models (LLMs) such asGPT-3, their performances still significantly underperform fine-tuned models inthe task of text classification. This is due to (1) the lack of reasoningability in addressing complex linguistic phenomena (e.g., intensification,contrast, irony etc); (2) limited number of tokens allowed in in-contextlearning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adoptsa progressive reasoning strategy tailored to addressing the complex linguisticphenomena involved in text classification: CARP first prompts LLMs to findsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),based on which a diagnostic reasoning process is induced for final decisions.To further address the limited-token issue, CARP uses a fine-tuned model on thesupervised dataset for $k$NN demonstration search in the in-context learning,allowing the model to take the advantage of both LLM's generalization abilityand the task-specific evidence provided by the full labeled dataset.Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-usedtext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) onAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performancecomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARPdelivers impressive abilities on low-resource and domain-adaptation setups.Specifically, using 16 examples per class, CARP achieves comparableperformances to supervised models with 1,024 examples per class.</description><author>Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, Guoyin Wang</author><pubDate>Mon, 22 May 2023 16:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08377v2</guid></item><item><title>Effective Bilevel Optimization via Minimax Reformulation</title><link>http://arxiv.org/abs/2305.13153v1</link><description>Bilevel optimization has found successful applications in various machinelearning problems, including hyper-parameter optimization, data cleaning, andmeta-learning. However, its huge computational cost presents a significantchallenge for its utilization in large-scale problems. This challenge arisesdue to the nested structure of the bilevel formulation, where eachhyper-gradient computation necessitates a costly inner optimization procedure.To address this issue, we propose a reformulation of bilevel optimization as aminimax problem, effectively decoupling the outer-inner dependency. Under mildconditions, we show these two problems are equivalent. Furthermore, weintroduce a multi-stage gradient descent and ascent (GDA) algorithm to solvethe resulting minimax problem with convergence guarantees. Extensiveexperimental results demonstrate that our method outperforms state-of-the-artbilevel methods while significantly reducing the computational cost.</description><author>Xiaoyu Wang, Rui Pan, Renjie Pi, Tong Zhang</author><pubDate>Mon, 22 May 2023 16:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13153v1</guid></item><item><title>And/or trade-off in artificial neurons: impact on adversarial robustness</title><link>http://arxiv.org/abs/2102.07389v3</link><description>Despite the success of neural networks, the issue of classificationrobustness remains, particularly highlighted by adversarial examples. In thispaper, we address this challenge by focusing on the continuum of functionsimplemented in artificial neurons, ranging from pure AND gates to pure ORgates. Our hypothesis is that the presence of a sufficient number of OR-likeneurons in a network can lead to classification brittleness and increasedvulnerability to adversarial attacks. We define AND-like neurons and proposemeasures to increase their proportion in the network. These measures involverescaling inputs to the [-1,1] interval and reducing the number of points inthe steepest section of the sigmoidal activation function. A crucial componentof our method is the comparison between a neuron's output distribution when fedwith the actual dataset and a randomised version called the "scrambleddataset." Experimental results on the MNIST dataset suggest that our approachholds promise as a direction for further exploration.</description><author>Alessandro Fontana</author><pubDate>Mon, 22 May 2023 16:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.07389v3</guid></item></channel></rss>