<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 06 May 2024 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Vibe-Eval: A hard evaluation suite for measuring progress of multimodal language models</title><link>http://arxiv.org/abs/2405.02287v1</link><description>We introduce Vibe-Eval: a new open benchmark and framework for evaluatingmultimodal chat models. Vibe-Eval consists of 269 visual understanding prompts,including 100 of hard difficulty, complete with gold-standard responsesauthored by experts. Vibe-Eval is open-ended and challenging with dualobjectives: (i) vibe checking multimodal chat models for day-to-day tasks and(ii) rigorously testing and probing the capabilities of present frontiermodels. Notably, our hard set contains &gt;50% questions that all frontier modelsanswer incorrectly. We explore the nuances of designing, evaluating, andranking models on ultra challenging prompts. We also discuss trade-offs betweenhuman and automatic evaluation, and show that automatic model evaluation usingReka Core roughly correlates to human judgment. We offer free API access forthe purpose of lightweight evaluation and plan to conduct formal humanevaluations for public models that perform well on the Vibe-Eval's automaticscores. We release the evaluation code and data, seehttps://github.com/reka-ai/reka-vibe-eval</description><author>Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Mikel Artetxe, Yi Tay</author><pubDate>Fri, 03 May 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02287v1</guid></item><item><title>DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos</title><link>http://arxiv.org/abs/2405.02280v1</link><description>Existing VLMs can track in-the-wild 2D video objects while current generativemodels provide powerful visual priors for synthesizing novel views for thehighly under-constrained 2D-to-3D object lifting. Building upon this excitingprogress, we present DreamScene4D, the first approach that can generatethree-dimensional dynamic scenes of multiple objects from monocular in-the-wildvideos with large object motion across occlusions and novel viewpoints. Our keyinsight is to design a "decompose-then-recompose" scheme to factorize both thewhole video scene and each object's 3D motion. We first decompose the videoscene by using open-vocabulary mask trackers and an adapted image diffusionmodel to segment, track, and amodally complete the objects and background inthe video. Each object track is mapped to a set of 3D Gaussians that deform andmove in space and time. We also factorize the observed motion into multiplecomponents to handle fast motion. The camera motion can be inferred byre-rendering the background to match the video frames. For the object motion,we first model the object-centric deformation of the objects by leveragingrendering losses and multi-view generative priors in an object-centric frame,then optimize object-centric to world-frame transformations by comparing therendered outputs against the perceived pixel and optical flow. Finally, werecompose the background and objects and optimize for relative object scalesusing monocular depth prediction guidance. We show extensive results on thechallenging DAVIS, Kubric, and self-captured videos, detail some limitations,and provide future directions. Besides 4D scene generation, our results showthat DreamScene4D enables accurate 2D point motion tracking by projecting theinferred 3D trajectories to 2D, while never explicitly trained to do so.</description><author>Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki</author><pubDate>Fri, 03 May 2024 18:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02280v1</guid></item><item><title>A Careful Examination of Large Language Model Performance on Grade School Arithmetic</title><link>http://arxiv.org/abs/2405.00332v3</link><description>Large language models (LLMs) have achieved impressive success on manybenchmarks for mathematical reasoning. However, there is growing concern thatsome of this performance actually reflects dataset contamination, where dataclosely resembling benchmark questions leaks into the training data, instead oftrue reasoning ability. To investigate this claim rigorously, we commissionGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style andcomplexity of the established GSM8k benchmark, the gold standard for measuringelementary mathematical reasoning. We ensure that the two benchmarks arecomparable across important metrics such as human solve rates, number of stepsin solution, answer magnitude, and more. When evaluating leading open- andclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, withseveral families of models (e.g., Phi and Mistral) showing evidence ofsystematic overfitting across almost all model sizes. At the same time, manymodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) showminimal signs of overfitting. Further analysis suggests a positive relationship(Spearman's r^2=0.32) between a model's probability of generating an examplefrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting thatmany models may have partially memorized GSM8k.</description><author>Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue</author><pubDate>Fri, 03 May 2024 18:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00332v3</guid></item><item><title>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</title><link>http://arxiv.org/abs/2309.12284v4</link><description>Large language models (LLMs) have pushed the limits of natural languageunderstanding and exhibited excellent problem-solving ability. Despite thegreat success, most existing open-source LLMs (e.g., LLaMA-2) are still faraway from satisfactory for solving mathematical problem due to the complexreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tunedlanguage model that specializes in mathematical reasoning. Specifically, westart by bootstrapping mathematical questions by rewriting the question frommultiple perspectives without extra knowledge, which results in a new datasetcalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.Experimental results on two popular benchmarks (i.e., GSM8K and MATH) formathematical reasoning demonstrate that MetaMath outperforms a suite ofopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the samesize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all theMetaMathQA dataset, the MetaMath models with different model sizes and thetraining code for public use.</description><author>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu</author><pubDate>Fri, 03 May 2024 18:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12284v4</guid></item><item><title>InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification</title><link>http://arxiv.org/abs/2109.07319v4</link><description>Automatic annotation of short-text data to a large number of target labels,referred to as Short Text Extreme Classification, has found numerousapplications including prediction of related searches and productrecommendation. In this paper, we propose a convolutional architectureInceptionXML which is light-weight, yet powerful, and robust to the inherentlack of word-order in short-text queries encountered in search andrecommendation. We demonstrate the efficacy of applying convolutions byrecasting the operation along the embedding dimension instead of the worddimension as applied in conventional CNNs for text classification. Towardsscaling our model to datasets with millions of labels, we also propose SyncXMLpipeline which improves upon the shortcomings of the recently proposed dynamichard-negative mining technique for label short-listing by synchronizing thelabel-shortlister and extreme classifier. SyncXML not only reduces theinference time to half but is also an order of magnitude smaller thanstate-of-the-art Astec in terms of model size. Through a comprehensiveempirical comparison, we show that not only can InceptionXML outperformexisting approaches on benchmark datasets but also the transformer baselinesrequiring only 2% FLOPs. The code for InceptionXML is available athttps://github.com/xmc-aalto/inceptionxml.</description><author>Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, Rohit Babbar</author><pubDate>Fri, 03 May 2024 18:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.07319v4</guid></item><item><title>Structural Pruning of Pre-trained Language Models via Neural Architecture Search</title><link>http://arxiv.org/abs/2405.02267v1</link><description>Pre-trained language models (PLM), for example BERT or RoBERTa, mark thestate-of-the-art for natural language understanding task when fine-tuned onlabeled data. However, their large size poses challenges in deploying them forinference in real-world applications, due to significant GPU memoryrequirements and high inference latency. This paper explores neuralarchitecture search (NAS) for structural pruning to find sub-parts of thefine-tuned network that optimally trade-off efficiency, for example in terms ofmodel size or latency, and generalization performance. We also show how we canutilize more recently developed two-stage weight-sharing NAS approaches in thissetting to accelerate the search process. Unlike traditional pruning methodswith fixed thresholds, we propose to adopt a multi-objective approach thatidentifies the Pareto optimal set of sub-networks, allowing for a more flexibleand automated compression process.</description><author>Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau</author><pubDate>Fri, 03 May 2024 18:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02267v1</guid></item><item><title>On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?</title><link>http://arxiv.org/abs/2405.02266v1</link><description>The development of large vision-language models, notably CLIP, has catalyzedresearch into effective adaptation techniques, with a particular focus on softprompt tuning. Conjointly, test-time augmentation, which utilizes multipleaugmented views of a single image to enhance zero-shot generalization, isemerging as a significant area of interest. This has predominantly directedresearch efforts toward test-time prompt tuning. In contrast, we introduce arobust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-basedmethods without requiring this intensive training procedure. This positions MTAas an ideal solution for both standalone and API-based applications.Additionally, our method does not rely on ad hoc rules (e.g., confidencethreshold) used in some previous test-time augmentation techniques to filterthe augmented views. Instead, MTA incorporates a quality assessment variablefor each view directly into its optimization process, termed as the inliernessscore. This score is jointly optimized with a density mode seeking process,leading to an efficient training- and hyperparameter-free approach. Weextensively benchmark our method on 15 datasets and demonstrate MTA'ssuperiority and computational efficiency. Deployed easily as plug-and-playmodule on top of zero-shot models and state-of-the-art few-shot methods, MTAshows systematic and consistent improvements.</description><author>Maxime Zanella, Ismail Ben Ayed</author><pubDate>Fri, 03 May 2024 18:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02266v1</guid></item><item><title>Constrained Reinforcement Learning Under Model Mismatch</title><link>http://arxiv.org/abs/2405.01327v2</link><description>Existing studies on constrained reinforcement learning (RL) may obtain awell-performing policy in the training environment. However, when deployed in areal environment, it may easily violate constraints that were originallysatisfied during training because there might be model mismatch between thetraining and real environments. To address the above challenge, we formulatethe problem as constrained RL under model uncertainty, where the goal is tolearn a good policy that optimizes the reward and at the same time satisfy theconstraint under model mismatch. We develop a Robust Constrained PolicyOptimization (RCPO) algorithm, which is the first algorithm that applies tolarge/continuous state space and has theoretical guarantees on worst-casereward improvement and constraint violation at each iteration during thetraining. We demonstrate the effectiveness of our algorithm on a set of RLtasks with constraints.</description><author>Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou</author><pubDate>Fri, 03 May 2024 18:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01327v2</guid></item><item><title>Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network</title><link>http://arxiv.org/abs/2402.11319v3</link><description>Flexible continuum manipulators are valued for minimally invasive surgery,offering access to confined spaces through nonlinear paths. However,cable-driven manipulators face control difficulties due to hysteresis fromcabling effects such as friction, elongation, and coupling. These effects aredifficult to model due to nonlinearity and the difficulties become even moreevident when dealing with long and coupled, multi-segmented manipulator. Thispaper proposes a data-driven approach based on Deep Neural Networks (DNN) tocapture these nonlinear and previous states-dependent characteristics of cableactuation. We collect physical joint configurations according to command jointconfigurations using RGBD sensing and 7 fiducial markers to model thehysteresis of the proposed manipulator. Result on a study comparing theestimation performance of four DNN models show that the Temporal ConvolutionNetwork (TCN) demonstrates the highest predictive capability. Leveragingtrained TCNs, we build a control algorithm to compensate for hysteresis.Tracking tests in task space using unseen trajectories show that the proposedcontrol algorithm reduces the average position and orientation error by 61.39%(from 13.7mm to 5.29 mm) and 64.04% (from 31.17{\deg} to 11.21{\deg}),respectively. This result implies that the proposed calibrated controllereffectively reaches the desired configurations by estimating the hysteresis ofthe manipulator. Applying this method in real surgical scenarios has thepotential to enhance control precision and improve surgical performance.</description><author>Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang</author><pubDate>Fri, 03 May 2024 18:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11319v3</guid></item><item><title>BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</title><link>http://arxiv.org/abs/2310.04420v3</link><description>Understanding the functional organization of higher visual cortex is acentral focus in neuroscience. Past studies have primarily mapped the visualand semantic selectivity of neural populations using hand-selected stimuli,which may potentially bias results towards pre-existing hypotheses of visualcortex functionality. Moving beyond conventional approaches, we introduce adata-driven method that generates natural language descriptions for imagespredicted to maximally activate individual voxels of interest. Our method --Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon therich embedding space learned by a contrastive vision-language model andutilizes a pre-trained large language model to generate interpretable captions.We validate our method through fine-grained voxel-level captioning acrosshigher-order visual regions. We further perform text-conditioned imagesynthesis with the captions, and show that our images are semantically coherentand yield high predicted activations. Finally, to demonstrate how our methodenables scientific discovery, we perform exploratory investigations on thedistribution of "person" representations in the brain, and discoverfine-grained semantic selectivity in body-selective areas. Unlike earlierstudies that decode text, our method derives voxel-wise captions of semanticselectivity. Our results show that BrainSCUBA is a promising means forunderstanding functional preferences in the brain, and provides motivation forfurther hypothesis-driven investigation of visual cortex.</description><author>Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe</author><pubDate>Fri, 03 May 2024 18:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04420v3</guid></item><item><title>Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</title><link>http://arxiv.org/abs/2310.07626v2</link><description>Satellite-based remote sensing missions have revolutionized our understandingof the Ocean state and dynamics. Among them, space-borne altimetry providesvaluable measurements of Sea Surface Height (SSH), which is used to estimatesurface geostrophic currents. Due to the sensor technology employed, importantgaps occur in SSH observations. Complete SSH maps are produced using linearOptimal Interpolations (OI) such as the widely-used Data Unification andAltimeter Combination System (duacs). On the other hand, Sea SurfaceTemperature (SST) products have much higher data coverage and SST is physicallylinked to geostrophic currents through advection. We propose a newmulti-variate Observing System Simulation Experiment (OSSE) emulating 20 yearsof SSH and SST satellite observations. We train an Attention-BasedEncoder-Decoder deep learning network (abed) on this data, comparing twosettings: one with access to ground truth during training and one without. Onour OSSE, we compare abed reconstructions when trained using either supervisedor unsupervised loss functions, with or without SST information. We evaluatethe SSH interpolations in terms of eddy detection. We also introduce a new wayto transfer the learning from simulation to observations by doing a supervisedpre-training on our OSSE followed by an unsupervised fine-tuning on satellitedata. On real SSH observations from the Ocean Data Challenge 2021, we find thatthis learning strategy combined with the use of SST leads to a decrease of 24%of the root mean squared error compared to duacs.</description><author>Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria</author><pubDate>Fri, 03 May 2024 18:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07626v2</guid></item><item><title>What matters when building vision-language models?</title><link>http://arxiv.org/abs/2405.02246v1</link><description>The growing interest in vision-language models (VLMs) has been driven byimprovements in large language models and vision transformers. Despite theabundance of literature on this subject, we observe that critical decisionsregarding the design of VLMs are often not justified. We argue that theseunsupported decisions impede progress in the field by making it difficult toidentify which choices improve model performance. To address this issue, weconduct extensive experiments around pre-trained models, architecture choice,data, and training methods. Our consolidation of findings includes thedevelopment of Idefics2, an efficient foundational VLM of 8 billion parameters.Idefics2 achieves state-of-the-art performance within its size category acrossvarious multimodal benchmarks, and is often on par with models four times itssize. We release the model (base, instructed, and chat) along with the datasetscreated for its training.</description><author>Hugo Laurençon, Léo Tronchon, Matthieu Cord, Victor Sanh</author><pubDate>Fri, 03 May 2024 18:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02246v1</guid></item><item><title>LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues</title><link>http://arxiv.org/abs/2403.00462v2</link><description>Spurred by recent advances in Large Language Models (LLMs), virtualassistants are poised to take a leap forward in terms of their dialoguecapabilities. Yet a major bottleneck to achieving genuinely transformativetask-oriented dialogue capabilities remains the scarcity of high quality data.Existing datasets, while impressive in scale, have limited domain coverage andcontain few genuinely challenging conversational phenomena; those which arepresent are typically unlabelled, making it difficult to assess the strengthsand weaknesses of models without time-consuming and costly human evaluation.Moreover, creating high quality dialogue data has until now requiredconsiderable human input, limiting both the scale of these datasets and theability to rapidly bootstrap data for a new target domain. We aim to overcomethese issues with LUCID, a modularised and highly automated LLM-driven datageneration system that produces realistic, diverse and challenging dialogues.We use LUCID to generate a seed dataset of 4,277 conversations across 100intents to demonstrate its capabilities, with a human review findingconsistently high quality labels in the generated data.</description><author>Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, Anders Johannsen</author><pubDate>Fri, 03 May 2024 17:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00462v2</guid></item><item><title>Subgraph2vec: A random walk-based algorithm for embedding knowledge graphs</title><link>http://arxiv.org/abs/2405.02240v1</link><description>Graph is an important data representation which occurs naturally in the realworld applications \cite{goyal2018graph}. Therefore, analyzing graphs providesusers with better insights in different areas such as anomaly detection\cite{ma2021comprehensive}, decision making \cite{fan2023graph}, clustering\cite{tsitsulin2023graph}, classification \cite{wang2021mixup} and etc.However, most of these methods require high levels of computational time andspace. We can use other ways like embedding to reduce these costs. Knowledgegraph (KG) embedding is a technique that aims to achieve the vectorrepresentation of a KG. It represents entities and relations of a KG in alow-dimensional space while maintaining the semantic meanings of them. Thereare different methods for embedding graphs including random walk-based methodssuch as node2vec, metapath2vec and regpattern2vec. However, most of thesemethods bias the walks based on a rigid pattern usually hard-coded in thealgorithm. In this work, we introduce \textit{subgraph2vec} for embedding KGswhere walks are run inside a user-defined subgraph. We use this embedding forlink prediction and prove our method has better performance in most cases incomparison with the previous ones.</description><author>Elika Bozorgi, Saber Soleimani, Sakher Khalil Alqaiidi, Hamid Reza Arabnia, Krzysztof Kochut</author><pubDate>Fri, 03 May 2024 17:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02240v1</guid></item><item><title>PSentScore: Evaluating Sentiment Polarity in Dialogue Summarization</title><link>http://arxiv.org/abs/2307.12371v2</link><description>Automatic dialogue summarization is a well-established task with the goal ofdistilling the most crucial information from human conversations into concisetextual summaries. However, most existing research has predominantly focused onsummarizing factual information, neglecting the affective content, which canhold valuable insights for analyzing, monitoring, or facilitating humaninteractions. In this paper, we introduce and assess a set of measuresPSentScore, aimed at quantifying the preservation of affective content indialogue summaries. Our findings indicate that state-of-the-art summarizationmodels do not preserve well the affective content within their summaries.Moreover, we demonstrate that a careful selection of the training set fordialogue samples can lead to improved preservation of affective content in thegenerated summaries, albeit with a minor reduction in content-related metrics.</description><author>Yongxin Zhou, Fabien Ringeval, François Portet</author><pubDate>Fri, 03 May 2024 17:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12371v2</guid></item><item><title>TULIP: Transformer for Upsampling of LiDAR Point Clouds</title><link>http://arxiv.org/abs/2312.06733v4</link><description>LiDAR Upsampling is a challenging task for the perception systems of robotsand autonomous vehicles, due to the sparse and irregular structure oflarge-scale scene contexts. Recent works propose to solve this problem byconverting LiDAR data from 3D Euclidean space into an image super-resolutionproblem in 2D image space. Although their methods can generate high-resolutionrange images with fine-grained details, the resulting 3D point clouds oftenblur out details and predict invalid points. In this paper, we propose TULIP, anew method to reconstruct high-resolution LiDAR point clouds fromlow-resolution LiDAR input. We also follow a range image-based approach butspecifically modify the patch and window geometries of a Swin-Transformer-basednetwork to better fit the characteristics of range images. We conducted severalexperiments on three public real-world and simulated datasets. TULIPoutperforms state-of-the-art methods in all relevant metrics and generatesrobust and more realistic point clouds than prior works.</description><author>Bin Yang, Patrick Pfreundschuh, Roland Siegwart, Marco Hutter, Peyman Moghadam, Vaishakh Patil</author><pubDate>Fri, 03 May 2024 17:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06733v4</guid></item><item><title>SelfVC: Voice Conversion With Iterative Refinement using Self Transformations</title><link>http://arxiv.org/abs/2310.09653v2</link><description>We propose SelfVC, a training strategy to iteratively improve a voiceconversion model with self-synthesized examples. Previous efforts on voiceconversion focus on factorizing speech into explicitly disentangledrepresentations that separately encode speaker characteristics and linguisticcontent. However, disentangling speech representations to capture suchattributes using task-specific loss terms can lead to information loss. In thiswork, instead of explicitly disentangling attributes with loss terms, wepresent a framework to train a controllable voice conversion model on entangledspeech representations derived from self-supervised learning (SSL) and speakerverification models. First, we develop techniques to derive prosodicinformation from the audio signal and SSL representations to train predictivesubmodules in the synthesis model. Next, we propose a training strategy toiteratively improve the synthesis model for voice conversion, by creating achallenging training objective using self-synthesized examples. We demonstratethat incorporating such self-synthesized examples during training improves thespeaker similarity of generated speech as compared to a baseline voiceconversion model trained solely on heuristically perturbed inputs. Ourframework is trained without any text and achieves state-of-the-art results inzero-shot voice conversion on metrics evaluating naturalness, speakersimilarity, and intelligibility of synthesized audio.</description><author>Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley</author><pubDate>Fri, 03 May 2024 17:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09653v2</guid></item><item><title>Learning Optimal Deterministic Policies with Stochastic Policy Gradients</title><link>http://arxiv.org/abs/2405.02235v1</link><description>Policy gradient (PG) methods are successful approaches to deal withcontinuous reinforcement learning (RL) problems. They learn stochasticparametric (hyper)policies by either exploring in the space of actions or inthe space of parameters. Stochastic controllers, however, are often undesirablefrom a practical perspective because of their lack of robustness, safety, andtraceability. In common practice, stochastic (hyper)policies are learned onlyto deploy their deterministic version. In this paper, we make a step towardsthe theoretical understanding of this practice. After introducing a novelframework for modeling this scenario, we study the global convergence to thebest deterministic policy, under (weak) gradient domination assumptions. Then,we illustrate how to tune the exploration level used for learning to optimizethe trade-off between the sample complexity and the performance of the deployeddeterministic policy. Finally, we quantitatively compare action-based andparameter-based exploration, giving a formal guise to intuitive results.</description><author>Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, Matteo Papini</author><pubDate>Fri, 03 May 2024 17:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02235v1</guid></item><item><title>FocusLearn: Fully-Interpretable, High-Performance Modular Neural Networks for Time Series</title><link>http://arxiv.org/abs/2311.16834v4</link><description>Multivariate time series have many applications, from healthcare andmeteorology to life science. Although deep learning models have shown excellentpredictive performance for time series, they have been criticised for being"black-boxes" or non-interpretable. This paper proposes a novel modular neuralnetwork model for multivariate time series prediction that is interpretable byconstruction. A recurrent neural network learns the temporal dependencies inthe data while an attention-based feature selection component selects the mostrelevant features and suppresses redundant features used in the learning of thetemporal dependencies. A modular deep network is trained from the selectedfeatures independently to show the users how features influence outcomes,making the model interpretable. Experimental results show that this approachcan outperform state-of-the-art interpretable Neural Additive Models (NAM) andvariations thereof in both regression and classification of time series tasks,achieving a predictive performance that is comparable to the topnon-interpretable methods for time series, LSTM and XGBoost.</description><author>Qiqi Su, Christos Kloukinas, Artur d'Avila Garcez</author><pubDate>Fri, 03 May 2024 17:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16834v4</guid></item><item><title>REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs</title><link>http://arxiv.org/abs/2405.02228v1</link><description>Automatic citation generation for sentences in a document or report isparamount for intelligence analysts, cybersecurity, news agencies, andeducation personnel. In this research, we investigate whether large languagemodels (LLMs) are capable of generating references based on two forms ofsentence queries: (a) Direct Queries, LLMs are asked to provide author names ofthe given research article, and (b) Indirect Queries, LLMs are asked to providethe title of a mentioned article when given a sentence from a differentarticle. To demonstrate where LLM stands in this task, we introduce a largedataset called REASONS comprising abstracts of the 12 most popular domains ofscientific research on arXiv. From around 20K research articles, we make thefollowing deductions on public and proprietary LLMs: (a) State-of-the-art,often called anthropomorphic GPT-4 and GPT-3.5, suffers from high passpercentage (PP) to minimize the hallucination rate (HR). When tested withPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevantmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmentedgeneration (RAG) using Mistral demonstrates consistent and robust citationsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. TheHR across all domains and models decreased by an average of 41.93% and the PPwas reduced to 0% in most cases. In terms of generation quality, the average F1Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing withadversarial samples showed that LLMs, including the Advance RAG Mistral,struggle to understand context, but the extent of this issue was small inMistral and GPT-4-Preview. Our study con tributes valuable insights into thereliability of RAG for automated citation generation tasks.</description><author>Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur</author><pubDate>Fri, 03 May 2024 17:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02228v1</guid></item><item><title>SATO: Stable Text-to-Motion Framework</title><link>http://arxiv.org/abs/2405.01461v2</link><description>Is the Text to Motion model robust? Recent advancements in Text to Motionmodels primarily stem from more accurate predictions of specific actions.However, the text modality typically relies solely on pre-trained ContrastiveLanguage-Image Pretraining (CLIP) models. Our research has uncovered asignificant issue with the text-to-motion model: its predictions often exhibitinconsistent outputs, resulting in vastly different or even incorrect poseswhen presented with semantically similar or identical text inputs. In thispaper, we undertake an analysis to elucidate the underlying causes of thisinstability, establishing a clear link between the unpredictability of modeloutputs and the erratic attention patterns of the text encoder module.Consequently, we introduce a formal framework aimed at addressing this issue,which we term the Stable Text-to-Motion Framework (SATO). SATO consists ofthree modules, each dedicated to stable attention, stable prediction, andmaintaining a balance between accuracy and robustness trade-off. We present amethodology for constructing an SATO that satisfies the stability of attentionand prediction. To verify the stability of the model, we introduced a newtextual synonym perturbation dataset based on HumanML3D and KIT-ML. Resultsshow that SATO is significantly more stable against synonyms and other slightperturbations while keeping its high accuracy performance.</description><author>Wenshuo Chen, Hongru Xiao, Erhang Zhang, Lijie Hu, Lei Wang, Mengyuan Liu, Chen Chen</author><pubDate>Fri, 03 May 2024 17:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01461v2</guid></item><item><title>Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks</title><link>http://arxiv.org/abs/2405.02225v1</link><description>This paper introduces a framework for post-processing machine learning modelsso that their predictions satisfy multi-group fairness guarantees. Based on thecelebrated notion of multicalibration, we introduce $(\mathbf{s},\mathcal{G},\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) formulti-dimensional mappings $\mathbf{s}$, constraint set $\mathcal{G}$, and apre-specified threshold level $\alpha$. We propose associated algorithms toachieve this notion in general settings. This framework is then applied todiverse scenarios encompassing different fairness concerns, including falsenegative rate control in image segmentation, prediction set conditionaluncertainty quantification in hierarchical classification, and de-biased textgeneration in language models. We conduct numerical studies on several datasetsand tasks.</description><author>Lujing Zhang, Aaron Roth, Linjun Zhang</author><pubDate>Fri, 03 May 2024 17:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02225v1</guid></item><item><title>Privately Aligning Language Models with Reinforcement Learning</title><link>http://arxiv.org/abs/2310.16960v2</link><description>Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.</description><author>Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</author><pubDate>Fri, 03 May 2024 17:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16960v2</guid></item><item><title>Discretization Error of Fourier Neural Operators</title><link>http://arxiv.org/abs/2405.02221v1</link><description>Operator learning is a variant of machine learning that is designed toapproximate maps between function spaces from data. The Fourier Neural Operator(FNO) is a common model architecture used for operator learning. The FNOcombines pointwise linear and nonlinear operations in physical space withpointwise linear operations in Fourier space, leading to a parameterized mapacting between function spaces. Although FNOs formally involve convolutions offunctions on a continuum, in practice the computations are performed on adiscretized grid, allowing efficient implementation via the FFT. In this paper,the aliasing error that results from such a discretization is quantified andalgebraic rates of convergence in terms of the grid resolution are obtained asa function of the regularity of the input. Numerical experiments that validatethe theory and describe model stability are performed.</description><author>Samuel Lanthaler, Andrew M. Stuart, Margaret Trautner</author><pubDate>Fri, 03 May 2024 17:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02221v1</guid></item><item><title>Designed Dithering Sign Activation for Binary Neural Networks</title><link>http://arxiv.org/abs/2405.02220v1</link><description>Binary Neural Networks emerged as a cost-effective and energy-efficientsolution for computer vision tasks by binarizing either network weights oractivations. However, common binary activations, such as the Sign activationfunction, abruptly binarize the values with a single threshold, losingfine-grained details in the feature outputs. This work proposes an activationthat applies multiple thresholds following dithering principles, shifting theSign activation function for each pixel according to a spatially periodicthreshold kernel. Unlike literature methods, the shifting is defined jointlyfor a set of adjacent pixels, taking advantage of spatial correlations.Experiments over the classification task demonstrate the effectiveness of thedesigned dithering Sign activation function as an alternative activation forbinary neural networks, without increasing the computational cost. Further,DeSign balances the preservation of details with the efficiency of binaryoperations.</description><author>Brayan Monroy, Juan Estupiñan, Tatiana Gelvez-Barrera, Jorge Bacca, Henry Arguello</author><pubDate>Fri, 03 May 2024 17:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02220v1</guid></item><item><title>Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models</title><link>http://arxiv.org/abs/2401.10647v4</link><description>In the rapidly advancing field of artificial intelligence, the concept ofRed-Teaming or Jailbreaking large language models (LLMs) has emerged as acrucial area of study. This approach is especially significant in terms ofassessing and enhancing the safety and robustness of these models. This paperinvestigates the intricate consequences of such modifications through modelediting, uncovering a complex relationship between enhancing model accuracy andpreserving its ethical integrity. Our in-depth analysis reveals a strikingparadox: while injecting accurate information is crucial for model reliability,it can paradoxically destabilize the model's foundational framework, resultingin unpredictable and potentially unsafe behaviors. Additionally, we propose abenchmark dataset NicheHazardQA to investigate this unsafe behavior both withinthe same and cross topical domain. This aspect of our research sheds light onhow the edits, impact the model's safety metrics and guardrails. Our findingsshow that model editing serves as a cost-effective tool for topical red-teamingby methodically applying targeted edits and evaluating the resultant modelbehavior.</description><author>Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria</author><pubDate>Fri, 03 May 2024 17:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10647v4</guid></item><item><title>GReAT: A Graph Regularized Adversarial Training Method</title><link>http://arxiv.org/abs/2310.05336v2</link><description>This paper presents GReAT (Graph Regularized Adversarial Training), a novelregularization method designed to enhance the robust classification performanceof deep learning models. Adversarial examples, characterized by subtleperturbations that can mislead models, pose a significant challenge in machinelearning. Although adversarial training is effective in defending against suchattacks, it often overlooks the underlying data structure. In response, GReATintegrates graph based regularization into the adversarial training process,leveraging the data's inherent structure to enhance model robustness. Byincorporating graph information during training, GReAT defends againstadversarial attacks and improves generalization to unseen data. Extensiveevaluations on benchmark datasets demonstrate that GReAT outperforms state ofthe art methods in robustness, achieving notable improvements in classificationaccuracy. Specifically, compared to the second best methods, GReAT achieves aperformance increase of approximately 4.87% for CIFAR10 against FGSM attack and10.57% for SVHN against FGSM attack. Additionally, for CIFAR10, GReATdemonstrates a performance increase of approximately 11.05% against PGD attack,and for SVHN, a 5.54% increase against PGD attack. This paper provides detailedinsights into the proposed methodology, including numerical results andcomparisons with existing approaches, highlighting the significant impact ofGReAT in advancing the performance of deep learning models.</description><author>Samet Bayram, Kenneth Barner</author><pubDate>Fri, 03 May 2024 17:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05336v2</guid></item><item><title>Multispectral Fine-Grained Classification of Blackgrass in Wheat and Barley Crops</title><link>http://arxiv.org/abs/2405.02218v1</link><description>As the burden of herbicide resistance grows and the environmentalrepercussions of excessive herbicide use become clear, new ways of managingweed populations are needed. This is particularly true for cereal crops, likewheat and barley, that are staple food crops and occupy a globally significantportion of agricultural land. Even small improvements in weed managementpractices across these major food crops worldwide would yield considerablebenefits for both the environment and global food security. Blackgrass is amajor grass weed which causes particular problems in cereal crops in north-westEurope, a major cereal production area, because it has high levels of ofherbicide resistance and is well adapted to agronomic practice in this region.With the use of machine vision and multispectral imaging, we investigate theeffectiveness of state-of-the-art methods to identify blackgrass in wheat andbarley crops. As part of this work, we provide a large dataset with which weevaluate several key aspects of blackgrass weed recognition. Firstly, wedetermine the performance of different CNN and transformer-based architectureson images from unseen fields. Secondly, we demonstrate the role that differentspectral bands have on the performance of weed classification. Lastly, weevaluate the role of dataset size in classification performance for each of themodels trialled. We find that even with a fairly modest quantity of trainingdata an accuracy of almost 90% can be achieved on images from unseen fields.</description><author>Madeleine Darbyshire, Shaun Coutts, Eleanor Hammond, Fazilet Gokbudak, Cengiz Oztireli, Petra Bosilj, Junfeng Gao, Elizabeth Sklar, Simon Parsons</author><pubDate>Fri, 03 May 2024 17:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02218v1</guid></item><item><title>From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality?</title><link>http://arxiv.org/abs/2403.11894v2</link><description>Deep learning (DL) has substantially enhanced natural language processing(NLP) in healthcare research. However, the increasing complexity of DL-basedNLP necessitates transparent model interpretability, or at leastexplainability, for reliable decision-making. This work presents a thoroughscoping review of explainable and interpretable DL in healthcare NLP. The term"eXplainable and Interpretable Artificial Intelligence" (XIAI) is introduced todistinguish XAI from IAI. Different models are further categorized based ontheir functionality (model-, input-, output-based) and scope (local, global).Our analysis shows that attention mechanisms are the most prevalent emergingIAI technique. The use of IAI is growing, distinguishing it from XAI. The majorchallenges identified are that most XIAI does not explore "global" modellingprocesses, the lack of best practices, and the lack of systematic evaluationand benchmarks. One important opportunity is to use attention mechanisms toenhance multi-modal XIAI for personalized medicine. Additionally, combining DLwith causal logic holds promise. Our discussion encourages the integration ofXIAI in Large Language Models (LLMs) and domain-specific smaller models. Inconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.Collaboration with domain experts, end-users, and policymakers can lead toready-to-use XIAI methods across NLP and medical tasks. While challenges exist,XIAI techniques offer a valuable foundation for interpretable NLP algorithms inhealthcare.</description><author>Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou</author><pubDate>Fri, 03 May 2024 17:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11894v2</guid></item><item><title>Automatic Programming: Large Language Models and Beyond</title><link>http://arxiv.org/abs/2405.02213v1</link><description>Automatic programming has seen increasing popularity due to the emergence oftools like GitHub Copilot which rely on Large Language Models (LLMs). At thesame time, automatically generated code faces challenges during deployment dueto concerns around quality and trust. In this article, we study automatedcoding in a general sense and study the concerns around code quality, securityand related issues of programmer responsibility. These are key issues fororganizations while deciding on the usage of automatically generated code. Wediscuss how advances in software engineering such as program repair andanalysis can enable automatic programming. We conclude with a forward lookingview, focusing on the programming environment of the near future, whereprogrammers may need to switch to different roles to fully utilize the power ofautomatic programming. Automated repair of automatically generated programsfrom LLMs, can help produce higher assurance code from LLMs, along withevidence of assurance</description><author>Michael R. Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, Patanamon Thongtanunam</author><pubDate>Fri, 03 May 2024 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02213v1</guid></item><item><title>LangProp: A code optimization framework using Large Language Models applied to driving</title><link>http://arxiv.org/abs/2401.10314v2</link><description>We propose LangProp, a framework for iteratively optimizing code generated bylarge language models (LLMs), in both supervised and reinforcement learningsettings. While LLMs can generate sensible coding solutions zero-shot, they areoften sub-optimal. Especially for code generation tasks, it is likely that theinitial code will fail on certain edge cases. LangProp automatically evaluatesthe code performance on a dataset of input-output pairs, catches anyexceptions, and feeds the results back to the LLM in the training loop, so thatthe LLM can iteratively improve the code it generates. By adopting a metric-and data-driven training paradigm for this code optimization procedure, onecould easily adapt findings from traditional machine learning techniques suchas imitation learning, DAgger, and reinforcement learning. We show LangProp'sapplicability to general domains such as Sudoku and CartPole, as well asdemonstrate the first proof of concept of automated code optimization forautonomous driving in CARLA. We show that LangProp can generate interpretableand transparent policies that can be verified and improved in a metric- anddata-driven way. Our code is available athttps://github.com/shuishida/LangProp.</description><author>Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, João F. Henriques, Anthony Hu</author><pubDate>Fri, 03 May 2024 17:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10314v2</guid></item><item><title>A separability-based approach to quantifying generalization: which layer is best?</title><link>http://arxiv.org/abs/2405.01524v2</link><description>Generalization to unseen data remains poorly understood for deep learningclassification and foundation models. How can one assess the ability ofnetworks to adapt to new or extended versions of their input space in thespirit of few-shot learning, out-of-distribution generalization, and domainadaptation? Which layers of a network are likely to generalize best? We providea new method for evaluating the capacity of networks to represent a sampleddomain, regardless of whether the network has been trained on all classes inthe domain. Our approach is the following: after fine-tuning state-of-the-artpre-trained models for visual classification on a particular domain, we assesstheir performance on data from related but distinct variations in that domain.Generalization power is quantified as a function of the latent embeddings ofunseen data from intermediate layers for both unsupervised and supervisedsettings. Working throughout all stages of the network, we find that (i) highclassification accuracy does not imply high generalizability; and (ii) deeperlayers in a model do not always generalize the best, which has implications forpruning. Since the trends observed across datasets are largely consistent, weconclude that our approach reveals (a function of) the intrinsic capacity ofthe different layers of a model to generalize.</description><author>Luciano Dyballa, Evan Gerritz, Steven W. Zucker</author><pubDate>Fri, 03 May 2024 17:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01524v2</guid></item><item><title>Regularized Q-learning through Robust Averaging</title><link>http://arxiv.org/abs/2405.02201v1</link><description>We propose a new Q-learning variant, called 2RA Q-learning, that addressessome weaknesses of existing Q-learning methods in a principled manner. One suchweakness is an underlying estimation bias which cannot be controlled and oftenresults in poor performance. We propose a distributionally robust estimator forthe maximum expected value term, which allows us to precisely control the levelof estimation bias introduced. The distributionally robust estimator admits aclosed-form solution such that the proposed algorithm has a computational costper iteration comparable to Watkins' Q-learning. For the tabular case, we showthat 2RA Q-learning converges to the optimal policy and analyze its asymptoticmean-squared error. Lastly, we conduct numerical experiments for varioussettings, which corroborate our theoretical findings and indicate that 2RAQ-learning often performs better than existing methods.</description><author>Peter Schmitt-Förster, Tobias Sutter</author><pubDate>Fri, 03 May 2024 16:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02201v1</guid></item><item><title>Position Paper: Rethinking Empirical Research in Machine Learning: Addressing Epistemic and Methodological Challenges of Experimentation</title><link>http://arxiv.org/abs/2405.02200v1</link><description>We warn against a common but incomplete understanding of empirical researchin machine learning (ML) that leads to non-replicable results, makes findingsunreliable, and threatens to undermine progress in the field. To overcome thisalarming situation, we call for more awareness of the plurality of ways ofgaining knowledge experimentally but also of some epistemic limitations. Inparticular, we argue most current empirical ML research is fashioned asconfirmatory research while it should rather be considered exploratory.</description><author>Moritz Herrmann, F. Julian D. Lange, Katharina Eggensperger, Giuseppe Casalicchio, Marcel Wever, Matthias Feurer, David Rügamer, Eyke Hüllermeier, Anne-Laure Boulesteix, Bernd Bischl</author><pubDate>Fri, 03 May 2024 16:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02200v1</guid></item><item><title>Public-private funding models in open source software development: A case study on scikit-learn</title><link>http://arxiv.org/abs/2404.06484v5</link><description>Governments are increasingly funding open source software (OSS) developmentto support software security, digital sovereignty, and national competitivenessin science and innovation, amongst others. However, little is known about howOSS developers evaluate the relative benefits and drawbacks of governmentalfunding for OSS. This study explores this question through a case study onscikit-learn, a Python library for machine learning, funded by public researchgrants, commercial sponsorship, micro-donations, and a 32 euro million grantannounced in France's artificial intelligence strategy. Through 25 interviewswith scikit-learn's maintainers and funders, this study makes two keycontributions. First, it contributes empirical findings about the benefits anddrawbacks of public and private funding in an impactful OSS project, and thegovernance protocols employed by the maintainers to balance the diverseinterests of their community and funders. Second, it offers practical lessonson funding for OSS developers, governments, and companies based on theexperience of scikit-learn. The paper concludes with key recommendations forpractitioners and future research directions.</description><author>Cailean Osborne</author><pubDate>Fri, 03 May 2024 16:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06484v5</guid></item><item><title>Impact of emoji exclusion on the performance of Arabic sarcasm detection models</title><link>http://arxiv.org/abs/2405.02195v1</link><description>The complex challenge of detecting sarcasm in Arabic speech on social mediais increased by the language diversity and the nature of sarcastic expressions.There is a significant gap in the capability of existing models to effectivelyinterpret sarcasm in Arabic, which mandates the necessity for moresophisticated and precise detection methods. In this paper, we investigate theimpact of a fundamental preprocessing component on sarcasm speech detection.While emojis play a crucial role in mitigating the absence effect of bodylanguage and facial expressions in modern communication, their impact onautomated text analysis, particularly in sarcasm detection, remainsunderexplored. We investigate the impact of emoji exclusion from datasets onthe performance of sarcasm detection models in social media content for Arabicas a vocabulary-super rich language. This investigation includes the adaptationand enhancement of AraBERT pre-training models, specifically by excludingemojis, to improve sarcasm detection capabilities. We use AraBERT pre-trainingto refine the specified models, demonstrating that the removal of emojis cansignificantly boost the accuracy of sarcasm detection. This approachfacilitates a more refined interpretation of language, eliminating thepotential confusion introduced by non-textual elements. The evaluated AraBERTmodels, through the focused strategy of emoji removal, adeptly navigate thecomplexities of Arabic sarcasm. This study establishes new benchmarks in Arabicnatural language processing and presents valuable insights for social mediaplatforms.</description><author>Ghalyah H. Aleryani, Wael Deabes, Khaled Albishre, Alaa E. Abdel-Hakim</author><pubDate>Fri, 03 May 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02195v1</guid></item><item><title>Improving Interpretation Faithfulness for Vision Transformers</title><link>http://arxiv.org/abs/2311.17983v2</link><description>Vision Transformers (ViTs) have achieved state-of-the-art performance forvarious vision tasks. One reason behind the success lies in their ability toprovide plausible innate explanations for the behavior of neural architectures.However, ViTs suffer from issues with explanation faithfulness, as their focalpoints are fragile to adversarial attacks and can be easily changed with evenslight perturbations on the input image. In this paper, we propose a rigorousapproach to mitigate these issues by introducing Faithful ViTs (FViTs). Brieflyspeaking, an FViT should have the following two properties: (1) The top-$k$indices of its self-attention vector should remain mostly unchanged under inputperturbation, indicating stable explanations; (2) The prediction distributionshould be robust to perturbations. To achieve this, we propose a new methodcalled Denoised Diffusion Smoothing (DDS), which adopts randomized smoothingand diffusion-based denoising. We theoretically prove that processing ViTsdirectly with DDS can turn them into FViTs. We also show that Gaussian noise isnearly optimal for both $\ell_2$ and $\ell_\infty$-norm cases. Finally, wedemonstrate the effectiveness of our approach through comprehensive experimentsand evaluations. Results show that FViTs are more robust against adversarialattacks while maintaining the explainability of attention, indicating higherfaithfulness.</description><author>Lijie Hu, Yixin Liu, Ninghao Liu, Mengdi Huai, Lichao Sun, Di Wang</author><pubDate>Fri, 03 May 2024 16:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17983v2</guid></item><item><title>Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine Learning</title><link>http://arxiv.org/abs/2405.02191v1</link><description>Peat, a crucial component in whisky production, imparts distinctive andirreplaceable flavours to the final product. However, the extraction of peatdisrupts ancient ecosystems and releases significant amounts of carbon,contributing to climate change. This paper aims to address this issue byconducting a feasibility study on enhancing peat use efficiency in whiskymanufacturing through non-destructive analysis using hyperspectral imaging.Results show that shot-wave infrared (SWIR) data is more effective foranalyzing peat samples and predicting total phenol levels, with accuracies upto 99.81%.</description><author>Yijun Yan, Jinchang Ren, Barry Harrison, Oliver Lewis, Yinhe Li, Ping Ma</author><pubDate>Fri, 03 May 2024 16:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02191v1</guid></item><item><title>Automated National Urban Map Extraction</title><link>http://arxiv.org/abs/2404.06202v2</link><description>Developing countries usually lack the proper governance means to generate andregularly update a national rooftop map. Using traditional photogrammetry andsurveying methods to produce a building map at the federal level is costly andtime consuming. Using earth observation and deep learning methods, we canbridge this gap and propose an automated pipeline to fetch such national urbanmaps. This paper aims to exploit the power of fully convolutional neuralnetworks for multi-class buildings' instance segmentation to leverage highobject-wise accuracy results. Buildings' instance segmentation from sub-meterhigh-resolution satellite images can be achieved with relatively highpixel-wise metric scores. We detail all engineering steps to replicate thiswork and ensure highly accurate results in dense and slum areas witnessed inregions that lack proper urban planning in the Global South. We applied a casestudy of the proposed pipeline to Lebanon and successfully produced the firstcomprehensive national building footprint map with approximately 1 Millionunits with an 84% accuracy. The proposed architecture relies on advancedaugmentation techniques to overcome dataset scarcity, which is often the casein developing countries.</description><author>Hasan Nasrallah, Abed Ellatif Samhat, Cristiano Nattero, Ali J. Ghandour</author><pubDate>Fri, 03 May 2024 16:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06202v2</guid></item><item><title>Policy design in experiments with unknown interference</title><link>http://arxiv.org/abs/2011.08174v9</link><description>This paper studies experimental designs for estimation and inference onpolicies with spillover effects. Units are organized into a finite number oflarge clusters and interact in unknown ways within each cluster. First, weintroduce a single-wave experiment that, by varying the randomization acrosscluster pairs, estimates the marginal effect of a change in treatmentprobabilities, taking spillover effects into account. Using the marginaleffect, we propose a test for policy optimality. Second, we design amultiple-wave experiment to estimate welfare-maximizing treatment rules. Weprovide strong theoretical guarantees and an implementation in a large-scalefield experiment.</description><author>Davide Viviano, Jess Rudder</author><pubDate>Fri, 03 May 2024 16:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.08174v9</guid></item><item><title>Weisfeiler-Lehman goes Dynamic: An Analysis of the Expressive Power of Graph Neural Networks for Attributed and Dynamic Graphs</title><link>http://arxiv.org/abs/2210.03990v2</link><description>Graph Neural Networks (GNNs) are a large class of relational models for graphprocessing. Recent theoretical studies on the expressive power of GNNs havefocused on two issues. On the one hand, it has been proven that GNNs are aspowerful as the Weisfeiler-Lehman test (1-WL) in their ability to distinguishgraphs. Moreover, it has been shown that the equivalence enforced by 1-WLequals unfolding equivalence. On the other hand, GNNs turned out to beuniversal approximators on graphs modulo the constraints enforced by1-WL/unfolding equivalence. However, these results only apply to StaticAttributed Undirected Homogeneous Graphs (SAUHG) with node attributes. Incontrast, real-life applications often involve a much larger variety of graphtypes. In this paper, we conduct a theoretical analysis of the expressive powerof GNNs for two other graph domains that are particularly interesting inpractical applications, namely dynamic graphs and SAUGHs with edge attributes.Dynamic graphs are widely used in modern applications; hence, the study of theexpressive capability of GNNs in this domain is essential for practical reasonsand, in addition, it requires a new analyzing approach due to the difference inthe architecture of dynamic GNNs compared to static ones. On the other hand,the examination of SAUHGs is of particular relevance since they act as astandard form for all graph types: it has been shown that all graph types canbe transformed without loss of information to SAUHGs with both attributes onnodes and edges. This paper considers generic GNN models and appropriate 1-WLtests for those domains. Then, the known results on the expressive power ofGNNs are extended to the mentioned domains: it is proven that GNNs have thesame capability as the 1-WL test, the 1-WL equivalence equals unfoldingequivalence and that GNNs are universal approximators modulo 1-WL/unfoldingequivalence.</description><author>Silvia Beddar-Wiesing, Giuseppe Alessio D'Inverno, Caterina Graziani, Veronica Lachi, Alice Moallemy-Oureh, Franco Scarselli, Josephine Maria Thomas</author><pubDate>Fri, 03 May 2024 16:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03990v2</guid></item><item><title>Optimistic Regret Bounds for Online Learning in Adversarial Markov Decision Processes</title><link>http://arxiv.org/abs/2405.02188v1</link><description>The Adversarial Markov Decision Process (AMDP) is a learning framework thatdeals with unknown and varying tasks in decision-making applications likerobotics and recommendation systems. A major limitation of the AMDP formalism,however, is pessimistic regret analysis results in the sense that although thecost function can change from one episode to the next, the evolution in manysettings is not adversarial. To address this, we introduce and study a newvariant of AMDP, which aims to minimize regret while utilizing a set of costpredictors. For this setting, we develop a new policy search method thatachieves a sublinear optimistic regret with high probability, that is a regretbound which gracefully degrades with the estimation power of the costpredictors. Establishing such optimistic regret bounds is nontrivial given that(i) as we demonstrate, the existing importance-weighted cost estimators cannotestablish optimistic bounds, and (ii) the feedback model of AMDP is different(and more realistic) than the existing optimistic online learning works. Ourresult, in particular, hinges upon developing a novel optimistically biasedcost estimator that leverages cost predictors and enables a high-probabilityregret analysis without imposing restrictive assumptions. We further discusspractical extensions of the proposed scheme and demonstrate its efficacynumerically.</description><author>Sang Bin Moon, Abolfazl Hashemi</author><pubDate>Fri, 03 May 2024 16:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02188v1</guid></item><item><title>Accelerating Convergence in Bayesian Few-Shot Classification</title><link>http://arxiv.org/abs/2405.01507v2</link><description>Bayesian few-shot classification has been a focal point in the field offew-shot learning. This paper seamlessly integrates mirror descent-basedvariational inference into Gaussian process-based few-shot classification,addressing the challenge of non-conjugate inference. By leveragingnon-Euclidean geometry, mirror descent achieves accelerated convergence byproviding the steepest descent direction along the corresponding manifold. Italso exhibits the parameterization invariance property concerning thevariational distribution. Experimental results demonstrate competitiveclassification accuracy, improved uncertainty quantification, and fasterconvergence compared to baseline models. Additionally, we investigate theimpact of hyperparameters and components. Code is publicly available athttps://github.com/keanson/MD-BSFC.</description><author>Tianjun Ke, Haoqun Cao, Feng Zhou</author><pubDate>Fri, 03 May 2024 16:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01507v2</guid></item><item><title>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</title><link>http://arxiv.org/abs/2311.04157v2</link><description>We present a novel usage of Transformers to make image classificationinterpretable. Unlike mainstream classifiers that wait until the last fullyconnected layer to incorporate class information to make predictions, weinvestigate a proactive approach, asking each class to search for itself in animage. We realize this idea via a Transformer encoder-decoder inspired byDEtection TRansformer (DETR). We learn "class-specific" queries (one for eachclass) as input to the decoder, enabling each class to localize its patterns inan image via cross-attention. We name our approach INterpretable TRansformer(INTR), which is fairly easy to implement and exhibits several compellingproperties. We show that INTR intrinsically encourages each class to attenddistinctively; the cross-attention weights thus provide a faithfulinterpretation of the prediction. Interestingly, via "multi-head"cross-attention, INTR could identify different "attributes" of a class, makingit particularly suitable for fine-grained classification and analysis, which wedemonstrate on eight datasets. Our code and pre-trained models are publiclyaccessible at the Imageomics Institute GitHub site:https://github.com/Imageomics/INTR.</description><author>Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel Stevens, Kaiya L. Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</author><pubDate>Fri, 03 May 2024 16:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04157v2</guid></item><item><title>Metalearners for Ranking Treatment Effects</title><link>http://arxiv.org/abs/2405.02183v1</link><description>Efficiently allocating treatments with a budget constraint constitutes animportant challenge across various domains. In marketing, for example, the useof promotions to target potential customers and boost conversions is limited bythe available budget. While much research focuses on estimating causal effects,there is relatively limited work on learning to allocate treatments whileconsidering the operational context. Existing methods for uplift modeling orcausal inference primarily estimate treatment effects, without considering howthis relates to a profit maximizing allocation policy that respects budgetconstraints. The potential downside of using these methods is that theresulting predictive model is not aligned with the operational context.Therefore, prediction errors are propagated to the optimization of the budgetallocation problem, subsequently leading to a suboptimal allocation policy. Wepropose an alternative approach based on learning to rank. Our proposedmethodology directly learns an allocation policy by prioritizing instances interms of their incremental profit. We propose an efficient sampling procedurefor the optimization of the ranking model to scale our methodology tolarge-scale data sets. Theoretically, we show how learning to rank can maximizethe area under a policy's incremental profit curve. Empirically, we validateour methodology and show its effectiveness in practice through a series ofexperiments on both synthetic and real-world data.</description><author>Toon Vanderschueren, Wouter Verbeke, Felipe Moraes, Hugo Manuel Proença</author><pubDate>Fri, 03 May 2024 16:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02183v1</guid></item><item><title>Imitation Learning in Discounted Linear MDPs without exploration assumptions</title><link>http://arxiv.org/abs/2405.02181v1</link><description>We present a new algorithm for imitation learning in infinite horizon linearMDPs dubbed ILARL which greatly improves the bound on the number oftrajectories that the learner needs to sample from the environment. Inparticular, we remove exploration assumptions required in previous works and weimprove the dependence on the desired accuracy $\epsilon$ from$\mathcal{O}\br{\epsilon^{-5}}$ to $\mathcal{O}\br{\epsilon^{-4}}$. Our resultrelies on a connection between imitation learning and online learning in MDPswith adversarial losses. For the latter setting, we present the first resultfor infinite horizon linear MDP which may be of independent interest. Moreover,we are able to provide a strengthen result for the finite horizon case where weachieve $\mathcal{O}\br{\epsilon^{-2}}$. Numerical experiments with linearfunction approximation shows that ILARL outperforms other commonly usedalgorithms.</description><author>Luca Viano, Stratis Skoulakis, Volkan Cevher</author><pubDate>Fri, 03 May 2024 16:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02181v1</guid></item><item><title>A Flow-Based Model for Conditional and Probabilistic Electricity Consumption Profile Generation and Prediction</title><link>http://arxiv.org/abs/2405.02180v1</link><description>Residential Load Profile (RLP) generation and prediction are critical for theoperation and planning of distribution networks, particularly as diverselow-carbon technologies are increasingly integrated. This paper introduces anovel flow-based generative model, termed Full Convolutional Profile Flow(FCPFlow), which is uniquely designed for both conditional and unconditionalRLP generation, and for probabilistic load forecasting. By introducing two newlayers--the invertible linear layer and the invertible normalization layer--theproposed FCPFlow architecture shows three main advantages compared totraditional statistical and contemporary deep generative models: 1) it iswell-suited for RLP generation under continuous conditions, such as varyingweather and annual electricity consumption, 2) it shows superior scalability indifferent datasets compared to traditional statistical, and 3) it alsodemonstrates better modeling capabilities in capturing the complex correlationof RLPs compared with deep generative models.</description><author>Weijie Xia, Chenguang Wang, Peter Palensky, Pedro P. Vergara</author><pubDate>Fri, 03 May 2024 16:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02180v1</guid></item><item><title>Training-Free Deepfake Voice Recognition by Leveraging Large-Scale Pre-Trained Models</title><link>http://arxiv.org/abs/2405.02179v1</link><description>Generalization is a main issue for current audio deepfake detectors, whichstruggle to provide reliable results on out-of-distribution data. Given thespeed at which more and more accurate synthesis methods are developed, it isvery important to design techniques that work well also on data they were nottrained for.In this paper we study the potential of large-scale pre-trainedmodels for audio deepfake detection, with special focus on generalizationability. To this end, the detection problem is reformulated in a speakerverification framework and fake audios are exposed by the mismatch between thevoice sample under test and the voice of the claimed identity. With thisparadigm, no fake speech sample is necessary in training, cutting off any linkwith the generation method at the root, and ensuring full generalizationability. Features are extracted by general-purpose large pre-trained models,with no need for training or fine-tuning on specific fake detection or speakerverification datasets. At detection time only a limited set of voice fragmentsof the identity under test is required. Experiments on several datasetswidespread in the community show that detectors based on pre-trained modelsachieve excellent performance and show strong generalization ability, rivalingsupervised methods on in-distribution data and largely overcoming them onout-of-distribution data.</description><author>Alessandro Pianese, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva</author><pubDate>Fri, 03 May 2024 16:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02179v1</guid></item><item><title>Assessing and Verifying Task Utility in LLM-Powered Applications</title><link>http://arxiv.org/abs/2405.02178v1</link><description>The rapid development of Large Language Models (LLMs) has led to a surge inapplications that facilitate collaboration among multiple agents, assistinghumans in their daily tasks. However, a significant gap remains in assessing towhat extent LLM-powered applications genuinely enhance user experience and taskexecution efficiency. This highlights the need to verify utility of LLM-poweredapplications, particularly by ensuring alignment between the application'sfunctionality and end-user needs. We introduce AgentEval, a novel frameworkdesigned to simplify the utility verification process by automaticallyproposing a set of criteria tailored to the unique purpose of any givenapplication. This allows for a comprehensive assessment, quantifying theutility of an application against the suggested criteria. We present acomprehensive analysis of the effectiveness and robustness of AgentEval for twoopen source datasets including Math Problem solving and ALFWorld House-holdrelated tasks. For reproducibility purposes, we make the data, code and all thelogs publicly available at https://bit.ly/3w3yKcS .</description><author>Negar Arabzadeh, Siging Huo, Nikhil Mehta, Qinqyun Wu, Chi Wang, Ahmed Awadallah, Charles L. A. Clarke, Julia Kiseleva</author><pubDate>Fri, 03 May 2024 16:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02178v1</guid></item><item><title>Visual Environment Assessment for Safe Autonomous Quadrotor Landing</title><link>http://arxiv.org/abs/2311.10065v3</link><description>Autonomous identification and evaluation of safe landing zones are ofparamount importance for ensuring the safety and effectiveness of aerial robotsin the event of system failures, low battery, or the successful completion ofspecific tasks. In this paper, we present a novel approach for detection andassessment of potential landing sites for safe quadrotor landing. Our solutionefficiently integrates 2D and 3D environmental information, eliminating theneed for external aids such as GPS and computationally intensive elevationmaps. The proposed pipeline combines semantic data derived from a NeuralNetwork (NN), to extract environmental features, with geometric data obtainedfrom a disparity map, to extract critical geometric attributes such as slope,flatness, and roughness. We define several cost metrics based on theseattributes to evaluate safety, stability, and suitability of regions in theenvironments and identify the most suitable landing area. Our approach runs inreal-time on quadrotors equipped with limited computational capabilities.Experimental results conducted in diverse environments demonstrate that theproposed method can effectively assess and identify suitable landing areas,enabling the safe and autonomous landing of a quadrotor.</description><author>Mattia Secchiero, Nishanth Bobbili, Yang Zhou, Giuseppe Loianno</author><pubDate>Fri, 03 May 2024 16:25:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10065v3</guid></item><item><title>Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset</title><link>http://arxiv.org/abs/2405.02175v1</link><description>Hoaxes are a recognised form of disinformation created deliberately, withpotential serious implications in the credibility of reference knowledgeresources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is thatthey often are written according to the official style guidelines. In thiswork, we first provide a systematic analysis of the similarities anddiscrepancies between legitimate and hoax Wikipedia articles, and introduceHoaxpedia, a collection of 311 Hoax articles (from existing literature as wellas official Wikipedia lists) alongside semantically similar real articles. Wereport results of binary classification experiments in the task of predictingwhether a Wikipedia article is real or hoax, and analyze several settings aswell as a range of language models. Our results suggest that detectingdeceitful content in Wikipedia based on content alone, despite not having beenexplored much in the past, is a promising direction.</description><author>Hsuvas Borkakoty, Luis Espinosa-Anke</author><pubDate>Fri, 03 May 2024 16:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02175v1</guid></item><item><title>Zero-shot generalization across architectures for visual classification</title><link>http://arxiv.org/abs/2402.14095v4</link><description>Generalization to unseen data is a key desideratum for deep networks, but itsrelation to classification accuracy is unclear. Using a minimalist visiondataset and a measure of generalizability, we show that popular networks, fromdeep convolutional networks (CNNs) to transformers, vary in their power toextrapolate to unseen classes both across layers and across architectures.Accuracy is not a good predictor of generalizability, and generalization variesnon-monotonically with layer depth.</description><author>Evan Gerritz, Luciano Dyballa, Steven W. Zucker</author><pubDate>Fri, 03 May 2024 16:25:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14095v4</guid></item><item><title>Visual Enumeration is Challenging for Large-scale Generative AI</title><link>http://arxiv.org/abs/2402.03328v2</link><description>Humans can readily judge the number of objects in a visual scene, evenwithout counting, and such a skill has been documented in many animal speciesand babies prior to language development and formal schooling. Numericaljudgments are error-free for small sets, while for larger collections responsesbecome approximate, with variability increasing proportionally to the targetnumber. This response pattern is observed for items of all kinds, despitevariation in object features (such as color or shape), suggesting that ourvisual number sense relies on abstract representations of numerosity. Here, weinvestigate whether large-scale generative Artificial Intelligence (AI) systemshave a human-like number sense, which should allow them to reliably name thenumber of objects in simple visual stimuli or generate images containing atarget number of items in the 1-10 range. Surprisingly, most of the foundationmodels considered have a poor number sense: They make striking errors even withsmall numbers, the response variability does not increase in a systematic way,and the pattern of errors depends on object category. Only the most recentproprietary systems exhibit signatures of a visual number sense. Our findingsdemonstrate that having an intuitive visual understanding of number remainschallenging for foundation models, which in turn might be detrimental to theperceptual grounding of numeracy that in humans is crucial for mathematicallearning.</description><author>Alberto Testolin, Kuinan Hou, Marco Zorzi</author><pubDate>Fri, 03 May 2024 16:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03328v2</guid></item><item><title>Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations</title><link>http://arxiv.org/abs/2405.02171v1</link><description>In this paper, we consider two challenging issues in reference-basedsuper-resolution (RefSR) for smartphone, (i) how to choose a proper referenceimage, and (ii) how to learn RefSR in a self-supervised manner. Particularly,we propose a novel self-supervised learning approach for real-world RefSR fromobservations at dual and multiple camera zooms. Firstly, considering thepopularity of multiple cameras in modern smartphones, the more zoomed(telephoto) image can be naturally leveraged as the reference to guide thesuper-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us achance to learn a deep network that performs SR from the dual zoomedobservations (DZSR). Secondly, for self-supervised learning of DZSR, we takethe telephoto image instead of an additional high-resolution image as thesupervision information, and select a center patch from it as the reference tosuper-resolve the corresponding ultra-wide image patch. To mitigate the effectof the misalignment between ultra-wide low-resolution (LR) patch and telephotoground-truth (GT) image during training, we first adopt patch-based opticalflow alignment and then design an auxiliary-LR to guide the deforming of thewarped LR features. To generate visually pleasing results, we present localoverlapped sliced Wasserstein loss to better represent the perceptualdifference between GT and output in the feature space. During testing, DZSR canbe directly deployed to super-solve the whole ultra-wide image with thereference of the telephoto image. In addition, we further take multiple zoomedobservations to explore self-supervised RefSR, and present a progressive fusionscheme for the effective utilization of reference images. Experiments show thatour methods achieve better quantitative and qualitative performance againststate-of-the-arts. Codes are available athttps://github.com/cszhilu1998/SelfDZSR_PlusPlus.</description><author>Zhilu Zhang, Ruohao Wang, Hongzhi Zhang, Wangmeng Zuo</author><pubDate>Fri, 03 May 2024 16:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02171v1</guid></item><item><title>Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization</title><link>http://arxiv.org/abs/2205.00400v3</link><description>Weakly-supervised temporal action localization (WTAL) intends to detectaction instances with only weak supervision, e.g., video-level labels. Thecurrent~\textit{de facto} pipeline locates action instances by thresholding andgrouping continuous high-score regions on temporal class activation sequences.In this route, the capacity of the model to recognize the relationships betweenadjacent snippets is of vital importance which determines the quality of theaction boundaries. However, it is error-prone since the variations betweenadjacent snippets are typically subtle, and unfortunately this is overlooked inthe literature. To tackle the issue, we propose a novel WTAL approach namedConvex Combination Consistency between Neighbors (C$^3$BN). C$^3$BN consists oftwo key ingredients: a micro data augmentation strategy that increases thediversity in-between adjacent snippets by convex combination of adjacentsnippets, and a macro-micro consistency regularization that enforces the modelto be invariant to the transformations~\textit{w.r.t.} video semantics, snippetpredictions, and snippet representations. Consequently, fine-grained patternsin-between adjacent snippets are enforced to be explored, thereby resulting ina more robust action boundary localization. Experimental results demonstratethe effectiveness of C$^3$BN on top of various baselines for WTAL withvideo-level and point-level supervisions. Code is athttps://github.com/Qinying-Liu/C3BN.</description><author>Qinying Liu, Zilei Wang, Ruoxi Chen, Zhilin Li</author><pubDate>Fri, 03 May 2024 16:17:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.00400v3</guid></item><item><title>Forensic License Plate Recognition with Compression-Informed Transformers</title><link>http://arxiv.org/abs/2207.14686v3</link><description>Forensic license plate recognition (FLPR) remains an open challenge in legalcontexts such as criminal investigations, where unreadable license plates (LPs)need to be deciphered from highly compressed and/or low resolution footage,e.g., from surveillance cameras. In this work, we propose a side-informedTransformer architecture that embeds knowledge on the input compression levelto improve recognition under strong compression. We show the effectiveness ofTransformers for license plate recognition (LPR) on a low-quality real-worlddataset. We also provide a synthetic dataset that includes strongly degraded,illegible LP images and analyze the impact of knowledge embedding on it. Thenetwork outperforms existing FLPR methods and standard state-of-the art imagerecognition models while requiring less parameters. For the severest degradedimages, we can improve recognition by up to 8.9 percent points.</description><author>Denise Moussa, Anatol Maier, Andreas Spruck, Jürgen Seiler, Christian Riess</author><pubDate>Fri, 03 May 2024 16:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14686v3</guid></item><item><title>From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks</title><link>http://arxiv.org/abs/2310.11884v2</link><description>In this paper, we review recent approaches for explaining concepts in neuralnetworks. Concepts can act as a natural link between learning and reasoning:once the concepts are identified that a neural learning system uses, one canintegrate those concepts with a reasoning system for inference or use areasoning system to act upon them to improve or enhance the learning system. Onthe other hand, knowledge can not only be extracted from neural networks butconcept knowledge can also be inserted into neural network architectures. Sinceintegrating learning and reasoning is at the core of neuro-symbolic AI, theinsights gained from this survey can serve as an important step towardsrealizing neuro-symbolic AI based on explainable concepts.</description><author>Jae Hee Lee, Sergio Lanza, Stefan Wermter</author><pubDate>Fri, 03 May 2024 16:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11884v2</guid></item><item><title>EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer</title><link>http://arxiv.org/abs/2405.02165v1</link><description>Deciphering the intricacies of the human brain has captivated curiosity forcenturies. Recent strides in Brain-Computer Interface (BCI) technology,particularly using motor imagery, have restored motor functions such asreaching, grasping, and walking in paralyzed individuals. However, unravelingnatural language from brain signals remains a formidable challenge.Electroencephalography (EEG) is a non-invasive technique used to recordelectrical activity in the brain by placing electrodes on the scalp. Previousstudies of EEG-to-text decoding have achieved high accuracy on small closedvocabularies, but still fall short of high accuracy when dealing with largeopen vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracyof open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEGpre-training to enhance the learning of semantics from EEG signals and proposesa multi-view transformer to model the EEG signal processing by differentspatial regions of the brain. Experiments show that EEG2TEXT has superiorperformance, outperforming the state-of-the-art baseline methods by a largemargin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows greatpotential for a high-performance open-vocabulary brain-to-text system tofacilitate communication.</description><author>Hanwen Liu, Daniel Hajialigol, Benny Antony, Aiguo Han, Xuan Wang</author><pubDate>Fri, 03 May 2024 16:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02165v1</guid></item><item><title>Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models</title><link>http://arxiv.org/abs/2405.02162v1</link><description>In the field of robotics and computer vision, efficient and accurate semanticmapping remains a significant challenge due to the growing demand forintelligent machines that can comprehend and interact with complexenvironments. Conventional panoptic mapping methods, however, are limited bypredefined semantic classes, thus making them ineffective for handling novel orunforeseen objects. In response to this limitation, we introduce the UnifiedPromptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances infoundation models to enable real-time, on-demand label generation using naturallanguage prompts. By incorporating a dynamic labeling strategy into traditionalpanoptic mapping techniques, UPPM provides significant improvements inadaptability and versatility while maintaining high performance levels in mapreconstruction. We demonstrate our approach on real-world and simulateddatasets. Results show that UPPM can accurately reconstruct scenes and segmentobjects while generating rich semantic labels through natural languageinteractions. A series of ablation experiments validated the advantages offoundation model-based labeling over fixed label sets.</description><author>Mohamad Al Mdfaa, Raghad Salameh, Sergey Zagoruyko, Gonzalo Ferrer</author><pubDate>Fri, 03 May 2024 16:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02162v1</guid></item><item><title>Simulating the economic impact of rationality through reinforcement learning and agent-based modelling</title><link>http://arxiv.org/abs/2405.02161v1</link><description>Agent-based models (ABMs) are simulation models used in economics to overcomesome of the limitations of traditional frameworks based on general equilibriumassumptions. However, agents within an ABM follow predetermined, not fullyrational, behavioural rules which can be cumbersome to design and difficult tojustify. Here we leverage multi-agent reinforcement learning (RL) to expand thecapabilities of ABMs with the introduction of fully rational agents that learntheir policy by interacting with the environment and maximising a rewardfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework byextending a paradigmatic macro ABM from the economic literature. We show thatgradually substituting ABM firms in the model with RL agents, trained tomaximise profits, allows for a thorough study of the impact of rationality onthe economy. We find that RL agents spontaneously learn three distinctstrategies for maximising profits, with the optimal strategy depending on thelevel of market competition and rationality. We also find that RL agents withindependent policies, and without the ability to communicate with each other,spontaneously learn to segregate into different strategic groups, thusincreasing market power and overall profits. Finally, we find that a higherdegree of rationality in the economy always improves the macroeconomicenvironment as measured by total output, depending on the specific rationalpolicy, this can come at the cost of higher instability. Our R-MABM frameworkis general, it allows for stable multi-agent learning, and represents aprincipled and robust direction to extend existing economic simulators.</description><author>Simone Brusatin, Tommaso Padoan, Andrea Coletta, Domenico Delli Gatti, Aldo Glielmo</author><pubDate>Fri, 03 May 2024 16:08:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02161v1</guid></item><item><title>Deep Learning Forecasts Caldera Collapse Events at Kilauea Volcano</title><link>http://arxiv.org/abs/2404.19351v2</link><description>During the three month long eruption of Kilauea volcano, Hawaii in 2018, thepre-existing summit caldera collapsed in over 60 quasi-periodic failure events.The last 40 of these events, which generated Mw &gt;5 very long period (VLP)earthquakes, had inter-event times between 0.8 - 2.2 days. These failure eventsoffer a unique dataset for testing methods for predicting earthquake recurrencebased on locally recorded GPS, tilt, and seismicity data. In this work, wetrain a deep learning graph neural network (GNN) to predict the time-to-failureof the caldera collapse events using only a fraction of the data recorded atthe start of each cycle. We find that the GNN generalizes to unseen data andcan predict the time-to-failure to within a few hours using only 0.5 days ofdata, substantially improving upon a null model based only on inter-eventstatistics. Predictions improve with increasing input data length, and are mostaccurate when using high-SNR tilt-meter data. Applying the trained GNN tosynthetic data with different magma pressure decay times predicts failure at anearly constant stress threshold, revealing that the GNN is sensing theunderling physics of caldera collapse. These findings demonstrate thepredictability of caldera collapse sequences under well monitored conditions,and highlight the potential of machine learning methods for forecasting realworld catastrophic events with limited training data.</description><author>Ian W. McBrearty, Paul Segall</author><pubDate>Fri, 03 May 2024 16:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19351v2</guid></item><item><title>Multi-method Integration with Confidence-based Weighting for Zero-shot Image Classification</title><link>http://arxiv.org/abs/2405.02155v1</link><description>This paper introduces a novel framework for zero-shot learning (ZSL), i.e.,to recognize new categories that are unseen during training, by using amulti-model and multi-alignment integration method. Specifically, we proposethree strategies to enhance the model's performance to handle ZSL: 1) Utilizingthe extensive knowledge of ChatGPT and the powerful image generationcapabilities of DALL-E to create reference images that can precisely describeunseen categories and classification boundaries, thereby alleviating theinformation bottleneck issue; 2) Integrating the results of text-imagealignment and image-image alignment from CLIP, along with the image-imagealignment results from DINO, to achieve more accurate predictions; 3)Introducing an adaptive weighting mechanism based on confidence levels toaggregate the outcomes from different prediction methods. Experimental resultson multiple datasets, including CIFAR-10, CIFAR-100, and TinyImageNet,demonstrate that our model can significantly improve classification accuracycompared to single-model approaches, achieving AUROC scores above 96% acrossall test datasets, and notably surpassing 99% on the CIFAR-10 dataset.</description><author>Siqi Yin, Lifan Jiang</author><pubDate>Fri, 03 May 2024 16:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02155v1</guid></item><item><title>Neural Context Flows for Learning Generalizable Dynamical Systems</title><link>http://arxiv.org/abs/2405.02154v1</link><description>Neural Ordinary Differential Equations typically struggle to generalize tonew dynamical behaviors created by parameter changes in the underlying system,even when the dynamics are close to previously seen behaviors. The issue getsworse when the changing parameters are unobserved, i.e., their value orinfluence is not directly measurable when collecting data. We introduce NeuralContext Flow (NCF), a framework that encodes said unobserved parameters in alatent context vector as input to a vector field. NCFs leveragedifferentiability of the vector field with respect to the parameters, alongwith first-order Taylor expansion to allow any context vector to influencetrajectories from other parameters. We validate our method and compare it toestablished Multi-Task and Meta-Learning alternatives, showing competitiveperformance in mean squared error for in-domain and out-of-distributionevaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scottproblems. This study holds practical implications for foundational models inscience and related areas that benefit from conditional neural ODEs. Our codeis openly available at https://github.com/ddrous/ncflow.</description><author>Roussel Desmond Nzoyem, David A. W. Barton, Tom Deakin</author><pubDate>Fri, 03 May 2024 16:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02154v1</guid></item><item><title>Deep Reinforcement Learning in Parameterized Action Space</title><link>http://arxiv.org/abs/1511.04143v5</link><description>Recent work has shown that deep neural networks are capable of approximatingboth value functions and policies in reinforcement learning domains featuringcontinuous state and action spaces. However, to the best of our knowledge noprevious work has succeeded at using deep neural networks in structured(parameterized) continuous action spaces. To fill this gap, this paper focuseson learning within the domain of simulated RoboCup soccer, which features asmall set of discrete action types, each of which is parameterized withcontinuous variables. The best learned agent can score goals more reliably thanthe 2012 RoboCup champion agent. As such, this paper represents a successfulextension of deep reinforcement learning to the class of parameterized actionspace MDPs.</description><author>Matthew Hausknecht, Peter Stone</author><pubDate>Fri, 03 May 2024 16:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1511.04143v5</guid></item><item><title>GMP-ATL: Gender-augmented Multi-scale Pseudo-label Enhanced Adaptive Transfer Learning for Speech Emotion Recognition via HuBERT</title><link>http://arxiv.org/abs/2405.02151v1</link><description>The continuous evolution of pre-trained speech models has greatly advancedSpeech Emotion Recognition (SER). However, there is still potential forenhancement in the performance of these methods. In this paper, we presentGMP-ATL (Gender-augmented Multi-scale Pseudo-label Adaptive Transfer Learning),a novel HuBERT-based adaptive transfer learning framework for SER.Specifically, GMP-ATL initially employs the pre-trained HuBERT, implementingmulti-task learning and multi-scale k-means clustering to acquire frame-levelgender-augmented multi-scale pseudo-labels. Then, to fully leverage bothobtained frame-level and utterance-level emotion labels, we incorporate modelretraining and fine-tuning methods to further optimize GMP-ATL. Experiments onIEMOCAP show that our GMP-ATL achieves superior recognition performance, with aWAR of 80.0\% and a UAR of 82.0\%, surpassing state-of-the-art unimodal SERmethods, while also yielding comparable results with multimodal SER approaches.</description><author>Yu Pan, Yuguang Yang, Heng Lu, Lei Ma, Jianjun Zhao</author><pubDate>Fri, 03 May 2024 15:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02151v1</guid></item><item><title>The Impact of Differential Feature Under-reporting on Algorithmic Fairness</title><link>http://arxiv.org/abs/2401.08788v2</link><description>Predictive risk models in the public sector are commonly developed usingadministrative data that is more complete for subpopulations that more greatlyrely on public services. In the United States, for instance, information onhealth care utilization is routinely available to government agencies forindividuals supported by Medicaid and Medicare, but not for the privatelyinsured. Critiques of public sector algorithms have identified suchdifferential feature under-reporting as a driver of disparities in algorithmicdecision-making. Yet this form of data bias remains understudied from atechnical viewpoint. While prior work has examined the fairness impacts ofadditive feature noise and features that are clearly marked as missing, thesetting of data missingness absent indicators (i.e. differential featureunder-reporting) has been lacking in research attention. In this work, wepresent an analytically tractable model of differential feature under-reportingwhich we then use to characterize the impact of this kind of data bias onalgorithmic fairness. We demonstrate how standard missing data methodstypically fail to mitigate bias in this setting, and propose a new set ofmethods specifically tailored to differential feature under-reporting. Ourresults show that, in real world data settings, under-reporting typically leadsto increasing disparities. The proposed solution methods show success inmitigating increases in unfairness.</description><author>Nil-Jana Akpinar, Zachary C. Lipton, Alexandra Chouldechova</author><pubDate>Fri, 03 May 2024 15:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08788v2</guid></item><item><title>Towards a Formal Creativity Theory: Preliminary results in Novelty and Transformativeness</title><link>http://arxiv.org/abs/2405.02148v1</link><description>Formalizing creativity-related concepts has been a long-term goal ofComputational Creativity. To the same end, we explore Formal Learning Theory inthe context of creativity. We provide an introduction to the main concepts ofthis framework and a re-interpretation of terms commonly found in creativitydiscussions, proposing formal definitions for novelty and transformationalcreativity. This formalisation marks the beginning of a research branch we callFormal Creativity Theory, exploring how learning can be included as preparationfor exploratory behaviour and how learning is a key part of transformationalcreative behaviour. By employing these definitions, we argue that, whilenovelty is neither necessary nor sufficient for transformational creativity ingeneral, when using an inspiring set, rather than a sequence of experiences, anagent actually requires novelty for transformational creativity to occur.</description><author>Luís Espírito Santo, Geraint Wiggins, Amílcar Cardoso</author><pubDate>Fri, 03 May 2024 15:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02148v1</guid></item><item><title>Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks</title><link>http://arxiv.org/abs/2207.14682v4</link><description>Freely available and easy-to-use audio editing tools make it straightforwardto perform audio splicing. Convincing forgeries can be created by combiningvarious speech samples from the same person. Detection of such splices isimportant both in the public sector when considering misinformation, and in alegal context to verify the integrity of evidence. Unfortunately, most existingdetection algorithms for audio splicing use handcrafted features and makespecific assumptions. However, criminal investigators are often faced withaudio samples from unconstrained sources with unknown characteristics, whichraises the need for more generally applicable methods. With this work, we aim to take a first step towards unconstrained audiosplicing detection to address this need. We simulate various attack scenariosin the form of post-processing operations that may disguise splicing. Wepropose a Transformer sequence-to-sequence (seq2seq) network for splicingdetection and localization. Our extensive evaluation shows that the proposedmethod outperforms existing dedicated approaches for splicing detection [3, 10]as well as the general-purpose networks EfficientNet [28] and RegNet [25].</description><author>Denise Moussa, Germans Hirsch, Christian Riess</author><pubDate>Fri, 03 May 2024 15:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.14682v4</guid></item><item><title>MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain</title><link>http://arxiv.org/abs/2405.02144v1</link><description>Medical texts are notoriously challenging to read. Properly measuring theirreadability is the first step towards making them more accessible. In thispaper, we present a systematic study on fine-grained readability measurementsin the medical domain at both sentence-level and span-level. We introduce a newdataset MedReadMe, which consists of manually annotated readability ratings andfine-grained complex span annotation for 4,520 sentences, featuring two novel"Google-Easy" and "Google-Hard" categories. It supports our quantitativeanalysis, which covers 650 linguistic features and automatic complex word andjargon identification. Enabled by our high-quality annotation, we benchmark andimprove several state-of-the-art sentence-level readability metrics for themedical domain specifically, which include unsupervised, supervised, andprompting-based methods using recently developed large language models (LLMs).Informed by our fine-grained complex span annotation, we find that adding asingle feature, capturing the number of jargon spans, into existing readabilityformulas can significantly improve their correlation with human judgments. Wewill publicly release the dataset and code.</description><author>Chao Jiang, Wei Xu</author><pubDate>Fri, 03 May 2024 15:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02144v1</guid></item><item><title>Multi-Objective Recommendation via Multivariate Policy Learning</title><link>http://arxiv.org/abs/2405.02141v1</link><description>Real-world recommender systems often need to balance multiple objectives whendeciding which recommendations to present to users. These include behaviouralsignals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.diversity, fairness). Scalarisation methods are commonly used to handle thisbalancing task, where a weighted average of per-objective reward signalsdetermines the final score used for ranking. Naturally, how these weights arecomputed exactly, is key to success for any online platform. We frame this as adecision-making task, where the scalarisation weights are actions taken tomaximise an overall North Star reward (e.g. long-term user retention orgrowth). We extend existing policy learning methods to the continuousmultivariate action domain, proposing to maximise a pessimistic lower bound onthe North Star reward that the learnt policy will yield. Typical lower boundsbased on normal approximations suffer from insufficient coverage, and wepropose an efficient and effective policy-dependent correction for this. Weprovide guidance to design stochastic data collection policies, as well ashighly sensitive reward signals. Empirical observations from simulations,offline and online experiments highlight the efficacy of our deployed approach.</description><author>Olivier Jeunen, Jatin Mandav, Ivan Potapov, Nakul Agarwal, Sourabh Vaid, Wenzhe Shi, Aleksei Ustimenko</author><pubDate>Fri, 03 May 2024 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02141v1</guid></item><item><title>An Information Theoretic Perspective on Conformal Prediction</title><link>http://arxiv.org/abs/2405.02140v1</link><description>Conformal Prediction (CP) is a distribution-free uncertainty estimationframework that constructs prediction sets guaranteed to contain the true answerwith a user-specified probability. Intuitively, the size of the prediction setencodes a general notion of uncertainty, with larger sets associated withhigher degrees of uncertainty. In this work, we leverage information theory toconnect conformal prediction to other notions of uncertainty. More precisely,we prove three different ways to upper bound the intrinsic uncertainty, asdescribed by the conditional entropy of the target variable given the inputs,by combining CP with information theoretical inequalities. Moreover, wedemonstrate two direct and useful applications of such connection betweenconformal prediction and information theory: (i) more principled and effectiveconformal training objectives that generalize previous approaches and enableend-to-end training of machine learning models from scratch, and (ii) a naturalmechanism to incorporate side information into conformal prediction. Weempirically validate both applications in centralized and federated learningsettings, showing our theoretical results translate to lower inefficiency(average prediction set size) for popular CP methods.</description><author>Alvaro H. C. Correia, Fabio Valerio Massoli, Christos Louizos, Arash Behboodi</author><pubDate>Fri, 03 May 2024 15:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02140v1</guid></item><item><title>Unsupervised Learning of Phylogenetic Trees via Split-Weight Embedding</title><link>http://arxiv.org/abs/2312.16074v2</link><description>Unsupervised learning has become a staple in classical machine learning,successfully identifying clustering patterns in data across a broad range ofdomain applications. Surprisingly, despite its accuracy and elegant simplicity,unsupervised learning has not been sufficiently exploited in the realm ofphylogenetic tree inference. The main reason for the delay in adoption ofunsupervised learning in phylogenetics is the lack of a meaningful, yet simple,way of embedding phylogenetic trees into a vector space. Here, we propose thesimple yet powerful split-weight embedding which allows us to fit standardclustering algorithms to the space of phylogenetic trees. We show that oursplit-weight embedded clustering is able to recover meaningful evolutionaryrelationships in simulated and real (Adansonia baobabs) data.</description><author>Yibo Kong, George P. Tiley, Claudia Solis-Lemus</author><pubDate>Fri, 03 May 2024 15:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16074v2</guid></item><item><title>Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection</title><link>http://arxiv.org/abs/2405.02134v1</link><description>Researchers and practitioners operating on a limited budget face thecost-performance trade-off dilemma. The challenging decision often centers onwhether to use a large LLM with better performance or a smaller one withreduced costs. This has motivated recent research in the optimisation of LLMcalls. Either a cascading strategy is used, where a smaller LLM or both arecalled sequentially, or a routing strategy is used, where only one model isever called. Both scenarios are dependent on a decision criterion which istypically implemented by an extra neural model. In this work, we propose asimpler solution; we use only the uncertainty of the generations of the smallLLM as the decision criterion. We compare our approach with both cascading androuting strategies using three different pairs of pre-trained small and largeLLMs, on nine different tasks and against approaches that require an additionalneural model. Our experiments reveal this simple solution optimally balancescost and performance, outperforming existing methods on 25 out of 27experimental setups.</description><author>Guillem Ramírez, Alexandra Birch, Ivan Titov</author><pubDate>Fri, 03 May 2024 15:38:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02134v1</guid></item><item><title>Fairness Without Demographics in Human-Centered Federated Learning</title><link>http://arxiv.org/abs/2404.19725v2</link><description>Federated learning (FL) enables collaborative model training while preservingdata privacy, making it suitable for decentralized human-centered AIapplications. However, a significant research gap remains in ensuring fairnessin these systems. Current fairness strategies in FL require knowledge ofbias-creating/sensitive attributes, clashing with FL's privacy principles.Moreover, in human-centered datasets, sensitive attributes may remain latent.To tackle these challenges, we present a novel bias mitigation approachinspired by "Fairness without Demographics" in machine learning. The presentedapproach achieves fairness without needing knowledge of sensitive attributes byminimizing the top eigenvalue of the Hessian matrix during training, ensuringequitable loss landscapes across FL participants. Notably, we introduce a novelFL aggregation scheme that promotes participating models based on error ratesand loss landscape curvature attributes, fostering fairness across the FLsystem. This work represents the first approach to attaining "Fairness withoutDemographics" in human-centered FL. Through comprehensive evaluation, ourapproach demonstrates effectiveness in balancing fairness and efficacy acrossvarious real-world applications, FL setups, and scenarios involving single andmultiple bias-inducing factors, representing a significant advancement inhuman-centered FL.</description><author>Shaily Roy, Harshit Sharma, Asif Salekin</author><pubDate>Fri, 03 May 2024 15:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19725v2</guid></item><item><title>Learning from Evolution: Improving Collective Decision-Making Mechanisms using Insights from Evolutionary Robotics</title><link>http://arxiv.org/abs/2405.02133v1</link><description>Collective decision-making enables multi-robot systems to act autonomously inreal-world environments. Existing collective decision-making mechanisms sufferfrom the so-called speed versus accuracy trade-off or rely on high complexity,e.g., by including global communication. Recent work has shown that moreefficient collective decision-making mechanisms based on artificial neuralnetworks can be generated using methods from evolutionary computation. A majordrawback of these decision-making neural networks is their limitedinterpretability. Analyzing evolved decision-making mechanisms can help usimprove the efficiency of hand-coded decision-making mechanisms whilemaintaining a higher interpretability. In this paper, we analyze evolvedcollective decision-making mechanisms in detail and hand-code two newdecision-making mechanisms based on the insights gained. In benchmarkexperiments, we show that the newly implemented collective decision-makingmechanisms are more efficient than the state-of-the-art collectivedecision-making mechanisms voter model and majority rule.</description><author>Tanja Katharina Kaiser</author><pubDate>Fri, 03 May 2024 15:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02133v1</guid></item><item><title>Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets</title><link>http://arxiv.org/abs/2405.02132v1</link><description>Large Language Models have demonstrated unparalleled effectiveness in variousNLP tasks, and integrating LLMs with automatic speech recognition is becoming amainstream paradigm. Building upon this momentum, our research delves into anindepth examination of this paradigm on a large opensource Chinese dataset.Specifically, our research aims to evaluate the impact of variousconfigurations of speech encoders, LLMs, and projector modules in the contextof the speech foundation encoderLLM ASR paradigm. Furthermore, we introduce athreestage training approach, expressly developed to enhance the model'sability to align auditory and textual information. The implementation of thisapproach, alongside the strategic integration of ASR components, enabled us toachieve the SOTA performance on the AISHELL1, TestNet, and TestMeeting testsets. Our analysis presents an empirical foundation for future research inLLMbased ASR systems and offers insights into optimizing performance usingChinese datasets. We will publicly release all scripts used for datapreparation, training, inference, and scoring, as well as pretrained models andtraining logs to promote reproducible research.</description><author>Xuelong Geng, Tianyi Xu, Kun Wei, Bingsheng Mu, Hongfei Xue, He Wang, Yangze Li, Pengcheng Guo, Yuhang Dai, Longhao Li, Mingchen Shao, Lei Xie</author><pubDate>Fri, 03 May 2024 15:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02132v1</guid></item><item><title>Physics-informed generative neural networks for RF propagation prediction with application to indoor body perception</title><link>http://arxiv.org/abs/2405.02131v1</link><description>Electromagnetic (EM) body models designed to predict Radio-Frequency (RF)propagation are time-consuming methods which prevent their adoption in strictreal-time computational imaging problems, such as human body localization andsensing. Physics-informed Generative Neural Network (GNN) models have beenrecently proposed to reproduce EM effects, namely to simulate or reconstructmissing data or samples by incorporating relevant EM principles andconstraints. The paper discusses a Variational Auto-Encoder (VAE) model whichis trained to reproduce the effects of human motions on the EM field andincorporate EM body diffraction principles. Proposed physics-informedgenerative neural network models are verified against both classicaldiffraction-based EM tools and full-wave EM body simulations.</description><author>Federica Fieramosca, Vittorio Rampa, Michele D'Amico, Stefano Savazzi</author><pubDate>Fri, 03 May 2024 15:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02131v1</guid></item><item><title>Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry with GPT-4-Turbo</title><link>http://arxiv.org/abs/2405.02128v1</link><description>The rapid advancement in artificial intelligence and natural languageprocessing has led to the development of large-scale datasets aimed atbenchmarking the performance of machine learning models. Herein, we introduce'RetChemQA,' a comprehensive benchmark dataset designed to evaluate thecapabilities of such models in the domain of reticular chemistry. This datasetincludes both single-hop and multi-hop question-answer pairs, encompassingapproximately 45,000 Q&amp;As for each type. The questions have been extracted froman extensive corpus of literature containing about 2,530 research papers frompublishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group,among others. The dataset has been generated using OpenAI's GPT-4 Turbo, acutting-edge model known for its exceptional language understanding andgeneration capabilities. In addition to the Q&amp;A dataset, we also release adataset of synthesis conditions extracted from the corpus of literature used inthis study. The aim of RetChemQA is to provide a robust platform for thedevelopment and evaluation of advanced machine learning algorithms,particularly for the reticular chemistry community. The dataset is structuredto reflect the complexities and nuances of real-world scientific discourse,thereby enabling nuanced performance assessments across a variety of tasks. Thedataset is available at the following link:https://github.com/nakulrampal/RetChemQA</description><author>Nakul Rampal, Kaiyu Wang, Matthew Burigana, Lingxiang Hou, Juri Al-Johani, Anna Sackmann, Hanan S. Murayshid, Walaa Abdullah Al-Sumari, Arwa M. Al-Abdulkarim, Nahla Eid Al-Hazmi, Majed O. Al-Awad, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi</author><pubDate>Fri, 03 May 2024 15:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02128v1</guid></item><item><title>TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on Self-Supervised Learning and Knowledge Transfer</title><link>http://arxiv.org/abs/2405.02124v1</link><description>In this paper, we present a novel approach for text independentphone-to-audio alignment based on phoneme recognition, representation learningand knowledge transfer. Our method leverages a self-supervised model (wav2vec2)fine-tuned for phoneme recognition using a Connectionist TemporalClassification (CTC) loss, a dimension reduction model and a frame-levelphoneme classifier trained thanks to forced-alignment labels (using MontrealForced Aligner) to produce multi-lingual phonetic representations, thusrequiring minimal additional training. We evaluate our model using syntheticnative data from the TIMIT dataset and the SCRIBE dataset for American andBritish English, respectively. Our proposed model outperforms thestate-of-the-art (charsiu) in statistical metrics and has applications inlanguage learning and speech processing systems. We leave experiments on otherlanguages for future work but the design of the system makes it easilyadaptable to other languages.</description><author>Noé Tits, Prernna Bhatnagar, Thierry Dutoit</author><pubDate>Fri, 03 May 2024 15:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02124v1</guid></item><item><title>Can We Identify Unknown Audio Recording Environments in Forensic Scenarios?</title><link>http://arxiv.org/abs/2405.02119v1</link><description>Audio recordings may provide important evidence in criminal investigations.One such case is the forensic association of the recorded audio to therecording location. For example, a voice message may be the only investigativecue to narrow down the candidate sites for a crime. Up to now, several worksprovide tools for closed-set recording environment classification underrelatively clean recording conditions. However, in forensic investigations, thecandidate locations are case-specific. Thus, closed-set tools are notapplicable without retraining on a sufficient amount of training samples foreach case and respective candidate set. In addition, a forensic tool has todeal with audio material from uncontrolled sources with variable properties andquality. In this work, we therefore attempt a major step towards practical forensicapplication scenarios. We propose a representation learning framework calledEnvId, short for environment identification. EnvId avoids case-specificretraining. Instead, it is the first tool for robust few-shot classification ofunseen environment locations. We demonstrate that EnvId can handle forensicallychallenging material. It provides good quality predictions even under unseensignal degradations, environment characteristics or recording positionmismatches. Our code and datasets will be made publicly available upon acceptance.</description><author>Denise Moussa, Germans Hirsch, Christian Riess</author><pubDate>Fri, 03 May 2024 15:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02119v1</guid></item><item><title>Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2405.02114v1</link><description>The accuracy and robustness of 3D human pose estimation (HPE) are limited by2D pose detection errors and 2D to 3D ill-posed challenges, which have drawngreat attention to Multi-Hypothesis HPE research. Most existing MH-HPE methodsare based on generative models, which are computationally expensive anddifficult to train. In this study, we propose a Probabilistic Restoration 3DHuman Pose Estimation framework (PRPose) that can be integrated with anylightweight single-hypothesis model. Specifically, PRPose employs a weaklysupervised approach to fit the hidden probability distribution of the 2D-to-3Dlifting process in the Single-Hypothesis HPE model and then reverse-map thedistribution to the 2D pose input through an adaptive noise sampling strategyto generate reasonable multi-hypothesis samples effectively. Extensiveexperiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight theeffectiveness and efficiency of PRPose. Code is available at:https://github.com/xzhouzeng/PRPose.</description><author>Xianzhou Zeng, Hao Qin, Ming Kong, Luyuan Chen, Qiang Zhu</author><pubDate>Fri, 03 May 2024 15:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02114v1</guid></item><item><title>Three-Dimensional Amyloid-Beta PET Synthesis from Structural MRI with Conditional Generative Adversarial Networks</title><link>http://arxiv.org/abs/2405.02109v1</link><description>Motivation: Alzheimer's Disease hallmarks include amyloid-beta deposits andbrain atrophy, detectable via PET and MRI scans, respectively. PET isexpensive, invasive and exposes patients to ionizing radiation. MRI is cheaper,non-invasive, and free from ionizing radiation but limited to measuring brainatrophy. Goal: To develop an 3D image translation model that synthesizes amyloid-betaPET images from T1-weighted MRI, exploiting the known relationship betweenamyloid-beta and brain atrophy. Approach: The model was trained on 616 PET/MRI pairs and validated with 264pairs. Results: The model synthesized amyloid-beta PET images from T1-weighted MRIwith high-degree of similarity showing high SSIM and PSNR metrics(SSIM&gt;0.95&amp;PSNR=28). Impact: Our model proves the feasibility of synthesizing amyloid-beta PETimages from structural MRI ones, significantly enhancing accessibility forlarge-cohort studies and early dementia detection, while also reducing cost,invasiveness, and radiation exposure.</description><author>Fernando Vega, Abdoljalil Addeh, M. Ethan MacDonald</author><pubDate>Fri, 03 May 2024 15:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02109v1</guid></item><item><title>Practical Performance Guarantees for Pipelined DNN Inference</title><link>http://arxiv.org/abs/2311.03703v2</link><description>We optimize pipeline parallelism for deep neural network (DNN) inference bypartitioning model graphs into $k$ stages and minimizing the running time ofthe bottleneck stage, including communication. We give practical and effectivealgorithms for this NP-hard problem, but our emphasis is on tackling thepractitioner's dilemma of deciding when a solution is good enough. To this end,we design novel mixed-integer programming (MIP) relaxations for proving lowerbounds. Applying these methods to a diverse testbed of 369 production models,for $k \in \{2, 4, 8, 16, 32, 64\}$, we empirically show that these lowerbounds are strong enough to be useful in practice. Our lower bounds aresubstantially stronger than standard combinatorial bounds. For example,evaluated via geometric means across our production testbed with $k = 16$pipeline stages, our MIP formulations raised the lower bound from 0.4598 to0.9452, expressed as a fraction of the best partition found. In other words,our improved lower bounds closed the optimality gap by a factor of 9.855x.</description><author>Aaron Archer, Matthew Fahrbach, Kuikui Liu, Prakash Prabhu</author><pubDate>Fri, 03 May 2024 15:05:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03703v2</guid></item><item><title>Can Large Language Models (or Humans) Disentangle Text?</title><link>http://arxiv.org/abs/2403.16584v2</link><description>We investigate the potential of large language models (LLMs) to disentangletext variables--to remove the textual traces of an undesired forbidden variablein a task sometimes known as text distillation and closely related to thefairness in AI and causal inference literature. We employ a range of variousLLM approaches in an attempt to disentangle text by identifying and removinginformation about a target variable while preserving other relevant signals. Weshow that in the strong test of removing sentiment, the statistical associationbetween the processed text and sentiment is still detectable to machinelearning classifiers post-LLM-disentanglement. Furthermore, we find that humanannotators also struggle to disentangle sentiment while preserving othersemantic content. This suggests there may be limited separability betweenconcept variables in some text contexts, highlighting limitations of methodsrelying on text-level transformations and also raising questions about therobustness of disentanglement methods that achieve statistical independence inrepresentation space.</description><author>Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson</author><pubDate>Fri, 03 May 2024 15:04:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16584v2</guid></item><item><title>Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph</title><link>http://arxiv.org/abs/2405.02105v1</link><description>Structured science summaries or research contributions using properties ordimensions beyond traditional keywords enhances science findability. Currentmethods, such as those used by the Open Research Knowledge Graph (ORKG),involve manually curating properties to describe research papers' contributionsin a structured manner, but this is labor-intensive and inconsistent betweenthe domain expert human curators. We propose using Large Language Models (LLMs)to automatically suggest these properties. However, it's essential to assessthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task beforeapplication. Our study performs a comprehensive comparative analysis betweenORKG's manually curated properties and those generated by the aforementionedstate-of-the-art LLMs. We evaluate LLM performance through four uniqueperspectives: semantic alignment and deviation with ORKG properties,fine-grained properties mapping accuracy, SciNCL embeddings-based cosinesimilarity, and expert surveys comparing manual annotations with LLM outputs.These evaluations occur within a multidisciplinary science setting. Overall,LLMs show potential as recommendation systems for structuring science, butfurther finetuning is recommended to improve their alignment with scientifictasks and mimicry of human expertise.</description><author>Vladyslav Nechakhin, Jennifer D'Souza, Steffen Eger</author><pubDate>Fri, 03 May 2024 15:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02105v1</guid></item><item><title>Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning</title><link>http://arxiv.org/abs/2305.16602v2</link><description>Learning to infer labels in an open world, i.e., in an environment where thetarget ``labels'' are unknown, is an important characteristic for achievingautonomy. Foundation models, pre-trained on enormous amounts of data, haveshown remarkable generalization skills through prompting, particularly inzero-shot inference. However, their performance is restricted to thecorrectness of the target label's search space, i.e., candidate labels providedin the prompt. This target search space can be unknown or exceptionally largein an open world, severely restricting their performance. To tackle thischallenging problem, we propose a two-step, neuro-symbolic framework calledALGO - Action Learning with Grounded Object recognition that uses symbolicknowledge stored in large-scale knowledge bases to infer activities inegocentric videos with limited supervision. First, we propose a neuro-symbolicprompting approach that uses object-centric vision-language models as a noisyoracle to ground objects in the video through evidence-based reasoning. Second,driven by prior commonsense knowledge, we discover plausible activities throughan energy-based symbolic pattern theory framework and learn to groundknowledge-based action (verb) concepts in the video. Extensive experiments onfour publicly available datasets (EPIC-Kitchens, GTEA Gaze, GTEA Gaze Plus, andCharades-Ego) demonstrate its performance on open-world activity inference. Wealso show that ALGO can be extended to zero-shot inference and demonstrate itscompetitive performance on the Charades-Ego dataset.</description><author>Sanjoy Kundu, Shubham Trehan, Sathyanarayanan N. Aakur</author><pubDate>Fri, 03 May 2024 15:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16602v2</guid></item><item><title>Reconstructions of Jupiter's magnetic field using physics informed neural networks</title><link>http://arxiv.org/abs/2403.07507v2</link><description>Magnetic sounding using data collected from the Juno mission can be used toprovide constraints on Jupiter's interior. However, inwards continuation ofreconstructions assuming zero electrical conductivity and a representation inspherical harmonics are limited by the enhancement of noise at small scales.Here we describe new reconstructions of Jupiter's internal magnetic field basedon physics-informed neural networks and either the first 33 (PINN33) or thefirst 50 (PINN50) of Juno's orbits. The method can resolve local structures,and allows for weak ambient electrical currents. Our models are not hampered bynoise amplification at depth, and offer a much clearer picture of the interiorstructure. We estimate that the dynamo boundary is at a fractional radius of0.8. At this depth, the magnetic field is arranged into longitudinal bands, andstrong local features such as the great blue spot appear to be rooted inneighbouring structures of oppositely signed flux.</description><author>Philip W. Livermore, Leyuan Wu, Longwei Chen, Sjoerd A. L. de Ridder</author><pubDate>Fri, 03 May 2024 15:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07507v2</guid></item><item><title>Compressing neural network by tensor network with exponentially fewer variational parameters</title><link>http://arxiv.org/abs/2305.06058v2</link><description>Neural network (NN) designed for challenging machine learning tasks is ingeneral a highly nonlinear mapping that contains massive variationalparameters. High complexity of NN, if unbounded or unconstrained, mightunpredictably cause severe issues including over-fitting, loss ofgeneralization power, and unbearable cost of hardware. In this work, we proposea general compression scheme that significantly reduces the variationalparameters of NN by encoding them to deep automatically-differentiable tensornetwork (ADTN) that contains exponentially-fewer free parameters. Superiorcompression performance of our scheme is demonstrated on severalwidely-recognized NN's (FC-2, LeNet-5, AlextNet, ZFNet and VGG-16) and datasets(MNIST, CIFAR-10 and CIFAR-100). For instance, we compress two linear layers inVGG-16 with approximately $10^{7}$ parameters to two ADTN's with just 424parameters, where the testing accuracy on CIFAR-10 is improved from $90.17 \%$to $91.74\%$. Our work suggests TN as an exceptionally efficient mathematicalstructure for representing the variational parameters of NN's, which exhibitssuperior compressibility over the commonly-used matrices and multi-way arrays.</description><author>Yong Qing, Ke Li, Peng-Fei Zhou, Shi-Ju Ran</author><pubDate>Fri, 03 May 2024 14:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06058v2</guid></item><item><title>Discrete Aware Matrix Completion via Convexized $\ell_0$-Norm Approximation</title><link>http://arxiv.org/abs/2405.02101v1</link><description>We consider a novel algorithm, for the completion of partially observedlow-rank matrices in a structured setting where each entry can be chosen from afinite discrete alphabet set, such as in common recommender systems. Theproposed low-rank matrix completion (MC) method is an improved variation ofstate-of-the-art (SotA) discrete aware matrix completion method which wepreviously proposed, in which discreteness is enforced by an $\ell_0$-normregularizer, not by replaced with the $\ell_1$-norm, but instead approximatedby a continuous and differentiable function normalized via fractionalprogramming (FP) under a proximal gradient (PG) framework. Simulation resultsdemonstrate the superior performance of the new method compared to the SotAtechniques as well as the earlier $\ell_1$-norm-based discrete-aware matrixcompletion approach.</description><author>Niclas Führling, Kengo Ando, Giuseppe Thadeu Freitas de Abreu, David González G., Osvaldo Gonsa</author><pubDate>Fri, 03 May 2024 14:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02101v1</guid></item><item><title>Explainable Light-Weight Deep Learning Pipeline for Improved Drought Stress Identification</title><link>http://arxiv.org/abs/2404.10073v2</link><description>Early identification of drought stress in crops is vital for implementingeffective mitigation measures and reducing yield loss. Non-invasive imagingtechniques hold immense potential by capturing subtle physiological changes inplants under water deficit. Sensor based imaging data serves as a rich sourceof information for machine learning and deep learning algorithms, facilitatingfurther analysis aimed at identifying drought stress. While these approachesyield favorable results, real-time field applications requires algorithmsspecifically designed for the complexities of natural agricultural conditions.Our work proposes a novel deep learning framework for classifying droughtstress in potato crops captured by UAVs in natural settings. The novelty liesin the synergistic combination of a pre-trained network with carefully designedcustom layers. This architecture leverages feature extraction capabilities ofthe pre-trained network while the custom layers enable targeted dimensionalityreduction and enhanced regularization, ultimately leading to improvedperformance. A key innovation of our work involves the integration ofGradient-Class Activation Mapping (Grad-CAM), an explainability technique.Grad-CAM sheds light on the internal workings of the deep learning model,typically referred to as a black box. By visualizing the focus areas of themodel within the images, Grad-CAM fosters interpretability and builds trust inthe decision-making process of the model. Our proposed framework achievessuperior performance, particularly with the DenseNet121 pre-trained network,reaching a precision of 97% to identify the stressed class with an overallaccuracy of 91%. Comparative analysis of existing state-of-the-art objectdetection algorithms reveals the superiority of our approach in significantlyhigher precision and accuracy.</description><author>Aswini Kumar Patra, Lingaraj Sahoo</author><pubDate>Fri, 03 May 2024 14:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10073v2</guid></item><item><title>Forecasting Ferry Passenger Flow Using Long-Short Term Memory Neural Networks</title><link>http://arxiv.org/abs/2405.02098v1</link><description>With recent studies related to Neural Networks being used on differentforecasting and time series investigations, this study aims to expand thesecontexts to ferry passenger traffic. The primary objective of the study is toinvestigate and evaluate an LSTM-based Neural Networks' capability to forecastferry passengers of two ports in the Philippines. The proposed model's fittingand evaluation of the passenger flow forecasting of the two ports is based onmonthly passenger traffic from 2016 to 2022 data that was acquired from thePhilippine Ports Authority (PPA). This work uses Mean Absolute Percentage Error(MAPE) as its primary metric to evaluate the model's forecasting capability.The proposed LSTM-based Neural Networks model achieved 72% forecasting accuracyto the Batangas port ferry passenger data and 74% forecasting accuracy to theMindoro port ferry passenger data. Using Keras and Scikit-learn Pythonlibraries, this work concludes a reasonable forecasting performance of thepresented LSTM model. Aside from these notable findings, this study alsorecommends further investigation and studies on employing other statistical,machine learning, and deep learning methods on forecasting ferry passengerflows.</description><author>Daniel Fesalbon</author><pubDate>Fri, 03 May 2024 14:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02098v1</guid></item><item><title>Advanced Detection of Source Code Clones via an Ensemble of Unsupervised Similarity Measures</title><link>http://arxiv.org/abs/2405.02095v1</link><description>The capability of accurately determining code similarity is crucial in manytasks related to software development. For example, it might be essential toidentify code duplicates for performing software maintenance. This researchintroduces a novel ensemble learning approach for code similarity assessment,combining the strengths of multiple unsupervised similarity measures. The keyidea is that the strengths of a diverse set of similarity measures cancomplement each other and mitigate individual weaknesses, leading to improvedperformance. Preliminary results show that while Transformers-based CodeBERTand its variant GraphCodeBERT are undoubtedly the best option in the presenceof abundant training data, in the case of specific small datasets (up to 500samples), our ensemble achieves similar results, without prejudice to theinterpretability of the resulting solution, and with a much lower associatedcarbon footprint due to training. The source code of this novel approach can bedownloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.</description><author>Jorge Martinez-Gil</author><pubDate>Fri, 03 May 2024 14:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02095v1</guid></item><item><title>Human-AI Coevolution</title><link>http://arxiv.org/abs/2306.13723v2</link><description>Human-AI coevolution, defined as a process in which humans and AI algorithmscontinuously influence each other, increasingly characterises our society, butis understudied in artificial intelligence and complexity science literature.Recommender systems and assistants play a prominent role in human-AIcoevolution, as they permeate many facets of daily life and influence humanchoices on online platforms. The interaction between users and AI results in apotentially endless feedback loop, wherein users' choices generate data totrain AI models, which, in turn, shape subsequent user preferences. Thishuman-AI feedback loop has peculiar characteristics compared to traditionalhuman-machine interaction and gives rise to complex and often ``unintended''social outcomes. This paper introduces Coevolution AI as the cornerstone for anew field of study at the intersection between AI and complexity sciencefocused on the theoretical, empirical, and mathematical investigation of thehuman-AI feedback loop. In doing so, we: (i) outline the pros and cons ofexisting methodologies and highlight shortcomings and potential ways forcapturing feedback loop mechanisms; (ii) propose a reflection at theintersection between complexity science, AI and society; (iii) providereal-world examples for different human-AI ecosystems; and (iv) illustratechallenges to the creation of such a field of study, conceptualising them atincreasing levels of abstraction, i.e., technical, epistemological, legal andsocio-political.</description><author>Dino Pedreschi, Luca Pappalardo, Emanuele Ferragina, Ricardo Baeza-Yates, Albert-Laszlo Barabasi, Frank Dignum, Virginia Dignum, Tina Eliassi-Rad, Fosca Giannotti, Janos Kertesz, Alistair Knott, Yannis Ioannidis, Paul Lukowicz, Andrea Passarella, Alex Sandy Pentland, John Shawe-Taylor, Alessandro Vespignani</author><pubDate>Fri, 03 May 2024 14:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13723v2</guid></item><item><title>Process Mining Embeddings: Learning Vector Representations for Petri Nets</title><link>http://arxiv.org/abs/2404.17129v2</link><description>Process mining offers powerful techniques for discovering, analyzing, andenhancing real-world business processes. In this context, Petri nets provide anexpressive means of modeling process behavior. However, directly analyzing andcomparing intricate Petri net presents challenges. This study introducesPetriNet2Vec, a novel unsupervised methodology based on Natural LanguageProcessing concepts inspired by Doc2Vec and designed to facilitate theeffective comparison, clustering, and classification of process modelsrepresented as embedding vectors. These embedding vectors allow us to quantifysimilarities and relationships between different process models. Ourmethodology was experimentally validated using the PDC Dataset, featuring 96diverse Petri net models. We performed cluster analysis, created UMAPvisualizations, and trained a decision tree to provide compelling evidence forthe capability of PetriNet2Vec to discern meaningful patterns and relationshipsamong process models and their constituent tasks. Through a series ofexperiments, we demonstrated that PetriNet2Vec was capable of learning thestructure of Petri nets, as well as the main properties used to simulate theprocess models of our dataset. Furthermore, our results showcase the utility ofthe learned embeddings in two crucial downstream tasks within process miningenhancement: process classification and process retrieval.</description><author>Juan G. Colonna, Ahmed A. Fares, Márcio Duarte, Ricardo Sousa</author><pubDate>Fri, 03 May 2024 14:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17129v2</guid></item><item><title>On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box</title><link>http://arxiv.org/abs/2308.09381v2</link><description>Attribution methods shed light on the explainability of data-drivenapproaches such as deep learning models by uncovering the most influentialfeatures in a to-be-explained decision. While determining feature attributionsvia gradients delivers promising results, the internal access required foracquiring gradients can be impractical under safety concerns, thus limiting theapplicability of gradient-based approaches. In response to such limitedflexibility, this paper presents \methodAbr~(gradient-estimation-basedexplanation), an approach that produces gradient-like explanations through onlyquery-level access. The proposed approach holds a set of fundamental propertiesfor attribution methods, which are mathematically rigorously proved, ensuringthe quality of its explanations. In addition to the theoretical analysis, witha focus on image data, the experimental results empirically demonstrate thesuperiority of the proposed method over state-of-the-art black-box methods andits competitive performance compared to methods with full access.</description><author>Yi Cai, Gerhard Wunder</author><pubDate>Fri, 03 May 2024 14:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09381v2</guid></item><item><title>Multi-level projection with exponential parallel speedup; Application to sparse auto-encoders neural networks</title><link>http://arxiv.org/abs/2405.02086v1</link><description>The $\ell_{1,\infty}$ norm is an efficient structured projection but thecomplexity of the best algorithm is unfortunately $\mathcal{O}\big(n m \log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. In this paper, we propose anew bi-level projection method for which we show that the time complexity forthe $\ell_{1,\infty}$ norm is only $\mathcal{O}\big(n m \big)$ for a matrix in$\mathbb{R}^{n\times m}$, and $\mathcal{O}\big(n + m \big)$ with full parallelpower. We generalize our method to tensors and we propose a new multi-levelprojection, having an induced decomposition that yields a linear parallelspeedup up to an exponential speedup factor, resulting in a time complexitylower-bounded by the sum of the dimensions. Experiments show that our bi-level$\ell_{1,\infty}$ projection is $2.5$ times faster than the actual fastestalgorithm provided by \textit{Chu et. al.} while providing same accuracy andbetter sparsity in neural networks applications.</description><author>Guillaume Perez, Michel Barlaud</author><pubDate>Fri, 03 May 2024 14:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02086v1</guid></item><item><title>A semantic loss for ontology classification</title><link>http://arxiv.org/abs/2405.02083v1</link><description>Deep learning models are often unaware of the inherent constraints of thetask they are applied to. However, many downstream tasks require logicalconsistency. For ontology classification tasks, such constraints includesubsumption and disjointness relations between classes. In order to increase the consistency of deep learning models, we propose asemantic loss that combines label-based loss with terms penalising subsumption-or disjointness-violations. Our evaluation on the ChEBI ontology shows that thesemantic loss is able to decrease the number of consistency violations byseveral orders of magnitude without decreasing the classification performance.In addition, we use the semantic loss for unsupervised learning. We show thatthis can further improve consistency on data from a distribution outside thescope of the supervised training.</description><author>Simon Flügel, Martin Glauer, Till Mossakowski, Fabian Neuhaus</author><pubDate>Fri, 03 May 2024 14:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02083v1</guid></item><item><title>A comparative study of conformal prediction methods for valid uncertainty quantification in machine learning</title><link>http://arxiv.org/abs/2405.02082v1</link><description>In the past decades, most work in the area of data analysis and machinelearning was focused on optimizing predictive models and getting better resultsthan what was possible with existing models. To what extent the metrics withwhich such improvements were measured were accurately capturing the intendedgoal, whether the numerical differences in the resulting values weresignificant, or whether uncertainty played a role in this study and if itshould have been taken into account, was of secondary importance. Whereasprobability theory, be it frequentist or Bayesian, used to be the gold standardin science before the advent of the supercomputer, it was quickly replaced infavor of black box models and sheer computing power because of their ability tohandle large data sets. This evolution sadly happened at the expense ofinterpretability and trustworthiness. However, while people are still trying toimprove the predictive power of their models, the community is starting torealize that for many applications it is not so much the exact prediction thatis of importance, but rather the variability or uncertainty. The work in this dissertation tries to further the quest for a world whereeveryone is aware of uncertainty, of how important it is and how to embrace itinstead of fearing it. A specific, though general, framework that allows anyoneto obtain accurate uncertainty estimates is singled out and analysed. Certainaspects and applications of the framework -- dubbed `conformal prediction' --are studied in detail. Whereas many approaches to uncertainty quantificationmake strong assumptions about the data, conformal prediction is, at the time ofwriting, the only framework that deserves the title `distribution-free'. Noparametric assumptions have to be made and the nonparametric results also holdwithout having to resort to the law of large numbers in the asymptotic regime.</description><author>Nicolas Dewolf</author><pubDate>Fri, 03 May 2024 14:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02082v1</guid></item><item><title>A Mutual Information Perspective on Federated Contrastive Learning</title><link>http://arxiv.org/abs/2405.02081v1</link><description>We investigate contrastive learning in the federated setting through the lensof SimCLR and multi-view mutual information maximization. In doing so, weuncover a connection between contrastive representation learning and userverification; by adding a user verification loss to each client's local SimCLRloss we recover a lower bound to the global multi-view mutual information. Toaccommodate for the case of when some labelled data are available at theclients, we extend our SimCLR variant to the federated semi-supervised setting.We see that a supervised SimCLR objective can be obtained with two changes: a)the contrastive loss is computed between datapoints that share the same labeland b) we require an additional auxiliary head that predicts the correct labelsfrom either of the two views. Along with the proposed SimCLR extensions, wealso study how different sources of non-i.i.d.-ness can impact the performanceof federated unsupervised learning through global mutual informationmaximization; we find that a global objective is beneficial for some sources ofnon-i.i.d.-ness but can be detrimental for others. We empirically evaluate ourproposed extensions in various tasks to validate our claims and furthermoredemonstrate that our proposed modifications generalize to other pretrainingmethods.</description><author>Christos Louizos, Matthias Reisser, Denis Korzhenkov</author><pubDate>Fri, 03 May 2024 14:15:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02081v1</guid></item><item><title>Fisher Mask Nodes for Language Model Merging</title><link>http://arxiv.org/abs/2403.09891v3</link><description>Fine-tuning pre-trained models provides significant advantages in downstreamperformance. The ubiquitous nature of pre-trained models such as BERT and itsderivatives in natural language processing has also led to a proliferation oftask-specific fine-tuned models. As these models typically only perform onetask well, additional training or ensembling is required in multi-taskscenarios. The growing field of model merging provides a solution, dealing withthe challenge of combining multiple task-specific models into a singlemulti-task model. In this study, we introduce a novel model merging method forTransformers, combining insights from previous work in Fisher-weightedaveraging and the use of Fisher information in model pruning. Utilizing theFisher information of mask nodes within the Transformer architecture, we devisea computationally efficient weighted-averaging scheme. Our method exhibits aregular and significant performance increase across various models in the BERTfamily, outperforming full-scale Fisher-weighted averaging in a fraction of thecomputational cost, with baseline performance improvements of up to +6.5 and aspeedup between 57.4x and 321.7x across models. Our results prove the potentialof our method in current multi-task learning environments and suggest itsscalability and adaptability to new model architectures and learning scenarios.</description><author>Thennal D K, Ganesh Nathan, Suchithra M S</author><pubDate>Fri, 03 May 2024 14:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09891v3</guid></item></channel></rss>