<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 19 Aug 2024 01:00:22 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing</title><link>http://arxiv.org/abs/2408.02558v3</link><description>With the EU AI Act effective from 1 August 2024, high-risk applications likecredit scoring must adhere to stringent transparency and quality standards,including algorithmic fairness evaluations. Consequently, developing tools forauditing algorithmic fairness has become crucial. This paper addresses a keyquestion: how can we scientifically audit algorithmic fairness? It is vital todetermine whether adverse decisions result from algorithmic discrimination orthe subjects' inherent limitations. We introduce a novel auditing framework,``peer-induced fairness'', leveraging counterfactual fairness and advancedcausal inference techniques within credit approval systems. Our approachassesses fairness at the individual level through peer comparisons, independentof specific AI methodologies. It effectively tackles challenges like datascarcity and imbalance, common in traditional models, particularly in creditapproval. Model-agnostic and flexible, the framework functions as both aself-audit tool for stakeholders and an external audit tool for regulators,offering ease of integration. It also meets the EU AI Act's transparencyrequirements by providing clear feedback on whether adverse decisions stem frompersonal capabilities or discrimination. We demonstrate the framework'susefulness by applying it to SME credit approval, revealing significant bias:41.51% of micro-firms face discrimination compared to non-micro firms. Thesefindings highlight the framework's potential for diverse AI applications.</description><author>Shiqi Fang, Zexun Chen, Jake Ansell</author><pubDate>Fri, 16 Aug 2024 12:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02558v3</guid></item><item><title>Covert Bias: The Severity of Social Views' Unalignment in Language Models Towards Implicit and Explicit Opinion</title><link>http://arxiv.org/abs/2408.08212v2</link><description>While various approaches have recently been studied for bias identification,little is known about how implicit language that does not explicitly convey aviewpoint affects bias amplification in large language models. To examine theseverity of bias toward a view, we evaluated the performance of two downstreamtasks where the implicit and explicit knowledge of social groups were used.First, we present a stress test evaluation by using a biased model in edgecases of excessive bias scenarios. Then, we evaluate how LLMs calibratelinguistically in response to both implicit and explicit opinions when they arealigned with conflicting viewpoints. Our findings reveal a discrepancy in LLMperformance in identifying implicit and explicit opinions, with a generaltendency of bias toward explicit opinions of opposing stances. Moreover, thebias-aligned models generate more cautious responses using uncertainty phrasescompared to the unaligned (zero-shot) base models. The direct, incautiousresponses of the unaligned models suggest a need for further refinement ofdecisiveness by incorporating uncertainty markers to enhance their reliability,especially on socially nuanced topics with high subjectivity.</description><author>Abeer Aldayel, Areej Alokaili, Rehab Alahmadi</author><pubDate>Fri, 16 Aug 2024 11:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08212v2</guid></item><item><title>Beyond Full Label: Single-Point Prompt for Infrared Small Target Label Generation</title><link>http://arxiv.org/abs/2408.08191v2</link><description>In this work, we make the first attempt to construct a learning-basedsingle-point annotation paradigm for infrared small target label generation(IRSTLG). Our intuition is that label generation requires just one more pointprompt than target detection: IRSTLG can be regarded as an infrared smalltarget detection (IRSTD) task with the target location hint. Based on thisinsight, we introduce an energy double guided single-point prompt (EDGSP)framework, which adeptly transforms the target detection network into a refinedlabel generation method. Specifically, the proposed EDGSP includes: 1) targetenergy initialization (TEI) to create a foundational outline for sufficientshape evolution of pseudo label, 2) double prompt embedding (DPE) for rapidlocalization of interested regions and reinforcement of individual differencesto avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminatefalse alarms. Experimental results show that pseudo labels generated by threebaselines equipped with EDGSP achieve 100% object-level probability ofdetection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1kdatasets, with a pixel-level intersection over union (IoU) improvement of13.28% over state-of-the-art (SOTA) label generation methods. In the practicalapplication of downstream IRSTD, EDGSP realizes, for the first time, asingle-point generated pseudo mask beyond the full label. Even with coarsesingle-point annotations, it still achieves 99.5% performance of full labeling.</description><author>Shuai Yuan, Hanlin Qin, Renke Kou, Xiang Yan, Zechuan Li, Chenxu Peng, Abd-Krim Seghouane</author><pubDate>Fri, 16 Aug 2024 11:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08191v2</guid></item><item><title>MathBridge: A Large Corpus Dataset for Translating Spoken Mathematical Expressions into $LaTeX$ Formulas for Improved Readability</title><link>http://arxiv.org/abs/2408.07081v3</link><description>Improving the readability of mathematical expressions in text-based documentsuch as subtitle of mathematical video, is an significant task. To achievethis, mathematical expressions should be convert to compiled formulas. Forinstance, the spoken expression ``x equals minus b plus or minus the squareroot of b squared minus four a c, all over two a'' from automatic speechrecognition is more readily comprehensible when displayed as a compiled formula$x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. To convert mathematical spokensentences to compiled formulas, two processes are required: spoken sentencesare converted into LaTeX formulas, and LaTeX formulas are converted intocompiled formulas. The latter can be managed by using LaTeX engines. However,there is no way to do the former effectively. Even if we try to solve thisusing language models, there is no paired data between spoken sentences andLaTeX formulas to train it. In this paper, we introduce MathBridge, the firstextensive dataset for translating mathematical spoken sentences into LaTeXformulas. MathBridge comprises approximately 23 million LaTeX formulas pairedwith the corresponding mathematical spoken sentences. Through comprehensiveevaluations, including fine-tuning with proposed data, we discovered thatMathBridge significantly enhances the capabilities of pretrained languagemodels for converting to LaTeX formulas from mathematical spoken sentences.Specifically, for the T5-large model, the sacreBLEU score increased from 4.77to 46.8, demonstrating substantial enhancement.</description><author>Kyudan Jung, Sieun Hyeon, Jeong Youn Kwon, Nam-Joon Kim, Hyun Gon Ryu, Hyuk-Jae Lee, Jaeyoung Do</author><pubDate>Fri, 16 Aug 2024 09:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07081v3</guid></item><item><title>Adaptive Learning of Consistency and Inconsistency Information for Fake News Detection</title><link>http://arxiv.org/abs/2408.08013v2</link><description>The rapid advancement of social media platforms has significantly reduced thecost of information dissemination, yet it has also led to a proliferation offake news, posing a threat to societal trust and credibility. Most of fake newsdetection research focused on integrating text and image information torepresent the consistency of multiple modes in news content, while paying lessattention to inconsistent information. Besides, existing methods that leveragedinconsistent information often caused one mode overshadowing another, leadingto ineffective use of inconsistent clue. To address these issues, we propose anadaptive multi-modal feature fusion network (MFF-Net). Inspired by humanjudgment processes for determining truth and falsity in news, MFF-Net focuseson inconsistent parts when news content is generally consistent and consistentparts when it is generally inconsistent. Specifically, MFF-Net extractssemantic and global features from images and texts respectively, and learnsconsistency information between modes through a multiple feature fusion module.To deal with the problem of modal information being easily masked, we design asingle modal feature filtering strategy to capture inconsistent informationfrom corresponding modes separately. Finally, similarity scores are calculatedbased on global features with adaptive adjustments made to achieve weightedfusion of consistent and inconsistent features. Extensive experimental resultsdemonstrate that MFF-Net outperforms state-of-the-art methods across threepublic news datasets derived from real social medias.</description><author>Aohan Li, Jiaxin Chen, Xin Liao, Dengyong Zhang</author><pubDate>Fri, 16 Aug 2024 09:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08013v2</guid></item><item><title>Data-driven identification of latent port-Hamiltonian systems</title><link>http://arxiv.org/abs/2408.08185v2</link><description>Conventional physics-based modeling techniques involve high effort, e.g.,time and expert knowledge, while data-driven methods often lackinterpretability, structure, and sometimes reliability. To mitigate this, wepresent a data-driven system identification framework that derives models inthe port-Hamiltonian (pH) formulation. This formulation is suitable formulti-physical systems while guaranteeing the useful system theoreticalproperties of passivity and stability. Our framework combines linear andnonlinear reduction with structured, physics-motivated system identification.In this process, high-dimensional state data obtained from possibly nonlinearsystems serves as input for an autoencoder, which then performs two tasks: (i)nonlinearly transforming and (ii) reducing this data onto a low-dimensionallatent space. In this space, a linear pH system, that satisfies the pHproperties per construction, is parameterized by the weights of a neuralnetwork. The mathematical requirements are met by defining the pH matricesthrough Cholesky factorizations. The neural networks that define the coordinatetransformation and the pH system are identified in a joint optimization processto match the dynamics observed in the data while defining a linear pH system inthe latent space. The learned, low-dimensional pH system can describe evennonlinear systems and is rapidly computable due to its small size. The methodis exemplified by a parametric mass-spring-damper and a nonlinear pendulumexample, as well as the high-dimensional model of a disc brake with linearthermoelastic behavior.</description><author>Johannes Rettberg, Jonas Kneifl, Julius Herb, Patrick Buchfink, Jörg Fehr, Bernard Haasdonk</author><pubDate>Fri, 16 Aug 2024 07:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08185v2</guid></item><item><title>DeepClair: Utilizing Market Forecasts for Effective Portfolio Selection</title><link>http://arxiv.org/abs/2407.13427v3</link><description>Utilizing market forecasts is pivotal in optimizing portfolio selectionstrategies. We introduce DeepClair, a novel framework for portfolio selection.DeepClair leverages a transformer-based time-series forecasting model topredict market trends, facilitating more informed and adaptable portfoliodecisions. To integrate the forecasting model into a deep reinforcementlearning-driven portfolio selection framework, we introduced a two-stepstrategy: first, pre-training the time-series model on market data, followed byfine-tuning the portfolio selection architecture using this model.Additionally, we investigated the optimization technique, Low-Rank Adaptation(LoRA), to enhance the pre-trained forecasting model for fine-tuning ininvestment scenarios. This work bridges market forecasting and portfolioselection, facilitating the advancement of investment strategies.</description><author>Donghee Choi, Jinkyu Kim, Mogan Gim, Jinho Lee, Jaewoo Kang</author><pubDate>Fri, 16 Aug 2024 06:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13427v3</guid></item><item><title>FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance</title><link>http://arxiv.org/abs/2408.08189v2</link><description>Synthesizing motion-rich and temporally consistent videos remains a challengein artificial intelligence, especially when dealing with extended durations.Existing text-to-video (T2V) models commonly employ spatial cross-attention fortext control, equivalently guiding different frame generations withoutframe-specific textual guidance. Thus, the model's capacity to comprehend thetemporal logic conveyed in prompts and generate videos with coherent motion isrestricted. To tackle this limitation, we introduce FancyVideo, an innovativevideo generator that improves the existing text-control mechanism with thewell-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGMincorporates the Temporal Information Injector (TII), Temporal Affinity Refiner(TAR), and Temporal Feature Booster (TFB) at the beginning, middle, and end ofcross-attention, respectively, to achieve frame-specific textual guidance.Firstly, TII injects frame-specific information from latent features into textconditions, thereby obtaining cross-frame textual conditions. Then, TAR refinesthe correlation matrix between cross-frame textual conditions and latentfeatures along the time dimension. Lastly, TFB boosts the temporal consistencyof latent features. Extensive experiments comprising both quantitative andqualitative evaluations demonstrate the effectiveness of FancyVideo. Our videodemo, code and model are available at https://360cvgroup.github.io/FancyVideo/.</description><author>Jiasong Feng, Ao Ma, Jing Wang, Bo Cheng, Xiaodan Liang, Dawei Leng, Yuhui Yin</author><pubDate>Fri, 16 Aug 2024 06:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08189v2</guid></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>http://arxiv.org/abs/2408.07930v2</link><description>Recent In-Context Learning based methods have achieved remarkable success inText-to-SQL task. However, there is still a large gap between the performanceof these models and human performance on datasets with complex database schemaand difficult questions, such as BIRD. Besides, existing work has neglected tosupervise intermediate steps when solving questions iteratively with questiondecomposition methods, and the schema linking methods used in these works arevery rudimentary. To address these issues, we propose MAG-SQL, a multi-agentgenerative approach with soft schema linking and iterative Sub-SQL refinement.In our framework, an entity-based method with tables' summary is used to selectthe columns in database, and a novel targets-conditions decomposition method isintroduced to decompose those complex questions. Additionally, we build aiterative generating module which includes a Sub-SQL Generator and Sub-SQLRefiner, introducing external oversight for each step of generation. Through aseries of ablation studies, the effectiveness of each agent in our frameworkhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQLachieves an execution accuracy of 61.08%, compared to the baseline accuracy of46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.Besides, our approach makes similar progress on Spider.</description><author>Wenxuan Xie, Gaochen Wu, Bowen Zhou</author><pubDate>Fri, 16 Aug 2024 02:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07930v2</guid></item><item><title>OC3D: Weakly Supervised Outdoor 3D Object Detection with Only Coarse Click Annotation</title><link>http://arxiv.org/abs/2408.08092v2</link><description>LiDAR-based outdoor 3D object detection has received widespread attention.However, training 3D detectors from the LiDAR point cloud typically relies onexpensive bounding box annotations. This paper presents OC3D, an innovativeweakly supervised method requiring only coarse clicks on the bird's eye view ofthe 3D point cloud. A key challenge here is the absence of complete geometricdescriptions of the target objects from such simple click annotations. Toaddress this problem, our proposed OC3D adopts a two-stage strategy. In thefirst stage, we initially design a novel dynamic and static classificationstrategy and then propose the Click2Box and Click2Mask modules to generatebox-level and mask-level pseudo-labels for static and dynamic instances,respectively. In the second stage, we design a Mask2Box module, leveraging thelearning capabilities of neural networks to update mask-level pseudo-labels,which contain less information, to box-level pseudo-labels. Experimentalresults on the widely used KITTI and nuScenes datasets demonstrate that ourOC3D with only coarse clicks achieves state-of-the-art performance compared toweakly-supervised 3D detection methods. Combining OC3D with a missing clickmining strategy, we propose an OC3D++ pipeline, which requires only 0.2%annotation cost in the KITTI dataset to achieve performance comparable to fullysupervised methods. The code will be made publicly available.</description><author>Qiming Xia, Hongwei Lin, Wei Ye, Hai Wu, Yadan Luo, Shijia Zhao, Xin Li, Chenglu Wen</author><pubDate>Fri, 16 Aug 2024 02:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08092v2</guid></item><item><title>Distilling Reasoning Ability from Large Language Models with Adaptive Thinking</title><link>http://arxiv.org/abs/2404.09170v5</link><description>Chain of thought finetuning (cot-finetuning) aims to endow small languagemodels (SLM) with reasoning ability to improve their performance towardsspecific tasks by allowing them to imitate the reasoning procedure of largelanguage models (LLM) beyond simply predicting the answers. Most existingcot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM togenerate a rationale before providing an answer. This mechanism enables SLM toanalyze and think about complex questions, but it also makes answer correctnesshighly sensitive to minor errors in rationale. Therefore, we propose a robustpost-thinking mechanism to generate answers before rationale. Thanks to thisanswer-first setting, 1) the answer can escape from the adverse effects causedby minor errors in the rationale; 2) the rationale serves as an error amplifierto the answer, which makes the SLM focus on learning hard samples; 3) theinferring efficiency can also benefit from the setting since users can stop thegeneration right after answers are outputted when inference is conducted.However, although the post-thinking mechanism brings many advantages andimproves the overall performance of SLM on specific tasks, it may lose theability to think about the questions and decompose complex questions intosimple sub-questions compared to pre-thinking mechanism. Therefore, aplug-and-play adaptive-thinking mechanism is proposed with the aid of the softprompt tuning to integrate the merits of the pre-thinking mechanism andpost-thinking mechanism, in which a perception module is introduced toadaptively prompt SLM answer or think first based on perceiving the complexityof the questions. Extensive experiments are conducted across 12 reasoning tasksand 2 representative language models to demonstrate the effectiveness of theproposed mechanism.</description><author>Xiaoshu Chen, Sihang Zhou, Ke Liang, Xinwang Liu</author><pubDate>Fri, 16 Aug 2024 02:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09170v5</guid></item><item><title>Exploring learning environments for label\-efficient cancer diagnosis</title><link>http://arxiv.org/abs/2408.07988v2</link><description>Despite significant research efforts and advancements, cancer remains aleading cause of mortality. Early cancer prediction has become a crucial focusin cancer research to streamline patient care and improve treatment outcomes.Manual tumor detection by histopathologists can be time consuming, promptingthe need for computerized methods to expedite treatment planning. Traditionalapproaches to tumor detection rely on supervised learning, necessitates a largeamount of annotated data for model training. However, acquiring such extensivelabeled data can be laborious and time\-intensive. This research examines thethree learning environments: supervised learning (SL), semi\-supervisedlearning (Semi\-SL), and self\-supervised learning (Self\-SL): to predictkidney, lung, and breast cancer. Three pre\-trained deep learning models(Residual Network\-50, Visual Geometry Group\-16, and EfficientNetB0) areevaluated based on these learning settings using seven carefully curatedtraining sets. To create the first training set (TS1), SL is applied to allannotated image samples. Five training sets (TS2\-TS6) with different ratios oflabeled and unlabeled cancer images are used to evaluateSemi\-SL. Unlabeledcancer images from the final training set (TS7) are utilized for Self\-SLassessment. Among different learning environments, outcomes from the Semi\-SLsetting show a strong degree of agreement with the outcomes achieved in the SLsetting. The uniform pattern of observations from the pre\-trained modelsacross all three datasets validates the methodology and techniques of theresearch. Based on modest number of labeled samples and minimal computing cost,our study suggests that the Semi\-SL option can be a highly viable replacementfor the SL option under label annotation constraint scenarios.</description><author>Samta Rani, Tanvir Ahmad, Sarfaraz Masood, Chandni Saxena</author><pubDate>Fri, 16 Aug 2024 01:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07988v2</guid></item><item><title>Interactive Character Control with Auto-Regressive Motion Diffusion Models</title><link>http://arxiv.org/abs/2306.00416v4</link><description>Real-time character control is an essential component for interactiveexperiences, with a broad range of applications, including physics simulations,video games, and virtual reality. The success of diffusion models for imagesynthesis has led to the use of these models for motion synthesis. However, themajority of these motion diffusion models are primarily designed for offlineapplications, where space-time models are used to synthesize an entire sequenceof frames simultaneously with a pre-specified length. To enable real-timemotion synthesis with diffusion model that allows time-varying controls, wepropose A-MDM (Auto-regressive Motion Diffusion Model). Our conditionaldiffusion model takes an initial pose as input, and auto-regressively generatessuccessive motion frames conditioned on the previous frame. Despite itsstreamlined network architecture, which uses simple MLPs, our framework iscapable of generating diverse, long-horizon, and high-fidelity motionsequences. Furthermore, we introduce a suite of techniques for incorporatinginteractive controls into A-MDM, such as task-oriented sampling, in-painting,and hierarchical reinforcement learning. These techniques enable a pre-trainedA-MDM to be efficiently adapted for a variety of new downstream tasks. Weconduct a comprehensive suite of experiments to demonstrate the effectivenessof A-MDM, and compare its performance against state-of-the-art auto-regressivemethods.</description><author>Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, Xue Bin Peng</author><pubDate>Fri, 16 Aug 2024 01:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00416v4</guid></item><item><title>A Survey of Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2301.08028v3</link><description>While deep reinforcement learning (RL) has fueled multiple high-profilesuccesses in machine learning, it is held back from more widespread adoption byits often poor data efficiency and the limited generality of the policies itproduces. A promising approach for alleviating these limitations is to cast thedevelopment of better RL algorithms as a machine learning problem itself in aprocess called meta-RL. Meta-RL is most commonly studied in a problem settingwhere, given a distribution of tasks, the goal is to learn a policy that iscapable of adapting to any new task from the task distribution with as littledata as possible. In this survey, we describe the meta-RL problem setting indetail as well as its major variations. We discuss how, at a high level,meta-RL research can be clustered based on the presence of a task distributionand the learning budget available for each individual task. Using theseclusters, we then survey meta-RL algorithms and applications. We conclude bypresenting the open problems on the path to making meta-RL part of the standardtoolbox for a deep RL practitioner.</description><author>Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, Shimon Whiteson</author><pubDate>Fri, 16 Aug 2024 00:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08028v3</guid></item><item><title>A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites</title><link>http://arxiv.org/abs/2408.07846v2</link><description>Unit tests represent the most basic level of testing within the softwaretesting lifecycle and are crucial to ensuring software correctness. Designingand creating unit tests is a costly and labor-intensive process that is ripefor automation. Recently, Large Language Models (LLMs) have been applied tovarious aspects of software development, including unit test generation.Although several empirical studies evaluating LLMs' capabilities in test codegeneration exist, they primarily focus on simple scenarios, such as thestraightforward generation of unit tests for individual methods. Theseevaluations often involve independent and small-scale test units, providing alimited view of LLMs' performance in real-world software development scenarios.Moreover, previous studies do not approach the problem at a suitable scale forreal-life applications. Generated unit tests are often evaluated via manualintegration into the original projects, a process that limits the number oftests executed and reduces overall efficiency. To address these gaps, we havedeveloped an approach for generating and evaluating more real-life complexitytest suites. Our approach focuses on class-level test code generation andautomates the entire process from test generation to test assessment. In thiswork, we present AgoneTest: an automated system for generating test suites forJava projects and a comprehensive and principled methodology for evaluating thegenerated test suites. Starting from a state-of-the-art dataset (i.e.,Methods2Test), we built a new dataset for comparing human-written tests withthose generated by LLMs. Our key contributions include a scalable automatedsoftware system, a new dataset, and a detailed methodology for evaluating testquality.</description><author>Andrea Lops, Fedelucio Narducci, Azzurra Ragone, Michelantonio Trizio, Claudio Bartolini</author><pubDate>Fri, 16 Aug 2024 00:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07846v2</guid></item><item><title>Can Large Language Models Understand Symbolic Graphics Programs?</title><link>http://arxiv.org/abs/2408.08313v1</link><description>Assessing the capabilities of large language models (LLMs) is oftenchallenging, in part, because it is hard to find tasks to which they have notbeen exposed during training. We take one step to address this challenge byturning to a new task: focusing on symbolic graphics programs, which are apopular representation for graphics content that procedurally generates visualdata. LLMs have shown exciting promise towards program synthesis, but do theyunderstand symbolic graphics programs? Unlike conventional programs, symbolicgraphics programs can be translated to graphics content. Here, we characterizean LLM's understanding of symbolic programs in terms of their ability to answerquestions related to the graphics content. This task is challenging as thequestions are difficult to answer from the symbolic programs alone -- yet, theywould be easy to answer from the corresponding graphics content as we verifythrough a human experiment. To understand symbolic programs, LLMs may need topossess the ability to imagine how the corresponding graphics content wouldlook without directly accessing the rendered visual content. We use this taskto evaluate LLMs by creating a large benchmark for the semantic understandingof symbolic graphics programs. This benchmark is built via program-graphicscorrespondence, hence requiring minimal human efforts. We evaluate current LLMson our benchmark to elucidate a preliminary assessment of their ability toreason about visual scenes from programs. We find that this task distinguishesexisting LLMs and models considered good at reasoning perform better. Lastly,we introduce Symbolic Instruction Tuning (SIT) to improve this ability.Specifically, we query GPT4-o with questions and images generated by symbolicprograms. Such data are then used to finetune an LLM. We also find that SITdata can improve the general instruction following ability of LLMs.</description><author>Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf</author><pubDate>Thu, 15 Aug 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08313v1</guid></item><item><title>HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning</title><link>http://arxiv.org/abs/2408.08312v1</link><description>To achieve dexterity comparable to that of humans, robots must intelligentlyprocess tactile sensor data. Taxel-based tactile signals often have lowspatial-resolution, with non-standardized representations. In this paper, wepropose a novel framework, HyperTaxel, for learning a geometrically-informedrepresentation of taxel-based tactile signals to address challenges associatedwith their spatial resolution. We use this representation and a contrastivelearning objective to encode and map sparse low-resolution taxel signals tohigh-resolution contact surfaces. To address the uncertainty inherent in thesesignals, we leverage joint probability distributions across multiplesimultaneous contacts to improve taxel hyper-resolution. We evaluate ourrepresentation by comparing it with two baselines and present results thatsuggest our representation outperforms the baselines. Furthermore, we presentqualitative results that demonstrate the learned representation captures thegeometric features of the contact surface, such as flatness, curvature, andedges, and generalizes across different objects and sensor configurations.Moreover, we present results that suggest our representation improves theperformance of various downstream tasks, such as surface classification, 6Din-hand pose estimation, and sim-to-real transfer.</description><author>Hongyu Li, Snehal Dikhale, Jinda Cui, Soshi Iba, Nawid Jamali</author><pubDate>Thu, 15 Aug 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08312v1</guid></item><item><title>ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws</title><link>http://arxiv.org/abs/2408.08310v1</link><description>High-quality data is crucial for the pre-training performance of largelanguage models. Unfortunately, existing quality filtering methods rely on aknown high-quality dataset as reference, which can introduce potential bias andcompromise diversity. In this paper, we propose ScalingFilter, a novel approachthat evaluates text quality based on the perplexity difference between twolanguage models trained on the same data, thereby eliminating the influence ofthe reference dataset in the filtering process. An theoretical analysis showsthat ScalingFilter is equivalent to an inverse utilization of scaling laws.Through training models with 1.3B parameters on the same data source processedby various quality filters, we find ScalingFilter can improve zero-shotperformance of pre-trained models in downstream tasks. To assess the biasintroduced by quality filtering, we introduce semantic diversity, a metric ofutilizing text embedding models for semantic representations. Extensiveexperiments reveal that semantic diversity is a reliable indicator of datasetdiversity, and ScalingFilter achieves an optimal balance between downstreamperformance and semantic diversity.</description><author>Ruihang Li, Yixuan Wei, Miaosen Zhang, Nenghai Yu, Han Hu, Houwen Peng</author><pubDate>Thu, 15 Aug 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08310v1</guid></item><item><title>Understanding the Local Geometry of Generative Model Manifolds</title><link>http://arxiv.org/abs/2408.08307v1</link><description>Deep generative models learn continuous representations of complex datamanifolds using a finite number of samples during training. For a pre-trainedgenerative model, the common way to evaluate the quality of the manifoldrepresentation learned, is by computing global metrics like Fr\'echet InceptionDistance using a large number of generated and real samples. However,generative model performance is not uniform across the learned manifold, e.g.,for \textit{foundation models} like Stable Diffusion generation performance canvary significantly based on the conditioning or initial noise vector beingdenoised. In this paper we study the relationship between the \textit{localgeometry of the learned manifold} and downstream generation. Based on thetheory of continuous piecewise-linear (CPWL) generators, we use three geometricdescriptors - scaling ($\psi$), rank ($\nu$), and complexity ($\delta$) - tocharacterize a pre-trained generative model manifold locally. We providequantitative and qualitative evidence showing that for a given latent, thelocal descriptors are correlated with generation aesthetics, artifacts,uncertainty, and even memorization. Finally we demonstrate that training a\textit{reward model} on the local geometry can allow controlling thelikelihood of a generated sample under the learned distribution.</description><author>Ahmed Imtiaz Humayun, Ibtihel Amara, Candice Schumann, Golnoosh Farnadi, Negar Rostamzadeh, Mohammad Havaei</author><pubDate>Thu, 15 Aug 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08307v1</guid></item><item><title>On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee</title><link>http://arxiv.org/abs/2303.06815v3</link><description>Model compression is a crucial part of deploying neural networks (NNs),especially when the memory and storage of computing devices are limited in manyapplications. This paper focuses on two model compression techniques: low-rankapproximation and weight pruning in neural networks, which are very popularnowadays. However, training NN with low-rank approximation and weight pruningalways suffers significant accuracy loss and convergence issues. In this paper,a holistic framework is proposed for model compression from a novel perspectiveof nonconvex optimization by designing an appropriate objective function. Then,we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve thenonconvex optimization. One advantage of our algorithm is that an efficientiteration scheme can be derived with closed-form, which is gradient-free.Therefore, our algorithm will not suffer from vanishing/exploding gradientproblems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property of ourobjective function, we show that our algorithm globally converges to a criticalpoint at the rate of O(1/k), where k denotes the number of iterations. Lastly,extensive experiments with tensor train decomposition and weight pruningdemonstrate the efficiency and superior performance of the proposed framework.Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD</description><author>Chenyang Li, Jihoon Chung, Mengnan Du, Haimin Wang, Xianlian Zhou, Bo Shen</author><pubDate>Thu, 15 Aug 2024 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06815v3</guid></item><item><title>Towards Flexible Visual Relationship Segmentation</title><link>http://arxiv.org/abs/2408.08305v1</link><description>Visual relationship understanding has been studied separately in human-objectinteraction(HOI) detection, scene graph generation(SGG), and referringrelationships(RR) tasks. Given the complexity and interconnectedness of thesetasks, it is crucial to have a flexible framework that can effectively addressthese tasks in a cohesive manner. In this work, we propose FleVRS, a singlemodel that seamlessly integrates the above three aspects in standard andpromptable visual relationship segmentation, and further possesses thecapability for open-vocabulary segmentation to adapt to novel scenarios. FleVRSleverages the synergy between text and image modalities, to ground varioustypes of relationships from images and use textual features fromvision-language models to visual conceptual understanding. Empirical validationacross various datasets demonstrates that our framework outperforms existingmodels in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ onHICO-DET, +11.4 $Acc$ on VRD, +4.7 $mAP$ on unseen HICO-DET. Our FleVRSrepresents a significant step towards a more intuitive, comprehensive, andscalable understanding of visual relationships.</description><author>Fangrui Zhu, Jianwei Yang, Huaizu Jiang</author><pubDate>Thu, 15 Aug 2024 17:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08305v1</guid></item><item><title>Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors</title><link>http://arxiv.org/abs/2408.08302v1</link><description>In this paper, we explore the capabilities of state-of-the-art large languagemodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-leveltransportation engineering problems. We introduce TransportBench, a benchmarkdataset that includes a sample of transportation engineering problems on a widerange of subjects in the context of planning, design, management, and controlof transportation systems. This dataset is used by human experts to evaluatethe capabilities of various commercial and open-sourced LLMs, especially theiraccuracy, consistency, and reasoning behaviors, in solving transportationengineering problems. Our comprehensive analysis uncovers the unique strengthsand limitations of each LLM, e.g. our analysis shows the impressive accuracyand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solvingTransportBench problems. Our study marks a thrilling first step towardharnessing artificial general intelligence for complex transportationchallenges.</description><author>Usman Syed, Ethan Light, Xingang Guo, Huan Zhang, Lianhui Qin, Yanfeng Ouyang, Bin Hu</author><pubDate>Thu, 15 Aug 2024 17:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08302v1</guid></item><item><title>HELP: Hierarchical Embeddings-based Log Parsing</title><link>http://arxiv.org/abs/2408.08300v1</link><description>Logs are a first-hand source of information for software maintenance andfailure diagnosis. Log parsing, which converts semi-structured log messagesinto structured templates, is a prerequisite for automated log analysis taskssuch as anomaly detection, troubleshooting, and root cause analysis. However,existing log parsers fail in real-world systems for three main reasons. First,traditional heuristics-based parsers require handcrafted features and domainknowledge, which are difficult to generalize at scale. Second, existing largelanguage model-based parsers rely on periodic offline processing, limitingtheir effectiveness in real-time use cases. Third, existing online parsingalgorithms are susceptible to log drift, where slight log changes create falsepositives that drown out real anomalies. To address these challenges, wepropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the firstonline semantic-based parser to leverage LLMs for performant and cost-effectivelog parsing. We achieve this through a novel hierarchical embeddings module,which fine-tunes a text embedding model to cluster logs before parsing,reducing querying costs by multiple orders of magnitude. To combat log drift,we also develop an iterative rebalancing module, which periodically updatesexisting log groupings. We evaluate HELP extensively on 14 public large-scaledatasets, showing that HELP achieves significantly higher F1-weighted groupingand parsing accuracy than current state-of-the-art online log parsers. We alsoimplement HELP into Iudex's production observability platform, confirmingHELP's practicality in a production environment. Our results show that HELP iseffective and efficient for high-throughput real-world log parsing.</description><author>Andy Xu, Arno Gau</author><pubDate>Thu, 15 Aug 2024 17:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08300v1</guid></item><item><title>SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training</title><link>http://arxiv.org/abs/2408.08295v1</link><description>In recent years, continual learning with pre-training (CLPT) has receivedwidespread interest, instead of its traditional focus of training from scratch.The use of strong pre-trained models (PTMs) can greatly facilitate knowledgetransfer and alleviate catastrophic forgetting, but also suffers fromprogressive overfitting of pre-trained knowledge into specific downstreamtasks. A majority of current efforts often keep the PTMs frozen and incorporatetask-specific prompts to instruct representation learning, coupled with aprompt selection process for inference. However, due to the limited capacity ofprompt parameters, this strategy demonstrates only sub-optimal performance incontinual learning. In comparison, tuning all parameters of PTMs often providesthe greatest potential for representation learning, making sequentialfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.To this end, we present an in-depth analysis of the progressive overfittingproblem from the lens of Seq FT. Considering that the overly fastrepresentation learning and the biased classification layer constitute thisparticular problem, we introduce the advanced Slow Learner with ClassifierAlignment (SLCA++) framework to unleash the power of Seq FT, serving as astrong baseline approach for CLPT. Our approach involves a Slow Learner toselectively reduce the learning rate of backbone parameters, and a ClassifierAlignment to align the disjoint classification layers in a post-hoc fashion. Wefurther enhance the efficacy of SL with a symmetric cross-entropy loss, as wellas employ a parameter-efficient strategy to implement Seq FT with SLCA++.Across a variety of continual learning scenarios on image classificationbenchmarks, our approach provides substantial improvements and outperformsstate-of-the-art methods by a large margin. Code:https://github.com/GengDavid/SLCA.</description><author>Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei</author><pubDate>Thu, 15 Aug 2024 17:50:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08295v1</guid></item><item><title>Aliasing and Label-Independent Decomposition of Risk: Beyond the bias-variance trade-off</title><link>http://arxiv.org/abs/2408.08294v1</link><description>A central problem in data science is to use potentially noisy samples of anunknown function to predict function values for unseen inputs. In classicalstatistics, the predictive error is understood as a trade-off between the biasand the variance that balances model simplicity with its ability to fit complexfunctions. However, over-parameterized models exhibit counter-intuitivebehaviors, such as "double descent" in which models of increasing complexityexhibit decreasing generalization error. We introduce an alternative paradigmcalled the generalized aliasing decomposition. We explain the asymptoticallysmall error of complex models as a systematic "de-aliasing" that occurs in theover-parameterized regime. In the limit of large models, the contribution dueto aliasing vanishes, leaving an expression for the asymptotic total error wecall the invertibility failure of very large models on few training points.Because the generalized aliasing decomposition can be explicitly calculatedfrom the relationship between model class and samples without seeing any datalabels, it can answer questions related to experimental design and modelselection before collecting data or performing experiments. We demonstrate thisapproach using several examples, including classical regression problems and acluster expansion model used in materials science.</description><author>Mark K. Transtrum, Gus L. W. Hart, Tyler J. Jarvis, Jared P. Whitehead</author><pubDate>Thu, 15 Aug 2024 17:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08294v1</guid></item><item><title>The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community</title><link>http://arxiv.org/abs/2408.08291v1</link><description>Human-model conversations provide a window into users' real-world scenarios,behavior, and needs, and thus are a valuable resource for model development andresearch. While for-profit companies collect user data through the APIs oftheir models, using it internally to improve their own models, the open sourceand research community lags behind. We introduce the ShareLM collection, a unified set of human conversationswith large language models, and its accompanying plugin, a Web extension forvoluntarily contributing user-model conversations. Where few platforms sharetheir chats, the ShareLM plugin adds this functionality, thus, allowing usersto share conversations from most platforms. The plugin allows the user to ratetheir conversations, both at the conversation and the response levels, anddelete conversations they prefer to keep private before they ever leave theuser's local storage. We release the plugin conversations as part of theShareLM collection, and call for more community effort in the field of openhuman-model data. The code, plugin, and data are available.</description><author>Shachar Don-Yehiya, Leshem Choshen, Omri Abend</author><pubDate>Thu, 15 Aug 2024 17:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08291v1</guid></item><item><title>Nearest Neighbor Classification for Classical Image Upsampling</title><link>http://arxiv.org/abs/2403.19611v2</link><description>Given a set of ordered pixel data in the form of an image, our goal is toperform upsampling on the data such that: the resulting resolution is improvedby some factor, the final result passes the human test, having added new,believable, and realistic information and detail to the image, the timecomplexity for upscaling is relatively close to that of lossy upscalingimplementations.</description><author>Evan Matthews, Nicolas Prate</author><pubDate>Thu, 15 Aug 2024 17:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19611v2</guid></item><item><title>Scene-wise Adaptive Network for Dynamic Cold-start Scenes Optimization in CTR Prediction</title><link>http://arxiv.org/abs/2408.07278v2</link><description>In the realm of modern mobile E-commerce, providing users with nearbycommercial service recommendations through location-based online services hasbecome increasingly vital. While machine learning approaches have shown promisein multi-scene recommendation, existing methodologies often struggle to addresscold-start problems in unprecedented scenes: the increasing diversity ofcommercial choices, along with the short online lifespan of scenes, give riseto the complexity of effective recommendations in online and dynamic scenes. Inthis work, we propose Scene-wise Adaptive Network (SwAN), a novel approach thatemphasizes high-performance cold-start online recommendations for new scenes.Our approach introduces several crucial capabilities, including scenesimilarity learning, user-specific scene transition cognition, scene-specificinformation construction for the new scene, and enhancing the diverged logicalinformation between scenes. We demonstrate SwAN's potential to optimize dynamicmulti-scene recommendation problems by effectively online handling cold-startrecommendations for any newly arrived scenes. More encouragingly, SwAN has beensuccessfully deployed in Meituan's online catering recommendation service,which serves millions of customers per day, and SwAN has achieved a 5.64% CTRindex improvement relative to the baselines and a 5.19% increase in daily ordervolume proportion.</description><author>Wenhao Li, Jie Zhou, Chuan Luo, Chao Tang, Kun Zhang, Shixiong Zhao</author><pubDate>Thu, 15 Aug 2024 17:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07278v2</guid></item><item><title>Absence of Closed-Form Descriptions for Gradient Flow in Two-Layer Narrow Networks</title><link>http://arxiv.org/abs/2408.08286v1</link><description>In the field of machine learning, comprehending the intricate trainingdynamics of neural networks poses a significant challenge. This paper exploresthe training dynamics of neural networks, particularly whether these dynamicscan be expressed in a general closed-form solution. We demonstrate that thedynamics of the gradient flow in two-layer narrow networks is not an integrablesystem. Integrable systems are characterized by trajectories confined tosubmanifolds defined by level sets of first integrals (invariants),facilitating predictable and reducible dynamics. In contrast, non-integrablesystems exhibit complex behaviors that are difficult to predict. To establishthe non-integrability, we employ differential Galois theory, which focuses onthe solvability of linear differential equations. We demonstrate that undermild conditions, the identity component of the differential Galois group of thevariational equations of the gradient flow is non-solvable. This resultconfirms the system's non-integrability and implies that the training dynamicscannot be represented by Liouvillian functions, precluding a closed-formsolution for describing these dynamics. Our findings highlight the necessity ofemploying numerical methods to tackle optimization problems within neuralnetworks. The results contribute to a deeper understanding of neural networktraining dynamics and their implications for machine learning optimizationstrategies.</description><author>Yeachan Park</author><pubDate>Thu, 15 Aug 2024 17:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08286v1</guid></item><item><title>Accurate and efficient structure elucidation from routine one-dimensional NMR spectra using multitask machine learning</title><link>http://arxiv.org/abs/2408.08284v1</link><description>Rapid determination of molecular structures can greatly accelerate workflowsacross many chemical disciplines. However, elucidating structure using onlyone-dimensional (1D) NMR spectra, the most readily accessible data, remains anextremely challenging problem because of the combinatorial explosion of thenumber of possible molecules as the number of constituent atoms is increased.Here, we introduce a multitask machine learning framework that predicts themolecular structure (formula and connectivity) of an unknown compound solelybased on its 1D 1H and/or 13C NMR spectra. First, we show how a transformerarchitecture can be constructed to efficiently solve the task, traditionallyperformed by chemists, of assembling large numbers of molecular fragments intomolecular structures. Integrating this capability with a convolutional neuralnetwork (CNN), we build an end-to-end model for predicting structure fromspectra that is fast and accurate. We demonstrate the effectiveness of thisframework on molecules with up to 19 heavy (non-hydrogen) atoms, a size forwhich there are trillions of possible structures. Without relying on any priorchemical knowledge such as the molecular formula, we show that our approachpredicts the exact molecule 69.6% of the time within the first 15 predictions,reducing the search space by up to 11 orders of magnitude.</description><author>Frank Hu, Michael S. Chen, Grant M. Rotskoff, Matthew W. Kanan, Thomas E. Markland</author><pubDate>Thu, 15 Aug 2024 17:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08284v1</guid></item><item><title>Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation</title><link>http://arxiv.org/abs/2402.11907v2</link><description>Aligning large language models (LLMs) with human expectations withouthuman-annotated preference data is an important problem. In this paper, wepropose a method to evaluate the response preference by using the outputprobabilities of response pairs under contrastive prompt pairs, which couldachieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Basedon this, we propose an automatic alignment method, Direct Large Model Alignment(DLMA). First, we use contrastive prompt pairs to automatically generatepreference data. Then, we continue to evaluate the generated preference datausing contrastive prompt pairs and calculate a self-rewarding score. Finally,we use the DPO algorithm to effectively align LLMs by combining thisself-rewarding score. In the experimental stage, our DLMA method could surpassthe \texttt{RLHF} method without relying on human-annotated preference data.</description><author>Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, Lijie Wen</author><pubDate>Thu, 15 Aug 2024 17:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11907v2</guid></item><item><title>Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model</title><link>http://arxiv.org/abs/2408.08282v1</link><description>Enabling humanoid robots to perform autonomously loco-manipulation inunstructured environments is crucial and highly challenging for achievingembodied intelligence. This involves robots being able to plan their actionsand behaviors in long-horizon tasks while using multi-modality to perceivedeviations between task execution and high-level planning. Recently, largelanguage models (LLMs) have demonstrated powerful planning and reasoningcapabilities for comprehension and processing of semantic information throughrobot control tasks, as well as the usability of analytical judgment anddecision-making for multi-modal inputs. To leverage the power of LLMs towardshumanoid loco-manipulation, we propose a novel language-model based frameworkthat enables robots to autonomously plan behaviors and low-level executionunder given textual instructions, while observing and correcting failures thatmay occur during task execution. To systematically evaluate this framework ingrounding LLMs, we created the robot 'action' and 'sensing' behavior libraryfor task planning, and conducted mobile manipulation tasks and experiments inboth simulated and real environments using the CENTAURO robot, and verified theeffectiveness and application of this approach in robotic tasks with autonomousbehavioral planning.</description><author>Jin Wang, Arturo Laurenzi, Nikos Tsagarakis</author><pubDate>Thu, 15 Aug 2024 17:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08282v1</guid></item><item><title>BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts</title><link>http://arxiv.org/abs/2408.08274v1</link><description>The Mixture of Experts (MoE) framework has become a popular architecture forlarge language models due to its superior performance over dense models.However, training MoEs from scratch in a large-scale regime is prohibitivelyexpensive. Existing methods mitigate this by pre-training multiple dense expertmodels independently and using them to initialize an MoE. This is done by usingexperts' feed-forward network (FFN) to initialize the MoE's experts whilemerging other parameters. However, this method limits the reuse of dense modelparameters to only the FFN layers, thereby constraining the advantages when"upcycling" these models into MoEs. We propose BAM (Branch-Attend-Mix), asimple yet effective method that addresses this shortcoming. BAM makes full useof specialized dense models by not only using their FFN to initialize the MoElayers but also leveraging experts' attention parameters fully by initializingthem into a soft-variant of Mixture of Attention (MoA) layers. We explore twomethods for upcycling attention parameters: 1) initializing separate attentionexperts from dense models including all attention parameters for the best modelperformance; and 2) sharing key and value parameters across all experts tofacilitate for better inference efficiency. To further improve efficiency, weadopt a parallel attention transformer architecture to MoEs, which allows theattention experts and FFN experts to be computed concurrently. Our experimentson seed models ranging from 590 million to 2 billion parameters demonstratethat BAM surpasses baselines in both perplexity and downstream taskperformance, within the same computational and data constraints.</description><author>Qizhen Zhang, Nikolas Gritsch, Dwaraknath Gnaneshwar, Simon Guo, David Cairuz, Bharat Venkitesh, Jakob Foerster, Phil Blunsom, Sebastian Ruder, Ahmet Ustun, Acyr Locatelli</author><pubDate>Thu, 15 Aug 2024 17:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08274v1</guid></item><item><title>Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interaction</title><link>http://arxiv.org/abs/2408.08272v1</link><description>When learning in strategic environments, a key question is whether agents canovercome uncertainty about their preferences to achieve outcomes they couldhave achieved absent any uncertainty. Can they do this solely throughinteractions with each other? We focus this question on the ability of agentsto attain the value of their Stackelberg optimal strategy and study the impactof information asymmetry. We study repeated interactions in fully strategicenvironments where players' actions are decided based on learning algorithmsthat take into account their observed histories and knowledge of the game. Westudy the pure Nash equilibria (PNE) of a meta-game where players choose thesealgorithms as their actions. We demonstrate that if one player has perfectknowledge about the game, then any initial informational gap persists. That is,while there is always a PNE in which the informed agent achieves herStackelberg value, there is a game where no PNE of the meta-game allows thepartially informed player to achieve her Stackelberg value. On the other hand,if both players start with some uncertainty about the game, the quality ofinformation alone does not determine which agent can achieve her Stackelbergvalue. In this case, the concept of information asymmetry becomes nuanced anddepends on the game's structure. Overall, our findings suggest that repeatedstrategic interactions alone cannot facilitate learning effectively enough toearn an uninformed player her Stackelberg value.</description><author>Nivasini Ananthakrishnan, Nika Haghtalab, Chara Podimata, Kunhe Yang</author><pubDate>Thu, 15 Aug 2024 17:17:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08272v1</guid></item><item><title>HeightLane: BEV Heightmap guided 3D Lane Detection</title><link>http://arxiv.org/abs/2408.08270v1</link><description>Accurate 3D lane detection from monocular images presents significantchallenges due to depth ambiguity and imperfect ground modeling. Previousattempts to model the ground have often used a planar ground assumption withlimited degrees of freedom, making them unsuitable for complex roadenvironments with varying slopes. Our study introduces HeightLane, aninnovative method that predicts a height map from monocular images by creatinganchors based on a multi-slope assumption. This approach provides a detailedand accurate representation of the ground. HeightLane employs the predictedheightmap along with a deformable attention-based spatial feature transformframework to efficiently convert 2D image features into 3D bird's eye view(BEV) features, enhancing spatial understanding and lane structure recognition.Additionally, the heightmap is used for the positional encoding of BEVfeatures, further improving their spatial accuracy. This explicit viewtransformation bridges the gap between front-view perceptions and spatiallyaccurate BEV representations, significantly improving detection performance. Toaddress the lack of the necessary ground truth (GT) height map in the originalOpenLane dataset, we leverage the Waymo dataset and accumulate its LiDAR datato generate a height map for the drivable area of each scene. The GT heightmapsare used to train the heightmap extraction module from monocular images.Extensive experiments on the OpenLane validation set show that HeightLaneachieves state-of-the-art performance in terms of F-score, highlighting itspotential in real-world applications.</description><author>Chaesong Park, Eunbin Seo, Jongwoo Lim</author><pubDate>Thu, 15 Aug 2024 17:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08270v1</guid></item><item><title>A Distributed Privacy Preserving Model for the Detection of Alzheimer's Disease</title><link>http://arxiv.org/abs/2312.10237v3</link><description>BACKGROUND: Segmentation of medical data, concerns about personal healthinformation (PHI) breaches, and the direct and indirect costs of consolidatingand managing such segmented date should motivate diagnostic machine learning(DML) researchers to identify privacy-preserving machine learning algorithmsthat can train on distributed or decentralized datasets of differentmodalities. Federated learning models provide such a decentralized machinelearning framework in which multiple investigators in possession of disparatedatasets and working on different devices or servers can train collaborativelya global machine learning models without ever having to exchange local data andthus can meet statutory PHI protections. To this end, a vertical federatelearning model is devised and tested for efficacy in the detection ofAlzheimer's Disease (AD). METHODS: The second version of Open Access Series of Imaging Studies -- withits panoply of demographic, imaging, and clinical assessment datasets -- wasused to test a multimodal vertical federated learning (VFL) model for ADdetection. RESULTS: By training and validating this VFL model on the demographic,clinical, and MRI data in OASIS-2, an 82.9\% accuracy rate is achieved,consistent with previously reported results. CONCLUSIONS: The VFL architecture proposed herein offers a novel distributedarchitecture, enabling collaborative learning across diverse sources of medicaldata while respecting statutory privacy constraints. By leveraging multiplemodalities of data, the robustness and accuracy of AD detection can beenhanced. This model not only contributes to the advancement of federatedlearning techniques but also holds promise for overcoming the hurdles posed bydata segmentation in medical research.</description><author>Paul K. Mandal</author><pubDate>Thu, 15 Aug 2024 17:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10237v3</guid></item><item><title>EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2403.18080v2</link><description>We present EgoPoseFormer, a simple yet effective transformer-based model forstereo egocentric human pose estimation. The main challenge in egocentric poseestimation is overcoming joint invisibility, which is caused by self-occlusionor a limited field of view (FOV) of head-mounted cameras. Our approachovercomes this challenge by incorporating a two-stage pose estimation paradigm:in the first stage, our model leverages the global information to estimate eachjoint's coarse location, then in the second stage, it employs a DETR styletransformer to refine the coarse locations by exploiting fine-grained stereovisual features. In addition, we present a Deformable Stereo Attentionoperation to enable our transformer to effectively process multi-view features,which enables it to accurately localize each joint in the 3D world. We evaluateour method on the stereo UnrealEgo dataset and show it significantlyoutperforms previous approaches while being computationally efficient: itimproves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and13.1% FLOPs compared to the state-of-the-art. Surprisingly, with propertraining settings, we find that even our first-stage pose proposal network canachieve superior performance compared to previous arts. We also show that ourmethod can be seamlessly extended to monocular settings, which achievesstate-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm(21% improvement) compared to the best existing method with only 60.7% modelparameters and 36.4% FLOPs. Code is available at:https://github.com/ChenhongyiYang/egoposeformer .</description><author>Chenhongyi Yang, Anastasia Tkach, Shreyas Hampali, Linguang Zhang, Elliot J. Crowley, Cem Keskin</author><pubDate>Thu, 15 Aug 2024 17:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18080v2</guid></item><item><title>InVAErt networks for amortized inference and identifiability analysis of lumped parameter hemodynamic models</title><link>http://arxiv.org/abs/2408.08264v1</link><description>Estimation of cardiovascular model parameters from electronic health records(EHR) poses a significant challenge primarily due to lack of identifiability.Structural non-identifiability arises when a manifold in the space ofparameters is mapped to a common output, while practical non-identifiabilitycan result due to limited data, model misspecification, or noise corruption. Toaddress the resulting ill-posed inverse problem, optimization-based or Bayesianinference approaches typically use regularization, thereby limiting thepossibility of discovering multiple solutions. In this study, we use inVAErtnetworks, a neural network-based, data-driven framework for enhanced digitaltwin analysis of stiff dynamical systems. We demonstrate the flexibility andeffectiveness of inVAErt networks in the context of physiological inversion ofa six-compartment lumped parameter hemodynamic model from synthetic data toreal data with missing components.</description><author>Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</author><pubDate>Thu, 15 Aug 2024 17:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08264v1</guid></item><item><title>mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis</title><link>http://arxiv.org/abs/2408.08261v1</link><description>This paper introduces mhGPT, a lightweight generative pre-trained transformertrained on mental health-related social media and PubMed articles. Fine-tunedfor specific mental health tasks, mhGPT was evaluated under limited hardwareconstraints and compared with state-of-the-art models like MentaLLaMA andGemma. Despite having only 1.98 billion parameters and using just 5% of thedataset, mhGPT outperformed larger models and matched the performance of modelstrained on significantly more data. The key contributions include integratingdiverse mental health data, creating a custom tokenizer, and optimizing asmaller architecture for low-resource settings. This research could advanceAI-driven mental health care, especially in areas with limited computing power.</description><author>Dae-young Kim, Rebecca Hwa, Muhammad Mahbubur Rahman</author><pubDate>Thu, 15 Aug 2024 17:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08261v1</guid></item><item><title>GSVD-NMF: Recovering Missing Features in Non-negative Matrix Factorization</title><link>http://arxiv.org/abs/2408.08260v1</link><description>Non-negative matrix factorization (NMF) is an important tool in signalprocessing and widely used to separate mixed sources into their components.However, NMF is NP-hard and thus may fail to discover the ideal factorization;moreover, the number of components may not be known in advance and thusfeatures may be missed or incompletely separated. To recover missing componentsfrom under-complete NMF, we introduce GSVD-NMF, which proposes new componentsbased on the generalized singular value decomposition (GSVD) betweenpreliminary NMF results and the SVD of the original matrix. Simulation andexperimental results demonstrate that GSVD-NMF often recovers missing featuresfrom under-complete NMF and helps NMF achieve better local optima.</description><author>Youdong Guo, Timothy E. Holy</author><pubDate>Thu, 15 Aug 2024 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08260v1</guid></item><item><title>Snuffy: Efficient Whole Slide Image Classifier</title><link>http://arxiv.org/abs/2408.08258v1</link><description>Whole Slide Image (WSI) classification with multiple instance learning (MIL)in digital pathology faces significant computational challenges. Currentmethods mostly rely on extensive self-supervised learning (SSL) forsatisfactory performance, requiring long training periods and considerablecomputational resources. At the same time, no pre-training affects performancedue to domain shifts from natural images to WSIs. We introduce\textbf{\textit{Snuffy}} architecture, a novel MIL-pooling method based onsparse transformers that mitigates performance loss with limited pre-trainingand enables continual few-shot pre-training as a competitive option. Oursparsity pattern is tailored for pathology and is theoretically proven to be auniversal approximator with the tightest probabilistic sharp bound on thenumber of layers for sparse transformers, to date. We demonstrate Snuffy'seffectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superiorWSI and patch-level accuracies. The code is available on\url{https://github.com/jafarinia/snuffy}.</description><author>Hossein Jafarinia, Alireza Alipanah, Danial Hamdi, Saeed Razavi, Nahal Mirzaie, Mohammad Hossein Rohban</author><pubDate>Thu, 15 Aug 2024 16:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08258v1</guid></item><item><title>Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding</title><link>http://arxiv.org/abs/2408.08252v1</link><description>Diffusion models excel at capturing the natural design spaces of images,molecules, DNA, RNA, and protein sequences. However, rather than merelygenerating designs that are natural, we often aim to optimize downstream rewardfunctions while preserving the naturalness of these design spaces. Existingmethods for achieving this goal often require ``differentiable'' proxy models(\textit{e.g.}, classifier guidance or DPS) or involve computationallyexpensive fine-tuning of diffusion models (\textit{e.g.}, classifier-freeguidance, RL-based fine-tuning). In our work, we propose a new method toaddress these challenges. Our algorithm is an iterative sampling method thatintegrates soft value functions, which looks ahead to how intermediate noisystates lead to high rewards in the future, into the standard inferenceprocedure of pre-trained diffusion models. Notably, our approach avoidsfine-tuning generative models and eliminates the need to constructdifferentiable models. This enables us to (1) directly utilizenon-differentiable features/reward feedback, commonly used in many scientificdomains, and (2) apply our method to recent discrete diffusion models in aprincipled way. Finally, we demonstrate the effectiveness of our algorithmacross several domains, including image generation, molecule generation, andDNA/RNA sequence generation. The code is available at\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.</description><author>Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Aviv Regev, Sergey Levine, Masatoshi Uehara</author><pubDate>Thu, 15 Aug 2024 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08252v1</guid></item><item><title>DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection</title><link>http://arxiv.org/abs/2308.10015v3</link><description>Automatic fingerprint recognition systems suffer from the threat ofpresentation attacks due to their wide range of deployment in areas includingnational borders and commercial applications. A presentation attack can beperformed by creating a spoof of a user's fingerprint with or without theirconsent. This paper presents a dynamic ensemble of deep CNN and handcraftedfeatures to detect presentation attacks in known-material and unknown-materialprotocols of the livness detection competition. The proposed presentationattack detection model, in this way, utilizes the capabilities of both deep CNNand handcrafted features techniques and exhibits better performance than theirindividual performances. We have validated our proposed method on benchmarkdatabases from the Liveness Detection Competition in 2015, 2017, and 2019,yielding overall accuracy of 96.10\%, 96.49\%, and 94.99\% on them,respectively. The proposed method outperforms state-of-the-art methods in termsof classification accuracy.</description><author>Anuj Rai, Parsheel Kumar Tiwari, Jyotishna Baishya, Ram Prakash Sharma, Somnath Dey</author><pubDate>Thu, 15 Aug 2024 16:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10015v3</guid></item><item><title>Computer Vision Model Compression Techniques for Embedded Systems: A Survey</title><link>http://arxiv.org/abs/2408.08250v1</link><description>Deep neural networks have consistently represented the state of the art inmost computer vision problems. In these scenarios, larger and more complexmodels have demonstrated superior performance to smaller architectures,especially when trained with plenty of representative data. With the recentadoption of Vision Transformer (ViT) based architectures and advancedConvolutional Neural Networks (CNNs), the total number of parameters of leadingbackbone architectures increased from 62M parameters in 2012 with AlexNet to 7Bparameters in 2024 with AIM-7B. Consequently, deploying such deep architecturesfaces challenges in environments with processing and runtime constraints,particularly in embedded systems. This paper covers the main model compressiontechniques applied for computer vision tasks, enabling modern models to be usedin embedded systems. We present the characteristics of compression subareas,compare different approaches, and discuss how to choose the best technique andexpected variations when analyzing it on various embedded devices. We alsoshare codes to assist researchers and new practitioners in overcoming initialimplementation challenges for each subarea and present trends for ModelCompression. Case studies for compression models are available at\href{https://github.com/venturusbr/cv-model-compression}{https://github.com/venturusbr/cv-model-compression}.</description><author>Alexandre Lopes, Fernando Pereira dos Santos, Diulhio de Oliveira, Mauricio Schiezaro, Helio Pedrini</author><pubDate>Thu, 15 Aug 2024 16:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08250v1</guid></item><item><title>Conformalized Answer Set Prediction for Knowledge Graph Embedding</title><link>http://arxiv.org/abs/2408.08248v1</link><description>Knowledge graph embeddings (KGE) apply machine learning methods on knowledgegraphs (KGs) to provide non-classical reasoning capabilities based onsimilarities and analogies. The learned KG embeddings are typically used toanswer queries by ranking all potential answers, but rankings often lack ameaningful probabilistic interpretation - lower-ranked answers do notnecessarily have a lower probability of being true. This limitation makes itdifficult to distinguish plausible from implausible answers, posing challengesfor the application of KGE methods in high-stakes domains like medicine. Weaddress this issue by applying the theory of conformal prediction that allowsgenerating answer sets, which contain the correct answer with probabilisticguarantees. We explain how conformal prediction can be used to generate suchanswer sets for link prediction tasks. Our empirical evaluation on fourbenchmark datasets using six representative KGE methods validates that thegenerated answer sets satisfy the probabilistic guarantees given by the theoryof conformal prediction. We also demonstrate that the generated answer setsoften have a sensible size and that the size adapts well with respect to thedifficulty of the query.</description><author>Yuqicheng Zhu, Nico Potyka, Jiarong Pan, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab</author><pubDate>Thu, 15 Aug 2024 16:36:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08248v1</guid></item><item><title>DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction</title><link>http://arxiv.org/abs/2312.03298v3</link><description>Point cloud streaming is increasingly getting popular, evolving into the normfor interactive service delivery and the future Metaverse. However, thesubstantial volume of data associated with point clouds presents numerouschallenges, particularly in terms of high bandwidth consumption and largestorage capacity. Despite various solutions proposed thus far, with a focus onpoint cloud compression, upsampling, and completion, thesereconstruction-related methods continue to fall short in delivering highfidelity point cloud output. As a solution, in DiffPMAE, we propose aneffective point cloud reconstruction architecture. Inspired by self-supervisedlearning concepts, we combine Masked Auto-Encoding and Diffusion Modelmechanism to remotely reconstruct point cloud data. By the nature of thisreconstruction process, DiffPMAE can be extended to many related downstreamtasks including point cloud compression, upsampling and completion. LeveragingShapeNet-55 and ModelNet datasets with over 60000 objects, we validate theperformance of DiffPMAE exceeding many state-of-the-art methods in-terms ofauto-encoding and downstream tasks considered.</description><author>Yanlong Li, Chamara Madarasingha, Kanchana Thilakarathna</author><pubDate>Thu, 15 Aug 2024 16:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03298v3</guid></item><item><title>A Conflicts-free, Speed-lossless KAN-based Reinforcement Learning Decision System for Interactive Driving in Roundabouts</title><link>http://arxiv.org/abs/2408.08242v1</link><description>Safety and efficiency are crucial for autonomous driving in roundabouts,especially in the context of mixed traffic where autonomous vehicles (AVs) andhuman-driven vehicles coexist. This paper introduces a learning-based algorithmtailored to foster safe and efficient driving behaviors across varying levelsof traffic flows in roundabouts. The proposed algorithm employs a deepQ-learning network to effectively learn safe and efficient driving strategiesin complex multi-vehicle roundabouts. Additionally, a KAN (Kolmogorov-Arnoldnetwork) enhances the AVs' ability to learn their surroundings robustly andprecisely. An action inspector is integrated to replace dangerous actions toavoid collisions when the AV interacts with the environment, and a routeplanner is proposed to enhance the driving efficiency and safety of the AVs.Moreover, a model predictive control is adopted to ensure stability andprecision of the driving actions. The results show that our proposed systemconsistently achieves safe and efficient driving whilst maintaining a stabletraining process, as evidenced by the smooth convergence of the reward functionand the low variance in the training curves across various traffic flows.Compared to state-of-the-art benchmarks, the proposed algorithm achieves alower number of collisions and reduced travel time to destination.</description><author>Zhihao Lin, Zhen Tian, Qi Zhang, Ziyang Ye, Hanyang Zhuang, Jianglin Lan</author><pubDate>Thu, 15 Aug 2024 16:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08242v1</guid></item><item><title>The Threats of Embodied Multimodal LLMs: Jailbreaking Robotic Manipulation in the Physical World</title><link>http://arxiv.org/abs/2407.20242v2</link><description>Embodied artificial intelligence (AI) represents an artificial intelligencesystem that interacts with the physical world through sensors and actuators,seamlessly integrating perception and action. This design enables AI to learnfrom and operate within complex, real-world environments. Large Language Models(LLMs) deeply explore language instructions, playing a crucial role in devisingplans for complex tasks. Consequently, they have progressively shown immensepotential in empowering embodied AI, with LLM-based embodied AI emerging as afocal point of research within the community. It is foreseeable that, over thenext decade, LLM-based embodied AI robots are expected to proliferate widely,becoming commonplace in homes and industries. However, a critical safety issuethat has long been hiding in plain sight is: could LLM-based embodied AIperpetrate harmful behaviors? Our research investigates for the first time howto induce threatening actions in embodied AI, confirming the severe risks posedby these soon-to-be-marketed robots, which starkly contravene Asimov's ThreeLaws of Robotics and threaten human safety. Specifically, we formulate theconcept of embodied AI jailbreaking and expose three critical securityvulnerabilities: first, jailbreaking robotics through compromised LLM; second,safety misalignment between action and language spaces; and third, deceptiveprompts leading to unaware hazardous behaviors. We also analyze potentialmitigation measures and advocate for community awareness regarding the safetyof embodied AI applications in the physical world.</description><author>Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Yichen Wang, Lulu Xue, Minghui Li, Shengshan Hu, Leo Yu Zhang</author><pubDate>Thu, 15 Aug 2024 16:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20242v2</guid></item><item><title>On the Impact of Uncertainty and Calibration on Likelihood-Ratio Membership Inference Attacks</title><link>http://arxiv.org/abs/2402.10686v2</link><description>In a membership inference attack (MIA), an attacker exploits theoverconfidence exhibited by typical machine learning models to determinewhether a specific data point was used to train a target model. In this paper,we analyze the performance of the state-of-the-art likelihood ratio attack(LiRA) within an information-theoretical framework that allows theinvestigation of the impact of the aleatoric uncertainty in the true datageneration process, of the epistemic uncertainty caused by a limited trainingdata set, and of the calibration level of the target model. We compare threedifferent settings, in which the attacker receives decreasingly informativefeedback from the target model: confidence vector (CV) disclosure, in which theoutput probability vector is released; true label confidence (TLC) disclosure,in which only the probability assigned to the true label is made available bythe model; and decision set (DS) disclosure, in which an adaptive predictionset is produced as in conformal prediction. We derive bounds on the advantageof an MIA adversary with the aim of offering insights into the impact ofuncertainty and calibration on the effectiveness of MIAs. Simulation resultsdemonstrate that the derived analytical bounds predict well the effectivenessof MIAs.</description><author>Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone</author><pubDate>Thu, 15 Aug 2024 16:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10686v2</guid></item><item><title>Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation</title><link>http://arxiv.org/abs/2408.08234v1</link><description>Object pose estimation is essential to many industrial applications involvingrobotic manipulation, navigation, and augmented reality. Current generalizableobject pose estimators, i.e., approaches that do not need to be trained perobject, rely on accurate 3D models. Predominantly, CAD models are used, whichcan be hard to obtain in practice. At the same time, it is often possible toacquire images of an object. Naturally, this leads to the question whether 3Dmodels reconstructed from images are sufficient to facilitate accurate objectpose estimation. We aim to answer this question by proposing a novel benchmarkfor measuring the impact of 3D reconstruction quality on pose estimationaccuracy. Our benchmark provides calibrated images for object reconstructionregistered with the test images of the YCB-V dataset for pose evaluation underthe BOP benchmark format. Detailed experiments with multiple state-of-the-art3D reconstruction and object pose estimation approaches show that the geometryproduced by modern reconstruction methods is often sufficient for accurate poseestimation. Our experiments lead to interesting observations: (1) Standardmetrics for measuring 3D reconstruction quality are not necessarily indicativeof pose estimation accuracy, which shows the need for dedicated benchmarks suchas ours. (2) Classical, non-learning-based approaches can perform on par withmodern learning-based reconstruction techniques and can even offer a betterreconstruction time-pose accuracy tradeoff. (3) There is still a sizable gapbetween performance with reconstructed and with CAD models. To foster researchon closing this gap, our benchmark is publicly available athttps://github.com/VarunBurde/reconstruction_pose_benchmark}.</description><author>Varun Burde, Assia Benbihi, Pavel Burget, Torsten Sattler</author><pubDate>Thu, 15 Aug 2024 15:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08234v1</guid></item><item><title>The Z-Gromov-Wasserstein Distance</title><link>http://arxiv.org/abs/2408.08233v1</link><description>The Gromov-Wasserstein (GW) distance is a powerful tool for comparing metricmeasure spaces which has found broad applications in data science and machinelearning. Driven by the need to analyze datasets whose objects haveincreasingly complex structure (such as node and edge-attributed graphs),several variants of GW distance have been introduced in the recent literature.With a view toward establishing a general framework for the theory of GW-likedistances, this paper considers a vast generalization of the notion of a metricmeasure space: for an arbitrary metric space $Z$, we define a $Z$-network to bea measure space endowed with a kernel valued in $Z$. We introduce a method forcomparing $Z$-networks by defining a generalization of GW distance, which werefer to as $Z$-Gromov-Wasserstein ($Z$-GW) distance. This constructionsubsumes many previously known metrics and offers a unified approach tounderstanding their shared properties. The paper demonstrates that the $Z$-GWdistance defines a metric on the space of $Z$-networks which retains desirableproperties of $Z$, such as separability, completeness, and geodesicity. Many ofthese properties were unknown for existing variants of GW distance that fallunder our framework. Our focus is on foundational theory, but our results alsoinclude computable lower bounds and approximations of the distance which willbe useful for practical applications.</description><author>Martin Bauer, Facundo Mémoli, Tom Needham, Mao Nishino</author><pubDate>Thu, 15 Aug 2024 15:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08233v1</guid></item><item><title>Explaining an Agent's Future Beliefs through Temporally Decomposing Future Reward Estimators</title><link>http://arxiv.org/abs/2408.08230v1</link><description>Future reward estimation is a core component of reinforcement learningagents; i.e., Q-value and state-value functions, predicting an agent's sum offuture rewards. Their scalar output, however, obfuscates when or whatindividual future rewards an agent may expect to receive. We address this bymodifying an agent's future reward estimator to predict their next N expectedrewards, referred to as Temporal Reward Decomposition (TRD). This unlocks novelexplanations of agent behaviour. Through TRD we can: estimate when an agent mayexpect to receive a reward, the value of the reward and the agent's confidencein receiving it; measure an input feature's temporal importance to the agent'saction decisions; and predict the influence of different actions on futurerewards. Furthermore, we show that DQN agents trained on Atari environments canbe efficiently retrained to incorporate TRD with minimal impact on performance.</description><author>Mark Towers, Yali Du, Christopher Freeman, Timothy J. Norman</author><pubDate>Thu, 15 Aug 2024 15:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08230v1</guid></item><item><title>Rethinking Medical Anomaly Detection in Brain MRI: An Image Quality Assessment Perspective</title><link>http://arxiv.org/abs/2408.08228v1</link><description>Reconstruction-based methods, particularly those leveraging autoencoders,have been widely adopted to perform anomaly detection in brain MRI. While mostexisting works try to improve detection accuracy by proposing new modelstructures or algorithms, we tackle the problem through image qualityassessment, an underexplored perspective in the field. We propose a fusionquality loss function that combines Structural Similarity Index Measure losswith l1 loss, offering a more comprehensive evaluation of reconstructionquality. Additionally, we introduce a data pre-processing strategy thatenhances the average intensity ratio (AIR) between normal and abnormal regions,further improving the distinction of anomalies. By fusing the aforementionedtwo methods, we devise the image quality assessment (IQA) approach. Theproposed IQA approach achieves significant improvements (&gt;10%) in terms of Dicecoefficient (DICE) and Area Under the Precision-Recall Curve (AUPRC) on theBraTS21 (T2, FLAIR) and MSULB datasets when compared with state-of-the-artmethods. These results highlight the importance of invoking the comprehensiveimage quality assessment in medical anomaly detection and provide a newperspective for future research in this field.</description><author>Zixuan Pan, Jun Xia, Zheyu Yan, Guoyue Xu, Yawen Wu, Zhenge Jia, Jianxu Chen, Yiyu Shi</author><pubDate>Thu, 15 Aug 2024 15:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08228v1</guid></item><item><title>Problem Solving Through Human-AI Preference-Based Cooperation</title><link>http://arxiv.org/abs/2408.07461v2</link><description>While there is a widespread belief that artificial general intelligence (AGI)-- or even superhuman AI -- is imminent, complex problems in expert domains arefar from being solved. We argue that such problems require human-AI cooperationand that the current state of the art in generative AI is unable to play therole of a reliable partner due to a multitude of shortcomings, includinginability to keep track of a complex solution artifact (e.g., a softwareprogram), limited support for versatile human preference expression and lack ofadapting to human preference in an interactive setting. To address thesechallenges, we propose HAI-Co2, a novel human-AI co-construction framework. Weformalize HAI-Co2 and discuss the difficult open research problems that itfaces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacycompared to monolithic generative AI models.</description><author>Subhabrata Dutta, Timo Kaufmann, Goran Glavaš, Ivan Habernal, Kristian Kersting, Frauke Kreuter, Mira Mezini, Iryna Gurevych, Eyke Hüllermeier, Hinrich Schuetze</author><pubDate>Thu, 15 Aug 2024 15:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07461v2</guid></item><item><title>Evolving A* to Efficiently Solve the k Shortest-Path Problem (Extended Version)</title><link>http://arxiv.org/abs/2408.08227v1</link><description>The problem of finding the shortest path in a graph G(V, E) has been widelystudied. However, in many applications it is necessary to compute an arbitrarynumber of them, k. Even though the problem has raised a lot of interest fromdifferent research communities and many applications of it are known, it hasnot been addressed to the same extent as the single shortest path problem. Thebest algorithm known for efficiently solving this task has a time complexity ofO (|E| + |V|log{|V|}+k|V|)$ when computing paths in explicit form, and is basedon best-first search. This paper introduces a new search algorithm with thesame time complexity, which results from a natural evolution of A* thus, itpreserves all its interesting properties, making it widely applicable to manydifferent domains. Experiments in various testbeds show a significantimprovement in performance over the state of the art, often by one or twoorders of magnitude.</description><author>Carlos Linares López, Ian Herman</author><pubDate>Thu, 15 Aug 2024 15:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08227v1</guid></item><item><title>Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction</title><link>http://arxiv.org/abs/2408.08226v1</link><description>Knowledge graph embedding (KGE) models are often used to predict missinglinks for knowledge graphs (KGs). However, multiple KG embeddings can performalmost equally well for link prediction yet suggest conflicting predictions forcertain queries, termed \textit{predictive multiplicity} in literature. Thisbehavior poses substantial risks for KGE-based applications in high-stakedomains but has been overlooked in KGE research. In this paper, we definepredictive multiplicity in link prediction. We introduce evaluation metrics andmeasure predictive multiplicity for representative KGE methods on commonly usedbenchmark datasets. Our empirical study reveals significant predictivemultiplicity in link prediction, with $8\%$ to $39\%$ testing queriesexhibiting conflicting predictions. To address this issue, we proposeleveraging voting methods from social choice theory, significantly mitigatingconflicts by $66\%$ to $78\%$ according to our experiments.</description><author>Yuqicheng Zhu, Nico Potyka, Mojtaba Nayyeri, Bo Xiong, Yunjie He, Evgeny Kharlamov, Steffen Staab</author><pubDate>Thu, 15 Aug 2024 15:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08226v1</guid></item><item><title>Evetac: An Event-based Optical Tactile Sensor for Robotic Manipulation</title><link>http://arxiv.org/abs/2312.01236v2</link><description>Optical tactile sensors have recently become popular. They provide highspatial resolution, but struggle to offer fine temporal resolutions. Toovercome this shortcoming, we study the idea of replacing the RGB camera withan event-based camera and introduce a new event-based optical tactile sensorcalled Evetac. Along with hardware design, we develop touch processingalgorithms to process its measurements online at 1000 Hz. We devise anefficient algorithm to track the elastomer's deformation through the imprintedmarkers despite the sensor's sparse output. Benchmarking experimentsdemonstrate Evetac's capabilities of sensing vibrations up to 498 Hz,reconstructing shear forces, and significantly reducing data rates compared toRGB optical tactile sensors. Moreover, Evetac's output and the marker trackingprovide meaningful features for learning data-driven slip detection andprediction models. The learned models form the basis for a robust and adaptiveclosed-loop grasp controller capable of handling a wide range of objects. Webelieve that fast and efficient event-based tactile sensors like Evetac will beessential for bringing human-like manipulation capabilities to robotics. Thesensor design is open-sourced at https://sites.google.com/view/evetac .</description><author>Niklas Funk, Erik Helmut, Georgia Chalvatzaki, Roberto Calandra, Jan Peters</author><pubDate>Thu, 15 Aug 2024 15:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01236v2</guid></item><item><title>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</title><link>http://arxiv.org/abs/2408.06292v2</link><description>One of the grand challenges of artificial general intelligence is developingagents capable of conducting scientific research and discovering new knowledge.While frontier models have already been used as aides to human scientists, e.g.for brainstorming ideas, writing code, or prediction tasks, they still conductonly a small part of the scientific process. This paper presents the firstcomprehensive framework for fully automatic scientific discovery, enablingfrontier large language models to perform research independently andcommunicate their findings. We introduce The AI Scientist, which generatesnovel research ideas, writes code, executes experiments, visualizes results,describes its findings by writing a full scientific paper, and then runs asimulated review process for evaluation. In principle, this process can berepeated to iteratively develop ideas in an open-ended fashion, acting like thehuman scientific community. We demonstrate its versatility by applying it tothree distinct subfields of machine learning: diffusion modeling,transformer-based language modeling, and learning dynamics. Each idea isimplemented and developed into a full paper at a cost of less than $15 perpaper. To evaluate the generated papers, we design and validate an automatedreviewer, which we show achieves near-human performance in evaluating paperscores. The AI Scientist can produce papers that exceed the acceptancethreshold at a top machine learning conference as judged by our automatedreviewer. This approach signifies the beginning of a new era in scientificdiscovery in machine learning: bringing the transformative benefits of AIagents to the entire research process of AI itself, and taking us closer to aworld where endless affordable creativity and innovation can be unleashed onthe world's most challenging problems. Our code is open-sourced athttps://github.com/SakanaAI/AI-Scientist</description><author>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha</author><pubDate>Thu, 15 Aug 2024 15:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06292v2</guid></item><item><title>Enhancing Sharpness-Aware Minimization by Learning Perturbation Radius</title><link>http://arxiv.org/abs/2408.08222v1</link><description>Sharpness-aware minimization (SAM) is to improve model generalization bysearching for flat minima in the loss landscape. The SAM update consists of onestep for computing the perturbation and the other for computing the updategradient. Within the two steps, the choice of the perturbation radius iscrucial to the performance of SAM, but finding an appropriate perturbationradius is challenging. In this paper, we propose a bilevel optimizationframework called LEarning the perTurbation radiuS (LETS) to learn theperturbation radius for sharpness-aware minimization algorithms. Specifically,in the proposed LETS method, the upper-level problem aims at seeking a goodperturbation radius by minimizing the squared generalization gap between thetraining and validation losses, while the lower-level problem is the SAMoptimization problem. Moreover, the LETS method can be combined with anyvariant of SAM. Experimental results on various architectures and benchmarkdatasets in computer vision and natural language processing demonstrate theeffectiveness of the proposed LETS method in improving the performance of SAM.</description><author>Xuehao Wang, Weisen Jiang, Shuai Fu, Yu Zhang</author><pubDate>Thu, 15 Aug 2024 15:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08222v1</guid></item><item><title>GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization</title><link>http://arxiv.org/abs/2312.05133v2</link><description>This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing3D Gaussian representations to effectively factorize the scene into materialproperties, light, and geometry. The key contributions lie in three-fold. Wecompute the normal of each 3D Gaussian using the shortest eigenvector, with adirectional masking scheme forcing accurate normal estimation without externalsupervision. We adopt an efficient voxel-based indirect illumination tracingscheme that stores direction-aware outgoing radiance in each 3D Gaussian todisentangle secondary illumination for approximating multi-bounce lighttransport. To further enhance the illumination disentanglement, we represent ahigh-resolution environmental map with a learnable low-resolution map and alightweight, fully convolutional network. Our method achieves state-of-the-artperformance in both relighting and novel view synthesis tasks among therecently proposed inverse rendering methods while achieving real-timerendering. This substantiates our proposed method's efficacy and broadapplicability, highlighting its potential as an influential tool in variousreal-time interactive graphics applications such as material editing andrelighting. The code will be released at https://github.com/guduxiaolang/GIR.</description><author>Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jian Zhang, Bin Zhou, Errui Ding, Jingdong Wang</author><pubDate>Thu, 15 Aug 2024 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05133v2</guid></item><item><title>Quantifying Memorization and Detecting Training Data of Pre-trained Language Models using Japanese Newspaper</title><link>http://arxiv.org/abs/2404.17143v2</link><description>Dominant pre-trained language models (PLMs) have demonstrated the potentialrisk of memorizing and outputting the training data. While this concern hasbeen discussed mainly in English, it is also practically important to focus ondomain-specific PLMs. In this study, we pre-trained domain-specific GPT-2models using a limited corpus of Japanese newspaper articles and evaluatedtheir behavior. Experiments replicated the empirical finding that memorizationof PLMs is related to the duplication in the training data, model size, andprompt length, in Japanese the same as in previous English studies.Furthermore, we attempted membership inference attacks, demonstrating that thetraining data can be detected even in Japanese, which is the same trend as inEnglish. The study warns that domain-specific PLMs, sometimes trained withvaluable private data, can ''copy and paste'' on a large scale.</description><author>Shotaro Ishihara, Hiromu Takahashi</author><pubDate>Thu, 15 Aug 2024 15:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17143v2</guid></item><item><title>RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science</title><link>http://arxiv.org/abs/2408.08217v1</link><description>Large language models (LLMs) have enhanced our ability to rapidly analyze andclassify unstructured natural language data. However, concerns regarding cost,network limitations, and security constraints have posed challenges for theirintegration into work processes. In this study, we adopt a systems designapproach to employing LLMs as imperfect data annotators for downstreamsupervised learning tasks, introducing novel system intervention measures aimedat improving classification performance. Our methodology outperformsLLM-generated labels in seven of eight tests, demonstrating an effectivestrategy for incorporating LLMs into the design and deployment of specialized,supervised learning models present in many industry use cases.</description><author>David Farr, Nico Manzonelli, Iain Cruickshank, Jevin West</author><pubDate>Thu, 15 Aug 2024 15:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08217v1</guid></item><item><title>DPM: Clustering Sensitive Data through Separation</title><link>http://arxiv.org/abs/2307.02969v2</link><description>Clustering is an important tool for data exploration where the goal is tosubdivide a data set into disjoint clusters that fit well into the underlyingdata structure. When dealing with sensitive data, privacy-preserving algorithmsaim to approximate the non-private baseline while minimising the leakage ofsensitive information. State-of-the-art privacy-preserving clusteringalgorithms tend to output clusters that are good in terms of the standardmetrics, inertia, silhouette score, and clustering accuracy, however, theclustering result strongly deviates from the non-private KMeans baseline. Inthis work, we present a privacy-preserving clustering algorithm called \DPMthat recursively separates a data set into clusters based on a geometricalclustering approach. In addition, \DPM estimates most of the data-dependenthyper-parameters in a privacy-preserving way. We prove that \DPM preservesDifferential Privacy and analyse the utility guarantees of \DPM. Finally, weconduct an extensive empirical evaluation for synthetic and real-life datasets. We show that \DPM achieves state-of-the-art utility on the standardclustering metrics and yields a clustering result much closer to that of thepopular non-private KMeans algorithm without requiring the number of classes.</description><author>Johannes Liebenow, Yara Schütt, Tanya Braun, Marcel Gehrke, Florian Thaeter, Esfandiar Mohammadi</author><pubDate>Thu, 15 Aug 2024 15:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02969v2</guid></item><item><title>The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation</title><link>http://arxiv.org/abs/2408.08216v1</link><description>Image-to-Image translation in Generative Artificial Intelligence (GenerativeAI) has been a central focus of research, with applications spanninghealthcare, remote sensing, physics, chemistry, photography, and more. Amongthe numerous methodologies, Generative Adversarial Networks (GANs) withcontrastive learning have been particularly successful. This study aims todemonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replacethe Multi-layer Perceptron (MLP) method in generative AI, particularly in thesubdomain of image-to-image translation, to achieve better generative quality.Our novel approach replaces the two-layer MLP with a two-layer KAN in theexisting Contrastive Unpaired Image-to-Image Translation (CUT) model,developing the KAN-CUT model. This substitution favors the generation of moreinformative features in low-dimensional vector representations, whichcontrastive learning can utilize more effectively to produce high-qualityimages in the target domain. Extensive experiments, detailed in the resultssection, demonstrate the applicability of KAN in conjunction with contrastivelearning and GANs in Generative AI, particularly for image-to-imagetranslation. This work suggests that KAN could be a valuable component in thebroader generative AI domain.</description><author>Arpan Mahara, Naphtali D. Rishe, Liangdong Deng</author><pubDate>Thu, 15 Aug 2024 15:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08216v1</guid></item><item><title>Deep Learning Innovations for Underwater Waste Detection: An In-Depth Analysis</title><link>http://arxiv.org/abs/2405.18299v2</link><description>Addressing the issue of submerged underwater trash is crucial forsafeguarding aquatic ecosystems and preserving marine life. While identifyingdebris present on the surface of water bodies is straightforward, assessing theunderwater submerged waste is a challenge due to the image distortions causedby factors such as light refraction, absorption, suspended particles, colorshifts, and occlusion. This paper conducts a comprehensive review ofstate-of-the-art architectures and on the existing datasets to establish abaseline for submerged waste and trash detection. The primary goal remains toestablish the benchmark of the object localization techniques to be leveragedby advanced underwater sensors and autonomous underwater vehicles. The ultimateobjective is to explore the underwater environment, to identify, and removeunderwater debris. The absence of benchmarks (dataset or algorithm) in manyresearches emphasizes the need for a more robust algorithmic solution. Throughthis research, we aim to give performance comparative analysis of variousunderwater trash detection algorithms.</description><author>Jaskaran Singh Walia, Pavithra L K</author><pubDate>Thu, 15 Aug 2024 15:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18299v2</guid></item><item><title>An Event Structure-aware Generative Model for Biomedical Event Extraction</title><link>http://arxiv.org/abs/2408.06583v3</link><description>Biomedical Event Extraction (BEE) is a challenging task that involvesmodeling complex relationships between fine-grained entities in biomedicaltext. BEE has traditionally been formulated as a classification problem. Withthe recent technological advancements in large language models (LLMs),generation-based models that cast event extraction as a sequence generationproblem have attracted much attention from the NLP research communities.However, current generative models often overlook the importance ofcross-instance information from complex event structures such as nested eventsand overlapping events, which contribute quite significantly in the benchmarkdatasets. In this paper, we propose an event structure-aware generative modelcalled GenBEE, which can capture complex event structures in biomedical textfor biomedical event extraction. In particular, GenBEE constructs event promptsthat distill knowledge from LLMs for incorporating both label semantics andargument dependency relationships into the proposed model. In addition, GenBEEalso generates prefixes with event structural prompts to incorporate structuralfeatures for improving the model's overall performance. We have evaluated theproposed GenBEE model on three widely used biomedical event extractionbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show thatGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,and achieved competitive results when compared to the state-of-the-artclassification-based models on the PHEE dataset.</description><author>Haohan Yuan, Siu Cheung Hui, Haopeng Zhang</author><pubDate>Thu, 15 Aug 2024 15:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06583v3</guid></item><item><title>Moving Healthcare AI-Support Systems for Visually Detectable Diseases onto Constrained Devices</title><link>http://arxiv.org/abs/2408.08215v1</link><description>Image classification usually requires connectivity and access to the cloudwhich is often limited in many parts of the world, including hard to reachrural areas. TinyML aims to solve this problem by hosting AI assistants onconstrained devices, eliminating connectivity issues by processing data withinthe device itself, without internet or cloud access. This pilot study exploresthe use of tinyML to provide healthcare support with low spec devices in lowconnectivity environments, focusing on diagnosis of skin diseases and theethical use of AI assistants in a healthcare setting. To investigate this,10,000 images of skin lesions were used to train a model for classifyingvisually detectable diseases (VDDs). The model weights were then offloaded to aRaspberry Pi with a webcam attached, to be used for the classification of skinlesions without internet access. It was found that the developed prototypeachieved a test accuracy of 78% and a test loss of 1.08.</description><author>Tess Watt, Christos Chrysoulas, Peter J Barclay</author><pubDate>Thu, 15 Aug 2024 15:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08215v1</guid></item><item><title>Federated Fairness Analytics: Quantifying Fairness in Federated Learning</title><link>http://arxiv.org/abs/2408.08214v1</link><description>Federated Learning (FL) is a privacy-enhancing technology for distributed ML.By training models locally and aggregating updates - a federation learnstogether, while bypassing centralised data collection. FL is increasinglypopular in healthcare, finance and personal computing. However, it inheritsfairness challenges from classical ML and introduces new ones, resulting fromdifferences in data quality, client participation, communication constraints,aggregation methods and underlying hardware. Fairness remains an unresolvedissue in FL and the community has identified an absence of succinct definitionsand metrics to quantify fairness; to address this, we propose FederatedFairness Analytics - a methodology for measuring fairness. Our definition offairness comprises four notions with novel, corresponding metrics. They aresymptomatically defined and leverage techniques originating from XAI,cooperative game-theory and networking engineering. We tested a range ofexperimental settings, varying the FL approach, ML task and data settings. Theresults show that statistical heterogeneity and client participation affectfairness and fairness conscious approaches such as Ditto and q-FedAvgmarginally improve fairness-performance trade-offs. Using our techniques, FLpractitioners can uncover previously unobtainable insights into their system'sfairness, at differing levels of granularity in order to address fairnesschallenges in FL. We have open-sourced our work at:https://github.com/oscardilley/federated-fairness.</description><author>Oscar Dilley, Juan Marcelo Parra-Ullauri, Rasheed Hussain, Dimitra Simeonidou</author><pubDate>Thu, 15 Aug 2024 15:23:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08214v1</guid></item><item><title>Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion</title><link>http://arxiv.org/abs/2408.08212v1</link><description>While various approaches have recently been studied for bias identification,little is known about how implicit language that does not explicitly convey aviewpoint affects bias amplification in large language models.To examine theseverity of bias toward a view, we evaluated the performance of two downstreamtasks where the implicit and explicit knowledge of social groups were used.First, we present a stress test evaluation by using a biased model in edgecases of excessive bias scenarios. Then, we evaluate how LLMs calibratelinguistically in response to both implicit and explicit opinions when they arealigned with conflicting viewpoints. Our findings reveal a discrepancy in LLMperformance in identifying implicit and explicit opinions, with a generaltendency of bias toward explicit opinions of opposing stances. Moreover, thebias-aligned models generate more cautious responses using uncertainty phrasescompared to the unaligned (zero-shot) base models. The direct, incautiousresponses of the unaligned models suggest a need for further refinement ofdecisiveness by incorporating uncertainty markers to enhance their reliability,especially on socially nuanced topics with high subjectivity.</description><author>Abeer Aldayel, Areej Alokaili, Rehab Alahmadi</author><pubDate>Thu, 15 Aug 2024 15:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08212v1</guid></item><item><title>Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners</title><link>http://arxiv.org/abs/2407.15508v2</link><description>Large Language Models (LLMs) showcase remarkable performance and robustdeductive capabilities, yet their expansive size complicates deployment andraises environmental concerns due to substantial resource consumption. Therecent development of a quantization technique known as LearnableSingular-value Increment (LSI) has addressed some of these quantizationchallenges. Leveraging insights from LSI and our extensive research, we havedeveloped innovative methods that enhance the performance of quantized LLMs,particularly in low-bit settings. Our methods consistently deliverstate-of-the-art results across various quantization scenarios and offer deeptheoretical insights into the quantization process, elucidating the potentialof quantized models for widespread application.</description><author>Yifei Gao, Jie Ou, Lei Wang, Fanhua Shang, Jaji Wu, Jun Cheng</author><pubDate>Thu, 15 Aug 2024 15:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15508v2</guid></item><item><title>Learned Multimodal Compression for Autonomous Driving</title><link>http://arxiv.org/abs/2408.08211v1</link><description>Autonomous driving sensors generate an enormous amount of data. In thispaper, we explore learned multimodal compression for autonomous driving,specifically targeted at 3D object detection. We focus on camera and LiDARmodalities and explore several coding approaches. One approach involves jointcoding of fused modalities, while others involve coding one modality first,followed by conditional coding of the other modality. We evaluate theperformance of these coding schemes on the nuScenes dataset. Our experimentalresults indicate that joint coding of fused modalities yields better resultscompared to the alternatives.</description><author>Hadi Hadizadeh, Ivan V. Bajić</author><pubDate>Thu, 15 Aug 2024 15:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08211v1</guid></item><item><title>Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models</title><link>http://arxiv.org/abs/2408.08210v1</link><description>Recent advances in AI have been significantly driven by the capabilities oflarge language models (LLMs) to solve complex problems in ways that resemblehuman thinking. However, there is an ongoing debate about the extent to whichLLMs are capable of actual reasoning. Central to this debate are two keyprobabilistic concepts that are essential for connecting causes to theireffects: the probability of necessity (PN) and the probability of sufficiency(PS). This paper introduces a framework that is both theoretical and practical,aimed at assessing how effectively LLMs are able to replicate real-worldreasoning mechanisms using these probabilistic measures. By viewing LLMs asabstract machines that process information through a natural languageinterface, we examine the conditions under which it is possible to computesuitable approximations of PN and PS. Our research marks an important steptowards gaining a deeper understanding of when LLMs are capable of reasoning,as illustrated by a series of math examples.</description><author>Javier González, Aditya V. Nori</author><pubDate>Thu, 15 Aug 2024 15:19:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08210v1</guid></item><item><title>Examining Common Paradigms in Multi-Task Learning</title><link>http://arxiv.org/abs/2311.04698v5</link><description>While multi-task learning (MTL) has gained significant attention in recentyears, its underlying mechanisms remain poorly understood. Recent methods didnot yield consistent performance improvements over single task learning (STL)baselines, underscoring the importance of gaining more profound insights aboutchallenges specific to MTL. In our study, we investigate paradigms in MTL inthe context of STL: First, the impact of the choice of optimizer has only beenmildly investigated in MTL. We show the pivotal role of common STL tools suchas the Adam optimizer in MTL empirically in various experiments. To furtherinvestigate Adam's effectiveness, we theoretical derive a partial loss-scaleinvariance under mild assumptions. Second, the notion of gradient conflicts hasoften been phrased as a specific problem in MTL. We delve into the role ofgradient conflicts in MTL and compare it to STL. For angular gradient alignmentwe find no evidence that this is a unique problem in MTL. We emphasizedifferences in gradient magnitude as the main distinguishing factor. Overall,we find surprising similarities between STL and MTL suggesting to considermethods from both fields in a broader context.</description><author>Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott</author><pubDate>Thu, 15 Aug 2024 15:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04698v5</guid></item><item><title>LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation</title><link>http://arxiv.org/abs/2408.08208v1</link><description>Sequential recommendation systems fundamentally rely on users' historicalinteraction sequences, which are often contaminated by noisy interactions.Identifying these noisy interactions accurately without additional informationis particularly difficult due to the lack of explicit supervisory signals todenote noise. Large Language Models (LLMs), equipped with extensive openknowledge and semantic reasoning abilities, present a promising avenue tobridge this information gap. However, employing LLMs for denoising insequential recommendation introduces notable challenges: 1) Direct applicationof pretrained LLMs may not be competent for the denoising task, frequentlygenerating nonsensical responses; 2) Even after fine-tuning, the reliability ofLLM outputs remains questionable, especially given the complexity of the taskand th inherent hallucinatory issue of LLMs. To tackle these challenges, we propose LLM4DSR, a tailored approach fordenoising sequential recommendation using LLMs. We constructed aself-supervised fine-tuning task to activate LLMs' capabilities to identifynoisy items and suggest replacements. Furthermore, we developed an uncertaintyestimation module that ensures only high-confidence responses are utilized forsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing thecorrected sequences to be flexibly applied across various recommendationmodels. Extensive experiments validate the superiority of LLM4DSR over existingmethods across three datasets and three recommendation backbones.</description><author>Bohao Wang, Feng Liu, Jiawei Chen, Yudi Wu, Xingyu Lou, Jun Wang, Yan Feng, Chun Chen, Can Wang</author><pubDate>Thu, 15 Aug 2024 15:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08208v1</guid></item><item><title>WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian Splatting</title><link>http://arxiv.org/abs/2408.08206v1</link><description>The underwater 3D scene reconstruction is a challenging, yet interestingproblem with applications ranging from naval robots to VR experiences. Theproblem was successfully tackled by fully volumetric NeRF-based methods whichcan model both the geometry and the medium (water). Unfortunately, thesemethods are slow to train and do not offer real-time rendering. More recently,3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.However, because it is an explicit method that renders only the geometry, itcannot render the medium and is therefore unsuited for underwaterreconstruction. Therefore, we propose a novel approach that fuses volumetricrendering with 3DGS to handle underwater data effectively. Our method employs3DGS for explicit geometry representation and a separate volumetric field(queried once per pixel) for capturing the scattering medium. This dualrepresentation further allows the restoration of the scenes by removing thescattering medium. Our method outperforms state-of-the-art NeRF-based methodsin rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, itdoes so while offering real-time rendering performance, addressing theefficiency limitations of existing methods. Web:https://water-splatting.github.io</description><author>Huapeng Li, Wenxuan Song, Tianao Xu, Alexandre Elsig, Jonas Kulhanek</author><pubDate>Thu, 15 Aug 2024 15:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08206v1</guid></item><item><title>A Multi-task Adversarial Attack Against Face Authentication</title><link>http://arxiv.org/abs/2408.08205v1</link><description>Deep-learning-based identity management systems, such as face authenticationsystems, are vulnerable to adversarial attacks. However, existing attacks aretypically designed for single-task purposes, which means they are tailored toexploit vulnerabilities unique to the individual target rather than beingadaptable for multiple users or systems. This limitation makes them unsuitablefor certain attack scenarios, such as morphing, universal, transferable, andcounter attacks. In this paper, we propose a multi-task adversarial attackalgorithm called MTADV that are adaptable for multiple users or systems. Byinterpreting these scenarios as multi-task attacks, MTADV is applicable to bothsingle- and multi-task attacks, and feasible in the white- and gray-boxsettings. Furthermore, MTADV is effective against various face datasets,including LFW, CelebA, and CelebA-HQ, and can work with different deep learningmodels, such as FaceNet, InsightFace, and CurricularFace. Importantly, MTADVretains its feasibility as a single-task attack targeting a single user/system.To the best of our knowledge, MTADV is the first adversarial attack method thatcan target all of the aforementioned scenarios in one algorithm.</description><author>Hanrui Wang, Shuo Wang, Cunjian Chen, Massimo Tistarelli, Zhe Jin</author><pubDate>Thu, 15 Aug 2024 15:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08205v1</guid></item><item><title>Towards Practical Human Motion Prediction with LiDAR Point Clouds</title><link>http://arxiv.org/abs/2408.08202v1</link><description>Human motion prediction is crucial for human-centric multimedia understandingand interacting. Current methods typically rely on ground truth human poses asobserved input, which is not practical for real-world scenarios where only rawvisual sensor data is available. To implement these methods in practice, apre-phrase of pose estimation is essential. However, such two-stage approachesoften lead to performance degradation due to the accumulation of errors.Moreover, reducing raw visual data to sparse keypoint representationssignificantly diminishes the density of information, resulting in the loss offine-grained features. In this paper, we propose \textit{LiDAR-HMP}, the firstsingle-LiDAR-based 3D human motion prediction approach, which receives the rawLiDAR point cloud as input and forecasts future 3D human poses directly.Building upon our novel structure-aware body feature descriptor, LiDAR-HMPadaptively maps the observed motion manifold to future poses and effectivelymodels the spatial-temporal correlations of human motions for furtherrefinement of prediction results. Extensive experiments show that our methodachieves state-of-the-art performance on two public benchmarks and demonstratesremarkable robustness and efficacy in real-world deployments.</description><author>Xiao Han, Yiming Ren, Yichen Yao, Yujing Sun, Yuexin Ma</author><pubDate>Thu, 15 Aug 2024 15:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08202v1</guid></item><item><title>Heavy Labels Out! Dataset Distillation with Label Space Lightening</title><link>http://arxiv.org/abs/2408.08201v1</link><description>Dataset distillation or condensation aims to condense a large-scale trainingdataset into a much smaller synthetic one such that the training performance ofdistilled and original sets on neural networks are similar. Although the numberof training samples can be reduced substantially, current state-of-the-artmethods heavily rely on enormous soft labels to achieve satisfactoryperformance. As a result, the required storage can be comparable even tooriginal datasets, especially for large-scale ones. To solve this problem,instead of storing these heavy labels, we propose a novel label-lighteningframework termed HeLlO aiming at effective image-to-label projectors, withwhich synthetic labels can be directly generated online from synthetic images.Specifically, to construct such projectors, we leverage prior knowledge inopen-source foundation models, e.g., CLIP, and introduce a LoRA-likefine-tuning strategy to mitigate the gap between pre-trained and targetdistributions, so that original models for soft-label generation can bedistilled into a group of low-rank matrices. Moreover, an effective imageoptimization method is proposed to further mitigate the potential error betweenthe original and distilled label generators. Extensive experiments demonstratethat with only about 0.003% of the original storage required for a complete setof soft labels, we achieve comparable performance to current state-of-the-artdataset distillation methods on large-scale datasets. Our code will beavailable.</description><author>Ruonan Yu, Songhua Liu, Zigeng Chen, Jingwen Ye, Xinchao Wang</author><pubDate>Thu, 15 Aug 2024 15:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08201v1</guid></item><item><title>Exact Tensor Completion Powered by Slim Transforms</title><link>http://arxiv.org/abs/2402.03468v2</link><description>In this work, a tensor completion problem is studied, which aims to perfectlyrecover the tensor from partial observations. The existing theoreticalguarantee requires the involved transform to be orthogonal, which hinders itsapplications. In this paper, jumping out of the constraints of isotropy andself-adjointness, the theoretical guarantee of exact tensor completion witharbitrary linear transforms is established by directly operating the tensors inthe transform domain. With the enriched choices of transforms, a new analysisobtained by the proof discloses why slim transforms outperform their squarecounterparts from a theoretical level. Our model and proof greatly enhance theflexibility of tensor completion and extensive experiments validate thesuperiority of the proposed method.</description><author>Li Ge, Lin Chen, Yudong Chen, Xue Jiang</author><pubDate>Thu, 15 Aug 2024 15:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03468v2</guid></item><item><title>MagicFace: Training-free Universal-Style Human Image Customized Synthesis</title><link>http://arxiv.org/abs/2408.07433v2</link><description>Existing human image personalized generation methods often require tedioustraining: either fine-tuning with a few images or retraining on large-scaledatasets. In such cases, these methods are prone to overfitting and encounterdifficulties when personalizing individuals of diverse styles. Moreover, thesetraining-based approaches also struggle with multi-concept human imagecustomizing. To this end, we propose MagicFace, the first method foruniversal-style human image personalized synthesis that enablessingle/multi-concept customization for humans of any style in a training-freemanner. MagicFace introduces a coarse-to-fine generation pipeline, involvingtwo sequential stages: semantic scene construction and concept featureinjection. This is achieved by our Reference-aware Self-Attention (RSA) andRegion-grouped Blend Attention (RBA) mechanisms. Specifically, in the firststage, RSA enables the latent image to query features from reference conceptssimultaneously, extracting the coarse-grained overall semantic understanding tofacilitate the initial semantic layout establishment. In the second stage, weemploy an attention-based semantic segmentation method to pinpoint thegenerated regions of all concepts in the latent image at each step. Followingthis, RBA divides the pixels of the latent image into semantic groups, witheach group querying fine-grained features from its reference concept, whichensures precise attribute alignment and feature injection. Throughout thetwo-stage process, a weight mask strategy is employed to ensure the modelfocuses more on the reference concepts. Extensive experiments demonstrate oursuperiority in both human-centric subject-to-image synthesis and multi-concepthuman image customization. Our approach also can be applied to texturetransformation, further enhancing its versatility and applicability.</description><author>Yibin Wang, Weizhong Zhang, Cheng Jin</author><pubDate>Thu, 15 Aug 2024 15:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07433v2</guid></item><item><title>Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition</title><link>http://arxiv.org/abs/2311.09552v2</link><description>Recognizing fallacies is crucial for ensuring the quality and validity ofarguments across various domains. However, computational fallacy recognitionfaces challenges due to the diverse genres, domains, and types of fallaciesfound in datasets. This leads to a highly multi-class, and even multi-label,setup with substantial class imbalance. In this study, we aim to enhanceexisting models for fallacy recognition by incorporating additional context andby leveraging large language models to generate synthetic data, thus increasingthe representation of the infrequent classes. We experiment with GPT3.5 togenerate synthetic examples and we examine the impact of prompt settings forthis. Moreover, we explore zero-shot and few-shot scenarios to evaluate theeffectiveness of using the generated examples for training smaller modelswithin a unified fallacy recognition framework. Furthermore, we analyze theoverlap between the synthetic data and existing fallacy datasets. Finally, weinvestigate the usefulness of providing supplementary context for detectingfallacy types that need such context, e.g., diversion fallacies. Our evaluationresults demonstrate consistent improvements across fallacy types, datasets, andgenerators. The code and the synthetic datasets are all publicly available.</description><author>Tariq Alhindi, Smaranda Muresan, Preslav Nakov</author><pubDate>Thu, 15 Aug 2024 15:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09552v2</guid></item><item><title>A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection</title><link>http://arxiv.org/abs/2406.14176v2</link><description>This paper addresses the challenge of developing a robust audio-visualdeepfake detection model. In practical use cases, new generation algorithms arecontinually emerging, and these algorithms are not encountered during thedevelopment of detection methods. This calls for the generalization ability ofthe method. Additionally, to ensure the credibility of detection methods, it isbeneficial for the model to interpret which cues from the video indicate it isfake. Motivated by these considerations, we then propose a multi-stream fusionapproach with one-class learning as a representation-level regularizationtechnique. We study the generalization problem of audio-visual deepfakedetection by creating a new benchmark by extending and re-splitting theexisting FakeAVCeleb dataset. The benchmark contains four categories of fakevideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,and Unsynchronized videos). The experimental results demonstrate that ourapproach surpasses the previous models by a large margin. Furthermore, ourproposed framework offers interpretability, indicating which modality the modelidentifies as more likely to be fake. The source code is released athttps://github.com/bok-bok/MSOC.</description><author>Kyungbok Lee, You Zhang, Zhiyao Duan</author><pubDate>Thu, 15 Aug 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14176v2</guid></item><item><title>Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple Matching with Entity and Relation Texts</title><link>http://arxiv.org/abs/2407.15588v2</link><description>Cross-lingual entity alignment (EA) enables the integration of multipleknowledge graphs (KGs) across different languages, providing users withseamless access to diverse and comprehensive knowledge. Existing methods,mostly supervised, face challenges in obtaining labeled entity pairs. Toaddress this, recent studies have shifted towards self-supervised andunsupervised frameworks. Despite their effectiveness, these approaches havelimitations: (1) Relation passing: mainly focusing on the entity whileneglecting the semantic information of relations, (2) Isomorphic assumption:assuming isomorphism between source and target graphs, which leads to noise andreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noisein the textual features, especially when encountering inconsistent translationsor Out-Of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, anunsupervised and robust cross-lingual EA pipeline that jointly performsEntity-level and Relation-level Alignment by neighbor triple matching strategyusing semantic textual features of relations and entities. Its refinement stepiteratively enhances results by fusing entity-level and relation-levelalignments based on neighbor triple matching. The additional verification stepexamines the entities' neighbor triples as the linearized text. ThisAlign-then-Verify pipeline rigorously assesses alignment results, achievingnear-perfect alignment even in the presence of noisy textual features ofentities. Our extensive experiments demonstrate that the robustness and generalapplicability of ERAlign improved the accuracy and effectiveness of EA tasks,contributing significantly to knowledge-oriented applications.</description><author>Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee</author><pubDate>Thu, 15 Aug 2024 14:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15588v2</guid></item><item><title>Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation</title><link>http://arxiv.org/abs/2408.08192v1</link><description>Mean field games (MFGs) model the interactions within a large-populationmulti-agent system using the population distribution. Traditional learningmethods for MFGs are based on fixed-point iteration (FPI), which calculatesbest responses and induced population distribution separately and sequentially.However, FPI-type methods suffer from inefficiency and instability, due tooscillations caused by the forward-backward procedure. This paper considers anonline learning method for MFGs, where an agent updates its policy andpopulation estimates simultaneously and fully asynchronously, resulting in asimple stochastic gradient descent (SGD) type method called SemiSGD. Not onlydoes SemiSGD exhibit numerical stability and efficiency, but it also provides anovel perspective by treating the value function and population distribution asa unified parameter. We theoretically show that SemiSGD directs this unifiedparameter along a descent direction to the mean field equilibrium. Motivated bythis perspective, we develop a linear function approximation (LFA) for both thevalue function and the population distribution, resulting in the firstpopulation-aware LFA for MFGs on continuous state-action space. Finite-timeconvergence and approximation error analysis are provided for SemiSGD equippedwith population-aware LFA.</description><author>Chenyu Zhang, Xu Chen, Xuan Di</author><pubDate>Thu, 15 Aug 2024 14:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08192v1</guid></item><item><title>Beyond Full Label: Single-Point Prompt for Infrared Small Target Label Generation</title><link>http://arxiv.org/abs/2408.08191v1</link><description>In this work, we make the first attempt to construct a learning-basedsingle-point annotation paradigm for infrared small target label generation(IRSTLG). Our intuition is that label generation requires just one more pointprompt than target detection: IRSTLG can be regarded as an infrared smalltarget detection (IRSTD) task with the target location hint. Based on thisinsight, we introduce an energy double guided single-point prompt (EDGSP)framework, which adeptly transforms the target detection network into a refinedlabel generation method. Specifically, the proposed EDGSP includes: 1) targetenergy initialization (TEI) to create a foundational outline for sufficientshape evolution of pseudo label, 2) double prompt embedding (DPE) for rapidlocalization of interested regions and reinforcement of individual differencesto avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminatefalse alarms. Experimental results show that pseudo labels generated by threebaselines equipped with EDGSP achieve 100% object-level probability ofdetection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1kdatasets, with a pixel-level intersection over union (IoU) improvement of13.28% over state-of-the-art label generation methods. Additionally, thedownstream detection task reveals that our centroid-annotated pseudo labelssurpass full labels, even with coarse single-point annotations, it stillachieves 99.5% performance of full labeling.</description><author>Shuai Yuan, Hanlin Qin, Renke Kou, Xiang Yan, Zechuan Li, Chenxu Peng, Abd-Krim Seghouane</author><pubDate>Thu, 15 Aug 2024 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08191v1</guid></item><item><title>FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance</title><link>http://arxiv.org/abs/2408.08189v1</link><description>Synthesizing motion-rich and temporally consistent videos remains a challengein artificial intelligence, especially when dealing with extended durations.Existing text-to-video (T2V) models commonly employ spatial cross-attention fortext control, equivalently guiding different frame generations withoutframe-specific textual guidance. Thus, the model's capacity to comprehend thetemporal logic conveyed in prompts and generate videos with coherent motion isrestricted. To tackle this limitation, we introduce FancyVideo, an innovativevideo generator that improves the existing text-control mechanism with thewell-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGMincorporates the Temporal Information Injector (TII), Temporal Affinity Refiner(TAR), and Temporal Feature Booster (TFB) at the beginning, middle, and end ofcross-attention, respectively, to achieve frame-specific textual guidance.Firstly, TII injects frame-specific information from latent features into textconditions, thereby obtaining cross-frame textual conditions. Then, TAR refinesthe correlation matrix between cross-frame textual conditions and latentfeatures along the time dimension. Lastly, TFB boosts the temporal consistencyof latent features. Extensive experiments comprising both quantitative andqualitative evaluations demonstrate the effectiveness of FancyVideo. Ourapproach achieves state-of-the-art T2V generation results on the EvalCrafterbenchmark and facilitates the synthesis of dynamic and consistent videos. Thevideo show results can be available at https://fancyvideo.github.io/, and wewill make our code and model weights publicly available.</description><author>Jiasong Feng, Ao Ma, Jing Wang, Bo Cheng, Xiaodan Liang, Dawei Leng, Yuhui Yin</author><pubDate>Thu, 15 Aug 2024 14:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08189v1</guid></item><item><title>Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy</title><link>http://arxiv.org/abs/2408.08188v1</link><description>Long-horizon planning is hindered by challenges such as uncertaintyaccumulation, computational complexity, delayed rewards and incompleteinformation. This work proposes an approach to exploit the task hierarchy fromhuman instructions to facilitate multi-robot planning. Using Large LanguageModels (LLMs), we propose a two-step approach to translate multi-sentenceinstructions into a structured language, Hierarchical Linear Temporal Logic(LTL), which serves as a formal representation for planning. Initially, LLMstransform the instructions into a hierarchical representation defined asHierarchical Task Tree, capturing the logical and temporal relations amongtasks. Following this, a domain-specific fine-tuning of LLM translatessub-tasks of each task into flat LTL formulas, aggregating them to formhierarchical LTL specifications. These specifications are then leveraged forplanning using off-the-shelf planners. Our framework not only bridges the gapbetween instructions and algorithmic planning but also showcases the potentialof LLMs in harnessing hierarchical reasoning to automate multi-robot taskplanning. Through evaluations in both simulation and real-world experimentsinvolving human participants, we demonstrate that our method can handle morecomplex instructions compared to existing methods. The results indicate thatour approach achieves higher success rates and lower costs in multi-robot taskallocation and plan generation. Demos videos are available athttps://youtu.be/7WOrDKxIMIs .</description><author>Shaojun Xu, Xusheng Luo, Yutong Huang, Letian Leng, Ruixuan Liu, Changliu Liu</author><pubDate>Thu, 15 Aug 2024 14:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08188v1</guid></item><item><title>Data-driven identification of latent port-Hamiltonian systems</title><link>http://arxiv.org/abs/2408.08185v1</link><description>Conventional physics-based modeling techniques involve high effort, e.g.,time and expert knowledge, while data-driven methods often lackinterpretability, structure, and sometimes reliability. To mitigate this, wepresent a data-driven system identification framework that derives models inthe port-Hamiltonian (pH) formulation. This formulation is suitable formulti-physical systems while guaranteeing the useful system theoreticalproperties of passivity and stability. Our framework combines linear andnonlinear reduction with structured, physics-motivated system identification.In this process, high-dimensional state data obtained from possibly nonlinearsystems serves as input for an autoencoder, which then performs two tasks: (i)nonlinearly transforming and (ii) reducing this data onto a low-dimensionallatent space. In this space, a linear pH system, that satisfies the pHproperties per construction, is parameterized by the weights of a neuralnetwork. The mathematical requirements are met by defining the pH matricesthrough Cholesky factorizations. The neural networks that define the coordinatetransformation and the pH system are identified in a joint optimization processto match the dynamics observed in the data while defining a linear pH system inthe latent space. The learned, low-dimensional pH system can describe evennonlinear systems and is rapidly computable due to its small size. The methodis exemplified by a parametric mass-spring-damper and a nonlinear pendulumexample, as well as the high-dimensional model of a disc brake with linearthermoelastic behavior.</description><author>Johannes Rettberg, Jonas Kneifl, Julius Herb, Patrick Buchfink, Jörg Fehr, Bernard Haasdonk</author><pubDate>Thu, 15 Aug 2024 14:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08185v1</guid></item><item><title>Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion</title><link>http://arxiv.org/abs/2408.08184v1</link><description>This work addresses the challenge of quantifying originality in text-to-image(T2I) generative diffusion models, with a focus on copyright originality. Webegin by evaluating T2I models' ability to innovate and generalize throughcontrolled experiments, revealing that stable diffusion models can effectivelyrecreate unseen elements with sufficiently diverse training data. Then, our keyinsight is that concepts and combinations of image elements the model isfamiliar with, and saw more during training, are more concisly represented inthe model's latent space. We hence propose a method that leverages textualinversion to measure the originality of an image based on the number of tokensrequired for its reconstruction by the model. Our approach is inspired by legaldefinitions of originality and aims to assess whether a model can produceoriginal content without relying on specific prompts or having the trainingdata of the model. We demonstrate our method using both a pre-trained stablediffusion model and a synthetic dataset, showing a correlation between thenumber of tokens and image originality. This work contributes to theunderstanding of originality in generative models and has implications forcopyright infringement cases.</description><author>Adi Haviv, Shahar Sarfaty, Uri Hacohen, Niva Elkin-Koren, Roi Livni, Amit H Bermano</author><pubDate>Thu, 15 Aug 2024 14:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08184v1</guid></item><item><title>Your Turn: Real-World Turning Angle Estimation for Parkinson's Disease Severity Assessment</title><link>http://arxiv.org/abs/2408.08182v1</link><description>People with Parkinson's Disease (PD) often experience progressively worseninggait, including changes in how they turn around, as the disease progresses.Existing clinical rating tools are not capable of capturing hour-by-hourvariations of PD symptoms, as they are confined to brief assessments withinclinic settings. Measuring real-world gait turning angles continuously andpassively is a component step towards using gait characteristics as sensitiveindicators of disease progression in PD. This paper presents a deeplearning-based approach to automatically quantify turning angles by extracting3D skeletons from videos and calculating the rotation of hip and knee joints.We utilise state-of-the-art human pose estimation models, Fastpose and StridedTransformer, on a total of 1386 turning video clips from 24 subjects (12 peoplewith PD and 12 healthy control volunteers), trimmed from a PD dataset ofunscripted free-living videos in a home-like setting (Turn-REMAP). We alsocurate a turning video dataset, Turn-H3.6M, from the public Human3.6M humanpose benchmark with 3D ground truth, to further validate our method. Previousgait research has primarily taken place in clinics or laboratories evaluatingscripted gait outcomes, but this work focuses on real-world settings wherecomplexities exist, such as baggy clothing and poor lighting. Due todifficulties in obtaining accurate ground truth data in a free-living setting,we quantise the angle into the nearest bin $45^\circ$ based on the manuallabelling of expert clinicians. Our method achieves a turning calculationaccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\deg}, and a weightedprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore theuse of single monocular camera data to quantify turns by PD patients in a homesetting.</description><author>Qiushuo Cheng, Catherine Morgan, Arindam Sikdar, Alessandro Masullo, Alan Whone, Majid Mirmehdi</author><pubDate>Thu, 15 Aug 2024 14:36:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08182v1</guid></item><item><title>Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data</title><link>http://arxiv.org/abs/2408.07673v2</link><description>A grid search, at the cost of training and testing a large number of models,is an effective way to optimize the prediction performance of deep learningmodels. A challenging task concerning grid search is the time management.Without a good time management scheme, a grid search can easily be set off as amission that will not finish in our lifetime. In this study, we introduce aheuristic three-stage mechanism for managing the running time of low-budgetgrid searches, and the sweet-spot grid search (SSGS) and randomized grid search(RGS) strategies for improving model prediction performance, in predicting the5-year, 10-year, and 15-year risk of breast cancer metastasis. We develop deepfeedforward neural network (DFNN) models and optimize them through gridsearches. We conduct eight cycles of grid searches by applying our three-stagemechanism and SSGS and RGS strategies. We conduct various SHAP analysesincluding unique ones that interpret the importance of the DFNN-modelhyperparameters. Our results show that grid search can greatly improve modelprediction. The grid searches we conducted improved the risk prediction of5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and17.3% respectively, over the average performance of all corresponding models wetrained using the RGS strategy. We not only demonstrate best model performancebut also characterize grid searches from various aspects such as theircapabilities of discovering decent models and the unit grid search time. Thethree-stage mechanism worked effectively. It made our low-budget grid searchesfeasible and manageable, and in the meantime helped improve model predictionperformance. Our SHAP analyses identified both clinical risk factors importantfor the prediction of future risk of breast cancer metastasis, and DFNN-modelhyperparameters important to the prediction of performance scores.</description><author>Xia Jiang, Yijun Zhou, Chuhan Xu, Adam Brufsky, Alan Wells</author><pubDate>Thu, 15 Aug 2024 14:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07673v2</guid></item><item><title>Machine learning empowered Modulation detection for OFDM-based signals</title><link>http://arxiv.org/abs/2408.08179v1</link><description>We propose a blind ML-based modulation detection for OFDM-based technologies.Unlike previous works that assume an ideal environment with precise knowledgeof subcarrier count and cyclic prefix location, we consider blind modulationdetection while accounting for realistic environmental parameters andimperfections. Our approach employs a ResNet network to simultaneously detectthe modulation type and accurately locate the cyclic prefix. Specifically,after eliminating the environmental impact from the signal and accuratelyextracting the OFDM symbols, we convert these symbols into scatter plots. Dueto their unique shapes, these scatter plots are then classified using ResNet.As a result, our proposed modulation classification method can be applied toany OFDM-based technology without prior knowledge of the transmitted signal. Weevaluate its performance across various modulation schemes and subcarriernumbers. Simulation results show that our method achieves a modulationdetection accuracy exceeding $80\%$ at an SNR of $10$ dB and $95\%$ at an SNRof $25$ dB.</description><author>Ali Pourranjbar, Georges Kaddoum, Verdier Assoume Mba, Sahil Garg, Satinder Singh</author><pubDate>Thu, 15 Aug 2024 14:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08179v1</guid></item><item><title>Towards a Science Exocortex</title><link>http://arxiv.org/abs/2406.17809v2</link><description>Artificial intelligence (AI) methods are poised to revolutionize intellectualwork, with generative AI enabling automation of text analysis, text generation,and simple decision making or reasoning. The impact to science is only justbeginning, but the opportunity is significant since scientific research reliesfundamentally on extended chains of cognitive work. Here, we review the stateof the art in agentic AI systems, and discuss how these methods could beextended to have even greater impact on science. We propose the development ofan exocortex, a synthetic extension of a person's cognition. A scienceexocortex could be designed as a swarm of AI agents, with each agentindividually streamlining specific researcher tasks, and whoseinter-communication leads to emergent behavior that greatly extend theresearcher's cognition and volition.</description><author>Kevin G. Yager</author><pubDate>Thu, 15 Aug 2024 14:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17809v2</guid></item><item><title>Separable Hamiltonian Neural Networks</title><link>http://arxiv.org/abs/2309.01069v4</link><description>Hamiltonian neural networks (HNNs) are state-of-the-art models that regressthe vector field of a dynamical system under the learning bias of Hamilton'sequations. A recent observation is that embedding a bias regarding the additiveseparability of the Hamiltonian reduces the regression complexity and improvesregression performance. We propose separable HNNs that embed additiveseparability within HNNs using observational, learning, and inductive biases.We show that the proposed models are more effective than the HNN at regressingthe Hamiltonian and the vector field. Consequently, the proposed models predictthe dynamics and conserve the total energy of the Hamiltonian system moreaccurately.</description><author>Zi-Yu Khoo, Dawen Wu, Jonathan Sze Choong Low, Stéphane Bressan</author><pubDate>Thu, 15 Aug 2024 14:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01069v4</guid></item><item><title>Localized Sparse Principal Component Analysis of Multivariate Time Series in Frequency Domain</title><link>http://arxiv.org/abs/2408.08177v1</link><description>Principal component analysis has been a main tool in multivariate analysisfor estimating a low dimensional linear subspace that explains most of thevariability in the data. However, in high-dimensional regimes, naive estimatesof the principal loadings are not consistent and difficult to interpret. In thecontext of time series, principal component analysis of spectral densitymatrices can provide valuable, parsimonious information about the behavior ofthe underlying process, particularly if the principal components areinterpretable in that they are sparse in coordinates and localized in frequencybands. In this paper, we introduce a formulation and consistent estimationprocedure for interpretable principal component analysis for high-dimensionaltime series in the frequency domain. An efficient frequency-sequentialalgorithm is developed to compute sparse-localized estimates of thelow-dimensional principal subspaces of the signal process. The method ismotivated by and used to understand neurological mechanisms from high-densityresting-state EEG in a study of first episode psychosis.</description><author>Jamshid Namdari, Amita Manatunga, Fabio Ferrarelli, Robert Krafty</author><pubDate>Thu, 15 Aug 2024 14:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08177v1</guid></item><item><title>PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition</title><link>http://arxiv.org/abs/2403.17695v2</link><description>We present PlainMamba: a simple non-hierarchical state space model (SSM)designed for general visual recognition. The recent Mamba model has shown howSSMs can be highly competitive with other architectures on sequential data andinitial attempts have been made to apply it to images. In this paper, wefurther adapt the selective scanning process of Mamba to the visual domain,enhancing its ability to learn features from two-dimensional images by (i) acontinuous 2D scanning process that improves spatial continuity by ensuringadjacency of tokens in the scanning sequence, and (ii) direction-aware updatingwhich enables the model to discern the spatial relations of tokens by encodingdirectional information. Our architecture is designed to be easy to use andeasy to scale, formed by stacking identical PlainMamba blocks, resulting in amodel with constant width throughout all layers. The architecture is furthersimplified by removing the need for special tokens. We evaluate PlainMamba on avariety of visual recognition tasks, achieving performance gains over previousnon-hierarchical models and is competitive with hierarchical alternatives. Fortasks requiring high-resolution inputs, in particular, PlainMamba requires muchless computing while maintaining high performance. Code and models areavailable at: https://github.com/ChenhongyiYang/PlainMamba .</description><author>Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley</author><pubDate>Thu, 15 Aug 2024 14:30:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17695v2</guid></item><item><title>Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature Learning</title><link>http://arxiv.org/abs/2407.13322v3</link><description>Many remote photoplethysmography (rPPG) estimation models have achievedpromising performance in the training domain but often fail to accuratelyestimate physiological signals or heart rates (HR) in the target domains.Domain generalization (DG) or domain adaptation (DA) techniques are thereforeadopted during the offline training stage to adapt the model to eitherunobserved or observed target domains by utilizing all available source domaindata. However, in rPPG estimation problems, the adapted model usuallyencounters challenges in estimating target data with significant domainvariation. In contrast, Test-Time Adaptation (TTA) enables the model toadaptively estimate rPPG signals in various unseen domains by online adaptingto unlabeled target data without referring to any source data. In this paper,we first establish a new TTA-rPPG benchmark that encompasses various domaininformation and HR distributions to simulate the challenges encountered inreal-world rPPG estimation. Next, we propose a novel synthetic signal-guidedrPPG estimation framework to address the forgetting issue during the TTA stageand to enhance the adaptation capability of the pre-trained rPPG model. To thisend, we develop a synthetic signal-guided feature learning method bysynthesizing pseudo rPPG signals as pseudo ground truths to guide a conditionalgenerator in generating latent rPPG features. In addition, we design aneffective spectral-based entropy minimization technique to encourage the rPPGmodel to learn new target domain information. Both the generated rPPG featuresand synthesized rPPG signals prevent the rPPG model from overfitting to targetdata and forgetting previously acquired knowledge, while also broadly coveringvarious heart rate (HR) distributions. Our extensive experiments on theTTA-rPPG benchmark show that the proposed method achieves superior performance.</description><author>Pei-Kai Huang, Tzu-Hsien Chen, Ya-Ting Chan, Kuan-Wen Chen, Chiou-Ting Hsu</author><pubDate>Thu, 15 Aug 2024 14:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13322v3</guid></item><item><title>Towards flexible perception with visual memory</title><link>http://arxiv.org/abs/2408.08172v1</link><description>Training a neural network is a monolithic endeavor, akin to carving knowledgeinto stone: once the process is completed, editing the knowledge in a networkis nearly impossible, since all information is distributed across the network'sweights. We here explore a simple, compelling alternative by marrying therepresentational power of deep neural networks with the flexibility of adatabase. Decomposing the task of image classification into image similarity(from a pre-trained embedding) and search (via fast nearest neighbor retrievalfrom a knowledge database), we build a simple and flexible visual memory thathas the following key capabilities: (1.) The ability to flexibly add dataacross scales: from individual samples all the way to entire classes andbillion-scale data; (2.) The ability to remove data through unlearning andmemory pruning; (3.) An interpretable decision-mechanism on which we canintervene to control its behavior. Taken together, these capabilitiescomprehensively demonstrate the benefits of an explicit visual memory. We hopethat it might contribute to a conversation on how knowledge should berepresented in deep vision models -- beyond carving it in ``stone'' weights.</description><author>Robert Geirhos, Priyank Jaini, Austin Stone, Sourabh Medapati, Xi Yi, George Toderici, Abhijit Ogale, Jonathon Shlens</author><pubDate>Thu, 15 Aug 2024 14:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08172v1</guid></item><item><title>Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook</title><link>http://arxiv.org/abs/2401.06542v3</link><description>In the realm of modern autonomous driving, the perception system isindispensable for accurately assessing the state of the surroundingenvironment, thereby enabling informed prediction and planning. The key step tothis system is related to 3D object detection that utilizes vehicle-mountedsensors such as LiDAR and cameras to identify the size, the category, and thelocation of nearby objects. Despite the surge in 3D object detection methodsaimed at enhancing detection precision and efficiency, there is a gap in theliterature that systematically examines their resilience against environmentalvariations, noise, and weather changes. This study emphasizes the importance ofrobustness, alongside accuracy and latency, in evaluating perception systemsunder practical scenarios. Our work presents an extensive survey ofcamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,thoroughly evaluating their trade-off between accuracy, latency, androbustness, particularly on datasets like KITTI-C and nuScenes-C to ensure faircomparisons. Among these, multi-modal 3D detection approaches exhibit superiorrobustness, and a novel taxonomy is introduced to reorganize the literature forenhanced clarity. This survey aims to offer a more practical perspective on thecurrent capabilities and the constraints of 3D object detection algorithms inreal-world applications, thus steering future research towardsrobustness-centric advancements.</description><author>Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, Caiyan Jia</author><pubDate>Thu, 15 Aug 2024 14:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06542v3</guid></item><item><title>Two Completely Parameter-Free Alternating Gradient Projection Algorithms for Nonconvex-(strongly) Concave Minimax Problems</title><link>http://arxiv.org/abs/2407.21372v2</link><description>Due to their importance in various emerging applications, efficientalgorithms for solving minimax problems have recently received increasingattention. However, many existing algorithms require prior knowledge of theproblem parameters in order to achieve optimal iteration complexity. In thispaper, we propose two completely parameter-free alternating gradient projectionalgorithms, i.e., the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm, tosolve the smooth nonconvex-strongly concave and nonconvex-concave minimaxproblems respectively using a backtracking strategy, which does not requireprior knowledge of parameters such as the Lipschtiz constant $L$ or thestrongly concave constant $\mu$. Moreover, we show that the total number ofgradient calls of the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm toobtain an $\varepsilon$-stationary point is upper bounded by $\mathcal{O}\left(L\kappa^3\varepsilon^{-2} \right)$ and $\mathcal{O}\left( L^4\varepsilon^{-4}\right)$ respectively, where $\kappa$ is the condition number. As far as weknow, the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm are the firstcompletely parameter-free algorithms for solving nonconvex-strongly concaveminimax problems and nonconvex-concave minimax problems respectively. Numericalresults validate the efficiency of the proposed PF-AGP algorithm.</description><author>Junnan Yang, Huiling Zhang, Zi Xu</author><pubDate>Thu, 15 Aug 2024 14:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21372v2</guid></item></channel></rss>