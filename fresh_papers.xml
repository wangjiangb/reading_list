<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 08 Nov 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>OtterHD: A High-Resolution Multi-modality Model</title><link>http://arxiv.org/abs/2311.04219v1</link><description>In this paper, we present OtterHD-8B, an innovative multimodal model evolvedfrom Fuyu-8B, specifically engineered to interpret high-resolution visualinputs with granular precision. Unlike conventional models that are constrainedby fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexibleinput dimensions, ensuring its versatility across various inferencerequirements. Alongside this model, we introduce MagnifierBench, an evaluationframework designed to scrutinize models' ability to discern minute details andspatial relationships of small objects. Our comparative analysis reveals thatwhile current leading models falter on this benchmark, OtterHD-8B, particularlywhen directly processing high-resolution inputs, outperforms its counterpartsby a substantial margin. The findings illuminate the structural variances invisual information processing among different models and the influence that thevision encoders' pre-training resolution disparities have on modeleffectiveness within such benchmarks. Our study highlights the critical role offlexibility and high-resolution input capabilities in large multimodal modelsand also exemplifies the potential inherent in the Fuyu architecture'ssimplicity for handling complex visual data.</description><author>Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu</author><pubDate>Tue, 07 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04219v1</guid></item><item><title>Towards Garment Sewing Pattern Reconstruction from a Single Image</title><link>http://arxiv.org/abs/2311.04218v1</link><description>Garment sewing pattern represents the intrinsic rest shape of a garment, andis the core for many applications like fashion design, virtual try-on, anddigital avatars. In this work, we explore the challenging problem of recoveringgarment sewing patterns from daily photos for augmenting these applications. Tosolve the problem, we first synthesize a versatile dataset, named SewFactory,which consists of around 1M images and ground-truth sewing patterns for modeltraining and quantitative evaluation. SewFactory covers a wide range of humanposes, body shapes, and sewing patterns, and possesses realistic appearancesthanks to the proposed human texture synthesis network. Then, we propose atwo-level Transformer network called Sewformer, which significantly improvesthe sewing pattern prediction performance. Extensive experiments demonstratethat the proposed framework is effective in recovering sewing patterns and wellgeneralizes to casually-taken human photos. Code, dataset, and pre-trainedmodels are available at: https://sewformer.github.io.</description><author>Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan</author><pubDate>Tue, 07 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04218v1</guid></item><item><title>Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning</title><link>http://arxiv.org/abs/2311.04215v1</link><description>Personal sensing, leveraging data passively and near-continuously collectedwith wearables from patients in their ecological environment, is a promisingparadigm to monitor mood disorders (MDs), a major determinant of worldwidedisease burden. However, collecting and annotating wearable data is veryresource-intensive. Studies of this kind can thus typically afford to recruitonly a couple dozens of patients. This constitutes one of the major obstaclesto applying modern supervised machine learning techniques to MDs detection. Inthis paper, we overcome this data bottleneck and advance the detection of MDsacute episode vs stable state from wearables data on the back of recentadvances in self-supervised learning (SSL). This leverages unlabelled data tolearn representations during pre-training, subsequently exploited for asupervised task. First, we collected open-access datasets recording with anEmpatica E4 spanning different, unrelated to MD monitoring, personal sensingtasks -- from emotion recognition in Super Mario players to stress detection inundergraduates -- and devised a pre-processing pipeline performing on-/off-bodydetection, sleep-wake detection, segmentation, and (optionally) featureextraction. With 161 E4-recorded subjects, we introduce E4SelfLearning, thelargest to date open access collection, and its pre-processing pipeline.Second, we show that SSL confidently outperforms fully-supervised pipelinesusing either our novel E4-tailored Transformer architecture (E4mer) orclassical baseline XGBoost: 81.23% against 75.35% (E4mer) and 72.02% (XGBoost)correctly classified recording segments from 64 (half acute, half stable)patients. Lastly, we illustrate that SSL performance is strongly associatedwith the specific surrogate task employed for pre-training as well as withunlabelled data availability.</description><author>Filippo Corponi, Bryan M. Li, Gerard Anmella, Clàudia Valenzuela-Pascual, Ariadna Mas, Isabella Pacchiarotti, Marc Valentí, Iria Grande, Antonio Benabarre, Marina Garriga, Eduard Vieta, Allan H Young, Stephen M. Lawrie, Heather C. Whalley, Diego Hidalgo-Mazzei, Antonio Vergari</author><pubDate>Tue, 07 Nov 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04215v1</guid></item><item><title>Video Instance Matting</title><link>http://arxiv.org/abs/2311.04212v1</link><description>Conventional video matting outputs one alpha matte for all instancesappearing in a video frame so that individual instances are not distinguished.While video instance segmentation provides time-consistent instance masks,results are unsatisfactory for matting applications, especially due to appliedbinarization. To remedy this deficiency, we propose Video InstanceMatting~(VIM), that is, estimating alpha mattes of each instance at each frameof a video sequence. To tackle this challenging problem, we present MSG-VIM, aMask Sequence Guided Video Instance Matting neural network, as a novel baselinemodel for VIM. MSG-VIM leverages a mixture of mask augmentations to makepredictions robust to inaccurate and inconsistent mask guidance. Itincorporates temporal mask and temporal feature guidance to improve thetemporal consistency of alpha matte predictions. Furthermore, we build a newbenchmark for VIM, called VIM50, which comprises 50 video clips with multiplehuman instances as foreground objects. To evaluate performances on the VIMtask, we introduce a suitable metric called Video Instance-aware MattingQuality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50benchmark and outperforms existing methods by a large margin. The project isopen-sourced at https://github.com/SHI-Labs/VIM.</description><author>Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Humphrey Shi</author><pubDate>Tue, 07 Nov 2023 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04212v1</guid></item><item><title>Deep Hashing via Householder Quantization</title><link>http://arxiv.org/abs/2311.04207v1</link><description>Hashing is at the heart of large-scale image similarity search, and recentmethods have been substantially improved through deep learning techniques. Suchalgorithms typically learn continuous embeddings of the data. To avoid asubsequent costly binarization step, a common solution is to employ lossfunctions that combine a similarity learning term (to ensure similar images aregrouped to nearby embeddings) and a quantization penalty term (to ensure thatthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,the interaction between these two terms can make learning harder and theembeddings worse. We propose an alternative quantization strategy thatdecomposes the learning problem in two stages: first, perform similaritylearning over the embedding space with no quantization; second, find an optimalorthogonal transformation of the embeddings so each coordinate of the embeddingis close to its sign, and then quantize the transformed embedding through thesign function. In the second step, we parametrize orthogonal transformationsusing Householder matrices to efficiently leverage stochastic gradient descent.Since similarity measures are usually invariant under orthogonaltransformations, this quantization strategy comes at no cost in terms ofperformance. The resulting algorithm is unsupervised, fast, hyperparameter-freeand can be run on top of any existing deep hashing or metric learningalgorithm. We provide extensive experimental results showing that this approachleads to state-of-the-art performance on widely used image datasets, and,unlike other quantization strategies, brings consistent improvements inperformance to existing deep hashing algorithms.</description><author>Lucas R. Schwengber, Lucas Resende, Paulo Orenstein, Roberto I. Oliveira</author><pubDate>Tue, 07 Nov 2023 18:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04207v1</guid></item><item><title>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</title><link>http://arxiv.org/abs/2311.04205v1</link><description>Misunderstandings arise not only in interpersonal communication but alsobetween humans and Large Language Models (LLMs). Such discrepancies can makeLLMs interpret seemingly unambiguous questions in unexpected ways, yieldingincorrect responses. While it is widely acknowledged that the quality of aprompt, such as a question, significantly impacts the quality of the responseprovided by LLMs, a systematic method for crafting questions that LLMs canbetter comprehend is still underdeveloped. In this paper, we present a methodnamed `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expandquestions posed by humans and provide responses in a single prompt. Thisapproach serves as a simple yet effective prompting method for improvingperformance. We also introduce a two-step variant of RaR, where a rephrasingLLM first rephrases the question and then passes the original and rephrasedquestions together to a different responding LLM. This facilitates theeffective utilization of rephrased questions generated by one LLM with another.Our experiments demonstrate that our methods significantly improve theperformance of different models across a wide range to tasks. We furtherprovide a comprehensive comparison between RaR and the popular Chain-of-Thought(CoT) methods, both theoretically and empirically. We show that RaR iscomplementary to CoT and can be combined with CoT to achieve even betterperformance. Our work not only contributes to enhancing LLM performanceefficiently and effectively but also sheds light on a fair evaluation of LLMcapabilities. Data and codes are available athttps://github.com/uclaml/Rephrase-and-Respond.</description><author>Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu</author><pubDate>Tue, 07 Nov 2023 18:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04205v1</guid></item><item><title>Co-training and Co-distillation for Quality Improvement and Compression of Language Models</title><link>http://arxiv.org/abs/2311.02849v2</link><description>Knowledge Distillation (KD) compresses computationally expensive pre-trainedlanguage models (PLMs) by transferring their knowledge to smaller models,allowing their use in resource-constrained or real-time settings. However, mostsmaller models fail to surpass the performance of the original larger model,resulting in sacrificing performance to improve inference speed. To addressthis issue, we propose Co-Training and Co-Distillation (CTCD), a novelframework that improves performance and inference speed together by co-trainingtwo models while mutually distilling knowledge. The CTCD framework successfullyachieves this based on two significant findings: 1) Distilling knowledge fromthe smaller model to the larger model during co-training improves theperformance of the larger model. 2) The enhanced performance of the largermodel further boosts the performance of the smaller model. The CTCD frameworkshows promise as it can be combined with existing techniques like architecturedesign or data augmentation, replacing one-way KD methods, to achieve furtherperformance improvement. Extensive ablation studies demonstrate theeffectiveness of CTCD, and the small model distilled by CTCD outperforms theoriginal larger model by a significant margin of 1.66 on the GLUE benchmark.</description><author>Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min</author><pubDate>Tue, 07 Nov 2023 18:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02849v2</guid></item><item><title>Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study</title><link>http://arxiv.org/abs/2311.04199v1</link><description>Large Multimodal Models (LMMs) have demonstrated impressive performanceacross various vision and language tasks, yet their potential applications inrecommendation tasks with visual assistance remain unexplored. To bridge thisgap, we present a preliminary case study investigating the recommendationcapabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct aseries of qualitative test samples spanning multiple domains and employ thesesamples to assess the quality of GPT-4V's responses within recommendationscenarios. Evaluation results on these test samples prove that GPT-4V hasremarkable zero-shot recommendation abilities across diverse domains, thanks toits robust visual-text comprehension capabilities and extensive generalknowledge. However, we have also identified some limitations in using GPT-4Vfor recommendations, including a tendency to provide similar responses whengiven similar inputs. This report concludes with an in-depth discussion of thechallenges and research opportunities associated with utilizing GPT-4V inrecommendation scenarios. Our objective is to explore the potential ofextending LMMs from vision and language tasks to recommendation tasks. We hopeto inspire further research into next-generation multimodal generativerecommendation models, which can enhance user experiences by offering greaterdiversity and interactivity. All images and prompts used in this report will beaccessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.</description><author>Peilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining Hua, Jaeboum Kim</author><pubDate>Tue, 07 Nov 2023 18:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04199v1</guid></item><item><title>JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction</title><link>http://arxiv.org/abs/2311.04196v1</link><description>Product attribute value extraction is an important task in e-Commerce whichcan help several downstream applications such as product search andrecommendation. Most previous models handle this task using sequence labelingor question answering method which rely on the sequential position informationof values in the product text and are vulnerable to data discrepancy betweentraining and testing. This limits their generalization ability to real-worldscenario in which each product can have multiple descriptions across variousshopping platforms with different composition of text and style. They also havelimited zero-shot ability to new values. In this paper, we propose a multi-tasklearning model with value generation/classification and attribute predictioncalled JPAVE to predict values without the necessity of position information ofvalues in the text. Furthermore, the copy mechanism in value generator and thevalue attention module in value classifier help our model address the datadiscrepancy issue by only focusing on the relevant part of input text andignoring other information which causes the discrepancy issue such as sentencestructure in the text. Besides, two variants of our model are designed foropen-world and closed-world scenarios. In addition, copy mechanism introducedin the first variant based on value generation can improve its zero-shotability for identifying unseen values. Experimental results on a public datasetdemonstrate the superiority of our model compared with strong baselines and itsgeneralization ability of predicting new values.</description><author>Zhongfen Deng, Hao Peng, Tao Zhang, Shuaiqi Liu, Wenting Zhao, Yibo Wang, Philip S. Yu</author><pubDate>Tue, 07 Nov 2023 18:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04196v1</guid></item><item><title>Selective Visual Representations Improve Convergence and Generalization for Embodied AI</title><link>http://arxiv.org/abs/2311.04193v1</link><description>Embodied AI models often employ off the shelf vision backbones like CLIP toencode their visual observations. Although such general purpose representationsencode rich syntactic and semantic information about the scene, much of thisinformation is often irrelevant to the specific task at hand. This introducesnoise within the learning process and distracts the agent's focus fromtask-relevant visual cues. Inspired by selective attention in humans-theprocess through which people filter their perception based on theirexperiences, knowledge, and the task at hand-we introduce a parameter-efficientapproach to filter visual stimuli for embodied AI. Our approach induces atask-conditioned bottleneck using a small learnable codebook module. Thiscodebook is trained jointly to optimize task reward and acts as atask-conditioned selective filter over the visual observation. Our experimentsshowcase state-of-the-art performance for object goal navigation and objectdisplacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR,and ManipulaTHOR. The filtered representations produced by the codebook arealso able generalize better and converge faster when adapted to othersimulation environments such as Habitat. Our qualitative analyses show thatagents explore their environments more effectively and their representationsretain task-relevant information like target object recognition while ignoringsuperfluous information about other objects. Code and pretrained models areavailable at our project website: https://embodied-codebook.github.io.</description><author>Ainaz Eftekhar, Kuo-Hao Zeng, Jiafei Duan, Ali Farhadi, Ani Kembhavi, Ranjay Krishna</author><pubDate>Tue, 07 Nov 2023 18:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04193v1</guid></item><item><title>JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models</title><link>http://arxiv.org/abs/2311.04192v1</link><description>Image captioning studies heavily rely on automatic evaluation metrics such asBLEU and METEOR. However, such n-gram-based metrics have been shown tocorrelate poorly with human evaluation, leading to the proposal of alternativemetrics such as SPICE for English; however, no equivalent metrics have beenestablished for other languages. Therefore, in this study, we propose anautomatic evaluation metric called JaSPICE, which evaluates Japanese captionsbased on scene graphs. The proposed method generates a scene graph fromdependencies and the predicate-argument structure, and extends the graph usingsynonyms. We conducted experiments employing 10 image captioning models trainedon STAIR Captions and PFN-PIC and constructed the Shichimi dataset, whichcontains 103,170 human evaluations. The results showed that our metricoutperformed the baseline metrics for the correlation coefficient with thehuman evaluation.</description><author>Yuiga Wada, Kanta Kaneda, Komei Sugiura</author><pubDate>Tue, 07 Nov 2023 18:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04192v1</guid></item><item><title>Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter</title><link>http://arxiv.org/abs/2311.04190v1</link><description>The compact muon solenoid (CMS) experiment is a general-purpose detector forhigh-energy collision at the large hadron collider (LHC) at CERN. It employs anonline data quality monitoring (DQM) system to promptly spot and diagnoseparticle data acquisition problems to avoid data quality loss. In this study,we present semi-supervised spatio-temporal anomaly detection (AD) monitoringfor the physics particle reading channels of the hadronic calorimeter (HCAL) ofthe CMS using three-dimensional digi-occupancy map data of the DQM. We proposethe GraphSTAD system, which employs convolutional and graph neural networks tolearn local spatial characteristics induced by particles traversing thedetector, and global behavior owing to shared backend circuit connections andhousing boxes of the channels, respectively. Recurrent neural networks capturethe temporal evolution of the extracted spatial features. We have validated theaccuracy of the proposed AD system in capturing diverse channel fault typesusing the LHC Run-2 collision data sets. The GraphSTAD system has achievedproduction-level accuracy and is being integrated into the CMS core productionsystem--for real-time monitoring of the HCAL. We have also provided aquantitative performance comparison with alternative benchmark models todemonstrate the promising leverage of the presented system.</description><author>Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, David Yu, Pavel Parygin, Jay Dittmann, Georgia Karapostoli, Markus Seidel, Rosamaria Venditti, Luka Lambrecht, Emanuele Usai, Muhammad Ahmad, Javier Fernandez Menendez, Kaori Maeshima, the CMS-HCAL Collaboration</author><pubDate>Tue, 07 Nov 2023 18:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04190v1</guid></item><item><title>SpaDeLeF: A Dataset for Hierarchical Classification of Lexical Functions for Collocations in Spanish</title><link>http://arxiv.org/abs/2311.04189v1</link><description>In natural language processing (NLP), lexical function is a concept tounambiguously represent semantic and syntactic features of words and phrases intext first crafted in the Meaning-Text Theory. Hierarchical classification oflexical functions involves organizing these features into a tree-like hierarchyof categories or labels. This is a challenging task as it requires a goodunderstanding of the context and the relationships among words and phrases intext. It also needs large amounts of labeled data to train language modelseffectively. In this paper, we present a dataset of most frequent Spanishverb-noun collocations and sentences where they occur, each collocation isassigned to one of 37 lexical functions defined as classes for a hierarchicalclassification task. Each class represents a relation between the noun and theverb in a collocation involving their semantic and syntactic features. Wecombine the classes in a tree-based structure, and introduce classificationobjectives for each level of the structure. The dataset was created bydependency tree parsing and matching of the phrases in Spanish news. We providebaselines and data splits for each objective.</description><author>Yevhen Kostiuk, Grigori Sidorov, Olga Kolesnikova</author><pubDate>Tue, 07 Nov 2023 18:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04189v1</guid></item><item><title>Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences</title><link>http://arxiv.org/abs/2302.09043v2</link><description>Self-supervised feature learning enables perception systems to benefit fromthe vast raw data recorded by vehicle fleets worldwide. While video-levelself-supervised learning approaches have shown strong generalizability onclassification tasks, the potential to learn dense representations fromsequential data has been relatively unexplored. In this work, we propose TempO,a temporal ordering pretext task for pre-training region-level featurerepresentations for perception tasks. We embed each frame by an unordered setof proposal feature vectors, a representation that is natural for objectdetection or tracking systems, and formulate the sequential ordering bypredicting frame transition probabilities in a transformer-based multi-framearchitecture whose complexity scales less than quadratic with respect to thesequence length. Extensive evaluations on the BDD100K, nuImages, and MOT17datasets show that our TempO pre-training approach outperforms single-frameself-supervised learning methods as well as supervised transfer learninginitialization strategies, achieving an improvement of +0.7% in mAP for objectdetection and +2.0% in the HOTA score for multi-object tracking.</description><author>Christopher Lang, Alexander Braun, Lars Schillingmann, Karsten Haug, Abhinav Valada</author><pubDate>Tue, 07 Nov 2023 18:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09043v2</guid></item><item><title>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</title><link>http://arxiv.org/abs/2310.09478v3</link><description>Large language models have shown their remarkable capabilities as a generalinterface for various language-related applications. Motivated by this, wetarget to build a unified interface for completing many vision-language tasksincluding image description, visual question answering, and visual grounding,among others. The challenge is to use a single model for performing diversevision-language tasks effectively with simple multi-modal instructions. Towardsthis objective, we introduce MiniGPT-v2, a model that can be treated as aunified interface for better handling various vision-language tasks. We proposeusing unique identifiers for different tasks when training the model. Theseidentifiers enable our model to better distinguish each task instructioneffortlessly and also improve the model learning efficiency for each task.After the three-stage training, the experimental results show that MiniGPT-v2achieves strong performance on many visual question-answering and visualgrounding benchmarks compared to other vision-language generalist models. Ourmodel and codes are available at https://minigpt-v2.github.io/</description><author>Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny</author><pubDate>Tue, 07 Nov 2023 18:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09478v3</guid></item><item><title>Explicit Planning Helps Language Models in Logical Reasoning</title><link>http://arxiv.org/abs/2303.15714v4</link><description>Language models have been shown to perform remarkably well on a wide range ofnatural language processing tasks. In this paper, we propose LEAP, a novelsystem that uses language models to perform multi-step logical reasoning andincorporates explicit planning into the inference procedure. Explicit planningenables the system to make more informed reasoning decisions at each step bylooking ahead into their future effects. Moreover, we propose a trainingstrategy that safeguards the planning process from being led astray by spuriousfeatures. Our full system significantly outperforms other competing methods onmultiple standard datasets. When using small T5 models as its core selectionand deduction components, our system performs competitively compared to GPT-3despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).When using GPT-3.5, it significantly outperforms chain-of-thought prompting onthe challenging PrOntoQA dataset. We have conducted extensive empirical studiesto demonstrate that explicit planning plays a crucial role in the system'sperformance.</description><author>Hongyu Zhao, Kangrui Wang, Mo Yu, Hongyuan Mei</author><pubDate>Tue, 07 Nov 2023 18:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15714v4</guid></item><item><title>On Leakage in Machine Learning Pipelines</title><link>http://arxiv.org/abs/2311.04179v1</link><description>Machine learning (ML) provides powerful tools for predictive modeling. ML'spopularity stems from the promise of sample-level prediction with applicationsacross a variety of fields from physics and marketing to healthcare. However,if not properly implemented and evaluated, ML pipelines may contain leakagetypically resulting in overoptimistic performance estimates and failure togeneralize to new data. This can have severe negative financial and societalimplications. Our aim is to expand understanding associated with causes leadingto leakage when designing, implementing, and evaluating ML pipelines.Illustrated by concrete examples, we provide a comprehensive overview anddiscussion of various types of leakage that may arise in ML pipelines.</description><author>Leonard Sasse, Eliana Nicolaisen-Sobesky, Juergen Dukart, Simon B. Eickhoff, Michael Götz, Sami Hamdan, Vera Komeyer, Abhijit Kulkarni, Juha Lahnakoski, Bradley C. Love, Federico Raimondo, Kaustubh R. Patil</author><pubDate>Tue, 07 Nov 2023 18:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04179v1</guid></item><item><title>Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2311.04177v1</link><description>Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,(Bubeck et al., 2023)) on modern LLMs have shown that they are capable ofperforming amazing tasks typically necessitating human-level intelligence.However, unlike humans, frozen LLMs do not improve over time; they neitheracquire new knowledge nor learn from their successes or failures. Someapproaches to improving the intelligence of LLMs include fine-tuning modelsbased on problem-solving performance (Zelikman et al., 2022), and buildingbigger and more sophisticated models (Bubeck et al., 2023). However, thesemethods have the drawback of requiring substantial data and computationalresources to retrain existing models. In this paper, we explore the use ofRetrieval Augmented Generation, also known as RAG (Lewis et al., 2021) toimprove problem-solving performance. We propose ARM-RAG (Auxiliary RationaleMemory for Retrieval Augmented Generation), a system that learns from itssuccesses without incurring high training costs. We demonstrate that thestorage and subsequent retrieval of reasoning chains have a positive influenceon performance in grade-school math problems.</description><author>Eric Melz</author><pubDate>Tue, 07 Nov 2023 18:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04177v1</guid></item><item><title>How to Scale Your EMA</title><link>http://arxiv.org/abs/2307.13813v3</link><description>Preserving training dynamics across batch sizes is an important tool forpractical machine learning as it enables the trade-off between batch size andwall-clock time. This trade-off is typically enabled by a scaling rule, forexample, in stochastic gradient descent, one should scale the learning ratelinearly with the batch size. Another important machine learning tool is themodel EMA, a functional copy of a target model, whose parameters move towardsthose of its target model according to an Exponential Moving Average (EMA) at arate parameterized by a momentum hyperparameter. This model EMA can improve therobustness and generalization of supervised learning, stabilizepseudo-labeling, and provide a learning signal for Self-Supervised Learning(SSL). Prior works have not considered the optimization of the model EMA whenperforming scaling, leading to different training dynamics across batch sizesand lower model performance. In this work, we provide a scaling rule foroptimization in the presence of a model EMA and demonstrate the rule's validityacross a range of architectures, optimizers, and data modalities. We also showthe rule's validity where the model EMA contributes to the optimization of thetarget model, enabling us to train EMA-based pseudo-labeling and SSL methods atsmall and large batch sizes. For SSL, we enable training of BYOL up to batchsize 24,576 without sacrificing performance, a 6$\times$ wall-clock timereduction under idealized hardware settings.</description><author>Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russ Webb</author><pubDate>Tue, 07 Nov 2023 17:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13813v3</guid></item><item><title>Roots and Requirements for Collaborative AIs</title><link>http://arxiv.org/abs/2303.12040v5</link><description>The vision of AI collaborators is a staple of mythology and science fiction,where artificial agents with special talents assist human partners and teams.In this dream, sophisticated AIs understand nuances of collaboration and humancommunication. The AI as collaborator dream is different from computer toolsthat augment human intelligence (IA) or intermediate human collaboration. Suchtools have their roots in the 1960s and helped to drive an informationtechnology revolution. They can be useful but they are not intelligent and donot collaborate as effectively as skilled people. With the increase of hybridand remote work since the COVID pandemic, the benefits and requirements forbetter coordination, collaboration, and communication are becoming a hot topicin the workplace. Employers and workers face choices and trade-offs as theynegotiate the options for working from home versus working at the office. Manyfactors such as the high costs of homes near employers are impeding a massreturn to the office. Government advisory groups and leaders in AI haveadvocated for years that AIs should be transparent and effective collaborators.Nonetheless, robust AIs that collaborate like talented people remain out ofreach. Are AI teammates part of a solution? How artificially intelligent (AI)could and should they be? This position paper reviews the arc of technology andpublic calls for human-machine teaming. It draws on earlier research inpsychology and the social sciences about what human-like collaborationrequires. This paper sets a context for a second science-driven paper thatadvocates a radical shift in technology and methodology for creating resilient,intelligent, and human-compatible AIs (Stefik &amp; Price, 2023). The aspirationalgoal is that such AIs would learn, share what they learn, and collaborate toachieve high capabilities.</description><author>Mark Stefik</author><pubDate>Tue, 07 Nov 2023 17:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12040v5</guid></item><item><title>HADES: Fast Singularity Detection with Local Measure Comparison</title><link>http://arxiv.org/abs/2311.04171v1</link><description>We introduce Hades, an unsupervised algorithm to detect singularities indata. This algorithm employs a kernel goodness-of-fit test, and as aconsequence it is much faster and far more scaleable than the existingtopology-based alternatives. Using tools from differential geometry and optimaltransport theory, we prove that Hades correctly detects singularities with highprobability when the data sample lives on a transverse intersection ofequidimensional manifolds. In computational experiments, Hades recoverssingularities in synthetically generated data, branching points in road networkdata, intersection rings in molecular conformation space, and anomalies inimage data.</description><author>Uzu Lim, Harald Oberhauser, Vidit Nanda</author><pubDate>Tue, 07 Nov 2023 17:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04171v1</guid></item><item><title>How Many Neurons Does it Take to Approximate the Maximum?</title><link>http://arxiv.org/abs/2307.09212v2</link><description>We study the size of a neural network needed to approximate the maximumfunction over $d$ inputs, in the most basic setting of approximating withrespect to the $L_2$ norm, for continuous distributions, for a network thatuses ReLU activations. We provide new lower and upper bounds on the widthrequired for approximation across various depths. Our results establish newdepth separations between depth 2 and 3, and depth 3 and 5 networks, as well asproviding a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$construction which approximates the maximum function. Our depth separationresults are facilitated by a new lower bound for depth 2 networks approximatingthe maximum function over the uniform distribution, assuming an exponentialupper bound on the size of the weights. Furthermore, we are able to use thisdepth 2 lower bound to provide tight bounds on the number of neurons needed toapproximate the maximum by a depth 3 network. Our lower bounds are ofpotentially broad interest as they apply to the widely studied and used\emph{max} function, in contrast to many previous results that base theirbounds on specially constructed or pathological functions and distributions.</description><author>Itay Safran, Daniel Reichman, Paul Valiant</author><pubDate>Tue, 07 Nov 2023 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09212v2</guid></item><item><title>Perturbed examples reveal invariances shared by language models</title><link>http://arxiv.org/abs/2311.04166v1</link><description>An explosion of work in language is leading to ever-increasing numbers ofavailable natural language processing models, with little understanding of hownew models compare to better-understood models. One major reason for thisdifficulty is saturating benchmark datasets, which may not reflect welldifferences in model performance in the wild. In this work, we propose a novelframework for comparing two natural language processing models by revealingtheir shared invariance to interpretable input perturbations that are designedto target a specific linguistic capability (e.g., Synonym-Invariance,Typo-Invariance). Via experiments on models from within the same and acrossdifferent architecture families, this framework offers a number of insightsabout how changes in models (e.g., distillation, increase in size, amount ofpre-training) affect multiple well-defined linguistic capabilities.Furthermore, we also demonstrate how our framework can enable evaluation of theinvariances shared between models that are available as commercial black-boxAPIs (e.g., InstructGPT family) and models that are relatively betterunderstood (e.g., GPT-2). Across several experiments, we observe that largelanguage models share many of the invariances encoded by models of varioussizes, whereas the invariances encoded by large language models are only sharedby other large models. Possessing a wide variety of invariances may be a keyreason for the recent successes of large language models, and our framework canshed light on the types of invariances that are retained by or emerge in newmodels.</description><author>Ruchit Rawal, Mariya Toneva</author><pubDate>Tue, 07 Nov 2023 17:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04166v1</guid></item><item><title>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain</title><link>http://arxiv.org/abs/2311.00660v2</link><description>Rain generation algorithms have the potential to improve the generalizationof deraining methods and scene understanding in rainy conditions. However, inpractice, they produce artifacts and distortions and struggle to control theamount of rain generated due to a lack of proper constraints. In this paper, wepropose an unpaired image-to-image translation framework for generatingrealistic rainy images. We first introduce a Triangular Probability Similarity(TPS) constraint to guide the generated images toward clear and rainy images inthe discriminator manifold, thereby minimizing artifacts and distortions duringrain generation. Unlike conventional contrastive learning approaches, whichindiscriminately push negative samples away from the anchors, we propose aSemantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushingforce of negative samples based on the semantic similarity between the clearand the rainy images and the feature similarity between the anchor and thenegative samples. Experiments demonstrate realistic rain generation withminimal artifacts and distortions, which benefits image deraining and objectdetection in rain. Furthermore, the method can be used to generate realisticsnowy and night images, underscoring its potential for broader applicability.Code is available at https://github.com/ShenZheng2000/TPSeNCE.</description><author>Shen Zheng, Changjie Lu, Srinivasa G. Narasimhan</author><pubDate>Tue, 07 Nov 2023 17:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00660v2</guid></item><item><title>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</title><link>http://arxiv.org/abs/2310.19102v2</link><description>The growing demand for Large Language Models (LLMs) in applications such ascontent generation, intelligent chatbots, and sentiment analysis posesconsiderable challenges for LLM service providers. To efficiently use GPUresources and boost throughput, batching multiple requests has emerged as apopular paradigm; to further speed up batching, LLM quantization techniquesreduce memory consumption and increase computing capacity. However, prevalentquantization schemes (e.g., 8-bit weight-activation quantization) cannot fullyleverage the capabilities of modern GPUs, such as 4-bit integer operators,resulting in sub-optimal performance. To maximize LLMs' serving throughput, we introduce Atom, a low-bitquantization method that achieves high throughput improvements with negligibleaccuracy loss. Atom significantly boosts serving throughput by using low-bitoperators and considerably reduces memory consumption via low-bit quantization.It attains high accuracy by applying a novel mixed-precision and fine-grainedquantization process. We evaluate Atom on 4-bit weight-activation quantizationsetups in the serving context. Atom improves end-to-end throughput by up to$7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8quantization, while maintaining the same latency target.</description><author>Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris Kasikci</author><pubDate>Tue, 07 Nov 2023 17:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19102v2</guid></item><item><title>Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization</title><link>http://arxiv.org/abs/2311.04163v1</link><description>We identify a new phenomenon in neural network optimization which arises fromthe interaction of depth and a particular heavy-tailed structure in naturaldata. Our result offers intuitive explanations for several previously reportedobservations about network training dynamics. In particular, it implies aconceptually new cause for progressive sharpening and the edge of stability; wealso highlight connections to other concepts in optimization and generalizationincluding grokking, simplicity bias, and Sharpness-Aware Minimization. Experimentally, we demonstrate the significant influence of paired groups ofoutliers in the training data with strong opposing signals: consistent, largemagnitude features which dominate the network output throughout training andprovide gradients which point in opposite directions. Due to these outliers,early optimization enters a narrow valley which carefully balances the opposinggroups; subsequent sharpening causes their loss to rise rapidly, oscillatingbetween high on one group and then the other, until the overall loss spikes. Wedescribe how to identify these groups, explore what sets them apart, andcarefully study their effect on the network's optimization and behavior. Wecomplement these experiments with a mechanistic explanation on a toy example ofopposing signals and a theoretical analysis of a two-layer linear network on asimple model. Our finding enables new qualitative predictions of trainingbehavior which we confirm experimentally. It also provides a new lens throughwhich to study and improve modern training practices for stochasticoptimization, which we highlight via a case study of Adam versus SGD.</description><author>Elan Rosenfeld, Andrej Risteski</author><pubDate>Tue, 07 Nov 2023 17:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04163v1</guid></item><item><title>Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems</title><link>http://arxiv.org/abs/2311.04161v1</link><description>We consider stochastic optimization problems with heavy-tailed noise withstructured density. For such problems, we show that it is possible to getfaster rates of convergence than $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$, whenthe stochastic gradients have finite moments of order $\alpha \in (1, 2]$. Inparticular, our analysis allows the noise norm to have an unboundedexpectation. To achieve these results, we stabilize stochastic gradients, usingsmoothed medians of means. We prove that the resulting estimates havenegligible bias and controllable variance. This allows us to carefullyincorporate them into clipped-SGD and clipped-SSTM and derive newhigh-probability complexity bounds in the considered setup.</description><author>Nikita Puchkin, Eduard Gorbunov, Nikolay Kutuzov, Alexander Gasnikov</author><pubDate>Tue, 07 Nov 2023 17:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04161v1</guid></item><item><title>Computing Approximate $\ell_p$ Sensitivities</title><link>http://arxiv.org/abs/2311.04158v1</link><description>Recent works in dimensionality reduction for regression tasks have introducedthe notion of sensitivity, an estimate of the importance of a specificdatapoint in a dataset, offering provable guarantees on the quality of theapproximation after removing low-sensitivity datapoints via subsampling.However, fast algorithms for approximating $\ell_p$ sensitivities, which weshow is equivalent to approximate $\ell_p$ regression, are known for only the$\ell_2$ setting, in which they are termed leverage scores. In this work, we provide efficient algorithms for approximating $\ell_p$sensitivities and related summary statistics of a given matrix. In particular,for a given $n \times d$ matrix, we compute $\alpha$-approximation to its$\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations.For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$sensitivities), we provide an algorithm based on importance sampling of$\ell_p$ Lewis weights, which computes a constant factor approximation to thetotal sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivitycomputations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up toa $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize allthese results to $\ell_p$ norms for $p &gt; 1$. Lastly, we experimentally showthat for a wide class of matrices in real-world datasets, the total sensitivitycan be quickly approximated and is significantly smaller than the theoreticalprediction, demonstrating that real-world datasets have low intrinsic effectivedimensionality.</description><author>Swati Padmanabhan, David P. Woodruff, Qiuyi, Zhang</author><pubDate>Tue, 07 Nov 2023 17:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04158v1</guid></item><item><title>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</title><link>http://arxiv.org/abs/2311.04157v1</link><description>We present a novel usage of Transformers to make image classificationinterpretable. Unlike mainstream classifiers that wait until the lastfully-connected layer to incorporate class information to make predictions, weinvestigate a proactive approach, asking each class to search for itself in animage. We realize this idea via a Transformer encoder-decoder inspired byDEtection TRansformer (DETR). We learn ``class-specific'' queries (one for eachclass) as input to the decoder, enabling each class to localize its patterns inan image via cross-attention. We name our approach INterpretable TRansformer(INTR), which is fairly easy to implement and exhibits several compellingproperties. We show that INTR intrinsically encourages each class to attenddistinctively; the cross-attention weights thus provide a faithfulinterpretation of the prediction. Interestingly, via ``multi-head''cross-attention, INTR could identify different ``attributes'' of a class,making it particularly suitable for fine-grained classification and analysis,which we demonstrate on eight datasets. Our code and pre-trained model arepublicly accessible at https://github.com/Imageomics/INTR.</description><author>Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel Stevens, Kaiya Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</author><pubDate>Tue, 07 Nov 2023 17:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04157v1</guid></item><item><title>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</title><link>http://arxiv.org/abs/2311.04155v1</link><description>Large language models (LLMs) have shown impressive success in variousapplications. However, these models are often not well aligned with humanintents, which calls for additional treatments on them, that is, the alignmentproblem. To make LLMs better follow user instructions, existing alignmentmethods mostly focus on further training them. However, the extra training ofLLMs are usually expensive in terms of GPU compute; worse still, LLMs ofinterest are oftentimes not accessible for user-demanded training, such asGPTs. In this work, we take a different perspective -- Black-Box PromptOptimization (BPO) -- to perform alignments. The idea is to optimize userprompts to suit LLMs' input understanding, so as to best realize users' intentswithout updating LLMs' parameters. BPO is model-agnostic and the empiricalresults demonstrate that the BPO-aligned ChatGPT yields a 22\% increase in thewin rate against its original version, and 10\% for GPT-4. Importantly, the\model-aligned LLMs can outperform the same models aligned by PPO and DPO, andit also brings additional performance gains when combining \model with PPO orDPO. Code and datasets are released at https://github.com/thu-coai/BPO.</description><author>Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang</author><pubDate>Tue, 07 Nov 2023 17:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04155v1</guid></item><item><title>High-fidelity 3D Reconstruction of Plants using Neural Radiance Field</title><link>http://arxiv.org/abs/2311.04154v1</link><description>Accurate reconstruction of plant phenotypes plays a key role in optimisingsustainable farming practices in the field of Precision Agriculture (PA).Currently, optical sensor-based approaches dominate the field, but the need forhigh-fidelity 3D reconstruction of crops and plants in unstructuredagricultural environments remains challenging. Recently, a promisingdevelopment has emerged in the form of Neural Radiance Field (NeRF), a novelmethod that utilises neural density fields. This technique has shown impressiveperformance in various novel vision synthesis tasks, but has remainedrelatively unexplored in the agricultural context. In our study, we focus ontwo fundamental tasks within plant phenotyping: (1) the synthesis of 2Dnovel-view images and (2) the 3D reconstruction of crop and plant models. Weexplore the world of neural radiance fields, in particular two SOTA methods:Instant-NGP, which excels in generating high-quality images with impressivetraining and inference speed, and Instant-NSR, which improves the reconstructedgeometry by incorporating the Signed Distance Function (SDF) during training.In particular, we present a novel plant phenotype dataset comprising real plantimages from production environments. This dataset is a first-of-its-kindinitiative aimed at comprehensively exploring the advantages and limitations ofNeRF in agricultural contexts. Our experimental results show that NeRFdemonstrates commendable performance in the synthesis of novel-view images andis able to achieve reconstruction results that are competitive with RealityCapture, a leading commercial software for 3D Multi-View Stereo (MVS)-basedreconstruction. However, our study also highlights certain drawbacks of NeRF,including relatively slow training speeds, performance limitations in cases ofinsufficient sampling, and challenges in obtaining geometry quality in complexsetups.</description><author>Kewei Hu, Ying Wei, Yaoqiang Pan, Hanwen Kang, Chao Chen</author><pubDate>Tue, 07 Nov 2023 17:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04154v1</guid></item><item><title>Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference</title><link>http://arxiv.org/abs/2311.04153v1</link><description>Using a fully Bayesian approach, Gaussian Process regression is extended toinclude marginalisation over the kernel choice and kernel hyperparameters. Inaddition, Bayesian model comparison via the evidence enables direct kernelcomparison. The calculation of the joint posterior was implemented with atransdimensional sampler which simultaneously samples over the discrete kernelchoice and their hyperparameters by embedding these in a higher-dimensionalspace, from which samples are taken using nested sampling. This method wasexplored on synthetic data from exoplanet transit light curve simulations. Thetrue kernel was recovered in the low noise region while no kernel was preferredfor larger noise. Furthermore, inference of the physical exoplanethyperparameters was conducted. In the high noise region, either the bias in theposteriors was removed, the posteriors were broadened or the accuracy of theinference was increased. In addition, the uncertainty in mean functionpredictive distribution increased due to the uncertainty in the kernel choice.Subsequently, the method was extended to marginalisation over mean functionsand noise models and applied to the inference of the present-day Hubbleparameter, $H_0$, from real measurements of the Hubble parameter as a functionof redshift, derived from the cosmologically model-independent cosmicchronometer and {\Lambda}CDM-dependent baryon acoustic oscillationobservations. The inferred $H_0$ values from the cosmic chronometers, baryonacoustic oscillations and combined datasets are $H_0$ = 66$\pm$6 km/s/Mpc,$H_0$ = 67$\pm$10 km/s/Mpc and $H_0$ = 69$\pm$6 km/s/Mpc, respectively. Thekernel posterior of the cosmic chronometers dataset prefers a non-stationarylinear kernel. Finally, the datasets are shown to be not in tension withln(R)=12.17$\pm$0.02.</description><author>Namu Kroupa, David Yallup, Will Handley, Michael Hobson</author><pubDate>Tue, 07 Nov 2023 17:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04153v1</guid></item><item><title>Leveraging Deep Learning for Abstractive Code Summarization of Unofficial Documentation</title><link>http://arxiv.org/abs/2310.15015v2</link><description>Usually, programming languages have official documentation to guidedevelopers with APIs, methods, and classes. However, researchers identifiedinsufficient or inadequate documentation examples and flaws with the API'scomplex structure as barriers to learning an API. As a result, developers mayconsult other sources (StackOverflow, GitHub, etc.) to learn more about an API.Recent research studies have shown that unofficial documentation is a valuablesource of information for generating code summaries. We, therefore, have beenmotivated to leverage such a type of documentation along with deep learningtechniques towards generating high-quality summaries for APIs discussed ininformal documentation. This paper proposes an automatic approach using theBART algorithm, a state-of-the-art transformer model, to generate summaries forAPIs discussed in StackOverflow. We built an oracle of human-generatedsummaries to evaluate our approach against it using ROUGE and BLEU metricswhich are the most widely used evaluation metrics in text summarization.Furthermore, we evaluated our summaries empirically against a previous work interms of quality. Our findings demonstrate that using deep learning algorithmscan improve summaries' quality and outperform the previous work by an averageof %57 for Precision, %66 for Recall, and %61 for F-measure, and it runs 4.4times faster.</description><author>AmirHossein Naghshzan, Latifa Guerrouj, Olga Baysal</author><pubDate>Tue, 07 Nov 2023 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15015v2</guid></item><item><title>Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions</title><link>http://arxiv.org/abs/2306.00904v3</link><description>Models that rely solely on pairwise relationships often fail to capture thecomplete statistical structure of the complex multivariate data found indiverse domains, such as socio-economic, ecological, or biomedical systems.Non-trivial dependencies between groups of more than two variables can play asignificant role in the analysis and modelling of such systems, yet extractingsuch high-order interactions from data remains challenging. Here, we introducea hierarchy of $d$-order ($d \geq 2$) interaction measures, increasinglyinclusive of possible factorisations of the joint probability distribution, anddefine non-parametric, kernel-based tests to establish systematically thestatistical significance of $d$-order interactions. We also establishmathematical links with lattice theory, which elucidate the derivation of theinteraction measures and their composite permutation tests; clarify theconnection of simplicial complexes with kernel matrix centring; and provide ameans to enhance computational efficiency. We illustrate our resultsnumerically with validations on synthetic data, and through an application toneuroimaging data.</description><author>Zhaolu Liu, Robert L. Peach, Pedro A. M. Mediano, Mauricio Barahona</author><pubDate>Tue, 07 Nov 2023 17:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00904v3</guid></item><item><title>HyperS2V: A Framework for Structural Representation of Nodes in Hyper Networks</title><link>http://arxiv.org/abs/2311.04149v1</link><description>In contrast to regular (simple) networks, hyper networks possess the abilityto depict more complex relationships among nodes and store extensiveinformation. Such networks are commonly found in real-world applications, suchas in social interactions. Learning embedded representations for nodes involvesa process that translates network structures into more simplified spaces,thereby enabling the application of machine learning approaches designed forvector data to be extended to network data. Nevertheless, there remains a needto delve into methods for learning embedded representations that prioritizestructural aspects. This research introduces HyperS2V, a node embeddingapproach that centers on the structural similarity within hyper networks.Initially, we establish the concept of hyper-degrees to capture the structuralproperties of nodes within hyper networks. Subsequently, a novel function isformulated to measure the structural similarity between different hyper-degreevalues. Lastly, we generate structural embeddings utilizing a multi-scalerandom walk framework. Moreover, a series of experiments, both intrinsic andextrinsic, are performed on both toy and real networks. The results underscorethe superior performance of HyperS2V in terms of both interpretability andapplicability to downstream tasks.</description><author>Shu Liu, Cameron Lai, Fujio Toriumi</author><pubDate>Tue, 07 Nov 2023 17:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04149v1</guid></item><item><title>Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach</title><link>http://arxiv.org/abs/2311.04148v1</link><description>Contactless fingerprint recognition offers a higher level of user comfort andaddresses hygiene concerns more effectively. However, it is also morevulnerable to presentation attacks such as photo paper, paper-printout, andvarious display attacks, which makes it more challenging to implement inbiometric systems compared to contact-based modalities. Limited research hasbeen conducted on presentation attacks in contactless fingerprint systems, andthese studies have encountered challenges in terms of generalization andscalability since both bonafide samples and presentation attacks are utilizedduring training model. Although this approach appears promising, it lacks theability to handle unseen attacks, which is a crucial factor for developing PADmethods that can generalize effectively. We introduced an innovativeanti-spoofing approach that combines an unsupervised autoencoder with aconvolutional block attention module to address the limitations of existingmethods. Our model is exclusively trained on bonafide images without exposureto any spoofed samples during the training phase. It is then evaluated againstvarious types of presentation attack images in the testing phase. The scheme weproposed has achieved an average BPCER of 0.96\% with an APCER of 1.6\% forpresentation attacks involving various types of spoofed samples.</description><author>Banafsheh Adami, Nima Karimian</author><pubDate>Tue, 07 Nov 2023 17:19:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04148v1</guid></item><item><title>Multi-resolution Time-Series Transformer for Long-term Forecasting</title><link>http://arxiv.org/abs/2311.04147v1</link><description>The performance of transformers for time-series forecasting has improvedsignificantly. Recent architectures learn complex temporal patterns bysegmenting a time-series into patches and using the patches as tokens. Thepatch size controls the ability of transformers to learn the temporal patternsat different frequencies: shorter patches are effective for learning localized,high-frequency patterns, whereas mining long-term seasonalities and trendsrequires longer patches. Inspired by this observation, we propose a novelframework, Multi-resolution Time-Series Transformer (MTST), which consists of amulti-branch architecture for simultaneous modeling of diverse temporalpatterns at different resolutions. In contrast to many existing time-seriestransformers, we employ relative positional encoding, which is better suitedfor extracting periodic components at different scales. Extensive experimentson several real-world datasets demonstrate the effectiveness of MTST incomparison to state-of-the-art forecasting techniques.</description><author>Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates</author><pubDate>Tue, 07 Nov 2023 17:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04147v1</guid></item><item><title>Differentiable Cutting-plane Layers for Mixed-integer Linear Optimization</title><link>http://arxiv.org/abs/2311.03350v2</link><description>We consider the problem of solving a family of parametric mixed-integerlinear optimization problems where some entries in the input data change. Weintroduce the concept of cutting-plane layer (CPL), i.e., a differentiablecutting-plane generator mapping the problem data and previous iterates tocutting planes. We propose a CPL implementation to generate split cuts, and bycombining several CPLs, we devise a differentiable cutting-plane algorithm thatexploits the repeated nature of parametric instances. In an offline phase, wetrain our algorithm by updating the internal parameters controlling the CPLs,thus altering cut generation. Once trained, our algorithm computes, withpredictable execution times and a fixed number of cuts, solutions with lowintegrality gaps. Preliminary computational tests show that our algorithmgeneralizes on unseen instances and captures underlying parametric structures.</description><author>Gabriele Dragotto, Stefan Clarke, Jaime Fernández Fisac, Bartolomeo Stellato</author><pubDate>Tue, 07 Nov 2023 17:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03350v2</guid></item><item><title>I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models</title><link>http://arxiv.org/abs/2311.04145v1</link><description>Video synthesis has recently made remarkable strides benefiting from therapid development of diffusion models. However, it still encounters challengesin terms of semantic accuracy, clarity and spatio-temporal continuity. Theyprimarily arise from the scarcity of well-aligned text-video data and thecomplex inherent structure of videos, making it difficult for the model tosimultaneously ensure semantic and qualitative excellence. In this report, wepropose a cascaded I2VGen-XL approach that enhances model performance bydecoupling these two factors and ensures the alignment of the input data byutilizing static images as a form of crucial guidance. I2VGen-XL consists oftwo stages: i) the base stage guarantees coherent semantics and preservescontent from input images by using two hierarchical encoders, and ii) therefinement stage enhances the video's details by incorporating an additionalbrief text and improves the resolution to 1280$\times$720. To improve thediversity, we collect around 35 million single-shot text-video pairs and 6billion text-image pairs to optimize the model. By this means, I2VGen-XL cansimultaneously enhance the semantic accuracy, continuity of details and clarityof generated videos. Through extensive experiments, we have investigated theunderlying principles of I2VGen-XL and compared it with current top methods,which can demonstrate its effectiveness on diverse data. The source code andmodels will be publicly available at \url{https://i2vgen-xl.github.io}.</description><author>Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, Jingren Zhou</author><pubDate>Tue, 07 Nov 2023 17:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04145v1</guid></item><item><title>What is Lost in Knowledge Distillation?</title><link>http://arxiv.org/abs/2311.04142v1</link><description>Deep neural networks (DNNs) have improved NLP tasks significantly, buttraining and maintaining such networks could be costly. Model compressiontechniques, such as, knowledge distillation (KD), have been proposed to addressthe issue; however, the compression process could be lossy. Motivated by this,our work investigates how a distilled student model differs from its teacher,if the distillation process causes any information losses, and if the lossfollows a specific pattern. Our experiments aim to shed light on the type oftasks might be less or more sensitive to KD by reporting data points on thecontribution of different factors, such as the number of layers or attentionheads. Results such as ours could be utilized when determining effective andefficient configurations to achieve optimal information transfers betweenlarger (teacher) and smaller (student) models.</description><author>Manas Mohanty, Tanya Roosta, Peyman Passban</author><pubDate>Tue, 07 Nov 2023 17:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04142v1</guid></item><item><title>Modelling Sentiment Analysis: LLMs and data augmentation techniques</title><link>http://arxiv.org/abs/2311.04139v1</link><description>This paper provides different approaches for a binary sentimentclassification on a small training dataset. LLMs that provided state-of-the-artresults in sentiment analysis and similar domains are being used, such as BERT,RoBERTa and XLNet.</description><author>Guillem Senabre Prades</author><pubDate>Tue, 07 Nov 2023 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04139v1</guid></item><item><title>Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis</title><link>http://arxiv.org/abs/2310.02641v2</link><description>Images degraded by geometric distortions pose a significant challenge toimaging and computer vision tasks such as object recognition. Deeplearning-based imaging models usually fail to give accurate performance forgeometrically distorted images. In this paper, we propose thedeformation-invariant neural network (DINN), a framework to address the problemof imaging tasks for geometrically distorted images. The DINN outputsconsistent latent features for images that are geometrically distorted butrepresent the same underlying object or scene. The idea of DINN is toincorporate a simple component, called the quasiconformal transformer network(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deepneural network that outputs a quasiconformal map, which can be used totransform a geometrically distorted image into an improved version that iscloser to the distribution of natural or good images. It first outputs aBeltrami coefficient, which measures the quasiconformality of the outputdeformation map. By controlling the Beltrami coefficient, the local geometricdistortion under the quasiconformal mapping can be controlled. The QCTN islightweight and simple, which can be readily integrated into other existingdeep neural networks to enhance their performance. Leveraging our framework, wehave developed an image classification network that achieves accurateclassification of distorted images. Our proposed framework has been applied torestore geometrically distorted images by atmospheric turbulence and waterturbulence. DINN outperforms existing GAN-based restoration methods under thesescenarios, demonstrating the effectiveness of the proposed framework.Additionally, we apply our proposed framework to the 1-1 verification of humanface images under atmospheric turbulence and achieve satisfactory performance,further demonstrating the efficacy of our approach.</description><author>Han Zhang, Qiguang Chen, Lok Ming Lui</author><pubDate>Tue, 07 Nov 2023 17:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02641v2</guid></item><item><title>PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces</title><link>http://arxiv.org/abs/2305.11915v2</link><description>In the paper, we describe in operator form classes of PDEs that admit PINN'serror estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert typelemma that is a tool for PINN's residuals bounding.</description><author>Jiexing Gao, Yurii Zakharian</author><pubDate>Tue, 07 Nov 2023 17:11:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11915v2</guid></item><item><title>Spatial-Language Attention Policies for Efficient Robot Learning</title><link>http://arxiv.org/abs/2304.11235v3</link><description>Despite great strides in language-guided manipulation, existing work has beenconstrained to table-top settings. Table-tops allow for perfect and consistentcamera angles, properties are that do not hold in mobile manipulation. Taskplans that involve moving around the environment must be robust to egocentricviews and changes in the plane and angle of grasp. A further challenge isensuring this is all true while still being able to learn skills efficientlyfrom limited data. We propose Spatial-Language Attention Policies (SLAP) as asolution. SLAP uses three-dimensional tokens as the input representation totrain a single multi-task, language-conditioned action prediction policy. Ourmethod shows an 80% success rate in the real world across eight tasks with asingle model, and a 47.5% success rate when unseen clutter and unseen objectconfigurations are introduced, even with only a handful of examples per task.This represents an improvement of 30% over prior work (20% given unseendistractors and configurations). We see a 4x improvement over baseline inmobile manipulation setting. In addition, we show how SLAPs robustness allowsus to execute Task Plans from open-vocabulary instructions using a largelanguage model for multi-step mobile manipulation. For videos, see the website:https://robotslap.github.io</description><author>Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton</author><pubDate>Tue, 07 Nov 2023 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11235v3</guid></item><item><title>K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways</title><link>http://arxiv.org/abs/2110.11048v3</link><description>Lane detection is a critical function for autonomous driving. With the recentdevelopment of deep learning and the publication of camera lane datasets andbenchmarks, camera lane detection networks (CLDNs) have been remarkablydeveloped. Unfortunately, CLDNs rely on camera images which are often distortednear the vanishing line and prone to poor lighting condition. This is incontrast with Lidar lane detection networks (LLDNs), which can directly extractthe lane lines on the bird's eye view (BEV) for motion planning and operaterobustly under various lighting conditions. However, LLDNs have not beenactively studied, mostly due to the absence of large public lidar lanedatasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's firstand the largest public urban road and highway lane dataset for Lidar. K-Lanehas more than 15K frames and contains annotations of up to six lanes undervarious road and traffic conditions, e.g., occluded roads of multiple occlusionlevels, roads at day and night times, merging (converging and diverging) andcurved lanes. We also provide baseline networks we term Lidar lane detectionnetworks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits thespatial characteristics of lane lines on the point cloud, which are sparse,thin, and stretched along the entire ground plane of the point cloud. Fromexperimental results, LLDN-GFC achieves the state-of-the-art performance withan F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strongperformance under various lighting conditions, which is unlike CLDNs, and alsorobust even in the case of severe occlusions, unlike LLDNs using theconventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, andcomplete development kits including evaluation, visualization and annotationtools are available at https://github.com/kaist-avelab/k-lane.</description><author>Donghee Paek, Seung-Hyun Kong, Kevin Tirta Wijaya</author><pubDate>Tue, 07 Nov 2023 17:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.11048v3</guid></item><item><title>Topologically Regularized Data Embeddings</title><link>http://arxiv.org/abs/2301.03338v2</link><description>Unsupervised representation learning methods are widely used for gaininginsight into high-dimensional, unstructured, or structured data. In some cases,users may have prior topological knowledge about the data, such as a knowncluster structure or the fact that the data is known to lie along a tree- orgraph-structured topology. However, generic methods to ensure such structure issalient in the low-dimensional representations are lacking. This negativelyimpacts the interpretability of low-dimensional embeddings, and plausiblydownstream learning tasks. To address this issue, we introduce topologicalregularization: a generic approach based on algebraic topology to incorporatetopological prior knowledge into low-dimensional embeddings. We introduce aclass of topological loss functions, and show that jointly optimizing anembedding loss with such a topological loss function as a regularizer yieldsembeddings that reflect not only local proximities but also the desiredtopological structure. We include a self-contained overview of the requiredfoundational concepts in algebraic topology, and provide intuitive guidance onhow to design topological loss functions for a variety of shapes, such asclusters, cycles, and bifurcations. We empirically evaluate the proposedapproach on computational efficiency, robustness, and versatility incombination with linear and non-linear dimensionality reduction and graphembedding methods.</description><author>Edith Heiter, Robin Vandaele, Tijl De Bie, Yvan Saeys, Jefrey Lijffijt</author><pubDate>Tue, 07 Nov 2023 17:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03338v2</guid></item><item><title>K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions</title><link>http://arxiv.org/abs/2206.08171v4</link><description>Unlike RGB cameras that use visible light bands (384$\sim$769 THz) and Lidarsthat use infrared bands (361$\sim$331 THz), Radars use relatively longerwavelength radio bands (77$\sim$81 GHz), resulting in robust measurements inadverse weathers. Unfortunately, existing Radar datasets only contain arelatively small number of samples compared to the existing camera and Lidardatasets. This may hinder the development of sophisticated data-driven deeplearning techniques for Radar-based perception. Moreover, most of the existingRadar datasets only provide 3D Radar tensor (3DRT) data that contain powermeasurements along the Doppler, range, and azimuth dimensions. As there is noelevation information, it is challenging to estimate the 3D bounding box of anobject from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novellarge-scale object detection dataset and benchmark that contains 35K frames of4D Radar tensor (4DRT) data with power measurements along the Doppler, range,azimuth, and elevation dimensions, together with carefully annotated 3Dbounding box labels of objects on the roads. K-Radar includes challengingdriving conditions such as adverse weathers (fog, rain, and snow) on variousroad structures (urban, suburban roads, alleyways, and highways). In additionto the 4DRT, we provide auxiliary measurements from carefully calibratedhigh-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide4DRT-based object detection baseline neural networks (baseline NNs) and showthat the height information is crucial for 3D object detection. And bycomparing the baseline NN with a similarly-structured Lidar-based neuralnetwork, we demonstrate that 4D Radar is a more robust sensor for adverseweather conditions. All codes are available athttps://github.com/kaist-avelab/k-radar.</description><author>Dong-Hee Paek, Seung-Hyun Kong, Kevin Tirta Wijaya</author><pubDate>Tue, 07 Nov 2023 17:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08171v4</guid></item><item><title>Automated Repair of Declarative Software Specifications in the Era of Large Language Models</title><link>http://arxiv.org/abs/2310.12425v2</link><description>The growing adoption of declarative software specification languages, coupledwith their inherent difficulty in debugging, has underscored the need foreffective and automated repair techniques applicable to such languages.Researchers have recently explored various methods to automatically repairdeclarative software specifications, such as template-based repair,feedback-driven iterative repair, and bounded exhaustive approaches. The latestdevelopments in large language models provide new opportunities for theautomatic repair of declarative specifications. In this study, we assess theeffectiveness of utilizing OpenAI's ChatGPT to repair software specificationswritten in the Alloy declarative language. Unlike imperative languages,specifications in Alloy are not executed but rather translated into logicalformulas and evaluated using backend constraint solvers to identifyspecification instances and counterexamples to assertions. Our evaluationfocuses on ChatGPT's ability to improve the correctness and completeness ofAlloy declarative specifications through automatic repairs. We analyze theresults produced by ChatGPT and compare them with those of leading automaticAlloy repair methods. Our study revealed that while ChatGPT falls short incomparison to existing techniques, it was able to successfully repair bugs thatno other technique could address. Our analysis also identified errors inChatGPT's generated repairs, including improper operator usage, type errors,higher-order logic misuse, and relational arity mismatches. Additionally, weobserved instances of hallucinations in ChatGPT-generated repairs andinconsistency in its results. Our study provides valuable insights for softwarepractitioners, researchers, and tool builders considering ChatGPT fordeclarative specification repairs.</description><author>Md Rashedul Hasan, Jiawei Li, Iftekhar Ahmed, Hamid Bagheri</author><pubDate>Tue, 07 Nov 2023 17:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12425v2</guid></item><item><title>Locating Cross-Task Sequence Continuation Circuits in Transformers</title><link>http://arxiv.org/abs/2311.04131v1</link><description>While transformer models exhibit strong capabilities on linguistic tasks,their complex architectures make them difficult to interpret. Recent work hasaimed to reverse engineer transformer models into human-readablerepresentations called circuits that implement algorithmic functions. We extendthis research by analyzing and comparing circuits for similar sequencecontinuation tasks, which include increasing sequences of digits, number words,and months. Through the application of circuit analysis techniques, we identifykey sub-circuits responsible for detecting sequence members and for predictingthe next member in a sequence. Our analysis reveals that semantically relatedsequences rely on shared circuit subgraphs with analogous roles. Overall,documenting shared computational structures enables better prediction of modelbehaviors, identification of errors, and safer editing procedures. Thismechanistic understanding of transformers is a critical step towards buildingmore robust, aligned, and interpretable language models.</description><author>Michael Lan, Fazl Barez</author><pubDate>Tue, 07 Nov 2023 16:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04131v1</guid></item><item><title>Generative learning for nonlinear dynamics</title><link>http://arxiv.org/abs/2311.04128v1</link><description>Modern generative machine learning models demonstrate surprising ability tocreate realistic outputs far beyond their training data, such as photorealisticartwork, accurate protein structures, or conversational text. These successessuggest that generative models learn to effectively parametrize and samplearbitrarily complex distributions. Beginning half a century ago, foundationalworks in nonlinear dynamics used tools from information theory to inferproperties of chaotic attractors from time series, motivating the developmentof algorithms for parametrizing chaos in real datasets. In this perspective, weaim to connect these classical works to emerging themes in large-scalegenerative statistical learning. We first consider classical attractorreconstruction, which mirrors constraints on latent representations learned bystate space models of time series. We next revisit early efforts to usesymbolic approximations to compare minimal discrete generators underlyingcomplex processes, a problem relevant to modern efforts to distill andinterpret black-box statistical models. Emerging interdisciplinary works bridgenonlinear dynamics and learning theory, such as operator-theoretic methods forcomplex fluid flows, or detection of broken detailed balance in biologicaldatasets. We anticipate that future machine learning techniques may revisitother classical concepts from nonlinear dynamics, such as transinformationdecay and complexity-entropy tradeoffs.</description><author>William Gilpin</author><pubDate>Tue, 07 Nov 2023 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04128v1</guid></item><item><title>Optimal Transport for Change Detection on LiDAR Point Clouds</title><link>http://arxiv.org/abs/2302.07025v4</link><description>Unsupervised change detection between airborne LiDAR data points, taken atseparate times over the same location, can be difficult due to unmatchingspatial support and noise from the acquisition system. Most current approachesto detect changes in point clouds rely heavily on the computation of DigitalElevation Models (DEM) images and supervised methods. Obtaining a DEM leads toLiDAR informational loss due to pixelisation, and supervision requires largeamounts of labelled data often unavailable in real-world scenarios. We proposean unsupervised approach based on the computation of the transport of 3D LiDARpoints over two temporal supports. The method is based on unbalanced optimaltransport and can be generalised to any change detection problem with LiDARdata. We apply our approach to publicly available datasets for monitoring urbansprawling in various noise and resolution configurations that mimic severalsensors used in practice. Our method allows for unsupervised multi-classclassification and outperforms the previous state-of-the-art unsupervisedapproaches by a significant margin.</description><author>Marco Fiorucci, Peter Naylor, Makoto Yamada</author><pubDate>Tue, 07 Nov 2023 16:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07025v4</guid></item><item><title>MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device</title><link>http://arxiv.org/abs/2310.01258v2</link><description>Neural video codecs have recently become competitive with standard codecssuch as HEVC in the low-delay setting. However, most neural codecs are largefloating-point networks that use pixel-dense warping operations for temporalmodeling, making them too computationally expensive for deployment on mobiledevices. Recent work has demonstrated that running a neural decoder in realtime on mobile is feasible, but shows this only for 720p RGB video. This workpresents the first neural video codec that decodes 1080p YUV420 video in realtime on a mobile device. Our codec relies on two major contributions. First, wedesign an efficient codec that uses a block-based motion compensation algorithmavailable on the warping core of the mobile accelerator, and we show how toquantize this model to integer precision. Second, we implement a fast decoderpipeline that concurrently runs neural network components on the neural signalprocessor, parallel entropy coding on the mobile GPU, and warping on thewarping core. Our codec outperforms the previous on-device codec by a largemargin with up to 48% BD-rate savings, while reducing the MAC count on thereceiver side by $10 \times$. We perform a careful ablation to demonstrate theeffect of the introduced motion compensation scheme, and ablate the effect ofmodel quantization.</description><author>Ties van Rozendaal, Tushar Singhal, Hoang Le, Guillaume Sautiere, Amir Said, Krishna Buska, Anjuman Raha, Dimitris Kalatzis, Hitarth Mehta, Frank Mayer, Liang Zhang, Markus Nagel, Auke Wiggers</author><pubDate>Tue, 07 Nov 2023 16:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01258v2</guid></item><item><title>Unveiling Safety Vulnerabilities of Large Language Models</title><link>http://arxiv.org/abs/2311.04124v1</link><description>As large language models become more prevalent, their possible harmful orinappropriate responses are a cause for concern. This paper introduces a uniquedataset containing adversarial examples in the form of questions, which we callAttaQ, designed to provoke such harmful or inappropriate responses. We assessthe efficacy of our dataset by analyzing the vulnerabilities of various modelswhen subjected to it. Additionally, we introduce a novel automatic approach foridentifying and naming vulnerable semantic regions - input semantic areas forwhich the model is likely to produce harmful outputs. This is achieved throughthe application of specialized clustering techniques that consider both thesemantic similarity of the input attacks and the harmfulness of the model'sresponses. Automatically identifying vulnerable semantic regions enhances theevaluation of model weaknesses, facilitating targeted improvements to itssafety mechanisms and overall reliability.</description><author>George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi</author><pubDate>Tue, 07 Nov 2023 16:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04124v1</guid></item><item><title>Generalizing to new geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation</title><link>http://arxiv.org/abs/2305.11531v4</link><description>Generation of simulated detector response to collision products is crucial todata analysis in particle physics, but computationally very expensive. Onesubdetector, the calorimeter, dominates the computational time due to the highgranularity of its cells and complexity of the interactions. Generative modelscan provide more rapid sample production, but currently require significanteffort to optimize performance for specific detector geometries, oftenrequiring many models to describe the varying cell sizes and arrangements,without the ability to generalize to other geometries. We develop a$\textit{geometry-aware}$ autoregressive model, which learns how thecalorimeter response varies with geometry, and is capable of generatingsimulated responses to unseen geometries without additional training. Thegeometry-aware model outperforms a baseline unaware model by over $50\%$ inseveral metrics such as the Wasserstein distance between the generated and thetrue distributions of key quantities which summarize the simulated response. Asingle geometry-aware model could replace the hundreds of generative modelscurrently designed for calorimeter simulation by physicists analyzing datacollected at the Large Hadron Collider. This proof-of-concept study motivatesthe design of a foundational model that will be a crucial tool for the study offuture detectors, dramatically reducing the large upfront investment usuallyneeded to develop generative calorimeter models.</description><author>Junze Liu, Aishik Ghosh, Dylan Smith, Pierre Baldi, Daniel Whiteson</author><pubDate>Tue, 07 Nov 2023 16:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11531v4</guid></item><item><title>Image Amodal Completion: A Survey</title><link>http://arxiv.org/abs/2207.02062v3</link><description>Existing computer vision systems can compete with humans in understanding thevisible parts of objects, but still fall far short of humans when it comes todepicting the invisible parts of partially occluded objects. Image amodalcompletion aims to equip computers with human-like amodal completion functionsto understand an intact object despite it being partially occluded. The mainpurpose of this survey is to provide an intuitive understanding of the researchhotspots, key technologies and future trends in the field of image amodalcompletion. Firstly, we present a comprehensive review of the latest literaturein this emerging field, exploring three key tasks in image amodal completion,including amodal shape completion, amodal appearance completion, and orderperception. Then we examine popular datasets related to image amodal completionalong with their common data collection methods and evaluation metrics.Finally, we discuss real-world applications and future research directions forimage amodal completion, facilitating the reader's understanding of thechallenges of existing technologies and upcoming research trends.</description><author>Jiayang Ao, Qiuhong Ke, Krista A. Ehinger</author><pubDate>Tue, 07 Nov 2023 16:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02062v3</guid></item><item><title>Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL</title><link>http://arxiv.org/abs/2310.04411v2</link><description>The divergence of the Q-value estimation has been a prominent issue inoffline RL, where the agent has no access to real dynamics. Traditional beliefsattribute this instability to querying out-of-distribution actions whenbootstrapping value targets. Though this issue can be alleviated with policyconstraints or conservative Q estimation, a theoretical understanding of theunderlying mechanism causing the divergence has been absent. In this work, weaim to thoroughly comprehend this mechanism and attain an improved solution. Wefirst identify a fundamental pattern, self-excitation, as the primary cause ofQ-value estimation divergence in offline RL. Then, we propose a novelSelf-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel(NTK) to measure the evolving property of Q-network at training, which providesan intriguing explanation of the emergence of divergence. For the first time,our theory can reliably decide whether the training will diverge at an earlystage, and even predict the order of the growth for the estimated Q-value, themodel's norm, and the crashing step when an SGD optimizer is used. Theexperiments demonstrate perfect alignment with this theoretic analysis.Building on our insights, we propose to resolve divergence from a novelperspective, namely improving the model's architecture for better extrapolatingbehavior. Through extensive empirical studies, we identify LayerNorm as a goodsolution to effectively avoid divergence without introducing detrimental bias,leading to superior performance. Experimental results prove that it can stillwork in some most challenging settings, i.e. using only 1 transitions of thedataset, where all previous methods fail. Moreover, it can be easily pluggedinto modern offline RL methods and achieve SOTA results on many challengingtasks. We also give unique insights into its effectiveness.</description><author>Yang Yue, Rui Lu, Bingyi Kang, Shiji Song, Gao Huang</author><pubDate>Tue, 07 Nov 2023 16:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04411v2</guid></item><item><title>Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection</title><link>http://arxiv.org/abs/2311.04109v1</link><description>Recently, pretrained language models have shown state-of-the-art performanceon the vulnerability detection task. These models are pretrained on a largecorpus of source code, then fine-tuned on a smaller supervised vulnerabilitydataset. Due to the different training objectives and the performance of themodels, it is interesting to consider whether the models have learned thesemantics of code relevant to vulnerability detection, namely bug semantics,and if so, how the alignment to bug semantics relates to model performance. Inthis paper, we analyze the models using three distinct methods:interpretability tools, attention analysis, and interaction matrix analysis. Wecompare the models' influential feature sets with the bug semantic featureswhich define the causes of bugs, including buggy paths and PotentiallyVulnerable Statements (PVS). We find that (1) better-performing models alsoaligned better with PVS, (2) the models failed to align strongly to PVS, and(3) the models failed to align at all to buggy paths. Based on our analysis, wedeveloped two annotation methods which highlight the bug semantics inside themodel's inputs. We evaluated our approach on four distinct transformer modelsand four vulnerability datasets and found that our annotations improved themodels' performance in the majority of settings - 11 out of 16, with up to 9.57points improvement in F1 score compared to conventional fine-tuning. We furtherfound that with our annotations, the models aligned up to 232% better topotentially vulnerable statements. Our findings indicate that it is helpful toprovide the model with information of the bug semantics, that the model canattend to it, and motivate future work in learning more complex path-based bugsemantics. Our code and data are available athttps://figshare.com/s/4a16a528d6874aad51a0.</description><author>Benjamin Steenhoek, Md Mahbubur Rahman, Shaila Sharmin, Wei Le</author><pubDate>Tue, 07 Nov 2023 16:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04109v1</guid></item><item><title>Interactive Semantic Map Representation for Skill-based Visual Object Navigation</title><link>http://arxiv.org/abs/2311.04107v1</link><description>Visual object navigation using learning methods is one of the key tasks inmobile robotics. This paper introduces a new representation of a scene semanticmap formed during the embodied agent interaction with the indoor environment.It is based on a neural network method that adjusts the weights of thesegmentation model with backpropagation of the predicted fusion loss valuesduring inference on a regular (backward) or delayed (forward) image sequence.We have implemented this representation into a full-fledged navigation approachcalled SkillTron, which can select robot skills from end-to-end policies basedon reinforcement learning and classic map-based planning methods. The proposedapproach makes it possible to form both intermediate goals for robotexploration and the final goal for object navigation. We conducted intensiveexperiments with the proposed approach in the Habitat environment, which showeda significant superiority in navigation quality metrics compared tostate-of-the-art approaches. The developed code and used custom datasets arepublicly available at github.com/AIRI-Institute/skill-fusion.</description><author>Tatiana Zemskova, Aleksei Staroverov, Kirill Muravyev, Dmitry Yudin, Aleksandr Panov</author><pubDate>Tue, 07 Nov 2023 16:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04107v1</guid></item><item><title>Large Language Models as Superpositions of Cultural Perspectives</title><link>http://arxiv.org/abs/2307.07870v3</link><description>Large Language Models (LLMs) are often misleadingly recognized as having apersonality or a set of values. We argue that an LLM can be seen as asuperposition of perspectives with different values and personality traits.LLMs exhibit context-dependent values and personality traits that change basedon the induced perspective (as opposed to humans, who tend to have morecoherent values and personality traits across contexts). We introduce theconcept of perspective controllability, which refers to a model's affordance toadopt various perspectives with differing values and personality traits. In ourexperiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to studyhow exhibited values and personality traits change based on differentperspectives. Through qualitative experiments, we show that LLMs expressdifferent values when those are (implicitly or explicitly) implied in theprompt, and that LLMs express different values even when those are notobviously implied (demonstrating their context-dependent nature). We thenconduct quantitative experiments to study the controllability of differentmodels (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), theeffectiveness of various methods for inducing perspectives, and the smoothnessof the models' drivability. We conclude by examining the broader implicationsof our work and outline a variety of associated scientific questions. Theproject website is available athttps://sites.google.com/view/llm-superpositions .</description><author>Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas, Peter Ford Dominey, Pierre-Yves Oudeyer</author><pubDate>Tue, 07 Nov 2023 16:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07870v3</guid></item><item><title>Optimisation via encodings: a renormalisation group perspective</title><link>http://arxiv.org/abs/2303.16258v2</link><description>Difficult, in particular NP-complete, optimization problems are traditionallysolved approximately using search heuristics. These are usually slowed down bythe rugged landscapes encountered, because local minima arrest the searchprocess. Cover-encoding maps were devised to circumvent this problem bytransforming the original landscape to one that is free of local minima andenriched in near-optimal solutions. By definition, these involve the mapping ofthe original (larger) search space into smaller subspaces, by processes thattypically amount to a form of coarse-graining. In this paper, we explore thedetails of this coarse-graining using formal arguments, as well as concreteexamples of cover-encoding maps, that are investigated analytically as well ascomputationally. Our results strongly suggest that the coarse-graining involvedin cover-encoding maps bears a strong resemblance to that encountered inrenormalisation group schemes. Given the apparently disparate nature of thesetwo formalisms, these strong similarities are rather startling, and suggestdeep mathematical underpinnings that await further exploration.</description><author>Konstantin Klemm, Anita Mehta, Peter F. Stadler</author><pubDate>Tue, 07 Nov 2023 16:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16258v2</guid></item><item><title>DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding</title><link>http://arxiv.org/abs/2311.04098v1</link><description>Recent advances in computer vision (CV) and natural language processing havebeen driven by exploiting big data on practical applications. However, theseresearch fields are still limited by the sheer volume, versatility, anddiversity of the available datasets. CV tasks, such as image captioning, whichhas primarily been carried out on natural images, still struggle to produceaccurate and meaningful captions on sketched images often included inscientific and technical documents. The advancement of other tasks such as 3Dreconstruction from 2D images requires larger datasets with multipleviewpoints. We introduce DeepPatent2, a large-scale dataset, providing morethan 2.7 million technical drawings with 132,890 object names and 22,394viewpoints extracted from 14 years of US design patent documents. Wedemonstrate the usefulness of DeepPatent2 with conceptual captioning. Wefurther provide the potential usefulness of our dataset to facilitate otherresearch areas such as 3D image reconstruction and image retrieval.</description><author>Kehinde Ajayi, Xin Wei, Martin Gryder, Winston Shields, Jian Wu, Shawn M. Jones, Michal Kucer, Diane Oyen</author><pubDate>Tue, 07 Nov 2023 16:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04098v1</guid></item><item><title>Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset</title><link>http://arxiv.org/abs/2311.04095v1</link><description>We present PD-REAL, a novel large-scale dataset for unsupervised anomalydetection (AD) in the 3D domain. It is motivated by the fact that 2D-onlyrepresentations in the AD task may fail to capture the geometric structures ofanomalies due to uncertainty in lighting conditions or shooting angles. PD-REALconsists entirely of Play-Doh models for 15 object categories and focuses onthe analysis of potential benefits from 3D information in a controlledenvironment. Specifically, objects are first created with six types ofanomalies, such as dent, crack, or perforation, and then photographed underdifferent lighting conditions to mimic real-world inspection scenarios. Todemonstrate the usefulness of 3D information, we use a commercially availableRealSense camera to capture RGB and depth images. Compared to the existing 3Ddataset for AD tasks, the data acquisition of PD-REAL is significantly cheaper,easily scalable and easier to control variables. Extensive evaluations withstate-of-the-art AD algorithms on our dataset demonstrate the benefits as wellas challenges of using 3D information. Our dataset can be downloaded fromhttps://github.com/Andy-cs008/PD-REAL</description><author>Jianjian Qin, Chunzhi Gu, Jun Yu, Chao Zhang</author><pubDate>Tue, 07 Nov 2023 16:05:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04095v1</guid></item><item><title>Proceedings of the 5th International Workshop on Reading Music Systems</title><link>http://arxiv.org/abs/2311.04091v1</link><description>The International Workshop on Reading Music Systems (WoRMS) is a workshopthat tries to connect researchers who develop systems for reading music, suchas in the field of Optical Music Recognition, with other researchers andpractitioners that could benefit from such systems, like librarians ormusicologists. The relevant topics of interest for the workshop include, butare not limited to: Music reading systems; Optical music recognition; Datasetsand performance evaluation; Image processing on music scores; Writeridentification; Authoring, editing, storing and presentation systems for musicscores; Multi-modal systems; Novel input-methods for music to produce writtenmusic; Web-based Music Information Retrieval services; Applications andprojects; Use-cases related to written music. These are the proceedings of the 5th International Workshop on Reading MusicSystems, held in Milan, Italy on Nov. 4th 2023.</description><author>Jorge Calvo-Zaragoza, Alexander Pacha, Elona Shatri</author><pubDate>Tue, 07 Nov 2023 16:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04091v1</guid></item><item><title>Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients' Speech</title><link>http://arxiv.org/abs/2311.04088v1</link><description>In disentangling the heterogeneity observed in psychopathology, personalityof the patients is considered crucial. While it has been demonstrated thatpersonality traits are reflected in the language used by a patient, wehypothesize that this enables automatic inference of the personality typedirectly from speech utterances, potentially more accurately than through atraditional questionnaire-based approach explicitly designed for personalityclassification. To validate this hypothesis, we adopt natural languageprocessing (NLP) and standard machine learning tools for classification. Wetest this on a dataset of recorded clinical diagnostic interviews (CDI) on asample of 79 patients diagnosed with major depressive disorder (MDD) -- acondition for which differentiated treatment based on personality styles hasbeen advocated -- and classified into anaclitic and introjective personalitystyles. We start by analyzing the interviews to see which linguistic featuresare associated with each style, in order to gain a better understanding of thestyles. Then, we develop automatic classifiers based on (a) standardizedquestionnaire responses; (b) basic text features, i.e., TF-IDF scores of wordsand word sequences; (c) more advanced text features, using LIWC (linguisticinquiry and word count) and context-aware features using BERT (bidirectionalencoder representations from transformers); (d) audio features. We find thatautomated classification with language-derived features (i.e., based on LIWC)significantly outperforms questionnaire-based classification models.Furthermore, the best performance is achieved by combining LIWC with thequestionnaire features. This suggests that more work should be put intodeveloping linguistically based automated techniques for characterizingpersonality, however questionnaires still to some extent complement suchmethods.</description><author>Semere Kiros Bitew, Vincent Schelstraete, Klim Zaporojets, Kimberly Van Nieuwenhove, Reitske Meganck, Chris Develder</author><pubDate>Tue, 07 Nov 2023 15:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04088v1</guid></item><item><title>Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study</title><link>http://arxiv.org/abs/2311.02747v2</link><description>Within (semi-)automated visual industrial inspection, learning-basedapproaches for assessing visual defects, including deep neural networks, enablethe processing of otherwise small defect patterns in pixel size onhigh-resolution imagery. The emergence of these often rarely occurring defectpatterns explains the general need for labeled data corpora. To alleviate thisissue and advance the current state of the art in unsupervised visualinspection, this work proposes a DifferNet-based solution enhanced withattention modules: AttentDifferNet. It improves image-level detection andclassification capabilities on three visual anomaly detection datasets forindustrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. Incomparison to the state of the art, AttentDifferNet achieves improved results,which are, in turn, highlighted throughout our quali-quantitative study. Ourquantitative evaluation shows an average improvement - compared to DifferNet -of 1.77 +/- 0.25 percentage points in overall AUROC considering all threedatasets, reaching SOTA results in InsPLAD-fault, an industrial inspectionin-the-wild dataset. As our variants to AttentDifferNet show great prospects inthe context of currently investigated approaches, a baseline is formulated,emphasizing the importance of attention for industrial anomaly detection bothin the wild and in controlled environments.</description><author>André Luiz Buarque Vieira e Silva, Francisco Simões, Danny Kowerko, Tobias Schlosser, Felipe Battisti, Veronica Teichrieb</author><pubDate>Tue, 07 Nov 2023 15:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02747v2</guid></item><item><title>Time-Efficient Reinforcement Learning with Stochastic Stateful Policies</title><link>http://arxiv.org/abs/2311.04082v1</link><description>Stateful policies play an important role in reinforcement learning, such ashandling partially observable environments, enhancing robustness, or imposingan inductive bias directly into the policy structure. The conventional methodfor training stateful policies is Backpropagation Through Time (BPTT), whichcomes with significant drawbacks, such as slow training due to sequentialgradient propagation and the occurrence of vanishing or exploding gradients.The gradient is often truncated to address these issues, resulting in a biasedpolicy update. We present a novel approach for training stateful policies bydecomposing the latter into a stochastic internal state kernel and a statelesspolicy, jointly optimized by following the stateful policy gradient. Weintroduce different versions of the stateful policy gradient theorem, enablingus to easily instantiate stateful variants of popular reinforcement learningand imitation learning algorithms. Furthermore, we provide a theoreticalanalysis of our new gradient estimator and compare it with BPTT. We evaluateour approach on complex continuous control tasks, e.g., humanoid locomotion,and demonstrate that our gradient estimator scales effectively with taskcomplexity while offering a faster and simpler alternative to BPTT.</description><author>Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</author><pubDate>Tue, 07 Nov 2023 15:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04082v1</guid></item><item><title>Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data</title><link>http://arxiv.org/abs/2311.04081v1</link><description>Ultrasound Localization Microscopy (ULM) enables imaging of vascularstructures in the micrometer range by accumulating contrast agent particlelocations over time. Precise and efficient target localization accuracy remainsan active research topic in the ULM field to further push the boundaries ofthis promising medical imaging technology. Existing work incorporatesDelay-And-Sum (DAS) beamforming into particle localization pipelines, whichultimately determines the ULM image resolution capability. In this paper wepropose to feed unprocessed Radio-Frequency (RF) data into a super-resolutionnetwork while bypassing DAS beamforming and its limitations. To facilitatethis, we demonstrate label projection and inverse point transformation betweenB-mode and RF coordinate space as required by our approach. We assess ourmethod against state-of-the-art techniques based on a public dataset featuringin silico and in vivo data. Results from our RF-trained network suggest thatexcluding DAS beamforming offers a great potential to optimize on the ULMresolution performance.</description><author>Christopher Hahne, Georges Chabouh, Olivier Couture, Raphael Sznitman</author><pubDate>Tue, 07 Nov 2023 15:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04081v1</guid></item><item><title>Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps</title><link>http://arxiv.org/abs/2311.04079v1</link><description>Autonomous driving has traditionally relied heavily on costly andlabor-intensive High Definition (HD) maps, hindering scalability. In contrast,Standard Definition (SD) maps are more affordable and have worldwide coverage,offering a scalable alternative. In this work, we systematically explore theeffect of SD maps for real-time lane-topology understanding. We propose a novelframework to integrate SD maps into online map prediction and propose aTransformer-based encoder, SD Map Encoder Representations from transFormers, toleverage priors in SD maps for the lane-topology prediction task. Thisenhancement consistently and significantly boosts (by up to 60%) lane detectionand topology prediction on current state-of-the-art online map predictionmethods without bells and whistles and can be immediately incorporated into anyTransformer-based lane-topology method. Code is available athttps://github.com/NVlabs/SMERF.</description><author>Katie Z Luo, Xinshuo Weng, Yan Wang, Shuang Wu, Jie Li, Kilian Q Weinberger, Yue Wang, Marco Pavone</author><pubDate>Tue, 07 Nov 2023 15:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04079v1</guid></item><item><title>Do LLMs exhibit human-like response biases? A case study in survey design</title><link>http://arxiv.org/abs/2311.04076v1</link><description>As large language models (LLMs) become more capable, there is growingexcitement about the possibility of using LLMs as proxies for humans inreal-world tasks where subjective labels are desired, such as in surveys andopinion polling. One widely-cited barrier to the adoption of LLMs is theirsensitivity to prompt wording -- but interestingly, humans also displaysensitivities to instruction changes in the form of response biases. As such,we argue that if LLMs are going to be used to approximate human opinions, it isnecessary to investigate the extent to which LLMs also reflect human responsebiases, if at all. In this work, we use survey design as a case study, wherehuman response biases caused by permutations in wordings of ``prompts'' havebeen extensively studied. Drawing from prior work in social psychology, wedesign a dataset and propose a framework to evaluate whether LLMs exhibithuman-like response biases in survey questionnaires. Our comprehensiveevaluation of nine models shows that popular open and commercial LLMs generallyfail to reflect human-like behavior. These inconsistencies tend to be moreprominent in models that have been instruction fine-tuned. Furthermore, even ifa model shows a significant change in the same direction as humans, we findthat perturbations that are not meant to elicit significant changes in humansmay also result in a similar change, suggesting that such a result could bepartially due to other spurious correlations. These results highlight thepotential pitfalls of using LLMs to substitute humans in parts of theannotation pipeline, and further underscore the importance of finer-grainedcharacterizations of model behavior. Our code, dataset, and collected samplesare available at https://github.com/lindiatjuatja/BiasMonkey</description><author>Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig</author><pubDate>Tue, 07 Nov 2023 15:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04076v1</guid></item><item><title>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</title><link>http://arxiv.org/abs/2310.10149v2</link><description>This study introduces the concept of "structural beauty" as an objectivecomputational approach for evaluating the aesthetic appeal of images. Throughthe utilization of the Segment anything model (SAM), we propose a method thatleverages recursive segmentation to extract finer-grained substructures.Additionally, by reconstructing the hierarchical structure, we obtain a moreaccurate representation of substructure quantity and hierarchy. This approachreproduces and extends our previous research, allowing for the simultaneousassessment of Livingness in full-color images without the need for grayscaleconversion or separate computations for foreground and background Livingness.Furthermore, the application of our method to the Scenic or Not dataset, arepository of subjective scenic ratings, demonstrates a high degree ofconsistency with subjective ratings in the 0-6 score range. This underscoresthat structural beauty is not solely a subjective perception, but aquantifiable attribute accessible through objective computation. Through ourcase studies, we have arrived at three significant conclusions. 1) our methoddemonstrates the capability to accurately segment meaningful objects, includingtrees, buildings, and windows, as well as abstract substructures withinpaintings. 2) we observed that the clarity of an image impacts ourcomputational results; clearer images tend to yield higher Livingness scores.However, for equally blurry images, Livingness does not exhibit a significantreduction, aligning with human visual perception. 3) our approach fundamentallydiffers from methods employing Convolutional Neural Networks (CNNs) forpredicting image scores. Our method not only provides computational results butalso offers transparency and interpretability, positioning it as a novel avenuein the realm of Explainable AI (XAI).</description><author>Yao Qianxiang, Bin Jiang</author><pubDate>Tue, 07 Nov 2023 15:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10149v2</guid></item><item><title>Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification</title><link>http://arxiv.org/abs/2301.09702v3</link><description>Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims tolearn identity information from labeled images in source domains and apply itto unlabeled images in a target domain. One major issue with many unsupervisedre-identification methods is that they do not perform well relative to largedomain variations such as illumination, viewpoint, and occlusions. In thispaper, we propose a Synthesis Model Bank (SMB) to deal with illuminationvariation in unsupervised person re-ID. The proposed SMB consists of severalconvolutional neural networks (CNN) for feature extraction and Mahalanobismatrices for distance metrics. They are trained using synthetic data withdifferent illumination conditions such that their synergistic effect makes theSMB robust against illumination variation. To better quantify the illuminationintensity and improve the quality of synthetic images, we introduce a new 3Dvirtual-human dataset for GAN-based image synthesis. From our experiments, theproposed SMB outperforms other synthesis methods on several re-ID benchmarks.</description><author>Jiaqi Guo, Amy R. Reibman, Edward J. Delp</author><pubDate>Tue, 07 Nov 2023 15:38:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09702v3</guid></item><item><title>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</title><link>http://arxiv.org/abs/2311.04072v1</link><description>Alignment with human preference is a desired property of large languagemodels (LLMs). Currently, the main alignment approach is based on reinforcementlearning from human feedback (RLHF). Despite the effectiveness of RLHF, it isintricate to implement and train, thus recent studies explore how to developalternative alignment approaches based on supervised fine-tuning (SFT). A majorlimitation of SFT is that it essentially does imitation learning, which cannotfully understand what are the expected behaviors. To address this issue, wepropose an improved alignment approach named FIGA. Different from priormethods, we incorporate fine-grained (i.e., token or phrase level) qualitysignals that are derived by contrasting good and bad responses. Our approachhas made two major contributions. Firstly, we curate a refined alignmentdataset that pairs initial responses and the corresponding revised ones.Secondly, we devise a new loss function can leverage fine-grained qualitysignals to instruct the learning of LLMs for alignment. Extensive experimentshave demonstrated the effectiveness of our approaches by comparing a number ofcompetitive baselines.</description><author>Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen</author><pubDate>Tue, 07 Nov 2023 15:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04072v1</guid></item><item><title>AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models</title><link>http://arxiv.org/abs/2311.01305v2</link><description>Large language models(LLMs) exhibit excellent performance across a variety oftasks, but they come with significant computational and storage costs.Quantizing these models is an effective way to alleviate this issue. However,existing methods struggle to strike a balance between model accuracy andhardware efficiency. This is where we introduce AWEQ, a post-training methodthat requires no additional training overhead. AWEQ excels in bothultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.There is an observation that weight quantization is less challenging thanactivation quantization. AWEQ transfers the difficulty of activationquantization to weights using channel equalization, achieving a balance betweenthe quantization difficulties of both, and thereby maximizing performance. Wehave further refined the equalization method to mitigate quantization biaserror, ensuring the robustness of the model. Extensive experiments on popularmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existingpost-training quantization methods for large models.</description><author>Baisong Li, Xingwang Wang, Haixiao Xu</author><pubDate>Tue, 07 Nov 2023 15:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01305v2</guid></item><item><title>Energy-based Calibrated VAE with Test Time Free Lunch</title><link>http://arxiv.org/abs/2311.04071v1</link><description>In this paper, we propose a novel Energy-Calibrated Generative Model thatutilizes a Conditional EBM for enhancing Variational Autoencoders (VAEs). VAEsare sampling efficient but often suffer from blurry generation results due tothe lack of training in the generative direction. On the other hand,Energy-Based Models (EBMs) can generate high-quality samples but requireexpensive Markov Chain Monte Carlo (MCMC) sampling. To address these issues, weintroduce a Conditional EBM for calibrating the generative direction duringtraining, without requiring it for test time sampling. Our approach enables thegenerative model to be trained upon data and calibrated samples with adaptiveweight, thereby enhancing efficiency and effectiveness without necessitatingMCMC sampling in the inference phase. We also show that the proposed approachcan be extended to calibrate normalizing flows and variational posterior.Moreover, we propose to apply the proposed method to zero-shot imagerestoration via neural transport prior and range-null theory. We demonstratethe effectiveness of the proposed method through extensive experiments invarious applications, including image generation and zero-shot imagerestoration. Our method shows state-of-the-art performance over single-stepnon-adversarial generation.</description><author>Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang</author><pubDate>Tue, 07 Nov 2023 15:35:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04071v1</guid></item><item><title>Latent Diffusion for Language Generation</title><link>http://arxiv.org/abs/2212.09462v2</link><description>Diffusion models have achieved great success in modeling continuous datamodalities such as images, audio, and video, but have seen limited use indiscrete domains such as language. Recent attempts to adapt diffusion tolanguage have presented diffusion as an alternative to existing pretrainedlanguage models. We view diffusion and existing language models ascomplementary. We demonstrate that encoder-decoder language models can beutilized to efficiently learn high-quality language autoencoders. We thendemonstrate that continuous diffusion models can be learned in the latent spaceof the language autoencoder, enabling us to sample continuous latentrepresentations that can be decoded into natural language with the pretraineddecoder. We validate the effectiveness of our approach for unconditional,class-conditional, and sequence-to-sequence language generation. We demonstrateacross multiple diverse data sets that our latent language diffusion models aresignificantly more effective than previous diffusion language models.</description><author>Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Q. Weinberger</author><pubDate>Tue, 07 Nov 2023 15:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09462v2</guid></item><item><title>LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs</title><link>http://arxiv.org/abs/2311.04069v1</link><description>Social behavior, defined as the process by which individuals act and react inresponse to others, is crucial for the function of societies and holds profoundimplications for mental health. To fully grasp the intricacies of socialbehavior and identify potential therapeutic targets for addressing socialdeficits, it is essential to understand its core principles. Although machinelearning algorithms have made it easier to study specific aspects of complexbehavior, current methodologies tend to focus primarily on single-animalbehavior. In this study, we introduce LISBET (seLf-supervIsed Social BEhavioralTransformer), a model designed to detect and segment social interactions. Ourmodel eliminates the need for feature selection and extensive human annotationby using self-supervised learning to detect and quantify social behaviors fromdynamic body parts tracking data. LISBET can be used in hypothesis-driven modeto automate behavior classification using supervised finetuning, and indiscovery-driven mode to segment social behavior motifs using unsupervisedlearning. We found that motifs recognized using the discovery-driven approachnot only closely match the human annotations but also correlate with theelectrophysiological activity of dopaminergic neurons in the Ventral TegmentalArea (VTA). We hope LISBET will help the community improve our understanding ofsocial behaviors and their neural underpinnings.</description><author>Giuseppe Chindemi, Benoit Girard, Camilla Bellone</author><pubDate>Tue, 07 Nov 2023 15:35:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04069v1</guid></item><item><title>ProS: Facial Omni-Representation Learning via Prototype-based Self-Distillation</title><link>http://arxiv.org/abs/2311.01929v2</link><description>This paper presents a novel approach, called Prototype-basedSelf-Distillation (ProS), for unsupervised face representation learning. Theexisting supervised methods heavily rely on a large amount of annotatedtraining facial data, which poses challenges in terms of data collection andprivacy concerns. To address these issues, we propose ProS, which leverages avast collection of unlabeled face images to learn a comprehensive facialomni-representation. In particular, ProS consists of two vision-transformers(teacher and student models) that are trained with different augmented images(cropping, blurring, coloring, etc.). Besides, we build a face-aware retrievalsystem along with augmentations to obtain the curated images comprisingpredominantly facial areas. To enhance the discrimination of learned features,we introduce a prototype-based matching loss that aligns the similaritydistributions between features (teacher or student) and a set of learnableprototypes. After pre-training, the teacher vision transformer serves as abackbone for downstream tasks, including attribute estimation, expressionrecognition, and landmark alignment, achieved through simple fine-tuning withadditional layers. Extensive experiments demonstrate that our method achievesstate-of-the-art performance on various tasks, both in full and few-shotsettings. Furthermore, we investigate pre-training with synthetic face images,and ProS exhibits promising performance in this scenario as well.</description><author>Xing Di, Yiyu Zheng, Xiaoming Liu, Yu Cheng</author><pubDate>Tue, 07 Nov 2023 15:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01929v2</guid></item><item><title>Multitask Multimodal Prompted Training for Interactive Embodied Task Completion</title><link>http://arxiv.org/abs/2311.04067v1</link><description>Interactive and embodied tasks pose at least two fundamental challenges toexisting Vision &amp; Language (VL) models, including 1) grounding language intrajectories of actions and observations, and 2) referential disambiguation. Totackle these challenges, we propose an Embodied MultiModal Agent (EMMA): aunified encoder-decoder model that reasons over images and trajectories, andcasts action prediction as multimodal text generation. By unifying all tasks astext generation, EMMA learns a language of actions which facilitates transferacross tasks. Different to previous modular approaches with independentlytrained components, we use a single multitask model where each task contributesto goal completion. EMMA performs on par with similar models on several VLbenchmarks and sets a new state-of-the-art performance (36.81% success rate) onthe Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guidedagents in the Alexa Arena</description><author>Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia</author><pubDate>Tue, 07 Nov 2023 15:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04067v1</guid></item><item><title>Can CLIP Help Sound Source Localization?</title><link>http://arxiv.org/abs/2311.04066v1</link><description>Large-scale pre-trained image-text models demonstrate remarkable versatilityacross diverse tasks, benefiting from their robust representationalcapabilities and effective multimodal alignment. We extend the application ofthese models, specifically CLIP, to the domain of sound source localization.Unlike conventional approaches, we employ the pre-trained CLIP model withoutexplicit text input, relying solely on the audio-visual correspondence. To thisend, we introduce a framework that translates audio signals into tokenscompatible with CLIP's text encoder, yielding audio-driven embeddings. Bydirectly using these embeddings, our method generates audio-grounded masks forthe provided audio, extracts audio-grounded image features from the highlightedregions, and aligns them with the audio-driven embeddings using theaudio-visual correspondence objective. Our findings suggest that utilizingpre-trained image-text models enable our model to generate more complete andcompact localization maps for the sounding objects. Extensive experiments showthat our method outperforms state-of-the-art approaches by a significantmargin.</description><author>Sooyoung Park, Arda Senocak, Joon Son Chung</author><pubDate>Tue, 07 Nov 2023 15:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04066v1</guid></item><item><title>Implementation and Comparison of Methods to Extract Reliability KPIs out of Textual Wind Turbine Maintenance Work Orders</title><link>http://arxiv.org/abs/2311.04064v1</link><description>Maintenance work orders are commonly used to document information about windturbine operation and maintenance. This includes details about proactive andreactive wind turbine downtimes, such as preventative and correctivemaintenance. However, the information contained in maintenance work orders isoften unstructured and difficult to analyze, making it challenging fordecision-makers to use this information for optimizing operation andmaintenance. To address this issue, this work presents three differentapproaches to calculate reliability key performance indicators from maintenancework orders. The first approach involves manual labeling of the maintenancework orders by domain experts, using the schema defined in an industrialguideline to assign the label accordingly. The second approach involves thedevelopment of a model that automatically labels the maintenance work ordersusing text classification methods. The third technique uses an AI-assistedtagging tool to tag and structure the raw maintenance information contained inthe maintenance work orders. The resulting calculated reliability keyperformance indicator of the first approach are used as a benchmark forcomparison with the results of the second and third approaches. The quality andtime spent are considered as criteria for evaluation. Overall, these threemethods make extracting maintenance information from maintenance work ordersmore efficient, enable the assessment of reliability key performance indicatorsand therefore support the optimization of wind turbine operation andmaintenance.</description><author>Marc-Alexander Lutz, Bastian Schäfermeier, Rachael Sexton, Michael Sharp, Alden Dima, Stefan Faulstich, Jagan Mohini Aluri</author><pubDate>Tue, 07 Nov 2023 15:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04064v1</guid></item><item><title>Towards Accelerated Model Training via Bayesian Data Selection</title><link>http://arxiv.org/abs/2308.10544v3</link><description>Mislabeled, duplicated, or biased data in real-world scenarios can lead toprolonged training and even hinder model convergence. Traditional solutionsprioritizing easy or hard samples lack the flexibility to handle such a varietysimultaneously. Recent work has proposed a more reasonable data selectionprinciple by examining the data's impact on the model's generalization loss.However, its practical adoption relies on less principled approximations andadditional holdout data. This work solves these problems by leveraging alightweight Bayesian treatment and incorporating off-the-shelf zero-shotpredictors built on large-scale pre-trained models. The resulting algorithm isefficient and easy to implement. We perform extensive empirical studies onchallenging benchmarks with considerable data noise and imbalance in the onlinebatch selection scenario, and observe superior training efficiency overcompetitive baselines. Notably, on the challenging WebVision benchmark, ourmethod can achieve similar predictive performance with significantly fewertraining iterations than leading data selection methods.</description><author>Zhijie Deng, Peng Cui, Jun Zhu</author><pubDate>Tue, 07 Nov 2023 15:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10544v3</guid></item><item><title>Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation</title><link>http://arxiv.org/abs/2311.04060v1</link><description>This paper identifies and addresses the problems with naively combining(reinforcement) learning-based controllers and state estimators for roboticin-hand manipulation. Specifically, we tackle the challenging task of purelytactile, goal-conditioned, dextrous in-hand reorientation with the handpointing downwards. Due to the limited sensing available, many controlstrategies that are feasible in simulation when having full knowledge of theobject's state do not allow for accurate state estimation. Hence, separatelytraining the controller and the estimator and combining the two at test timeleads to poor performance. We solve this problem by coupling the control policyto the state estimator already during training in simulation. This approachleads to more robust state estimation and overall higher performance on thetask while maintaining an interpretability advantage over end-to-end policylearning. With our GPU-accelerated implementation, learning from scratch takesa median training time of only 6.5 hours on a single, low-cost GPU. Insimulation experiments with the DLR-Hand II and for four significantlydifferent object shapes, we provide an in-depth analysis of the performance ofour approach. We demonstrate the successful sim2real transfer by rotating thefour objects to all 24 orientations in the $\pi/2$ discretization of SO(3),which has never been achieved for such a diverse set of shapes. Finally, ourmethod can reorient a cube consecutively to nine goals (median), which wasbeyond the reach of previous methods in this challenging setting.</description><author>Lennart Röstel, Johannes Pitz, Leon Sievers, Berthold Bäuml</author><pubDate>Tue, 07 Nov 2023 15:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04060v1</guid></item><item><title>mmFUSION: Multimodal Fusion for 3D Objects Detection</title><link>http://arxiv.org/abs/2311.04058v1</link><description>Multi-sensor fusion is essential for accurate 3D object detection inself-driving systems. Camera and LiDAR are the most commonly used sensors, andusually, their fusion happens at the early or late stages of 3D detectors withthe help of regions of interest (RoIs). On the other hand, fusion at theintermediate level is more adaptive because it does not need RoIs frommodalities but is complex as the features of both modalities are presented fromdifferent points of view. In this paper, we propose a new intermediate-levelmulti-modal fusion (mmFUSION) approach to overcome these challenges. First, themmFUSION uses separate encoders for each modality to compute features at adesired lower space volume. Second, these features are fused throughcross-modality and multi-modality attention mechanisms proposed in mmFUSION.The mmFUSION framework preserves multi-modal information and learns tocomplement modalities' deficiencies through attention weights. The strongmulti-modal features from the mmFUSION framework are fed to a simple 3Ddetection head for 3D predictions. We evaluate mmFUSION on the KITTI andNuScenes dataset where it performs better than available early, intermediate,late, and even two-stage based fusion schemes. The code with the mmdetection3Dproject plugin will be publicly available soon.</description><author>Javed Ahmad, Alessio Del Bue</author><pubDate>Tue, 07 Nov 2023 15:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04058v1</guid></item><item><title>Hierarchical Reinforcement Learning for Automatic Disease Diagnosis</title><link>http://arxiv.org/abs/2004.14254v2</link><description>Motivation: Disease diagnosis oriented dialogue system models the interactiveconsultation procedure as Markov Decision Process and reinforcement learningalgorithms are used to solve the problem. Existing approaches usually employ aflat policy structure that treat all symptoms and diseases equally for actionmaking. This strategy works well in the simple scenario when the action spaceis small, however, its efficiency will be challenged in the real environment.Inspired by the offline consultation process, we propose to integrate ahierarchical policy structure of two levels into the dialogue systemfor policylearning. The high-level policy consists of amastermodel that is responsiblefor triggering a low-levelmodel, the lowlevel policy consists of severalsymptom checkers and a disease classifier. The proposed policy structure iscapable to deal with diagnosis problem including large number of diseases andsymptoms. Results: Experimental results on three real-world datasets and a syntheticdataset demonstrate that our hierarchical framework achieves higher accuracyand symptom recall in disease diagnosis compared with existing systems. Weconstruct a benchmark including datasets and implementation of existingalgorithms to encourage follow-up researches. Availability: The code and data is available fromhttps://github.com/FudanDISC/DISCOpen-MedBox-DialoDiagnosis Contact: 21210980124@m.fudan.edu.cn Supplementary information: Supplementary data are available at Bioinformaticsonline.</description><author>Cheng Zhong, Kangenbei Liao, Wei Chen, Qianlong Liu, Baolin Peng, Xuanjing Huang, Jiajie Peng, Zhongyu Wei</author><pubDate>Tue, 07 Nov 2023 15:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2004.14254v2</guid></item><item><title>Multi-View Causal Representation Learning with Partial Observability</title><link>http://arxiv.org/abs/2311.04056v1</link><description>We present a unified framework for studying the identifiability ofrepresentations learned from simultaneously observed views, such as differentdata modalities. We allow a partially observed setting in which each viewconstitutes a nonlinear mixture of a subset of underlying latent variables,which can be causally related. We prove that the information shared across allsubsets of any number of views can be learned up to a smooth bijection usingcontrastive learning and a single encoder per view. We also provide graphicalcriteria indicating which latent variables can be identified through a simpleset of rules, which we refer to as identifiability algebra. Our generalframework and theoretical results unify and extend several previous works onmulti-view nonlinear ICA, disentanglement, and causal representation learning.We experimentally validate our claims on numerical, image, and multi-modal datasets. Further, we demonstrate that the performance of prior methods isrecovered in different special cases of our setup. Overall, we find that accessto multiple partial views enables us to identify a more fine-grainedrepresentation, under the generally milder assumption of partial observability.</description><author>Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello</author><pubDate>Tue, 07 Nov 2023 15:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04056v1</guid></item><item><title>Feature Space Renormalization for Semi-supervised Learning</title><link>http://arxiv.org/abs/2311.04055v1</link><description>Semi-supervised learning (SSL) has been proven to be a powerful method forleveraging unlabelled data to alleviate models' dependence on large labelleddatasets. The common framework among recent approaches is to train the model ona large amount of unlabelled data with consistency regularization to constrainthe model predictions to be invariant to input perturbation. However, theexisting SSL frameworks still have room for improvement in the consistencyregularization method. Instead of regularizing category predictions in thelabel space as in existing frameworks, this paper proposes a feature spacerenormalization (FSR) mechanism for SSL. First, we propose a feature spacerenormalization mechanism to substitute for the commonly used consistencyregularization mechanism to learn better discriminative features. To apply thismechanism, we start by building a basic model and an empirical model and thenintroduce our mechanism to renormalize the feature learning of the basic modelwith the guidance of the empirical model. Second, we combine the proposedmechanism with pseudo-labelling to obtain a novel effective SSL model namedFreMatch. The experimental results show that our method can achieve betterperformance on a variety of standard SSL benchmark datasets, and the proposedfeature space renormalization mechanism can also enhance the performance ofother SSL approaches.</description><author>Jun Sun, Zhongjie Mao, Chao Li, Chao Zhou, Xiao-Jun Wu</author><pubDate>Tue, 07 Nov 2023 15:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04055v1</guid></item><item><title>inkn'hue: Enhancing Manga Colorization from Multiple Priors with Alignment Multi-Encoder VAE</title><link>http://arxiv.org/abs/2311.01804v2</link><description>Manga, a form of Japanese comics and distinct visual storytelling, hascaptivated readers worldwide. Traditionally presented in black and white,manga's appeal lies in its ability to convey complex narratives and emotionsthrough intricate line art and shading. Yet, the desire to experience manga invibrant colors has sparked the pursuit of manga colorization, a task ofparamount significance for artists. However, existing methods, originallydesigned for line art and sketches, face challenges when applied to manga.These methods often fall short in achieving the desired results, leading to theneed for specialized manga-specific solutions. Existing approaches frequentlyrely on a single training step or extensive manual artist intervention, whichcan yield less satisfactory outcomes. To address these challenges, we propose aspecialized framework for manga colorization. Leveraging established models forshading and vibrant coloring, our approach aligns both using a multi-encoderVAE. This structured workflow ensures clear and colorful results, with theoption to incorporate reference images and manual hints.</description><author>Tawin Jiramahapokee</author><pubDate>Tue, 07 Nov 2023 15:06:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01804v2</guid></item><item><title>Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case</title><link>http://arxiv.org/abs/2208.14960v3</link><description>Gaussian processes are arguably the most important class of spatiotemporalmodels within machine learning. They encode prior information about the modeledfunction and can be used for exact or approximate Bayesian learning. In manyapplications, particularly in physical sciences and engineering, but also inareas such as geostatistics and neuroscience, invariance to symmetries is oneof the most fundamental forms of prior information one can consider. Theinvariance of a Gaussian process' covariance to such symmetries gives rise tothe most natural generalization of the concept of stationarity to such spaces.In this work, we develop constructive and practical techniques for buildingstationary Gaussian processes on a very large class of non-Euclidean spacesarising in the context of symmetries. Our techniques make it possible to (i)calculate covariance kernels and (ii) sample from prior and posterior Gaussianprocesses defined on such spaces, both in a practical manner. This work issplit into two parts, each involving different technical considerations: part Istudies compact spaces, while part II studies non-compact spaces possessingcertain structure. Our contributions make the non-Euclidean Gaussian processmodels we study compatible with well-understood computational techniquesavailable in standard Gaussian process software packages, thereby making themaccessible to practitioners.</description><author>Iskander Azangulov, Andrei Smolensky, Alexander Terenin, Viacheslav Borovitskiy</author><pubDate>Tue, 07 Nov 2023 15:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.14960v3</guid></item><item><title>Generative Structural Design Integrating BIM and Diffusion Model</title><link>http://arxiv.org/abs/2311.04052v1</link><description>Intelligent structural design using AI can effectively reduce time overheadand increase efficiency. It has potential to become the new design paradigm inthe future to assist and even replace engineers, and so it has become aresearch hotspot in the academic community. However, current methods have somelimitations to be addressed, whether in terms of application scope, visualquality of generated results, or evaluation metrics of results. This studyproposes a comprehensive solution. Firstly, we introduce building informationmodeling (BIM) into intelligent structural design and establishes a structuraldesign pipeline integrating BIM and generative AI, which is a powerfulsupplement to the previous frameworks that only considered CAD drawings. Inorder to improve the perceptual quality and details of generations, this studymakes 3 contributions. Firstly, in terms of generation framework, inspired bythe process of human drawing, a novel 2-stage generation framework is proposedto replace the traditional end-to-end framework to reduce the generationdifficulty for AI models. Secondly, in terms of generative AI tools adopted,diffusion models (DMs) are introduced to replace widely used generativeadversarial network (GAN)-based models, and a novel physics-based conditionaldiffusion model (PCDM) is proposed to consider different design prerequisites.Thirdly, in terms of neural networks, an attention block (AB) consisting of aself-attention block (SAB) and a parallel cross-attention block (PCAB) isdesigned to facilitate cross-domain data fusion. The quantitative andqualitative results demonstrate the powerful generation and representationcapabilities of PCDM. Necessary ablation studies are conducted to examine thevalidity of the methods. This study also shows that DMs have the potential toreplace GANs and become the new benchmark for generative problems in civilengineering.</description><author>Zhili He, Yu-Hsing Wang, Jian Zhang</author><pubDate>Tue, 07 Nov 2023 15:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04052v1</guid></item><item><title>3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images</title><link>http://arxiv.org/abs/2311.04049v1</link><description>Automatic prostate segmentation in TRUS images has always been a challengingproblem, since prostates in TRUS images have ambiguous boundaries andinhomogeneous intensity distribution. Although many prostate segmentationmethods have been proposed, they still need to be improved due to the lack ofsensibility to edge information. Consequently, the objective of this study isto devise a highly effective prostate segmentation method that overcomes theselimitations and achieves accurate segmentation of prostates in TRUS images. A3D edge-aware attention generative adversarial network (3D EAGAN)-basedprostate segmentation method is proposed in this paper, which consists of anedge-aware segmentation network (EASNet) that performs the prostatesegmentation and a discriminator network that distinguishes predicted prostatesfrom real prostates. The proposed EASNet is composed of anencoder-decoder-based U-Net backbone network, a detail compensation module,four 3D spatial and channel attention modules, an edge enhance module, and aglobal feature extractor. The detail compensation module is proposed tocompensate for the loss of detailed information caused by the down-samplingprocess of the encoder. The features of the detail compensation module areselectively enhanced by the 3D spatial and channel attention module.Furthermore, an edge enhance module is proposed to guide shallow layers in theEASNet to focus on contour and edge information in prostates. Finally, featuresfrom shallow layers and hierarchical features from the decoder module are fusedthrough the global feature extractor to predict the segmentation prostates.</description><author>Mengqing Liu, Xiao Shao, Liping Jiang, Kaizhi Wu</author><pubDate>Tue, 07 Nov 2023 15:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04049v1</guid></item><item><title>A graph convolutional autoencoder approach to model order reduction for parametrized PDEs</title><link>http://arxiv.org/abs/2305.08573v2</link><description>The present work proposes a framework for nonlinear model order reductionbased on a Graph Convolutional Autoencoder (GCA-ROM). In the reduced ordermodeling (ROM) context, one is interested in obtaining real-time and many-queryevaluations of parametric Partial Differential Equations (PDEs). Lineartechniques such as Proper Orthogonal Decomposition (POD) and Greedy algorithmshave been analyzed thoroughly, but they are more suitable when dealing withlinear and affine models showing a fast decay of the Kolmogorov n-width. On onehand, the autoencoder architecture represents a nonlinear generalization of thePOD compression procedure, allowing one to encode the main information in alatent set of variables while extracting their main features. On the otherhand, Graph Neural Networks (GNNs) constitute a natural framework for studyingPDE solutions defined on unstructured meshes. Here, we develop a non-intrusiveand data-driven nonlinear reduction approach, exploiting GNNs to encode thereduced manifold and enable fast evaluations of parametrized PDEs. We show thecapabilities of the methodology for several models: linear/nonlinear andscalar/vector problems with fast/slow decay in the physically and geometricallyparametrized setting. The main properties of our approach consist of (i) highgeneralizability in the low-data regime even for complex regimes, (ii) physicalcompliance with general unstructured grids, and (iii) exploitation of poolingand un-pooling operations to learn from scattered data.</description><author>Federico Pichi, Beatriz Moya, Jan S. Hesthaven</author><pubDate>Tue, 07 Nov 2023 15:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08573v2</guid></item><item><title>Extracting human interpretable structure-property relationships in chemistry using XAI and large language models</title><link>http://arxiv.org/abs/2311.04047v1</link><description>Explainable Artificial Intelligence (XAI) is an emerging field in AI thataims to address the opaque nature of machine learning models. Furthermore, ithas been shown that XAI can be used to extract input-output relationships,making them a useful tool in chemistry to understand structure-propertyrelationships. However, one of the main limitations of XAI methods is that theyare developed for technically oriented users. We propose the XpertAI frameworkthat integrates XAI methods with large language models (LLMs) accessingscientific literature to generate accessible natural language explanations ofraw chemical data automatically. We conducted 5 case studies to evaluate theperformance of XpertAI. Our results show that XpertAI combines the strengths ofLLMs and XAI tools in generating specific, scientific, and interpretableexplanations.</description><author>Geemi P. Wellawatte, Philippe Schwaller</author><pubDate>Tue, 07 Nov 2023 15:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04047v1</guid></item><item><title>Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features</title><link>http://arxiv.org/abs/2311.04046v1</link><description>Many capable large language models (LLMs) are developed via self-supervisedpre-training followed by a reinforcement-learning fine-tuning phase, oftenbased on human or AI feedback. During this stage, models may be guided by theirinductive biases to rely on simpler features which may be easier to extract, ata cost to robustness and generalisation. We investigate whether principlesgoverning inductive biases in the supervised fine-tuning of LLMs also applywhen the fine-tuning process uses reinforcement learning. Following Lovering etal (2021), we test two hypotheses: that features more $\textit{extractable}$after pre-training are more likely to be utilised by the final policy, and thatthe evidence for/against a feature predicts whether it will be utilised.Through controlled experiments on synthetic and natural language tasks, we findstatistically significant correlations which constitute strong evidence forthese hypotheses.</description><author>Diogo Cruz, Edoardo Pona, Alex Holness-Tofts, Elias Schmied, Víctor Abia Alonso, Charlie Griffin, Bogdan-Ionut Cirstea</author><pubDate>Tue, 07 Nov 2023 15:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04046v1</guid></item><item><title>Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery</title><link>http://arxiv.org/abs/2310.19109v2</link><description>This paper explores post-disaster analytics using multimodal deep learningmodels trained with curriculum learning method. Studying post-disasteranalytics is important as it plays a crucial role in mitigating the impact ofdisasters by providing timely and accurate insights into the extent of damageand the allocation of resources. We propose a curriculum learning strategy toenhance the performance of multimodal deep learning models. Curriculum learningemulates the progressive learning sequence in human education by training deeplearning models on increasingly complex data. Our primary objective is todevelop a curriculum-trained multimodal deep learning model, with a particularfocus on visual question answering (VQA) capable of jointly processing imageand text data, in conjunction with semantic segmentation for disaster analyticsusing theFloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021}dataset. To achieve this, U-Net model is used for semantic segmentation andimage encoding. A custom built text classifier is used for visual questionanswering. Existing curriculum learning methods rely on manually defineddifficulty functions. We introduce a novel curriculum learning approach termedDynamic Task and Weight Prioritization (DATWEP), which leverages agradient-based method to automatically decide task difficulty during curriculumlearning training, thereby eliminating the need for explicit difficultycomputation. The integration of DATWEP into our multimodal model showsimprovement on VQA performance. Source code is available athttps://github.com/fualsan/DATWEP.</description><author>Huseyin Fuat Alsan, Taner Arsan</author><pubDate>Tue, 07 Nov 2023 14:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19109v2</guid></item><item><title>P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models</title><link>http://arxiv.org/abs/2311.04044v1</link><description>The rapid development of language models (LMs) brings unprecedentedaccessibility and usage for both models and users. On the one hand, powerfulLMs, trained with massive textual data, achieve state-of-the-art performanceover numerous downstream NLP tasks. On the other hand, more and more attentionis paid to unrestricted model accesses that may bring malicious privacy risksof data leakage. To address these issues, many recent works proposeprivacy-preserving language models (PPLMs) with differential privacy (DP).Unfortunately, different DP implementations make it challenging for a faircomparison among existing PPLMs. In this paper, we present P-Bench, amulti-perspective privacy evaluation benchmark to empirically and intuitivelyquantify the privacy leakage of LMs. Instead of only protecting and measuringthe privacy of protected data with DP parameters, P-Bench sheds light on theneglected inference data privacy during actual usage. P-Bench first clearlydefines multi-faceted privacy objectives during private fine-tuning. Then,P-Bench constructs a unified pipeline to perform private fine-tuning. Lastly,P-Bench performs existing privacy attacks on LMs with pre-defined privacyobjectives as the empirical evaluation results. The empirical attack resultsare used to fairly and intuitively evaluate the privacy leakage of variousPPLMs. We conduct extensive experiments on three datasets of GLUE formainstream LMs.</description><author>Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yangqiu Song</author><pubDate>Tue, 07 Nov 2023 14:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04044v1</guid></item><item><title>Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios</title><link>http://arxiv.org/abs/2311.04042v1</link><description>Based on previous work, we assess the use of NIR-HSI images for calibratingmodels on two datasets, focusing on protein content regression and grainvariety classification. Limited reference data for protein content is expandedby subsampling and associating it with the bulk sample. However, this methodintroduces significant biases due to skewed leptokurtic predictiondistributions, affecting both PLS-R and deep CNN models. We propose adjustmentsto mitigate these biases, improving mean protein reference predictions.Additionally, we investigate the impact of grain-to-background ratios on bothtasks. Higher ratios yield more accurate predictions, but including lower-ratioimages in calibration enhances model robustness for such scenarios.</description><author>Ole-Christian Galbo Engstrøm, Erik Schou Dreier, Birthe Møller Jespersen, Kim Steenstrup Pedersen</author><pubDate>Tue, 07 Nov 2023 14:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04042v1</guid></item><item><title>Hilbert's projective metric for functions of bounded growth and exponential convergence of Sinkhorn's algorithm</title><link>http://arxiv.org/abs/2311.04041v1</link><description>We study versions of Hilbert's projective metric for spaces of integrablefunctions of bounded growth. These metrics originate from cones which arerelaxations of the cone of all non-negative functions, in the sense that theyinclude all functions having non-negative integral values when multiplied withcertain test functions. We show that kernel integral operators are contractionswith respect to suitable specifications of such metrics even for kernels whichare not bounded away from zero, provided that the decay to zero of the kernelis controlled. As an application to entropic optimal transport, we showexponential convergence of Sinkhorn's algorithm in settings where the marginaldistributions have sufficiently light tails compared to the growth of the costfunction.</description><author>Stephan Eckstein</author><pubDate>Tue, 07 Nov 2023 14:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04041v1</guid></item><item><title>Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data</title><link>http://arxiv.org/abs/2311.04040v1</link><description>Multi-task partially annotated data where each data point is annotated foronly a single task are potentially helpful for data scarcity if a network canleverage the inter-task relationship. In this paper, we study the jointlearning of object detection and semantic segmentation, the two most popularvision problems, from multi-task data with partial annotations. Extensiveexperiments are performed to evaluate each task performance and explore theircomplementarity when a multi-task network cannot optimize both taskssimultaneously. We propose employing knowledge distillation to leveragejoint-task optimization. The experimental results show favorable results formulti-task learning and knowledge distillation over single-task learning andeven full supervision scenario. All code and data splits are available athttps://github.com/lhoangan/multas</description><author>Hoàng-Ân Lê, Minh-Tan Pham</author><pubDate>Tue, 07 Nov 2023 14:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04040v1</guid></item><item><title>Causal Discovery Under Local Privacy</title><link>http://arxiv.org/abs/2311.04037v1</link><description>Differential privacy is a widely adopted framework designed to safeguard thesensitive information of data providers within a data set. It is based on theapplication of controlled noise at the interface between the server that storesand processes the data, and the data consumers. Local differential privacy is avariant that allows data providers to apply the privatization mechanismthemselves on their data individually. Therefore it provides protection also incontexts in which the server, or even the data collector, cannot be trusted.The introduction of noise, however, inevitably affects the utility of the data,particularly by distorting the correlations between individual data components.This distortion can prove detrimental to tasks such as causal discovery. Inthis paper, we consider various well-known locally differentially privatemechanisms and compare the trade-off between the privacy they provide, and theaccuracy of the causal structure produced by algorithms for causal learningwhen applied to data obfuscated by these mechanisms. Our analysis yieldsvaluable insights for selecting appropriate local differentially privateprotocols for causal discovery tasks. We foresee that our findings will aidresearchers and practitioners in conducting locally private causal discovery.</description><author>Rūta Binkytė, Carlos Pinzón, Szilvia Lestyán, Kangsoo Jung, Héber H. Arcolezi, Catuscia Palamidessi</author><pubDate>Tue, 07 Nov 2023 14:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04037v1</guid></item><item><title>Discordance Minimization-based Imputation Algorithms for Missing Values in Rating Data</title><link>http://arxiv.org/abs/2311.04035v1</link><description>Ratings are frequently used to evaluate and compare subjects in variousapplications, from education to healthcare, because ratings provide succinctyet credible measures for comparing subjects. However, when multiple ratinglists are combined or considered together, subjects often have missing ratings,because most rating lists do not rate every subject in the combined list. Inthis study, we propose analyses on missing value patterns using six real-worlddata sets in various applications, as well as the conditions for applicabilityof imputation algorithms. Based on the special structures and propertiesderived from the analyses, we propose optimization models and algorithms thatminimize the total rating discordance across rating providers to impute missingratings in the combined rating lists, using only the known rating information.The total rating discordance is defined as the sum of the pairwise discordancemetric, which can be written as a quadratic function. Computational experimentsbased on real-world and synthetic rating data sets show that the proposedmethods outperform the state-of-the-art general imputation methods in theliterature in terms of imputation accuracy.</description><author>Young Woong Park, Jinhak Kim, Dan Zhu</author><pubDate>Tue, 07 Nov 2023 14:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04035v1</guid></item></channel></rss>