<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 01 Feb 2024 06:01:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators</title><link>http://arxiv.org/abs/2401.18085v1</link><description>Diffusion models are capable of generating impressive images conditioned ontext descriptions, and extensions of these models allow users to edit images ata relatively coarse scale. However, the ability to precisely edit the layout,position, pose, and shape of objects in images with diffusion models is stilldifficult. To this end, we propose motion guidance, a zero-shot technique thatallows a user to specify dense, complex motion fields that indicate where eachpixel in an image should move. Motion guidance works by steering the diffusionsampling process with the gradients through an off-the-shelf optical flownetwork. Specifically, we design a guidance loss that encourages the sample tohave the desired motion, as estimated by a flow network, while also beingvisually similar to the source image. By simultaneously sampling from adiffusion model and guiding the sample to have low guidance loss, we can obtaina motion-edited image. We demonstrate that our technique works on complexmotions and produces high quality edits of real and generated images.</description><author>Daniel Geng, Andrew Owens</author><pubDate>Wed, 31 Jan 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18085v1</guid></item><item><title>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</title><link>http://arxiv.org/abs/2401.18084v1</link><description>The ability to associate touch with other modalities has huge implicationsfor humans and computational systems. However, multimodal learning with touchremains challenging due to the expensive data collection process andnon-standardized sensor outputs. We introduce UniTouch, a unified tactile modelfor vision-based touch sensors connected to multiple modalities, includingvision, language, and sound. We achieve this by aligning our UniTouchembeddings to pretrained image embeddings already associated with a variety ofother modalities. We further propose learnable sensor-specific tokens, allowingthe model to learn from a set of heterogeneous tactile sensors, all at the sametime. UniTouch is capable of conducting various touch sensing tasks in thezero-shot setting, from robot grasping prediction to touch image questionanswering. To the best of our knowledge, UniTouch is the first to demonstratesuch capabilities. Project page: https://cfeng16.github.io/UniTouch/</description><author>Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong</author><pubDate>Wed, 31 Jan 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18084v1</guid></item><item><title>Improved Scene Landmark Detection for Camera Localization</title><link>http://arxiv.org/abs/2401.18083v1</link><description>Camera localization methods based on retrieval, local feature matching, and3D structure-based pose estimation are accurate but require high storage, areslow, and are not privacy-preserving. A method based on scene landmarkdetection (SLD) was recently proposed to address these limitations. It involvestraining a convolutional neural network (CNN) to detect a few predetermined,salient, scene-specific 3D points or landmarks and computing camera pose fromthe associated 2D-3D correspondences. Although SLD outperformed existinglearning-based approaches, it was notably less accurate than 3D structure-basedmethods. In this paper, we show that the accuracy gap was due to insufficientmodel capacity and noisy labels during training. To mitigate the capacityissue, we propose to split the landmarks into subgroups and train a separatenetwork for each subgroup. To generate better training labels, we propose usingdense reconstructions to estimate visibility of scene landmarks. Finally, wepresent a compact architecture to improve memory efficiency. Accuracy wise, ourapproach is on par with state of the art structure based methods on theINDOOR-6 dataset but runs significantly faster and uses less storage. Code andmodels can be found at https://github.com/microsoft/SceneLandmarkLocalization.</description><author>Tien Do, Sudipta N. Sinha</author><pubDate>Wed, 31 Jan 2024 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18083v1</guid></item><item><title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title><link>http://arxiv.org/abs/2401.18079v1</link><description>LLMs are seeing growing use for applications such as document analysis andsummarization which require large context windows, and with these large contextwindows KV cache activations surface as the dominant contributor to memoryconsumption during inference. Quantization is a promising approach forcompressing KV cache activations; however, existing solutions fail to representactivations accurately in ultra-low precisions, such as sub-4-bit. In thiswork, we present KVQuant, which addresses this problem by incorporating novelmethods for quantizing cached KV activations, including: (i) Per-Channel KeyQuantization, where we adjust the dimension along which we quantize the Keyactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,where we quantize Key activations before the rotary positional embedding tomitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,where we derive per-layer sensitivity-weighted non-uniform datatypes thatbetter represent the distributions; (iv) Per-Vector Dense-and-SparseQuantization, where we isolate outliers separately for each vector to minimizeskews in quantization ranges; and (v) Q-Norm, where we normalize quantizationcentroids in order to mitigate distribution shift, providing additionalbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,and Mistral models, we achieve $&lt;0.1$ perplexity degradation with 3-bitquantization on both Wikitext-2 and C4, outperforming existing approaches. Ourmethod enables serving the LLaMA-7B model with a context length of up to 1million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.</description><author>Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami</author><pubDate>Wed, 31 Jan 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18079v1</guid></item><item><title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title><link>http://arxiv.org/abs/2401.18075v1</link><description>We propose CARFF: Conditional Auto-encoded Radiance Field for 3D SceneForecasting, a method for predicting future 3D scenes given past observations,such as 2D ego-centric images. Our method maps an image to a distribution overplausible 3D latent scene configurations using a probabilistic encoder, andpredicts the evolution of the hypothesized scenes through time. Our latentscene representation conditions a global Neural Radiance Field (NeRF) torepresent a 3D scene model, which enables explainable predictions andstraightforward downstream applications. This approach extends beyond previousneural rendering work by considering complex scenarios of uncertainty inenvironmental states and dynamics. We employ a two-stage training ofPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, weauto-regressively predict latent scene representations as a partiallyobservable Markov decision process, utilizing a mixture density network. Wedemonstrate the utility of our method in realistic scenarios using the CARLAdriving simulator, where CARFF can be used to enable efficient trajectory andcontingency planning in complex multi-agent autonomous driving scenariosinvolving visual occlusions.</description><author>Jiezhi Yang, Khushi Desai, Charles Packer, Harshil Bhatia, Nicholas Rhinehart, Rowan McAllister, Joseph Gonzalez</author><pubDate>Wed, 31 Jan 2024 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18075v1</guid></item><item><title>High-Quality Image Restoration Following Human Instructions</title><link>http://arxiv.org/abs/2401.16468v2</link><description>Image restoration is a fundamental problem that involves recovering ahigh-quality clean image from its degraded observation. All-In-One imagerestoration models can effectively restore images from various types and levelsof degradation using degradation-specific information as prompts to guide therestoration model. In this work, we present the first approach that useshuman-written instructions to guide the image restoration model. Given naturallanguage prompts, our model can recover high-quality images from their degradedcounterparts, considering multiple degradation types. Our method, InstructIR,achieves state-of-the-art results on several restoration tasks including imagedenoising, deraining, deblurring, dehazing, and (low-light) image enhancement.InstructIR improves +1dB over previous all-in-one restoration methods.Moreover, our dataset and results represent a novel benchmark for new researchon text-guided image restoration and enhancement. Our code, datasets and modelsare available at: https://github.com/mv-lab/InstructIR</description><author>Marcos V. Conde, Gregor Geigle, Radu Timofte</author><pubDate>Wed, 31 Jan 2024 18:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16468v2</guid></item><item><title>Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?</title><link>http://arxiv.org/abs/2401.18070v1</link><description>There is increasing interest in employing large language models (LLMs) ascognitive models. For such purposes, it is central to understand whichcognitive properties are well-modeled by LLMs, and which are not. In this work,we study the biases of LLMs in relation to those known in children when solvingarithmetic word problems. Surveying the learning science literature, we positthat the problem-solving process can be split into three distinct steps: textcomprehension, solution planning and solution execution. We construct tests foreach one in order to understand which parts of this process can be faithfullymodeled by current state-of-the-art LLMs. We generate a novel set of wordproblems for each of these tests, using a neuro-symbolic method that enablesfine-grained control over the problem features. We find evidence that LLMs,with and without instruction-tuning, exhibit human-like biases in both thetext-comprehension and the solution-planning steps of the solving process, butnot during the final step which relies on the problem's arithmetic expressions(solution execution).</description><author>Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan</author><pubDate>Wed, 31 Jan 2024 18:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18070v1</guid></item><item><title>Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models</title><link>http://arxiv.org/abs/2312.06712v2</link><description>Despite recent significant strides achieved by diffusion-based Text-to-Image(T2I) models, current systems are still less capable of ensuring decentcompositional generation aligned with text prompts, particularly for themulti-object generation. This work illuminates the fundamental reasons for suchmisalignment, pinpointing issues related to low attention activation scores andmask overlaps. While previous research efforts have individually tackled theseissues, we assert that a holistic approach is paramount. Thus, we propose twonovel objectives, the Separate loss and the Enhance loss, that reduce objectmask overlaps and maximize attention scores, respectively. Our method divergesfrom conventional test-time-adaptation techniques, focusing on finetuningcritical parameters, which enhances scalability and generalizability.Comprehensive evaluations demonstrate the superior performance of our model interms of image realism, text-image alignment, and adaptability, notablyoutperforming prominent baselines. Ultimately, this research paves the way forT2I diffusion models with enhanced compositional capacities and broaderapplicability.</description><author>Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, Martial Hebert</author><pubDate>Wed, 31 Jan 2024 18:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06712v2</guid></item><item><title>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</title><link>http://arxiv.org/abs/2401.18059v1</link><description>Retrieval-augmented language models can better adapt to changes in worldstate and incorporate long-tail knowledge. However, most existing methodsretrieve only short contiguous chunks from a retrieval corpus, limitingholistic understanding of the overall document context. We introduce the novelapproach of recursively embedding, clustering, and summarizing chunks of text,constructing a tree with differing levels of summarization from the bottom up.At inference time, our RAPTOR model retrieves from this tree, integratinginformation across lengthy documents at different levels of abstraction.Controlled experiments show that retrieval with recursive summaries offerssignificant improvements over traditional retrieval-augmented LMs on severaltasks. On question-answering tasks that involve complex, multi-step reasoning,we show state-of-the-art results; for example, by coupling RAPTOR retrievalwith the use of GPT-4, we can improve the best performance on the QuALITYbenchmark by 20% in absolute accuracy.</description><author>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning</author><pubDate>Wed, 31 Jan 2024 18:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18059v1</guid></item><item><title>LongAlign: A Recipe for Long Context Alignment of Large Language Models</title><link>http://arxiv.org/abs/2401.18058v1</link><description>Extending large language models to effectively handle long contexts requiresinstruction fine-tuning on input sequences of similar length. To address this,we present LongAlign -- a recipe of the instruction data, training, andevaluation for long context alignment. First, we construct a longinstruction-following dataset using Self-Instruct. To ensure the datadiversity, it covers a broad range of tasks from various long context sources.Second, we adopt the packing and sorted batching strategies to speed upsupervised fine-tuning on data with varied length distributions. Additionally,we develop a loss weighting method to balance the contribution to the lossacross different sequences during packing training. Third, we introduce theLongBench-Chat benchmark for evaluating instruction-following capabilities onqueries of 10k-100k in length. Experiments show that LongAlign outperformsexisting recipes for LLMs in long context tasks by up to 30\%, while alsomaintaining their proficiency in handling short, generic tasks. The code, data,and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.</description><author>Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li</author><pubDate>Wed, 31 Jan 2024 18:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18058v1</guid></item><item><title>Rank Supervised Contrastive Learning for Time Series Classification</title><link>http://arxiv.org/abs/2401.18057v1</link><description>Recently, various contrastive learning techniques have been developed tocategorize time series data and exhibit promising performance. A generalparadigm is to utilize appropriate augmentations and construct feasiblepositive samples such that the encoder can yield robust and discriminativerepresentations by mapping similar data points closer together in the featurespace while pushing dissimilar data points farther apart. Despite its efficacy,the fine-grained relative similarity (e.g., rank) information of positivesamples is largely ignored, especially when labeled samples are limited. Tothis end, we present Rank Supervised Contrastive Learning (RankSCL) to performtime series classification. Different from conventional contrastive learningframeworks, RankSCL augments raw data in a targeted way in the embedding spaceand adopts certain filtering rules to select more informative positive andnegative pairs of samples. Moreover, a novel rank loss is developed to assigndifferent weights for different levels of positive samples, enable the encoderto extract the fine-grained information of the same class, and produce a clearboundary among different classes. Thoroughly empirical studies on 128 UCRdatasets and 30 UEA datasets demonstrate that the proposed RankSCL can achievestate-of-the-art performance compared to existing baseline methods.</description><author>Qianying Ren, Dongsheng Luo, Dongjin Song</author><pubDate>Wed, 31 Jan 2024 18:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18057v1</guid></item><item><title>Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2401.18054v1</link><description>Continual learning (CL) is the research field that aims to build machinelearning models that can accumulate knowledge continuously over different taskswithout retraining from scratch. Previous studies have shown that pre-traininggraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)after fine-tuning, a setting which is closely related to CL. Thus, we focus onstudying GNN in the continual graph learning (CGL) setting. We propose thefirst continual graph learning benchmark for spatio-temporal graphs and use itto benchmark well-known CGL methods in this novel setting. The benchmark isbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based actionrecognition. Beyond benchmarking for standard performance metrics, we study theclass and task-order sensitivity of CGL methods, i.e., the impact of learningorder on each class/task's performance, and the architectural sensitivity ofCGL methods with backbone GNN at various widths and depths. We reveal thattask-order robust methods can still be class-order sensitive and observeresults that contradict previous empirical observations on architecturalsensitivity in CL.</description><author>Wei Wei, Tom De Schepper, Kevin Mets</author><pubDate>Wed, 31 Jan 2024 18:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18054v1</guid></item><item><title>Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior</title><link>http://arxiv.org/abs/2401.06550v2</link><description>Urban area-of-interest (AOI) refers to an integrated urban functional zonewith defined boundaries. The rapid development of urban commerce has resultedin an increased demand for more precise requirements in defining AOIs. However,existing research primarily concentrates on broad AOI mining for urban planningor regional economic analysis, failing to cater to the precise requirements ofmobile Internet online-to-offline businesses. These businesses necessitateaccuracy down to a specific community, school, or hospital. In this paper, wepropose an end-to-end multimodal deep learning algorithm for detecting AOIfence polygon using remote sensing images and multi-semantics referenceinformation. We then evaluate its timeliness through a cascaded module thatincorporates dynamic human mobility and logistics address information.Specifically, we begin by selecting a point-of-interest (POI) of specificcategory, and use it to recall corresponding remote sensing images, nearbyPOIs, road nodes, human mobility, and logistics addresses to build a multimodaldetection model based on transformer encoder-decoder architecture, titledAOITR. In the model, in addition to the remote sensing images, multi-semanticinformation including core POI and road nodes is embedded and reorganized asthe query content part for the transformer decoder to generate the AOI polygon.Meanwhile, relatively dynamic distribution features of human mobility, nearbyPOIs, and logistics addresses are used for AOI reliability evaluation through acascaded feedforward network. The experimental results demonstrate that ouralgorithm significantly outperforms two existing methods.</description><author>Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Xin Guo, Qiqi Zhu</author><pubDate>Wed, 31 Jan 2024 18:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06550v2</guid></item><item><title>Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning</title><link>http://arxiv.org/abs/2401.18047v1</link><description>Epidemiological models are best suitable to model an epidemic if the spreadpattern is stationary. To deal with non-stationary patterns and multiple wavesof an epidemic, we develop a hybrid model encompassing epidemic modeling,particle swarm optimization, and deep learning. The model mainly caters tothree objectives for better prediction: 1. Periodic estimation of the modelparameters. 2. Incorporating impact of all the aspects using data fitting andparameter optimization 3. Deep learning based prediction of the modelparameters. In our model, we use a system of ordinary differential equations(ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling,Particle Swarm Optimization (PSO) for model parameter optimization, andstacked-LSTM for forecasting the model parameters. Initial or one timeestimation of model parameters is not able to model multiple waves of anepidemic. So, we estimate the model parameters periodically (weekly). We usePSO to identify the optimum values of the model parameters. We next train thestacked-LSTM on the optimized parameters, and perform forecasting of the modelparameters for upcoming four weeks. Further, we fed the LSTM forecastedparameters into the SIRD model to forecast the number of COVID-19 cases. Weevaluate the model for highly affected three countries namely; the USA, India,and the UK. The proposed hybrid model is able to deal with multiple waves, andhas outperformed existing methods on all the three datasets.</description><author>Naresh Kumar, Seba Susan</author><pubDate>Wed, 31 Jan 2024 18:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18047v1</guid></item><item><title>Multipath parsing in the brain</title><link>http://arxiv.org/abs/2401.18046v1</link><description>Humans understand sentences word-by-word, in the order that they hear them.This incrementality entails resolving temporary ambiguities about syntacticrelationships. We investigate how humans process these syntactic ambiguities bycorrelating predictions from incremental generative dependency parsers withtimecourse data from people undergoing functional neuroimaging while listeningto an audiobook. In particular, we compare competing hypotheses regarding thenumber of developing syntactic analyses in play during word-by-wordcomprehension: one vs more than one. This comparison involves evaluatingsyntactic surprisal from a state-of-the-art dependency parser with LLM-adaptedencodings against an existing fMRI dataset. In both English and Chinese data,we find evidence for multipath parsing. Brain regions associated with thismultipath effect include bilateral superior temporal gyrus.</description><author>Berta Franzluebbers, Donald Dunagan, Miloš Stanojević, Jan Buys, John T. Hale</author><pubDate>Wed, 31 Jan 2024 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18046v1</guid></item><item><title>SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition</title><link>http://arxiv.org/abs/2401.18045v1</link><description>Recent advancements in language models have significantly enhancedperformance in multiple speech-related tasks. Existing speech language modelstypically utilize task-dependent prompt tokens to unify various speech tasks ina single model. However, this design omits the intrinsic connections betweendifferent speech tasks, which can potentially boost the performance of eachtask. In this work, we propose a novel decoder-only speech language model,SpeechComposer, that can unify common speech tasks by composing a fixed set ofprompt tokens. Built upon four primary tasks -- speech synthesis, speechrecognition, speech language modeling, and text language modeling --SpeechComposer can easily extend to more speech tasks via compositions ofwell-designed prompt tokens, like voice conversion and speech enhancement. Theunification of prompt tokens also makes it possible for knowledge sharing amongdifferent speech tasks in a more structured manner. Experimental resultsdemonstrate that our proposed SpeechComposer can improve the performance ofboth primary tasks and composite tasks, showing the effectiveness of the sharedprompt tokens. Remarkably, the unified decoder-only model achieves a comparableand even better performance than the baselines which are expert models designedfor single tasks.</description><author>Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue Wang, Xihua Wang, Shinji Watanabe, Ruihua Song</author><pubDate>Wed, 31 Jan 2024 18:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18045v1</guid></item><item><title>Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images</title><link>http://arxiv.org/abs/2311.06643v2</link><description>Federated learning (FL) is gaining increasing popularity in the medicaldomain for analyzing medical images, which is considered an effective techniqueto safeguard sensitive patient data and comply with privacy regulations.However, several recent studies have revealed that the default settings of FLmay leak private training data under privacy attacks. Thus, it is still unclearwhether and to what extent such privacy risks of FL exist in the medicaldomain, and if so, "how to mitigate such risks?". In this paper, first, wepropose a holistic framework for Medical data Privacy risk analysis andmitigation in Federated Learning (MedPFL) to analyze privacy risks and developeffective mitigation strategies in FL for protecting private medical data.Second, we demonstrate the substantial privacy risks of using FL to processmedical images, where adversaries can easily perform privacy attacks toreconstruct private medical images accurately. Third, we show that the defenseapproach of adding random noises may not always work effectively to protectmedical images against privacy attacks in FL, which poses unique and pressingchallenges associated with medical data for privacy protection.</description><author>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</author><pubDate>Wed, 31 Jan 2024 18:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06643v2</guid></item><item><title>Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability</title><link>http://arxiv.org/abs/2401.18040v1</link><description>End-to-end multi-task dialogue systems are usually designed with separatemodules for the dialogue pipeline. Among these, the policy module is essentialfor deciding what to do in response to user input. This policy is trained byreinforcement learning algorithms by taking advantage of an environment inwhich an agent receives feedback in the form of a reward signal. The currentdialogue systems, however, only provide meagre and simplistic rewards.Investigating intrinsic motivation reinforcement learning algorithms is thegoal of this study. Through this, the agent can quickly accelerate training andimprove its capacity to judge the quality of its actions by teaching it aninternal incentive system. In particular, we adapt techniques for randomnetwork distillation and curiosity-driven reinforcement learning to measure thefrequency of state visits and encourage exploration by using semanticsimilarity between utterances. Experimental results on MultiWOZ, aheterogeneous dataset, show that intrinsic motivation-based debate systemsoutperform policies that depend on extrinsic incentives. By adopting randomnetwork distillation, for example, which is trained using semantic similaritybetween user-system dialogues, an astounding average success rate of 73% isachieved. This is a significant improvement over the baseline Proximal PolicyOptimization (PPO), which has an average success rate of 60%. In addition,performance indicators such as booking rates and completion rates show a 10%rise over the baseline. Furthermore, these intrinsic incentive models helpimprove the system's policy's resilience in an increasing amount of domains.This implies that they could be useful in scaling up to settings that cover awider range of domains.</description><author>Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri, Sujatha Alla Old Dominion</author><pubDate>Wed, 31 Jan 2024 18:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18040v1</guid></item><item><title>Variable selection for Naïve Bayes classification</title><link>http://arxiv.org/abs/2401.18039v1</link><description>The Na\"ive Bayes has proven to be a tractable and efficient method forclassification in multivariate analysis. However, features are usuallycorrelated, a fact that violates the Na\"ive Bayes' assumption of conditionalindependence, and may deteriorate the method's performance. Moreover, datasetsare often characterized by a large number of features, which may complicate theinterpretation of the results as well as slow down the method's execution. In this paper we propose a sparse version of the Na\"ive Bayes classifierthat is characterized by three properties. First, the sparsity is achievedtaking into account the correlation structure of the covariates. Second,different performance measures can be used to guide the selection of features.Third, performance constraints on groups of higher interest can be included.Our proposal leads to a smart search, which yields competitive running times,whereas the flexibility in terms of performance measure for classification isintegrated. Our findings show that, when compared against well-referencedfeature selection approaches, the proposed sparse Na\"ive Bayes obtainscompetitive results regarding accuracy, sparsity and running times for balanceddatasets. In the case of datasets with unbalanced (or with differentimportance) classes, a better compromise between classification rates for thedifferent classes is achieved.</description><author>Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo, M. Remedios Sillero-Denamiel</author><pubDate>Wed, 31 Jan 2024 18:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18039v1</guid></item><item><title>Optimizing contrastive learning for cortical folding pattern detection</title><link>http://arxiv.org/abs/2401.18035v1</link><description>The human cerebral cortex has many bumps and grooves called gyri and sulci.Even though there is a high inter-individual consistency for the main corticalfolds, this is not the case when we examine the exact shapes and details of thefolding patterns. Because of this complexity, characterizing the corticalfolding variability and relating them to subjects' behavioral characteristicsor pathologies is still an open scientific problem. Classical approachesinclude labeling a few specific patterns, either manually orsemi-automatically, based on geometric distances, but the recent availabilityof MRI image datasets of tens of thousands of subjects makes moderndeep-learning techniques particularly attractive. Here, we build aself-supervised deep-learning model to detect folding patterns in the cingulateregion. We train a contrastive self-supervised model (SimCLR) on both HumanConnectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets withtopological-based augmentations on the cortical skeletons, which aretopological objects that capture the shape of the folds. We explore severalbackbone architectures (convolutional network, DenseNet, and PointNet) for theSimCLR. For evaluation and testing, we perform a linear classification task ona database manually labeled for the presence of the "double-parallel" foldingpattern in the cingulate region, which is related to schizophreniacharacteristics. The best model, giving a test AUC of 0.76, is a convolutionalnetwork with 6 layers, a 10-dimensional latent space, a linear projection head,and using the branch-clipping augmentation. This is the first time that aself-supervised deep learning model has been applied to cortical skeletons onsuch a large dataset and quantitatively evaluated. We can now envisage the nextstep: applying it to other brain regions to detect other biomarkers.</description><author>Aymeric Gaudin, Louise Guillon, Clara Fischer, Arnaud Cachia, Denis Rivière, Jean-François Mangin, Joël Chavas</author><pubDate>Wed, 31 Jan 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18035v1</guid></item><item><title>Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models</title><link>http://arxiv.org/abs/2401.18034v1</link><description>We present Gyan AI Paramanu ("atom"), a family of novel language models forIndian languages. It is a collection of auto-regressive monolingual, bilingual,and multilingual Indic language models pretrained from scratch on a single GPUfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models arepretrained with a context size of 1024 on a single GPU. The models are veryefficient, small, fast, and powerful. We have also developed an efficient mostadvanced Indic tokenizer that can even tokenize unseen languages. In order toavoid the "curse of multi-linguality" in our multilingual mParamanu model, wepretrained on comparable corpora by typological grouping using the same script.We performed human evaluation of our pretrained models for open end textgeneration on grammar, coherence, creativity, and factuality metrics forBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit modelsoutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despitebeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To runinference on our pretrained models, CPU is enough, and GPU is not needed. Wealso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugumodels on 23k instructions in respective languages. Our pretrained andinstruction-tuned models which are first of its kind, most powerful efficientsmall generative language models ever developed for Indic languages, and thevarious results lead to the conclusion that high quality generative languagemodels are possible without high amount of compute power and humongous numberof parameters. We plan to release our models at https://www.bharatgpts.com.</description><author>Mitodru Niyogi, Arnab Bhattacharya</author><pubDate>Wed, 31 Jan 2024 17:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18034v1</guid></item><item><title>Deep Network Approximation: Beyond ReLU to Diverse Activation Functions</title><link>http://arxiv.org/abs/2307.06555v5</link><description>This paper explores the expressive power of deep neural networks for adiverse range of activation functions. An activation function set $\mathscr{A}$is defined to encompass the majority of commonly used activation functions,such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$,$\mathtt{ELU}$, $\mathtt{CELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$,$\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$,$\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$,$\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activationfunction $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ anddepth $L$ can be approximated to arbitrary precision by a $\varrho$-activatednetwork of width $3N$ and depth $2L$ on any bounded set. This finding enablesthe extension of most approximation results achieved with $\mathtt{ReLU}$networks to a wide variety of other activation functions, albeit with slightlyincreased constants. Significantly, we establish that the (width,$\,$depth)scaling factors can be further reduced from $(3,2)$ to $(1,1)$ if $\varrho$falls within a specific subset of $\mathscr{A}$. This subset includesactivation functions such as $\mathtt{ELU}$, $\mathtt{CELU}$, $\mathtt{SELU}$,$\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, and$\mathtt{Mish}$.</description><author>Shijun Zhang, Jianfeng Lu, Hongkai Zhao</author><pubDate>Wed, 31 Jan 2024 17:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06555v5</guid></item><item><title>DROP: Decouple Re-Identification and Human Parsing with Task-specific Features for Occluded Person Re-identification</title><link>http://arxiv.org/abs/2401.18032v1</link><description>The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP)method for occluded person re-identification (ReID). Unlike mainstreamapproaches using global features for simultaneous multi-task learning of ReIDand human parsing, or relying on semantic information for attention guidance,DROP argues that the inferior performance of the former is due to distinctgranularity requirements for ReID and human parsing features. ReID focuses oninstance part-level differences between pedestrian parts, while human parsingcenters on semantic spatial context, reflecting the internal structure of thehuman body. To address this, DROP decouples features for ReID and humanparsing, proposing detail-preserving upsampling to combine varying resolutionfeature maps. Parsing-specific features for human parsing are decoupled, andhuman position information is exclusively added to the human parsing branch. Inthe ReID branch, a part-aware compactness loss is introduced to enhanceinstance-level part differences. Experimental results highlight the efficacy ofDROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke,surpassing two mainstream methods. The codebase is accessible athttps://github.com/shuguang-52/DROP.</description><author>Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu, Qingsong Zhao, Cairong Zhao</author><pubDate>Wed, 31 Jan 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18032v1</guid></item><item><title>Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI</title><link>http://arxiv.org/abs/2401.18028v1</link><description>Anticipating the negative impacts of emerging AI technologies is a challenge,especially in the early stages of development. An understudied approach to suchanticipation is the use of LLMs to enhance and guide this process. Despiteadvancements in LLMs and evaluation metrics to account for biases in generatedtext, it is unclear how well these models perform in anticipatory tasks.Specifically, the use of LLMs to anticipate AI impacts raises questions aboutthe quality and range of categories of negative impacts these models arecapable of generating. In this paper we leverage news media, a diverse datasource that is rich with normative assessments of emerging technologies, toformulate a taxonomy of impacts to act as a baseline for comparing against. Bycomputationally analyzing thousands of news articles published by hundreds ofonline news domains around the world, we develop a taxonomy consisting of tencategories of AI impacts. We then evaluate both instruction-based (GPT-4 andMistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)using a sample from this baseline. We find that the generated impacts usingMistral-7B, fine-tuned on impacts from the news media, tend to be qualitativelyon par with impacts generated using a larger scale model such as GPT-4.Moreover, we find that these LLMs generate impacts that largely reflect thetaxonomy of negative impacts identified in the news media, however the impactsproduced by instruction-based models had gaps in the production of certaincategories of impacts in comparison to fine-tuned models. This researchhighlights a potential bias in state-of-the-art LLMs when used for anticipatingimpacts and demonstrates the advantages of aligning smaller LLMs with a diverserange of impacts, such as those reflected in the news media, to better reflectsuch impacts during anticipatory exercises.</description><author>Mowafak Allaham, Nicholas Diakopoulos</author><pubDate>Wed, 31 Jan 2024 17:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18028v1</guid></item><item><title>Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization</title><link>http://arxiv.org/abs/2401.15496v2</link><description>Large language models (LLMs) like Llama, Baichuan and Bloom models showremarkable ability with instruction fine-tuning in many natural language tasks.Nevertheless, for the dialogue summarization task, which aims to generatesummaries for different roles in dialogue, most of the state-of-the-art methodsconduct on small models (e.g Bart and Bert). Existing methods try to add taskspecified optimization on small models like adding global-local centralityscore to models. In this paper, we propose an instruction fine-tuning model:Baichuan2-Sum, for role-oriented diaglouge summarization. By setting differentinstructions for different roles, the model can learn from the dialogueinteractions and output the expected summaries. Furthermore, we applied NEFTunetechnique to add suitable noise during training to improve the results. Theexperiments demonstrate that the proposed model achieves the newstate-of-the-art results on two public dialogue summarization datasets: CSDSand SAMSUM. We release our model and related codes to facilitate future studieson dialogue summarization task.</description><author>Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao</author><pubDate>Wed, 31 Jan 2024 17:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15496v2</guid></item><item><title>A cost-sensitive constrained Lasso</title><link>http://arxiv.org/abs/2401.18023v1</link><description>The Lasso has become a benchmark data analysis procedure, and numerousvariants have been proposed in the literature. Although the Lasso formulationsare stated so that overall prediction error is optimized, no full control overthe accuracy prediction on certain individuals of interest is allowed. In thiswork we propose a novel version of the Lasso in which quadratic performanceconstraints are added to Lasso-based objective functions, in such a way thatthreshold values are set to bound the prediction errors in the different groupsof interest (not necessarily disjoint). As a result, a constrained sparseregression model is defined by a nonlinear optimization problem. Thiscost-sensitive constrained Lasso has a direct application in heterogeneoussamples where data are collected from distinct sources, as it is standard inmany biomedical contexts. Both theoretical properties and empirical studiesconcerning the new method are explored in this paper. In addition, twoillustrations of the method on biomedical and sociological contexts areconsidered.</description><author>Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo, M. Remedios Sillero-Denamiel</author><pubDate>Wed, 31 Jan 2024 17:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18023v1</guid></item><item><title>Domain-Generalizable Multiple-Domain Clustering</title><link>http://arxiv.org/abs/2301.13530v2</link><description>This work generalizes the problem of unsupervised domain generalization tothe case in which no labeled samples are available (completely unsupervised).We are given unlabeled samples from multiple source domains, and we aim tolearn a shared predictor that assigns examples to semantically relatedclusters. Evaluation is done by predicting cluster assignments in previouslyunseen domains. Towards this goal, we propose a two-stage training framework:(1) self-supervised pre-training for extracting domain invariant semanticfeatures. (2) multi-head cluster prediction with pseudo labels, which rely onboth the feature space and cluster head prediction, further leveraging a novelprediction-based label smoothing scheme. We demonstrate empirically that ourmodel is more accurate than baselines that require fine-tuning using samplesfrom the target domain or some level of supervision. Our code is available athttps://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.</description><author>Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum</author><pubDate>Wed, 31 Jan 2024 17:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13530v2</guid></item><item><title>Prompt-Driven LLM Safeguarding via Directed Representation Optimization</title><link>http://arxiv.org/abs/2401.18018v1</link><description>Prepending model inputs with safety prompts is a common practice ofsafeguarding large language models (LLMs) from complying with queries thatcontain harmful intents. However, the working mechanisms of safety prompts havenot yet been fully understood, which hinders the potential for automaticallyoptimizing them for improved LLM safety. Motivated by this problem, weinvestigate the impact of safety prompts from the perspective of modelrepresentations. We find that in models' representation space, harmful andharmless queries can be largely distinguished, but this is not noticeablyenhanced by safety prompts. Instead, the queries' representations are moved bydifferent safety prompts in similar directions, where models become more proneto refusal (i.e., refusing to provide assistance) even when the queries areharmless. Inspired by these findings, we propose a method called DRO (DirectedRepresentation Optimization) for automatic safety prompt optimization. DROtreats safety prompts as continuous, trainable embeddings and learns to movethe representations of harmful/harmless queries along/opposite the direction inwhich the model's refusal probability increases. We demonstrate that DROremarkably improves the safeguarding performance of human-crafted safetyprompts and outperforms strong baselines, as evaluated on out-of-domainbenchmarks, without compromising the general model capability.</description><author>Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng</author><pubDate>Wed, 31 Jan 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18018v1</guid></item><item><title>Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms</title><link>http://arxiv.org/abs/2401.18017v1</link><description>The discovery of causal relationships in a set of random variables is afundamental objective of science and has also recently been argued as being anessential component towards real machine intelligence. One class of causaldiscovery techniques are founded based on the argument that there are inherentstructural asymmetries between the causal and anti-causal direction which couldbe leveraged in determining the direction of causation. To go about capturingthese discrepancies between cause and effect remains to be a challenge and manycurrent state-of-the-art algorithms propose to compare the norms of the kernelmean embeddings of the conditional distributions. In this work, we argue thatsuch approaches based on RKHS embeddings are insufficient in capturingprincipal markers of cause-effect asymmetry involving higher-order structuralvariabilities of the conditional distributions. We propose Kernel IntrinsicInvariance Measure with Heterogeneous Transform (KIIM-HT) which introduces anovel score measure based on heterogeneous transformation of RKHS embeddings toextract relevant higher-order moments of the conditional densities for causaldiscovery. Inference is made via comparing the score of each hypotheticalcause-effect direction. Tests and comparisons on a synthetic dataset, atwo-dimensional synthetic dataset and the real-world benchmark datasetT\"ubingen Cause-Effect Pairs verify our approach. In addition, we conduct asensitivity analysis to the regularization parameter to faithfully compareprevious work to our method and an experiment with trials on variedhyperparameter values to showcase the robustness of our algorithm.</description><author>Tim Tse, Zhitang Chen, Shengyu Zhu, Yue Liu</author><pubDate>Wed, 31 Jan 2024 17:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18017v1</guid></item><item><title>Causal Coordinated Concurrent Reinforcement Learning</title><link>http://arxiv.org/abs/2401.18012v1</link><description>In this work, we propose a novel algorithmic framework for data sharing andcoordinated exploration for the purpose of learning more data-efficient andbetter performing policies under a concurrent reinforcement learning (CRL)setting. In contrast to other work which make the assumption that all agentsact under identical environments, we relax this restriction and insteadconsider the formulation where each agent acts within an environment whichshares a global structure but also exhibits individual variations. Ouralgorithm leverages a causal inference algorithm in the form of Additive NoiseModel - Mixture Model (ANM-MM) in extracting model parameters governingindividual differentials via independence enforcement. We propose a new datasharing scheme based on a similarity measure of the extracted model parametersand demonstrate superior learning speeds on a set of autoregressive, pendulumand cart-pole swing-up tasks and finally, we show the effectiveness of diverseaction selection between common agents under a sparse reward setting. To thebest of our knowledge, this is the first work in considering non-identicalenvironments in CRL and one of the few works which seek to integrate causalinference with reinforcement learning (RL).</description><author>Tim Tse, Isaac Chan, Zhitang Chen</author><pubDate>Wed, 31 Jan 2024 17:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18012v1</guid></item><item><title>SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering</title><link>http://arxiv.org/abs/2401.17809v1</link><description>Model editing has recently gained widespread attention. Current model editingmethods primarily involve modifying model parameters or adding additionalmodules to the existing model. However, the former causes irreversible damageto LLMs, while the latter incurs additional inference overhead and fuzzy vectormatching is not always reliable. To address these issues, we propose anexpandable Subject Word Embedding Altering (SWEA) framework, which modifies therepresentation of subjects and achieve the goal of editing knowledge during theinference stage. SWEA uses precise key matching outside the model and performsreliable subject word embedding altering, thus protecting the original weightsof the model without increasing inference overhead. We then propose optimizingthen suppressing fusion method, which first optimizes the embedding vector forthe editing target and then suppresses the Knowledge Embedding Dimension (KED)to obtain the final fused embedding. We thus propose SWEAOS method for editingfactual knowledge in LLMs. We demonstrate the state-of-the-art performance ofSWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoningability of SWEAOS in editing knowledge, we evaluate it on the more complexRIPPLEEDITS benchmark. The results on two subdatasets demonstrate that ourSWEAOS possesses state-of-the-art reasoning ability.</description><author>Xiaopeng Li, Shasha Li, Bin Ji, Shezheng Song, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang</author><pubDate>Wed, 31 Jan 2024 13:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17809v1</guid></item><item><title>Advances in 3D Generation: A Survey</title><link>http://arxiv.org/abs/2401.17807v1</link><description>Generating 3D models lies at the core of computer graphics and has been thefocus of decades of research. With the emergence of advanced neuralrepresentations and generative models, the field of 3D content generation isdeveloping rapidly, enabling the creation of increasingly high-quality anddiverse 3D models. The rapid growth of this field makes it difficult to stayabreast of all recent developments. In this survey, we aim to introduce thefundamental methodologies of 3D generation methods and establish a structuredroadmap, encompassing 3D representation, generation methods, datasets, andcorresponding applications. Specifically, we introduce the 3D representationsthat serve as the backbone for 3D generation. Furthermore, we provide acomprehensive overview of the rapidly growing literature on generation methods,categorized by the type of algorithmic paradigms, including feedforwardgeneration, optimization-based generation, procedural generation, andgenerative novel view synthesis. Lastly, we discuss available datasets,applications, and open challenges. We hope this survey will help readersexplore this exciting topic and foster further advancements in the field of 3Dcontent generation.</description><author>Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, Ying Shan</author><pubDate>Wed, 31 Jan 2024 13:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17807v1</guid></item><item><title>CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting</title><link>http://arxiv.org/abs/2305.12095v4</link><description>Recent studies have demonstrated the great power of Transformer models fortime series forecasting. One of the key elements that lead to the transformer'ssuccess is the channel-independent (CI) strategy to improve the trainingrobustness. However, the ignorance of the correlation among different channelsin CI would limit the model's forecasting capacity. In this work, we design aspecial Transformer, i.e., {\bf C}hannel {\bf A}ligned {\bf R}obust Blen{\bf d}Transformer (CARD for short), that addresses key shortcomings of CI typeTransformer in time series forecasting. First, CARD introduces achannel-aligned attention structure that allows it to capture both temporalcorrelations among signals and dynamical dependence among multiple variablesover time. Second, in order to efficiently utilize the multi-scale knowledge,we design a token blend module to generate tokens with different resolutions.Third, we introduce a robust loss function for time series forecasting toalleviate the potential overfitting issue. This new loss function weights theimportance of forecasting over a finite horizon based on predictionuncertainties. Our evaluation of multiple long-term and short-term forecastingdatasets demonstrates that CARD significantly outperforms state-of-the-art timeseries forecasting methods. The code is available at the following anonymousrepository: \url{https://anonymous.4open.science/r/CARD-6EEC}</description><author>Wang Xue, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin</author><pubDate>Wed, 31 Jan 2024 13:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12095v4</guid></item><item><title>Biospheric AI</title><link>http://arxiv.org/abs/2401.17805v1</link><description>The dominant paradigm in AI ethics and value alignment is highlyanthropocentric. The focus of these disciplines is strictly on human valueswhich limits the depth and breadth of their insights. Recently, attempts toexpand to a sentientist perspective have been initiated. We argue that neitherof these outlooks is sufficient to capture the actual complexity of thebiosphere and ensure that AI does not damage it. Thus, we propose a newparadigm -- Biospheric AI that assumes an ecocentric perspective. We discusshypothetical ways in which such an AI might be designed. Moreover, we givedirections for research and application of the modern AI models that would beconsistent with the biospheric interests. All in all, this work attempts totake first steps towards a comprehensive program of research that focuses onthe interactions between AI and the biosphere.</description><author>Marcin Korecki</author><pubDate>Wed, 31 Jan 2024 13:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17805v1</guid></item><item><title>Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding</title><link>http://arxiv.org/abs/2308.11234v5</link><description>Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics thatasks us to compute collision-free paths for a team of agents, all moving acrossa shared map. Although many works appear on this topic, all current algorithmsstruggle as the number of agents grows. The principal reason is that existingapproaches typically plan free-flow optimal paths, which creates congestion. Totackle this issue, we propose a new approach for MAPF where agents are guidedto their destination by following congestion-avoiding paths. We evaluate theidea in two large-scale settings: one-shot MAPF, where each agent has a singledestination, and lifelong MAPF, where agents are continuously assigned newdestinations. Empirically, we report large improvements in solution quality forone-short MAPF and in overall throughput for lifelong MAPF.</description><author>Zhe Chen, Daniel Harabor, Jiaoyang Li, Peter J. Stuckey</author><pubDate>Wed, 31 Jan 2024 12:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11234v5</guid></item><item><title>SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes</title><link>http://arxiv.org/abs/2401.17803v1</link><description>Segment anything model (SAM) has demonstrated excellent generalizationcapabilities in common vision scenarios, yet lacking an understanding ofspecialized data. Although numerous works have focused on optimizing SAM fordownstream tasks, these task-specific approaches usually limit thegeneralizability to other downstream tasks. In this paper, we aim toinvestigate the impact of the general vision modules on finetuning SAM andenable them to generalize across all downstream tasks. We propose a simpleunified framework called SimAda for adapting SAM in underperformed scenes.Specifically, our framework abstracts the general modules of different methodsinto basic design elements, and we design four variants based on a sharedtheoretical framework. SimAda is simple yet effective, which removes alldataset-specific designs and focuses solely on general optimization, ensuringthat SimAda can be applied to all SAM-based and even Transformer-based models.We conduct extensive experiments on nine datasets of six downstream tasks. Theresults demonstrate that SimAda significantly improves the performance of SAMon multiple downstream tasks and achieves state-of-the-art performance on mostof them, without requiring task-specific designs. Code is available at:https://github.com/zongzi13545329/SimAda</description><author>Yiran Song, Qianyu Zhou, Xuequan Lu, Zhiwen Shao, Lizhuang Ma</author><pubDate>Wed, 31 Jan 2024 12:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17803v1</guid></item><item><title>Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning</title><link>http://arxiv.org/abs/2401.17802v1</link><description>Contrastive representation learning is crucial in time series analysis as italleviates the issue of data noise and incompleteness as well as sparsity ofsupervision signal. However, existing constrastive learning frameworks usuallyfocus on intral-temporal features, which fails to fully exploit the intricatenature of time series data. To address this issue, we propose DE-TSMCL, aninnovative distillation enhanced framework for long sequence time seriesforecasting. Specifically, we design a learnable data augmentation mechanismwhich adaptively learns whether to mask a timestamp to obtain optimizedsub-sequences. Then, we propose a contrastive learning task with momentumupdate to explore inter-sample and intra-temporal correlations of time seriesto learn the underlying structure feature on the unlabeled time series.Meanwhile, we design a supervised task to learn more robust representations andfacilitate the contrastive learning process. Finally, we jointly optimize theabove two tasks. By developing model loss from multiple tasks, we can learneffective representations for downstream forecasting task. Extensiveexperiments, in comparison with state-of-the-arts, well demonstrate theeffectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%.</description><author>Haozhi Gao, Qianqian Ren, Jinbao Li</author><pubDate>Wed, 31 Jan 2024 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17802v1</guid></item><item><title>M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval</title><link>http://arxiv.org/abs/2401.17797v1</link><description>We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-trainingtowards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP.Upon popular image-text models like CLIP, most current adaptation-basedvideo-text pre-training methods are confronted by three major issues, i.e.,noisy data corpus, time-consuming pre-training, and limited performance gain.Towards this end, we conduct a comprehensive study including four criticalsteps in video-text pre-training. Specifically, we investigate 1) datafiltering and refinement, 2) video input type selection, 3) temporal modeling,and 4) video feature enhancement. We then summarize this empirical study intothe M2-RAAP recipe, where our technical contributions lie in 1) the datafiltering and text re-writing pipeline resulting in 1M high-quality bilingualvideo-text pairs, 2) the replacement of video inputs with key-frames toaccelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy toenhance video features. We conduct extensive experiments by adapting threeimage-text foundation models on two refined video-text datasets from differentlanguages, validating the robustness and reproducibility of M2-RAAP foradaptation-based pre-training. Results demonstrate that M2-RAAP yields superiorperformance with significantly reduced data (-90%) and time consumption (-95%),establishing a new SOTA on four English zero-shot retrieval datasets and twoChinese ones. We are preparing our refined bilingual data annotations andcodebase, which will be available athttps://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.</description><author>Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, Qingpei Guo</author><pubDate>Wed, 31 Jan 2024 12:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17797v1</guid></item><item><title>Rectify the Regression Bias in Long-Tailed Object Detection</title><link>http://arxiv.org/abs/2401.15885v2</link><description>Long-tailed object detection faces great challenges because of its extremelyimbalanced class distribution. Recent methods mainly focus on theclassification bias and its loss function design, while ignoring the subtleinfluence of the regression branch. This paper shows that the regression biasexists and does adversely and seriously impact the detection accuracy. Whileexisting methods fail to handle the regression bias, the class-specificregression head for rare classes is hypothesized to be the main cause of it inthis paper. As a result, three kinds of viable solutions to cater for the rarecategories are proposed, including adding a class-agnostic branch, clusteringheads and merging heads. The proposed methods brings in consistent andsignificant improvements over existing long-tailed detection methods,especially in rare and common classes. The proposed method achievesstate-of-the-art performance in the large vocabulary LVIS dataset withdifferent backbones and architectures. It generalizes well to more difficultevaluation metrics, relatively balanced datasets, and the mask branch. This isthe first attempt to reveal and explore rectifying of the regression bias inlong-tailed object detection.</description><author>Ke Zhu, Minghao Fu, Jie Shao, Tianyu Liu, Jianxin Wu</author><pubDate>Wed, 31 Jan 2024 12:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15885v2</guid></item><item><title>Variational Autoencoding of Dental Point Clouds</title><link>http://arxiv.org/abs/2307.10895v3</link><description>Digital dentistry has made significant advancements, yet numerous challengesremain. This paper introduces the FDI 16 dataset, an extensive collection oftooth meshes and point clouds. Additionally, we present a novel approach:Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoderdesigned for point clouds. Notably, prior latent variable models for pointclouds lack a one-to-one correspondence between input and output points.Instead, they rely on optimizing Chamfer distances, a metric that lacks anormalized distributional counterpart, rendering it unsuitable forprobabilistic modeling. We replace the explicit minimization of Chamferdistances with a suitable encoder, increasing computational efficiency whilesimplifying the probabilistic extension. This allows for straightforwardapplication in various tasks, including mesh generation, shape completion, andrepresentation learning. Empirically, we provide evidence of lowerreconstruction error in dental reconstruction and interpolation, showcasingstate-of-the-art performance in dental sample generation while identifyingvaluable latent representations.</description><author>Johan Ziruo Ye, Thomas Ørkild, Peter Lempel Søndergaard, Søren Hauberg</author><pubDate>Wed, 31 Jan 2024 12:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10895v3</guid></item><item><title>RCT Rejection Sampling for Causal Estimation Evaluation</title><link>http://arxiv.org/abs/2307.15176v3</link><description>Confounding is a significant obstacle to unbiased estimation of causaleffects from observational data. For settings with high-dimensional covariates-- such as text data, genomics, or the behavioral social sciences --researchers have proposed methods to adjust for confounding by adapting machinelearning methods to the goal of causal estimation. However, empiricalevaluation of these adjustment methods has been challenging and limited. Inthis work, we build on a promising empirical evaluation strategy thatsimplifies evaluation design and uses real data: subsampling randomizedcontrolled trials (RCTs) to create confounded observational datasets whileusing the average causal effects from the RCTs as ground-truth. We contribute anew sampling algorithm, which we call RCT rejection sampling, and providetheoretical guarantees that causal identification holds in the observationaldata to allow for valid comparisons to the ground-truth RCT. Using syntheticdata, we show our algorithm indeed results in low bias when oracle estimatorsare evaluated on the confounded samples, which is not always the case for apreviously proposed algorithm. In addition to this identification result, wehighlight several finite data considerations for evaluation designers who planto use RCT rejection sampling on their own datasets. As a proof of concept, weimplement an example evaluation pipeline and walk through these finite dataconsiderations with a novel, real-world RCT -- which we release publicly --consisting of approximately 70k observations and text data as high-dimensionalcovariates. Together, these contributions build towards a broader agenda ofimproved empirical evaluation for causal estimation.</description><author>Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya</author><pubDate>Wed, 31 Jan 2024 12:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15176v3</guid></item><item><title>A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM</title><link>http://arxiv.org/abs/2401.15378v3</link><description>There exist challenges in learning and understanding religions as thepresence of complexity and depth of religious doctrines and teachings. Chatbotsas question-answering systems can help in solving these challenges. LLMchatbots use NLP techniques to establish connections between topics andaccurately respond to complex questions. These capabilities make it perfect tobe used in enlightenment on religion as a question answering chatbot. However,LLMs also have a tendency to generate false information, known ashallucination. The responses of the chatbots can include content that insultspersonal religious beliefs, interfaith conflicts, and controversial orsensitive topics. It needs to avoid such cases without promoting hate speech oroffending certain groups of people or their beliefs. This study uses a vectordatabase-based Retrieval Augmented Generation (RAG) approach to enhance theaccuracy and transparency of LLMs. Our question-answering system is called as"MufassirQAS". We created a vector database with several open-access books thatinclude Turkish context. These are Turkish translations, and interpretations onIslam. We worked on creating system prompts with care, ensuring they provideinstructions that prevent harmful, offensive, or disrespectful responses. Wealso tested the MufassirQAS and ChatGPT with sensitive questions. We got betterperformance with our system. Study and enhancements are still in progress.Results and future works are given.</description><author>Ahmet Yusuf Alan, Enis Karaarslan, Ömer Aydin</author><pubDate>Wed, 31 Jan 2024 12:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15378v3</guid></item><item><title>Vanishing Gradients in Reinforcement Finetuning of Language Models</title><link>http://arxiv.org/abs/2310.20703v2</link><description>Pretrained language models are commonly aligned with human preferences anddownstream tasks via reinforcement finetuning (RFT), which refers to maximizinga (possibly learned) reward function using policy gradient algorithms. Thiswork identifies a fundamental optimization obstacle in RFT: we prove that theexpected gradient for an input vanishes when its reward standard deviationunder the model is small, even if the expected reward is far from optimal.Through experiments on an RFT benchmark and controlled environments, as well asa theoretical analysis, we then demonstrate that vanishing gradients due tosmall reward standard deviation are prevalent and detrimental, leading toextremely slow reward maximization. Lastly, we explore ways to overcomevanishing gradients in RFT. We find the common practice of an initialsupervised finetuning (SFT) phase to be the most promising candidate, whichsheds light on its importance in an RFT pipeline. Moreover, we show that arelatively small number of SFT optimization steps on as few as 1% of the inputsamples can suffice, indicating that the initial SFT phase need not beexpensive in terms of compute and data labeling efforts. Overall, our resultsemphasize that being mindful for inputs whose expected gradient vanishes, asmeasured by the reward standard deviation, is crucial for successful executionof RFT.</description><author>Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</author><pubDate>Wed, 31 Jan 2024 12:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20703v2</guid></item><item><title>An Empirical Study of Scaling Law for OCR</title><link>http://arxiv.org/abs/2401.00028v3</link><description>The laws of model size, data volume, computation and model performance havebeen extensively studied in the field of Natural Language Processing (NLP).However, the scaling laws in Optical Character Recognition (OCR) have not yetbeen investigated. To address this, we conducted comprehensive studies thatinvolved examining the correlation between performance and the scale of models,data volume and computation in the field of text recognition.Conclusively, thestudy demonstrates smooth power laws between performance and model size, aswell as training data volume, when other influencing factors are held constant.Additionally, we have constructed a large-scale dataset called REBU-Syn, whichcomprises 6 million real samples and 18 million synthetic samples. Based on ourscaling law and new dataset, we have successfully trained a scene textrecognition model, achieving a new state-ofthe-art on 6 common test benchmarkswith a top-1 average accuracy of 97.42%. The models and dataset are publiclyavailable at https://github.com/large-ocr-model/large-ocr-model.github.io.</description><author>Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han</author><pubDate>Wed, 31 Jan 2024 12:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00028v3</guid></item><item><title>Graph Transformers without Positional Encodings</title><link>http://arxiv.org/abs/2401.17791v1</link><description>Recently, Transformers for graph representation learning have becomeincreasingly popular, achieving state-of-the-art performance on a wide-varietyof datasets, either alone or in combination with message-passing graph neuralnetworks (MP-GNNs). Infusing graph inductive-biases in the innatelystructure-agnostic transformer architecture in the form of structural orpositional encodings (PEs) is key to achieving these impressive results.However, designing such encodings is tricky and disparate attempts have beenmade to engineer such encodings including Laplacian eigenvectors, relativerandom-walk probabilities (RRWP), spatial encodings, centrality encodings, edgeencodings etc. In this work, we argue that such encodings may not be requiredat all, provided the attention mechanism itself incorporates information aboutthe graph structure. We introduce Eigenformer, which uses a novelspectrum-aware attention mechanism cognizant of the Laplacian spectrum of thegraph, and empirically show that it achieves performance comparable to SOTAMP-GNN architectures and Graph Transformers on a number of standard GNNbenchmark datasets, even surpassing the SOTA on some datasets. We also findthat our architecture is much faster to train in terms of number of epochs,presumably due to the innate graph inductive biases.</description><author>Ayush Garg</author><pubDate>Wed, 31 Jan 2024 12:33:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17791v1</guid></item><item><title>RADIN: Souping on a Budget</title><link>http://arxiv.org/abs/2401.17790v1</link><description>Model Soups, extending Stochastic Weights Averaging (SWA), combine modelsfine-tuned with different hyperparameters. Yet, their adoption is hindered bycomputational challenges due to subset selection issues. In this paper, wepropose to speed up model soups by approximating soups performance usingaveraged ensemble logits performances. Theoretical insights validate thecongruence between ensemble logits and weight averaging soups across any mixingratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out byallowing flexible evaluation budgets, enabling users to adjust his budget ofexploration adapted to his resources while increasing performance at lowerbudget compared to previous greedy approach (up to 4% on ImageNet).</description><author>Thibaut Menes, Olivier Risser-Maroix</author><pubDate>Wed, 31 Jan 2024 12:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17790v1</guid></item><item><title>Robustly overfitting latents for flexible neural image compression</title><link>http://arxiv.org/abs/2401.17789v1</link><description>Neural image compression has made a great deal of progress. State-of-the-artmodels are based on variational autoencoders and are outperforming classicalmodels. Neural compression models learn to encode an image into a quantizedlatent representation that can be efficiently sent to the decoder, whichdecodes the quantized latent into a reconstructed image. While these modelshave proven successful in practice, they lead to sub-optimal results due toimperfect optimization and limitations in the encoder and decoder capacity.Recent work shows how to use stochastic Gumbel annealing (SGA) to refine thelatents of pre-trained neural image compression models. We extend this idea byintroducing SGA+, which contains three different methods that build upon SGA.Further, we give a detailed analysis of our proposed methods, show how theyimprove performance, and show that they are less sensitive to hyperparameterchoices. Besides, we show how each method can be extended to three- instead oftwo-class rounding. Finally, we show how refinement of the latents with ourbest-performing method improves the compression performance on the Tecnickdataset and how it can be deployed to partly move along the rate-distortioncurve.</description><author>Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai</author><pubDate>Wed, 31 Jan 2024 12:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17789v1</guid></item><item><title>SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms</title><link>http://arxiv.org/abs/2401.17783v1</link><description>SDRDPy is a desktop application that allows experts an intuitive graphic andtabular representation of the knowledge extracted by any supervised descriptiverule discovery algorithm. The application is able to provide an analysis of thedata showing the relevant information of the data set and the relationshipbetween the rules, data and the quality measures associated for each ruleregardless of the tool where algorithm has been executed. All of theinformation is presented in a user-friendly application in order to facilitateexpert analysis and also the exportation of reports in different formats.</description><author>M. A. Padilla-Rascon, P. Gonzalez, C. J. Carmona</author><pubDate>Wed, 31 Jan 2024 12:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17783v1</guid></item><item><title>Vision-Assisted Digital Twin Creation for mmWave Beam Management</title><link>http://arxiv.org/abs/2401.17781v1</link><description>In the context of communication networks, digital twin technology provides ameans to replicate the radio frequency (RF) propagation environment as well asthe system behaviour, allowing for a way to optimize the performance of adeployed system based on simulations. One of the key challenges in theapplication of Digital Twin technology to mmWave systems is the prevalentchannel simulators' stringent requirements on the accuracy of the 3D DigitalTwin, reducing the feasibility of the technology in real applications. Wepropose a practical Digital Twin creation pipeline and a channel simulator,that relies only on a single mounted camera and position information. Wedemonstrate the performance benefits compared to methods that do not explicitlymodel the 3D environment, on downstream sub-tasks in beam acquisition, usingthe real-world dataset of the DeepSense6G challenge</description><author>Maximilian Arnold, Bence Major, Fabio Valerio Massoli, Joseph B. Soriaga, Arash Behboodi</author><pubDate>Wed, 31 Jan 2024 12:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17781v1</guid></item><item><title>A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees</title><link>http://arxiv.org/abs/2401.17780v1</link><description>We study a primal-dual reinforcement learning (RL) algorithm for the onlineconstrained Markov decision processes (CMDP) problem, wherein the agentexplores an optimal policy that maximizes return while satisfying constraints.Despite its widespread practical use, the existing theoretical literature onprimal-dual RL algorithms for this problem only provides sublinear regretguarantees and fails to ensure convergence to optimal policies. In this paper,we introduce a novel policy gradient primal-dual algorithm with uniformprobably approximate correctness (Uniform-PAC) guarantees, simultaneouslyensuring convergence to optimal policies, sublinear regret, and polynomialsample complexity for any target accuracy. Notably, this represents the firstUniform-PAC algorithm for the online CMDP problem. In addition to thetheoretical guarantees, we empirically demonstrate in a simple CMDP that ouralgorithm converges to optimal policies, while an existing algorithm exhibitsoscillatory performance and constraint violation.</description><author>Toshinori Kitamura, Tadashi Kozuno, Masahiro Kato, Yuki Ichihara, Soichiro Nishimori, Akiyoshi Sannai, Sho Sonoda, Wataru Kumagai, Yutaka Matsuo</author><pubDate>Wed, 31 Jan 2024 12:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17780v1</guid></item><item><title>SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection</title><link>http://arxiv.org/abs/2401.08357v2</link><description>Existing multi-focus image fusion (MFIF) methods often fail to preserve theuncertain transition region and detect small focus areas within large defocusedregions accurately. To address this issue, this study proposes a newsmall-area-aware MFIF algorithm for enhancing object detection capability.First, we enhance the pixel attributes within the small focus and boundaryregions, which are subsequently combined with visual saliency detection toobtain the pre-fusion results used to discriminate the distribution of focusedpixels. To accurately ensure pixel focus, we consider the source image as acombination of focused, defocused, and uncertain regions and propose athree-region segmentation strategy. Finally, we design an effective pixelselection rule to generate segmentation decision maps and obtain the finalfusion results. Experiments demonstrated that the proposed method canaccurately detect small and smooth focus areas while improving object detectionperformance, outperforming existing methods in both subjective and objectiveevaluations. The source code is available at https://github.com/ixilai/SAMF.</description><author>Xilai Li, Xiaosong Li, Haishu Tan, Jinyang Li</author><pubDate>Wed, 31 Jan 2024 12:18:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08357v2</guid></item><item><title>Double InfoGAN for Contrastive Analysis</title><link>http://arxiv.org/abs/2401.17776v1</link><description>Contrastive Analysis (CA) deals with the discovery of what is common and whatis distinctive of a target domain compared to a background one. This is ofgreat interest in many applications, such as medical imaging. Currentstate-of-the-art (SOTA) methods are latent variable models based on VAE(CA-VAEs). However, they all either ignore important constraints or they don'tenforce fundamental assumptions. This may lead to sub-optimal solutions wheredistinctive factors are mistaken for common ones (or viceversa). Furthermore,the generated images have a rather poor quality, typical of VAEs, decreasingtheir interpretability and usefulness. Here, we propose Double InfoGAN, thefirst GAN based method for CA that leverages the high-quality synthesis of GANand the separation power of InfoGAN. Experimental results on four visualdatasets, from simple synthetic examples to complex medical images, show thatthe proposed method outperforms SOTA CA-VAEs in terms of latent separation andimage quality. Datasets and code are available online.</description><author>Florence Carton, Robin Louiset, Pietro Gori</author><pubDate>Wed, 31 Jan 2024 12:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17776v1</guid></item><item><title>Bridging the Gap between Multi-focus and Multi-modal: A Focused Integration Framework for Multi-modal Image Fusion</title><link>http://arxiv.org/abs/2311.01886v2</link><description>Multi-modal image fusion (MMIF) integrates valuable information fromdifferent modality images into a fused one. However, the fusion of multiplevisible images with different focal regions and infrared images is aunprecedented challenge in real MMIF applications. This is because of thelimited depth of the focus of visible optical lenses, which impedes thesimultaneous capture of the focal information within the same scene. To addressthis issue, in this paper, we propose a MMIF framework for joint focusedintegration and modalities information extraction. Specifically, asemi-sparsity-based smoothing filter is introduced to decompose the images intostructure and texture components. Subsequently, a novel multi-scale operator isproposed to fuse the texture components, capable of detecting significantinformation by considering the pixel focus attributes and relevant data fromvarious modal images. Additionally, to achieve an effective capture of sceneluminance and reasonable contrast maintenance, we consider the distribution ofenergy information in the structural components in terms of multi-directionalfrequency variance and information entropy. Extensive experiments on existingMMIF datasets, as well as the object detection and depth estimation tasks,consistently demonstrate that the proposed algorithm can surpass thestate-of-the-art methods in visual perception and quantitative evaluation. Thecode is available at https://github.com/ixilai/MFIF-MMIF.</description><author>Xilai Li, Xiaosong Li, Tao Ye, Xiaoqi Cheng, Wuyang Liu, Haishu Tan</author><pubDate>Wed, 31 Jan 2024 12:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01886v2</guid></item><item><title>SNP-S3: Shared Network Pre-training and Significant Semantic Strengthening for Various Video-Text Tasks</title><link>http://arxiv.org/abs/2401.17773v1</link><description>We present a framework for learning cross-modal video representations bydirectly pre-training on raw data to facilitate various downstream video-texttasks. Our main contributions lie in the pre-training framework and proxytasks. First, based on the shortcomings of two mainstream pixel-levelpre-training architectures (limited applications or less efficient), we proposeShared Network Pre-training (SNP). By employing one shared BERT-type network torefine textual and cross-modal features simultaneously, SNP is lightweight andcould support various downstream applications. Second, based on the intuitionthat people always pay attention to several "significant words" whenunderstanding a sentence, we propose the Significant Semantic Strengthening(S3) strategy, which includes a novel masking and matching proxy task topromote the pre-training performance. Experiments conducted on three downstreamvideo-text tasks and six datasets demonstrate that, we establish a newstate-of-the-art in pixel-level video-text pre-training; we also achieve asatisfactory balance between the pre-training efficiency and the fine-tuningperformance. The codebase are available athttps://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp.</description><author>Xingning Dong, Qingpei Guo, Tian Gan, Qing Wang, Jianlong Wu, Xiangyuan Ren, Yuan Cheng, Wei Chu</author><pubDate>Wed, 31 Jan 2024 12:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17773v1</guid></item><item><title>Consistent Signal Reconstruction from Streaming Multivariate Time Series</title><link>http://arxiv.org/abs/2308.12459v2</link><description>Digitalizing real-world analog signals typically involves sampling in timeand discretizing in amplitude. Subsequent signal reconstructions inevitablyincur an error that depends on the amplitude resolution and the temporaldensity of the acquired samples. From an implementation viewpoint, consistentsignal reconstruction methods have proven a profitable error-rate decay as thesampling rate increases. Despite that, these results are obtained under offlinesettings. Therefore, a research gap exists regarding methods for consistentsignal reconstruction from data streams. Solving this problem is of greatimportance because such methods could run at a lower computational cost thanthe existing offline ones or be used under real-time requirements withoutlosing the benefits of ensuring consistency. In this paper, we formalize forthe first time the concept of consistent signal reconstruction from streamingtime-series data. Then, we present a signal reconstruction method able toenforce consistency and also exploit the spatiotemporal dependencies ofstreaming multivariate time-series data to further reduce the signalreconstruction error. Our experiments show that our proposed method achieves afavorable error-rate decay with the sampling rate compared to a similar butnon-consistent reconstruction.</description><author>Emilio Ruiz-Moreno, Luis Miguel López-Ramos, Baltasar Beferull-Lozano</author><pubDate>Wed, 31 Jan 2024 11:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12459v2</guid></item><item><title>Fine-Grained Zero-Shot Learning: Advances, Challenges, and Prospects</title><link>http://arxiv.org/abs/2401.17766v1</link><description>Recent zero-shot learning (ZSL) approaches have integrated fine-grainedanalysis, i.e., fine-grained ZSL, to mitigate the commonly known seen/unseendomain bias and misaligned visual-semantics mapping problems, and have madeprofound progress. Notably, this paradigm differs from existing close-setfine-grained methods and, therefore, can pose unique and nontrivial challenges.However, to the best of our knowledge, there remains a lack of systematicsummaries of this topic. To enrich the literature of this domain and provide asound basis for its future development, in this paper, we present a broadreview of recent advances for fine-grained analysis in ZSL. Concretely, wefirst provide a taxonomy of existing methods and techniques with a thoroughanalysis of each category. Then, we summarize the benchmark, covering publiclyavailable datasets, models, implementations, and some more details as alibrary. Last, we sketch out some related applications. In addition, we discussvital challenges and suggest potential future directions.</description><author>Jingcai Guo, Zhijie Rao, Song Guo, Jingren Zhou, Dacheng Tao</author><pubDate>Wed, 31 Jan 2024 11:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17766v1</guid></item><item><title>Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes</title><link>http://arxiv.org/abs/2312.06353v3</link><description>Pre-trained large language models (LLMs) need fine-tuning to improve theirresponsiveness to natural language instructions. Federated learning offers away to fine-tune LLMs using the abundant data on end devices withoutcompromising data privacy. Most existing federated fine-tuning methods for LLMsrely on parameter-efficient fine-tuning techniques, which may not reach theperformance height possible with full-parameter tuning. However, federatedfull-parameter tuning of LLMs is a non-trivial problem due to the immensecommunication cost. This work introduces FedKSeed that employs zeroth-orderoptimization with a finite set of random seeds. It significantly reducestransmission requirements between the server and clients to just a few randomseeds and scalar gradients, amounting to only a few thousand bytes, makingfederated full-parameter tuning of billion-sized LLMs possible on devices.Building on it, we develop a strategy enabling probability-differentiated seedsampling, prioritizing perturbations with greater impact on model accuracy.Experiments across six scenarios with various LLMs, datasets and datapartitions demonstrate that our approach outperforms existing federated LLMfine-tuning methods in both communication efficiency and new taskgeneralization.</description><author>Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng</author><pubDate>Wed, 31 Jan 2024 11:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06353v3</guid></item><item><title>PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</title><link>http://arxiv.org/abs/2309.10896v2</link><description>This document presents PLVS: a real-time system that leverages sparse SLAM,volumetric mapping, and 3D unsupervised incremental segmentation. PLVS standsfor Points, Lines, Volumetric mapping, and Segmentation. It supports RGB-D andStereo cameras, which may be optionally equipped with IMUs. The SLAM module iskeyframe-based, and extracts and tracks sparse points and line segments asfeatures. Volumetric mapping runs in parallel with respect to the SLAMfront-end and generates a 3D reconstruction of the explored environment byfusing point clouds backprojected from keyframes. Different volumetric mappingmethods are supported and integrated in PLVS. We use a novel reprojection errorto bundle-adjust line segments. This error exploits available depth informationto stabilize the position estimates of line segment endpoints. An incrementaland geometric-based segmentation method is implemented and integrated for RGB-Dcameras in the PLVS framework. We present qualitative and quantitativeevaluations of the PLVS framework on some publicly available datasets. Theappendix details the adopted stereo line triangulation method and provides aderivation of the Jacobians we used for line error terms. The software isavailable as open-source.</description><author>Luigi Freda</author><pubDate>Wed, 31 Jan 2024 11:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10896v2</guid></item><item><title>Convergence of Expectation-Maximization Algorithm with Mixed-Integer Optimization</title><link>http://arxiv.org/abs/2401.17763v1</link><description>The convergence of expectation-maximization (EM)-based algorithms typicallyrequires continuity of the likelihood function with respect to all the unknownparameters (optimization variables). The requirement is not met when parameterscomprise both discrete and continuous variables, making the convergenceanalysis nontrivial. This paper introduces a set of conditions that ensure theconvergence of a specific class of EM algorithms that estimate a mixture ofdiscrete and continuous parameters. Our results offer a new analysis techniquefor iterative algorithms that solve mixed-integer non-linear optimizationproblems. As a concrete example, we prove the convergence of the EM-basedsparse Bayesian learning algorithm in [1] that estimates the state of a lineardynamical system with jointly sparse inputs and bursty missing observations.Our results establish that the algorithm in [1] converges to the set ofstationary points of the maximum likelihood cost with respect to the continuousoptimization variables.</description><author>Geethu Joseph</author><pubDate>Wed, 31 Jan 2024 11:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17763v1</guid></item><item><title>Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator</title><link>http://arxiv.org/abs/2401.17760v1</link><description>Linear discriminant analysis (LDA) is a widely used technique for dataclassification. The method offers adequate performance in many classificationproblems, but it becomes inefficient when the data covariance matrix isill-conditioned. This often occurs when the feature space's dimensionality ishigher than or comparable to the training data size. Regularized LDA (RLDA)methods based on regularized linear estimators of the data covariance matrixhave been proposed to cope with such a situation. The performance of RLDAmethods is well studied, with optimal regularization schemes already proposed.In this paper, we investigate the capability of a positive semidefiniteridge-type estimator of the inverse covariance matrix that coincides with anonlinear (NL) covariance matrix estimator. The estimator is derived byreformulating the score function of the optimal classifier utilizing linearestimation methods, which eventually results in the proposed NL-RLDAclassifier. We derive asymptotic and consistent estimators of the proposedtechnique's misclassification rate under the assumptions of a double-asymptoticregime and multivariate Gaussian model for the classes. The consistentestimator, coupled with a one-dimensional grid search, is used to set the valueof the regularization parameter required for the proposed NL-RLDA classifier.Performance evaluations based on both synthetic and real data demonstrate theeffectiveness of the proposed classifier. The proposed technique outperformsstate-of-art methods over multiple datasets. When compared to state-of-the-artmethods across various datasets, the proposed technique exhibits superiorperformance.</description><author>Maaz Mahadi, Tarig Ballal, Muhammad Moinuddin, Tareq Y. Al-Naffouri, Ubaid M. Al-Saggaf</author><pubDate>Wed, 31 Jan 2024 11:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17760v1</guid></item><item><title>Tiered approach for rapid damage characterisation of infrastructure enabled by remote sensing and deep learning technologies</title><link>http://arxiv.org/abs/2401.17759v1</link><description>Critical infrastructure such as bridges are systematically targeted duringwars and conflicts. This is because critical infrastructure is vital forenabling connectivity and transportation of people and goods, and hence,underpinning the national and international defence planning and economicgrowth. Mass destruction of bridges, along with minimal or no accessibility tothese assets during natural and anthropogenic disasters, prevents us fromdelivering rapid recovery. As a result, systemic resilience is drasticallyreduced. A solution to this challenge is to use technology for stand-offobservations. Yet, no method exists to characterise damage at different scales,i.e. regional, asset, and structural (component), and more so there is littleor no systematic correlation between assessments at scale. We propose anintegrated three-level tiered approach to fill this capability gap, and wedemonstrate the methods for damage characterisation enabled by fit-for-purposedigital technologies. Next, this method is applied and validated to a casestudy in Ukraine that includes 17 bridges. From macro to micro, we deploytechnology at scale, from Sentinel-1 SAR images, crowdsourced information, andhigh-resolution images to deep learning for damaged infrastructure. For thefirst time, the interferometric coherence difference and semantic segmentationof images were deployed to improve the reliability of damage characterisationsfrom regional to infrastructure component level, when enhanced assessmentaccuracy is required. This integrated method improves the speed ofdecision-making, and thus, enhances resilience. Keywords: criticalinfrastructure, damage characterisation, targeted attacks, restoration</description><author>Nadiia Kopiika, Andreas Karavias, Pavlos Krassakis, Zehao Ye, Jelena Ninic, Nataliya Shakhovska, Nikolaos Koukouzas, Sotirios Argyroudis, Stergios-Aristoteles Mitoulis</author><pubDate>Wed, 31 Jan 2024 11:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17759v1</guid></item><item><title>Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks</title><link>http://arxiv.org/abs/2210.00822v3</link><description>We consider the problem of estimating the marginal independence structure ofa Bayesian network from observational data, learning an undirected graph wecall the unconditional dependence graph. We show that unconditional dependencegraphs of Bayesian networks correspond to the graphs having equal independenceand intersection numbers. Using this observation, a Gr\"obner basis for a toricideal associated to unconditional dependence graphs of Bayesian networks isgiven and then extended by additional binomial relations to connect the spaceof all such graphs. An MCMC method, called GrUES (Gr\"obner-based UnconditionalEquivalence Search), is implemented based on the resulting moves and applied tosynthetic Gaussian data. GrUES recovers the true marginal independencestructure via a penalized maximum likelihood or MAP estimate at a higher ratethan simple independence tests while also yielding an estimate of theposterior, for which the $20\%$ HPD credible sets include the true structure ata high rate for data-generating graphs with density at least $0.5$.</description><author>Danai Deligeorgaki, Alex Markham, Pratik Misra, Liam Solus</author><pubDate>Wed, 31 Jan 2024 11:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00822v3</guid></item><item><title>CauESC: A Causal Aware Model for Emotional Support Conversation</title><link>http://arxiv.org/abs/2401.17755v1</link><description>Emotional Support Conversation aims at reducing the seeker's emotionaldistress through supportive response. Existing approaches have two limitations:(1) They ignore the emotion causes of the distress, which is important forfine-grained emotion understanding; (2) They focus on the seeker's own mentalstate rather than the emotional dynamics during interaction between speakers.To address these issues, we propose a novel framework CauESC, which firstlyrecognizes the emotion causes of the distress, as well as the emotion effectstriggered by the causes, and then understands each strategy of verbal groomingindependently and integrates them skillfully. Experimental results on thebenchmark dataset demonstrate the effectiveness of our approach and show thebenefits of emotion understanding from cause to effect andindependent-integrated strategy modeling.</description><author>Wei Chen, Hengxu Lin, Qun Zhang, Xiaojin Zhang, Xiang Bai, Xuanjing Huang, Zhongyu Wei</author><pubDate>Wed, 31 Jan 2024 11:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17755v1</guid></item><item><title>Efficient Large Language Models: A Survey</title><link>http://arxiv.org/abs/2312.03863v3</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities inimportant tasks such as natural language understanding, language generation,and complex reasoning and have the potential to make a substantial impact onour society. Such capabilities, however, come with the considerable resourcesthey demand, highlighting the strong need to develop effective techniques foraddressing their efficiency challenges.In this survey, we provide a systematicand comprehensive review of efficient LLMs research. We organize the literaturein a taxonomy consisting of three main categories, covering distinct yetinterconnected efficient LLMs topics from model-centric, data-centric, andframework-centric perspective, respectively. We have also created a GitHubrepository where we compile the papers featured in this survey athttps://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will activelymaintain this repository and incorporate new research as it emerges. We hopeour survey can serve as a valuable resource to help researchers andpractitioners gain a systematic understanding of the research developments inefficient LLMs and inspire them to contribute to this important and excitingfield.</description><author>Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang</author><pubDate>Wed, 31 Jan 2024 11:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03863v3</guid></item><item><title>PF-GNN: Differentiable particle filtering based approximation of universal graph representations</title><link>http://arxiv.org/abs/2401.17752v1</link><description>Message passing Graph Neural Networks (GNNs) are known to be limited inexpressive power by the 1-WL color-refinement test for graph isomorphism. Othermore expressive models either are computationally expensive or needpreprocessing to extract structural features from the graph. In this work, wepropose to make GNNs universal by guiding the learning process with exactisomorphism solver techniques which operate on the paradigm ofIndividualization and Refinement (IR), a method to artificially introduceasymmetry and further refine the coloring when 1-WL stops. Isomorphism solversgenerate a search tree of colorings whose leaves uniquely identify the graph.However, the tree grows exponentially large and needs hand-crafted pruningtechniques which are not desirable from a learning perspective. We take aprobabilistic view and approximate the search tree of colorings (i.e.embeddings) by sampling multiple paths from root to leaves of the search tree.To learn more discriminative representations, we guide the sampling processwith particle filter updates, a principled approach for sequential stateestimation. Our algorithm is end-to-end differentiable, can be applied with anyGNN as backbone and learns richer graph representations with only linearincrease in runtime. Experimental evaluation shows that our approachconsistently outperforms leading GNN models on both synthetic benchmarks forisomorphism detection as well as real-world datasets.</description><author>Mohammed Haroon Dupty, Yanfei Dong, Wee Sun Lee</author><pubDate>Wed, 31 Jan 2024 11:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17752v1</guid></item><item><title>Large Trajectory Models are Scalable Motion Predictors and Planners</title><link>http://arxiv.org/abs/2310.19620v2</link><description>Motion prediction and planning are vital tasks in autonomous driving, andrecent efforts have shifted to machine learning-based approaches. Thechallenges include understanding diverse road topologies, reasoning trafficdynamics over a long time horizon, interpreting heterogeneous behaviors, andgenerating policies in a large continuous state space. Inspired by the successof large language models in addressing similar complexities through modelscaling, we introduce a scalable trajectory model called State Transformer(STR). STR reformulates the motion prediction and motion planning problems byarranging observations, states, and actions into one unified sequence modelingtask. Our approach unites trajectory generation problems with other sequencemodeling problems, powering rapid iterations with breakthroughs in neighbordomains such as language modeling. Remarkably, experimental results reveal thatlarge trajectory models (LTMs), such as STR, adhere to the scaling laws bypresenting outstanding adaptability and learning efficiency. Qualitativeresults further demonstrate that LTMs are capable of making plausiblepredictions in scenarios that diverge significantly from the training datadistribution. LTMs also learn to make complex reasonings for long-termplanning, without explicit loss designs or costly high-level annotations.</description><author>Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo, Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao</author><pubDate>Wed, 31 Jan 2024 11:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19620v2</guid></item><item><title>SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models</title><link>http://arxiv.org/abs/2401.17749v1</link><description>Large language models (LLMs) have recently garnered significantaccomplishments in various exploratory tasks, even surpassing the performanceof traditional reinforcement learning-based methods that have historicallydominated the agent-based field. The purpose of this paper is to investigatethe efficacy of LLMs in executing real-time strategy war tasks within theStarCraft II gaming environment. In this paper, we introduce SwarmBrain, anembodied agent leveraging LLM for real-time strategy implementation in theStarCraft II game environment. The SwarmBrain comprises two key components: 1)a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designedto orchestrate macro-level strategies from a high-level perspective. Thismatrix emulates the overarching consciousness of the Zerg intelligence brain,synthesizing strategic foresight with the aim of allocating resources,directing expansion, and coordinating multi-pronged assaults. 2) a SwarmReflexNet, which is agile counterpart to the calculated deliberation of theOvermind Intelligence Matrix. Due to the inherent latency in LLM reasoning, theSwarm ReflexNet employs a condition-response state machine framework, enablingexpedited tactical responses for fundamental Zerg unit maneuvers. In theexperimental setup, SwarmBrain is in control of the Zerg race in confrontationwith an Computer-controlled Terran adversary. Experimental results show thecapacity of SwarmBrain to conduct economic augmentation, territorial expansion,and tactical formulation, and it shows the SwarmBrain is capable of achievingvictory against Computer players set at different difficulty levels.</description><author>Xiao Shao, Weifu Jiang, Fei Zuo, Mengqing Liu</author><pubDate>Wed, 31 Jan 2024 11:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17749v1</guid></item><item><title>Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations</title><link>http://arxiv.org/abs/2401.06633v2</link><description>Retrieval models aim at selecting a small set of item candidates which matchthe preference of a given user. They play a vital role in large-scalerecommender systems since subsequent models such as rankers highly depend onthe quality of item candidates. However, most existing retrieval models employa single-round inference paradigm, which may not adequately capture the dynamicnature of user preferences and stuck in one area in the item space. In thispaper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm forrecommender systems that iteratively refines user representations to bettercapture potential candidates in the full item space. Ada-Retrieval comprisestwo key modules: the item representation adapter and the user representationadapter, designed to inject context information into items' and users'representations. The framework maintains a model-agnostic design, allowingseamless integration with various backbone models such as RNNs or Transformers.We perform experiments on three widely used public datasets, incorporating fivepowerful sequential recommenders as backbone models. Our results demonstratethat Ada-Retrieval significantly enhances the performance of various basemodels, with consistent improvements observed across different datasets. Ourcode and data are publicly available at:https://github.com/ll0ruc/Ada-Retrieval.</description><author>Lei Li, Jianxun Lian, Xiao Zhou, Xing Xie</author><pubDate>Wed, 31 Jan 2024 11:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06633v2</guid></item><item><title>Algorithmic Robust Forecast Aggregation</title><link>http://arxiv.org/abs/2401.17743v1</link><description>Forecast aggregation combines the predictions of multiple forecasters toimprove accuracy. However, the lack of knowledge about forecasters' informationstructure hinders optimal aggregation. Given a family of informationstructures, robust forecast aggregation aims to find the aggregator withminimal worst-case regret compared to the omniscient aggregator. Previousapproaches for robust forecast aggregation rely on heuristic observations andparameter tuning. We propose an algorithmic framework for robust forecastaggregation. Our framework provides efficient approximation schemes for generalinformation aggregation with a finite family of possible informationstructures. In the setting considered by Arieli et al. (2018) where two agentsreceive independent signals conditioned on a binary state, our framework alsoprovides efficient approximation schemes by imposing Lipschitz conditions onthe aggregator or discrete conditions on agents' reports. Numerical experimentsdemonstrate the effectiveness of our method by providing a nearly optimalaggregator in the setting considered by Arieli et al. (2018).</description><author>Yongkang Guo, Jason D. Hartline, Zhihuan Huang, Yuqing Kong, Anant Shah, Fang-Yi Yu</author><pubDate>Wed, 31 Jan 2024 11:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17743v1</guid></item><item><title>Causal Machine Learning for Cost-Effective Allocation of Development Aid</title><link>http://arxiv.org/abs/2401.16986v2</link><description>The Sustainable Development Goals (SDGs) of the United Nations provide ablueprint of a better future by 'leaving no one behind', and, to achieve theSDGs by 2030, poor countries require immense volumes of development aid. Inthis paper, we develop a causal machine learning framework for predictingheterogeneous treatment effects of aid disbursements to inform effective aidallocation. Specifically, our framework comprises three components: (i) abalancing autoencoder that uses representation learning to embedhigh-dimensional country characteristics while addressing treatment selectionbias; (ii) a counterfactual generator to compute counterfactual outcomes forvarying aid volumes to address small sample-size settings; and (iii) aninference model that is used to predict heterogeneous treatment-responsecurves. We demonstrate the effectiveness of our framework using data withofficial development aid earmarked to end HIV/AIDS in 105 countries, amountingto more than USD 5.2 billion. For this, we first show that our frameworksuccessfully computes heterogeneous treatment-response curves usingsemi-synthetic data. Then, we demonstrate our framework using real-world HIVdata. Our framework points to large opportunities for a more effective aidallocation, suggesting that the total number of new HIV infections could bereduced by up to 3.3% (~50,000 cases) compared to the current allocationpractice.</description><author>Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel</author><pubDate>Wed, 31 Jan 2024 11:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16986v2</guid></item><item><title>Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance</title><link>http://arxiv.org/abs/2401.17741v1</link><description>This paper presents Haris, an advanced autonomous mobile robot system fortracking the location of vehicles in crowded car parks using license platerecognition. The system employs simultaneous localization and mapping (SLAM)for autonomous navigation and precise mapping of the parking area, eliminatingthe need for GPS dependency. In addition, the system utilizes a sophisticatedframework using computer vision techniques for object detection and automaticlicense plate recognition (ALPR) for reading and associating license platenumbers with location data. This information is subsequently synchronized witha back-end service and made accessible to users via a user-friendly mobile app,offering effortless vehicle location and alleviating congestion within theparking facility. The proposed system has the potential to improve themanagement of short-term large outdoor parking areas in crowded places such assports stadiums. The demo of the robot can be found onhttps://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.</description><author>Layth Hamad, Muhammad Asif Khan, Hamid Menouar, Fethi Filali, Amr Mohamed</author><pubDate>Wed, 31 Jan 2024 11:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17741v1</guid></item><item><title>Operator learning without the adjoint</title><link>http://arxiv.org/abs/2401.17739v1</link><description>There is a mystery at the heart of operator learning: how can one recover anon-self-adjoint operator from data without probing the adjoint? Currentpractical approaches suggest that one can accurately recover an operator whileonly using data generated by the forward action of the operator without accessto the adjoint. However, naively, it seems essential to sample the action ofthe adjoint. In this paper, we partially explain this mystery by proving thatwithout querying the adjoint, one can approximate a family of non-self-adjointinfinite-dimensional compact operators via projection onto a Fourier basis. Wethen apply the result to recovering Green's functions of elliptic partialdifferential operators and derive an adjoint-free sample complexity bound.While existing theory justifies low sample complexity in operator learning,ours is the first adjoint-free analysis that attempts to close the gap betweentheory and practice.</description><author>Nicolas Boullé, Diana Halikias, Samuel E. Otto, Alex Townsend</author><pubDate>Wed, 31 Jan 2024 10:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17739v1</guid></item><item><title>Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification</title><link>http://arxiv.org/abs/2401.17738v1</link><description>This study investigates the potential of using smartwatches with built-inmicrophone sensors for monitoring coughs and detecting various cough types. Weconducted a study involving 32 participants and collected 9 hours of audio datain a controlled manner. Afterward, we processed this data using a structuredapproach, resulting in 223 positive cough samples. We further improved thedataset through augmentation techniques and employed a specialized 1D CNNmodel. This model achieved an impressive accuracy rate of 98.49% whilenon-walking and 98.2% while walking, showing smartwatches can detect cough.Moreover, our research successfully identified four distinct types of coughsusing clustering techniques.</description><author>Pranay Jaiswal, Haroon R. Lone</author><pubDate>Wed, 31 Jan 2024 10:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17738v1</guid></item><item><title>Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation</title><link>http://arxiv.org/abs/2401.17737v1</link><description>Interpretability and transparency are essential for incorporating causaleffect models from observational data into policy decision-making. They canprovide trust for the model in the absence of ground truth labels to evaluatethe accuracy of such models. To date, attempts at transparent causal effectestimation consist of applying post hoc explanation methods to black-boxmodels, which are not interpretable. Here, we present BICauseTree: aninterpretable balancing method that identifies clusters where naturalexperiments occur locally. Our approach builds on decision trees with acustomized objective function to improve balancing and reduce treatmentallocation bias. Consequently, it can additionally detect subgroups presentingpositivity violations, exclude them, and provide a covariate-based definitionof the target population we can infer from and generalize to. We evaluate themethod's performance using synthetic and realistic datasets, explore itsbias-interpretability tradeoff, and show that it is comparable with existingapproaches.</description><author>Lucile Ter-Minassian, Liran Szlak, Ehud Karavani, Chris Holmes, Yishai Shimoni</author><pubDate>Wed, 31 Jan 2024 10:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17737v1</guid></item><item><title>On Inference Stability for Diffusion Models</title><link>http://arxiv.org/abs/2312.12431v2</link><description>Denoising Probabilistic Models (DPMs) represent an emerging domain ofgenerative models that excel in generating diverse and high-quality images.However, most current training methods for DPMs often neglect the correlationbetween timesteps, limiting the model's performance in generating imageseffectively. Notably, we theoretically point out that this issue can be causedby the cumulative estimation gap between the predicted and the actualtrajectory. To minimize that gap, we propose a novel \textit{sequence-aware}loss that aims to reduce the estimation gap to enhance the sampling quality.Furthermore, we theoretically show that our proposed loss function is a tighterupper bound of the estimation loss in comparison with the conventional loss inDPMs. Experimental results on several benchmark datasets including CIFAR10,CelebA, and CelebA-HQ consistently show a remarkable improvement of ourproposed method regarding the image generalization quality measured by FID andInception Score compared to several DPM baselines. Our code and pre-trainedcheckpoints are available at \url{https://github.com/VinAIResearch/SA-DPM}.</description><author>Viet Nguyen, Giang Vu, Tung Nguyen Thanh, Khoat Than, Toan Tran</author><pubDate>Wed, 31 Jan 2024 10:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12431v2</guid></item><item><title>Leveraging Human-Machine Interactions for Computer Vision Dataset Quality Enhancement</title><link>http://arxiv.org/abs/2401.17736v1</link><description>Large-scale datasets for single-label multi-class classification, such as\emph{ImageNet-1k}, have been instrumental in advancing deep learning andcomputer vision. However, a critical and often understudied aspect is thecomprehensive quality assessment of these datasets, especially regardingpotential multi-label annotation errors. In this paper, we introduce alightweight, user-friendly, and scalable framework that synergizes human andmachine intelligence for efficient dataset validation and quality enhancement.We term this novel framework \emph{Multilabelfy}. Central to Multilabelfy is anadaptable web-based platform that systematically guides annotators through there-evaluation process, effectively leveraging human-machine interactions toenhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, wefound that approximately $47.88\%$ of the images contained at least two labels,underscoring the need for more rigorous assessments of such influentialdatasets. Furthermore, our analysis showed a negative correlation between thenumber of potential labels per image and model top-1 accuracy, illuminating acrucial factor in model evaluation and selection. Our open-source framework,Multilabelfy, offers a convenient, lightweight solution for datasetenhancement, emphasizing multi-label proportions. This study tackles majorchallenges in dataset integrity and provides key insights into modelperformance evaluation. Moreover, it underscores the advantages of integratinghuman expertise with machine capabilities to produce more robust models andtrustworthy data development. The source code for Multilabelfy will beavailable at https://github.com/esla/Multilabelfy. \keywords{Computer Vision \and Dataset Quality Enhancement \and DatasetValidation \and Human-Computer Interaction \and Multi-label Annotation.}</description><author>Esla Timothy Anzaku, Hyesoo Hong, Jin-Woo Park, Wonjun Yang, Kangmin Kim, JongBum Won, Deshika Vinoshani Kumari Herath, Arnout Van Messem, Wesley De Neve</author><pubDate>Wed, 31 Jan 2024 10:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17736v1</guid></item><item><title>Towards Physical Plausibility in Neuroevolution Systems</title><link>http://arxiv.org/abs/2401.17733v1</link><description>The increasing usage of Artificial Intelligence (AI) models, especially DeepNeural Networks (DNNs), is increasing the power consumption during training andinference, posing environmental concerns and driving the need for moreenergy-efficient algorithms and hardware solutions. This work addresses thegrowing energy consumption problem in Machine Learning (ML), particularlyduring the inference phase. Even a slight reduction in power usage can lead tosignificant energy savings, benefiting users, companies, and the environment.Our approach focuses on maximizing the accuracy of Artificial Neural Network(ANN) models using a neuroevolutionary framework whilst minimizing their powerconsumption. To do so, power consumption is considered in the fitness function.We introduce a new mutation strategy that stochastically reintroduces modulesof layers, with power-efficient modules having a higher chance of being chosen.We introduce a novel technique that allows training two separate models in asingle training step whilst promoting one of them to be more power efficientthan the other while maintaining similar accuracy. The results demonstrate areduction in power consumption of ANN models by up to 29.2% without asignificant decrease in predictive performance.</description><author>Gabriel Cortês, Nuno Lourenço, Penousal Machado</author><pubDate>Wed, 31 Jan 2024 10:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17733v1</guid></item><item><title>Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models</title><link>http://arxiv.org/abs/2401.14440v2</link><description>Recent studies of the emergent capabilities of transformer-based NaturalLanguage Understanding (NLU) models have indicated that they have anunderstanding of lexical and compositional semantics. We provide evidence thatsuggests these claims should be taken with a grain of salt: we find thatstate-of-the-art Natural Language Inference (NLI) models are sensitive towardsminor semantics preserving surface-form variations, which lead to sizableinconsistent model decisions during inference. Notably, this behaviour differsfrom valid and in-depth comprehension of compositional semantics, however doesneither emerge when evaluating model accuracy on standard benchmarks nor whenprobing for syntactic, monotonic, and logically robust reasoning. We propose anovel framework to measure the extent of semantic sensitivity. To this end, weevaluate NLI models on adversarially generated examples containing minorsemantics-preserving surface-form input noise. This is achieved usingconditional text generation, with the explicit condition that the NLI modelpredicts the relationship between the original and adversarial inputs as asymmetric equivalence entailment. We systematically study the effects of thephenomenon across NLI models for $\textbf{in-}$ and $\textbf{out-of-}$ domainsettings. Our experiments show that semantic sensitivity causes performancedegradations of $12.92\%$ and $23.71\%$ average over $\textbf{in-}$ and$\textbf{out-of-}$ domain settings, respectively. We further perform ablationstudies, analysing this phenomenon across models, datasets, and variations ininference and show that semantic sensitivity can lead to major inconsistencywithin model predictions.</description><author>Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein</author><pubDate>Wed, 31 Jan 2024 10:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14440v2</guid></item><item><title>COMET: Contrastive Mean Teacher for Online Source-Free Universal Domain Adaptation</title><link>http://arxiv.org/abs/2401.17728v1</link><description>In real-world applications, there is often a domain shift from training totest data. This observation resulted in the development of test-time adaptation(TTA). It aims to adapt a pre-trained source model to the test data withoutrequiring access to the source data. Thereby, most existing works are limitedto the closed-set assumption, i.e. there is no category shift between sourceand target domain. We argue that in a realistic open-world setting a categoryshift can appear in addition to a domain shift. This means, individual sourceclasses may not appear in the target domain anymore, samples of new classes maybe part of the target domain or even both at the same time. Moreover, in manyreal-world scenarios the test data is not accessible all at once but arrivessequentially as a stream of batches demanding an immediate prediction. Hence,TTA must be applied in an online manner. To the best of our knowledge, thecombination of these aspects, i.e. online source-free universal domainadaptation (online SF-UniDA), has not been studied yet. In this paper, weintroduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario.It applies a contrastive loss to rebuild a feature space where the samples ofknown classes build distinct clusters and the samples of new classes separatewell from them. It is complemented by an entropy loss which ensures that theclassifier output has a small entropy for samples of known classes and a largeentropy for samples of new classes to be easily detected and rejected asunknown. To provide the losses with reliable pseudo labels, they are embeddedinto a mean teacher (MT) framework. We evaluate our method across two datasetsand all category shifts to set an initial benchmark for online SF-UniDA.Thereby, COMET yields state-of-the-art performance and proves to be consistentand robust across a variety of different scenarios.</description><author>Pascal Schlachter, Bin Yang</author><pubDate>Wed, 31 Jan 2024 10:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17728v1</guid></item><item><title>Cognitive TransFuser: Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction</title><link>http://arxiv.org/abs/2308.02126v2</link><description>Sensor fusion approaches for intelligent self-driving agents remain key todriving scene understanding given visual global contexts acquired from inputsensors. Specifically, for the local waypoint prediction task, single-modalitynetworks are still limited by strong dependency on the sensitivity of the inputsensor, and thus recent works therefore promote the use of multiple sensors infusion in feature level in practice. While it is well known that multiple datamodalities encourage mutual contextual exchange, it requires global 3D sceneunderstanding in real-time with minimal computation upon deployment topractical driving scenarios, thereby placing greater significance on thetraining strategy given a limited number of practically usable sensors. In thislight, we exploit carefully selected auxiliary tasks that are highly correlatedwith the target task of interest (e.g., traffic light recognition and semanticsegmentation) by fusing auxiliary task features and also using auxiliary headsfor waypoint prediction based on imitation learning. Our RGB-LIDAR-basedmulti-task feature fusion network, coined Cognitive TransFuser, augments andexceeds the baseline network by a significant margin for safer and morecomplete road navigation in the CARLA simulator. We validate the proposednetwork on the Town05 Short and Town05 Long Benchmark through extensiveexperiments, achieving up to 44.2 FPS real-time inference time.</description><author>Hwan-Soo Choi, Jongoh Jeong, Young Hoo Cho, Kuk-Jin Yoon, Jong-Hwan Kim</author><pubDate>Wed, 31 Jan 2024 10:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02126v2</guid></item><item><title>Enhancing Large Language Model with Decomposed Reasoning for Emotion Cause Pair Extraction</title><link>http://arxiv.org/abs/2401.17716v1</link><description>Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairsrepresenting emotions and their causes in a document. Existing methods tend tooverfit spurious correlations, such as positional bias in existing benchmarkdatasets, rather than capturing semantic features. Inspired by recent work, weexplore leveraging large language model (LLM) to address ECPE task withoutadditional training. Despite strong capabilities, LLMs suffer fromuncontrollable outputs, resulting in mediocre performance. To address this, weintroduce chain-of-thought to mimic human cognitive process and propose theDecomposed Emotion-Cause Chain (DECC) framework. Combining inducing inferenceand logical pruning, DECC guides LLMs to tackle ECPE task. We further enhancethe framework by incorporating in-context learning. Experiment resultsdemonstrate the strength of DECC compared to state-of-the-art supervisedfine-tuning methods. Finally, we analyze the effectiveness of each componentand the robustness of the method in various scenarios, including different LLMbases, rebalanced datasets, and multi-pair extraction.</description><author>Jialiang Wu, Yi Shen, Ziheng Zhang, Longjun Cai</author><pubDate>Wed, 31 Jan 2024 10:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17716v1</guid></item><item><title>Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models</title><link>http://arxiv.org/abs/2401.15469v2</link><description>The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolutionregional reanalysis dataset for the European domain. In recent years it hasshown significant utility across various climate-related tasks, ranging fromforecasting and climate change research to renewable energy prediction,resource management, air quality risk assessment, and the forecasting of rareevents, among others. Unfortunately, the availability of CERRA is lagging twoyears behind the current date, due to constraints in acquiring the requisiteexternal data and the intensive computational demands inherent in itsgeneration. As a solution, this paper introduces a novel method using diffusionmodels to approximate CERRA downscaling in a data-driven manner, withoutadditional informations. By leveraging the lower resolution ERA5 dataset, whichprovides boundary conditions for CERRA, we approach this as a super-resolutiontask. Focusing on wind speed around Italy, our model, trained on existing CERRAdata, shows promising results, closely mirroring original CERRA data.Validation with in-situ observations further confirms the model's accuracy inapproximating ground measurements.</description><author>Fabio Merizzi, Andrea Asperti, Stefano Colamonaco</author><pubDate>Wed, 31 Jan 2024 10:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15469v2</guid></item><item><title>Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields</title><link>http://arxiv.org/abs/2305.11588v2</link><description>Text-driven 3D scene generation is widely applicable to video gaming, filmindustry, and metaverse applications that have a large demand for 3D scenes.However, existing text-to-3D generation methods are limited to producing 3Dobjects with simple geometries and dreamlike styles that lack realism. In thiswork, we present Text2NeRF, which is able to generate a wide range of 3D sceneswith complicated geometric structures and high-fidelity textures purely from atext prompt. To this end, we adopt NeRF as the 3D representation and leverage apre-trained text-to-image diffusion model to constrain the 3D reconstruction ofthe NeRF to reflect the scene description. Specifically, we employ thediffusion model to infer the text-related image as the content prior and use amonocular depth estimation method to offer the geometric prior. Both contentand geometric priors are utilized to update the NeRF model. To guaranteetextured and geometric consistency between different views, we introduce aprogressive scene inpainting and updating strategy for novel view synthesis ofthe scene. Our method requires no additional training data but only a naturallanguage description of the scene as the input. Extensive experimentsdemonstrate that our Text2NeRF outperforms existing methods in producingphoto-realistic, multi-view consistent, and diverse 3D scenes from a variety ofnatural language prompts. Our code is available athttps://github.com/eckertzhang/Text2NeRF.</description><author>Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao</author><pubDate>Wed, 31 Jan 2024 10:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11588v2</guid></item><item><title>3D-Plotting Algorithm for Insects using YOLOv5</title><link>http://arxiv.org/abs/2401.17714v1</link><description>In ecological research, accurately collecting spatiotemporal position data isa fundamental task for understanding the behavior and ecology of insects andother organisms. In recent years, advancements in computer vision techniqueshave reached a stage of maturity where they can support, and in some cases,replace manual observation. In this study, a simple and inexpensive method formonitoring insects in three dimensions (3D) was developed so that theirbehavior could be observed automatically in experimental environments. The mainachievements of this study have been to create a 3D monitoring algorithm usinginexpensive cameras and other equipment to design an adjusting algorithm fordepth error, and to validate how our plotting algorithm is quantitativelyprecise, all of which had not been realized in conventional studies. Byoffering detailed 3D visualizations of insects, the plotting algorithm aidsresearchers in more effectively comprehending how insects interact within theirenvironments.</description><author>Daisuke Mori, Hiroki Hayami, Yasufumi Fujimoto, Isao Goto</author><pubDate>Wed, 31 Jan 2024 10:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17714v1</guid></item><item><title>Prediction of multitasking performance post-longitudinal tDCS via EEG-based functional connectivity and machine learning methods</title><link>http://arxiv.org/abs/2401.17711v1</link><description>Predicting and understanding the changes in cognitive performance, especiallyafter a longitudinal intervention, is a fundamental goal in neuroscience.Longitudinal brain stimulation-based interventions like transcranial directcurrent stimulation (tDCS) induce short-term changes in the resting membranepotential and influence cognitive processes. However, very little research hasbeen conducted on predicting these changes in cognitive performancepost-intervention. In this research, we intend to address this gap in theliterature by employing different EEG-based functional connectivity analysesand machine learning algorithms to predict changes in cognitive performance ina complex multitasking task. Forty subjects were divided into experimental andactive-control conditions. On Day 1, all subjects executed a multitasking taskwith simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjectsin the experimental condition undertook 15 minutes of 2mA anodal tDCSstimulation during task training. Subjects in the active-control conditionundertook 15 minutes of sham stimulation during task training. On Day 10, allsubjects again executed the multitasking task with EEG acquisition.Source-level functional connectivity metrics, namely phase lag index anddirected transfer function, were extracted from the EEG data on Day 1 and Day10. Various machine learning models were employed to predict changes incognitive performance. Results revealed that the multi-layer perceptron anddirected transfer function recorded a cross-validation training RMSE of 5.11%and a test RMSE of 4.97%. We discuss the implications of our results indeveloping real-time cognitive state assessors for accurately predictingcognitive performance in dynamic and complex tasks post-tDCS intervention</description><author>Akash K Rao, Shashank Uttrani, Vishnu K Menon, Darshil Shah, Arnav Bhavsar, Shubhajit Roy Chowdhury, Varun Dutt</author><pubDate>Wed, 31 Jan 2024 10:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17711v1</guid></item><item><title>ResFields: Residual Neural Fields for Spatiotemporal Signals</title><link>http://arxiv.org/abs/2309.03160v4</link><description>Neural fields, a category of neural networks trained to representhigh-frequency signals, have gained significant attention in recent years dueto their impressive performance in modeling complex 3D data, such as signeddistance (SDFs) or radiance fields (NeRFs), via a single multi-layer perceptron(MLP). However, despite the power and simplicity of representing signals withan MLP, these methods still face challenges when modeling large and complextemporal signals due to the limited capacity of MLPs. In this paper, we proposean effective approach to address this limitation by incorporating temporalresidual layers into neural fields, dubbed ResFields. It is a novel class ofnetworks specifically designed to effectively represent complex temporalsignals. We conduct a comprehensive analysis of the properties of ResFields andpropose a matrix factorization technique to reduce the number of trainableparameters and enhance generalization capabilities. Importantly, ourformulation seamlessly integrates with existing MLP-based neural fields andconsistently improves results across various challenging tasks: 2D videoapproximation, dynamic shape modeling via temporal SDFs, and dynamic NeRFreconstruction. Lastly, we demonstrate the practical utility of ResFields byshowcasing its effectiveness in capturing dynamic 3D scenes from sparse RGBDcameras of a lightweight capture system.</description><author>Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, Siyu Tang</author><pubDate>Wed, 31 Jan 2024 10:02:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03160v4</guid></item><item><title>Aesthetic Preference Prediction in Interior Design: Fuzzy Approach</title><link>http://arxiv.org/abs/2401.17710v1</link><description>Interior design is all about creating spaces that look and feel good.However, the subjective nature of aesthetic preferences presents a significantchallenge in defining and quantifying what makes an interior design visuallyappealing. The current paper addresses this gap by introducing a novelmethodology for quantifying and predicting aesthetic preferences in interiordesign. Our study combines fuzzy logic with image processing techniques. Wecollected a dataset of interior design images from social media platforms,focusing on essential visual attributes such as color harmony, lightness, andcomplexity. We integrate these features using weighted average to compute ageneral aesthetic score. Our approach considers individual color preferences incalculating the overall aesthetic preference. We initially gather user ratingsfor primary colors like red, brown, and others to understand their preferences.Then, we use the pixel count of the top five dominant colors in the image toget the color scheme preference. The color scheme preference and the aestheticscore are then passed as inputs to the fuzzy inference system to calculate anoverall preference score. This score represents a comprehensive measure of theuser's preference for a particular interior design, considering their colorchoices and general aesthetic appeal. We used the 2AFC (Two-Alternative ForcedChoice) method to validate our methodology, achieving a notable hit rate of0.7. This study can help designers and professionals better understand and meetpeople's interior design preferences, especially in a world that relies heavilyon digital media.</description><author>Ayana Adilova, Pakizar Shamoi</author><pubDate>Wed, 31 Jan 2024 09:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17710v1</guid></item><item><title>Do self-supervised speech and language models extract similar representations as human brain?</title><link>http://arxiv.org/abs/2310.04645v2</link><description>Speech and language models trained through self-supervised learning (SSL)demonstrate strong alignment with brain activity during speech and languageperception. However, given their distinct training modalities, it remainsunclear whether they correlate with the same neural aspects. We directlyaddress this question by evaluating the brain prediction performance of tworepresentative SSL models, Wav2Vec2.0 and GPT-2, designed for speech andlanguage tasks. Our findings reveal that both models accurately predict speechresponses in the auditory cortex, with a significant correlation between theirbrain predictions. Notably, shared speech contextual information betweenWav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brainactivity, surpassing static semantic and lower-level acoustic-phoneticinformation. These results underscore the convergence of speech contextualrepresentations in SSL models and their alignment with the neural networkunderlying speech perception, offering valuable insights into both SSL modelsand the neural basis of speech and language processing.</description><author>Peili Chen, Linyang He, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li</author><pubDate>Wed, 31 Jan 2024 09:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04645v2</guid></item><item><title>What Is Fairness? On the Role of Protected Attributes and Fictitious Worlds</title><link>http://arxiv.org/abs/2205.09622v4</link><description>A growing body of literature in fairness-aware ML (fairML) aspires tomitigate machine learning (ML)-related unfairness in automated decision-making(ADM) by defining metrics that measure fairness of an ML model and by proposingmethods that ensure that trained ML models achieve low values in those metrics.However, the underlying concept of fairness, i.e., the question of whatfairness is, is rarely discussed, leaving a considerable gap between centuriesof philosophical discussion and recent adoption of the concept in the MLcommunity. In this work, we try to bridge this gap by formalizing a consistentconcept of fairness and by translating the philosophical considerations into aformal framework for the training and evaluation of ML models in ADM systems.We derive that fairness problems can already arise without the presence ofprotected attributes (PAs), pointing out that fairness and predictiveperformance are not irreconcilable counterparts, but rather that the latter isnecessary to achieve the former. Moreover, we argue why and how causalconsiderations are necessary when assessing fairness in the presence of PAs byproposing a fictitious, normatively desired (FiND) world where the PAs have nocausal effects. In practice, this FiND world must be approximated by a warpedworld, for which the causal effects of the PAs must be removed from thereal-world data. Eventually, we achieve greater linguistic clarity for thediscussion of fairML. We propose first algorithms for practical applicationsand present illustrative experiments on COMPAS data.</description><author>Ludwig Bothmann, Kristina Peters, Bernd Bischl</author><pubDate>Wed, 31 Jan 2024 09:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09622v4</guid></item><item><title>Predicting suicidal behavior among Indian adults using childhood trauma, mental health questionnaires and machine learning cascade ensembles</title><link>http://arxiv.org/abs/2401.17705v1</link><description>Among young adults, suicide is India's leading cause of death, accounting foran alarming national suicide rate of around 16%. In recent years, machinelearning algorithms have emerged to predict suicidal behavior using variousbehavioral traits. But to date, the efficacy of machine learning algorithms inpredicting suicidal behavior in the Indian context has not been explored inliterature. In this study, different machine learning algorithms and ensembleswere developed to predict suicide behavior based on childhood trauma, differentmental health parameters, and other behavioral factors. The dataset wasacquired from 391 individuals from a wellness center in India. Informationregarding their childhood trauma, psychological wellness, and other mentalhealth issues was acquired through standardized questionnaires. Resultsrevealed that cascade ensemble learning methods using a support vector machine,decision trees, and random forest were able to classify suicidal behavior withan accuracy of 95.04% using data from childhood trauma and mental healthquestionnaires. The study highlights the potential of using these machinelearning ensembles to identify individuals with suicidal tendencies so thattargeted interinterventions could be provided efficiently.</description><author>Akash K Rao, Gunjan Y Trivedi, Riri G Trivedi, Anshika Bajpai, Gajraj Singh Chauhan, Vishnu K Menon, Kathirvel Soundappan, Hemalatha Ramani, Neha Pandya, Varun Dutt</author><pubDate>Wed, 31 Jan 2024 09:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17705v1</guid></item><item><title>WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts</title><link>http://arxiv.org/abs/2401.17703v1</link><description>The Winograd Schema Challenge (WSC) serves as a prominent benchmark forevaluating machine understanding. While Large Language Models (LLMs) excel atanswering WSC questions, their ability to generate such questions remains lessexplored. In this work, we propose Tree-of-Experts (ToE), a novel promptingmethod which enhances the generation of WSC instances (50% valid cases vs. 10%in recent methods). Using this approach, we introduce WSC+, a novel datasetcomprising 3,026 LLM-generated sentences. Notably, we extend the WSC frameworkby incorporating new 'ambiguous' and 'offensive' categories, providing a deeperinsight into model overconfidence and bias. Our analysis reveals nuances ingeneration-evaluation consistency, suggesting that LLMs may not alwaysoutperform in evaluating their own generated questions when compared to thosecrafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves anaccuracy of 68.7%, significantly below the human benchmark of 95.1%.</description><author>Pardis Sadat Zahraei, Ali Emami</author><pubDate>Wed, 31 Jan 2024 09:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17703v1</guid></item><item><title>Classification of executive functioning performance post-longitudinal tDCS using functional connectivity and machine learning methods</title><link>http://arxiv.org/abs/2401.17700v1</link><description>Executive functioning is a cognitive process that enables humans to plan,organize, and regulate their behavior in a goal-directed manner. Understandingand classifying the changes in executive functioning after longitudinalinterventions (like transcranial direct current stimulation (tDCS)) has notbeen explored in the literature. This study employs functional connectivity andmachine learning algorithms to classify executive functioning performancepost-tDCS. Fifty subjects were divided into experimental and placebo controlgroups. EEG data was collected while subjects performed an executivefunctioning task on Day 1. The experimental group received tDCS during tasktraining from Day 2 to Day 8, while the control group received sham tDCS. OnDay 10, subjects repeated the tasks specified on Day 1. Different functionalconnectivity metrics were extracted from EEG data and eventually used forclassifying executive functioning performance using different machine learningalgorithms. Results revealed that a novel combination of partial directedcoherence and multi-layer perceptron (along with recursive feature elimination)resulted in a high classification accuracy of 95.44%. We discuss theimplications of our results in developing real-time neurofeedback systems forassessing and enhancing executive functioning performance post-tDCSadministration.</description><author>Akash K Rao, Vishnu K Menon, Shashank Uttrani, Ayushman Dixit, Dipanshu Verma, Varun Dutt</author><pubDate>Wed, 31 Jan 2024 09:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17700v1</guid></item><item><title>Unified Physical-Digital Face Attack Detection</title><link>http://arxiv.org/abs/2401.17699v1</link><description>Face Recognition (FR) systems can suffer from physical (i.e., print photo)and digital (i.e., DeepFake) attacks. However, previous related work rarelyconsiders both situations at the same time. This implies the deployment ofmultiple models and thus more computational burden. The main reasons for thislack of an integrated model are caused by two factors: (1) The lack of adataset including both physical and digital attacks with ID consistency whichmeans the same ID covers the real face and all attack types; (2) Given thelarge intra-class variance between these two attacks, it is difficult to learna compact feature space to detect both attacks simultaneously. To address theseissues, we collect a Unified physical-digital Attack dataset, calledUniAttackData. The dataset consists of $1,800$ participations of 2 and 12physical and digital attacks, respectively, resulting in a total of 29,706videos. Then, we propose a Unified Attack Detection framework based onVision-Language Models (VLMs), namely UniAttackDetection, which includes threemain modules: the Teacher-Student Prompts (TSP) module, focused on acquiringunified and specific knowledge respectively; the Unified Knowledge Mining (UKM)module, designed to capture a comprehensive feature space; and the Sample-LevelPrompt Interaction (SLPI) module, aimed at grasping sample-level semantics.These three modules seamlessly form a robust unified attack detectionframework. Extensive experiments on UniAttackData and three other datasetsdemonstrate the superiority of our approach for unified face attack detection.</description><author>Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, Zhen Lei</author><pubDate>Wed, 31 Jan 2024 09:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17699v1</guid></item><item><title>Datacube segmentation via Deep Spectral Clustering</title><link>http://arxiv.org/abs/2401.17695v1</link><description>Extended Vision techniques are ubiquitous in physics. However, the data cubessteaming from such analysis often pose a challenge in their interpretation, dueto the intrinsic difficulty in discerning the relevant information from thespectra composing the data cube. Furthermore, the huge dimensionality of data cube spectra poses a complextask in its statistical interpretation; nevertheless, this complexity containsa massive amount of statistical information that can be exploited in anunsupervised manner to outline some essential properties of the case study athand, e.g.~it is possible to obtain an image segmentation via (deep) clusteringof data-cube's spectra, performed in a suitably defined low-dimensionalembedding space. To tackle this topic, we explore the possibility of applying unsupervisedclustering methods in encoded space, i.e. perform deep clustering on thespectral properties of datacube pixels. A statistical dimensional reduction isperformed by an ad hoc trained (Variational) AutoEncoder, in charge of mappingspectra into lower dimensional metric spaces, while the clustering process isperformed by a (learnable) iterative K-Means clustering algorithm. We apply this technique to two different use cases, of different physicalorigins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data onpictorial artworks, and a dataset of simulated astrophysical observations.</description><author>Alessandro Bombini, Fernando García-Avello Bofías, Caterina Bracci, Michele Ginolfi, Chiara Ruberto</author><pubDate>Wed, 31 Jan 2024 09:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17695v1</guid></item><item><title>GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality Assessment</title><link>http://arxiv.org/abs/2306.05658v2</link><description>Nowadays, most 3D model quality assessment (3DQA) methods have been aimed atimproving performance. However, little attention has been paid to thecomputational cost and inference time required for practical applications.Model-based 3DQA methods extract features directly from the 3D models, whichare characterized by their high degree of complexity. As a result, manyresearchers are inclined towards utilizing projection-based 3DQA methods.Nevertheless, previous projection-based 3DQA methods directly extract featuresfrom multi-projections to ensure quality prediction accuracy, which calls formore resource consumption and inevitably leads to inefficiency. Thus in thispaper, we address this challenge by proposing a no-reference (NR)projection-based \textit{\underline{G}rid \underline{M}ini-patch\underline{S}ampling \underline{3D} Model \underline{Q}uality\underline{A}ssessment (GMS-3DQA)} method. The projection images are renderedfrom six perpendicular viewpoints of the 3D model to cover sufficient qualityinformation. To reduce redundancy and inference resources, we propose amulti-projection grid mini-patch sampling strategy (MP-GMS), which samples gridmini-patches from the multi-projections and forms the sampled grid mini-patchesinto one quality mini-patch map (QMM). The Swin-Transformer tiny backbone isthen used to extract quality-aware features from the QMMs. The experimentalresults show that the proposed GMS-3DQA outperforms existing state-of-the-artNR-3DQA methods on the point cloud quality assessment databases. The efficiencyanalysis reveals that the proposed GMS-3DQA requires far less computationalresources and inference time than other 3DQA competitors. The code will beavailable at https://github.com/zzc-1998/GMS-3DQA.</description><author>Zicheng Zhang, Wei Sun, Houning Wu, Yingjie Zhou, Chunyi Li, Xiongkuo Min, Guangtao Zhai, Weisi Lin</author><pubDate>Wed, 31 Jan 2024 09:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05658v2</guid></item><item><title>Mitigating the Problem of Strong Priors in LMs with Context Extrapolation</title><link>http://arxiv.org/abs/2401.17692v1</link><description>Language models (LMs) have become important tools in a variety ofapplications, from data processing to the creation of instruction-followingassistants. But despite their advantages, LMs have certain idiosyncraticlimitations such as the problem of `strong priors', where a model learns tooutput typical continuations in response to certain, usually local, portions ofthe input regardless of any earlier instructions. For example, prompt injectionattacks can induce models to ignore explicit directives. In some cases, largermodels have been shown to be more susceptible to these problems than similarsmaller models, an example of the phenomenon of `inverse scaling'. We develop anew technique for mitigating the problem of strong priors: we take the originalset of instructions, produce a weakened version of the original prompt that iseven more susceptible to the strong priors problem, and then extrapolate thecontinuation away from the weakened prompt. This lets us infer how the modelwould continue a hypothetical strengthened set of instructions. Our techniqueconceptualises LMs as mixture models which combine a family of data generationprocesses, reinforcing the desired elements of the mixture. Our approach worksat inference time, removing any need for retraining. We apply it to elevenmodels including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and findimprovements in 41/44. Across all 44 combinations the median increase inproportion of tasks completed is 40%.</description><author>Raymond Douglas, Andis Draguns, Tomáš Gavenčiak</author><pubDate>Wed, 31 Jan 2024 09:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17692v1</guid></item><item><title>EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning</title><link>http://arxiv.org/abs/2401.17690v1</link><description>We propose EnCLAP, a novel framework for automated audio captioning. EnCLAPemploys two acoustic representation models, EnCodec and CLAP, along with apretrained language model, BART. We also introduce a new training objectivecalled masked codec modeling that improves acoustic awareness of the pretrainedlanguage model. Experimental results on AudioCaps and Clotho demonstrate thatour model surpasses the performance of baseline models. Source code will beavailable at https://github.com/jaeyeonkim99/EnCLAP . An online demo isavailable at https://huggingface.co/spaces/enclap-team/enclap .</description><author>Jaeyeon Kim, Jaeyoon Jung, Jinjoo Lee, Sang Hoon Woo</author><pubDate>Wed, 31 Jan 2024 09:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17690v1</guid></item><item><title>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2401.17686v1</link><description>Recent advancements have significantly augmented the reasoning capabilitiesof Large Language Models (LLMs) through various methodologies, especiallychain-of-thought (CoT) reasoning. However, previous methods fail to addressreasoning errors in intermediate steps, leading to accumulative errors.In thispaper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoTand deductive reasoning with step-wise beam search for LLMs. Our approachdeploys a verifier, verifying the deducibility of a reasoning step and itspremises, thus alleviating the error accumulation. Furthermore, we introduce ascalable and labor-free data construction method to amplify our model'sverification capabilities. Extensive experiments demonstrate that our approachsignificantly enhances the base performance of LLMs of various scales (7B, 13B,70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres,including arithmetic, commonsense, and symbolic. Moreover, our analysis provesDBS's capability of detecting diverse and subtle reasoning errors androbustness on different model scales.</description><author>Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su</author><pubDate>Wed, 31 Jan 2024 09:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17686v1</guid></item><item><title>Convergence analysis of t-SNE as a gradient flow for point cloud on a manifold</title><link>http://arxiv.org/abs/2401.17675v1</link><description>We present a theoretical foundation regarding the boundedness of the t-SNEalgorithm. t-SNE employs gradient descent iteration with Kullback-Leibler (KL)divergence as the objective function, aiming to identify a set of points thatclosely resemble the original data points in a high-dimensional space,minimizing KL divergence. Investigating t-SNE properties such as perplexity andaffinity under a weak convergence assumption on the sampled dataset, we examinethe behavior of points generated by t-SNE under continuous gradient flow.Demonstrating that points generated by t-SNE remain bounded, we leverage thisinsight to establish the existence of a minimizer for KL divergence.</description><author>Seonghyeon Jeong, Hau-Tieng Wu</author><pubDate>Wed, 31 Jan 2024 08:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17675v1</guid></item><item><title>Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain</title><link>http://arxiv.org/abs/2401.17671v1</link><description>Recent advancements in artificial intelligence have sparked interest in theparallels between large language models (LLMs) and human neural processing,particularly in language comprehension. While prior research has establishedsimilarities in the representation of LLMs and the brain, the underlyingcomputational principles that cause this convergence, especially in the contextof evolving LLMs, remain elusive. Here, we examined a diverse selection ofhigh-performance LLMs with similar parameter sizes to investigate the factorscontributing to their alignment with the brain's language processingmechanisms. We find that as LLMs achieve higher performance on benchmark tasks,they not only become more brain-like as measured by higher performance whenpredicting neural responses from LLM embeddings, but also their hierarchicalfeature extraction pathways map more closely onto the brain's while using fewerlayers to do the same encoding. We also compare the feature extraction pathwaysof the LLMs to each other and identify new ways in which high-performing modelshave converged toward similar hierarchical processing mechanisms. Finally, weshow the importance of contextual information in improving model performanceand brain similarity. Our findings reveal the converging aspects of languageprocessing in the brain and LLMs and offer new directions for developing modelsthat align more closely with human cognitive processing.</description><author>Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima Mesgarani</author><pubDate>Wed, 31 Jan 2024 08:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17671v1</guid></item></channel></rss>