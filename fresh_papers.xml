<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 02 Apr 2024 06:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TinyLLM: Learning a Small Student from Multiple Large Language Models</title><link>http://arxiv.org/abs/2402.04616v2</link><description>Transferring the reasoning capability from stronger large language models(LLMs) to smaller ones has been quite appealing, as smaller LLMs are moreflexible to deploy with less expense. Among the existing solutions, knowledgedistillation stands out due to its outstanding efficiency and generalization.However, existing methods suffer from several drawbacks, including limitedknowledge diversity and the lack of rich contextual information. To solve theproblems and facilitate the learning of compact language models, we proposeTinyLLM, a new knowledge distillation paradigm to learn a small student LLMfrom multiple large teacher LLMs. In particular, we encourage the student LLMto not only generate the correct answers but also understand the rationalesbehind these answers. Given that different LLMs possess diverse reasoningskills, we guide the student model to assimilate knowledge from various teacherLLMs. We further introduce an in-context example generator and ateacher-forcing Chain-of-Thought strategy to ensure that the rationales areaccurate and grounded in contextually appropriate scenarios. Extensiveexperiments on six datasets across two reasoning tasks demonstrate thesuperiority of our method. Results show that TinyLLM can outperform largeteacher LLMs significantly, despite a considerably smaller model size.</description><author>Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla</author><pubDate>Mon, 01 Apr 2024 02:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04616v2</guid></item><item><title>Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting</title><link>http://arxiv.org/abs/2403.19898v2</link><description>Denoising diffusion probabilistic models for image inpainting aim to add thenoise to the texture of image during the forward process and recover maskedregions with unmasked ones of the texture via the reverse denoisingprocess.Despite the meaningful semantics generation,the existing arts sufferfrom the semantic discrepancy between masked and unmasked regions, since thesemantically dense unmasked texture fails to be completely degraded while themasked regions turn to the pure noise in diffusion process,leading to the largediscrepancy between them. In this paper,we aim to answer how unmasked semanticsguide texture denoising process;together with how to tackle the semanticdiscrepancy,to facilitate the consistent and meaningful semantics generation.To this end,we propose a novel structure-guided diffusion model namedStrDiffusion,to reformulate the conventional texture denoising process understructure guidance to derive a simplified denoising objective for imageinpainting,while revealing:1)the semantically sparse structure is beneficial totackle semantic discrepancy in early stage, while dense texture generatesreasonable semantics in late stage;2)the semantics from unmasked regionsessentially offer the time-dependent structure guidance for the texturedenoising process,benefiting from the time-dependent sparsity of the structuresemantics.For the denoising process,a structure-guided neural network istrained to estimate the simplified denoising objective by exploiting theconsistency of the denoised structure between masked and unmaskedregions.Besides,we devise an adaptive resampling strategy as a formal criterionas whether structure is competent to guide the texture denoising process,whileregulate their semantic correlations.Extensive experiments validate the meritsof StrDiffusion over the state-of-the-arts.Our code is available athttps://github.com/htyjers/StrDiffusion.</description><author>Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui</author><pubDate>Mon, 01 Apr 2024 02:27:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19898v2</guid></item><item><title>An Extensible Framework for Open Heterogeneous Collaborative Perception</title><link>http://arxiv.org/abs/2401.13964v3</link><description>Collaborative perception aims to mitigate the limitations of single-agentperception, such as occlusions, by facilitating data exchange among multipleagents. However, most current works consider a homogeneous scenario where allagents use identity sensors and perception models. In reality, heterogeneousagent types may continually emerge and inevitably face a domain gap whencollaborating with existing agents. In this paper, we introduce a new openheterogeneous problem: how to accommodate continually emerging newheterogeneous agent types into collaborative perception, while ensuring highperception performance and low integration cost? To address this problem, wepropose HEterogeneous ALliance (HEAL), a novel extensible collaborativeperception framework. HEAL first establishes a unified feature space withinitial agents via a novel multi-scale foreground-aware Pyramid Fusion network.When heterogeneous new agents emerge with previously unseen modalities ormodels, we align them to the established unified space with an innovativebackward alignment. This step only involves individual training on the newagent type, thus presenting extremely low training costs and highextensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a newlarge-scale dataset with more diverse sensor types. Extensive experiments onOPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods inperformance while reducing the training parameters by 91.5% when integrating 3new agent types. We further implement a comprehensive codebase at:https://github.com/yifanlu0227/HEAL</description><author>Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Yanfeng Wang, Siheng Chen</author><pubDate>Mon, 01 Apr 2024 02:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13964v3</guid></item><item><title>WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar</title><link>http://arxiv.org/abs/2403.12686v2</link><description>The perception of waterways based on human intent is significant forautonomous navigation and operations of Unmanned Surface Vehicles (USVs) inwater environments. Inspired by visual grounding, we introduce WaterVG, thefirst visual grounding dataset designed for USV-based waterway perception basedon human prompts. WaterVG encompasses prompts describing multiple targets, withannotations at the instance level including bounding boxes and masks. Notably,WaterVG includes 11,568 samples with 34,987 referred targets, whose promptsintegrates both visual and radar characteristics. The pattern of text-guidedtwo sensors equips a finer granularity of text prompts with visual and radarfeatures of referred targets. Moreover, we propose a low-power visual groundingmodel, Potamoi, which is a multi-task model with a well-designed PhasedHeterogeneous Modality Fusion (PHMF) mode, including Adaptive Radar Weighting(ARW) and Multi-Head Slim Cross Attention (MHSCA). Exactly, ARW extractsrequired radar features to fuse with vision for prompt alignment. MHSCA is anefficient fusion module with a remarkably small parameter count and FLOPs,elegantly fusing scenario context captured by two sensors with linguisticfeatures, which performs expressively on visual grounding tasks. Comprehensiveexperiments and evaluations have been conducted on WaterVG, where our Potamoiarchives state-of-the-art performances compared with counterparts.</description><author>Runwei Guan, Liye Jia, Fengyufan Yang, Shanliang Yao, Erick Purwanto, Xiaohui Zhu, Eng Gee Lim, Jeremy Smith, Ka Lok Man, Xuming Hu, Yutao Yue</author><pubDate>Mon, 01 Apr 2024 02:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12686v2</guid></item><item><title>PACE: A Large-Scale Dataset with Pose Annotations in Cluttered Environments</title><link>http://arxiv.org/abs/2312.15130v2</link><description>Pose estimation is a crucial task in computer vision and robotics, enablingthe tracking and manipulation of objects in images or videos. While severaldatasets exist for pose estimation, there is a lack of large-scale datasetsspecifically focusing on cluttered scenes with occlusions. We introduce PACE(Pose Annotations in Cluttered Environments), a large-scale benchmark designedto advance the development and evaluation of pose estimation methods incluttered scenarios. PACE consists of 54,945 frames with 257,673 annotationsacross 300 videos, covering 576 objects from 44 categories and featuring a mixof rigid and articulated items in cluttered scenes. To annotate the real-worlddata efficiently, we developed an innovative annotation system utilizing acalibrated 3-camera setup. We test state-of-the-art algorithms in PACE alongtwo tracks: pose estimation, and object pose tracking, revealing thebenchmark's challenges and research opportunities. Our code and data isavailable on https://github.com/qq456cvb/PACE.</description><author>Yang You, Kai Xiong, Zhening Yang, Zhengxiang Huang, Junwei Zhou, Ruoxi Shi, Zhou Fang, Adam W. Harley, Leonidas Guibas, Cewu Lu</author><pubDate>Mon, 01 Apr 2024 01:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15130v2</guid></item><item><title>Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition</title><link>http://arxiv.org/abs/2403.07953v2</link><description>Exploiting sparsity in deep neural networks (DNNs) has been a promising areato meet the growing computation need of modern DNNs. However, in practice,sparse DNN acceleration still faces a key challenge. To minimize the overheadof sparse acceleration, hardware designers have proposed structured sparsehardware support recently, which provides limited flexibility and requiresextra model fine-tuning. Moreover, any sparse model fine-tuned for certainstructured sparse hardware cannot be accelerated by other structured hardware.To bridge the gap between sparse DNN models and hardware, this paper proposestensor approximation via structured decomposition (TASD), which leverages thedistributive property in linear algebra to turn any sparse tensor into a seriesof structured sparse tensors. Next, we develop a software framework, TASDER, toaccelerate DNNs by searching layer-wise, high-quality structured decompositionfor both weight and activation tensors so that they can be accelerated by anysystems with structured sparse hardware support. Evaluation results show that,by exploiting prior structured sparse hardware baselines, our method canaccelerate off-the-shelf dense and sparse DNNs without fine-tuning and improvesenergy-delay-product by up to 83% and 74% on average.</description><author>Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W. Keckler, Tushar Krishna</author><pubDate>Mon, 01 Apr 2024 00:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07953v2</guid></item><item><title>Multi-Review Fusion-in-Context</title><link>http://arxiv.org/abs/2403.15351v2</link><description>Grounded text generation, encompassing tasks such as long-formquestion-answering and summarization, necessitates both content selection andcontent consolidation. Current end-to-end methods are difficult to control andinterpret due to their opaqueness. Accordingly, recent works have proposed amodular approach, with separate components for each step. Specifically, wefocus on the second subtask, of generating coherent text given pre-selectedcontent in a multi-document setting. Concretely, we formalize Fusion-in-Context(FiC) as a standalone task, whose input consists of source texts withhighlighted spans of targeted content. A model then needs to generate acoherent passage that includes all and only the target information. Our workincludes the development of a curated dataset of 1000 instances in the reviewsdomain, alongside a novel evaluation framework for assessing the faithfulnessand coverage of highlights, which strongly correlate to human judgment. Severalbaseline models exhibit promising outcomes and provide insightful analyses.This study lays the groundwork for further exploration of modular textgeneration in the multi-document setting, offering potential improvements inthe quality and reliability of generated content. Our benchmark, FuseReviews,including the dataset, evaluation framework, and designated leaderboard, can befound at https://fusereviews.github.io/.</description><author>Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan</author><pubDate>Mon, 01 Apr 2024 00:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15351v2</guid></item><item><title>Riemannian Laplace Approximation with the Fisher Metric</title><link>http://arxiv.org/abs/2311.02766v4</link><description>Laplace's method approximates a target density with a Gaussian distributionat its mode. It is computationally efficient and asymptotically exact forBayesian inference due to the Bernstein-von Mises theorem, but for complextargets and finite-data posteriors it is often too crude an approximation. Arecent generalization of the Laplace Approximation transforms the Gaussianapproximation according to a chosen Riemannian geometry providing a richerapproximation family, while still retaining computational efficiency. However,as shown here, its properties depend heavily on the chosen metric, indeed themetric adopted in previous work results in approximations that are overlynarrow as well as being biased even at the limit of infinite data. We correctthis shortcoming by developing the approximation family further, deriving twoalternative variants that are exact at the limit of infinite data, extendingthe theoretical analysis of the method, and demonstrating practicalimprovements in a range of experiments.</description><author>Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</author><pubDate>Mon, 01 Apr 2024 00:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02766v4</guid></item><item><title>Investigating Recurrent Transformers with Dynamic Halt</title><link>http://arxiv.org/abs/2402.00976v2</link><description>In this paper, we study the inductive biases of two major approaches toaugmenting Transformers with a recurrent mechanism - (1) the approach ofincorporating a depth-wise recurrence similar to Universal Transformers; and(2) the approach of incorporating a chunk-wise temporal recurrence likeTemporal Latent Bottleneck. Furthermore, we propose and investigate novel waysto extend and combine the above methods - for example, we propose a globalmean-based dynamic halting mechanism for Universal Transformer and anaugmentation of Temporal Latent Bottleneck with elements from UniversalTransformer. We compare the models and probe their inductive biases in severaldiagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling,ListOps, and Logical Inference.</description><author>Jishnu Ray Chowdhury, Cornelia Caragea</author><pubDate>Mon, 01 Apr 2024 00:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00976v2</guid></item><item><title>Efficient Post-training Quantization with FP8 Formats</title><link>http://arxiv.org/abs/2309.14592v2</link><description>Recent advances in deep learning methods such as LLMs and Diffusion modelshave created a need for improved quantization methods that can meet thecomputational demands of these modern architectures while maintaining accuracy.Towards this goal, we study the advantages of FP8 data formats forpost-training quantization across 75 unique network architectures covering awide range of tasks, including machine translation, language modeling, textgeneration, image classification, generation, and segmentation. We examinethree different FP8 representations (E5M2, E4M3, and E3M4) to study the effectsof varying degrees of trade-off between dynamic range and precision on modelaccuracy. Based on our extensive study, we developed a quantization workflowthat generalizes across different network architectures. Our empirical resultsshow that FP8 formats outperform INT8 in multiple aspects, including workloadcoverage (92.64% vs. 65.87%), model accuracy and suitability for a broaderrange of operations. Furthermore, our findings suggest that E4M3 is bettersuited for NLP models, whereas E3M4 performs marginally better than E4M3 oncomputer vision tasks. The code is publicly available on Intel NeuralCompressor: https://github.com/intel/neural-compressor.</description><author>Haihao Shen, Naveen Mellempudi, Xin He, Qun Gao, Chang Wang, Mengni Wang</author><pubDate>Mon, 01 Apr 2024 00:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14592v2</guid></item><item><title>Language-only Efficient Training of Zero-shot Composed Image Retrieval</title><link>http://arxiv.org/abs/2312.01998v2</link><description>Composed image retrieval (CIR) task takes a composed query of image and text,aiming to search relative images for both conditions. Conventional CIRapproaches need a training dataset composed of triplets of query image, querytext, and target image, which is very expensive to collect. Several recentworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issuewithout using pre-collected triplets. However, the existing ZS-CIR methods showlimited backbone scalability and generalizability due to the lack of diversityof the input texts during training. We propose a novel CIR framework, onlyusing language for its training. Our LinCIR (Language-only training for CIR)can be trained only with text datasets by a novel self-supervision namedself-masking projection (SMP). We project the text latent embedding to thetoken embedding space and construct a new text by replacing the keyword tokensof the original text. Then, we let the new and original texts have the samelatent embedding vector. With this simple strategy, LinCIR is surprisinglyefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in48 minutes and shows the best ZS-CIR performances on four different CIRbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervisedmethod on FashionIQ. Code is available at https://github.com/navervision/lincir</description><author>Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, Sangdoo Yun</author><pubDate>Sun, 31 Mar 2024 23:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01998v2</guid></item><item><title>Safer-Instruct: Aligning Language Models with Automated Preference Data</title><link>http://arxiv.org/abs/2311.08685v3</link><description>Reinforcement learning from human feedback (RLHF) is a vital strategy forenhancing model capability in language models. However, annotating preferencedata for RLHF is a resource-intensive and creativity-demanding process, whileexisting automatic generation methods face limitations in data diversity andquality. In response, we present Safer-Instruct, a novel pipeline forautomatically constructing large-scale preference data. Our approach leveragesreversed instruction tuning, instruction induction, and expert model evaluationto efficiently generate high-quality preference data without human annotators.To verify the effectiveness of Safer-Instruct, we apply the pipeline toconstruct a safety preference dataset as a case study. Finetuning an Alpacamodel on this synthetic dataset not only demonstrates improved harmlessness butalso outperforms models fine-tuned on human-annotated safety preference data,all the while maintaining a competitive edge in downstream tasks. Importantly,our Safer-Instruct framework is versatile and can be applied to generatepreference data across various domains, extending its utility beyond safetypreferences. It addresses the challenges in preference data acquisition andadvances the development of more capable and responsible AI systems. Fordataset and code implementation, seehttps://github.com/uscnlp-lime/safer-instruct</description><author>Taiwei Shi, Kai Chen, Jieyu Zhao</author><pubDate>Sun, 31 Mar 2024 23:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08685v3</guid></item><item><title>Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework</title><link>http://arxiv.org/abs/2311.13958v2</link><description>Recently, numerous tensor singular value decomposition (t-SVD)-based tensorrecovery methods have shown promise in processing visual data, such as colorimages and videos. However, these methods often suffer from severe performancedegradation when confronted with tensor data exhibiting non-smooth changes. Ithas been commonly observed in real-world scenarios but ignored by thetraditional t-SVD-based methods. In this work, we introduce a novel tensorrecovery model with a learnable tensor nuclear norm to address such achallenge. We develop a new optimization algorithm named the AlternatingProximal Multiplier Method (APMM) to iteratively solve the proposed tensorcompletion model. Theoretical analysis demonstrates the convergence of theproposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimizationproblem. In addition, we propose a multi-objective tensor recovery frameworkbased on APMM to efficiently explore the correlations of tensor data across itsvarious dimensions, providing a new perspective on extending the t-SVD-basedmethod to higher-order tensor cases. Numerical experiments demonstrated theeffectiveness of the proposed method in tensor completion.</description><author>Jingjing Zheng, Wanglong Lu, Wenzhe Wang, Yankai Cao, Xiaoqin Zhang, Xianta Jiang</author><pubDate>Sun, 31 Mar 2024 23:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13958v2</guid></item><item><title>Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis</title><link>http://arxiv.org/abs/2403.11487v2</link><description>We present a novel approach to automatically synthesize "wayfindinginstructions" for an embodied robot agent. In contrast to prior approaches thatare heavily reliant on human-annotated datasets designed exclusively forspecific simulation platforms, our algorithm uses in-context learning tocondition an LLM to generate instructions using just a few references. Using anLLM-based Visual Question Answering strategy, we gather detailed informationabout the environment which is used by the LLM for instruction synthesis. Weimplement our approach on multiple simulation platforms including Matterport3D,AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.We subjectively evaluate our approach via a user study and observe that 83.3%of users find the synthesized instructions accurately capture the details ofthe environment and show characteristics similar to those of human-generatedinstructions. Further, we conduct zero-shot navigation with multiple approacheson the REVERIE dataset using the generated instructions, and observe very closecorrelation with the baseline on standard success metrics (&lt; 1% change in SR),quantifying the viability of generated instructions in replacinghuman-annotated data. We finally discuss the applicability of our approach inenabling a generalizable evaluation of embodied navigation policies. To thebest of our knowledge, ours is the first LLM-driven approach capable ofgenerating "human-like" instructions in a platform-agnostic manner, withouttraining.</description><author>Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha</author><pubDate>Sun, 31 Mar 2024 23:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11487v2</guid></item><item><title>ElasticDiffusion: Training-free Arbitrary Size Image Generation through Global-Local Content Separation</title><link>http://arxiv.org/abs/2311.18822v2</link><description>Diffusion models have revolutionized image generation in recent years, yetthey are still limited to a few sizes and aspect ratios. We proposeElasticDiffusion, a novel training-free decoding method that enables pretrainedtext-to-image diffusion models to generate images with various sizes.ElasticDiffusion attempts to decouple the generation trajectory of a pretrainedmodel into local and global signals. The local signal controls low-level pixelinformation and can be estimated on local patches, while the global signal isused to maintain overall structural consistency and is estimated with areference image. We test our method on CelebA-HQ (faces) and LAION-COCO(objects/indoor/outdoor scenes). Our experiments and qualitative results showsuperior image coherence quality across aspect ratios compared toMultiDiffusion and the standard decoding strategy of Stable Diffusion. Projectpage: https://elasticdiffusion.github.io/</description><author>Moayed Haji-Ali, Guha Balakrishnan, Vicente Ordonez</author><pubDate>Sun, 31 Mar 2024 22:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18822v2</guid></item><item><title>DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training</title><link>http://arxiv.org/abs/2310.03294v2</link><description>FlashAttention (Dao, 2023) effectively reduces the quadratic peak memoryusage to linear in training transformer-based large language models (LLMs) on asingle GPU. In this paper, we introduce DISTFLASHATTN, a distributedmemory-efficient attention mechanism optimized for long-context LLMs training.We propose three key techniques: token-level workload balancing, overlappingkey-value communication, and a rematerialization-aware gradient checkpointingalgorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequencelengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 -5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 -2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67xand 1.26 - 1.88x speedup compared to recent Ring Attention andDeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.</description><author>Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Xuezhe Ma, Ion Stoica, Joseph E. Gonzalez, Hao Zhang</author><pubDate>Sun, 31 Mar 2024 22:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03294v2</guid></item><item><title>CLRmatchNet: Enhancing Curved Lane Detection with Deep Matching Process</title><link>http://arxiv.org/abs/2309.15204v2</link><description>Lane detection plays a crucial role in autonomous driving by providing vitaldata to ensure safe navigation. Modern algorithms rely on anchor-baseddetectors, which are then followed by a label-assignment process to categorizetraining detections as positive or negative instances based on learnedgeometric attributes. Accurate label assignment has great impact on the modelperformance, that is usually relying on a pre-defined classical cost functionevaluating GT-prediction alignment. However, classical label assignment methodsface limitations due to their reliance on predefined cost functions derivedfrom low-dimensional models, potentially impacting their optimality. Ourresearch introduces MatchNet, a deep learning submodule-based approach aimed atimproving the label assignment process. Integrated into a state-of-the-art lanedetection network such as the Cross Layer Refinement Network for Lane Detection(CLRNet), MatchNet replaces the conventional label assignment process with asubmodule network. The integrated model, CLRmatchNet, surpasses CLRNet, showingsubstantial improvements in scenarios involving curved lanes, with remarkableimprovement across all backbones of +2.8% for ResNet34, +2.3% for ResNet101,and +2.96% for DLA34. In addition, it maintains or even improves comparableresults in other sections. Our method boosts the confidence level in lanedetection, allowing an increase in the confidence threshold. Our code isavailable at: https://github.com/sapirkontente/CLRmatchNet.git</description><author>Sapir Kontente, Roy Orfaig, Ben-Zion Bobrovsky</author><pubDate>Sun, 31 Mar 2024 21:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15204v2</guid></item><item><title>ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</title><link>http://arxiv.org/abs/2311.09476v2</link><description>Evaluating retrieval-augmented generation (RAG) systems traditionally relieson hand annotations for input queries, passages to retrieve, and responses togenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluatingRAG systems along the dimensions of context relevance, answer faithfulness, andanswer relevance. By creating its own synthetic training data, ARES finetuneslightweight LM judges to assess the quality of individual RAG components. Tomitigate potential prediction errors, ARES utilizes a small set ofhuman-annotated datapoints for prediction-powered inference (PPI). Across eightdifferent knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARESaccurately evaluates RAG systems while using only a few hundred humanannotations during evaluation. Furthermore, ARES judges remain effective acrossdomain shifts, proving accurate even after changing the type of queries and/ordocuments used in the evaluated RAG systems. We make our code and datasetspublicly available on Github.</description><author>Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia</author><pubDate>Sun, 31 Mar 2024 21:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09476v2</guid></item><item><title>StructLM: Towards Building Generalist Models for Structured Knowledge Grounding</title><link>http://arxiv.org/abs/2402.16671v3</link><description>Structured data sources, such as tables, graphs, and databases, areubiquitous knowledge sources. Despite the demonstrated capabilities of largelanguage models (LLMs) on plain text, their proficiency in interpreting andutilizing structured data remains limited. Our investigation reveals a notabledeficiency in LLMs' ability to process structured data, e.g., ChatGPT lagsbehind state-of-the-art (SoTA) model by an average of 35%. To augment theStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed acomprehensive instruction tuning dataset comprising 1.1 million examples.Utilizing this dataset, we train a series of models, referred to as StructLM,based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. OurStructLM series surpasses task-specific models on 14 out of 18 evaluateddatasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,StructLM demonstrates strong generalization across 6 novel held-out SKG tasks,outperforming TableLlama by an average of 35\% and Flan-UL2 20B by an averageof 10\%. Contrary to expectations, we observe that scaling model size offersmarginal benefits, with StructLM-34B showing only slight improvements overStructLM-7B. This suggests that structured knowledge grounding is still achallenging task and requires more innovative design to push to a new level.</description><author>Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen</author><pubDate>Sun, 31 Mar 2024 21:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16671v3</guid></item><item><title>Complex Logical Reasoning over Knowledge Graphs using Large Language Models</title><link>http://arxiv.org/abs/2305.01157v3</link><description>Reasoning over knowledge graphs (KGs) is a challenging task that requires adeep understanding of the complex relationships between entities and theunderlying logic of their relations. Current approaches rely on learninggeometries to embed entities in vector space for logical query operations, butthey suffer from subpar performance on complex queries and dataset-specificrepresentations. In this paper, we propose a novel decoupled approach,Language-guided Abstract Reasoning over Knowledge graphs (LARK), thatformulates complex KG reasoning as a combination of contextual KG search andlogical query reasoning, to leverage the strengths of graph extractionalgorithms and large language models (LLM), respectively. Our experimentsdemonstrate that the proposed approach outperforms state-of-the-art KGreasoning methods on standard benchmark datasets across several logical queryconstructs, with significant performance gain for queries of higher complexity.Furthermore, we show that the performance of our approach improvesproportionally to the increase in size of the underlying LLM, enabling theintegration of the latest advancements in LLMs for logical reasoning over KGs.Our work presents a new direction for addressing the challenges of complex KGreasoning and paves the way for future research in this area.</description><author>Nurendra Choudhary, Chandan K. Reddy</author><pubDate>Sun, 31 Mar 2024 20:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01157v3</guid></item><item><title>Citation: A Key to Building Responsible and Accountable Large Language Models</title><link>http://arxiv.org/abs/2307.02185v3</link><description>Large Language Models (LLMs) bring transformative benefits alongside uniquechallenges, including intellectual property (IP) and ethical concerns. Thisposition paper explores a novel angle to mitigate these risks, drawingparallels between LLMs and established web systems. We identify "citation" -the acknowledgement or reference to a source or evidence - as a crucial yetmissing component in LLMs. Incorporating citation could enhance contenttransparency and verifiability, thereby confronting the IP and ethical issuesin the deployment of LLMs. We further propose that a comprehensive citationmechanism for LLMs should account for both non-parametric and parametriccontent. Despite the complexity of implementing such a citation mechanism,along with the potential pitfalls, we advocate for its development. Building onthis foundation, we outline several research problems in this area, aiming toguide future explorations towards building more responsible and accountableLLMs.</description><author>Jie Huang, Kevin Chen-Chuan Chang</author><pubDate>Sun, 31 Mar 2024 20:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02185v3</guid></item><item><title>FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs</title><link>http://arxiv.org/abs/2403.05766v2</link><description>Planning is a crucial task for agents in task oriented dialogs (TODs). Humanagents typically resolve user issues by following predefined workflows,decomposing workflow steps into actionable items, and performing actions byexecuting APIs in order; all of which require reasoning and planning. With therecent advances in LLMs, there have been increasing attempts to use them fortask planning and API usage. However, the faithfulness of the plans topredefined workflows and API dependencies, is not guaranteed with LLMs.Moreover, workflows in real life are often custom-defined and prone to changes;hence, adaptation is desirable. To study this, we propose the problem offaithful planning in TODs that needs to resolve user intents by followingpredefined flows and preserving API dependencies. To solve this problem, wepropose FLAP, a Flow-Adhering Planning algorithm based on constrained decodingwith lookahead heuristic for LLMs. Our algorithm alleviates the need forfinetuning LLMs using domain specific (plan/dependency) data, enables quickadaptation to predefined flows, and outperforms other decoding andprompting-based baselines. Further, our algorithm empowers smaller LLMs (7B) toperform at par larger LLMs (30B-40B).</description><author>Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, Arshit Gupta</author><pubDate>Sun, 31 Mar 2024 20:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05766v2</guid></item><item><title>In-context Learning and Gradient Descent Revisited</title><link>http://arxiv.org/abs/2311.07772v4</link><description>In-context learning (ICL) has shown impressive results in few-shot learningtasks, yet its underlying mechanism is still not fully understood. A recentline of work suggests that ICL performs gradient descent (GD)-basedoptimization implicitly. While appealing, much of the research focuses onsimplified settings, where the parameters of a shallow model are optimized. Inthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasksand models. We find gaps in evaluation, both in terms of problematic metricsand insufficient baselines. We show that surprisingly, even untrained modelsachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,we explore a major discrepancy in the flow of information throughout the modelbetween ICL and GD, which we term Layer Causality. We propose a simple GD-basedoptimization procedure that respects layer causality, and show it improvessimilarity scores significantly.</description><author>Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar</author><pubDate>Sun, 31 Mar 2024 20:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07772v4</guid></item><item><title>Virtual imaging trials improved the transparency and reliability of AI systems in COVID-19 imaging</title><link>http://arxiv.org/abs/2308.09730v2</link><description>The credibility of AI models in medical imaging is often challenged byreproducibility issues and obscured clinical insights, a reality highlightedduring the COVID-19 pandemic by many reports of near-perfect artificialintelligence (AI) models that all failed to generalize. To address theseconcerns, we propose a virtual imaging trial framework, employing a diversecollection of medical images that are both clinical and simulated. In thisstudy, COVID-19 serves as a case example to unveil the intrinsic and extrinsicfactors influencing AI performance. Our findings underscore a significantimpact of dataset characteristics on AI efficacy. Even when trained on large,diverse clinical datasets with thousands of patients, AI performance plummetedby up to 20% in generalization. However, virtual imaging trials offer a robustplatform for objective assessment, unveiling nuanced insights into therelationships between patient- and physics-based factors and AI performance.For instance, disease extent markedly influenced AI efficacy, computedtomography (CT) out-performed chest radiography (CXR), while imaging doseexhibited minimal impact. Using COVID-19 as a case study, this virtual imagingtrial study verified that radiology AI models often suffer from areproducibility crisis. Virtual imaging trials not only offered a solution forobjective performance assessment but also extracted several clinical insights.This study illuminates the path for leveraging virtual imaging to augment thereliability, transparency, and clinical relevance of AI in medical imaging.</description><author>Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo</author><pubDate>Sun, 31 Mar 2024 20:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09730v2</guid></item><item><title>DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation Networks for Remote Sensing Imagery</title><link>http://arxiv.org/abs/2311.13716v2</link><description>Semi-supervised learning aims to help reduce the cost of the manual labellingprocess by leveraging valuable features extracted from a substantial pool ofunlabeled data alongside a limited set of labelled data during the trainingphase. Since pixel-level manual labelling in large-scale remote sensing imageryis expensive, semi-supervised learning becomes an appropriate solution to this.However, most of the existing consistency learning frameworks based on networkperturbation are very bulky. There is still a lack of lightweight and efficientperturbation methods to promote the diversity of features and the precision ofpseudo labels during training. In order to fill this gap, we propose DiverseNetwhich explores multi-head and multi-model semi-supervised learning algorithmsby simultaneously enhancing precision and diversity during training. The twoproposed methods in the DiverseNet family, namely DiverseHead and DiverseModel,both achieve the better semantic segmentation performance in four widelyutilised remote sensing imagery data sets compared to state-of-the-artsemi-supervised learning methods. Meanwhile, the proposed DiverseHeadarchitecture is simple and relatively lightweight in terms of parameter spacecompared to the state-of-the-art methods whilst reaching high-performanceresults for all the tested data sets.</description><author>Wanli Ma, Oktay Karakus, Paul L. Rosin</author><pubDate>Sun, 31 Mar 2024 20:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13716v2</guid></item><item><title>Faster ISNet for Background Bias Mitigation on Deep Neural Networks</title><link>http://arxiv.org/abs/2401.08409v2</link><description>Bias or spurious correlations in image backgrounds can impact neuralnetworks, causing shortcut learning (Clever Hans Effect) and hamperinggeneralization to real-world data. ISNet, a recently introduced architecture,proposed the optimization of Layer-Wise Relevance Propagation (LRP, anexplanation technique) heatmaps, to mitigate the influence of backgrounds ondeep classifiers. However, ISNet's training time scales linearly with thenumber of classes in an application. Here, we propose reformulatedarchitectures whose training time becomes independent from this number.Additionally, we introduce a concise and model-agnostic LRP implementation. Wechallenge the proposed architectures using synthetic background bias, andCOVID-19 detection in chest X-rays, an application that commonly presentsbackground bias. The networks hindered background attention and shortcutlearning, surpassing multiple state-of-the-art models on out-of-distributiontest datasets. Representing a potentially massive training speed improvementover ISNet, the proposed architectures introduce LRP optimization into a gamutof applications that the original model cannot feasibly handle.</description><author>Pedro R. A. S. Bassi, Sergio Decherchi, Andrea Cavalli</author><pubDate>Sun, 31 Mar 2024 20:01:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08409v2</guid></item><item><title>Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance</title><link>http://arxiv.org/abs/2312.10671v2</link><description>We introduce Open3DIS, a novel solution designed to tackle the problem ofOpen-Vocabulary Instance Segmentation within 3D scenes. Objects within 3Denvironments exhibit diverse shapes, scales, and colors, making preciseinstance-level identification a challenging task. Recent advancements inOpen-Vocabulary scene understanding have made significant strides in this areaby employing class-agnostic 3D instance proposal networks for objectlocalization and learning queryable features for each 3D mask. While thesemethods produce high-quality instance proposals, they struggle with identifyingsmall-scale and geometrically ambiguous objects. The key idea of our method isa new module that aggregates 2D instance masks across frames and maps them togeometrically coherent point cloud regions as high-quality object proposalsaddressing the above limitations. These are then combined with 3Dclass-agnostic instance proposals to include a wide range of objects in thereal world. To validate our approach, we conducted experiments on threeprominent datasets, including ScanNet200, S3DIS, and Replica, demonstratingsignificant performance gains in segmenting objects with diverse categoriesover the state-of-the-art approaches.</description><author>Phuc D. A. Nguyen, Tuan Duc Ngo, Chuang Gan, Evangelos Kalogerakis, Anh Tran, Cuong Pham, Khoi Nguyen</author><pubDate>Sun, 31 Mar 2024 19:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10671v2</guid></item><item><title>EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy</title><link>http://arxiv.org/abs/2311.06234v2</link><description>Traversing terrain with good traction is crucial for achieving fast off-roadnavigation. Instead of manually designing costs based on terrain features,existing methods learn terrain properties directly from data viaself-supervision to automatically penalize trajectories moving throughundesirable terrain, but challenges remain to properly quantify and mitigatethe risk due to uncertainty in learned models. To this end, this work proposesa unified framework to learn uncertainty-aware traction model and planrisk-aware trajectories. For uncertainty quantification, we efficiently modelboth aleatoric and epistemic uncertainty by learning discrete tractiondistributions and probability densities of the traction predictor's latentfeatures. Leveraging evidential deep learning, we parameterize Dirichletdistributions with the network outputs and propose a novel uncertainty-awaresquared Earth Mover's distance loss with a closed-form expression that improveslearning accuracy and navigation performance. For risk-aware navigation, theproposed planner simulates state trajectories with the worst-case expectedtraction to handle aleatoric uncertainty, and penalizes trajectories movingthrough terrain with high epistemic uncertainty. Our approach is extensivelyvalidated in simulation and on wheeled and quadruped robots, showing improvednavigation performance compared to methods that assume no slip, assume theexpected traction, or optimize for the worst-case expected cost.</description><author>Xiaoyi Cai, Siddharth Ancha, Lakshay Sharma, Philip R. Osteen, Bernadette Bucher, Stephen Phillips, Jiuguang Wang, Michael Everett, Nicholas Roy, Jonathan P. How</author><pubDate>Sun, 31 Mar 2024 19:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06234v2</guid></item><item><title>Teaching Language Models to Self-Improve through Interactive Demonstrations</title><link>http://arxiv.org/abs/2310.13522v2</link><description>The self-improving ability of large language models (LLMs), enabled byprompting them to analyze and revise their own outputs, has garneredsignificant interest in recent research. However, this ability has been shownto be absent and difficult to learn for smaller models, thus widening theperformance gap between state-of-the-art LLMs and more cost-effective andfaster ones. To reduce this gap, we introduce TriPosT, a training algorithmthat endows smaller models with such self-improvement ability, and show thatour approach can improve a LLaMA-7b's performance on math and reasoning tasksby up to 7.13%. In contrast to prior work, we achieve this by using the smallermodel to interact with LLMs to collect feedback and improvements on its owngenerations. We then replay this experience to train the small model. Ourexperiments on four math and reasoning datasets show that the interactiveexperience of learning from and correcting its own mistakes is crucial forsmall models to improve their performance.</description><author>Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu</author><pubDate>Sun, 31 Mar 2024 19:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13522v2</guid></item><item><title>Object Recognition as Next Token Prediction</title><link>http://arxiv.org/abs/2312.02142v4</link><description>We present an approach to pose object recognition as next token prediction.The idea is to apply a language decoder that auto-regressively predicts thetext tokens from image embeddings to form labels. To ground this predictionprocess in auto-regression, we customize a non-causal attention mask for thedecoder, incorporating two key features: modeling tokens from different labelsto be independent, and treating image tokens as a prefix. This maskingmechanism inspires an efficient method - one-shot sampling - to simultaneouslysample tokens of multiple labels in parallel and rank generated labels by theirprobabilities during inference. To further enhance the efficiency, we propose asimple strategy to construct a compact decoder by simply discarding theintermediate blocks of a pretrained language model. This approach yields adecoder that matches the full model's performance while being notably moreefficient. The code is available at https://github.com/kaiyuyue/nxtp</description><author>Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim</author><pubDate>Sun, 31 Mar 2024 19:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02142v4</guid></item><item><title>DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models</title><link>http://arxiv.org/abs/2402.12289v3</link><description>A primary hurdle of autonomous driving in urban environments is understandingcomplex and long-tail scenarios, such as challenging road conditions anddelicate human behaviors. We introduce DriveVLM, an autonomous driving systemleveraging Vision-Language Models (VLMs) for enhanced scene understanding andplanning capabilities. DriveVLM integrates a unique combination ofchain-of-thought (CoT) modules for scene description, scene analysis, andhierarchical planning. Furthermore, recognizing the limitations of VLMs inspatial reasoning and heavy computational requirements, we proposeDriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM withthe traditional autonomous driving pipeline. DriveVLM-Dual achieves robustspatial understanding and real-time inference speed. Extensive experiments onboth the nuScenes dataset and our SUP-AD dataset demonstrate the effectivenessof DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existingmethods in complex and unpredictable driving conditions.</description><author>Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao</author><pubDate>Sun, 31 Mar 2024 18:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12289v3</guid></item><item><title>A New Benchmark and Model for Challenging Image Manipulation Detection</title><link>http://arxiv.org/abs/2311.14218v2</link><description>The ability to detect manipulation in multimedia data is vital in digitalforensics. Existing Image Manipulation Detection (IMD) methods are mainly basedon detecting anomalous features arisen from image editing or double compressionartifacts. All existing IMD techniques encounter challenges when it comes todetecting small tampered regions from a large image. Moreover,compression-based IMD approaches face difficulties in cases of doublecompression of identical quality factors. To investigate the State-of-The-Art(SoTA) IMD methods in those challenging conditions, we introduce a newChallenging Image Manipulation Detection (CIMD) benchmark dataset, whichconsists of two subsets, for evaluating editing-based and compression-based IMDmethods, respectively. The dataset images were manually taken and tampered withhigh-quality annotations. In addition, we propose a new two-branch networkmodel based on HRNet that can better detect both the image-editing andcompression artifacts in those challenging conditions. Extensive experiments onthe CIMD benchmark show that our model significantly outperforms SoTA IMDmethods on CIMD.</description><author>Zhenfei Zhang, Mingyang Li, Ming-Ching Chang</author><pubDate>Sun, 31 Mar 2024 18:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14218v2</guid></item><item><title>Weisfeiler and Leman Go Measurement Modeling: Probing the Validity of the WL Test</title><link>http://arxiv.org/abs/2307.05775v3</link><description>The expressive power of graph neural networks is usually measured bycomparing how many pairs of graphs or nodes an architecture can possiblydistinguish as non-isomorphic to those distinguishable by the $k$-dimensionalWeisfeiler-Leman ($k$-WL) test. In this paper, we uncover misalignments betweengraph machine learning practitioners' conceptualizations of expressive powerand $k$-WL through a systematic analysis of the reliability and validity of$k$-WL. We conduct a survey ($n = 18$) of practitioners to surface theirconceptualizations of expressive power and their assumptions about $k$-WL. Incontrast to practitioners' beliefs, our analysis (which draws from graph theoryand benchmark auditing) reveals that $k$-WL does not guarantee isometry, can beirrelevant to real-world graph tasks, and may not promote generalization ortrustworthiness. We argue for extensional definitions and measurement ofexpressive power based on benchmarks. We further contribute guiding questionsfor constructing such benchmarks, which is critical for graph machine learningpractitioners to develop and transparently communicate our understandings ofexpressive power.</description><author>Arjun Subramonian, Adina Williams, Maximilian Nickel, Yizhou Sun, Levent Sagun</author><pubDate>Sun, 31 Mar 2024 18:03:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05775v3</guid></item><item><title>Low-Rank Adaptation for Multilingual Summarization: An Empirical Study</title><link>http://arxiv.org/abs/2311.08572v2</link><description>Although the advancements of pre-trained Large Language Models havesignificantly accelerated recent progress in NLP, their ever-increasing sizeposes significant challenges for conventional fine-tuning, especially inmemory-intensive tasks. We investigate the potential of Parameter-EfficientFine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain ofmultilingual summarization, a task that is both challenging (due to typicallylong inputs), and relatively unexplored. We conduct an extensive study acrossdifferent data availability scenarios, including high- and low-data settings,and cross-lingual transfer, leveraging models of different sizes. Our findingsreveal that LoRA is competitive with full fine-tuning when trained with highquantities of data, and excels in low-data scenarios and cross-lingualtransfer. We also study different strategies for few-shot cross-lingualtransfer, finding that continued LoRA tuning outperforms full fine-tuning andthe dynamic composition of language-specific LoRA modules.</description><author>Chenxi Whitehouse, Fantine Huot, Jasmijn Bastings, Mostafa Dehghani, Chu-Cheng Lin, Mirella Lapata</author><pubDate>Sun, 31 Mar 2024 18:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08572v2</guid></item><item><title>Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates</title><link>http://arxiv.org/abs/2309.11281v3</link><description>Neural radiance field is an emerging rendering method that generateshigh-quality multi-view consistent images from a neural scene representationand volume rendering. Although neural radiance field-based techniques arerobust for scene reconstruction, their ability to add or remove objects remainslimited. This paper proposes a new language-driven approach for objectmanipulation with neural radiance fields through dataset updates. Specifically,to insert a new foreground object represented by a set of multi-view imagesinto a background radiance field, we use a text-to-image diffusion model tolearn and generate combined images that fuse the object of interest into thegiven background across views. These combined images are then used for refiningthe background radiance field so that we can render view-consistent imagescontaining both the object and the background. To ensure view consistency, wepropose a dataset updates strategy that prioritizes radiance field trainingwith camera views close to the already-trained views prior to propagating thetraining to remaining views. We show that under the same dataset updatesstrategy, we can easily adapt our method for object insertion using data fromtext-to-3D models as well as object removal. Experimental results show that ourmethod generates photorealistic images of the edited scenes, and outperformsstate-of-the-art methods in 3D reconstruction and neural radiance fieldblending.</description><author>Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung</author><pubDate>Sun, 31 Mar 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11281v3</guid></item><item><title>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</title><link>http://arxiv.org/abs/2403.19647v2</link><description>We introduce methods for discovering and applying sparse feature circuits.These are causally implicated subnetworks of human-interpretable features forexplaining language model behaviors. Circuits identified in prior work consistof polysemantic and difficult-to-interpret units like attention heads orneurons, rendering them unsuitable for many downstream applications. Incontrast, sparse feature circuits enable detailed understanding ofunanticipated mechanisms. Because they are based on fine-grained units, sparsefeature circuits are useful for downstream tasks: We introduce SHIFT, where weimprove the generalization of a classifier by ablating features that a humanjudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervisedand scalable interpretability pipeline by discovering thousands of sparsefeature circuits for automatically discovered model behaviors.</description><author>Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller</author><pubDate>Sun, 31 Mar 2024 17:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19647v2</guid></item><item><title>SE(3)-Stochastic Flow Matching for Protein Backbone Generation</title><link>http://arxiv.org/abs/2310.02391v3</link><description>The computational design of novel protein structures has the potential toimpact numerous scientific disciplines greatly. Toward this goal, we introduceFoldFlow, a series of novel generative models of increasing modeling powerbased on the flow-matching paradigm over $3\mathrm{D}$ rigid motions -- i.e.the group $\text{SE}(3)$ -- enabling accurate modeling of protein backbones. Wefirst introduce FoldFlow-Base, a simulation-free approach to learningdeterministic continuous-time dynamics and matching invariant targetdistributions on $\text{SE}(3)$. We next accelerate training by incorporatingRiemannian optimal transport to create FoldFlow-OT, leading to the constructionof both more simple and stable flows. Finally, we design FoldFlow-SFM, couplingboth Riemannian OT and simulation-free training to learn stochasticcontinuous-time dynamics over $\text{SE}(3). Our family of FoldFlow, generativemodels offers several key advantages over previous approaches to the generativemodeling of proteins: they are more stable and faster to train thandiffusion-based approaches, and our models enjoy the ability to map anyinvariant source distribution to any invariant target distribution over$\text{SE}(3)$. Empirically, we validate FoldFlow, on protein backbonegeneration of up to $300$ amino acids leading to high-quality designable,diverse, and novel samples.</description><author>Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong</author><pubDate>Sun, 31 Mar 2024 17:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02391v3</guid></item><item><title>Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems</title><link>http://arxiv.org/abs/2403.18998v2</link><description>Microservice-based systems (MSS) may experience failures in various faultcategories due to their complex and dynamic nature. To effectively handlefailures, AIOps tools utilize trace-based anomaly detection and root causeanalysis. In this paper, we propose a novel framework for few-shot abnormaltrace classification for MSS. Our framework comprises two main components: (1)Multi-Head Attention Autoencoder for constructing system-specific tracerepresentations, which enables (2) Transformer Encoder-based Model-AgnosticMeta-Learning to perform effective and efficient few-shot learning for abnormaltrace classification. The proposed framework is evaluated on two representativeMSS, Trainticket and OnlineBoutique, with open datasets. The results show thatour framework can adapt the learned knowledge to classify new, unseen abnormaltraces of novel fault categories both within the same system it was initiallytrained on and even in the different MSS. Within the same MSS, our frameworkachieves an average accuracy of 93.26\% and 85.2\% across 50 meta-testing tasksfor Trainticket and OnlineBoutique, respectively, when provided with 10instances for each task. In a cross-system context, our framework gets anaverage accuracy of 92.19\% and 84.77\% for the same meta-testing tasks of therespective system, also with 10 instances provided for each task. Our workdemonstrates the applicability of achieving few-shot abnormal traceclassification for MSS and shows how it can enable cross-system adaptability.This opens an avenue for building more generalized AIOps tools that requireless system-specific data labeling for anomaly detection and root causeanalysis.</description><author>Yuqing Wang, Mika V. Mantylä, Serge Demeyer, Mutlu Beyazit, Joanna Kisaakye, Jesse Nyyssölä</author><pubDate>Sun, 31 Mar 2024 17:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18998v2</guid></item><item><title>Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs</title><link>http://arxiv.org/abs/2308.16859v2</link><description>In this article, the optimal sample complexity of learning the underlyinginteractions or dependencies of a Linear Dynamical System (LDS) over a DirectedAcyclic Graph (DAG) is studied. We call such a DAG underlying an LDS asdynamical DAG (DDAG). In particular, we consider a DDAG where the nodaldynamics are driven by unobserved exogenous noise sources that are wide-sensestationary (WSS) in time but are mutually uncorrelated, and have the same{power spectral density (PSD)}. Inspired by the static DAG setting, a metricand an algorithm based on the PSD matrix of the observed time series areproposed to reconstruct the DDAG. It is shown that the optimal samplecomplexity (or length of state trajectory) needed to learn the DDAG is$n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is the maximumnumber of parents per node. To prove the sample complexity upper bound, aconcentration bound for the PSD estimation is derived, under two differentsampling strategies. A matching min-max lower bound using generalized Fano'sinequality also is provided, thus showing the order optimality of the proposedalgorithm.</description><author>Mishfad Shaikh Veedu, Deepjyoti Deka, Murti V. Salapaka</author><pubDate>Sun, 31 Mar 2024 17:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16859v2</guid></item><item><title>TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization</title><link>http://arxiv.org/abs/2402.13249v2</link><description>Single document news summarization has seen substantial progress onfaithfulness in recent years, driven by research on the evaluation of factualconsistency, or hallucinations. We ask whether these advances carry over toother text summarization domains. We propose a new evaluation benchmark ontopic-focused dialogue summarization, generated by LLMs of varying sizes. Weprovide binary sentence-level human annotations of the factual consistency ofthese summaries along with detailed explanations of factually inconsistentsentences. Our analysis shows that existing LLMs hallucinate significantamounts of factual errors in the dialogue domain, regardless of the model'ssize. On the other hand, when LLMs, including GPT-4, serve as binary factualevaluators, they perform poorly and can be outperformed by prevailingstate-of-the-art specialized factuality evaluation metrics. Finally, weconducted an analysis of hallucination types with a curated error taxonomy. Wefind that there are diverse errors and error distributions in model-generatedsummaries and that non-LLM based metrics can capture all error types betterthan LLM-based evaluators.</description><author>Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, Kathleen McKeown</author><pubDate>Sun, 31 Mar 2024 16:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13249v2</guid></item><item><title>Modular Blind Video Quality Assessment</title><link>http://arxiv.org/abs/2402.19276v4</link><description>Blind video quality assessment (BVQA) plays a pivotal role in evaluating andimproving the viewing experience of end-users across a wide range ofvideo-based platforms and services. Contemporary deep learning-based modelsprimarily analyze video content in its aggressively subsampled format, whilebeing blind to the impact of the actual spatial resolution and frame rate onvideo quality. In this paper, we propose a modular BVQA model and a method oftraining it to improve its modularity. Our model comprises a base qualitypredictor, a spatial rectifier, and a temporal rectifier, responding to thevisual content and distortion, spatial resolution, and frame rate changes onvideo quality, respectively. During training, spatial and temporal rectifiersare dropped out with some probabilities to render the base quality predictor astandalone BVQA model, which should work better with the rectifiers. Extensiveexperiments on both professionally-generated content and user-generated contentvideo databases show that our quality model achieves superior or comparableperformance to current methods. Additionally, the modularity of our modeloffers an opportunity to analyze existing video quality databases in terms oftheir spatial and temporal complexity.</description><author>Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, Kede Ma</author><pubDate>Sun, 31 Mar 2024 16:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19276v4</guid></item><item><title>Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI</title><link>http://arxiv.org/abs/2403.17683v2</link><description>This report provide a detailed description of the method that we explored andproposed in the WECIA Emotion Prediction Competition (EPC), which predicts aperson's emotion through an artistic work with a comment. The dataset of thiscompetition is ArtELingo, designed to encourage work on diversity acrosslanguages and cultures. The dataset has two main challenges, namely modalimbalance problem and language-cultural differences problem. In order toaddress this issue, we propose a simple yet effective approach calledsingle-multi modal with Emotion-Cultural specific prompt(ECSP), which focuseson using the single modal message to enhance the performance of multimodalmodels and a well-designed prompt to reduce cultural differences problem. Toclarify, our approach contains two main blocks:(1)XLM-R\cite{conneau2019unsupervised} based unimodal model andX$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specificprompt. Our approach ranked first in the final test with a score of 0.627.</description><author>Shengdong Xu, Zhouyang Chi, Yang Yang</author><pubDate>Sun, 31 Mar 2024 15:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17683v2</guid></item><item><title>Multi-Channel Orthogonal Transform-Based Perceptron Layers for Efficient ResNets</title><link>http://arxiv.org/abs/2303.06797v2</link><description>In this paper, we propose a set of transform-based neural network layers asan alternative to the $3\times3$ Conv2D layers in Convolutional Neural Networks(CNNs). The proposed layers can be implemented based on orthogonal transformssuch as the Discrete Cosine Transform (DCT), Hadamard transform (HT), andbiorthogonal Block Wavelet Transform (BWT). Furthermore, by taking advantage ofthe convolution theorems, convolutional filtering operations are performed inthe transform domain using element-wise multiplications. Trainablesoft-thresholding layers, that remove noise in the transform domain, bringnonlinearity to the transform domain layers. Compared to the Conv2D layer,which is spatial-agnostic and channel-specific, the proposed layers arelocation-specific and channel-specific. Moreover, these proposed layers reducethe number of parameters and multiplications significantly while improving theaccuracy results of regular ResNets on the ImageNet-1K classification task.Furthermore, they can be inserted with a batch normalization layer before theglobal average pooling layer in the conventional ResNets as an additional layerto improve classification accuracy.</description><author>Hongyi Pan, Emadeldeen Hamdan, Xin Zhu, Salih Atici, Ahmet Enis Cetin</author><pubDate>Sun, 31 Mar 2024 15:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06797v2</guid></item><item><title>The Hidden Attention of Mamba Models</title><link>http://arxiv.org/abs/2403.01590v2</link><description>The Mamba layer offers an efficient selective state space model (SSM) that ishighly effective in modeling multiple domains, including NLP, long-rangesequence processing, and computer vision. Selective SSMs are viewed as dualmodels, in which one trains in parallel on the entire sequence via an IO-awareparallel scan, and deploys in an autoregressive manner. We add a third view andshow that such models can be viewed as attention-driven models. This newperspective enables us to empirically and theoretically compare the underlyingmechanisms to that of the self-attention layers in transformers and allows usto peer inside the inner workings of the Mamba model with explainabilitymethods. Our code is publicly available.</description><author>Ameen Ali, Itamar Zimerman, Lior Wolf</author><pubDate>Sun, 31 Mar 2024 15:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01590v2</guid></item><item><title>Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel</title><link>http://arxiv.org/abs/2311.01276v3</link><description>Graph Neural Networks (GNNs) have been widely adopted for drug discovery withmolecular graphs. Nevertheless, current GNNs mainly excel in leveragingshort-range interactions (SRI) but struggle to capture long-range interactions(LRI), both of which are crucial for determining molecular properties. Totackle this issue, we propose a method to abstract the collective informationof atomic groups into a few $\textit{Neural Atoms}$ by implicitly projectingthe atoms of a molecular. Specifically, we explicitly exchange the informationamong neural atoms and project them back to the atoms' representations as anenhancement. With this mechanism, neural atoms establish the communicationchannels among distant nodes, effectively reducing the interaction scope ofarbitrary node pairs into a single hop. To provide an inspection of our methodfrom a physical perspective, we reveal its connection to the traditional LRIcalculation method, Ewald Summation. The Neural Atom can enhance GNNs tocapture LRI by approximating the potential LRI of the molecular. We conductextensive experiments on four long-range graph benchmarks, covering graph-leveland link-level tasks on molecular graphs. We achieve up to a 27.32% and 38.27%improvement in the 2D and 3D scenarios, respectively. Empirically, our methodcan be equipped with an arbitrary GNN to help capture LRI. Code and datasetsare publicly available in https://github.com/tmlr-group/NeuralAtom.</description><author>Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han</author><pubDate>Sun, 31 Mar 2024 15:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01276v3</guid></item><item><title>Intrinsic Gaussian Vector Fields on Manifolds</title><link>http://arxiv.org/abs/2310.18824v2</link><description>Various applications ranging from robotics to climate science requiremodeling signals on non-Euclidean domains, such as the sphere. Gaussian processmodels on manifolds have recently been proposed for such tasks, in particularwhen uncertainty quantification is needed. In the manifold setting,vector-valued signals can behave very differently from scalar-valued ones, withmuch of the progress so far focused on modeling the latter. The former,however, are crucial for many applications, such as modeling wind speeds orforce fields of unknown dynamical systems. In this paper, we propose novelGaussian process models for vector-valued signals on manifolds that areintrinsically defined and account for the geometry of the space inconsideration. We provide computational primitives needed to deploy theresulting Hodge-Mat\'ern Gaussian vector fields on the two-dimensional sphereand the hypertori. Further, we highlight two generalization directions:discrete two-dimensional meshes and "ideal" manifolds like hyperspheres, Liegroups, and homogeneous spaces. Finally, we show that our Gaussian vectorfields constitute considerably more refined inductive biases than the extrinsicfields proposed before.</description><author>Daniel Robert-Nicoud, Andreas Krause, Viacheslav Borovitskiy</author><pubDate>Sun, 31 Mar 2024 15:12:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18824v2</guid></item><item><title>Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning</title><link>http://arxiv.org/abs/2309.14771v2</link><description>Large language models (LLMs) enable in-context learning (ICL) by conditioningon a few labeled training examples as a text-based prompt, eliminating the needfor parameter updates and achieving competitive performance. In this paper, wedemonstrate that factual knowledge is imperative for the performance of ICL inthree core facets: the inherent knowledge learned in LLMs, the factualknowledge derived from the selected in-context examples, and the knowledgebiases in LLMs for output generation. To unleash the power of LLMs in few-shotlearning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT)framework to further improve the performance of ICL: 1) injecting knowledgeinto LLMs during continual self-supervised pre-training, 2) judiciouslyselecting the examples for ICL with high knowledge relevance, and 3)calibrating the prediction results based on prior knowledge. We evaluate theproposed approaches on autoregressive models (e.g., GPT-style LLMs) overmultiple text classification and question-answering tasks. Experimental resultsdemonstrate that KICT substantially outperforms strong baselines and improvesby more than 13% and 7% on text classification and question-answering tasks,respectively.</description><author>Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, Ming Gao</author><pubDate>Sun, 31 Mar 2024 14:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14771v2</guid></item><item><title>Instruction Tuning with Human Curriculum</title><link>http://arxiv.org/abs/2310.09518v3</link><description>In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore thepotential advantages of employing diverse curriculum strategies, and (3)delineate a synthetic instruction-response generation framework thatcomplements our theoretical approach. Distinct from the existing instructiontuning dataset, our generation pipeline is systematically structured to emulatethe sequential and orderly characteristic of human learning. Additionally, wedescribe a methodology for generating instruction-response datasets thatextensively span the various stages of human education, from middle schoolthrough the graduate level, utilizing educational subject catalogs. Before training, we meticulously organize the instruction data to ensure thatquestions escalate in difficulty regarding (A) the subject matter and (B) theintricacy of the instructions. The findings of our study reveal thatsubstantial improvements in performance can be achieved through the mereapplication of curriculum ordering to instruction data (achieving gains of+4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard)compared to random shuffling. This enhancement is achieved without incurringadditional computational expenses. Through comprehensive experimentation, weobserve that the advantages of our proposed method are consistently evidentacross nine benchmarks.</description><author>Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo</author><pubDate>Sun, 31 Mar 2024 14:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09518v3</guid></item><item><title>C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion</title><link>http://arxiv.org/abs/2403.14119v3</link><description>In deep learning, test-time adaptation has gained attention as a method formodel fine-tuning without the need for labeled data. A prime exemplification isthe recently proposed test-time prompt tuning for large-scale vision-languagemodels such as CLIP. Unfortunately, these prompts have been mainly developed toimprove accuracy, overlooking the importance of calibration, which is a crucialaspect for quantifying prediction uncertainty. However, traditional calibrationmethods rely on substantial amounts of labeled data, making them impracticalfor test-time scenarios. To this end, this paper explores calibration duringtest-time prompt tuning by leveraging the inherent properties of CLIP. Througha series of observations, we find that the prompt choice significantly affectsthe calibration in CLIP, where the prompts leading to higher text featuredispersion result in better-calibrated predictions. Introducing the AverageText Feature Dispersion (ATFD), we establish its relationship with calibrationerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),for optimizing prompts during test-time with enhanced calibration. Throughextensive experiments on different CLIP architectures and datasets, we showthat C-TPT can effectively improve the calibration of test-time prompt tuningwithout needing labeled data. The code is publicly accessible athttps://github.com/hee-suk-yoon/C-TPT.</description><author>Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo</author><pubDate>Sun, 31 Mar 2024 14:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14119v3</guid></item><item><title>STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model</title><link>http://arxiv.org/abs/2403.12418v2</link><description>Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,and non-stationary, leading to the continuous challenge of spatial-temporalgraph learning. In the past few years, various GNN-based methods have beenproposed to solely focus on mimicking the relationships among node individualsof the STG network, ignoring the significance of modeling the intrinsicfeatures that exist in STG system over time. In contrast, modern SelectiveState Space Models (SSSMs) present a new approach which treat STG Network as asystem, and meticulously explore the STG system's dynamic state evolutionacross temporal dimension. In this work, we introduce Spatial-Temporal GraphMamba (STG-Mamba) as the first exploration of leveraging the powerful selectivestate space models for STG learning by treating STG Network as a system, andemploying the Graph Selective State Space Block (GS3B) to preciselycharacterize the dynamic evolution of STG networks. STG-Mamba is formulated asan Encoder-Decoder architecture, which takes GS3B as the basic module, forefficient sequential data modeling. Furthermore, to strengthen GNN's ability ofmodeling STG data under the setting of SSSMs, we propose Kalman Filtering GraphNeural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothlyfits in the context of selective state space evolution, and at the same timekeeps linear complexity. Extensive empirical studies are conducted on threebenchmark STG forecasting datasets, demonstrating the performance superiorityand computational efficiency of STG-Mamba. It not only surpasses existingstate-of-the-art methods in terms of STG forecasting performance, but alsoeffectively alleviate the computational bottleneck of large-scale graphnetworks in reducing the computational cost of FLOPs and test inference time.</description><author>Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster</author><pubDate>Sun, 31 Mar 2024 14:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12418v2</guid></item><item><title>Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization</title><link>http://arxiv.org/abs/2403.19456v2</link><description>Personalized generation paradigms empower designers to customize visualintellectual properties with the help of textual descriptions by tuning oradapting pre-trained text-to-image models on a few images. Recent works exploreapproaches for concurrently customizing both content and detailed visual styleappearance. However, these existing approaches often generate images where thecontent and style are entangled. In this study, we reconsider the customizationof content and style concepts from the perspective of parameter spaceconstruction. Unlike existing methods that utilize a shared parameter space forcontent and style, we propose a learning framework that separates the parameterspace to facilitate individual learning of content and style, thereby enablingdisentangled content and style. To achieve this goal, we introduce "partlylearnable projection" (PLP) matrices to separate the original adapters intodivided sub-parameter spaces. We propose "break-for-make" customizationlearning pipeline based on PLP, which is simple yet effective. We break theoriginal adapters into "up projection" and "down projection", train content andstyle PLPs individually with the guidance of corresponding textual prompts inthe separate adapters, and maintain generalization by employing amulti-correspondence projection learning strategy. Based on the adapters brokenapart for separate training content and style, we then make the entityparameter space by reconstructing the content and style PLPs matrices, followedby fine-tuning the combined adapter to generate the target object with thedesired appearance. Experiments on various styles, including textures,materials, and artistic style, show that our method outperformsstate-of-the-art single/multiple concept learning pipelines in terms ofcontent-style-prompt alignment.</description><author>Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Oliver Deussen, Weiming Dong, Jintao Li, Tong-Yee Lee</author><pubDate>Sun, 31 Mar 2024 14:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19456v2</guid></item><item><title>Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition</title><link>http://arxiv.org/abs/2311.18254v2</link><description>With the recent surge in the use of touchscreen devices, free-hand sketchinghas emerged as a promising modality for human-computer interaction. Whileprevious research has focused on tasks such as recognition, retrieval, andgeneration of familiar everyday objects, this study aims to create a SketchInput Method Editor (SketchIME) specifically designed for a professional C4Isystem. Within this system, sketches are utilized as low-fidelity prototypesfor recommending standardized symbols in the creation of comprehensivesituation maps. This paper also presents a systematic dataset comprising 374specialized sketch types, and proposes a simultaneous recognition andsegmentation architecture with multilevel supervision between recognition andsegmentation to improve performance and enhance interpretability. Byincorporating few-shot domain adaptation and class-incremental learning, thenetwork's ability to adapt to new users and extend to new task-specific classesis significantly enhanced. Results from experiments conducted on both theproposed dataset and the SPG dataset illustrate the superior performance of theproposed architecture. Our dataset and code are publicly available athttps://github.com/GuangmingZhu/SketchIME.</description><author>Guangming Zhu, Siyuan Wang, Qing Cheng, Kelong Wu, Hao Li, Liang Zhang</author><pubDate>Sun, 31 Mar 2024 14:13:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18254v2</guid></item><item><title>Improving Plasticity in Online Continual Learning via Collaborative Learning</title><link>http://arxiv.org/abs/2312.00600v2</link><description>Online Continual Learning (CL) solves the problem of learning theever-emerging new classification tasks from a continuous data stream. Unlikeits offline counterpart, in online CL, the training data can only be seen once.Most existing online CL research regards catastrophic forgetting (i.e., modelstability) as almost the only challenge. In this paper, we argue that themodel's capability to acquire new knowledge (i.e., model plasticity) is anotherchallenge in online CL. While replay-based strategies have been shown to beeffective in alleviating catastrophic forgetting, there is a notable gap inresearch attention toward improving model plasticity. To this end, we proposeCollaborative Continual Learning (CCL), a collaborative learning based strategyto improve the model's capability in acquiring new concepts. Additionally, weintroduce Distillation Chain (DC), a collaborative learning scheme to boost thetraining of the models. We adapt CCL-DC to existing representative online CLworks. Extensive experiments demonstrate that even if the learners arewell-trained with state-of-the-art online CL methods, our strategy can stillimprove model plasticity dramatically, and thereby improve the overallperformance by a large margin. The source code of our work is available athttps://github.com/maorong-wang/CCL-DC.</description><author>Maorong Wang, Nicolas Michel, Ling Xiao, Toshihiko Yamasaki</author><pubDate>Sun, 31 Mar 2024 13:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00600v2</guid></item><item><title>High-Fidelity Lake Extraction via Two-Stage Prompt Enhancement: Establishing a Novel Baseline and Benchmark</title><link>http://arxiv.org/abs/2308.08443v2</link><description>Lake extraction from remote sensing imagery is a complex challenge due to thevaried lake shapes and data noise. Current methods rely on multispectral imagedatasets, making it challenging to learn lake features accurately from pixelarrangements. This, in turn, affects model learning and the creation ofaccurate segmentation masks. This paper introduces a prompt-based datasetconstruction approach that provides approximate lake locations using point,box, and mask prompts. We also propose a two-stage prompt enhancementframework, LEPrompter, with prompt-based and prompt-free stages duringtraining. The prompt-based stage employs a prompt encoder to extract priorinformation, integrating prompt tokens and image embedding through self- andcross-attention in the prompt decoder. Prompts are deactivated to ensureindependence during inference, enabling automated lake extraction withoutintroducing additional parameters and GFlops. Extensive experiments showcaseperformance improvements of our proposed approach compared to the previousstate-of-the-art method. The source code is available athttps://github.com/BastianChen/LEPrompter.</description><author>Ben Chen, Xuechao Zou, Kai Li, Yu Zhang, Junliang Xing, Pin Tao</author><pubDate>Sun, 31 Mar 2024 13:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08443v2</guid></item><item><title>Deep Neural Networks Fused with Textures for Image Classification</title><link>http://arxiv.org/abs/2308.01813v2</link><description>Fine-grained image classification (FGIC) is a challenging task in computervision for due to small visual differences among inter-subcategories, but,large intra-class variations. Deep learning methods have achieved remarkablesuccess in solving FGIC. In this paper, we propose a fusion approach to addressFGIC by combining global texture with local patch-based information. The firstpipeline extracts deep features from various fixed-size non-overlapping patchesand encodes features by sequential modelling using the long short-term memory(LSTM). Another path computes image-level textures at multiple scales using thelocal binary patterns (LBP). The advantages of both streams are integrated torepresent an efficient feature vector for image classification. The method istested on eight datasets representing the human faces, skin lesions, fooddishes, marine lives, etc. using four standard backbone CNNs. Our method hasattained better classification accuracy over existing methods with notablemargins.</description><author>Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri</author><pubDate>Sun, 31 Mar 2024 13:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01813v2</guid></item><item><title>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</title><link>http://arxiv.org/abs/2308.00225v2</link><description>Recent studies show that instruction tuning (IT) and reinforcement learningfrom human feedback (RLHF) improve the abilities of large language models (LMs)dramatically. While these tuning methods can help align models with humanobjectives and generate high-quality text, not much is known about theirpotential adverse effects. In this work, we investigate the effect of IT andRLHF on decision making and reasoning in LMs, focusing on three cognitivebiases - the decoy effect, the certainty effect, and the belief bias - all ofwhich are known to influence human decision-making and reasoning. Our findingshighlight the presence of these biases in various models from the GPT-3,Mistral, and T5 families. Notably, we find a stronger presence of biases inmodels that have undergone instruction tuning, such as Flan-T5,Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step towardcomprehending cognitive biases in instruction-tuned LMs, which is crucial forthe development of more reliable and unbiased language models.</description><author>Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov</author><pubDate>Sun, 31 Mar 2024 13:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00225v2</guid></item><item><title>Object-level Geometric Structure Preserving for Natural Image Stitching</title><link>http://arxiv.org/abs/2402.12677v2</link><description>The topic of stitching images with globally natural structures holdsparamount significance. Current methodologies exhibit the ability to preservelocal geometric structures, yet fall short in maintaining relationships betweenthese geometric structures. In this paper, we endeavor to safeguard theoverall, OBJect-level structures within images based on Global SimilarityPrior, while concurrently mitigating distortion and ghosting artifacts withOBJ-GSP. Our approach leverages the Segment Anything Model to extract geometricstructures with semantic information, enhancing the algorithm's ability topreserve objects in a manner that aligns more intuitively with humanperception. We seek to identify spatial constraints that govern therelationships between various geometric boundaries. Recognizing that multiplegeometric boundaries collectively define complete objects, we employ triangularmeshes to safeguard not only individual geometric structures but also theoverall shapes of objects within the images. Empirical evaluations acrossmultiple image stitching datasets demonstrate that our method establishes a newstate-of-the-art benchmark in image stitching. Our implementation and datasetis publicly available at https://github.com/RussRobin/OBJ-GSP .</description><author>Wenxiao Cai, Wankou Yang</author><pubDate>Sun, 31 Mar 2024 13:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12677v2</guid></item><item><title>Self-Adaptive Sampling for Efficient Video Question-Answering on Image--Text Models</title><link>http://arxiv.org/abs/2307.04192v4</link><description>Video question-answering is a fundamental task in the field of videounderstanding. Although current vision--language models (VLMs) equipped withVideo Transformers have enabled temporal modeling and yielded superior results,they are at the cost of huge computational power and thus too expensive todeploy in real-time application scenarios. An economical workaround onlysamples a small portion of frames to represent the main content of that videoand tune an image--text model on these sampled frames. Recent videounderstanding models usually randomly sample a set of frames or clips,regardless of internal correlations between their visual contents, nor theirrelevance to the problem. We argue that such kinds of aimless sampling may omitthe key frames from which the correct answer can be deduced, and the situationgets worse when the sampling sparsity increases, which always happens as thevideo lengths increase. To mitigate this issue, we propose two frame samplingstrategies, namely the most domain frames (MDF) and most implied frames (MIF),to maximally preserve those frames that are most likely vital to the givenquestions. MDF passively minimizes the risk of key frame omission in abootstrap manner, while MIS actively searches key frames customized for eachvideo--question pair with the assistance of auxiliary models. The experimentalresults on three public datasets from three advanced VLMs (CLIP, GIT andAll-in-one) demonstrate that our proposed strategies can boost the performancefor image-text pretrained models. The source codes pertaining to the methodproposed in this paper are publicly available athttps://github.com/declare-lab/sas-vqa.</description><author>Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria</author><pubDate>Sun, 31 Mar 2024 13:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04192v4</guid></item><item><title>HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation</title><link>http://arxiv.org/abs/2311.12588v2</link><description>In this work, we present a novel dense-correspondence method for 6DoF objectpose estimation from a single RGB-D image. While many existing data-drivenmethods achieve impressive performance, they tend to be time-consuming due totheir reliance on rendering-based refinement approaches. To circumvent thislimitation, we present HiPose, which establishes 3D-3D correspondences in acoarse-to-fine manner with a hierarchical binary surface encoding. Unlikeprevious dense-correspondence methods, we estimate the correspondence surfaceby employing point-to-surface matching and iteratively constricting the surfaceuntil it becomes a correspondence point while gradually removing outliers.Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstratethat our method surpasses all refinement-free methods and is even on par withexpensive refinement-based approaches. Crucially, our approach iscomputationally efficient and enables real-time critical applications with highaccuracy requirements.</description><author>Yongliang Lin, Yongzhi Su, Praveen Nathan, Sandeep Inuganti, Yan Di, Martin Sundermeyer, Fabian Manhardt, Didier Stricke, Jason Rambach, Yu Zhang</author><pubDate>Sun, 31 Mar 2024 13:06:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12588v2</guid></item><item><title>CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification</title><link>http://arxiv.org/abs/2302.02314v4</link><description>The COVID-19 pandemic has resulted in hundreds of million cases and numerousdeaths worldwide. Here, we develop a novel classification network CECT bycontrollable ensemble convolutional neural network and transformer to provide atimely and accurate COVID-19 diagnosis. The CECT is composed of a parallelconvolutional encoder block, an aggregate transposed-convolutional decoderblock, and a windowed attention classification block. Each block capturesfeatures at different scales from 28 $\times$ 28 to 224 $\times$ 224 from theinput, composing enriched and comprehensive information. Different fromexisting methods, our CECT can capture features at both multi-local and globalscales without any sophisticated module design. Moreover, the contribution oflocal features at different scales can be controlled with the proposed ensemblecoefficients. We evaluate CECT on two public COVID-19 datasets and it reachesthe highest accuracy of 98.1% in the intra-dataset evaluation, outperformingexisting state-of-the-art methods. Moreover, the developed CECT achieves anaccuracy of 90.9% on the unseen dataset in the inter-dataset evaluation,showing extraordinary generalization ability. With remarkable feature captureability and generalization ability, we believe CECT can be extended to othermedical scenarios as a powerful diagnosis tool. Code is available athttps://github.com/NUS-Tim/CECT.</description><author>Zhaoshan Liu, Lei Shen</author><pubDate>Sun, 31 Mar 2024 12:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02314v4</guid></item><item><title>CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2303.11797v2</link><description>Open-vocabulary semantic segmentation presents the challenge of labeling eachpixel within an image based on a wide range of text descriptions. In this work,we introduce a novel cost-based approach to adapt vision-language foundationmodels, notably CLIP, for the intricate task of semantic segmentation. Throughaggregating the cosine similarity score, i.e., the cost volume between imageand text embeddings, our method potently adapts CLIP for segmenting seen andunseen classes by fine-tuning its encoders, addressing the challenges faced byexisting methods in handling unseen classes. Building upon this, we exploremethods to effectively aggregate the cost volume considering its multi-modalnature of being established between image and text embeddings. Furthermore, weexamine various methods for efficiently fine-tuning CLIP.</description><author>Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim</author><pubDate>Sun, 31 Mar 2024 12:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11797v2</guid></item><item><title>Can Language Models Laugh at YouTube Short-form Videos?</title><link>http://arxiv.org/abs/2310.14159v3</link><description>As short-form funny videos on social networks are gaining popularity, itbecomes demanding for AI models to understand them for better communicationwith humans. Unfortunately, previous video humor datasets target specificdomains, such as speeches or sitcoms, and mostly focus on verbal cues. Wecurate a user-generated dataset of 10K multimodal funny videos from YouTube,called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify bothverbal and visual elements contributing to humor. After filtering, we annotateeach video with timestamps and text explanations for funny moments. OurExFunTube is unique over existing datasets in that our videos cover a widerange of domains with various types of humor that necessitate a multimodalunderstanding of the content. Also, we develop a zero-shot video-to-textprompting to maximize video humor understanding of large language models(LLMs). With three different evaluation methods using automatic scores,rationale quality experiments, and human evaluations, we show that ourprompting significantly improves LLMs' ability for humor explanation.</description><author>Dayoon Ko, Sangho Lee, Gunhee Kim</author><pubDate>Sun, 31 Mar 2024 11:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14159v3</guid></item><item><title>SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing</title><link>http://arxiv.org/abs/2312.04553v2</link><description>Can we capture shape and reflectance in stealth? Such capability would bevaluable for many application domains in vision, xR, robotics, and HCI. Weintroduce structured polarization for invisible depth and reflectance sensing(SPIDeRS), the first depth and reflectance sensing method using patterns ofpolarized light. The key idea is to modulate the angle of linear polarization(AoLP) of projected light at each pixel. The use of polarization makes itinvisible and lets us recover not only depth but also directly surface normalsand even reflectance. We implement SPIDeRS with a liquid crystal spatial lightmodulator (SLM) and a polarimetric camera. We derive a novel method forrobustly extracting the projected structured polarization pattern from thepolarimetric object appearance. We evaluate the effectiveness of SPIDeRS byapplying it to a number of real-world objects. The results show that our methodsuccessfully reconstructs object shapes of various materials and is robust todiffuse reflection and ambient light. We also demonstrate relighting usingrecovered surface normals and reflectance. We believe SPIDeRS opens a newavenue of polarization use in visual sensing.</description><author>Tomoki Ichikawa, Shohei Nobuhara, Ko Nishino</author><pubDate>Sun, 31 Mar 2024 11:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04553v2</guid></item><item><title>Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction</title><link>http://arxiv.org/abs/2312.01196v2</link><description>Reconstructing dynamic objects from monocular videos is a severelyunderconstrained and challenging problem, and recent work has approached it invarious directions. However, owing to the ill-posed nature of this problem,there has been no solution that can provide consistent, high-quality novelviews from camera positions that are significantly different from the trainingviews. In this work, we introduce Neural Parametric Gaussians (NPGs) to take onthis challenge by imposing a two-stage approach: first, we fit a low-rankneural deformation model, which then is used as regularization for non-rigidreconstruction in the second stage. The first stage learns the object'sdeformations such that it preserves consistency in novel views. The secondstage obtains high reconstruction quality by optimizing 3D Gaussians that aredriven by the coarse model. To this end, we introduce a local 3D Gaussianrepresentation, where temporally shared Gaussians are anchored in and deformedby local oriented volumes. The resulting combined model can be rendered asradiance fields, resulting in high-quality photo-realistic reconstructions ofthe non-rigidly deforming objects. We demonstrate that NPGs achieve superiorresults compared to previous works, especially in challenging scenarios withfew multi-view cues.</description><author>Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen</author><pubDate>Sun, 31 Mar 2024 11:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01196v2</guid></item><item><title>gRNAde: Geometric Deep Learning for 3D RNA inverse design</title><link>http://arxiv.org/abs/2305.14749v4</link><description>Computational RNA design tasks are often posed as inverse problems, wheresequences are designed based on adopting a single desired secondary structurewithout considering 3D geometry and conformational diversity. We introducegRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to designsequences that explicitly account for structure and dynamics. Under the hood,gRNAde is a multi-state Graph Neural Network that generates candidate RNAsequences conditioned on one or more 3D backbone structures where theidentities of the bases are unknown. On a single-state fixed backbone re-designbenchmark of 14 RNA structures from the PDB identified by Das et al. [2010],gRNAde obtains higher native sequence recovery rates (56% on average) comparedto Rosetta (45% on average), taking under a second to produce designs comparedto the reported hours for Rosetta. We further demonstrate the utility of gRNAdeon a new benchmark of multi-state design for structurally flexible RNAs, aswell as zero-shot ranking of mutational fitness landscapes in a retrospectiveanalysis of a recent RNA polymerase ribozyme structure. Open source code:https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Alex Morehead, Pietro Liò</author><pubDate>Sun, 31 Mar 2024 11:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v4</guid></item><item><title>Deep Convolutional Framelet Denoising for Panoramic by Mixed Wavelet Integration</title><link>http://arxiv.org/abs/2302.10306v2</link><description>Enhancing quality and removing noise during preprocessing is one of the mostcritical steps in image processing. X-ray images are created by photonscolliding with atoms and the variation in scattered noise absorption. Thisnoise leads to a deterioration in the graph's medical quality and, at times,results in repetition, thereby increasing the patient's effective dose. One ofthe most critical challenges in this area has consistently been lowering theimage noise. Techniques like BM3d, low-pass filters, and Autoencoder have takenthis step. Owing to their structural design and high rate of repetition, neuralnetworks employing diverse architectures have, over the past decade, achievednoise reduction with satisfactory outcomes, surpassing the traditional BM3D andlow-pass filters. The combination of the Hankel matrix with neural networksrepresents one of these configurations. The Hankel matrix aims to identify alocal circle by separating individual values into local and non-localcomponents, utilizing a non-local matrix. A non-local matrix can be createdusing the wave or DCT. This paper suggests integrating the waveform with theDaubechies (D4) wavelet due to its higher energy concentration and employs theu-Net neural network architecture, which incorporates the waveform exclusivelyat each stage. The outcomes were evaluated using the PSNR and SSIM criteria,and the outcomes were verified by using various waves. The effectiveness of aone-wave network has increased from 0.5% to 1.2%, according to studies done onother datasets.</description><author>Masoud Shahraki Mohammadi, Seyed Javad Seyed Mahdavi Chabok</author><pubDate>Sun, 31 Mar 2024 11:01:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10306v2</guid></item><item><title>TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation</title><link>http://arxiv.org/abs/2401.12987v2</link><description>Emotion Recognition in Conversation (ERC) plays a crucial role in enablingdialogue systems to effectively respond to user requests. The emotions in aconversation can be identified by the representations from various modalities,such as audio, visual, and text. However, due to the weak contribution ofnon-verbal modalities to recognize emotions, multimodal ERC has always beenconsidered a challenging task. In this paper, we propose Teacher-leadingMultimodal fusion network for ERC (TelME). TelME incorporates cross-modalknowledge distillation to transfer information from a language model acting asthe teacher to the non-verbal students, thereby optimizing the efficacy of theweak modalities. We then combine multimodal features using a shifting fusionapproach in which student networks support the teacher. TelME achievesstate-of-the-art performance in MELD, a multi-speaker conversation dataset forERC. Finally, we demonstrate the effectiveness of our components throughadditional experiments.</description><author>Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, Min Song</author><pubDate>Sun, 31 Mar 2024 10:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12987v2</guid></item><item><title>Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention</title><link>http://arxiv.org/abs/2310.14770v2</link><description>Learning with abstention is a key scenario where the learner can abstain frommaking a prediction at some cost. In this paper, we analyze the score-basedformulation of learning with abstention in the multi-class classificationsetting. We introduce new families of surrogate losses for the abstention lossfunction, which include the state-of-the-art surrogate losses in thesingle-stage setting and a novel family of loss functions in the two-stagesetting. We prove strong non-asymptotic and hypothesis set-specific consistencyguarantees for these surrogate losses, which upper-bound the estimation errorof the abstention loss function in terms of the estimation error of thesurrogate loss. Our bounds can help compare different score-based surrogatesand guide the design of novel abstention algorithms by minimizing the proposedsurrogate losses. We experimentally evaluate our new algorithms on CIFAR-10,CIFAR-100, and SVHN datasets and the practical significance of our newsurrogate losses and two-stage abstention algorithms. Our results also showthat the relative performance of the state-of-the-art score-based surrogatelosses can vary across datasets.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Sun, 31 Mar 2024 10:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14770v2</guid></item><item><title>OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods</title><link>http://arxiv.org/abs/2312.08255v3</link><description>Optical coherence tomography (OCT) is a non-invasive imaging technique withextensive clinical applications in ophthalmology. OCT enables the visualizationof the retinal layers, playing a vital role in the early detection andmonitoring of retinal diseases. OCT uses the principle of light waveinterference to create detailed images of the retinal microstructures, makingit a valuable tool for diagnosing ocular conditions. This work presents anopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeledaccording to disease group and retinal pathology. The dataset consists of OCTrecords of patients with Age-related Macular Degeneration (AMD), DiabeticMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). Theimages were acquired with an Optovue Avanti RTVue XR using raster scanningprotocols with dynamic scan length and image resolution. Each retinal b-scanwas acquired by centering on the fovea and interpreted and cataloged by anexperienced retinal specialist. In this work, we applied Deep Learningclassification techniques to this new open-access dataset.</description><author>Mikhail Kulyabin, Aleksei Zhdanov, Anastasia Nikiforova, Andrey Stepichev, Anna Kuznetsova, Mikhail Ronkin, Vasilii Borisov, Alexander Bogachev, Sergey Korotkich, Paul A Constable, Andreas Maier</author><pubDate>Sun, 31 Mar 2024 10:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08255v3</guid></item><item><title>Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images</title><link>http://arxiv.org/abs/2402.05869v2</link><description>We introduce a novel approach to learn geometries such as depth and surfacenormal from images while incorporating geometric context. The difficulty ofreliably capturing geometric context in existing methods impedes their abilityto accurately enforce the consistency between the different geometricproperties, thereby leading to a bottleneck of geometric estimation quality. Wetherefore propose the Adaptive Surface Normal (ASN) constraint, a simple yetefficient method. Our approach extracts geometric context that encodes thegeometric variations present in the input image and correlates depth estimationwith geometric constraints. By dynamically determining reliable local geometryfrom randomly sampled candidates, we establish a surface normal constraint,where the validity of these candidates is evaluated using the geometriccontext. Furthermore, our normal estimation leverages the geometric context toprioritize regions that exhibit significant geometric variations, which makesthe predicted normals accurately capture intricate and detailed geometricinformation. Through the integration of geometric context, our method unifiesdepth and surface normal estimations within a cohesive framework, which enablesthe generation of high-quality 3D geometry from images. We validate thesuperiority of our approach over state-of-the-art methods through extensiveevaluations and comparisons on diverse indoor and outdoor datasets, showcasingits efficiency and robustness.</description><author>Xiaoxiao Long, Yuhang Zheng, Yupeng Zheng, Beiwen Tian, Cheng Lin, Lingjie Liu, Hao Zhao, Guyue Zhou, Wenping Wang</author><pubDate>Sun, 31 Mar 2024 10:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05869v2</guid></item><item><title>Principled Approaches for Learning to Defer with Multiple Experts</title><link>http://arxiv.org/abs/2310.14774v2</link><description>We present a study of surrogate losses and algorithms for the general problemof learning to defer with multiple experts. We first introduce a new family ofsurrogate losses specifically tailored for the multiple-expert setting, wherethe prediction and deferral functions are learned simultaneously. We then provethat these surrogate losses benefit from strong $H$-consistency bounds. Weillustrate the application of our analysis through several examples ofpractical surrogate losses, for which we give explicit guarantees. These lossfunctions readily lead to the design of new learning to defer algorithms basedon their minimization. While the main focus of this work is a theoreticalanalysis, we also report the results of several experiments on SVHN andCIFAR-10 datasets.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Sun, 31 Mar 2024 10:15:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14774v2</guid></item><item><title>HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images</title><link>http://arxiv.org/abs/2311.15672v2</link><description>As for human avatar reconstruction, contemporary techniques commonlynecessitate the acquisition of costly data and struggle to achieve satisfactoryresults from a small number of casual images. In this paper, we investigatethis task from a few-shot unconstrained photo album. The reconstruction ofhuman avatars from such data sources is challenging because of limited dataamount and dynamic articulated poses. For handling dynamic data, we integrate askinning mechanism with deep marching tetrahedra (DMTet) to form a drivabletetrahedral representation, which drives arbitrary mesh topologies generated bythe DMTet for the adaptation of unconstrained images. To effectively mineinstructive information from few-shot data, we devise a two-phase optimizationmethod with few-shot reference and few-shot guidance. The former focuses onaligning avatar identity with reference images, while the latter aims togenerate plausible appearances for unseen regions. Overall, our framework,called HaveFun, can undertake avatar reconstruction, rendering, and animation.Extensive experiments on our developed benchmarks demonstrate that HaveFunexhibits substantially superior performance in reconstructing the human bodyand hand. Project website: https://seanchenxy.github.io/HaveFunWeb/.</description><author>Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, Baoyuan Wang</author><pubDate>Sun, 31 Mar 2024 10:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15672v2</guid></item><item><title>Semantic-Fused Multi-Granularity Cross-City Traffic Prediction</title><link>http://arxiv.org/abs/2302.11774v2</link><description>Accurate traffic prediction is essential for effective urban management andthe improvement of transportation efficiency. Recently, data-driven trafficprediction methods have been widely adopted, with better performance thantraditional approaches. However, they often require large amounts of data foreffective training, which becomes challenging given the prevalence of datascarcity in regions with inadequate sensing infrastructures. To address thisissue, we propose a Semantic-Fused Multi-Granularity Transfer Learning (SFMGTL)model to achieve knowledge transfer across cities with fused semantics atdifferent granularities. In detail, we design a semantic fusion module to fusevarious semantics while conserving static spatial dependencies viareconstruction losses. Then, a fused graph is constructed based on nodefeatures through graph structure learning. Afterwards, we implementhierarchical node clustering to generate graphs with different granularity. Toextract feasible meta-knowledge, we further introduce common and privatememories and obtain domain-invariant features via adversarial training. It isworth noting that our work jointly addresses semantic fusion andmulti-granularity issues in transfer learning. We conduct extensive experimentson six real-world datasets to verify the effectiveness of our SFMGTL model bycomparing it with other state-of-the-art baselines. Afterwards, we also performablation and case studies, demonstrating that our model possesses substantiallyfewer parameters compared to baseline models. Moreover, we illustrate howknowledge transfer aids the model in accurately predicting demands, especiallyduring peak hours. The codes can be found athttps://github.com/zeonchen/SFMGTL.</description><author>Kehua Chen, Yuxuan Liang, Jindong Han, Siyuan Feng, Meixin Zhu, Hai Yang</author><pubDate>Sun, 31 Mar 2024 10:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11774v2</guid></item><item><title>Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms</title><link>http://arxiv.org/abs/2310.14772v2</link><description>We study the key framework of learning with abstention in the multi-classclassification setting. In this setting, the learner can choose to abstain frommaking a prediction with some pre-defined cost. We present a series of newtheoretical and algorithmic results for this learning problem in thepredictor-rejector framework. We introduce several new families of surrogatelosses for which we prove strong non-asymptotic and hypothesis set-specificconsistency guarantees, thereby resolving positively two existing openquestions. These guarantees provide upper bounds on the estimation error of theabstention loss function in terms of that of the surrogate loss. We analyzeboth a single-stage setting where the predictor and rejector are learnedsimultaneously and a two-stage setting crucial in applications, where thepredictor is learned in a first stage using a standard surrogate loss such ascross-entropy. These guarantees suggest new multi-class abstention algorithmsbased on minimizing these surrogate losses. We also report the results ofextensive experiments comparing these algorithms to the currentstate-of-the-art algorithms on CIFAR-10, CIFAR-100 and SVHN datasets. Ourresults demonstrate empirically the benefit of our new surrogate losses andshow the remarkable performance of our broadly applicable two-stage abstentionalgorithm.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Sun, 31 Mar 2024 10:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14772v2</guid></item><item><title>A Provably Accurate Randomized Sampling Algorithm for Logistic Regression</title><link>http://arxiv.org/abs/2402.16326v3</link><description>In statistics and machine learning, logistic regression is a widely-usedsupervised learning technique primarily employed for binary classificationtasks. When the number of observations greatly exceeds the number of predictorvariables, we present a simple, randomized sampling-based algorithm forlogistic regression problem that guarantees high-quality approximations to boththe estimated probabilities and the overall discrepancy of the model. Ouranalysis builds upon two simple structural conditions that boil down torandomized matrix multiplication, a fundamental and well-understood primitiveof randomized numerical linear algebra. We analyze the properties of estimatedprobabilities of logistic regression when leverage scores are used to sampleobservations, and prove that accurate approximations can be achieved with asample whose size is much smaller than the total number of observations. Tofurther validate our theoretical findings, we conduct comprehensive empiricalevaluations. Overall, our work sheds light on the potential of using randomizedsampling approaches to efficiently approximate the estimated probabilities inlogistic regression, offering a practical and computationally efficientsolution for large-scale datasets.</description><author>Agniva Chowdhury, Pradeep Ramuhalli</author><pubDate>Sun, 31 Mar 2024 09:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16326v3</guid></item><item><title>Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems</title><link>http://arxiv.org/abs/2307.16120v4</link><description>Combining the strengths of model-based iterative algorithms and data-drivendeep learning solutions, deep unrolling networks (DuNets) have become a populartool to solve inverse imaging problems. While DuNets have been successfullyapplied to many linear inverse problems, nonlinear problems tend to impair theperformance of the method. Inspired by momentum acceleration techniques thatare often used in optimization algorithms, we propose a recurrent momentumacceleration (RMA) framework that uses a long short-term memory recurrentneural network (LSTM-RNN) to simulate the momentum acceleration process. TheRMA module leverages the ability of the LSTM-RNN to learn and retain knowledgefrom the previous gradients. We apply RMA to two popular DuNets -- the learnedproximal gradient descent (LPGD) and the learned primal-dual (LPD) methods,resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental resultson two nonlinear inverse problems: a nonlinear deconvolution problem, and anelectrical impedance tomography problem with limited boundary measurements. Inthe first experiment we have observed that the improvement due to RMA largelyincreases with respect to the nonlinearity of the problem. The results of thesecond example further demonstrate that the RMA schemes can significantlyimprove the performance of DuNets in strongly ill-posed problems.</description><author>Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li</author><pubDate>Sun, 31 Mar 2024 09:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16120v4</guid></item><item><title>DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior</title><link>http://arxiv.org/abs/2308.15070v2</link><description>We present DiffBIR, a general restoration pipeline that could handledifferent blind image restoration tasks in a unified framework. DiffBIRdecouples blind image restoration problem into two stages: 1) degradationremoval: removing image-independent content; 2) information regeneration:generating the lost image content. Each stage is developed independently butthey work seamlessly in a cascaded manner. In the first stage, we userestoration modules to remove degradations and obtain high-fidelity restoredresults. For the second stage, we propose IRControlNet that leverages thegenerative ability of latent diffusion models to generate realistic details.Specifically, IRControlNet is trained based on specially produced conditionimages without distracting noisy content for stable generation performance.Moreover, we design a region-adaptive restoration guidance that can modify thedenoising process during inference without model re-training, allowing users tobalance realness and fidelity through a tunable guidance scale. Extensiveexperiments have demonstrated DiffBIR's superiority over state-of-the-artapproaches for blind image super-resolution, blind face restoration and blindimage denoising tasks on both synthetic and real-world datasets. The code isavailable at https://github.com/XPixelGroup/DiffBIR.</description><author>Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, Chao Dong</author><pubDate>Sun, 31 Mar 2024 09:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15070v2</guid></item><item><title>Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers</title><link>http://arxiv.org/abs/2312.12835v2</link><description>Byzantine machine learning has garnered considerable attention in light ofthe unpredictable faults that can occur in large-scale distributed learningsystems. The key to secure resilience against Byzantine machines in distributedlearning is resilient aggregation mechanisms. Although abundant resilientaggregation rules have been proposed, they are designed in ad-hoc manners,imposing extra barriers on comparing, analyzing, and improving the rules acrossperformance criteria. This paper studies near-optimal aggregation rules usingclustering in the presence of outliers. Our outlier-robust clustering approachutilizes geometric properties of the update vectors provided by workers. Ouranalysis show that constant approximations to the 1-center and 1-meanclustering problems with outliers provide near-optimal resilient aggregatorsfor metric-based criteria, which have been proven to be crucial in thehomogeneous and heterogeneous cases respectively. In addition, we discuss twocontradicting types of attacks under which no single aggregation rule isguaranteed to improve upon the naive average. Based on the discussion, wepropose a two-phase resilient aggregation framework. We run experiments forimage classification using a non-convex loss function. The proposed algorithmsoutperform previously known aggregation rules by a large margin with bothhomogeneous and heterogeneous data distributions among non-faulty workers. Codeand appendix are available at https://github.com/jerry907/AAAI24-RASHB.</description><author>Yuhao Yi, Ronghui You, Hong Liu, Changxin Liu, Yuan Wang, Jiancheng Lv</author><pubDate>Sun, 31 Mar 2024 09:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12835v2</guid></item><item><title>Compressing Large Language Models by Streamlining the Unimportant Layer</title><link>http://arxiv.org/abs/2403.19135v2</link><description>Large language models (LLM) have been extensively applied in various naturallanguage tasks and domains, but their applicability is constrained by the largenumber of parameters of the models. Consequently, there is an increasingemphasis on compact models that exhibit high performance. In this study, weobserve that different layers in LLM have varying degrees of perturbation onthe hidden states, which allows us to identify less important layers. Based onthis phenomenon, we propose LLM-Streamline, which consists of two parts: layerpruning, where we remove a set of consecutive layers with the lowest importancein the model according to the target sparsity; and layer replacement, where wetrain a lightweight model to substitute the pruned layers, thereby mitigatingthe performance degradation caused by pruning. In our experiments, we utilizestructures such as a multi-layer perceptron (MLP) and a transformer layer aslightweight models and ultimately demonstrate that a single MLP can effectivelyfit the pruned layers. Comprehensive experiments show that our proposed method,LLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruningmethods.</description><author>Xiaodong Chen, Yuxuan Hu, Jing Zhang</author><pubDate>Sun, 31 Mar 2024 09:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19135v2</guid></item><item><title>Prompt Tuning with Soft Context Sharing for Vision-Language Models</title><link>http://arxiv.org/abs/2208.13474v2</link><description>Vision-language models have recently shown great potential on many tasks incomputer vision. Meanwhile, prior work demonstrates prompt tuning designed forvision-language models could acquire superior performance on few-shot imagerecognition compared to linear probe, a strong baseline. In practice, manyfew-shot tasks are inherently correlated, particularly within specializeddomains. However, such information is overlooked previously. Inspired by thefact that modeling task relationship by multi-task learning can usually boostperformance, we propose a novel method SoftCPT (Soft Context Sharing for PromptTuning) to tune pre-trained vision-language models on multiple target few-shottasks jointly. Specifically, we design a task-shared meta network to generateprompt context for each task using task name together with a learnable taskcontext as input. The parameters of this meta network as well as the taskcontext are tuned on the joint training set of all tasks. As such, the promptcontext of all tasks will be shared in a soft manner. Extensive experimentsacross four multi-task few-shot datasets covering 44 tasks and 1593 categoriesdemonstrate that SoftCPT significantly outperforms single-task prompt tuningmethods, highlighting the effectiveness of multi-task learning forvision-language prompt tuning. Code is available athttps://github.com/kding1225/softcpt.</description><author>Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Haojian Zhang, Shiming Xiang, Chunhong Pan</author><pubDate>Sun, 31 Mar 2024 09:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.13474v2</guid></item><item><title>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</title><link>http://arxiv.org/abs/2311.16096v3</link><description>Modeling animatable human avatars from RGB videos is a long-standing andchallenging problem. Recent works usually adopt MLP-based neural radiancefields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs toregress pose-dependent garment details. To this end, we introduce AnimatableGaussians, a new avatar representation that leverages powerful 2D CNNs and 3DGaussian splatting to create high-fidelity avatars. To associate 3D Gaussianswith the animatable avatar, we learn a parametric template from the inputvideos, and then parameterize the template on two front \&amp; back canonicalGaussian maps where each pixel represents a 3D Gaussian. The learned templateis adaptive to the wearing garments for modeling looser clothes like dresses.Such template-guided 2D parameterization enables us to employ a powerfulStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modelingdetailed dynamic appearances. Furthermore, we introduce a pose projectionstrategy for better generalization given novel poses. Overall, our method cancreate lifelike avatars with dynamic, realistic and generalized appearances.Experiments show that our method outperforms other state-of-the-art approaches.Code: https://github.com/lizhe00/AnimatableGaussians</description><author>Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</author><pubDate>Sun, 31 Mar 2024 09:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16096v3</guid></item><item><title>Instructing Robots by Sketching: Learning from Demonstration via Probabilistic Diagrammatic Teaching</title><link>http://arxiv.org/abs/2309.03835v3</link><description>Learning for Demonstration (LfD) enables robots to acquire new skills byimitating expert demonstrations, allowing users to communicate theirinstructions in an intuitive manner. Recent progress in LfD often relies onkinesthetic teaching or teleoperation as the medium for users to specify thedemonstrations. Kinesthetic teaching requires physical handling of the robot,while teleoperation demands proficiency with additional hardware. This paperintroduces an alternative paradigm for LfD called Diagrammatic Teaching.Diagrammatic Teaching aims to teach robots novel skills by prompting the userto sketch out demonstration trajectories on 2D images of the scene, these arethen synthesised as a generative model of motion trajectories in 3D task space.Additionally, we present the Ray-tracing Probabilistic Trajectory Learning(RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varyingprobability densities from the 2D sketches, applies ray-tracing to findcorresponding regions in 3D Cartesian space, and fits a probabilistic model ofmotion trajectories to these regions. New motion trajectories, which mimicthose sketched by the user, can then be generated from the probabilistic model.We empirically validate our framework both in simulation and on real robots,which include a fixed-base manipulator and a quadruped-mounted manipulator.</description><author>Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson</author><pubDate>Sun, 31 Mar 2024 08:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03835v3</guid></item><item><title>G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System</title><link>http://arxiv.org/abs/2210.09846v3</link><description>Navigating dynamic physical environments without obstructing or damaginghuman assets is of quintessential importance for social robots. In this work,we solve autonomous drone navigation's sub-problem of predicting out-of-domainhuman and agent trajectories using a deep generative model. Our method:General-PECNet or G-PECNet observes an improvement of 9.5\% on the FinalDisplacement Error (FDE) on 2020's benchmark: PECNet through a combination ofarchitectural improvements inspired by periodic activation functions andsynthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) andReinforcement Learning (RL). Additionally, we propose a simplegeometry-inspired metric for trajectory non-linearity and outlier detection,helpful for the task. Code available athttps://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git</description><author>Aryan Garg, Renu M. Rameshan</author><pubDate>Sun, 31 Mar 2024 08:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09846v3</guid></item><item><title>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</title><link>http://arxiv.org/abs/2402.09989v3</link><description>Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodaltask that aims to identify named entities, entity types and their correspondingvisual regions. GMNER task exhibits two challenging properties: 1) The weakcorrelation between image-text pairs in social media results in a significantportion of named entities being ungroundable. 2) There exists a distinctionbetween coarse-grained referring expressions commonly used in similar tasks(e.g., phrase localization, referring expression comprehension) andfine-grained named entities. In this paper, we propose RiVEG, a unifiedframework that reformulates GMNER into a joint MNER-VE-VG task by leveraginglarge language models (LLMs) as a connecting bridge. This reformulation bringstwo benefits: 1) It maintains the optimal MNER performance and eliminates theneed for employing object detection methods to pre-extract regional features,thereby naturally addressing two major limitations of existing GMNER methods.2) The introduction of entity expansion expression and Visual Entailment (VE)Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enablesRiVEG to effortlessly inherit the Visual Entailment and Visual Groundingcapabilities of any current or prospective multimodal pretraining models.Extensive experiments demonstrate that RiVEG outperforms state-of-the-artmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,6.21%, and 8.83% in all three subtasks.</description><author>Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan</author><pubDate>Sun, 31 Mar 2024 08:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09989v3</guid></item><item><title>Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework</title><link>http://arxiv.org/abs/2403.07636v4</link><description>Medical vision language pre-training (VLP) has emerged as a frontier ofresearch, enabling zero-shot pathological recognition by comparing the queryimage with the textual descriptions for each disease. Due to the complexsemantics of biomedical texts, current methods struggle to align medical imageswith key pathological findings in unstructured reports. This leads to themisalignment with the target disease's textual representation. In this paper,we introduce a novel VLP framework designed to dissect disease descriptionsinto their fundamental aspects, leveraging prior knowledge about the visualmanifestations of pathologies. This is achieved by consulting a large languagemodel and medical experts. Integrating a Transformer module, our approachaligns an input image with the diverse elements of a disease, generatingaspect-centric image representations. By consolidating the matches from eachaspect, we improve the compatibility between an image and its associateddisease. Additionally, capitalizing on the aspect-oriented representations, wepresent a dual-head Transformer tailored to process known and unknown diseases,optimizing the comprehensive detection efficacy. Conducting experiments onseven downstream datasets, ours improves the accuracy of recent methods by upto 8.56% and 17.26% for seen and unseen categories, respectively. Our code isreleased at https://github.com/HieuPhan33/MAVL.</description><author>Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans</author><pubDate>Sun, 31 Mar 2024 08:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07636v4</guid></item><item><title>UrbanGPT: Spatio-Temporal Large Language Models</title><link>http://arxiv.org/abs/2403.00813v2</link><description>Spatio-temporal prediction aims to forecast and gain insights into theever-changing dynamics of urban environments across both time and space. Itspurpose is to anticipate future patterns, trends, and events in diverse facetsof urban life, including transportation, population movement, and crime rates.Although numerous efforts have been dedicated to developing neural networktechniques for accurate predictions on spatio-temporal data, it is important tonote that many of these methods heavily depend on having sufficient labeleddata to generate precise spatio-temporal representations. Unfortunately, theissue of data scarcity is pervasive in practical urban sensing scenarios.Consequently, it becomes necessary to build a spatio-temporal model with stronggeneralization capabilities across diverse spatio-temporal learning scenarios.Taking inspiration from the remarkable achievements of large language models(LLMs), our objective is to create a spatio-temporal LLM that can exhibitexceptional generalization capabilities across a wide range of downstream urbantasks. To achieve this objective, we present the UrbanGPT, which seamlesslyintegrates a spatio-temporal dependency encoder with the instruction-tuningparadigm. This integration enables LLMs to comprehend the complexinter-dependencies across time and space, facilitating more comprehensive andaccurate predictions under data scarcity. To validate the effectiveness of ourapproach, we conduct extensive experiments on various public datasets, coveringdifferent spatio-temporal prediction tasks. The results consistentlydemonstrate that our UrbanGPT, with its carefully designed architecture,consistently outperforms state-of-the-art baselines. These findings highlightthe potential of building large language models for spatio-temporal learning,particularly in zero-shot scenarios where labeled data is scarce.</description><author>Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang</author><pubDate>Sun, 31 Mar 2024 07:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00813v2</guid></item><item><title>SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System</title><link>http://arxiv.org/abs/2312.01616v3</link><description>Accuracy and computational efficiency are the most important metrics toVisual Inertial Navigation System (VINS). The existing VINS algorithms witheither high accuracy or low computational complexity, are difficult to providethe high precision localization in resource-constrained devices. To this end,we propose a novel filter-based VINS framework named SchurVINS, which couldguarantee both high accuracy by building a complete residual model and lowcomputational complexity with Schur complement. Technically, we first formulatethe full residual model where Gradient, Hessian and observation covariance areexplicitly modeled. Then Schur complement is employed to decompose the fullmodel into ego-motion residual model and landmark residual model. Finally,Extended Kalman Filter (EKF) update is implemented in these two models withhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our methodnotably outperforms state-of-the-art (SOTA) methods in both accuracy andcomputational complexity. The experimental code of SchurVINS is available athttps://github.com/bytedance/SchurVINS.</description><author>Yunfei Fan, Tianyu Zhao, Guidong Wang</author><pubDate>Sun, 31 Mar 2024 06:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01616v3</guid></item><item><title>Segment Anything Model for Road Network Graph Extraction</title><link>http://arxiv.org/abs/2403.16051v2</link><description>We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) forextracting large-scale, vectorized road network graphs from satellite imagery.To predict graph geometry, we formulate it as a dense semantic segmentationtask, leveraging the inherent strengths of SAM. The image encoder of SAM isfine-tuned to produce probability masks for roads and intersections, from whichthe graph vertices are extracted via simple non-maximum suppression. To predictgraph topology, we designed a lightweight transformer-based graph neuralnetwork, which leverages the SAM image embeddings to estimate the edgeexistence probabilities between vertices. Our approach directly predicts thegraph vertices and edges for large regions without expensive and complexpost-processing heuristics, and is capable of building complete road networkgraphs spanning multiple square kilometers in a matter of seconds. With itssimple, straightforward, and minimalist design, SAM-Road achieves comparableaccuracy with the state-of-the-art method RNGDet++, while being 40 times fasteron the City-scale dataset. We thus demonstrate the power of a foundationalvision model when applied to a graph learning task. The code is available athttps://github.com/htcr/sam_road.</description><author>Congrui Hetang, Haoru Xue, Cindy Le, Tianwei Yue, Wenping Wang, Yihui He</author><pubDate>Sun, 31 Mar 2024 06:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16051v2</guid></item><item><title>RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception</title><link>http://arxiv.org/abs/2403.10145v2</link><description>The value of roadside perception, which could extend the boundaries ofautonomous driving and traffic management, has gradually become more prominentand acknowledged in recent years. However, existing roadside perceptionapproaches only focus on the single-infrastructure sensor system, which cannotrealize a comprehensive understanding of a traffic area because of the limitedsensing range and blind spots. Orienting high-quality roadside perception, weneed Roadside Cooperative Perception (RCooper) to achieve practicalarea-coverage roadside perception for restricted traffic areas. Rcooper has itsown domain-specific challenges, but further exploration is hindered due to thelack of datasets. We hence release the first real-world, large-scale RCooperdataset to bloom the research on practical roadside cooperative perception,including detection and tracking. The manually annotated dataset comprises 50kimages and 30k point clouds, including two representative traffic scenes (i.e.,intersection and corridor). The constructed benchmarks prove the effectivenessof roadside cooperation perception and demonstrate the direction of furtherresearch. Codes and dataset can be accessed at:https://github.com/AIR-THU/DAIR-RCooper.</description><author>Ruiyang Hao, Siqi Fan, Yingru Dai, Zhenlin Zhang, Chenxi Li, Yuntian Wang, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie</author><pubDate>Sun, 31 Mar 2024 06:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10145v2</guid></item><item><title>3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image</title><link>http://arxiv.org/abs/2401.06415v2</link><description>This paper introduces a novel pipeline to reconstruct the geometry ofinteracting multi-person in clothing on a globally coherent scene space from asingle image. The main challenge arises from the occlusion: a part of a humanbody is not visible from a single view due to the occlusion by others or theself, which introduces missing geometry and physical implausibility (e.g.,penetration). We overcome this challenge by utilizing two human priors forcomplete 3D geometry and surface contacts. For the geometry prior, an encoderlearns to regress the image of a person with missing body parts to the latentvectors; a decoder decodes these vectors to produce 3D features of theassociated geometry; and an implicit network combines these features with asurface normal map to reconstruct a complete and detailed 3D humans. For thecontact prior, we develop an image-space contact detector that outputs aprobability distribution of surface contacts between people in 3D. We use thesepriors to globally refine the body poses, enabling the penetration-free andaccurate reconstruction of interacting multi-person in clothing on the scenespace. The results demonstrate that our method is complete, globally coherent,and physically plausible compared to existing methods.</description><author>Junuk Cha, Hansol Lee, Jaewon Kim, Nhat Nguyen Bao Truong, Jae Shin Yoon, Seungryul Baek</author><pubDate>Sun, 31 Mar 2024 06:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06415v2</guid></item><item><title>Iterative Sketching for Secure Coded Regression</title><link>http://arxiv.org/abs/2308.04185v2</link><description>Linear regression is a fundamental and primitive problem in supervisedmachine learning, with applications ranging from epidemiology to finance. Inthis work, we propose methods for speeding up distributed linear regression. Wedo so by leveraging randomized techniques, while also ensuring security andstraggler resiliency in asynchronous distributed computing systems.Specifically, we randomly rotate the basis of the system of equations and thensubsample blocks, to simultaneously secure the information and reduce thedimension of the regression problem. In our setup, the basis rotationcorresponds to an encoded encryption in an approximate gradient coding scheme,and the subsampling corresponds to the responses of the non-straggling serversin the centralized coded computing framework. This results in a distributiveiterative stochastic approach for matrix compression and steepest descent.</description><author>Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O. Hero III</author><pubDate>Sun, 31 Mar 2024 06:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04185v2</guid></item><item><title>LangSplat: 3D Language Gaussian Splatting</title><link>http://arxiv.org/abs/2312.16084v2</link><description>Humans live in a 3D world and commonly use natural language to interact witha 3D scene. Modeling a 3D language field to support open-ended language queriesin 3D has gained increasing attention recently. This paper introducesLangSplat, which constructs a 3D language field that enables precise andefficient open-vocabulary querying within 3D spaces. Unlike existing methodsthat ground CLIP language embeddings in a NeRF model, LangSplat advances thefield by utilizing a collection of 3D Gaussians, each encoding languagefeatures distilled from CLIP, to represent the language field. By employing atile-based splatting technique for rendering language features, we circumventthe costly rendering process inherent in NeRF. Instead of directly learningCLIP embeddings, LangSplat first trains a scene-wise language autoencoder andthen learns language features on the scene-specific latent space, therebyalleviating substantial memory demands imposed by explicit modeling. Existingmethods struggle with imprecise and vague 3D language fields, which fail todiscern clear boundaries between objects. We delve into this issue and proposeto learn hierarchical semantics using SAM, thereby eliminating the need forextensively querying the language field across various scales and theregularization of DINO features. Extensive experimental results show thatLangSplat significantly outperforms the previous state-of-the-art method LERFby a large margin. Notably, LangSplat is extremely efficient, achieving a 199$\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. Westrongly recommend readers to check out our video results athttps://langsplat.github.io/</description><author>Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister</author><pubDate>Sun, 31 Mar 2024 05:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16084v2</guid></item><item><title>DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue</title><link>http://arxiv.org/abs/2308.08043v3</link><description>Large Language Models (LLMs), such as ChatGPT, are increasingly sophisticatedand exhibit capabilities closely resembling those of humans. A significantapplication of these LLMs is their use as chat agents, responding to humaninquiries across various domains. While current LLMs proficiently answergeneral questions, they often fall short in complex diagnostic scenarios suchas legal, medical, or other specialized consultations. These scenariostypically require Task-Oriented Dialogue (TOD), where an AI chat agent mustproactively pose questions and guide users toward specific goals or taskcompletion. Previous fine-tuning models have underperformed in TOD and the fullpotential of this capability in current LLMs has not yet been fully explored.In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovativeapproach that extends LLMs to more TOD scenarios. In addition to guiding usersto complete tasks, DiagGPT can effectively manage the status of all topicsthroughout the dialogue development. This feature enhances user experience andoffers a more flexible interaction in TOD. Our experiments demonstrate thatDiagGPT exhibits outstanding performance in conducting TOD with users, showingits potential for practical applications in various fields.</description><author>Lang Cao</author><pubDate>Sun, 31 Mar 2024 05:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08043v3</guid></item><item><title>Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</title><link>http://arxiv.org/abs/2403.04801v2</link><description>In this paper, we introduce a black-box prompt optimization method that usesan attacker LLM agent to uncover higher levels of memorization in a victimagent, compared to what is revealed by prompting the target model with thetraining data directly, which is the dominant approach of quantifyingmemorization in LLMs. We use an iterative rejection-sampling optimizationprocess to find instruction-based prompts with two main characteristics: (1)minimal overlap with the training data to avoid presenting the solutiondirectly to the model, and (2) maximal overlap between the victim model'soutput and the training data, aiming to induce the victim to spit out trainingdata. We observe that our instruction-based prompts generate outputs with 23.7%higher overlap with training data compared to the baseline prefix-suffixmeasurements. Our findings show that (1) instruction-tuned models can exposepre-training data as much as their base-models, if not more so, (2) contextsother than the original training data can lead to leakage, and (3) usinginstructions proposed by other LLMs can open a new avenue of automated attacksthat we should further study and explore. The code can be found athttps://github.com/Alymostafa/Instruction_based_attack .</description><author>Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana</author><pubDate>Sun, 31 Mar 2024 05:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04801v2</guid></item><item><title>Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil</title><link>http://arxiv.org/abs/2312.03041v2</link><description>This paper presents a novel deep learning model based on the transformerarchitecture to predict the load-deformation behavior of large bored piles inBangkok subsoil. The model encodes the soil profile and pile features astokenization input, and generates the load-deformation curve as output. Themodel also incorporates the previous sequential data of load-deformation curveinto the decoder to improve the prediction accuracy. The model alsoincorporates the previous sequential data of load-deformation curve into thedecoder. The model shows a satisfactory accuracy and generalization ability forthe load-deformation curve prediction, with a mean absolute error of 5.72% forthe test data. The model could also be used for parametric analysis and designoptimization of piles under different soil and pile conditions, pile crosssection, pile length and type of pile.</description><author>Sompote Youwai, Chissanupong Thongnoo</author><pubDate>Sun, 31 Mar 2024 05:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03041v2</guid></item><item><title>Ensuring User-side Fairness in Dynamic Recommender Systems</title><link>http://arxiv.org/abs/2308.15651v2</link><description>User-side group fairness is crucial for modern recommender systems, aiming toalleviate performance disparities among user groups defined by sensitiveattributes like gender, race, or age. In the ever-evolving landscape ofuser-item interactions, continual adaptation to newly collected data is crucialfor recommender systems to stay aligned with the latest user preferences.However, we observe that such continual adaptation often exacerbatesperformance disparities. This necessitates a thorough investigation intouser-side fairness in dynamic recommender systems, an area that has beenunexplored in the literature. This problem is challenging due to distributionshifts, frequent model updates, and non-differentiability of ranking metrics.To our knowledge, this paper presents the first principled study on ensuringuser-side fairness in dynamic recommender systems. We start with theoreticalanalyses on fine-tuning v.s. retraining, showing that the best practice isincremental fine-tuning with restart. Guided by our theoretical analyses, wepropose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework todynamically ensure user-side fairness over time. To overcome thenon-differentiability of recommendation metrics in the fairness loss, wefurther introduce Differentiable Hit (DH) as an improvement over the recentNeuralNDCG method, not only alleviating its gradient vanishing issue but alsoachieving higher efficiency. Besides that, we also address the instabilityissue of the fairness loss by leveraging the competing nature between therecommendation loss and the fairness loss. Through extensive experiments onreal-world datasets, we demonstrate that FADE effectively and efficientlyreduces performance disparities with little sacrifice in the overallrecommendation performance.</description><author>Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou, Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, Hanghang Tong</author><pubDate>Sun, 31 Mar 2024 05:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15651v2</guid></item><item><title>Guided Slot Attention for Unsupervised Video Object Segmentation</title><link>http://arxiv.org/abs/2303.08314v3</link><description>Unsupervised video object segmentation aims to segment the most prominentobject in a video sequence. However, the existence of complex backgrounds andmultiple foreground objects make this task challenging. To address this issue,we propose a guided slot attention network to reinforce spatial structuralinformation and obtain better foreground--background separation. The foregroundand background slots, which are initialized with query guidance, areiteratively refined based on interactions with template information.Furthermore, to improve slot--template interaction and effectively fuse globaland local features in the target and reference frames, K-nearest neighborsfiltering and a feature aggregation transformer are introduced. The proposedmodel achieves state-of-the-art performance on two popular datasets.Additionally, we demonstrate the robustness of the proposed model inchallenging scenes through various comparative experiments.</description><author>Minhyeok Lee, Suhwan Cho, Dogyoon Lee, Chaewon Park, Jungho Lee, Sangyoun Lee</author><pubDate>Sun, 31 Mar 2024 05:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08314v3</guid></item><item><title>Visual Hallucination: Definition, Quantification, and Prescriptive Remediations</title><link>http://arxiv.org/abs/2403.17306v2</link><description>The troubling rise of hallucination presents perhaps the most significantimpediment to the advancement of responsible AI. In recent times, considerableresearch has focused on detecting and mitigating hallucination in LargeLanguage Models (LLMs). However, it's worth noting that hallucination is alsoquite prevalent in Vision-Language models (VLMs). In this paper, we offer afine-grained discourse on profiling VLM hallucination based on two tasks: i)image captioning, and ii) Visual Question Answering (VQA). We delineate eightfine-grained orientations of visual hallucination: i) Contextual Guessing, ii)Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) GenderAnomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) NumericDiscrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publiclyavailable dataset comprising 2,000 samples generated using eight VLMs acrosstwo tasks of captioning and VQA along with human annotations for the categoriesas mentioned earlier.</description><author>Anku Rani, Vipula Rawte, Harshad Sharma, Neeraj Anand, Krishnav Rajbangshi, Amit Sheth, Amitava Das</author><pubDate>Sun, 31 Mar 2024 04:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17306v2</guid></item><item><title>DistillSpec: Improving Speculative Decoding via Knowledge Distillation</title><link>http://arxiv.org/abs/2310.08461v2</link><description>Speculative decoding (SD) accelerates large language model inference byemploying a faster draft model for generating multiple tokens, which are thenverified in parallel by the larger target model, resulting in the textgenerated according to the target model distribution. However, identifying acompact draft model that is well-aligned with the target model is challenging.To tackle this issue, we propose DistillSpec that uses knowledge distillationto better align the draft model with the target model, before applying SD.DistillSpec makes two key design choices, which we demonstrate via systematicstudy to be crucial to improving the draft and target alignment: utilizingon-policy data generation from the draft model, and tailoring the divergencefunction to the task and decoding strategy. Notably, DistillSpec yieldsimpressive 10 - 45% speedups over standard SD on a range of standardbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combineDistillSpec with lossy SD to achieve fine-grained control over the latency vs.task performance trade-off. Finally, in practical scenarios with models ofvarying sizes, first using distillation to boost the performance of the targetmodel and then applying DistillSpec to train a well-aligned draft model canreduce decoding latency by 6-10x with minimal performance drop, compared tostandard decoding without distillation.</description><author>Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal</author><pubDate>Sun, 31 Mar 2024 04:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08461v2</guid></item><item><title>A Survey on Deep Learning and State-of-the-art Applications</title><link>http://arxiv.org/abs/2403.17561v2</link><description>Deep learning, a branch of artificial intelligence, is a computational modelthat uses multiple layers of interconnected units (neurons) to learn intricatepatterns and representations directly from raw input data. Empowered by thislearning capability, it has become a powerful tool for solving complex problemsand is the core driver of many groundbreaking technologies and innovations.Building a deep learning model is a challenging task due to the algorithm`scomplexity and the dynamic nature of real-world problems. Several studies havereviewed deep learning concepts and applications. However, the studies mostlyfocused on the types of deep learning models and convolutional neural networkarchitectures, offering limited coverage of the state-of-the-art of deeplearning models and their applications in solving complex problems acrossdifferent domains. Therefore, motivated by the limitations, this study aims tocomprehensively review the state-of-the-art deep learning models in computervision, natural language processing, time series analysis and pervasivecomputing. We highlight the key features of the models and their effectivenessin solving the problems within each domain. Furthermore, this study presentsthe fundamentals of deep learning, various deep learning model types andprominent convolutional neural network architectures. Finally, challenges andfuture directions in deep learning research are discussed to offer a broaderperspective for future researchers.</description><author>Mohd Halim Mohd Noor, Ayokunle Olalekan Ige</author><pubDate>Sun, 31 Mar 2024 04:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17561v2</guid></item></channel></rss>