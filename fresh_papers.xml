<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 10 Aug 2023 06:00:42 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Scene-Generalizable Interactive Segmentation of Radiance Fields</title><link>http://arxiv.org/abs/2308.05104v1</link><description>Existing methods for interactive segmentation in radiance fields entailscene-specific optimization and thus cannot generalize across different scenes,which greatly limits their applicability. In this work we make the firstattempt at Scene-Generalizable Interactive Segmentation in Radiance Fields(SGISRF) and propose a novel SGISRF method, which can perform 3D objectsegmentation for novel (unseen) scenes represented by radiance fields, guidedby only a few interactive user clicks in a given set of multi-view 2D images.In particular, the proposed SGISRF focuses on addressing three crucialchallenges with three specially designed techniques. First, we devise theCross-Dimension Guidance Propagation to encode the scarce 2D user clicks intoinformative 3D guidance representations. Second, the Uncertainty-Eliminated 3DSegmentation module is designed to achieve efficient yet effective 3Dsegmentation. Third, Concealment-Revealed Supervised Learning scheme isproposed to reveal and correct the concealed 3D segmentation errors resultedfrom the supervision in 2D space with only 2D mask annotations. Extensiveexperiments on two real-world challenging benchmarks covering diverse scenesdemonstrate 1) effectiveness and scene-generalizability of the proposed method,2) favorable performance compared to classical method requiring scene-specificoptimization.</description><author>Songlin Tang, Wenjie Pei, Xin Tao, Tanghui Jia, Guangming Lu, Yu-Wing Tai</author><pubDate>Wed, 09 Aug 2023 18:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05104v1</guid></item><item><title>Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction</title><link>http://arxiv.org/abs/2308.05103v1</link><description>Diffusion MRI is commonly performed using echo-planar imaging (EPI) due toits rapid acquisition time. However, the resolution of diffusion-weightedimages is often limited by magnetic field inhomogeneity-related artifacts andblurring induced by T2- and T2*-relaxation effects. To address theselimitations, multi-shot EPI (msEPI) combined with parallel imaging techniquesis frequently employed. Nevertheless, reconstructing msEPI can be challengingdue to phase variation between multiple shots. In this study, we introduce anovel msEPI reconstruction approach called zero-MIRID (zero-shotself-supervised learning of Multi-shot Image Reconstruction for ImprovedDiffusion MRI). This method jointly reconstructs msEPI data by incorporatingdeep learning-based image regularization techniques. The network incorporatesCNN denoisers in both k- and image-spaces, while leveraging virtual coils toenhance image reconstruction conditioning. By employing a self-supervisedlearning technique and dividing sampled data into three groups, the proposedapproach achieves superior results compared to the state-of-the-art parallelimaging method, as demonstrated in an in-vivo experiment.</description><author>Jaejin Cho, Yohan Jun, Xiaoqing Wang, Caique Kobayashi, Berkin Bilgic</author><pubDate>Wed, 09 Aug 2023 18:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05103v1</guid></item><item><title>DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels</title><link>http://arxiv.org/abs/2308.05101v1</link><description>The enormous demand for annotated data brought forth by deep learningtechniques has been accompanied by the problem of annotation noise. Althoughthis issue has been widely discussed in machine learning literature, it hasbeen relatively unexplored in the context of "multi-label classification" (MLC)tasks which feature more complicated kinds of noise. Additionally, when thedomain in question has certain logical constraints, noisy annotations oftenexacerbate their violations, making such a system unacceptable to an expert.This paper studies the effect of label noise on domain rule violation incidentsin the MLC task, and incorporates domain rules into our learning algorithm tomitigate the effect of noise. We propose the Domain Obedient Self-supervisedTraining (DOST) paradigm which not only makes deep learning models more alignedto domain rules, but also improves learning performance in key metrics andminimizes the effect of annotation noise. This novel approach uses domainguidance to detect offending annotations and deter rule-violating predictionsin a self-supervised manner, thus making it more "data efficient" and domaincompliant. Empirical studies, performed over two large scale multi-labelclassification datasets, demonstrate that our method results in improvementacross the board, and often entirely counteracts the effect of noise.</description><author>Soumadeep Saha, Utpal Garain, Arijit Ukil, Arpan Pal, Sundeep Khandelwal</author><pubDate>Wed, 09 Aug 2023 18:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05101v1</guid></item><item><title>An out-of-distribution discriminator based on Bayesian neural network epistemic uncertainty</title><link>http://arxiv.org/abs/2210.10780v2</link><description>Neural networks have revolutionized the field of machine learning withincreased predictive capability. In addition to improving the predictions ofneural networks, there is a simultaneous demand for reliable uncertaintyquantification on estimates made by machine learning methods such as neuralnetworks. Bayesian neural networks (BNNs) are an important type of neuralnetwork with built-in capability for quantifying uncertainty. This paperdiscusses aleatoric and epistemic uncertainty in BNNs and how they can becalculated. With an example dataset of images where the goal is to identify theamplitude of an event in the image, it is shown that epistemic uncertaintytends to be lower in images which are well-represented in the training datasetand tends to be high in images which are not well-represented. An algorithm forout-of-distribution (OoD) detection with BNN epistemic uncertainty isintroduced along with various experiments demonstrating factors influencing theOoD detection capability in a BNN. The OoD detection capability with epistemicuncertainty is shown to be comparable to the OoD detection in the discriminatornetwork of a generative adversarial network (GAN) with comparable networkarchitecture.</description><author>Ethan Ancell, Christopher Bennett, Bert Debusschere, Sapan Agarwal, Park Hays, T. Patrick Xiao</author><pubDate>Wed, 09 Aug 2023 18:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10780v2</guid></item><item><title>LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation</title><link>http://arxiv.org/abs/2308.05095v1</link><description>In the text-to-image generation field, recent remarkable progress in StableDiffusion makes it possible to generate rich kinds of novel photorealisticimages. However, current models still face misalignment issues (e.g.,problematic spatial relation understanding and numeration failure) in complexnatural scenes, which impedes the high-faithfulness text-to-image generation.Although recent efforts have been made to improve controllability by givingfine-grained guidance (e.g., sketch and scribbles), this issue has not beenfundamentally tackled since users have to provide such guidance informationmanually. In this work, we strive to synthesize high-fidelity images that aresemantically aligned with a given textual prompt without any guidance. Towardthis end, we propose a coarse-to-fine paradigm to achieve layout planning andimage generation. Concretely, we first generate the coarse-grained layoutconditioned on a given textual prompt via in-context learning based on LargeLanguage Models. Afterward, we propose a fine-grained object-interactiondiffusion method to synthesize high-faithfulness images conditioned on theprompt and the automatically generated layout. Extensive experimentsdemonstrate that our proposed method outperforms the state-of-the-art models interms of layout and image generation. Our code and settings are available at\url{https://layoutllm-t2i.github.io}.</description><author>Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-Seng Chua</author><pubDate>Wed, 09 Aug 2023 18:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05095v1</guid></item><item><title>A degree of image identification at sub-human scales could be possible with more advanced clusters</title><link>http://arxiv.org/abs/2308.05092v1</link><description>The purpose of the research is to determine if currently availableself-supervised learning techniques can accomplish human level comprehension ofvisual images using the same degree and amount of sensory input that peopleacquire from. Initial research on this topic solely considered data volumescaling. Here, we scale both the volume of data and the quality of the image.This scaling experiment is a self-supervised learning method that may be donewithout any outside financing. We find that scaling up data volume and pictureresolution at the same time enables human-level item detection performance atsub-human sizes.We run a scaling experiment with vision transformers trained onup to 200000 images up to 256 ppi.</description><author>Prateek Y J</author><pubDate>Wed, 09 Aug 2023 18:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05092v1</guid></item><item><title>Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity</title><link>http://arxiv.org/abs/2303.05689v2</link><description>There is a recently discovered and intriguing phenomenon called NeuralCollapse: at the terminal phase of training a deep neural network forclassification, the within-class penultimate feature means and the associatedclassifier vectors of all flat classes collapse to the vertices of a simplexEquiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenonby fixing the related classifier weights to a pre-computed ETF to induce neuralcollapse and maximize the separation of the learned features when training withimbalanced data. In this work, we propose to fix the linear classifier of adeep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF,and use a cosine similarity-based auxiliary loss to learn hierarchy-awarepenultimate features that collapse to the HAFrame. We demonstrate that ourapproach reduces the mistake severity of the model's predictions whilemaintaining its top-1 accuracy on several datasets of varying scales withhierarchies of heights ranging from 3 to 12. Code:https://github.com/ltong1130ztr/HAFrame</description><author>Tong Liang, Jim Davis</author><pubDate>Wed, 09 Aug 2023 18:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05689v2</guid></item><item><title>Organizational Bulk Email Systems: Their Role and Performance in Remote Work</title><link>http://arxiv.org/abs/2308.05085v1</link><description>The COVID-19 pandemic has forced many employees to work from home.Organizational bulk emails now play a critical role to reach employees withcentral information in this work-from-home environment. However, we know fromour own recent work that organizational bulk email has problems: recipientsfail to retain the bulk messages they received from the organization;recipients and senders have different opinions on which bulk messages wereimportant; and communicators lack technology support to better target anddesign messages. In this position paper, first we review the prior work onevaluating, designing, and prototyping organizational communication systems.Second we review our recent findings and some research techniques we founduseful in studying organizational communication. Last we propose a researchagenda to study organizational communications in remote work environment andsuggest some key questions and potential study directions.</description><author>Ruoyan Kong, Haiyi Zhu, Joseph A. Konstan</author><pubDate>Wed, 09 Aug 2023 18:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05085v1</guid></item><item><title>Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling</title><link>http://arxiv.org/abs/2308.05081v1</link><description>Video Semantic Role Labeling (VidSRL) aims to detect the salient events fromgiven videos, by recognizing the predict-argument event structures and theinterrelationships between events. While recent endeavors have put forthmethods for VidSRL, they can be mostly subject to two key drawbacks, includingthe lack of fine-grained spatial scene perception and the insufficientlymodeling of video temporality. Towards this end, this work explores a novelholistic spatio-temporal scene graph (namely HostSG) representation based onthe existing dynamic scene graph structures, which well model both thefine-grained spatial semantics and temporal dynamics of videos for VidSRL.Built upon the HostSG, we present a nichetargeting VidSRL framework. Ascene-event mapping mechanism is first designed to bridge the gap between theunderlying scene structure and the high-level event semantic structure,resulting in an overall hierarchical scene-event (termed ICE) graph structure.We further perform iterative structure refinement to optimize the ICE graph,such that the overall structure representation can best coincide with end taskdemand. Finally, three subtask predictions of VidSRL are jointly decoded, wherethe end-to-end paradigm effectively avoids error propagation. On the benchmarkdataset, our framework boosts significantly over the current best-performingmodel. Further analyses are shown for a better understanding of the advances ofour methods.</description><author>Yu Zhao, Hao Fei, Yixin Cao, Bobo Li, Meishan Zhang, Jianguo Wei, Min Zhang, Tat-Seng Chua</author><pubDate>Wed, 09 Aug 2023 18:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05081v1</guid></item><item><title>Bayesian Inverse Transition Learning for Offline Settings</title><link>http://arxiv.org/abs/2308.05075v1</link><description>Offline Reinforcement learning is commonly used for sequentialdecision-making in domains such as healthcare and education, where the rewardsare known and the transition dynamics $T$ must be estimated on the basis ofbatch data. A key challenge for all tasks is how to learn a reliable estimateof the transition dynamics $T$ that produce near-optimal policies that are safeenough so that they never take actions that are far away from the best actionwith respect to their value functions and informative enough so that theycommunicate the uncertainties they have. Using data from an expert, we proposea new constraint-based approach that captures our desiderata for reliablylearning a posterior distribution of the transition dynamics $T$ that is freefrom gradients. Our results demonstrate that by using our constraints, we learna high-performing policy, while considerably reducing the policy's varianceover different datasets. We also explain how combining uncertainty estimationwith these constraints can help us infer a partial ranking of actions thatproduce higher returns, and helps us infer safer and more informative policiesfor planning.</description><author>Leo Benac, Sonali Parbhoo, Finale Doshi-Velez</author><pubDate>Wed, 09 Aug 2023 18:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05075v1</guid></item><item><title>SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval</title><link>http://arxiv.org/abs/2304.05731v2</link><description>The retrieval of 3D objects has gained significant importance in recent yearsdue to its broad range of applications in computer vision, computer graphics,virtual reality, and augmented reality. However, the retrieval of 3D objectspresents significant challenges due to the intricate nature of 3D models, whichcan vary in shape, size, and texture, and have numerous polygons and vertices.To this end, we introduce a novel SHREC challenge track that focuses onretrieving relevant 3D animal models from a dataset using sketch queries andexpedites accessing 3D models through available sketches. Furthermore, a newdataset named ANIMAR was constructed in this study, comprising a collection of711 unique 3D animal models and 140 corresponding sketch queries. Our contestrequires participants to retrieve 3D models based on complex and detailedsketches. We receive satisfactory results from eight teams and 204 runs.Although further improvement is necessary, the proposed task has the potentialto incentivize additional research in the domain of 3D object retrieval,potentially yielding benefits for a wide range of applications. We also provideinsights into potential areas of future research, such as improving techniquesfor feature extraction and matching and creating more diverse datasets toevaluate retrieval performance. https://aichallenge.hcmus.edu.vn/sketchanimar</description><author>Trung-Nghia Le, Tam V. Nguyen, Minh-Quan Le, Trong-Thuan Nguyen, Viet-Tham Huynh, Trong-Le Do, Khanh-Duy Le, Mai-Khiem Tran, Nhat Hoang-Xuan, Thang-Long Nguyen-Ho, Vinh-Tiep Nguyen, Nhat-Quynh Le-Pham, Huu-Phuc Pham, Trong-Vu Hoang, Quang-Binh Nguyen, Trong-Hieu Nguyen-Mau, Tuan-Luc Huynh, Thanh-Danh Le, Ngoc-Linh Nguyen-Ha, Tuong-Vy Truong-Thuy, Truong Hoai Phong, Tuong-Nghiem Diep, Khanh-Duy Ho, Xuan-Hieu Nguyen, Thien-Phuc Tran, Tuan-Anh Yang, Kim-Phat Tran, Nhu-Vinh Hoang, Minh-Quang Nguyen, Hoai-Danh Vo, Minh-Hoa Doan, Hai-Dang Nguyen, Akihiro Sugimoto, Minh-Triet Tran</author><pubDate>Wed, 09 Aug 2023 18:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05731v2</guid></item><item><title>Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI</title><link>http://arxiv.org/abs/2308.05074v1</link><description>In order to respond effectively in the aftermath of a disaster, emergencyservices and relief organizations rely on timely and accurate information aboutthe affected areas. Remote sensing has the potential to significantly reducethe time and effort required to collect such information by enabling a rapidsurvey of large areas. To achieve this, the main challenge is the automaticextraction of relevant information from remotely sensed data. In this work, weshow how the combination of drone-based data with deep learning methods enablesautomated and large-scale situation assessment. In addition, we demonstrate theintegration of onboard image processing techniques for the deployment ofautonomous drone-based aid delivery. The results show the feasibility of arapid and large-scale image analysis in the field, and that onboard imageprocessing can increase the safety of drone-based aid deliveries.</description><author>Nina Merkle, Reza Bahmanyar, Corentin Henry, Seyed Majid Azimi, Xiangtian Yuan, Simon Schopferer, Veronika Gstaiger, Stefan Auer, Anne Schneibel, Marc Wieland, Thomas Kraft</author><pubDate>Wed, 09 Aug 2023 18:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05074v1</guid></item><item><title>Do Perceptually Aligned Gradients Imply Adversarial Robustness?</title><link>http://arxiv.org/abs/2207.11378v3</link><description>Adversarially robust classifiers possess a trait that non-robust models donot -- Perceptually Aligned Gradients (PAG). Their gradients with respect tothe input align well with human perception. Several works have identified PAGas a byproduct of robust training, but none have considered it as a standalonephenomenon nor studied its own implications. In this work, we focus on thistrait and test whether \emph{Perceptually Aligned Gradients imply Robustness}.To this end, we develop a novel objective to directly promote PAG in trainingclassifiers and examine whether models with such gradients are more robust toadversarial attacks. Extensive experiments on multiple datasets andarchitectures validate that models with aligned gradients exhibit significantrobustness, exposing the surprising bidirectional connection between PAG androbustness. Lastly, we show that better gradient alignment leads to increasedrobustness and harness this observation to boost the robustness of existingadversarial training techniques.</description><author>Roy Ganz, Bahjat Kawar, Michael Elad</author><pubDate>Wed, 09 Aug 2023 18:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.11378v3</guid></item><item><title>Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri</title><link>http://arxiv.org/abs/2308.05070v1</link><description>Recent advancements in Digital Document Restoration (DDR) have led tosignificant breakthroughs in analyzing highly damaged written artifacts. Amongthose, there has been an increasing interest in applying ArtificialIntelligence techniques for virtually unwrapping and automatically detectingink on the Herculaneum papyri collection. This collection consists ofcarbonized scrolls and fragments of documents, which have been digitized viaX-ray tomography to allow the development of ad-hoc deep learning-based DDRsolutions. In this work, we propose a modification of the Fast FourierConvolution operator for volumetric data and apply it in a segmentationarchitecture for ink detection on the challenging Herculaneum papyri,demonstrating its suitability via deep experimental analysis. To encourage theresearch on this task and the application of the proposed operator to othertasks involving volumetric data, we will release our implementation(https://github.com/aimagelab/vffc)</description><author>Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Rita Cucchiara</author><pubDate>Wed, 09 Aug 2023 18:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05070v1</guid></item><item><title>Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors</title><link>http://arxiv.org/abs/2308.05068v1</link><description>Many segmentation networks have been proposed for 3D volumetric segmentationof tumors and organs at risk. Hospitals and clinical institutions seek toaccelerate and minimize the efforts of specialists in image segmentation.Still, in case of errors generated by these networks, clinicians would have tomanually edit the generated segmentation maps. Given a 3D volume and itsputative segmentation map, we propose an approach to identify and measureerroneous regions in the segmentation map. Our method can estimate error at anypoint or node in a 3D mesh generated from a possibly erroneous volumetricsegmentation map, serving as a Quality Assurance tool. We propose a graphneural network-based transformer based on the Nodeformer architecture tomeasure and classify the segmentation errors at any point. We have evaluatedour network on a high-resolution micro-CT dataset of the human inner-ear bonylabyrinth structure by simulating erroneous 3D segmentation maps. Our networkincorporates a convolutional encoder to compute node-centric features from theinput micro-CT data, the Nodeformer to learn the latent graph embeddings, and aMulti-Layer Perceptron (MLP) to compute and classify the node-wise errors. Ournetwork achieves a mean absolute error of ~0.042 over other Graph NeuralNetworks (GNN) and an accuracy of 79.53% over other GNNs in estimating andclassifying the node-wise errors, respectively. We also put forth vertex-normalprediction as a custom pretext task for pre-training the CNN encoder to improvethe network's overall performance. Qualitative analysis shows the efficiency ofour network in correctly classifying errors and reducing misclassifications.</description><author>Sneha Sree C, Mohammad Al Fahim, Keerthi Ram, Mohanasankar Sivaprakasam</author><pubDate>Wed, 09 Aug 2023 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05068v1</guid></item><item><title>TextANIMAR: Text-based 3D Animal Fine-Grained Retrieval</title><link>http://arxiv.org/abs/2304.06053v2</link><description>3D object retrieval is an important yet challenging task that has drawn moreand more attention in recent years. While existing approaches have made stridesin addressing this issue, they are often limited to restricted settings such asimage and sketch queries, which are often unfriendly interactions for commonusers. In order to overcome these limitations, this paper presents a novelSHREC challenge track focusing on text-based fine-grained retrieval of 3Danimal models. Unlike previous SHREC challenge tracks, the proposed task isconsiderably more challenging, requiring participants to develop innovativeapproaches to tackle the problem of text-based retrieval. Despite the increaseddifficulty, we believe this task can potentially drive useful applications inpractice and facilitate more intuitive interactions with 3D objects. Fivegroups participated in our competition, submitting a total of 114 runs. Whilethe results obtained in our competition are satisfactory, we note that thechallenges presented by this task are far from fully solved. As such, weprovide insights into potential areas for future research and improvements. Webelieve we can help push the boundaries of 3D object retrieval and facilitatemore user-friendly interactions via vision-language technologies.https://aichallenge.hcmus.edu.vn/textanimar</description><author>Trung-Nghia Le, Tam V. Nguyen, Minh-Quan Le, Trong-Thuan Nguyen, Viet-Tham Huynh, Trong-Le Do, Khanh-Duy Le, Mai-Khiem Tran, Nhat Hoang-Xuan, Thang-Long Nguyen-Ho, Vinh-Tiep Nguyen, Tuong-Nghiem Diep, Khanh-Duy Ho, Xuan-Hieu Nguyen, Thien-Phuc Tran, Tuan-Anh Yang, Kim-Phat Tran, Nhu-Vinh Hoang, Minh-Quang Nguyen, E-Ro Nguyen, Minh-Khoi Nguyen-Nhat, Tuan-An To, Trung-Truc Huynh-Le, Nham-Tan Nguyen, Hoang-Chau Luong, Truong Hoai Phong, Nhat-Quynh Le-Pham, Huu-Phuc Pham, Trong-Vu Hoang, Quang-Binh Nguyen, Hai-Dang Nguyen, Akihiro Sugimoto, Minh-Triet Tran</author><pubDate>Wed, 09 Aug 2023 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06053v2</guid></item><item><title>Learning-Augmented Model-Based Planning for Visual Exploration</title><link>http://arxiv.org/abs/2211.07898v2</link><description>We consider the problem of time-limited robotic exploration in previouslyunseen environments where exploration is limited by a predefined amount oftime. We propose a novel exploration approach using learning-augmentedmodel-based planning. We generate a set of subgoals associated with frontierson the current map and derive a Bellman Equation for exploration with thesesubgoals. Visual sensing and advances in semantic mapping of indoor scenes areexploited for training a deep convolutional neural network to estimateproperties associated with each frontier: the expected unobserved area beyondthe frontier and the expected timesteps (discretized actions) required toexplore it. The proposed model-based planner is guaranteed to explore the wholescene if time permits. We thoroughly evaluate our approach on a large-scalepseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. Wecompare our approach with classical and more recent RL-based explorationmethods. Our approach surpasses the greedy strategies by 2.1% and the RL-basedexploration methods by 8.4% in terms of coverage.</description><author>Yimeng Li, Arnab Debnath, Gregory Stein, Jana Kosecka</author><pubDate>Wed, 09 Aug 2023 17:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07898v2</guid></item><item><title>Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling</title><link>http://arxiv.org/abs/2308.05062v1</link><description>Solver competitions play a prominent role in assessing and advancing thestate of the art for solving many problems in AI and beyond. Notably, in manyareas of AI, competitions have had substantial impact in guiding research andapplications for many years, and for a solver to be ranked highly in acompetition carries considerable weight. But to which extent can we expectcompetition results to generalise to sets of problem instances different fromthose used in a particular competition? This is the question we investigatehere, using statistical resampling techniques. We show that the rankingsresulting from the standard interpretation of competition results can be verysensitive to even minor changes in the benchmark instance set used as the basisfor assessment and can therefore not be expected to carry over to other samplesfrom the same underlying instance distribution. To address this problem, weintroduce a novel approach to statistically meaningful analysis of competitionresults based on resampling performance data. Our approach produces confidenceintervals of competition scores as well as statistically robust solver rankingswith bounded error. Applied to recent SAT, AI planning and computer visioncompetitions, our analysis reveals frequent statistical ties in solverperformance as well as some inversions of ranks compared to the officialresults based on simple scoring.</description><author>Chris Fawcett, Mauro Vallati, Holger H. Hoos, Alfonso E. Gerevini</author><pubDate>Wed, 09 Aug 2023 17:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05062v1</guid></item><item><title>Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language</title><link>http://arxiv.org/abs/2308.05061v1</link><description>In the growing domain of scientific machine learning, in-context operatorlearning has demonstrated notable potential in learning operators from prompteddata during inference stage without weight updates. However, the currentmodel's overdependence on sensor data, may inadvertently overlook theinvaluable human insight into the operator. To address this, we present atransformation of in-context operator learning into a multi-modal paradigm. Wepropose the use of "captions" to integrate human knowledge about the operator,expressed through natural language descriptions and equations. We illustratehow this method not only broadens the flexibility and generality ofphysics-informed learning, but also significantly boosts learning performanceand reduces data needs. Furthermore, we introduce a more efficient neuralnetwork architecture for multi-modal in-context operator learning, referred toas "ICON-LM", based on a language-model-like architecture. We demonstrate theviability of "ICON-LM" for scientific machine learning tasks, which creates anew path for the application of language models.</description><author>Liu Yang, Tingwei Meng, Siting Liu, Stanley J. Osher</author><pubDate>Wed, 09 Aug 2023 17:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05061v1</guid></item><item><title>A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique</title><link>http://arxiv.org/abs/2308.05059v1</link><description>Deep learning has revolutionized industries like computer vision, naturallanguage processing, and speech recognition. However, back propagation, themain method for training deep neural networks, faces challenges likecomputational overhead and vanishing gradients. In this paper, we propose anovel instant parameter update methodology that eliminates the need forcomputing gradients at each layer. Our approach accelerates learning, avoidsthe vanishing gradient problem, and outperforms state-of-the-art methods onbenchmark data sets. This research presents a promising direction for efficientand effective deep neural network training.</description><author>Gokulprasath R</author><pubDate>Wed, 09 Aug 2023 17:41:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05059v1</guid></item><item><title>PAT: Position-Aware Transformer for Dense Multi-Label Action Detection</title><link>http://arxiv.org/abs/2308.05051v1</link><description>We present PAT, a transformer-based network that learns complex temporalco-occurrence action dependencies in a video by exploiting multi-scale temporalfeatures. In existing methods, the self-attention mechanism in transformersloses the temporal positional information, which is essential for robust actiondetection. To address this issue, we (i) embed relative positional encoding inthe self-attention mechanism and (ii) exploit multi-scale temporalrelationships by designing a novel non hierarchical network, in contrast to therecent transformer-based approaches that use a hierarchical structure. We arguethat joining the self-attention mechanism with multiple sub-sampling processesin the hierarchical approaches results in increased loss of positionalinformation. We evaluate the performance of our proposed approach on twochallenging dense multi-label benchmark datasets, and show that PAT improvesthe current state-of-the-art result by 1.1% and 0.6% mAP on the Charades andMultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-artmAP at 26.5% and 44.6%, respectively. We also perform extensive ablationstudies to examine the impact of the different components of our proposednetwork.</description><author>Faegheh Sardari, Armin Mustafa, Philip J. B. Jackson, Adrian Hilton</author><pubDate>Wed, 09 Aug 2023 17:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05051v1</guid></item><item><title>An Exact Kernel Equivalence for Finite Classification Models</title><link>http://arxiv.org/abs/2308.00824v3</link><description>We explore the equivalence between neural networks and kernel methods byderiving the first exact representation of any finite-size parametricclassification model trained with gradient descent as a kernel machine. Wecompare our exact representation to the well-known Neural Tangent Kernel (NTK)and discuss approximation error relative to the NTK and other non-exact pathkernel formulations. We experimentally demonstrate that the kernel can becomputed for realistic networks up to machine precision. We use this exactkernel to show that our theoretical contribution can provide useful insightsinto the predictions made by neural networks, particularly the way in whichthey generalize.</description><author>Brian Bell, Michael Geyer, David Glickenstein, Amanda Fernandez, Juston Moore</author><pubDate>Wed, 09 Aug 2023 17:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00824v3</guid></item><item><title>ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals</title><link>http://arxiv.org/abs/2308.03936v2</link><description>We propose an exhaustive methodology that leverages all levels of featureabstraction, targeting an enhancement in the generalizability of imageclassification to unobserved hospitals. Our approach incorporatesaugmentation-based self-supervision with common distribution shifts inhistopathology scenarios serving as the pretext task. This enables us to deriveinvariant features from training images without relying on training labels,thereby covering different abstraction levels. Moving onto the subsequentabstraction level, we employ a domain alignment module to facilitate furtherextraction of invariant features across varying training hospitals. Torepresent the highly specific features of participating hospitals, an encoderis trained to classify hospital labels, independent of their diagnostic labels.The features from each of these encoders are subsequently disentangled tominimize redundancy and segregate the features. This representation, whichspans a broad spectrum of semantic information, enables the development of amodel demonstrating increased robustness to unseen images from disparatedistributions. Experimental results from the PACS dataset (a domaingeneralization benchmark), a synthetic dataset created by applyinghistopathology-specific jitters to the MHIST dataset (defining differentdomains with varied distribution shifts), and a Renal Cell Carcinoma datasetderived from four image repositories from TCGA, collectively indicate that ourproposed model is adept at managing varying levels of image granularity. Thus,it shows improved generalizability when faced with new, out-of-distributionhospital images.</description><author>Milad Sikaroudi, Maryam Hosseini, Shahryar Rahnamayan, H. R. Tizhoosh</author><pubDate>Wed, 09 Aug 2023 17:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03936v2</guid></item><item><title>RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction</title><link>http://arxiv.org/abs/2308.05046v1</link><description>We present RadGraph2, a novel dataset for extracting information fromradiology reports that focuses on capturing changes in disease state and deviceplacement over time. We introduce a hierarchical schema that organizes entitiesbased on their relationships and show that using this hierarchy during trainingimproves the performance of an information extraction model. Specifically, wepropose a modification to the DyGIE++ framework, resulting in our model HGIE,which outperforms previous models in entity and relation extraction tasks. Wedemonstrate that RadGraph2 enables models to capture a wider variety offindings and perform better at relation extraction compared to those trained onthe original RadGraph dataset. Our work provides the foundation for developingautomated systems that can track disease progression over time and developinformation extraction models that leverage the natural hierarchy of labels inthe medical domain.</description><author>Sameer Khanna, Adam Dejl, Kibo Yoon, Quoc Hung Truong, Hanh Duong, Agustina Saenz, Pranav Rajpurkar</author><pubDate>Wed, 09 Aug 2023 17:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05046v1</guid></item><item><title>A Survey of Deep Learning: From Activations to Transformers</title><link>http://arxiv.org/abs/2302.00722v2</link><description>The past decade has witnessed remarkable advancements in deep learning, owingto the emergence of various architectures, layers, objectives, and optimizationtechniques. These consist of a multitude of variations of attention,normalization, skip connections, transformer, and self-supervised learningmethods, among others. Our goal is to furnish a comprehensive survey ofsignificant recent contributions in these domains to individuals with afundamental grasp of deep learning. Our aspiration is that an integrated andcomprehensive approach of influential recent works will facilitate theformation of new connections between different areas of deep learning. In ourdiscussion, we discuss multiple patterns that summarize the key strategies formany of the successful innovations over the last decade. We also include adiscussion on recent commercially built, closed-source models such as OpenAI'sGPT-4 and Google's PaLM 2.</description><author>Johannes Schneider, Michalis Vlachos</author><pubDate>Wed, 09 Aug 2023 17:17:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00722v2</guid></item><item><title>Sparse and Low-Rank High-Order Tensor Regression via Parallel Proximal Method</title><link>http://arxiv.org/abs/1911.12965v2</link><description>Recently, tensor data (or multidimensional array) have been generated in manymodern applications, such as functional magnetic resonance imaging (fMRI) inneuroscience and videos in video analysis. Many efforts are made in recentyears to predict the relationship between tensor features and univariateresponses. However, previously proposed methods either lose structuralinformation within tensor data or have prohibitively expensive time costs,especially for large-scale data with high-order structures. To address suchproblems, we propose the Sparse and Low-rank Tensor Regression (SLTR) model.Our model enforces sparsity and low-rankness of the tensor coefficient bydirectly applying $\ell_1$ norm and tensor nuclear norm, such that it preservesstructural information of the tensor. To make the solving procedure scalableand efficient, SLTR makes use of the proximal gradient method, which can beeasily implemented parallelly. We evaluate SLTR on several simulated datasetsand one video action recognition dataset. Experiment results show that,compared with previous models, SLTR can obtain a better solution with muchfewer time costs. Moreover, our model's predictions exhibit meaningfulinterpretations on the video dataset.</description><author>Jiaqi Zhang, Yinghao Cai, Zhaoyang Wang, Beilun Wang</author><pubDate>Wed, 09 Aug 2023 17:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1911.12965v2</guid></item><item><title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting</title><link>http://arxiv.org/abs/2305.08992v2</link><description>A myriad of algorithms for the automatic analysis of brain MR images isavailable to support clinicians in their decision-making. For brain tumorpatients, the image acquisition time series typically starts with a scan thatis already pathological. This poses problems, as many algorithms are designedto analyze healthy brains and provide no guarantees for images featuringlesions. Examples include but are not limited to algorithms for brain anatomyparcellation, tissue segmentation, and brain extraction. To solve this dilemma,we introduce the BraTS 2023 inpainting challenge. Here, the participants' taskis to explore inpainting techniques to synthesize healthy brain scans fromlesioned ones. The following manuscript contains the task formulation, dataset,and submission procedure. Later it will be updated to summarize the findings ofthe challenge. The challenge is organized as part of the BraTS 2023 challengehosted at the MICCAI 2023 conference in Vancouver, Canada.</description><author>Florian Kofler, Felix Meissen, Felix Steinbauer, Robert Graf, Eva Oswald, Ezequiel de da Rosa, Hongwei Bran Li, Ujjwal Baid, Florian Hoelzl, Oezguen Turgut, Izabela Horvath, Diana Waldmannstetter, Christina Bukas, Maruf Adewole, Syed Muhammad Anwar, Anastasia Janas, Anahita Fathi Kazerooni, Dominic LaBella, Ahmed W Moawad, Keyvan Farahani, James Eddy, Timothy Bergquist, Verena Chung, Russell Takeshi Shinohara, Farouk Dako, Walter Wiggins, Zachary Reitman, Chunhao Wang, Xinyang Liu, Zhifan Jiang, Ariana Familiar, Gian-Marco Conte, Elaine Johanson, Zeke Meier, Christos Davatzikos, John Freymann, Justin Kirby, Michel Bilello, Hassan M Fathallah-Shaykh, Roland Wiest, Jan Kirschke, Rivka R Colen, Aikaterini Kotrotsou, Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-André</author><pubDate>Wed, 09 Aug 2023 17:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08992v2</guid></item><item><title>Separate Anything You Describe</title><link>http://arxiv.org/abs/2308.05037v1</link><description>Language-queried audio source separation (LASS) is a new paradigm forcomputational auditory scene analysis (CASA). LASS aims to separate a targetsound from an audio mixture given a natural language query, which provides anatural and scalable interface for digital audio applications. Recent works onLASS, despite attaining promising separation performance on specific sources(e.g., musical instruments, limited classes of audio events), are unable toseparate audio concepts in the open domain. In this work, we introduceAudioSep, a foundation model for open-domain audio source separation withnatural language queries. We train AudioSep on large-scale multimodal datasetsand extensively evaluate its capabilities on numerous tasks including audioevent separation, musical instrument separation, and speech enhancement.AudioSep demonstrates strong separation performance and impressive zero-shotgeneralization ability using audio captions or text labels as queries,substantially outperforming previous audio-queried and language-queried soundseparation models. For reproducibility of this work, we will release the sourcecode, evaluation benchmark and pre-trained model at:https://github.com/Audio-AGI/AudioSep.</description><author>Xubo Liu, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark D. Plumbley, Wenwu Wang</author><pubDate>Wed, 09 Aug 2023 17:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05037v1</guid></item><item><title>Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems</title><link>http://arxiv.org/abs/2308.05036v1</link><description>In this paper, we propose a data-driven framework for collaborative widebandspectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs),which act as the secondary users to opportunistically utilize detected spectrumholes. To this end, we propose a multi-class classification problem forwideband spectrum sensing to detect vacant spectrum spots based on collectedI/Q samples. To enhance the accuracy of the spectrum sensing module, theoutputs from the multi-class classification by each individual UAV are fused ata server in the unmanned aircraft system traffic management (UTM) ecosystem. Inthe spectrum scheduling phase, we leverage reinforcement learning (RL)solutions to dynamically allocate the detected spectrum holes to the secondaryusers (i.e., UAVs). To evaluate the proposed methods, we establish acomprehensive simulation framework that generates a near-realistic syntheticdataset using MATLAB LTE toolbox by incorporating base-station~(BS) locationsin a chosen area of interest, performing ray-tracing, and emulating the primaryusers channel usage in terms of I/Q samples. This evaluation methodologyprovides a flexible framework to generate large spectrum datasets that could beused for developing ML/AI-based spectrum management solutions for aerialdevices.</description><author>Sravan Reddy Chintareddy, Keenan Roach, Kenny Cheung, Morteza Hashemi</author><pubDate>Wed, 09 Aug 2023 17:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05036v1</guid></item><item><title>Expert load matters: operating networks at high accuracy and low manual effort</title><link>http://arxiv.org/abs/2308.05035v1</link><description>In human-AI collaboration systems for critical applications, in order toensure minimal error, users should set an operating point based on modelconfidence to determine when the decision should be delegated to human experts.Samples for which model confidence is lower than the operating point would bemanually analysed by experts to avoid mistakes. Such systems can become trulyuseful only if they consider two aspects: models should be confident only forsamples for which they are accurate, and the number of samples delegated toexperts should be minimized. The latter aspect is especially crucial forapplications where available expert time is limited and expensive, such ashealthcare. The trade-off between the model accuracy and the number of samplesdelegated to experts can be represented by a curve that is similar to an ROCcurve, which we refer to as confidence operating characteristic (COC) curve. Inthis paper, we argue that deep neural networks should be trained by taking intoaccount both accuracy and expert load and, to that end, propose a newcomplementary loss function for classification that maximizes the area underthis COC curve. This promotes simultaneously the increase in network accuracyand the reduction in number of samples delegated to humans. We performexperiments on multiple computer vision and medical image datasets forclassification. Our results demonstrate that the proposed loss improvesclassification accuracy and delegates less number of decisions to experts,achieves better out-of-distribution samples detection and on par calibrationperformance compared to existing loss functions.</description><author>Sara Sangalli, Ertunc Erdil, Ender Konukoglu</author><pubDate>Wed, 09 Aug 2023 17:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05035v1</guid></item><item><title>DETRs with Collaborative Hybrid Assignments Training</title><link>http://arxiv.org/abs/2211.12860v5</link><description>In this paper, we provide the observation that too few queries assigned aspositive samples in DETR with one-to-one set matching leads to sparsesupervision on the encoder's output which considerably hurt the discriminativefeature learning of the encoder and vice visa for attention learning in thedecoder. To alleviate this, we present a novel collaborative hybrid assignmentstraining scheme, namely $\mathcal{C}$o-DETR, to learn more efficient andeffective DETR-based detectors from versatile label assignment manners. Thisnew training scheme can easily enhance the encoder's learning ability inend-to-end detectors by training the multiple parallel auxiliary headssupervised by one-to-many label assignments such as ATSS and Faster RCNN. Inaddition, we conduct extra customized positive queries by extracting thepositive coordinates from these auxiliary heads to improve the trainingefficiency of positive samples in the decoder. In inference, these auxiliaryheads are discarded and thus our method introduces no additional parameters andcomputational cost to the original detector while requiring no hand-craftednon-maximum suppression (NMS). We conduct extensive experiments to evaluate theeffectiveness of the proposed approach on DETR variants, including DAB-DETR,Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-artDINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCOval. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP onCOCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clearmargins with much fewer model sizes. Codes are available at\url{https://github.com/Sense-X/Co-DETR}.</description><author>Zhuofan Zong, Guanglu Song, Yu Liu</author><pubDate>Wed, 09 Aug 2023 17:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12860v5</guid></item><item><title>Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance</title><link>http://arxiv.org/abs/2308.05034v1</link><description>Provenance graphs are structured audit logs that describe the history of asystem's execution. Recent studies have explored a variety of techniques toanalyze provenance graphs for automated host intrusion detection, focusingparticularly on advanced persistent threats. Sifting through their designdocuments, we identify four common dimensions that drive the development ofprovenance-based intrusion detection systems (PIDSes): scope (can PIDSes detectmodern attacks that infiltrate across application boundaries?), attackagnosticity (can PIDSes detect novel attacks without a priori knowledge ofattack characteristics?), timeliness (can PIDSes efficiently monitor hostsystems as they run?), and attack reconstruction (can PIDSes distill attackactivity from large provenance graphs so that sysadmins can easily understandand quickly respond to system intrusion?). We present KAIROS, the first PIDSthat simultaneously satisfies the desiderata in all four dimensions, whereasexisting approaches sacrifice at least one and struggle to achieve comparabledetection performance. Kairos leverages a novel graph neural network-based encoder-decoderarchitecture that learns the temporal evolution of a provenance graph'sstructural changes to quantify the degree of anomalousness for each systemevent. Then, based on this fine-grained information, Kairos reconstructs attackfootprints, generating compact summary graphs that accurately describemalicious activity over a stream of system audit logs. Using state-of-the-artbenchmark datasets, we demonstrate that Kairos outperforms previous approaches.</description><author>Zijun Cheng, Qiujian Lv, Jinyuan Liang, Yan Wang, Degang Sun, Thomas Pasquier, Xueyuan Han</author><pubDate>Wed, 09 Aug 2023 17:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05034v1</guid></item><item><title>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning</title><link>http://arxiv.org/abs/2308.03999v2</link><description>A major challenge in Explainable AI is in correctly interpreting activationsof hidden neurons: accurate interpretations would provide insights into thequestion of what a deep learning system has internally detected as relevant onthe input, demystifying the otherwise black-box character of deep learningsystems. The state of the art indicates that hidden node activations can, insome cases, be interpretable in a way that makes sense to humans, butsystematic automated methods that would be able to hypothesize and verifyinterpretations of hidden neuron activations are underexplored. In this paper,we provide such a method and demonstrate that it provides meaningfulinterpretations. Our approach is based on using large-scale backgroundknowledge approximately 2 million classes curated from the Wikipedia concepthierarchy together with a symbolic reasoning approach called Concept Inductionbased on description logics, originally developed for applications in theSemantic Web field. Our results show that we can automatically attachmeaningful labels from the background knowledge to individual neurons in thedense layer of a Convolutional Neural Network through a hypothesis andverification process.</description><author>Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Eugene Vasserman, Pascal Hitzler</author><pubDate>Wed, 09 Aug 2023 16:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03999v2</guid></item><item><title>Density Crop-guided Semi-supervised Object Detection in Aerial Images</title><link>http://arxiv.org/abs/2308.05032v1</link><description>One of the important bottlenecks in training modern object detectors is theneed for labeled images where bounding box annotations have to be produced foreach object present in the image. This bottleneck is further exacerbated inaerial images where the annotators have to label small objects oftendistributed in clusters on high-resolution images. In recent days, themean-teacher approach trained with pseudo-labels and weak-strong augmentationconsistency is gaining popularity for semi-supervised object detection.However, a direct adaptation of such semi-supervised detectors for aerialimages where small clustered objects are often present, might not lead tooptimal results. In this paper, we propose a density crop-guidedsemi-supervised detector that identifies the cluster of small objects duringtraining and also exploits them to improve performance at inference. Duringtraining, image crops of clusters identified from labeled and unlabeled imagesare used to augment the training set, which in turn increases the chance ofdetecting small objects and creating good pseudo-labels for small objects onthe unlabeled images. During inference, the detector is not only able to detectthe objects of interest but also regions with a high density of small objects(density crops) so that detections from the input image and detections fromimage crops are combined, resulting in an overall more accurate objectprediction, especially for small objects. Empirical studies on the popularbenchmarks of VisDrone and DOTA datasets show the effectiveness of our densitycrop-guided semi-supervised detector with an average improvement of more than2\% over the basic mean-teacher method in COCO style AP. Our code is availableat: https://github.com/akhilpm/DroneSSOD.</description><author>Akhil Meethal, Eric Granger, Marco Pedersoli</author><pubDate>Wed, 09 Aug 2023 16:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05032v1</guid></item><item><title>An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images</title><link>http://arxiv.org/abs/2308.05026v1</link><description>Perception that involves multi-object detection and tracking, and trajectoryprediction are two major tasks of autonomous driving. However, they arecurrently mostly studied separately, which results in most trajectoryprediction modules being developed based on ground truth trajectories withouttaking into account that trajectories extracted from the detection and trackingmodules in real-world scenarios are noisy. These noisy trajectories can have asignificant impact on the performance of the trajectory predictor and can leadto serious prediction errors. In this paper, we build an end-to-end frameworkfor detection, tracking, and trajectory prediction called ODTP (OnlineDetection, Tracking and Prediction). It adopts the state-of-the-art onlinemulti-object tracking model, QD-3DT, for perception and trains the trajectorypredictor, DCENet++, directly based on the detection results without purelyrelying on ground truth trajectories. We evaluate the performance of ODTP onthe widely used nuScenes dataset for autonomous driving. Extensive experimentsshow that ODPT achieves high performance end-to-end trajectory prediction.DCENet++, with the enhanced dynamic maps, predicts more accurate trajectoriesthan its base model. It is also more robust when compared with other generativeand deterministic trajectory prediction models trained on noisy detectionresults.</description><author>Hao Cheng, Mengmeng liu, Lin Chen</author><pubDate>Wed, 09 Aug 2023 16:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05026v1</guid></item><item><title>MLIC: Multi-Reference Entropy Model for Learned Image Compression</title><link>http://arxiv.org/abs/2211.07273v5</link><description>Recently, learned image compression has achieved remarkable performance. Theentropy model, which estimates the distribution of the latent representation,plays a crucial role in boosting rate-distortion performance. However, mostentropy models only capture correlations in one dimension, while the latentrepresentation contain channel-wise, local spatial, and global spatialcorrelations. To tackle this issue, we propose the Multi-Reference EntropyModel (MEM) and the advanced version, MEM$^+$. These models capture thedifferent types of correlations present in latent representation. Specifically,We first divide the latent representation into slices. When decoding thecurrent slice, we use previously decoded slices as context and employ theattention map of the previously decoded slice to predict global correlations inthe current slice. To capture local contexts, we introduce two enhancedcheckerboard context capturing techniques that avoids performance degradation.Based on MEM and MEM$^+$, we propose image compression models MLIC andMLIC$^+$. Extensive experimental evaluations demonstrate that our MLIC andMLIC$^+$ models achieve state-of-the-art performance, reducing BD-rate by$8.05\%$ and $11.39\%$ on the Kodak dataset compared to VTM-17.0 when measuredin PSNR. Our code will be available at https://github.com/JiangWeibeta/MLIC.</description><author>Wei Jiang, Jiayu Yang, Yongqi Zhai, Peirong Ning, Feng Gao, Ronggang Wang</author><pubDate>Wed, 09 Aug 2023 16:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07273v5</guid></item><item><title>Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution</title><link>http://arxiv.org/abs/2308.05022v1</link><description>Transformer-based methods have exhibited remarkable potential in single imagesuper-resolution (SISR) by effectively extracting long-range dependencies.However, most of the current research in this area has prioritized the designof transformer blocks to capture global information, while overlooking theimportance of incorporating high-frequency priors, which we believe could bebeneficial. In our study, we conducted a series of experiments and found thattransformer structures are more adept at capturing low-frequency information,but have limited capacity in constructing high-frequency representations whencompared to their convolutional counterparts. Our proposed solution, thecross-refinement adaptive feature modulation transformer (CRAFT), integratesthe strengths of both convolutional and transformer structures. It comprisesthree key components: the high-frequency enhancement residual block (HFERB) forextracting high-frequency information, the shift rectangle window attentionblock (SRWAB) for capturing global information, and the hybrid fusion block(HFB) for refining the global representation. Our experiments on multipledatasets demonstrate that CRAFT outperforms state-of-the-art methods by up to0.29dB while using fewer parameters. The source code will be made available at:https://github.com/AVC2-UESTC/CRAFT-SR.git.</description><author>Ao Li, Le Zhang, Yun Liu, Ce Zhu</author><pubDate>Wed, 09 Aug 2023 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05022v1</guid></item><item><title>An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures</title><link>http://arxiv.org/abs/2308.04898v1</link><description>As we increasingly depend on software systems, the consequences of breachesin the software supply chain become more severe. High-profile cyber attackslike those on SolarWinds and ShadowHammer have resulted in significantfinancial and data losses, underlining the need for stronger cybersecurity. Oneway to prevent future breaches is by studying past failures. However,traditional methods of analyzing these failures require manually reading andsummarizing reports about them. Automated support could reduce costs and allowanalysis of more failures. Natural Language Processing (NLP) techniques such asLarge Language Models (LLMs) could be leveraged to assist the analysis offailures. In this study, we assessed the ability of Large Language Models(LLMs) to analyze historical software supply chain breaches. We used LLMs toreplicate the manual analysis of 69 software supply chain security failuresperformed by members of the Cloud Native Computing Foundation (CNCF). Wedeveloped prompts for LLMs to categorize these by four dimensions: type ofcompromise, intent, nature, and impact. GPT 3.5s categorizations had an averageaccuracy of 68% and Bard had an accuracy of 58% over these dimensions. Wereport that LLMs effectively characterize software supply chain failures whenthe source articles are detailed enough for consensus among manual analysts,but cannot yet replace human analysts. Future work can improve LLM performancein this context, and study a broader range of articles and failures.</description><author>Tanmay Singla, Dharun Anandayuvaraj, Kelechi G. Kalu, Taylor R. Schorlemmer, James C. Davis</author><pubDate>Wed, 09 Aug 2023 16:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04898v1</guid></item><item><title>Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization</title><link>http://arxiv.org/abs/2308.05021v1</link><description>While diffusion models have achieved promising performances in datasynthesis, they might suffer error propagation because of their cascadestructure, where the distributional mismatch spreads and magnifies through thechain of denoising modules. However, a strict analysis is expected since manysequential models such as Conditional Random Field (CRF) are free from errorpropagation. In this paper, we empirically and theoretically verify thatdiffusion models are indeed affected by error propagation and we then propose aregularization to address this problem. Our theoretical analysis reveals thatthe question can be reduced to whether every denoising module of the diffusionmodel is fault-tolerant. We derive insightful transition equations, indicatingthat the module can't recover from input errors and even propagates additionalerrors to the next module. Our analysis directly leads to a consistencyregularization scheme for diffusion models, which explicitly reduces thedistribution gap between forward and backward processes. We further introduce abootstrapping algorithm to reduce the computation cost of the regularizer. Ourexperimental results on multiple image datasets show that our regularizationeffectively handles error propagation and significantly improves theperformance of vanilla diffusion models.</description><author>Yangming Li, Zhaozhi Qian, Mihaela van der Schaar</author><pubDate>Wed, 09 Aug 2023 16:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05021v1</guid></item><item><title>When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis</title><link>http://arxiv.org/abs/2308.05017v1</link><description>Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeledset by leveraging prior knowledge from a labeled set with known classes.Despite its importance, there is a lack of theoretical foundations for NCD.This paper bridges the gap by providing an analytical framework to formalizeand investigate when and how known classes can help discover novel classes.Tailored to the NCD problem, we introduce a graph-theoretic representation thatcan be learned by a novel NCD Spectral Contrastive Loss (NSCL). Minimizing thisobjective is equivalent to factorizing the graph's adjacency matrix, whichallows us to derive a provable error bound and provide the sufficient andnecessary condition for NCD. Empirically, NSCL can match or outperform severalstrong baselines on common benchmark datasets, which is appealing for practicalusage while enjoying theoretical guarantees.</description><author>Yiyou Sun, Zhenmei Shi, Yingyu Liang, Yixuan Li</author><pubDate>Wed, 09 Aug 2023 16:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05017v1</guid></item><item><title>Non-Invasive Fairness in Learning through the Lens of Data Drift</title><link>http://arxiv.org/abs/2303.17566v4</link><description>Machine Learning (ML) models are widely employed to drive many modern datasystems. While they are undeniably powerful tools, ML models often demonstrateimbalanced performance and unfair behaviors. The root of this problem oftenlies in the fact that different subpopulations commonly display divergenttrends: as a learning algorithm tries to identify trends in the data, itnaturally favors the trends of the majority groups, leading to a model thatperforms poorly and unfairly for minority populations. Our goal is to improvethe fairness and trustworthiness of ML models by applying only non-invasiveinterventions, i.e., without altering the data or the learning algorithm. Weuse a simple but key insight: the divergence of trends between differentpopulations, and, consecutively, between a learned model and minoritypopulations, is analogous to data drift, which indicates the poor conformancebetween parts of the data and the trained model. We explore two strategies(model-splitting and reweighing) to resolve this drift, aiming to improve theoverall conformance of models to the underlying data. Both our methodsintroduce novel ways to employ the recently-proposed data profiling primitiveof Conformance Constraints. Our experimental evaluation over 7 real-worlddatasets shows that both DifFair and ConFair improve the fairness of ML models.We demonstrate scenarios where DifFair has an edge, though ConFair has thegreatest practical impact and outperforms other baselines. Moreover, as amodel-agnostic technique, ConFair stays robust when used against differentmodels than the ones on which the weights have been learned, which is not thecase for other state of the art.</description><author>Ke Yang, Alexandra Meliou</author><pubDate>Wed, 09 Aug 2023 16:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17566v4</guid></item><item><title>An Empirical Study of Bugs in Open-Source Federated Learning Framework</title><link>http://arxiv.org/abs/2308.05014v1</link><description>Federated learning (FL), as a decentralized machine learning solution to theprotection of users' private data, has become an important learning paradigm inrecent years, especially since the enforcement of stricter laws and regulationsin most countries. Therefore, a variety of FL frameworks are released tofacilitate the development and application of federated learning. Despite theconsiderable amount of research on the security and privacy of FL models andsystems, the security issues in FL frameworks have not been systematicallystudied yet. In this paper, we conduct the first empirical study on 1,112 FLframework bugs to investigate their characteristics. These bugs are manuallycollected, classified, and labeled from 12 open-source FL frameworks on GitHub.In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fixpatterns of these bugs and investigate their correlations and distributions on23 logical components and two main application scenarios. From the results ofour study, we present nine findings, discuss their implications, and propoundseveral suggestions to FL framework developers and security researchers on theFL frameworks.</description><author>Weijie Shao, Yuyang Gao, Fu Song, Sen Chen, Lingling Fan</author><pubDate>Wed, 09 Aug 2023 16:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05014v1</guid></item><item><title>MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model</title><link>http://arxiv.org/abs/2308.05012v1</link><description>Transit riders' feedback provided in ridership surveys, customer relationshipmanagement (CRM) channels, and in more recent times, through social media iskey for transit agencies to better gauge the efficacy of their services andinitiatives. Getting a holistic understanding of riders' experience through thefeedback shared in those instruments is often challenging, mostly due to theopen-ended, unstructured nature of text feedback. In this paper, we proposeleveraging traditional transit CRM feedback to develop and deploy atransit-topic-aware large language model (LLM) capable of classifyingopen-ended text feedback to relevant transit-specific topics. First, we utilizesemi-supervised learning to engineer a training dataset of 11 broad transittopics detected in a corpus of 6 years of customer feedback provided to theWashington Metropolitan Area Transit Authority (WMATA). We then use thisdataset to train and thoroughly evaluate a language model based on the RoBERTaarchitecture. We compare our LLM, MetRoBERTa, to classical machine learningapproaches utilizing keyword-based and lexicon representations. Our modeloutperforms those methods across all evaluation metrics, providing an averagetopic classification accuracy of 90%. Finally, we provide a value propositionof this work demonstrating how the language model, alongside additional textprocessing tools, can be applied to add structure to open-ended text sources offeedback like Twitter. The framework and results we present provide a pathwayfor an automated, generalizable approach for ingesting, visualizing, andreporting transit riders' feedback at scale, enabling agencies to betterunderstand and improve customer experience.</description><author>Michael Leong, Awad Abdelhalim, Jude Ha, Dianne Patterson, Gabriel L. Pincus, Anthony B. Harris, Michael Eichler, Jinhua Zhao</author><pubDate>Wed, 09 Aug 2023 16:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05012v1</guid></item><item><title>Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories</title><link>http://arxiv.org/abs/2308.05011v1</link><description>With the increasing volume of astronomical data generated by modern surveytelescopes, automated pipelines and machine learning techniques have becomecrucial for analyzing and extracting knowledge from these datasets. Anomalydetection, i.e. the task of identifying irregular or unexpected patterns in thedata, is a complex challenge in astronomy. In this paper, we proposeMulti-Class Deep Support Vector Data Description (MCDSVDD), an extension of thestate-of-the-art anomaly detection algorithm One-Class Deep SVDD, specificallydesigned to handle different inlier categories with distinct datadistributions. MCDSVDD uses a neural network to map the data into hyperspheres,where each hypersphere represents a specific inlier category. The distance ofeach sample from the centers of these hyperspheres determines the anomalyscore. We evaluate the effectiveness of MCDSVDD by comparing its performancewith several anomaly detection algorithms on a large dataset of astronomicallight-curves obtained from the Zwicky Transient Facility. Our resultsdemonstrate the efficacy of MCDSVDD in detecting anomalous sources whileleveraging the presence of different inlier categories. The code and the dataneeded to reproduce our results are publicly available athttps://github.com/mperezcarrasco/AnomalyALeRCE.</description><author>Pérez-Carrasco Manuel, Cabrera-Vives Guillermo, Hernández-García Lorena, Forster Francisco, Sánchez-Sáez Paula, Muñoz Arancibia Alejandra, Astorga Nicolás, Bauer Franz, Bayo Amelia, Cádiz-Leyton Martina, Catelan Marcio</author><pubDate>Wed, 09 Aug 2023 16:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05011v1</guid></item><item><title>Low-complexity subspace-descent over symmetric positive definite manifold</title><link>http://arxiv.org/abs/2305.02041v2</link><description>This work puts forth low-complexity Riemannian subspace descent algorithmsfor the minimization of functions over the symmetric positive definite (SPD)manifold. Different from the existing Riemannian gradient descent variants, theproposed approach utilizes carefully chosen subspaces that allow the update tobe written as a product of the Cholesky factor of the iterate and a sparsematrix. The resulting updates avoid the costly matrix operations like matrixexponentiation and dense matrix multiplication, which are generally required inalmost all other Riemannian optimization algorithms on SPD manifold. We furtheridentify a broad class of functions, arising in diverse applications, such askernel matrix learning, covariance estimation of Gaussian distributions,maximum likelihood parameter estimation of elliptically contoureddistributions, and parameter estimation in Gaussian mixture model problems,over which the Riemannian gradients can be calculated efficiently. The proposeduni-directional and multi-directional Riemannian subspace descent variantsincur per-iteration complexities of $\mathcal{O}(n)$ and $\mathcal{O}(n^2)$respectively, as compared to the $\mathcal{O}(n^3)$ or higher complexityincurred by all existing Riemannian gradient descent variants. The superiorruntime and low per-iteration complexity of the proposed algorithms is alsodemonstrated via numerical tests on large-scale covariance estimation problems.</description><author>Yogesh Darmwal, Ketan Rajawat</author><pubDate>Wed, 09 Aug 2023 16:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02041v2</guid></item><item><title>Connectivity Optimized Nested Graph Networks for Crystal Structures</title><link>http://arxiv.org/abs/2302.14102v2</link><description>Graph neural networks (GNNs) have been applied to a large variety ofapplications in materials science and chemistry. Here, we recapitulate thegraph construction for crystalline (periodic) materials and investigate itsimpact on the GNNs model performance. We suggest the asymmetric unit cell as arepresentation to reduce the number of atoms by using all symmetries of thesystem. This substantially reduced the computational cost and thus time neededto train large graph neural networks without any loss in accuracy. Furthermore,with a simple but systematically built GNN architecture based on messagepassing and line graph templates, we introduce a general architecture (NestedGraph Network, NGN) that is applicable to a wide range of tasks. We show thatour suggested models systematically improve state-of-the-art results across alltasks within the MatBench benchmark. Further analysis shows that optimizedconnectivity and deeper message functions are responsible for the improvement.Asymmetric unit cells and connectivity optimization can be generally applied to(crystal) graph networks, while our suggested nested graph framework will opennew ways of systematic comparison of GNN architectures.</description><author>Robin Ruff, Patrick Reiser, Jan Stühmer, Pascal Friederich</author><pubDate>Wed, 09 Aug 2023 16:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14102v2</guid></item><item><title>Deep Learning Model Transfer in Forest Mapping using Multi-source Satellite SAR and Optical Images</title><link>http://arxiv.org/abs/2308.05005v1</link><description>Deep learning (DL) models are gaining popularity in forest variableprediction using Earth Observation images. However, in practical forestinventories, reference datasets are often represented by plot- or stand-levelmeasurements, while high-quality representative wall-to-wall reference data forend-to-end training of DL models are rarely available. Transfer learningfacilitates expansion of the use of deep learning models into areas withsub-optimal training data by allowing pretraining of the model in areas wherehigh-quality teaching data are available. In this study, we perform a "modeltransfer" (or domain adaptation) of a pretrained DL model into a target areausing plot-level measurements and compare performance versus other machinelearning models. We use an earlier developed UNet based model (SeUNet) todemonstrate the approach on two distinct taiga sites with varying foreststructure and composition. Multisource Earth Observation (EO) data arerepresented by a combination of Copernicus Sentinel-1 C-band SAR and Sentinel-2multispectral images, JAXA ALOS-2 PALSAR-2 SAR mosaic and TanDEM-X bistaticinterferometric radar data. The training study site is located in FinnishLapland, while the target site is located in Southern Finland. By leveragingtransfer learning, the prediction of SeUNet achieved root mean squared error(RMSE) of 2.70 m and R$^2$ of 0.882, considerably more accurate thantraditional benchmark methods. We expect such forest-specific DL model transfercan be suitable also for other forest variables and other EO data sources thatare sensitive to forest structure.</description><author>Shaojia Ge, Oleg Antropov, Tuomas Häme, Ronald E. McRoberts, Jukka Miettinen</author><pubDate>Wed, 09 Aug 2023 16:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05005v1</guid></item><item><title>MetAug: Contrastive Learning via Meta Feature Augmentation</title><link>http://arxiv.org/abs/2203.05119v4</link><description>What matters for contrastive learning? We argue that contrastive learningheavily relies on informative features, or "hard" (positive or negative)features. Early works include more informative features by applying complexdata augmentations and large batch size or memory bank, and recent works designelaborate sampling approaches to explore informative features. The keychallenge toward exploring such features is that the source multi-view data isgenerated by applying random data augmentations, making it infeasible to alwaysadd useful information in the augmented data. Consequently, the informativenessof features learned from such augmented data is limited. In response, wepropose to directly augment the features in latent space, thereby learningdiscriminative representations without a large amount of input data. We performa meta learning technique to build the augmentation generator that updates itsnetwork parameters by considering the performance of the encoder. However,insufficient input data may lead the encoder to learn collapsed features andtherefore malfunction the augmentation generator. A new margin-injectedregularization is further added in the objective function to avoid the encoderlearning a degenerate mapping. To contrast all features in one gradientback-propagation step, we adopt the proposed optimization-driven unifiedcontrastive loss instead of the conventional contrastive loss. Empirically, ourmethod achieves state-of-the-art results on several benchmark datasets.</description><author>Jiangmeng Li, Wenwen Qiang, Changwen Zheng, Bing Su, Hui Xiong</author><pubDate>Wed, 09 Aug 2023 15:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05119v4</guid></item><item><title>Directed differential equation discovery using modified mutation and cross-over operators</title><link>http://arxiv.org/abs/2308.04996v1</link><description>The discovery of equations with knowledge of the process origin is a temptingprospect. However, most equation discovery tools rely on gradient methods,which offer limited control over parameters. An alternative approach is theevolutionary equation discovery, which allows modification of almost everyoptimization stage. In this paper, we examine the modifications that can beintroduced into the evolutionary operators of the equation discovery algorithm,taking inspiration from directed evolution techniques employed in fields suchas chemistry and biology. The resulting approach, dubbed directed equationdiscovery, demonstrates a greater ability to converge towards accuratesolutions than the conventional method. To support our findings, we presentexperiments based on Burgers', wave, and Korteweg--de Vries equations.</description><author>Elizaveta Ivanchik, Alexander Hvatov</author><pubDate>Wed, 09 Aug 2023 15:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04996v1</guid></item><item><title>Modeling Multiple Views via Implicitly Preserving Global Consistency and Local Complementarity</title><link>http://arxiv.org/abs/2209.07811v2</link><description>While self-supervised learning techniques are often used to mining implicitknowledge from unlabeled data via modeling multiple views, it is unclear how toperform effective representation learning in a complex and inconsistentcontext. To this end, we propose a methodology, specifically consistency andcomplementarity network (CoCoNet), which avails of strict global inter-viewconsistency and local cross-view complementarity preserving regularization tocomprehensively learn representations from multiple views. On the global stage,we reckon that the crucial knowledge is implicitly shared among views, andenhancing the encoder to capture such knowledge from data can improve thediscriminability of the learned representations. Hence, preserving the globalconsistency of multiple views ensures the acquisition of common knowledge.CoCoNet aligns the probabilistic distribution of views by utilizing anefficient discrepancy metric measurement based on the generalized slicedWasserstein distance. Lastly on the local stage, we propose a heuristiccomplementarity-factor, which joints cross-view discriminative knowledge, andit guides the encoders to learn not only view-wise discriminability but alsocross-view complementary information. Theoretically, we provide theinformation-theoretical-based analyses of our proposed CoCoNet. Empirically, toinvestigate the improvement gains of our approach, we conduct adequateexperimental validations, which demonstrate that CoCoNet outperforms thestate-of-the-art self-supervised methods by a significant margin proves thatsuch implicit consistency and complementarity preserving regularization canenhance the discriminability of latent representations.</description><author>Jiangmeng Li, Wenwen Qiang, Changwen Zheng, Bing Su, Farid Razzak, Ji-Rong Wen, Hui Xiong</author><pubDate>Wed, 09 Aug 2023 15:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.07811v2</guid></item><item><title>IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models</title><link>http://arxiv.org/abs/2308.04995v1</link><description>The availability of large-scale authentic face databases has been crucial tothe significant advances made in face recognition research over the pastdecade. However, legal and ethical concerns led to the recent retraction ofmany of these databases by their creators, raising questions about thecontinuity of future face recognition research without one of its keyresources. Synthetic datasets have emerged as a promising alternative toprivacy-sensitive authentic data for face recognition development. However,recent synthetic datasets that are used to train face recognition models suffereither from limitations in intra-class diversity or cross-class (identity)discrimination, leading to less optimal accuracies, far away from theaccuracies achieved by models trained on authentic data. This paper targetsthis issue by proposing IDiff-Face, a novel approach based on conditionallatent diffusion models for synthetic identity generation with realisticidentity variations for face recognition training. Through extensiveevaluations, our proposed synthetic-based face recognition approach pushed thelimits of state-of-the-art performances, achieving, for example, 98.00%accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from therecent synthetic-based face recognition solutions with 95.40% and bridging thegap to authentic-based face recognition with 99.82% accuracy.</description><author>Fadi Boutros, Jonas Henry Grebe, Arjan Kuijper, Naser Dame</author><pubDate>Wed, 09 Aug 2023 15:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04995v1</guid></item><item><title>AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities</title><link>http://arxiv.org/abs/2308.04992v1</link><description>Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., textand image) for a comprehensive understanding of entities. Despite the recentprogress of large-scale MMKGs, existing MMKGs neglect the multi-aspect natureof entities, limiting the ability to comprehend entities from variousperspectives. In this paper, we construct AspectMMKG, the first MMKG withaspect-related images by matching images to different entity aspects.Specifically, we collect aspect-related images from a knowledge base, andfurther extract aspect-related sentences from the knowledge base as queries toretrieve a large number of aspect-related images via an online image searchengine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and645,383 aspect-related images. We demonstrate the usability of AspectMMKG inentity aspect linking (EAL) downstream task and show that previous EAL modelsachieve a new state-of-the-art performance with the help of AspectMMKG. Tofacilitate the research on aspect-related MMKG, we further propose anaspect-related image retrieval (AIR) model, that aims to correct and expandaspect-related images in AspectMMKG. We train an AIR model to learn therelationship between entity image and entity aspect-related images byincorporating entity image, aspect, and aspect image information. Experimentalresults indicate that the AIR model could retrieve suitable images for a givenentity w.r.t different aspects.</description><author>Jingdan Zhang, Jiaan Wang, Xiaodan Wang, Zhixu Li, Yanghua Xiao</author><pubDate>Wed, 09 Aug 2023 15:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04992v1</guid></item><item><title>Foreground Object Search by Distilling Composite Image Feature</title><link>http://arxiv.org/abs/2308.04990v1</link><description>Foreground object search (FOS) aims to find compatible foreground objects fora given background image, producing realistic composite image. We observe thatcompetitive retrieval performance could be achieved by using a discriminator topredict the compatibility of composite image, but this approach hasunaffordable time cost. To this end, we propose a novel FOS method viadistilling composite feature (DiscoFOS). Specifically, the abovementioneddiscriminator serves as teacher network. The student network employs twoencoders to extract foreground feature and background feature. Theirinteraction output is enforced to match the composite image feature from theteacher network. Additionally, previous works did not release their datasets,so we contribute two datasets for FOS task: S-FOSD dataset with syntheticcomposite images and R-FOSD dataset with real composite images. Extensiveexperiments on our two datasets demonstrate the superiority of the proposedmethod over previous approaches. The dataset and code are available athttps://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.</description><author>Bo Zhang, Jiacheng Sui, Li Niu</author><pubDate>Wed, 09 Aug 2023 15:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04990v1</guid></item><item><title>Self-supervised Landmark Learning with Deformation Reconstruction and Cross-subject Consistency Objectives</title><link>http://arxiv.org/abs/2308.04987v1</link><description>A Point Distribution Model (PDM) is the basis of a Statistical Shape Model(SSM) that relies on a set of landmark points to represent a shape andcharacterize the shape variation. In this work, we present a self-supervisedapproach to extract landmark points from a given registration model for thePDMs. Based on the assumption that the landmarks are the points that have themost influence on registration, existing works learn a point-based registrationmodel with a small number of points to estimate the landmark points thatinfluence the deformation the most. However, such approaches assume that thedeformation can be captured by point-based registration and quality landmarkscan be learned solely with the deformation capturing objective. We argue thatdata with complicated deformations can not easily be modeled with point-basedregistration when only a limited number of points is used to extractinfluential landmark points. Further, landmark consistency is not assured inexisting approaches In contrast, we propose to extract landmarks based on agiven registration model, which is tailored for the target data, so we canobtain more accurate correspondences. Secondly, to establish the anatomicalconsistency of the predicted landmarks, we introduce a landmark discovery lossto explicitly encourage the model to predict the landmarks that areanatomically consistent across subjects. We conduct experiments on anosteoarthritis progression prediction task and show our method outperformsexisting image-based and point-based approaches.</description><author>Chun-Hung Chao, Marc Niethammer</author><pubDate>Wed, 09 Aug 2023 15:40:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04987v1</guid></item><item><title>Cumulative Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2308.04371v2</link><description>While language models are powerful and versatile, they often fail to addresshighly complex problems. This is because solving complex problems requiresdeliberate thinking, which has been only minimally guided during training. Inthis paper, we propose a new method called Cumulative Reasoning (CR), whichemploys language models in a cumulative and iterative manner to emulate humanthought processes. By decomposing tasks into smaller components, CR streamlinesthe problem-solving process, rendering it both more manageable and effective.For logical inference tasks, CR consistently outperforms existing methods withan improvement up to 9.3%, and achieves the astonishing accuracy of 98.04% onthe curated FOLIO wiki dataset. In the context of the Game of 24, CR achievesan accuracy of 94%, which signifies a substantial enhancement of 20% over theprevious state-of-the-art method.</description><author>Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao</author><pubDate>Wed, 09 Aug 2023 15:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04371v2</guid></item><item><title>Exploring Multilingual Text Data Distillation</title><link>http://arxiv.org/abs/2308.04982v1</link><description>With the rise of deep learning, large datasets and complex models have becomecommon, requiring significant computing power. To address this, datadistillation has emerged as a technique to quickly train models with lowermemory and time requirements. However, data distillation on text-based datasetshasn't been explored much because of the challenges rising due to its discretenature. Additionally, existing dataset distillation methods often struggle togeneralize to new architectures. In the paper, we propose several datadistillation techniques for multilingual text classification datasets usinglanguage-model-based learning methods. We conduct experiments to analyze theirperformance in terms of classification strength, and cross-architecturegeneralization. Furthermore, we investigate the language-specific fairness ofthe data summaries generated by these methods. Our approach builds uponexisting techniques, enhancing cross-architecture generalization in the textdata distillation domain.</description><author>Shivam Sahni, Harsh Patel</author><pubDate>Wed, 09 Aug 2023 15:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04982v1</guid></item><item><title>SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models</title><link>http://arxiv.org/abs/2108.13672v3</link><description>The application of Transformer neural networks to Electronic Health Records(EHR) is challenging due to the distinct, multidimensional sequential structureof EHR data, often leading to underperformance when compared to simpler linearmodels. Thus, the advantages of Transformers, such as efficient transferlearning and improved scalability are not fully exploited in EHR applications.To overcome these challenges, we introduce SANSformer, a novel attention-freesequential model designed specifically with inductive biases to cater for theunique characteristics of EHR data. Our main application area is predicting future healthcare utilization, acrucial task for effectively allocating healthcare resources. This task becomesparticularly difficult when dealing with divergent patient subgroups. Thesesubgroups, characterized by unique health trajectories and often small in size,such as patients with rare diseases, require specialized modeling approaches.To address this, we adopt a self-supervised pretraining strategy, which we termGenerative Summary Pretraining (GSP). GSP predicts summary statistics of afuture window in the patient's history based on their past health records, thusdemonstrating potential to deal with the noisy and complex nature of EHR data.We pretrain our models on a comprehensive health registry encompassing close toone million patients, before fine-tuning them for specific subgroup predictiontasks. In our evaluations, SANSformer consistently outshines strong EHR baselines.Importantly, our GSP pretraining method greatly enhances model performance,especially for smaller patient subgroups. Our findings underscore thesubstantial potential of bespoke attention-free models and self-supervisedpretraining for enhancing healthcare utilization predictions across a broadrange of patient groups.</description><author>Yogesh Kumar, Alexander Ilin, Henri Salo, Sangita Kulathinal, Maarit K. Leinonen, Pekka Marttinen</author><pubDate>Wed, 09 Aug 2023 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.13672v3</guid></item><item><title>Transferable Models for Bioacoustics with Human Language Supervision</title><link>http://arxiv.org/abs/2308.04978v1</link><description>Passive acoustic monitoring offers a scalable, non-invasive method fortracking global biodiversity and anthropogenic impacts on species. Althoughdeep learning has become a vital tool for processing this data, current modelsare inflexible, typically cover only a handful of species, and are limited bydata scarcity. In this work, we propose BioLingual, a new model forbioacoustics based on contrastive language-audio pretraining. We firstaggregate bioacoustic archives into a language-audio dataset, calledAnimalSpeak, with over a million audio-caption pairs holding information onspecies, vocalization context, and animal behavior. After training on thisdataset to connect language and audio representations, our model can identifyover a thousand species' calls across taxa, complete bioacoustic taskszero-shot, and retrieve animal vocalization recordings from natural textqueries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasksin the Benchmark of Animal Sounds. Given its broad taxa coverage and ability tobe flexibly queried in human language, we believe this model opens newparadigms in ecological monitoring and research, including free-text search onthe world's acoustic monitoring archives. We open-source our models, dataset,and code.</description><author>David Robinson, Adelaide Robinson, Lily Akrapongpisak</author><pubDate>Wed, 09 Aug 2023 15:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04978v1</guid></item><item><title>Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention</title><link>http://arxiv.org/abs/2201.01615v4</link><description>Multi-scale representations are crucial for semantic segmentation. Thecommunity has witnessed the flourish of semantic segmentation convolutionalneural networks (CNN) exploiting multi-scale contextual information. Motivatedby that the vision transformer (ViT) is powerful in image classification, somesemantic segmentation ViTs are recently proposed, most of them attainingimpressive results but at a cost of computational economy. In this paper, wesucceed in introducing multi-scale representations into semantic segmentationViT via window attention mechanism and further improves the performance andefficiency. To this end, we introduce large window attention which allows thelocal window to query a larger area of context window at only a littlecomputation overhead. By regulating the ratio of the context area to the queryarea, we enable the $\textit{large window attention}$ to capture the contextualinformation at multiple scales. Moreover, the framework of spatial pyramidpooling is adopted to collaborate with $\textit{the large window attention}$,which presents a novel decoder named $\textbf{la}$rge $\textbf{win}$dowattention spatial pyramid pooling (LawinASPP) for semantic segmentation ViT.Our resulting ViT, Lawin Transformer, is composed of an efficient hierachicalvision transformer (HVT) as encoder and a LawinASPP as decoder. The empiricalresults demonstrate that Lawin Transformer offers an improved efficiencycompared to the existing method. Lawin Transformer further sets newstate-of-the-art performance on Cityscapes (84.4% mIoU), ADE20K (56.2% mIoU)and COCO-Stuff datasets. The code will be released athttps://github.com/yan-hao-tian/lawin</description><author>Haotian Yan, Chuang Zhang, Ming Wu</author><pubDate>Wed, 09 Aug 2023 15:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.01615v4</guid></item><item><title>Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval</title><link>http://arxiv.org/abs/2305.05144v3</link><description>Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to thecross-domain nature of sketches and photos, as well as the semantic gap betweenseen and unseen image distributions. Previous methods fine-tune pre-trainedmodels with various side information and learning strategies to learn a compactfeature space that is shared between the sketch and photo domains and bridgesseen and unseen classes. However, these efforts are inadequate in adaptingdomains and transferring knowledge from seen to unseen classes. In this paper,we present an effective ``Adapt and Align'' approach to address the keychallenges. Specifically, we insert simple and lightweight domain adapters tolearn new abstract concepts of the sketch domain and improve cross-domainrepresentation capabilities. Inspired by recent advances in image-textfoundation models (e.g., CLIP) on zero-shot scenarios, we explicitly align thelearned image embedding with a more semantic text embedding to achieve thedesired knowledge transfer from seen to unseen classes. Extensive experimentson three benchmark datasets and two popular backbones demonstrate thesuperiority of our method in terms of retrieval accuracy and flexibility.</description><author>Shiyin Dong, Mingrui Zhu, Nannan Wang, Xinbo Gao</author><pubDate>Wed, 09 Aug 2023 15:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05144v3</guid></item><item><title>Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization</title><link>http://arxiv.org/abs/2212.10445v3</link><description>Foundation models are redefining how AI systems are built. Practitioners nowfollow a standard procedure to build their machine learning solutions: from apre-trained foundation model, they fine-tune the weights on the target task ofinterest. So, the Internet is swarmed by a handful of foundation modelsfine-tuned on many diverse tasks: these individual fine-tunings exist inisolation without benefiting from each other. In our opinion, this is a missedopportunity, as these specialized models contain rich and diverse features. Inthis paper, we thus propose model ratatouille, a new strategy to recycle themultiple fine-tunings of the same foundation model on diverse auxiliary tasks.Specifically, we repurpose these auxiliary weights as initializations formultiple parallel fine-tunings on the target task; then, we average allfine-tuned weights to obtain the final model. This recycling strategy aims atmaximizing the diversity in weights by leveraging the diversity in auxiliarytasks. Empirically, it improves the state of the art on the reference DomainBedbenchmark for out-of-distribution generalization. Looking forward, this workcontributes to the emerging paradigm of updatable machine learning where, akinto open-source software development, the community collaborates to reliablyupdate machine learning models. Our code is released:https://github.com/facebookresearch/ModelRatatouille.</description><author>Alexandre Ramé, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Léon Bottou, David Lopez-Paz</author><pubDate>Wed, 09 Aug 2023 15:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10445v3</guid></item><item><title>Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning</title><link>http://arxiv.org/abs/2308.04964v1</link><description>ModSecurity is widely recognized as the standard open-source Web ApplicationFirewall (WAF), maintained by the OWASP Foundation. It detects maliciousrequests by matching them against the Core Rule Set, identifying well-knownattack patterns. Each rule in the CRS is manually assigned a weight, based onthe severity of the corresponding attack, and a request is detected asmalicious if the sum of the weights of the firing rules exceeds a giventhreshold. In this work, we show that this simple strategy is largelyineffective for detecting SQL injection (SQLi) attacks, as it tends to blockmany legitimate requests, while also being vulnerable to adversarial SQLiattacks, i.e., attacks intentionally manipulated to evade detection. Toovercome these issues, we design a robust machine learning model, namedAdvModSec, which uses the CRS rules as input features, and it is trained todetect adversarial SQLi attacks. Our experiments show that AdvModSec, beingtrained on the traffic directed towards the protected web services, achieves abetter trade-off between detection and false positive rates, improving thedetection rate of the vanilla version of ModSecurity with CRS by 21%. Moreover,our approach is able to improve its adversarial robustness against adversarialSQLi attacks by 42%, thereby taking a step forward towards building more robustand trustworthy WAFs.</description><author>Biagio Montaruli, Luca Demetrio, Andrea Valenza, Battista Biggio, Luca Compagna, Davide Balzarotti, Davide Ariu, Luca Piras</author><pubDate>Wed, 09 Aug 2023 14:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04964v1</guid></item><item><title>Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining</title><link>http://arxiv.org/abs/2308.04396v2</link><description>One aim of Process Mining (PM) is the discovery of process models from eventlogs of information systems. PM has been successfully applied toprocess-oriented enterprise systems but is less suited for communication- anddocument-oriented Enterprise Collaboration Systems (ECS). ECS event logs arevery fine-granular and PM applied to their logs results in spaghetti models. Acommon solution for this is event abstraction, i.e., converting low-level logsinto more abstract high-level logs before running discovery algorithms. ECSlogs come with special characteristics that have so far not been fullyaddressed by existing event abstraction approaches. We aim to close this gapwith a tailored ECS event abstraction (ECSEA) approach that trains a model bycomparing recorded actual user activities (high-level traces) with thesystem-generated low-level traces (extracted from the ECS). The model allows usto automatically convert future low-level traces into an abstracted high-levellog that can be used for PM. Our evaluation shows that the algorithm producesaccurate results. ECSEA is a preprocessing method that is essential for theinterpretation of collaborative work activity in ECS, which we call SocialProcess Mining.</description><author>Jonas Blatt, Patrick Delfmann, Petra Schubert</author><pubDate>Wed, 09 Aug 2023 14:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04396v2</guid></item><item><title>CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks</title><link>http://arxiv.org/abs/2308.04961v1</link><description>Existing approaches for information cascade prediction fall into three maincategories: feature-driven methods, point process-based methods, and deeplearning-based methods. Among them, deep learning-based methods, characterizedby its superior learning and representation capabilities, mitigates theshortcomings inherent of the other methods. However, current deep learningmethods still face several persistent challenges. In particular, accuraterepresentation of user attributes remains problematic due to factors such asfake followers and complex network configurations. Previous algorithms thatfocus on the sequential order of user activations often neglect the richinsights offered by activation timing. Furthermore, these techniques often failto holistically integrate temporal and structural aspects, thus missing thenuanced propagation trends inherent in information cascades.To address theseissues, we propose the Cross-Domain Information Fusion Framework (CasCIFF),which is tailored for information cascade prediction. This framework exploitsmulti-hop neighborhood information to make user embeddings robust. Whenembedding cascades, the framework intentionally incorporates timestamps,endowing it with the ability to capture evolving patterns of informationdiffusion. In particular, the CasCIFF seamlessly integrates the tasks of userclassification and cascade prediction into a consolidated framework, therebyallowing the extraction of common features that prove useful for all tasks, astrategy anchored in the principles of multi-task learning.</description><author>Hongjun Zhu, Shun Yuan, Xin Liu, Kuo Chen, Chaolong Jia, Ying Qian</author><pubDate>Wed, 09 Aug 2023 14:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04961v1</guid></item><item><title>Understanding recent deep-learning techniques for identifying collective variables of molecular dynamics</title><link>http://arxiv.org/abs/2307.00365v2</link><description>High-dimensional metastable molecular system can often be characterised by afew features of the system, i.e. collective variables (CVs). Thanks to therapid advance in the area of machine learning and deep learning, various deeplearning-based CV identification techniques have been developed in recentyears, allowing accurate modelling and efficient simulation of complexmolecular systems. In this paper, we look at two different categories of deeplearning-based approaches for finding CVs, either by computing leadingeigenfunctions of infinitesimal generator or transfer operator associated tothe underlying dynamics, or by learning an autoencoder via minimisation ofreconstruction error. We present a concise overview of the mathematics behindthese two approaches and conduct a comparative numerical study of these twoapproaches on illustrative examples.</description><author>Wei Zhang, Christof Schütte</author><pubDate>Wed, 09 Aug 2023 14:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00365v2</guid></item><item><title>Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning</title><link>http://arxiv.org/abs/2308.04960v1</link><description>Privacy preservation has long been a concern in smart acoustic monitoringsystems, where speech can be passively recorded along with a target signal inthe system's operating environment. In this study, we propose the integrationof two commonly used approaches in privacy preservation: source separation andadversarial representation learning. The proposed system learns the latentrepresentation of audio recordings such that it prevents differentiatingbetween speech and non-speech recordings. Initially, the source separationnetwork filters out some of the privacy-sensitive data, and during theadversarial learning process, the system will learn privacy-preservingrepresentation on the filtered signal. We demonstrate the effectiveness of ourproposed method by comparing our method against systems without sourceseparation, without adversarial learning, and without both. Overall, ourresults suggest that the proposed system can significantly improve speechprivacy preservation compared to that of using source separation or adversariallearning solely while maintaining good performance in the acoustic monitoringtask.</description><author>Diep Luong, Minh Tran, Shayan Gharib, Konstantinos Drossos, Tuomas Virtanen</author><pubDate>Wed, 09 Aug 2023 14:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04960v1</guid></item><item><title>Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks</title><link>http://arxiv.org/abs/2308.04958v1</link><description>Advanced Air Mobility (AAM) introduces a new, efficient mode oftransportation with the use of vehicle autonomy and electrified aircraft toprovide increasingly autonomous transportation between previously underservedmarkets. Safe and efficient navigation of low altitude aircraft through highlydense environments requires the integration of a multitude of complexobservations, such as surveillance, knowledge of vehicle dynamics, and weather.The processing and reasoning on these observations pose challenges due to thevarious sources of uncertainty in the information while ensuring cooperationwith a variable number of aircraft in the airspace. These challenges coupledwith the requirement to make safety-critical decisions in real-time rule outthe use of conventional separation assurance techniques. We present adecentralized reinforcement learning framework to provide autonomousself-separation capabilities within AAM corridors with the use of speed andvertical maneuvers. The problem is formulated as a Markov Decision Process andsolved by developing a novel extension to the sample-efficient, off-policy softactor-critic (SAC) algorithm. We introduce the use of attention networks forvariable-length observation processing and a distributed computing architectureto achieve high training sample throughput as compared to existing approaches.A comprehensive numerical study shows that the proposed framework can ensuresafe and efficient separation of aircraft in high density, dynamic environmentswith various sources of uncertainty.</description><author>Marc W. Brittain, Luis E. Alvarez, Kara Breeden</author><pubDate>Wed, 09 Aug 2023 14:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04958v1</guid></item><item><title>ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction</title><link>http://arxiv.org/abs/2308.04956v1</link><description>Due to the extremely low signal-to-noise ratio (SNR) and unknown poses(projection angles and image translation) in cryo-EM experiments,reconstructing 3D structures from 2D images is very challenging. On top ofthese challenges, heterogeneous cryo-EM reconstruction also has an additionalrequirement: conformation classification. An emerging solution to this problemis called amortized inference, implemented using the autoencoder architectureor its variants. Instead of searching for the correctimage-to-pose/conformation mapping for every image in the dataset as innon-amortized methods, amortized inference only needs to train an encoder thatmaps images to appropriate latent spaces representing poses or conformations.Unfortunately, standard amortized-inference-based methods with entangled latentspaces have difficulty learning the distribution of conformations and posesfrom cryo-EM images. In this paper, we propose an unsupervised deep learningarchitecture called "ACE-HetEM" based on amortized inference. To explicitlyenforce the disentanglement of conformation classifications and poseestimations, we designed two alternating training tasks in our method:image-to-image task and pose-to-pose task. Results on simulated datasets showthat ACE-HetEM has comparable accuracy in pose estimation and produces evenbetter reconstruction resolution than non-amortized methods. Furthermore, weshow that ACE-HetEM is also applicable to real experimental datasets.</description><author>Weijie Chen, Lin Yao, Zeqing Xia, Yuhang Wang</author><pubDate>Wed, 09 Aug 2023 14:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04956v1</guid></item><item><title>Wirelessly Powered Federated Learning Networks: Joint Power Transfer, Data Sensing, Model Training, and Resource Allocation</title><link>http://arxiv.org/abs/2308.04953v1</link><description>Federated learning (FL) has found many successes in wireless networks;however, the implementation of FL has been hindered by the energy limitation ofmobile devices (MDs) and the availability of training data at MDs. How tointegrate wireless power transfer and mobile crowdsensing towards sustainableFL solutions is a research topic entirely missing from the open literature.This work for the first time investigates a resource allocation problem incollaborative sensing-assisted sustainable FL (S2FL) networks with the goal ofminimizing the total completion time. We investigate a practicalharvesting-sensing-training-transmitting protocol in which energy-limited MDsfirst harvest energy from RF signals, use it to gain a reward for userparticipation, sense the training data from the environment, train the localmodels at MDs, and transmit the model updates to the server. The totalcompletion time minimization problem of jointly optimizing power transfer,transmit power allocation, data sensing, bandwidth allocation, local modeltraining, and data transmission is complicated due to the non-convex objectivefunction, highly non-convex constraints, and strongly coupled variables. Wepropose a computationally-efficient path-following algorithm to obtain theoptimal solution via the decomposition technique. In particular, inner convexapproximations are developed for the resource allocation subproblem, and thesubproblems are performed alternatively in an iterative fashion. Simulationresults are provided to evaluate the effectiveness of the proposed S2FLalgorithm in reducing the completion time up to 21.45% in comparison with otherbenchmark schemes. Further, we investigate an extension of our work fromfrequency division multiple access (FDMA) to non-orthogonal multiple access(NOMA) and show that NOMA can speed up the total completion time 8.36% onaverage of the considered FL system.</description><author>Mai Le, Dinh Thai Hoang, Diep N. Nguyen, Won-Joo Hwang, Quoc-Viet Pham</author><pubDate>Wed, 09 Aug 2023 14:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04953v1</guid></item><item><title>Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation</title><link>http://arxiv.org/abs/2308.04952v1</link><description>Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot SemanticSegmentation (FSS) to simultaneously segment unseen classes and seen classesduring evaluation. Previous works leverage additional branch or prototypicalaggregation to eliminate the constrained setting of FSS. However,representation division and embedding prejudice, which heavily results in poorperformance of GFSS, have not been synthetical considered. We address theaforementioned problems by jointing the prototypical kernel learning andopen-set foreground perception. Specifically, a group of learnable kernels isproposed to perform segmentation with each kernel in charge of a stuff class.Then, we explore to merge the prototypical learning to the update of base-classkernels, which is consistent with the prototype knowledge aggregation offew-shot novel classes. In addition, a foreground contextual perception modulecooperating with conditional bias based inference is adopted to performclass-agnostic as well as open-set foreground detection, thus to mitigate theembedding prejudice and prevent novel targets from being misclassified asbackground. Moreover, we also adjust our method to the Class IncrementalFew-shot Semantic Segmentation (CIFSS) which takes the knowledge of novelclasses in a incremental stream. Extensive experiments on PASCAL-5i andCOCO-20i datasets demonstrate that our method performs better than previousstate-of-the-art.</description><author>Kai Huang, Feigege Wang, Ye Xi, Yutao Gao</author><pubDate>Wed, 09 Aug 2023 14:38:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04952v1</guid></item><item><title>Apple Vision Pro for Healthcare: "The Ultimate Display"? -- Entering the Wonderland of Precision</title><link>http://arxiv.org/abs/2308.04313v2</link><description>At the Worldwide Developers Conference (WWDC) in June 2023, Apple introducedthe Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, morespecifically it is a Virtual Reality (VR) device with an additional VideoSee-Through (VST) capability. The VST capability turns the Vision Pro also intoan Augmented Reality (AR) device. The AR feature is enabled by streaming thereal world via cameras to the (VR) screens in front of the user's eyes. This isof course not unique and similar to other devices, like the Varjo XR-3.Nevertheless, the Vision Pro has some interesting features, like an inside-outscreen that can show the headset wearers' eyes to "outsiders" or a button onthe top, called "Digital Crown", that allows you to seamlessly blend digitalcontent with your physical space by turning it. In addition, it is untethered,except for the cable to the battery, which makes the headset more agile,compared to the Varjo XR-3. This could actually come closer to the "UltimateDisplay", which Ivan Sutherland had already sketched in 1965. Not available tothe public yet, like the Ultimate Display, we want to take a look into thecrystal ball in this perspective to see if it can overcome some clinicalchallenges that - especially - AR still faces in the medical domain, but alsogo beyond and discuss if the Vision Pro could support clinicians in essentialtasks to spend more time with their patients.</description><author>Jan Egger, Christina Gsaxner, Xiaojun Chen, Jiang Bian, Jens Kleesiek, Behrus Puladi</author><pubDate>Wed, 09 Aug 2023 14:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04313v2</guid></item><item><title>Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection</title><link>http://arxiv.org/abs/2308.04950v1</link><description>Fake news is fake material in a news media format but is not processedproperly by news agencies. The fake material can provoke or defame significantentities or individuals or potentially even for the personal interests of thecreators, causing problems for society. Distinguishing fake news and real newsis challenging due to limited of domain knowledge and time constraints.According to the survey, the top three areas most exposed to hoaxes andmisinformation by residents are in Banten, DKI Jakarta and West Java. The modelof transformers is referring to an approach in the field of artificialintelligence (AI) in natural language processing utilizing the deep learningarchitectures. Transformers exercise a powerful attention mechanism to processtext in parallel and produce rich and contextual word representations. Aprevious study indicates a superior performance of a transformer model known asBERT over and above non transformer approach. However, some studies suggest theperformance can be improved with the use of improved BERT models known asALBERT and RoBERTa. However, the modified BERT models are not well explored fordetecting fake news in Bahasa Indonesia. In this research, we explore thosetransformer models and found that ALBERT outperformed other models with 87.6%accuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)respectively. Source code available at:https://github.com/Shafna81/fakenewsdetection.git</description><author>Shafna Fitria Nur Azizah, Hasan Dwi Cahyono, Sari Widya Sihwi, Wisnu Widiarto</author><pubDate>Wed, 09 Aug 2023 14:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04950v1</guid></item><item><title>Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2308.04949v1</link><description>End-to-end weakly supervised semantic segmentation aims at optimizing asegmentation model in a single-stage training process based on only imageannotations. Existing methods adopt an online-trained classification branch toprovide pseudo annotations for supervising the segmentation branch. However,this strategy makes the classification branch dominate the whole concurrenttraining process, hindering these two branches from assisting each other. Inour work, we treat these two branches equally by viewing them as diverse waysto generate the segmentation map, and add interactions on both theirsupervision and operation to achieve mutual promotion. For this purpose, abidirectional supervision mechanism is elaborated to force the consistencybetween the outputs of these two branches. Thus, the segmentation branch canalso give feedback to the classification branch to enhance the quality oflocalization seeds. Moreover, our method also designs interaction operationsbetween these two branches to exchange their knowledge to assist each other.Experiments indicate our work outperforms existing end-to-end weakly supervisedsegmentation methods.</description><author>Lei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu</author><pubDate>Wed, 09 Aug 2023 14:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04949v1</guid></item><item><title>Extrapolating Large Language Models to Non-English by Aligning Languages</title><link>http://arxiv.org/abs/2308.04948v1</link><description>Due to the unbalanced training data distribution, the language ability oflarge language models (LLMs) is often biased towards English. In this paper, wepropose to empower pre-trained LLMs on non-English languages by buildingsemantic alignment across languages. We perform instruction-tuning on LLaMAwith both translation task data and cross-lingual general task data to obtaincross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmarkXQUAD and MLQA show that x-LLaMA models outperform the Englishinstruction-tuned counterpart (Alpaca) by 42.50% on average on six non-Englishlanguages. Further experiments on Chinese benchmark C-Eval show that x-LLaMAachieves significant improvement on Chinese humanities tasks, outperformingAlpaca by 8.2%. We also discover that incorporating non-English text on thetarget side of translation data is particularly effective for boostingnon-English ability. Besides, we find that semantic alignment within LLM can befurther strengthened as translation task data scales up and we present theformulation of the underlying scaling law. Evaluation results on translationdataset Flores-101 show that \method outperforms previous LLaMA-based models inall evaluated directions. Code and data will be available at:https://github.com/OwenNJU/x-LLM.</description><author>Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li</author><pubDate>Wed, 09 Aug 2023 14:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04948v1</guid></item><item><title>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</title><link>http://arxiv.org/abs/2307.08131v2</link><description>Leveraging network information for predictive modeling has become widespreadin many domains. Within the realm of referral and targeted marketing,influencer detection stands out as an area that could greatly benefit from theincorporation of dynamic network representation due to the ongoing developmentof customer-brand relationships. To elaborate this idea, we introduceINFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic GraphNeural Networks that combines Graph Neural Networks (GNN) and Recurrent NeuralNetworks (RNN) with weighted loss functions, the Synthetic MinorityOversampling TEchnique (SMOTE) adapted for graph data, and a carefully craftedrolling-window strategy. To evaluate predictive performance, we utilize aunique corporate data set with networks of three cities and derive aprofit-driven evaluation methodology for influencer prediction. Our resultsshow how using RNN to encode temporal attributes alongside GNNs significantlyimproves predictive performance. We compare the results of various models todemonstrate the importance of capturing graph representation, temporaldependencies, and using a profit-driven methodology for evaluation.</description><author>Elena Tiukhova, Emiliano Penaloza, María Óskarsdóttir, Bart Baesens, Monique Snoeck, Cristián Bravo</author><pubDate>Wed, 09 Aug 2023 14:31:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08131v2</guid></item><item><title>Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey</title><link>http://arxiv.org/abs/2308.04947v1</link><description>Predicting stock prices presents a challenging research problem due to theinherent volatility and non-linear nature of the stock market. In recent years,knowledge-enhanced stock price prediction methods have shown groundbreakingresults by utilizing external knowledge to understand the stock market. Despitethe importance of these methods, there is a scarcity of scholarly works thatsystematically synthesize previous studies from the perspective of externalknowledge types. Specifically, the external knowledge can be modeled indifferent data structures, which we group into non-graph-based formats andgraph-based formats: 1) non-graph-based knowledge captures contextualinformation and multimedia descriptions specifically associated with anindividual stock; 2) graph-based knowledge captures interconnected andinterdependent information in the stock market. This survey paper aims toprovide a systematic and comprehensive description of methods for acquiringexternal knowledge from various unstructured data sources and thenincorporating it into stock price prediction models. We also explore fusionmethods for combining external knowledge with historical price features.Moreover, this paper includes a compilation of relevant datasets and delvesinto potential future research directions in this domain.</description><author>Liping Wang, Jiawei Li, Lifan Zhao, Zhizhuo Kou, Xiaohan Wang, Xinyi Zhu, Hao Wang, Yanyan Shen, Lei Chen</author><pubDate>Wed, 09 Aug 2023 14:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04947v1</guid></item><item><title>SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation</title><link>http://arxiv.org/abs/2308.04946v1</link><description>Generalisation of deep neural networks becomes vulnerable when distributionshifts are encountered between train (source) and test (target) domain data.Few-shot domain adaptation mitigates this issue by adapting deep neuralnetworks pre-trained on the source domain to the target domain using a randomlyselected and annotated support set from the target domain. This paper arguesthat randomly selecting the support set can be further improved for effectivelyadapting the pre-trained source models to the target domain. Alternatively, wepropose SelectNAdapt, an algorithm to curate the selection of the target domainsamples, which are then annotated and included in the support set. Inparticular, for the K-shot adaptation problem, we first leverageself-supervision to learn features of the target domain data. Then, we proposea per-class clustering scheme of the learned target domain features and selectK representative target samples using a distance-based scoring function.Finally, we bring our selection setup towards a practical ground by relying onpseudo-labels for clustering semantically similar target domain samples. Ourexperiments show promising results on three few-shot domain adaptationbenchmarks for image recognition compared to related approaches and thestandard random selection.</description><author>Youssef Dawoud, Gustavo Carneiro, Vasileios Belagiannis</author><pubDate>Wed, 09 Aug 2023 14:24:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04946v1</guid></item><item><title>LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking</title><link>http://arxiv.org/abs/2308.04945v1</link><description>The recent development and success of Large Language Models (LLMs)necessitate an evaluation of their performance across diverse NLP tasks indifferent languages. Although several frameworks have been developed and madepublicly available, their customization capabilities for specific tasks anddatasets are often complex for different users. In this study, we introduce theLLMeBench framework. Initially developed to evaluate Arabic NLP tasks usingOpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP taskand model, regardless of language. The framework also features zero- andfew-shot learning settings. A new custom dataset can be added in less than 10minutes, and users can use their own model API keys to evaluate the task athand. The developed framework has been already tested on 31 unique NLP tasksusing 53 publicly available datasets within 90 experimental setups, involvingapproximately 296K data points. We plan to open-source the framework for thecommunity (https://github.com/qcri/LLMeBench/). A video demonstrating theframework is available online (https://youtu.be/FkQn4UjYA0s).</description><author>Fahim Dalvi, Maram Hasanain, Sabri Boughorbel, Basel Mousi, Samir Abdaljalil, Nizi Nazar, Ahmed Abdelali, Shammur Absar Chowdhury, Hamdy Mubarak, Ahmed Ali, Majd Hawasly, Nadir Durrani, Firoj Alam</author><pubDate>Wed, 09 Aug 2023 14:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04945v1</guid></item><item><title>DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds</title><link>http://arxiv.org/abs/2308.04383v2</link><description>Point clouds are naturally sparse, while image pixels are dense. Theinconsistency limits feature fusion from both modalities for point-wise sceneflow estimation. Previous methods rarely predict scene flow from the entirepoint clouds of the scene with one-time inference due to the memoryinefficiency and heavy overhead from distance calculation and sorting involvedin commonly used farthest point sampling, KNN, and ball query algorithms forlocal feature aggregation. To mitigate these issues in scene flow learning, weregularize raw points to a dense format by storing 3D coordinates in 2D grids.Unlike the sampling operation commonly used in existing works, the dense 2Drepresentation 1) preserves most points in the given scene, 2) brings in asignificant boost of efficiency, and 3) eliminates the density gap betweenpoints and pixels, allowing us to perform effective feature fusion. We alsopresent a novel warping projection technique to alleviate the information lossproblem resulting from the fact that multiple points could be mapped into onegrid during projection when computing cost volume. Sufficient experimentsdemonstrate the efficiency and effectiveness of our method, outperforming theprior-arts on the FlyingThings3D and KITTI dataset.</description><author>Chensheng Peng, Guangming Wang, Xian Wan Lo, Xinrui Wu, Chenfeng Xu, Masayoshi Tomizuka, Wei Zhan, Hesheng Wang</author><pubDate>Wed, 09 Aug 2023 14:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04383v2</guid></item><item><title>Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography</title><link>http://arxiv.org/abs/2302.06436v2</link><description>The diagnostic quality of computed tomography (CT) scans is usuallyrestricted by the induced patient dose, scan speed, and image quality.Sparse-angle tomographic scans reduce radiation exposure and accelerate dataacquisition, but suffer from image artifacts and noise. Existing imageprocessing algorithms can restore CT reconstruction quality but often requirelarge training data sets or can not be used for truncated objects. This workpresents a self-supervised projection inpainting method that allows optimizingmissing projective views via gradient-based optimization. By reconstructingindependent stacks of projection data, a self-supervised loss is calculated inthe CT image domain and used to directly optimize projection image intensitiesto match the missing tomographic views constrained by the projection geometry.Our experiments on real X-ray microscope (XRM) tomographic mouse tibia bonescans show that our method improves reconstructions by 3.1-7.4%/7.7-17.6% interms of PSNR/SSIM with respect to the interpolation baseline. Our approach isapplicable as a flexible self-supervised projection inpainting tool fortomographic applications.</description><author>Fabian Wagner, Mareike Thies, Noah Maul, Laura Pfaff, Oliver Aust, Sabrina Pechmann, Christopher Syben, Andreas Maier</author><pubDate>Wed, 09 Aug 2023 14:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06436v2</guid></item><item><title>Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection</title><link>http://arxiv.org/abs/2308.04944v1</link><description>Anomaly detection (AD) in images, identifying significant deviations fromnormality, is a critical issue in computer vision. This paper introduces anovel approach to dimensionality reduction for AD using pre-trainedconvolutional neural network (CNN) that incorporate EfficientNet models. Weinvestigate the importance of component selection and propose two types of treesearch approaches, both employing a greedy strategy, for optimal eigencomponentselection. Our study conducts three main experiments to evaluate theeffectiveness of our approach. The first experiment explores the influence oftest set performance on component choice, the second experiment examines theperformance when we train on one anomaly type and evaluate on all other types,and the third experiment investigates the impact of using a minimum number ofimages for training and selecting them based on anomaly types. Our approachaims to find the optimal subset of components that deliver the highestperformance score, instead of focusing solely on the proportion of varianceexplained by each component and also understand the components behaviour indifferent settings. Our results indicate that the proposed method surpassesboth Principal Component Analysis (PCA) and Negated Principal ComponentAnalysis (NPCA) in terms of detection accuracy, even when using fewercomponents. Thus, our approach provides a promising alternative to conventionaldimensionality reduction techniques in AD, and holds potential to enhance theefficiency and effectiveness of AD systems.</description><author>Tetiana Gula, João P C Bertoldo</author><pubDate>Wed, 09 Aug 2023 14:19:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04944v1</guid></item><item><title>Differentially Private Graph Neural Network with Importance-Grained Noise Adaption</title><link>http://arxiv.org/abs/2308.04943v1</link><description>Graph Neural Networks (GNNs) with differential privacy have been proposed topreserve graph privacy when nodes represent personal and sensitive information.However, the existing methods ignore that nodes with different importance mayyield diverse privacy demands, which may lead to over-protect some nodes anddecrease model utility. In this paper, we study the problem ofimportance-grained privacy, where nodes contain personal data that need to bekept private but are critical for training a GNN. We propose NAP-GNN, anode-importance-grained privacy-preserving GNN algorithm with privacyguarantees based on adaptive differential privacy to safeguard nodeinformation. First, we propose a Topology-based Node Importance Estimation(TNIE) method to infer unknown node importance with neighborhood and centralityawareness. Second, an adaptive private aggregation method is proposed toperturb neighborhood aggregation from node-importance-grain. Third, we proposeto privately train a graph learning algorithm on perturbed aggregations inadaptive residual connection mode over multi-layers convolution for node-wisetasks. Theoretically analysis shows that NAP-GNN satisfies privacy guarantees.Empirical experiments over real-world graph datasets show that NAP-GNN achievesa better trade-off between privacy and accuracy.</description><author>Yuxin Qi, Xi Lin, Jun Wu</author><pubDate>Wed, 09 Aug 2023 14:18:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04943v1</guid></item><item><title>Semantic Communications for Artificial Intelligence Generated Content (AIGC) Toward Effective Content Creation</title><link>http://arxiv.org/abs/2308.04942v1</link><description>Artificial Intelligence Generated Content (AIGC) Services have significantpotential in digital content creation. The distinctive abilities of AIGC, suchas content generation based on minimal input, hold huge potential, especiallywhen integrating with semantic communication (SemCom). In this paper, a novelcomprehensive conceptual model for the integration of AIGC and SemCom isdeveloped. Particularly, a content generation level is introduced on top of thesemantic level that provides a clear outline of how AIGC and SemCom interactwith each other to produce meaningful and effective content. Moreover, a novelframework that employs AIGC technology is proposed as an encoder and decoderfor semantic information, considering the joint optimization of semanticextraction and evaluation metrics tailored to AIGC services. The framework canadapt to different types of content generated, the required quality, and thesemantic information utilized. By employing a Deep Q Network (DQN), a casestudy is presented that provides useful insights into the feasibility of theoptimization problem and its convergence characteristics.</description><author>Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Xuemin, Shen</author><pubDate>Wed, 09 Aug 2023 14:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04942v1</guid></item><item><title>Integrating large language models and active inference to understand eye movements in reading and dyslexia</title><link>http://arxiv.org/abs/2308.04941v1</link><description>We present a novel computational model employing hierarchical activeinference to simulate reading and eye movements. The model characterizeslinguistic processing as inference over a hierarchical generative model,facilitating predictions and inferences at various levels of granularity, fromsyllables to sentences. Our approach combines the strengths of large language models for realistictextual predictions and active inference for guiding eye movements toinformative textual information, enabling the testing of predictions. The modelexhibits proficiency in reading both known and unknown words and sentences,adhering to the distinction between lexical and nonlexical routes in dual-routetheories of reading. Notably, our model permits the exploration of maladaptiveinference effects on eye movements during reading, such as in dyslexia. Tosimulate this condition, we attenuate the contribution of priors during thereading process, leading to incorrect inferences and a more fragmented readingstyle, characterized by a greater number of shorter saccades. This alignmentwith empirical findings regarding eye movements in dyslexic individualshighlights the model's potential to aid in understanding the cognitiveprocesses underlying reading and eye movements, as well as how reading deficitsassociated with dyslexia may emerge from maladaptive predictive processing. In summary, our model represents a significant advancement in comprehendingthe intricate cognitive processes involved in reading and eye movements, withpotential implications for understanding and addressing dyslexia through thesimulation of maladaptive inference. It may offer valuable insights into thiscondition and contribute to the development of more effective interventions fortreatment.</description><author>Francesco Donnarumma, Mirco Frosolone, Giovanni Pezzulo</author><pubDate>Wed, 09 Aug 2023 14:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04941v1</guid></item><item><title>Person Re-Identification without Identification via Event Anonymization</title><link>http://arxiv.org/abs/2308.04402v2</link><description>Wide-scale use of visual surveillance in public spaces puts individualprivacy at stake while increasing resource consumption (energy, bandwidth, andcomputation). Neuromorphic vision sensors (event-cameras) have been recentlyconsidered a valid solution to the privacy issue because they do not capturedetailed RGB visual information of the subjects in the scene. However, recentdeep learning architectures have been able to reconstruct images from eventcameras with high fidelity, reintroducing a potential threat to privacy forevent-based vision applications. In this paper, we aim to anonymizeevent-streams to protect the identity of human subjects against such imagereconstruction attacks. To achieve this, we propose an end-to-end networkarchitecture jointly optimized for the twofold objective of preserving privacyand performing a downstream task such as person ReId. Our network learns toscramble events, enforcing the degradation of images recovered from the privacyattacker. In this work, we also bring to the community the first everevent-based person ReId dataset gathered to evaluate the performance of ourapproach. We validate our approach with extensive experiments and reportresults on the synthetic event data simulated from the publicly availableSoftBio dataset and our proposed Event-ReId dataset.</description><author>Shafiq Ahmad, Pietro Morerio, Alessio Del Bue</author><pubDate>Wed, 09 Aug 2023 14:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04402v2</guid></item><item><title>An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2308.04938v1</link><description>Communication is crucial in multi-agent reinforcement learning when agentsare not able to observe the full state of the environment. The most commonapproach to allow learned communication between agents is the use of adifferentiable communication channel that allows gradients to flow betweenagents as a form of feedback. However, this is challenging when we want to usediscrete messages to reduce the message size, since gradients cannot flowthrough a discrete communication channel. Previous work proposed methods todeal with this problem. However, these methods are tested in differentcommunication learning architectures and environments, making it hard tocompare them. In this paper, we compare several state-of-the-art discretizationmethods as well as a novel approach. We do this comparison in the context ofcommunication learning using gradients from other agents and perform tests onseveral environments. In addition, we present COMA-DIAL, a communicationlearning approach based on DIAL and COMA extended with learning rate scalingand adapted exploration. Using COMA-DIAL allows us to perform experiments onmore complex environments. Our results show that the novel ST-DRU method,proposed in this paper, achieves the best results out of all discretizationmethods across the different environments. It achieves the best or close to thebest performance in each of the experiments and is the only method that doesnot fail on any of the tested environments.</description><author>Astrid Vanneste, Simon Vanneste, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</author><pubDate>Wed, 09 Aug 2023 14:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04938v1</guid></item><item><title>JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition</title><link>http://arxiv.org/abs/2308.04934v1</link><description>We propose JEDI, a multi-dataset semi-supervised learning method, whichefficiently combines knowledge from multiple experts, learned on differentdatasets, to train and improve the performance of individual, per dataset,student models. Our approach achieves this by addressing two important problemsin current machine learning research: generalization across datasets andlimitations of supervised training due to scarcity of labeled data. We startwith an arbitrary number of experts, pretrained on their own specific dataset,which form the initial set of student models. The teachers are immediatelyderived by concatenating the feature representations from the penultimatelayers of the students. We then train all models in a student-teachersemi-supervised learning scenario until convergence. In our efficient approach,student-teacher training is carried out jointly and end-to-end, showing thatboth students and teachers improve their generalization capacity duringtraining. We validate our approach on four video action recognition datasets.By simultaneously considering all datasets within a unified semi-supervisedsetting, we demonstrate significant improvements over the initial experts.</description><author>Lucian Bicsi, Bogdan Alexe, Radu Tudor Ionescu, Marius Leordeanu</author><pubDate>Wed, 09 Aug 2023 14:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04934v1</guid></item><item><title>GeodesicPSIM: Predicting the Quality of Static Mesh with Texture Map via Geodesic Patch Similarity</title><link>http://arxiv.org/abs/2308.04928v1</link><description>Static meshes with texture maps have attracted considerable attention in bothindustrial manufacturing and academic research, leading to an urgentrequirement for effective and robust objective quality evaluation. However,current model-based static mesh quality metrics have obvious limitations: mostof them only consider geometry information, while color information is ignored,and they have strict constraints for the meshes' geometrical topology. Othermetrics, such as image-based and point-based metrics, are easily influenced bythe prepossessing algorithms, e.g., projection and sampling, hampering theirability to perform at their best. In this paper, we propose Geodesic PatchSimilarity (GeodesicPSIM), a novel model-based metric to accurately predicthuman perception quality for static meshes. After selecting a group keypoints,1-hop geodesic patches are constructed based on both the reference anddistorted meshes cleaned by an effective mesh cleaning algorithm. A two-steppatch cropping algorithm and a patch texture mapping module refine the size of1-hop geodesic patches and build the relationship between the mesh geometry andcolor information, resulting in the generation of 1-hop textured geodesicpatches. Three types of features are extracted to quantify the distortion:patch color smoothness, patch discrete mean curvature, and patch pixel coloraverage and variance. To the best of our knowledge, GeodesicPSIM is the firstmodel-based metric especially designed for static meshes with texture maps.GeodesicPSIM provides state-of-the-art performance in comparison withimage-based, point-based, and video-based metrics on a newly created andchallenging database. We also prove the robustness of GeodesicPSIM byintroducing different settings of hyperparameters. Ablation studies alsoexhibit the effectiveness of three proposed features and the patch croppingalgorithm.</description><author>Qi Yang, Joel Jung, Xiaozhong Xu, Shan Liu</author><pubDate>Wed, 09 Aug 2023 13:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04928v1</guid></item><item><title>Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery</title><link>http://arxiv.org/abs/2308.04923v1</link><description>Functionally significant coronary artery disease (CAD) is caused by plaquebuildup in the coronary arteries, potentially leading to narrowing of thearterial lumen, i.e. coronary stenosis, that significantly obstructs blood flowto the myocardium. The current reference for establishing the presence of afunctionally significant stenosis is invasive fractional flow reserve (FFR)measurement. To avoid invasive measurements, non-invasive prediction of FFRfrom coronary CT angiography (CCTA) has emerged. For this, machine learningapproaches, characterized by fast inference, are increasingly developed.However, these methods predict a single FFR value per artery i.e. they don'tprovide information about the stenosis location or treatment strategy. Wepropose a deep learning-based method to predict the FFR along the artery fromCCTA scans. This study includes CCTA images of 110 patients who underwentinvasive FFR pullback measurement in 112 arteries. First, a multi planarreconstruction (MPR) of the artery is fed to a variational autoencoder tocharacterize the artery, i.e. through the lumen area and unsupervised arteryencodings. Thereafter, a convolutional neural network (CNN) predicts the FFRalong the artery. The CNN is supervised by multiple loss functions, notably aloss function inspired by the Earth Mover's Distance (EMD) to predict thecorrect location of FFR drops and a histogram-based loss to explicitlysupervise the slope of the FFR curve. To train and evaluate our model,eight-fold cross-validation was performed. The resulting FFR curves show goodagreement with the reference allowing the distinction between diffuse and focalCAD distributions in most cases. Quantitative evaluation yielded a meanabsolute difference in the area under the FFR pullback curve (AUPC) of 1.7. Themethod may pave the way towards fast, accurate, automatic prediction of FFRalong the artery from CCTA.</description><author>Nils Hampe, Sanne G. M. van Velzen, Jean-Paul Aben, Carlos Collet, Ivana Išgum</author><pubDate>Wed, 09 Aug 2023 13:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04923v1</guid></item><item><title>Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis</title><link>http://arxiv.org/abs/2306.09417v3</link><description>With read-aloud speech synthesis achieving high naturalness scores, there isa growing research interest in synthesising spontaneous speech. However, humanspontaneous face-to-face conversation has both spoken and non-verbal aspects(here, co-speech gestures). Only recently has research begun to explore thebenefits of jointly synthesising these two modalities in a single system. Theprevious state of the art used non-probabilistic methods, which fail to capturethe variability of human speech and motion, and risk producing oversmoothingartefacts and sub-optimal synthesis quality. We present the firstdiffusion-based probabilistic model, called Diff-TTSG, that jointly learns tosynthesise speech and gestures together. Our method can be trained on smalldatasets from scratch. Furthermore, we describe a set of careful uni- andmulti-modal subjective tests for evaluating integrated speech and gesturesynthesis systems, and use them to validate our proposed approach. Please seehttps://shivammehta25.github.io/Diff-TTSG/ for video examples, data, and code.</description><author>Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter</author><pubDate>Wed, 09 Aug 2023 13:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09417v3</guid></item><item><title>Service Reservation and Pricing for Green Metaverses: A Stackelberg Game Approach</title><link>http://arxiv.org/abs/2308.04914v1</link><description>Metaverse enables users to communicate, collaborate and socialize with eachother through their digital avatars. Due to the spatio-temporalcharacteristics, co-located users are served well by performing their softwarecomponents in a collaborative manner such that a Metaverse service provider(MSP) eliminates redundant data transmission and processing, ultimatelyreducing the total energy consumption. The energyefficient service provision iscrucial for enabling the green and sustainable Metaverse. In this article, wetake an augmented reality (AR) application as an example to achieve this goal.Moreover, we study an economic issue on how the users reserve offloadingservices from the MSP and how the MSP determines an optimal charging pricesince each user is rational to decide whether to accept the offloading serviceby taking into account the monetary cost. A single-leader multi-followerStackelberg game is formulated between the MSP and users while each useroptimizes an offloading probability to minimize the weighted sum of time,energy consumption and monetary cost. Numerical results show that our schemeachieves energy savings and satisfies individual rationality simultaneouslycompared with the conventional schemes. Finally, we identify and discuss opendirections on how several emerging technologies are combined with thesustainable green Metaverse.</description><author>Xumin Huang, Yuan Wu, Jiawen Kang, Jiangtian Nie, Weifeng Zhong, Dong In Kim, Shengli Xie</author><pubDate>Wed, 09 Aug 2023 13:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04914v1</guid></item><item><title>LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following</title><link>http://arxiv.org/abs/2308.04913v1</link><description>E-commerce authoring involves creating attractive, abundant, and targetedpromotional content to drive product sales. The emergence of large languagemodels (LLMs) introduces an innovative paradigm, offering a unified solution toaddress various authoring tasks within this scenario. However, mainstream LLMstrained on general corpora with common sense knowledge reveal limitations infitting complex and personalized features unique to e-commerce products andcustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,raising concerns about safeguarding voluminous customer privacy data duringtransmission. This paper proposes the LLaMA-E, the unified and customizedinstruction-following language models focusing on diverse e-commerce authoringtasks. Specifically, the domain experts create the seed instruction set fromthe tasks of ads generation, query-enhanced product title rewriting, productclassification, purchase intent speculation, and general Q&amp;A. These tasksenable the models to comprehensively understand precise e-commerce authoringknowledge by interleaving features covering typical service aspects ofcustomers, sellers, and platforms. The GPT-3.5 is introduced as a teachermodel, which expands the seed instructions to form a training set for theLLaMA-E models with various scales. The experimental results show that theproposed LLaMA-E models achieve state-of-the-art results in quantitative andqualitative evaluations, also exhibiting the advantage in zero-shot scenes. Tothe best of our knowledge, this study is the first to serve the LLMs tospecific e-commerce authoring scenarios.</description><author>Kaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, Qing Li</author><pubDate>Wed, 09 Aug 2023 13:26:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04913v1</guid></item><item><title>Cross-view Semantic Alignment for Livestreaming Product Recognition</title><link>http://arxiv.org/abs/2308.04912v1</link><description>Live commerce is the act of selling products online through live streaming.The customer's diverse demands for online products introduce more challenges toLivestreaming Product Recognition. Previous works have primarily focused onfashion clothing data or utilize single-modal input, which does not reflect thereal-world scenario where multimodal data from various categories are present.In this paper, we present LPR4M, a large-scale multimodal dataset that covers34 categories, comprises 3 modalities (image, video, and text), and is 50?larger than the largest publicly available dataset. LPR4M contains diversevideos and noise modality pairs while exhibiting a long-tailed distribution,resembling real-world problems. Moreover, a cRoss-vIew semantiC alignmEnt(RICE) model is proposed to learn discriminative instance features from theimage and video views of the products. This is achieved through instance-levelcontrastive learning and cross-view patch-level feature propagation. A novelPatch Feature Reconstruction loss is proposed to penalize the semanticmisalignment between cross-view patches. Extensive experiments demonstrate theeffectiveness of RICE and provide insights into the importance of datasetdiversity and expressivity. The dataset and code are available athttps://github.com/adxcreative/RICE</description><author>Wenjie Yang, Yiyi Chen, Yan Li, Yanhua Cheng, Xudong Liu, Quan Chen, Han Li</author><pubDate>Wed, 09 Aug 2023 13:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04912v1</guid></item><item><title>SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation</title><link>http://arxiv.org/abs/2308.04911v1</link><description>Medical image analysis using deep learning is often challenged by limitedlabeled data and high annotation costs. Fine-tuning the entire network inlabel-limited scenarios can lead to overfitting and suboptimal performance.Recently, prompt tuning has emerged as a more promising technique thatintroduces a few additional tunable parameters as prompts to a task-agnosticpre-trained model, and updates only these parameters using supervision fromlimited labeled data while keeping the pre-trained model unchanged. However,previous work has overlooked the importance of selective labeling in downstreamtasks, which aims to select the most valuable downstream samples for annotationto achieve the best performance with minimum annotation cost. To address this,we propose a framework that combines selective labeling with prompt tuning(SLPT) to boost performance in limited labels. Specifically, we introduce afeature-aware prompt updater to guide prompt tuning and a TandEm SelectiveLAbeling (TESLA) strategy. TESLA includes unsupervised diversity selection andsupervised selection using prompt-based uncertainty. In addition, we propose adiversified visual prompt tuning strategy to provide multi-prompt-baseddiscrepant predictions for TESLA. We evaluate our method on liver tumorsegmentation and achieve state-of-the-art performance, outperformingtraditional fine-tuning with only 6% of tunable parameters, also achieving 94%of full-data performance by labeling only 5% of the data.</description><author>Fan Bai, Ke Yan, Xiaoyu Bai, Xinyu Mao, Xiaoli Yin, Jingren Zhou, Yu Shi, Le Lu, Max Q. -H. Meng</author><pubDate>Wed, 09 Aug 2023 13:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04911v1</guid></item><item><title>Causal Fourier Analysis on Directed Acyclic Graphs and Posets</title><link>http://arxiv.org/abs/2209.07970v3</link><description>We present a novel form of Fourier analysis, and associated signal processingconcepts, for signals (or data) indexed by edge-weighted directed acyclicgraphs (DAGs). This means that our Fourier basis yields an eigendecompositionof a suitable notion of shift and convolution operators that we define. DAGsare the common model to capture causal relationships between data values and inthis case our proposed Fourier analysis relates data with its causes under alinearity assumption that we define. The definition of the Fourier transformrequires the transitive closure of the weighted DAG for which several forms arepossible depending on the interpretation of the edge weights. Examples includelevel of influence, distance, or pollution distribution. Our framework isdifferent from prior GSP: it is specific to DAGs and leverages, and extends,the classical theory of Moebius inversion from combinatorics. For aprototypical application we consider DAGs modeling dynamic networks in whichedges change over time. Specifically, we model the spread of an infection onsuch a DAG obtained from real-world contact tracing data and learn theinfection signal from samples assuming sparsity in the Fourier domain.</description><author>Bastian Seifert, Chris Wendler, Markus Püschel</author><pubDate>Wed, 09 Aug 2023 13:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.07970v3</guid></item><item><title>Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks</title><link>http://arxiv.org/abs/2308.04909v1</link><description>This paper focuses on the impact of leveraging autonomous offensiveapproaches in Deep Reinforcement Learning (DRL) to train more robust agents byexploring the impact of applying adversarial learning to DRL for autonomoussecurity in Software Defined Networks (SDN). Two algorithms, Double DeepQ-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN orN2D), are compared. NEC2DQN was proposed in 2018 and is a new member of thedeep q-network (DQN) family of algorithms. The attacker has full observabilityof the environment and access to a causative attack that uses statemanipulation in an attempt to poison the learning process. The implementationof the attack is done under a white-box setting, in which the attacker hasaccess to the defender's model and experiences. Two games are played; in thefirst game, DDQN is a defender and N2D is an attacker, and in second game, theroles are reversed. The games are played twice; first, without an activecausative attack and secondly, with an active causative attack. For execution,three sets of game results are recorded in which a single set consists of 10game runs. The before and after results are then compared in order to see ifthere was actually an improvement or degradation. The results show that withminute parameter changes made to the algorithms, there was growth in theattacker's role, since it is able to win games. Implementation of theadversarial learning by the introduction of the causative attack showed thealgorithms are still able to defend the network according to their strengths.</description><author>Luke Borchjes, Clement Nyirenda, Louise Leenen</author><pubDate>Wed, 09 Aug 2023 13:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04909v1</guid></item><item><title>Go Beyond The Obvious: Probing the gap of INFORMAL reasoning ability between Humanity and LLMs by Detective Reasoning Puzzle Benchmark</title><link>http://arxiv.org/abs/2307.05113v2</link><description>Informal reasoning ability is the ability to reason based on common sense,experience, and intuition.Humans use informal reasoning every day to extractthe most influential elements for their decision-making from a large amount oflife-like information.With the rapid development of language models, therealization of general artificial intelligence has emerged with hope. Given theoutstanding informal reasoning ability of humans, how much informal reasoningability language models have has not been well studied by scholars.In order toexplore the gap between humans and language models in informal reasoningability, this paper constructs a Detective Reasoning Benchmark, which is anassembly of 1,200 questions gathered from accessible online resources, aims atevaluating the model's informal reasoning ability in real-lifecontext.Considering the improvement of the model's informal reasoning abilityrestricted by the lack of benchmark, we further propose a Self-Question PromptFramework that mimics human thinking to enhance the model's informal reasoningability.The goals of self-question are to find key elements, deeply investigatethe connections between these elements, encourage the relationship between eachelement and the problem, and finally, require the model to reasonably answerthe problem.The experimental results show that human performance greatlyoutperforms the SoTA Language Models in Detective Reasoning Benchmark.Besides,Self-Question is proven to be the most effective prompt engineering inimproving GPT-4's informal reasoning ability, but it still does not evensurpass the lowest score made by human participants.Upon acceptance of thepaper, the source code for the benchmark will be made publicly accessible.</description><author>Zhouhon Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Haoning Ye, Yikai Zhang, Wenhao Huang, Xiaoxuan Zhu, Qianyu He, Rui Xu, Sihang Jiang, Shusen Wang, Zili Wang, Hongwei Feng, Zhixu Li, Yanghua Xiao</author><pubDate>Wed, 09 Aug 2023 13:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05113v2</guid></item><item><title>GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters</title><link>http://arxiv.org/abs/2308.04905v1</link><description>Congestion Control (CC) plays a fundamental role in optimizing traffic inData Center Networks (DCN). Currently, DCNs mainly implement two main CCprotocols: DCTCP and DCQCN. Both protocols -- and their main variants -- arebased on Explicit Congestion Notification (ECN), where intermediate switchesmark packets when they detect congestion. The ECN configuration is thus acrucial aspect on the performance of CC protocols. Nowadays, network expertsset static ECN parameters carefully selected to optimize the average networkperformance. However, today's high-speed DCNs experience quick and abruptchanges that severely change the network state (e.g., dynamic trafficworkloads, incast events, failures). This leads to under-utilization andsub-optimal performance. This paper presents GraphCC, a novel MachineLearning-based framework for in-network CC optimization. Our distributedsolution relies on a novel combination of Multi-agent Reinforcement Learning(MARL) and Graph Neural Networks (GNN), and it is compatible with widelydeployed ECN-based CC protocols. GraphCC deploys distributed agents on switchesthat communicate with their neighbors to cooperate and optimize the global ECNconfiguration. In our evaluation, we test the performance of GraphCC under awide variety of scenarios, focusing on the capability of this solution to adaptto new scenarios unseen during training (e.g., new traffic workloads, failures,upgrades). We compare GraphCC with a state-of-the-art MARL-based solution forECN tuning -- ACC -- and observe that our proposed solution outperforms thestate-of-the-art baseline in all of the evaluation scenarios, showingimprovements up to $20\%$ in Flow Completion Time as well as significantreductions in buffer occupancy ($38.0-85.7\%$).</description><author>Guillermo Bernárdez, José Suárez-Varela, Xiang Shi, Shihan Xiao, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio</author><pubDate>Wed, 09 Aug 2023 13:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04905v1</guid></item><item><title>StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability</title><link>http://arxiv.org/abs/2308.04904v1</link><description>Video shakiness is an unpleasant distortion of User Generated Content (UGC)videos, which is usually caused by the unstable hold of cameras. In recentyears, many video stabilization algorithms have been proposed, yet no specificand accurate metric enables comprehensively evaluating the stability of videos.Indeed, most existing quality assessment models evaluate video quality as awhole without specifically taking the subjective experience of video stabilityinto consideration. Therefore, these models cannot measure the video stabilityexplicitly and precisely when severe shakes are present. In addition, there isno large-scale video database in public that includes various degrees of shakyvideos with the corresponding subjective scores available, which hinders thedevelopment of Video Quality Assessment for Stability (VQA-S). To this end, webuild a new database named StableDB that contains 1,952 diversely-shaky UGCvideos, where each video has a Mean Opinion Score (MOS) on the degree of videostability rated by 34 subjects. Moreover, we elaborately design a novel VQA-Smodel named StableVQA, which consists of three feature extractors to acquirethe optical flow, semantic, and blur features respectively, and a regressionlayer to predict the final stability score. Extensive experiments demonstratethat the StableVQA achieves a higher correlation with subjective opinions thanthe existing VQA-S models and generic VQA models. The database and codes areavailable at https://github.com/QMME/StableVQA.</description><author>Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, Xiongkuo Min, Guangtao Zhai, Ning Liu</author><pubDate>Wed, 09 Aug 2023 13:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04904v1</guid></item><item><title>Towards true discovery of the differential equations</title><link>http://arxiv.org/abs/2308.04901v1</link><description>Differential equation discovery, a machine learning subfield, is used todevelop interpretable models, particularly in nature-related applications. Byexpertly incorporating the general parametric form of the equation of motionand appropriate differential terms, algorithms can autonomously uncoverequations from data. This paper explores the prerequisites and tools forindependent equation discovery without expert input, eliminating the need forequation form assumptions. We focus on addressing the challenge of assessingthe adequacy of discovered equations when the correct equation is unknown, withthe aim of providing insights for reliable equation discovery without priorknowledge of the equation form.</description><author>Alexander Hvatov, Roman Titov</author><pubDate>Wed, 09 Aug 2023 13:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04901v1</guid></item></channel></rss>